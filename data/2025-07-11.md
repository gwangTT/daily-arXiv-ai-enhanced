<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 26]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 27]
- [cs.CV](#cs.CV) [Total: 29]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.LG](#cs.LG) [Total: 32]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 24]
- [cs.SE](#cs.SE) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 6]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: This paper develops an LLM-based framework that integrates symbolic reasoning and adaptive control for fault recovery and process management in chemical processes.


<details>
  <summary>Details</summary>
Motivation: Modern chemical processes are increasingly complex and face workforce shortages and intricate fault scenarios, necessitating advanced automation methods.

Method: The authors propose an agentic framework using FSMs, where LLMs handle fault recovery planning and process control. The architecture includes planning agents, simulation agents, and a Validator-Reprompting loop to refine plans.

Result: In tests with 180 randomly generated FSMs, GPT-4 variants outperformed other LLMs in accuracy and latency, achieving 100% valid-path success. Additionally, the framework performed competitively against PID controllers in modulating heater inputs under disturbances.

Conclusion: Structured feedback and modular agents enable LLMs to unify high-level planning with low-level control, presenting a resilient automation approach for chemical engineering challenges.

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [2] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: The paper addresses bias in AI for painting classification by introducing BOOST, an OOD-informed bias-mitigation method, to improve fairness and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the pervasive bias present in painting classification AI models, which arises due to imbalanced training datasets and results in reduced accuracy for rarely seen painting styles. Existing research has failed to adequately address this issue, particularly in out-of-distribution (OOD) scenarios.

Method: The paper proposes BOOST (Bias-Oriented OOD Sampling and Tuning), a novel approach that uses dynamic temperature scaling and sampling probabilities to mitigate bias by promoting a fairer representation of less-represented classes in the training dataset.

Result: Experiments were conducted on KaoKore and PACS datasets. The results indicate that BOOST effectively reduces class-wise biases and achieves a balance between improving performance and fairness. Additionally, a new metric, Same-Dataset OOD Detection Score (SODC), was introduced to measure bias reduction.

Conclusion: BOOST provides a robust solution for mitigating bias in art-based AI models by addressing data imbalances, ensuring fairness, and maintaining high classification performance.

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [3] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: This paper introduces State-Inference-Based Prompting (SIBP) to address rule violations in trading systems within dynamic game interactions, achieving high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with implementing rule-governed trading systems in games due to errors like item hallucinations and calculation mistakes, undermining player trust.

Method: The paper proposes a systematic framework (State-Inference-Based Prompting - SIBP), decomposing trading interactions into six distinct dialogue states for rule adherence through autonomous state inference, item referencing, and placeholder-based calculations.

Result: The method showed excellent performance with >97% compliance to trading states, >95% accuracy in item referencing, and 99.7% precision in price calculations across 100 dialogues, outperforming baseline methods.

Conclusion: SIBP is a reliable and efficient solution that enhances trust in NPC interactions for commercial games, offering a practical method to address trading system issues.

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [4] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: The paper addresses illicit supply chain detection using neurosymbolic methods and a question-tree approach with language models for better activity identification.


<details>
  <summary>Details</summary>
Motivation: Supply chains are complex, and identifying illicit activities like counterfeit parts, forced labor, or human trafficking is even harder due to sparse and unreliable data.

Method: The study explores neurosymbolic methods and employs a question-tree approach with large language models to analyze and classify news articles describing illicit activities.

Result: The approach provides insights into the comparison between manual and AI-driven classification of articles related to illicit supply chain activities.

Conclusion: Neurosymbolic methods and the question-tree approach enhance the ability to detect and evaluate illicit patterns in supply chains, paving the way for intelligent solutions with limited data dependencies.

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [5] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: The paper introduces cmbagent, a multi-agent system of about 30 Large Language Model agents designed to automate scientific research tasks, achieving superior performance in cosmology research and benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for automating complex scientific research and workflows without human intervention, enhancing productivity and accuracy.

Method: The system uses a Planning & Control strategy with 30 LLM agents, each specializing in tasks like retrieving scientific information, coding, interpreting results, and critiquing outputs. It fully automates workflows and executes code locally.

Result: The system successfully completed a PhD-level cosmology task, measured cosmological parameters using supernova data, and outperformed state-of-the-art LLMs in two benchmark tests.

Conclusion: cmbagent demonstrates significant potential for automating scientific research tasks efficiently and accurately, providing superior results, and is accessible for public use via multiple platforms.

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [6] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: The paper explores using large-language models as expert planners to improve efficient exploration in multi-agent reinforcement learning for planning-based tasks.


<details>
  <summary>Details</summary>
Motivation: Efficient exploration is a challenge in multi-agent reinforcement learning due to its inherent complexities and the need for better methods to solve tasks in environments involving multiple agents.

Method: The study investigates leveraging large-language models as expert planners to guide efficient exploration in planning-based environments for multiple agents.

Result: The application of large-language models demonstrates potential in improving exploration strategies in multi-agent settings for planning-based tasks.

Conclusion: Large-language models can serve as effective expert planners, aiding in efficient exploration for multi-agent reinforcement learning problems.

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [7] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: The paper introduces ViDove, a multimodal translation agent leveraging visual and contextual information to outperform state-of-the-art models in translation quality.


<details>
  <summary>Details</summary>
Motivation: Current translation agents struggle with integrating multimodal inputs like visual context, limiting their real-world applicability and human-like translation quality.

Method: ViDove combines visual and contextual data, a multimodal memory system, and long-short-term memory modules enriched with domain-specific knowledge to enhance translation tasks.

Result: ViDove achieves a 28% improvement in BLEU scores and 15% in SubER over prior state-of-the-art methods. Additionally, it introduces DoveBench, a benchmark with 17 hours of human-annotated data.

Conclusion: ViDove sets a new standard for multimodal translation tasks and demonstrates the importance of integrating domain-specific and multimodal memory systems for advancing translation accuracy.

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [8] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: The paper investigates alignment issues in LLMs with a focus on filtering methods for input prompts and outputs. It reveals computational obstacles that make prompt and output filtering ineffective under cryptographic hardness assumptions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the safety challenge in the deployment of LLMs, specifically preventing the generation of harmful information through various filtering techniques.

Method: The study relies on theoretical computational arguments and cryptographic hardness assumptions to prove the inefficiency and intractability of filters for LLM prompts and outputs.

Result: The results show that efficient filters cannot distinguish adversarial prompts from benign ones, and that computationally tractable output filtering is infeasible in certain settings.

Conclusion: External filtering solutions are inadequate for LLM safety. Aligned AI systems must integrate intelligence with judgment, making filters insufficient without internal model adjustments.

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [9] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: This paper introduces Sim-to-Dec, an integrated simulation-decision framework designed to enhance supply chain transportation strategies for responsiveness and economic efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of designing low-risk, effective transportation strategies in supply chains, which require balancing responsiveness and economic objectives.

Method: The authors propose Sim-to-Dec, combining a generative simulation module based on autoregressive modeling and a history-future dual-aware decision model iteratively optimized with simulation feedback.

Result: Experiments with real-world datasets show that Sim-to-Dec improves timely delivery rates and profitability compared to existing methods.

Conclusion: Sim-to-Dec offers a robust and adaptive approach to supply chain transportation strategy design, demonstrating generalization across settings and effectiveness in enhancing decision-making processes.

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [10] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: This paper introduces DrugMCTS, a novel framework combining multiple approaches to improve drug repurposing using large language models without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of large language models in scientific domains like drug discovery, particularly their inefficacy in extended reasoning beyond pretrained knowledge and the computational inefficiency of existing techniques.

Method: DrugMCTS combines retrieval-augmented generation (RAG), multi-agent collaboration, and Monte Carlo Tree Search to structure and refine reasoning without the need for domain-specific fine-tuning.

Result: DrugMCTS achieved over 20% improvement versus Deepseek-R1 on Qwen2.5-7B-Instruct and displayed higher recall and robustness on DrugBank and KIBA datasets compared to other LLMs and deep learning methods.

Conclusion: Structured reasoning, agent collaboration, and feedback-driven search mechanisms are crucial for advancing drug discovery applications of large language models.

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [11] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan,Changjiu Jiang,Yu Duan,Mingcong Lei,Jiageng Li,Yitian Hong,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: StarDojo is a benchmark designed for evaluating AI agents in open-ended production-living simulations, featuring 1,000 tasks across domains like farming, crafting, and social interactions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks that simultaneously evaluate both production activities and social interactions of AI agents.

Method: The benchmark uses Stardew Valley as the foundation to create 1,000 tasks across five domains and provides tools for parallel environment execution and multimodal model evaluation.

Result: State-of-the-art models, like GPT-4.1, achieved only a 12.7% success rate, revealing significant deficits in multimodal reasoning and manipulation.

Conclusion: StarDojo serves as a tool to advance research and development of AI agents capable of thriving in complex, open-ended environments.

Abstract: Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [12] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle,Thomas McGee,Hamza Giaffar,Taylor Webb,Ida Momennejad*

Main category: cs.AI

TL;DR: This paper proposes AlgEval, a systematic framework to investigate the algorithms learned and used by large language models (LLMs), with a focus on interpretability and algorithmic understanding.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research into the algorithms behind LLMs' problem-solving methods, countering the current focus on performance scaling without understanding emergent algorithms.

Method: Introduces AlgEval, which identifies algorithmic primitives through analysis of LLM latent representations, attention, and inference-time compute, and tests algorithmic hypotheses via circuit-level analysis.

Result: Presents a case study that illustrates hypotheses about emergent search algorithms and validates them through analysis of attention patterns and hidden states.

Conclusion: Algorithmic explanations enable human-understandable interpretability, more efficient training, and novel architectures, offering a shift from resource-intensive scaling to principled computational understanding.

Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [13] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: This paper studies undesired facets in rule-based machine learning models and introduces algorithms to analyze them.


<details>
  <summary>Details</summary>
Motivation: Explaining ML predictions is critical in high-risk domains as incorrect explanations can mislead human decision-makers.

Method: The study develops algorithms to identify negative overlap and redundancies in rule-based ML models.

Result: Popular rule-based ML tools are found to induce rule sets with negative aspects such as redundancy and negative overlap.

Conclusion: Despite widespread usage, rule-based ML models designed to enhance interpretability often have intrinsic flaws that can impair explanation quality.

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [14] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su,Di Wang,Chunyan Miao*

Main category: cs.AI

TL;DR: The paper introduces Context Pooling, a novel graph pooling method, to improve GNN-based models for link prediction in Knowledge Graphs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Recent studies reveal that standard graph aggregation methods do not significantly improve GNN-based models for link prediction in Knowledge Graphs.

Method: The authors developed Context Pooling, a graph pooling technique that identifies logical neighbors and generates query-specific graphs for inductive settings. It includes metrics for neighborhood precision and recall.

Result: The proposed method achieves state-of-the-art performance in 42 out of 48 experimental settings on three public datasets.

Conclusion: Context Pooling is a pioneering and effective improvement for GNN-based link prediction models, applicable to both transductive and inductive settings.

Abstract: Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [15] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi,Jim Black,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila*

Main category: cs.AI

TL;DR: The study evaluates fine-tuned Llama 3.2 models for extracting vaccine-related data from emergency department notes, demonstrating their superiority in accuracy and feasibility for real-time vaccine safety surveillance.


<details>
  <summary>Details</summary>
Motivation: Developing efficient tools for real-time vaccine safety surveillance from clinical narratives, addressing challenges in automating data extraction.

Method: Fine-tuned Llama 3.2 models were compared with prompt-engineered models and a rule-based approach, leveraging prompt engineering and human annotation for dataset creation.

Result: The fine-tuned Llama 3 billion parameter model outperformed other models in accuracy of vaccine name extraction, with model quantization enabling efficient resource utilization.

Conclusion: Large language models show promise in enhancing vaccine safety surveillance by automating clinical data extraction efficiently, supporting early detection of adverse events.

Abstract: This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [16] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli,Thomas Krak,Cassio de Campos*

Main category: cs.AI

TL;DR: The paper proposes a framework for efficiently propagating uncertainty in credal networks, using Dempster-Shafer theory to compute belief and plausibility functions in chains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve uncertainty representation and computational efficiency when performing belief inference in a specialized class of credal networks using Dempster-Shafer theory.

Method: The authors developed a framework based on the propagation of belief and plausibility functions to compute conservative intervals within credal networks, specifically focusing on chain structures.

Result: The framework demonstrated computational efficiency and robust uncertainty representation, with a comparison to classical sensitivity analysis showing its advantages and limitations.

Conclusion: The proposed belief inference framework is a practical, efficient method for uncertainty representation in credal networks, particularly chains.

Abstract: This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [17] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov,Abdelrahman Eldesokey,Michael Birsak,John Femiani,Bernard Ghanem,Peter Wonka*

Main category: cs.AI

TL;DR: The paper introduces PlanQA, a benchmark for evaluating large-language models' geometric and spatial reasoning abilities using structured indoor scene representations. Results show LLMs struggle with physical constraints and layout coherence.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of large-language models (LLMs) in geometric and spatial reasoning, particularly for real-world indoor scene layouts.

Method: The benchmark evaluates LLMs using structured representations (e.g., JSON, XML) of indoor spaces and diverse spatial questions, covering metric, topological reasoning, and interior design constraints.

Result: Findings reveal that while LLMs can handle basic queries, they are deficient in simulating physical constraints, maintaining spatial coherence, and adapting to layout changes.

Conclusion: PlanQA highlights weaknesses in current LLMs' spatial reasoning abilities and calls for further development of models capable of handling real-world layout challenges.

Abstract: We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [18] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian,Kai Yang,Ye Ouyang,Xiaozhou Ye*

Main category: cs.AI

TL;DR: Direct Preference Optimization (DPO) is sensitive to initialization and may misallocate probability mass, which can compromise model alignment and stability. To address these limitations, a bilevel optimization framework with regularization is proposed, achieving improved performance in reasoning and summarization benchmarks.


<details>
  <summary>Details</summary>
Motivation: Understand the theoretical limitations of DPO and propose a more stable method to align language models with human preferences.

Method: Introduce a bilevel optimization framework that combines supervised fine-tuning with a stable DPO objective, supported by a regularization scheme to improve preference alignment and stabilization.

Result: The proposed method improves reasoning accuracy and aligns output distributions better with intended preferences, surpassing standard DPO in reasoning and summarization benchmarks.

Conclusion: Stable preference optimization enhances reliability and interpretability in preference-based alignment, addressing DPO's theoretical limitations while enabling improved model alignment outcomes.

Abstract: Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [19] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin,Anne-Emmanuelle Ceulemans,François Glineur*

Main category: cs.AI

TL;DR: The paper develops a method to classify violins as reduced or non-reduced based on their contour lines, using parameters extracted from 3D geometric data.


<details>
  <summary>Details</summary>
Motivation: While experts observe differences in contour lines between reduced and non-reduced violins, these differences have not been studied quantitatively.

Method: Using 3D geometric meshes of 25 violins, the authors extract contour lines and fit them to parabola-like equations to derive parameters. These parameters are then analyzed and used for classification.

Result: The study shows geometry can predict size reduction to some degree, with the opening parameter beta being the most predictive.

Conclusion: Quantitative analysis of contour lines offers a feasible way to classify reduced versus non-reduced violins, despite challenges with borderline cases.

Abstract: The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [20] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard,Akshaya Jagadeesh,Alex Cook,Steele Billings,Nicholas Skytland,Alicia Llewellyn,Jackson Paull,Nathan Paull,Nolan Kurylo,Keatra Nesbitt,Robert Gruenewald,Anthony Jantzi,Omar Chavez*

Main category: cs.AI

TL;DR: The paper presents the Flourishing AI Benchmark (FAI Benchmark) to evaluate AI alignment with human well-being across seven dimensions, revealing substantial gaps in alignment among leading language models.


<details>
  <summary>Details</summary>
Motivation: Traditional AI benchmarks focus on technical proficiency or harm prevention, but there's a gap in evaluating AI's alignment with holistic human well-being. This paper addresses the need for a framework to assess AI's contribution to human flourishing.

Method: The FAI Benchmark uses 1,229 objective and subjective questions, specialized judge LLMs, cross-dimensional evaluation, and geometric mean scoring to measure AI alignment with seven dimensions of human flourishing.

Result: Testing 28 leading language models showed that while some achieved a score of 72/100, none were fully aligned in all seven dimensions, especially in Faith and Spirituality, Character and Virtue, and Meaning and Purpose.

Conclusion: The research highlights the need to develop AI systems that actively promote human flourishing, providing a new evaluation framework with critical ethical and developmental implications for AI technology.

Abstract: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [21] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu,Jiaqian Yu,Xiongfeng Peng,Yiwei Chen,Weiming Li,Jaewook Yoo,Sunghyun Chunag,Dongwook Lee,Daehyun Ji,Chao Zhang*

Main category: cs.AI

TL;DR: The paper introduces MoSE, a skill-focused Mixture-of-Experts model, aimed at improving end-to-end autonomous driving by mimicking human learning and reasoning processes.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in generalization and optimization faced by autonomous driving systems using general Mixture-of-Experts models, and to mimic human drivers' dynamic learning.

Method: The paper proposes MoSE, which employs a skill-oriented routing mechanism and hierarchical skill datasets to enable step-by-step learning and reasoning akin to human drivers.

Result: MoSE demonstrates state-of-the-art performance on CODA AD corner case reasoning tasks, surpassing larger models (8B+ parameters) with less computational demand (by at least 62.5%).

Conclusion: MoSE efficiently achieves better performance by streamlining computational resources and integrating auxiliary tasks into a single forward process, offering a novel approach to autonomous driving systems.

Abstract: Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [22] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek,Keondo Park,Jeonggil Ko,Min-hwan Oh,Taesik Gong,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: This paper proposes adopting adaptive sensing methods in AI to mimic biological sensory adjustments, improving model efficiency while reducing environmental, economic, and ethical costs.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the significant environmental, economic, and ethical concerns associated with scaling neural models and datasets, as well as a desire for equitable access and sustainable AI.

Method: The authors advocate for adaptive sensing, where input sensor parameters are dynamically modulated to mitigate covariate shifts and enhance efficiency. They provide empirical evidence demonstrating that small models with adaptive sensing outperform larger ones. Additionally, they propose a roadmap for incorporating this approach into diverse real-world domains.

Result: Empirical data shows smaller models employing adaptive sensing outperform larger, computationally intensive models. Furthermore, the paper identifies new pathways for adaptive sensing integration and future research.

Conclusion: Adaptive sensing offers a sustainable and efficient alternative for advancing AI systems, with potential for widespread application across industries. It also represents a step toward equitable and responsible AI development.

Abstract: Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [23] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd,Ada Diaconescu,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: This paper introduces algorithms with polynomial complexity to identify actual causes for better interpretability in machine learning, overcoming limitations of existing approaches in non-boolean, black-box, and stochastic systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in current XAI and causality frameworks, which fail to provide explanations in terms of 'actual causes' that non-expert users expect, and to overcome the computational issues in identifying such causes.

Method: Proposed a set of algorithms capable of identifying actual causes with polynomial complexity and tunable precision and exhaustiveness.

Result: The algorithms effectively identify causes in various system types (non-boolean, black-box, stochastic) and offer adjustable precision and computational trade-offs.

Conclusion: The proposed approach advances causal explanation in machine learning, addressing computational and practical challenges in identifying actual causes.

Abstract: Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [24] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang,Na Zhao,Jianglong Qing,Qing xu,Kaiwen Pan,Ting luo*

Main category: cs.AI

TL;DR: This paper introduces a novel framework combining prompt engineering and knowledge graphs to address limitations in legal dispute analysis by large language models, demonstrating improved performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of large language models in analyzing legal disputes, such as inadequate legal knowledge representation, shallow concept understanding, and weak reasoning capabilities.

Method: The approach includes a three-stage hierarchical prompt structure (task definition, knowledge background, reasoning guidance) and a three-layer knowledge graph (classification ontology, representation, instance layers). It uses legal reasoning templates, dynamic optimization, and semantic retrieval methods combined with web search technologies.

Result: Experimental results show significant improvements in analyzing legal disputes and a deeper comprehension of judicial decision-making logic for complex cases.

Conclusion: The framework provides a breakthrough in integrating AI with legal systems, offering a robust technical pathway for developing intelligent legal assistance systems.

Abstract: The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


### [25] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: The paper argues that diminishing returns in compute scaling will lead to smaller AI models achieving similar performance as larger, resource-intensive models, with important implications for AI strategy and policy.


<details>
  <summary>Details</summary>
Motivation: Address inequalities in AI model performance caused by the dominance of a few large companies scaling AI systems with immense computational resources.

Method: Developed a theoretical model to show diminishing returns of compute scaling, analyzed training loss proxies, and studied benchmark and empirical data on AI capability differences over time.

Result: Demonstrated that diminishing computational returns will diminish the performance gap between large and smaller AI models, with meek (resource-limited) models approaching the capabilities of top-tier models.

Conclusion: Suggests a closer performance parity among AI models in the future and calls for a reevaluation of AI strategy and policy to account for these changes.

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [26] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: This paper analyzes how generative AI assists in work activities across various occupations using data from anonymized interactions with Microsoft Bing Copilot, proposing an AI applicability score based on activity success and impact.


<details>
  <summary>Details</summary>
Motivation: To understand the effect of generative AI on economic activities, given its widespread adoption and transformative potential across diverse tasks.

Method: An analysis of 200k anonymized conversations with Bing Copilot to identify performed work activities, measure task success, and compute AI applicability scores for various occupations.

Result: The study found that information gathering, writing, teaching, and advising are commonly performed AI-assisted activities, and high AI applicability scores are linked to knowledge work occupations and sales.

Conclusion: Generative AI is most effective in assisting with knowledge-intensive and communicative tasks, with its impact varying across occupations based on task type and success.

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [27] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: The paper proposes MM2IM, a hardware-software co-designed accelerator for optimizing Transposed Convolutions (TCONV) on edge devices, demonstrating substantial performance and energy efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Current methods for implementing TCONV have inefficiencies like complex mapping, overlapping sums, and unused computations, causing a bottleneck in generative AI models, especially on resource-limited edge devices.

Method: The authors propose MM2IM, which uses hardware-software co-design to combine Matrix Multiplication (MatMul) with col2IM for efficient TCONV processing. They implemented MM2IM using the SECDA-TFLite toolkit.

Result: MM2IM achieved a 1.9x average speedup across 261 TCONV configurations, up to 4.2x speedup on TCONV layers from generative models, and outperformed similar accelerators by at least 2x GOPs/DSP. Additionally, it sped up DCGAN and pix2pix GAN models by up to 3x and reduced energy usage by 2.4x against a CPU baseline.

Conclusion: MM2IM effectively addresses TCONV inefficiencies for edge devices, improving speed and energy efficiency, and outperforms existing solutions in resource-constrained settings.

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: Large language models (LLMs) exhibit cognitive biases, influenced more by pretraining rather than finetuning or training randomness.


<details>
  <summary>Details</summary>
Motivation: To understand the origin of cognitive biases in LLMs and disentangle contributions from pretraining, finetuning, and training randomness.

Method: Two-step causal experimental approach: (1) Multiple finetunings with different random seeds to examine training stochasticity, and (2) cross-tuning by swapping instruction datasets between models to isolate bias sources.

Result: Findings show training randomness introduces minimal variability, while biases are primarily shaped by pretraining; models sharing pretrained backbones have more similarity in bias patterns than those sharing finetuning data.

Conclusion: Understanding and mitigating biases in finetuned models require insights into their pretraining mechanisms, emphasizing the need for principled evaluation strategies.

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [29] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: This paper examines the robustness of large language models (LLMs) in social science surveys, finding that they are prone to biases, such as recency bias, and sensitive to question perturbations.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can reliably act as proxies for human subjects in normative survey scenarios and understand their susceptibility to known response biases.

Method: Tested nine LLMs on World Values Survey (WVS) questions with 11 types of perturbations (e.g., question phrasing changes, answer structure variations) and conducted 167,000 simulated interviews to analyze robustness.

Result: LLMs are vulnerable to perturbations, exhibit consistent recency bias, and even larger models are sensitive to semantic variations and combined perturbations.

Conclusion: Prompt design and robustness testing are essential when using LLMs to generate synthetic survey data, as they align partially with human survey response biases but remain fragile under variations.

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [30] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: SynthTextEval is a toolkit designed for holistic evaluation of synthetic text, focusing on dimensions like utility, fairness, privacy risks, and expert feedback.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for consistent and principled evaluations of synthetic text, which is increasingly significant due to its potential in reducing privacy risks in sensitive domains like healthcare and law.

Method: The paper introduces a toolkit, SynthTextEval, which enables evaluations on synthetic text across several dimensions, such as downstream utility, fairness, privacy leakage risks, distribution differences, and qualitative expert insights.

Result: SynthTextEval demonstrates its capabilities effectively in evaluating synthetic data for high-stakes domains such as healthcare and law.

Conclusion: By standardizing evaluation metrics and demonstrating their importance, the toolkit advances the viability of synthetic text while enhancing privacy-preserving AI development.

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [31] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: This paper introduces a safety evaluation protocol tailored for medical large language models (LLMs), focusing on patient, clinician, and general user perspectives, and presents a new dataset, PatientSafetyBench, to address critical safety concerns.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the safety challenges of deploying medical LLMs, as their outputs can directly impact human health and are utilized by diverse users, necessitating domain-specific safety evaluations.

Method: The authors proposed a tailored safety evaluation protocol specific to the medical domain, introduced a dataset (PatientSafetyBench) with 466 samples across five safety-critical categories, and applied red-teaming protocols on the MediPhi model as a case study.

Result: The study quantitatively analyzed the safety of medical LLMs and demonstrated the utility of their red-teaming protocols and PatientSafetyBench dataset to evaluate safety from multiple user perspectives.

Conclusion: This work establishes a foundational framework for assessing and improving the safety of medical LLMs in diverse user scenarios, paving the way for safer applications in healthcare.

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [32] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: The paper develops a method to identify interruptions robustly in overlapping speech scenarios, especially in classroom settings with multiple concurrent group conversations.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of existing interruption detection systems, which primarily operate in single-conversation environments with clean audio, making them unsuitable for complex classroom settings where overlapping speech is widespread.

Method: The researchers analyze interruption detection in both single-conversation and multi-group dialogue settings and develop a state-of-the-art method incorporating linguistic and prosodic features to handle overlapping speech effectively.

Result: The proposed method is capable of reliably identifying interruptions in environments with overlapping speech, and it extracts meaningful linguistic and prosodic insights on interruptions in collaborative groups.

Conclusion: The findings establish the groundwork for deploying AI systems in classrooms to monitor collaborative group interactions and underscore the importance of accounting for overlapping speech when tracking group dialogues.

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [33] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: The paper proposes a Multi-agent Retrieval-Augmented Framework using multiple large language models to enhance the generation of counterspeech against health misinformation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating counterspeech suffer from limited evidence and lack control over the final output.

Method: The framework utilizes multiple LLMs and incorporates static and dynamic evidence into three steps: knowledge retrieval, evidence enhancement, and response refinement.

Result: The approach outperformed baseline methods in terms of politeness, relevance, informativeness, and factual accuracy, validated by both ablation studies and human evaluations.

Conclusion: The proposed framework effectively produces high-quality counterspeech addressing health misinformation with improved refinement and human preference.

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [34] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: The paper introduces a model combining Graph Neural Networks and Convolutional Neural Networks for efficient long-text processing, reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art models like Transformers are computationally inefficient for long texts due to quadratic complexity; hence, there's a need for more efficient architectures.

Method: A hybrid model utilizes character-level inputs, lattice-based graph structures, CNNs for local contexts, and information from Large Language Models, processed via real-time graph creation.

Result: Generated graphs showed notable semantic organization, and the model performed competitively in sentiment analysis and news categorization tasks.

Conclusion: The proposed architecture is efficient and effective for text classification, providing a viable alternative to Transformer-based models for processing extended documents.

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [35] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: The paper introduces MedReadCtrl, a framework for making AI-generated medical content more readable and accessible without losing meaning.


<details>
  <summary>Details</summary>
Motivation: Healthcare applications of generative AI require effective and personalized human-AI communication, especially for making medical content understandable to diverse audiences.

Method: The researchers developed MedReadCtrl, a readability-controlled instruction tuning framework for Adjusting LLM (Large Language Model) output complexity while preserving the original meaning.

Result: MedReadCtrl outperformed baseline models like GPT-4 across multiple metrics and domains, particularly excelling in readability, clinical task performance, and expert preference testing (71.7% preferred MedReadCtrl).

Conclusion: MedReadCtrl offers a scalable tool to enhance patient education and improve equitable access to care by providing readability-tuned, medically accurate AI outputs.

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [36] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: The study presents a scalable pipeline using large language models (LLMs) for effectively extracting eviction statuses and creating a large eviction-related social determinants of health (SDoH) dataset from unstructured clinical notes.


<details>
  <summary>Details</summary>
Motivation: Eviction, as an overlooked SDoH, has significant implications for health outcomes such as housing instability, unemployment, and mental health issues. However, eviction-related data is mostly uncoded in EHRs, which restricts its applications.

Method: The researchers developed a pipeline named SynthEHR-Eviction, combining large language models (LLMs), human-in-the-loop annotation, automated prompt optimization (APO), and fine-tuning to extract eviction data from clinical notes.

Result: Using the pipeline, the team created the largest eviction-related SDoH dataset comprising 14 categories, with fine-tuned LLMs achieving high Macro-F1 scores (88.8% for eviction and 90.3% for other SDoH). The pipeline improved annotation efficiency by over 80%.

Conclusion: The SynthEHR-Eviction pipeline facilitates scalable eviction detection from clinical notes, significantly reduces data annotation workload, outperforms other methods, and generalizes to broader information extraction tasks.

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [37] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: The paper explores distilling time series reasoning into small language models using synthetic datasets and systematic fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Develop interpretable small models for analyzing and explaining time series data effectively in natural language.

Method: Synthetic training datasets paired with multimodal annotations fine-tune compact models using evaluation metrics focused on specific properties like trend direction and extremum localization.

Result: Post-trained models demonstrate improved interpretive capabilities for capturing trends, noise patterns, and extremums in time series.

Conclusion: Successfully compressing time series understanding into lightweight models shows promise for interpretable and privacy-sensitive deployments.

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [38] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

TL;DR: The paper introduces the SAND framework, enabling LLM agents to better deliberate over candidate actions before deciding, leading to improved performance in interactive tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM tuning methods over-commit to plausible but suboptimal actions because of limited action space exploration, motivating the need for a framework that improves action deliberation.

Method: The paper proposes SAND, which leverages self-consistency action sampling and execution-guided action critique for step-wise deliberation. This generates iterative trajectories to finetune LLM agents.

Result: SAND achieves an average 20% improvement over initial supervised finetuning and outperforms state-of-the-art methods in representative interactive agent tasks.

Conclusion: The SAND framework significantly enhances reasoning and decision-making in LLM agents, addressing the shortcomings of traditional finetuning approaches.

Abstract: Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [39] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

TL;DR: This paper introduces RLEP, a two-phase reinforcement learning framework for large language models. It focuses on verified trajectories and replaying successful ones to improve learning stability and performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the instability and energy-intensive nature of reinforcement learning for large language models, as well as the issue of policy drifting from pretrained weights.

Method: RLEP uses a two-phase framework where verified trajectories are collected first and then replayed during further training. It optimizes with mini-batches blending new rollouts and replayed high-quality examples to focus learning on relevant paths.

Result: The paper reports faster convergence and improved accuracy on mathematical reasoning tasks compared to baselines, with performance gains ranging from 1.7% to 5.2% across different challenges.

Conclusion: RLEP effectively enhances reinforcement learning for large language models by steering training towards high-quality reasoning trajectories, improving stability, convergence speed, and final accuracy.

Abstract: Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [40] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: The paper conceptualizes 'machine bullshit' as LLM-generated statements indifferent to truth, introduces the Bullshit Index for quantification, and proposes a taxonomy for its forms.


<details>
  <summary>Details</summary>
Motivation: To address the emergent issue of large language models losing truthfulness and create a framework to understand and mitigate this behavior.

Method: Empirical evaluations using datasets, including Marketplace, Political Neutrality, and newly introduced BullshitEval. Analysis of machine bullshit under different fine-tuning techniques and inference strategies such as RLHF and CoT prompting.

Result: Results reveal that fine-tuning with RLHF intensifies machine bullshit, CoT prompting amplifies certain forms, and political contexts show dominance of specific bullshit strategies like weasel words.

Conclusion: The paper highlights challenges in achieving AI alignment, proposes methods to better understand machine truthfulness, and stresses the need for systematic solutions to address bullshit behaviors in LLMs.

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [41] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

TL;DR: The paper introduces PLAN-TUNING, a method to improve smaller language models' reasoning skills via planning strategies distilled from larger models.


<details>
  <summary>Details</summary>
Motivation: To enhance smaller open-source LLMs' problem-solving and reasoning abilities using post-training techniques inspired by human-like planning.

Method: PLAN-TUNING involves extracting synthetic 'planning trajectories' from large LLMs and fine-tuning smaller models using supervised and reinforcement learning objectives.

Result: Plan-tuned models achieve approximately 7% improvement on GSM8k and MATH benchmarks, with even better generalization on out-of-domain datasets (10% on OlympiadBench, 12% on AIME 2024).

Conclusion: PLAN-TUNING effectively improves smaller LLMs' reasoning performance by leveraging planning strategies, highlighting its potential for task-specific advancements.

Abstract: Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [42] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

TL;DR: TeaR is a system to enhance reasoning abilities of Language Models (LLMs) through optimized reasoning paths in code-related tasks, yielding substantial performance gains.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with overfitting to algorithmic patterns in code simulation tasks, hindering core reasoning abilities.

Method: TeaR utilizes meticulous data curation and reinforcement learning to teach optimized reasoning paths in code-related challenges.

Result: Experiments using diverse models and benchmarks indicate up to 35.9% improvement on reasoning tasks for selected models.

Conclusion: TeaR successfully refines reasoning abilities in LLMs by addressing inefficiencies in code simulation methodologies and leveraging improved reasoning pathways.

Abstract: Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [43] [Why is Your Language Model a Poor Implicit Reward Model?](https://arxiv.org/abs/2507.07981)
*Noam Razin,Yong Lin,Jiarui Yao,Sanjeev Arora*

Main category: cs.CL

TL;DR: This paper investigates the generalization gap between implicit reward models (IM-RMs) and explicit reward models (EX-RMs) in language models, finding that IM-RMs overly rely on superficial token-level cues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why implicit reward models (IM-RMs) generalize worse out-of-distribution compared to explicit reward models (EX-RMs), despite their architectural similarity.

Method: The authors employ theoretical analysis and experiments, comparing the behavior and generalization performance of IM-RMs and EX-RMs, while testing alternative hypotheses for the performance gap.

Result: The study finds that IM-RMs rely more on superficial token-level cues, leading to poorer generalization under token-level distribution shifts, as well as in-distribution. Alternative explanations, such as task difficulty differences, are ruled out.

Conclusion: Small design choices, such as how reward is computed, significantly influence the generalization behavior of reward models. IM-RMs are less robust due to their reliance on token-level cues.

Abstract: Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.

</details>


### [44] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

TL;DR: This paper develops a method combining DyGIE++ and multiple pre-trained BERT models to extract structured information on ORR catalysts from scientific literature.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of extracting structured information about ORR catalysts due to the complexity and diversity in scientific literature.

Method: They propose using named entity recognition (NER) and relation extraction (RE) models, specifically DyGIE++ fine-tuned with domain-specific BERT variants, on a manually annotated dataset for enhanced information extraction accuracy.

Result: The PubMedBERT model achieved the highest NER F1-score (82.19%) and the MatSciBERT attained the best RE F1-score (66.10%), demonstrating a high degree of reliability.

Conclusion: Domain-specific BERT models are more effective than general ones for ORR catalyst data extraction, showing potential for scalable automated literature analysis.

Abstract: The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


### [45] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

TL;DR: The paper investigates the limitations of transformer-based large language models (LLMs), particularly their inability to perform or verify computational and agentic tasks beyond a certain complexity.


<details>
  <summary>Details</summary>
Motivation: To understand the limitations of LLMs in performing and verifying tasks, especially in the context of agentic uses and their susceptibility to hallucinations.

Method: The authors analyze LLM capabilities through the lens of computational complexity, providing theoretical arguments and examples.

Result: LLMs are proven to be incapable of executing or verifying tasks that surpass a certain level of complexity.

Conclusion: The study highlights inherent limitations in LLMs, stressing the importance of recognizing their boundaries for real-world applications.

Abstract: With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [46] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: The paper introduces a framework to improve psychological counseling support by creating and refining a Chinese Psychological Support Dialogue Dataset (CPsDD), and a Comprehensive Agent Dialogue Support System (CADSS).


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of psychological support datasets, especially in non-English languages, making it challenging to develop effective counseling systems.

Method: The authors fine-tuned large language models using limited real-world data and expert knowledge to generate and refine psychological counseling dialogues. They created CPsDD using predefined paths for strategy and expert refinement. CADSS was built with components like a Profiler, Summarizer, Planner, and Supporter.

Result: Their CADSS system demonstrated state-of-the-art results in Strategy Prediction and Emotional Support Conversation tasks on CPsDD and ESConv datasets.

Conclusion: The framework effectively addresses the lack of non-English psychological support datasets and provides a robust system for delivering empathetic responses and scalable support strategies.

Abstract: The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [47] [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
*Mikey Elmers,Koji Inoue,Divesh Lala,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: This paper adapts voice activity projection (VAP) for triadic (three-person) conversations, demonstrating its effectiveness in predicting turn-taking using acoustic data.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in turn-taking prediction research, expanding it from dyadic to triadic multi-party conversations.

Method: VAP models were developed and trained on a Japanese dataset of triadic conversations across multiple topics to forecast voice activity for each speaker.

Result: VAP models trained on triadic conversations exceeded baseline performance, though accuracy varied depending on the type of conversation.

Conclusion: VAP is a viable tool for turn-taking prediction in triadic dialogues, with potential for integrating into spoken dialogue systems in future research.

Abstract: Turn-taking is a fundamental component of spoken dialogue, however
conventional studies mostly involve dyadic settings. This work focuses on
applying voice activity projection (VAP) to predict upcoming turn-taking in
triadic multi-party scenarios. The goal of VAP models is to predict the future
voice activity for each speaker utilizing only acoustic data. This is the first
study to extend VAP into triadic conversation. We trained multiple models on a
Japanese triadic dataset where participants discussed a variety of topics. We
found that the VAP trained on triadic conversation outperformed the baseline
for all models but that the type of conversation affected the accuracy. This
study establishes that VAP can be used for turn-taking in triadic dialogue
scenarios. Future work will incorporate this triadic VAP turn-taking model into
spoken dialogue systems.

</details>


### [48] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: This paper demonstrates the success of large language models (LLMs) in multilingual subjectivity detection through few-shot learning, achieving top rankings in the CheckThat! 2025 campaign.


<details>
  <summary>Details</summary>
Motivation: To explore robust methods for multilingual subjectivity detection, particularly addressing challenges posed by noisy data and inconsistent annotations.

Method: Using few-shot prompting with large language models (LLMs) and experimenting with advanced prompt engineering techniques like debating LLMs and particular example selection strategies.

Result: LLMs with carefully crafted prompts outperformed fine-tuned smaller language models (SLMs), achieving first place in Arabic and Polish and top rankings in other languages.

Conclusion: LLMs paired with few-shot learning provide an effective, adaptable approach for multilingual sentiment tasks, especially in settings with limited or inconsistent labeled data.

Abstract: This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [49] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

TL;DR: The paper studies Arabic-English retrieval-augmented generation (RAG) in domain-specific settings, finding that retrieval is a bottleneck when user query and document languages differ, and proposes a simple strategy to improve multilingual retrieval.


<details>
  <summary>Details</summary>
Motivation: To address the gap in cross-lingual RAG research within domain-specific contexts, especially considering the limitations of existing benchmarks that hide retrieval challenges due to language imbalances and pretraining data overlaps.

Method: The study involves developing benchmarks from real-world corporate datasets, examining query-document language combinations systematically, and proposing a retrieval strategy that enforces equal document retrieval from both languages.

Result: The study finds that retrieval is the main weakness in cross-lingual domain-specific RAG, particularly with language mismatches between queries and documents. The proposed strategy significantly improves retrieval performance across languages.

Conclusion: The research underscores the importance of focusing on retrieval improvements for effective domain-specific cross-lingual RAG and demonstrates that simple strategies can yield substantial practical benefits.

Abstract: Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [50] [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
*Jierun Chen,Tiezheng Yu,Haoli Bai,Lewei Yao,Jiannan Wu,Kaican Li,Fei Mi,Chaofan Tao,Lei Zhu,Manyi Zhang,Xiaohui Li,Lu Hou,Lifeng Shang,Qun Liu*

Main category: cs.CL

TL;DR: This paper investigates the impact of combining post-training techniques, supervised fine-tuning (SFT) and reinforcement learning (RL), in large vision-language models and finds no additive benefits despite their individual strengths.


<details>
  <summary>Details</summary>
Motivation: The authors seek to understand whether combining post-training techniques like SFT and RL can enhance the reasoning capabilities of vision-language models (VLMs) and overcome their individual limitations.

Method: The study rigorously evaluates multiple multimodal reasoning benchmarks using distinct and combined approaches to long chain-of-thought supervised fine-tuning (SFT) and reinforcement learning (RL). It assesses performance using varying training strategies.

Result: SFT excels in tackling difficult reasoning tasks but causes verbosity, while RL enhances generalization and brevity across task difficulty levels. However, their combination fails to yield additive performance benefits, creating trade-offs in accuracy and reasoning style.

Conclusion: The findings emphasize the need for more adaptive and integrated methods to effectively combine SFT and RL for improved reasoning in VLMs. Current approaches do not synergistically elevate performance.

Abstract: Large vision-language models (VLMs) increasingly adopt post-training
techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and
reinforcement learning (RL) to elicit sophisticated reasoning. While these
methods exhibit synergy in language-only models, their joint effectiveness in
VLMs remains uncertain. We present a systematic investigation into the distinct
roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning
benchmarks. We find that SFT improves performance on difficult questions by
in-depth, structured reasoning, but introduces verbosity and degrades
performance on simpler ones. In contrast, RL promotes generalization and
brevity, yielding consistent improvements across all difficulty levels, though
the improvements on the hardest questions are less prominent compared to SFT.
Surprisingly, combining them through two-staged, interleaved, or progressive
training strategies, as well as data mixing and model merging, all fails to
produce additive benefits, instead leading to trade-offs in accuracy, reasoning
style, and response length. This ``synergy dilemma'' highlights the need for
more seamless and adaptive approaches to unlock the full potential of combined
post-training techniques for reasoning VLMs.

</details>


### [51] [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: This paper introduces M4Doc, a framework enhancing Document Image Machine Translation (DIMT) by aligning an image-only encoder with Multimodal Large Language Models (MLLMs) for better visual-textual understanding.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in DIMT, particularly the lack of training data and the complex interplay of visual and textual information in document images.

Method: The proposed framework, M4Doc, aligns an image-only encoder with multimodal representations from pre-trained MLLMs, enabling lightweight DIMT models to learn visual-textual correlations efficiently. During inference, MLLMs are bypassed for computational efficiency.

Result: Comprehensive experiments showcase improved translation quality, enhanced cross-domain generalization, and better performance in handling challenging document image scenarios.

Conclusion: M4Doc represents significant progress in DIMT by effectively combining visual-textual knowledge, offering improved generalization and efficiency without relying on MLLMs during inference.

Abstract: Document Image Machine Translation (DIMT) aims to translate text within
document images, facing generalization challenges due to limited training data
and the complex interplay between visual and textual information. To address
these challenges, we introduce M4Doc, a novel single-to-mix modality alignment
framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an
image-only encoder with the multimodal representations of an MLLM, pre-trained
on large-scale document image datasets. This alignment enables a lightweight
DIMT model to learn crucial visual-textual correlations during training. During
inference, M4Doc bypasses the MLLM, maintaining computational efficiency while
benefiting from its multimodal knowledge. Comprehensive experiments demonstrate
substantial improvements in translation quality, especially in cross-domain
generalization and challenging document image scenarios.

</details>


### [52] [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
*Cooper Doyle*

Main category: cs.CL

TL;DR: This paper demonstrates a Bayesian framework for discrete-diffusion language models, achieving state-of-the-art performance using an ensemble inference approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to uncover a Bayesian interpretation of discrete-diffusion language models, enabling better token probability estimation and uncertainty quantification.

Method: The method involves showing that the expected denoiser output under the forward masking distribution recovers the posterior over clean tokens, with Monte Carlo marginalization converging to this posterior. The authors propose an ensemble of K mask-and-denoise passes for enhanced inference.

Result: The method achieves test perplexity of 8.8 on WikiText-2 with K=8, significantly outperforming GPT-2 Small (20.3 perplexity) using models of comparable size.

Conclusion: The proposed Bayesian-based technique offers a lightweight, training-free refinement enabling accurate token probabilities and uncertainty estimates, resulting in improved language model performance.

Abstract: We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.

</details>


### [53] [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
*Joyeeta Datta,Niclas Doll,Qusai Ramadan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: LLMs can be compressed using Knowledge Distillation (KD) while maintaining over 90% performance on QA tasks, reducing parameter counts significantly.


<details>
  <summary>Details</summary>
Motivation: To address the computational demands of LLMs and make them deployable in resource-constrained environments.

Method: The study applies Knowledge Distillation to compress LLMs and evaluates their performance on QA benchmarks (SQuAD and MLQA) under zero-shot and one-shot prompting.

Result: Student models achieve over 90% performance retention and reduce parameters by up to 57.1%, with one-shot prompting showing additional performance improvements.

Conclusion: KD is effective for creating compact and capable QA systems, demonstrating a balance between efficiency and performance for restricted-resource environments.

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.

</details>


### [54] [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
*Abhinav Java,Srivathsan Koundinyan,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: The paper explores efficient methods of answering complex questions using retrieval-augmented generation (RAG) models, emphasizing that improved prompting can outperform more resource-intensive state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Improve the efficiency of retrieval and reasoning processes for answering complex questions using large language models, while challenging existing claims about the necessity of large-scale fine-tuning.

Method: The authors utilized a standard ReAct pipeline with enhanced prompts and incorporated minimal supervised and RL-based fine-tuning to evaluate gains in both RAG accuracy and computational frugality.

Result: Improved prompts outperformed state-of-the-art methods in benchmarks like HotPotQA, and the proposed approach achieved competitive RAG metrics while reducing retrieval search costs by nearly half.

Conclusion: Large-scale fine-tuning is unnecessary for improving RAG metrics; enhanced prompts and minimal fine-tuning achieve high performance with lower computational costs.

Abstract: We consider the problem of answering complex questions, given access to a
large unstructured document corpus. The de facto approach to solving the
problem is to leverage language models that (iteratively) retrieve and reason
through the retrieved documents, until the model has sufficient information to
generate an answer. Attempts at improving this approach focus on
retrieval-augmented generation (RAG) metrics such as accuracy and recall and
can be categorized into two types: (a) fine-tuning on large question answering
(QA) datasets augmented with chain-of-thought traces, and (b) leveraging
RL-based fine-tuning techniques that rely on question-document relevance
signals. However, efficiency in the number of retrieval searches is an equally
important metric, which has received less attention. In this work, we show
that: (1) Large-scale fine-tuning is not needed to improve RAG metrics,
contrary to popular claims in recent literature. Specifically, a standard ReAct
pipeline with improved prompts can outperform state-of-the-art methods on
benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help
RAG from the perspective of frugality, i.e., the latency due to number of
searches at inference time. For example, we show that we can achieve
competitive RAG metrics at nearly half the cost (in terms of number of
searches) on popular RAG benchmarks, using the same base model, and at a small
training cost (1000 examples).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

TL;DR: The paper focuses on improving Multimodal Entity Linking (MEL) with a proposed Multi-level Mixture of Experts (MMoE) model, addressing mention ambiguity and dynamic modal content selection, and demonstrating superior performance over existing techniques.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of Multimodal Entity Linking by addressing the challenges of mention ambiguity and dynamic selection of significant modal content, shortcomings in existing approaches.

Method: The proposed Multi-level Mixture of Experts (MMoE) model introduces four components: (i) a description-aware mention enhancement module using large language models, (ii) a multimodal feature extraction module, (iii) an intra-level mixture of experts, and (iv) an inter-level mixture of experts modules that dynamically select relevant features.

Result: Experiments indicate the proposed MMoE model outperforms state-of-the-art methods in Multimodal Entity Linking tasks and demonstrates excellent adaptability and effectiveness.

Conclusion: The MMoE model addresses key challenges in Multimodal Entity Linking and offers a robust, adaptive approach for improving matching accuracy, with publicly available code for reproducibility.

Abstract: Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [56] [CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings](https://arxiv.org/abs/2507.07125)
*Cristina Mata,Kanchana Ranasinghe,Michael S. Ryoo*

Main category: cs.CV

TL;DR: The paper introduces CoPT, a novel loss function leveraging domain-agnostic text embeddings for unsupervised domain adaptation (UDA) in semantic segmentation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in UDA for semantic segmentation by utilizing text embeddings, as recent advancements in vision-language learning have not yet been effectively applied in this area.

Method: The method involves using the CoPT loss function, based on domain-agnostic text embeddings generated through a process called LLM Domain Template. A pre-trained CLIP model processes text descriptions of source and target domains generated by an LLM, which are then used to learn domain-invariant segmentation features.

Result: The experiments on four benchmarks demonstrate that CoPT significantly outperforms existing methods, setting a new state-of-the-art for UDA in semantic segmentation.

Conclusion: CoPT effectively leverages domain-agnostic text representations to overcome the domain adaptation challenge in semantic segmentation, advancing the field and providing a new standard for future research.

Abstract: Unsupervised domain adaptation (UDA) involves learning class semantics from
labeled data within a source domain that generalize to an unseen target domain.
UDA methods are particularly impactful for semantic segmentation, where
annotations are more difficult to collect than in image classification. Despite
recent advances in large-scale vision-language representation learning, UDA
methods for segmentation have not taken advantage of the domain-agnostic
properties of text. To address this, we present a novel Covariance-based
Pixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn
domain-invariant features in an image segmentation encoder. The text embeddings
are generated through our LLM Domain Template process, where an LLM is used to
generate source and target domain descriptions that are fed to a frozen CLIP
model and combined. In experiments on four benchmarks we show that a model
trained using CoPT achieves the new state of the art performance on UDA for
segmentation. The code can be found at https://github.com/cfmata/CoPT.

</details>


### [57] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

TL;DR: The paper introduces "Recall," an adversarial framework aimed at testing the robustness of machine unlearning in image generation models by exploiting multi-modal adversarial inputs. The results show that current unlearning methods are vulnerable and need improvement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address vulnerabilities and ethical concerns in image generation models, particularly focusing on how "unlearning" efforts can be compromised by multi-modal adversarial attacks.

Method: The authors developed "Recall," a framework that generates adversarial image prompts using the multi-modal conditioning abilities of diffusion models. It uses optimization guided by a single reference image to test the robustness of unlearning methods in IGMs.

Result: Experiments conducted on ten state-of-the-art unlearning methods and multiple tasks showcased that Recall is more effective than existing baselines in terms of adversarial performance, efficiency, and maintaining semantic relevance.

Conclusion: Current machine unlearning methods in IGMs have critical vulnerabilities when exposed to multi-modal adversarial inputs. The study stresses the urgent need to develop more robust strategies to enhance the safety and reliability of these models.

Abstract: Recent advances in image generation models (IGMs), particularly
diffusion-based architectures such as Stable Diffusion (SD), have markedly
enhanced the quality and diversity of AI-generated visual content. However,
their generative capability has also raised significant ethical, legal, and
societal concerns, including the potential to produce harmful, misleading, or
copyright-infringing content. To mitigate these concerns, machine unlearning
(MU) emerges as a promising solution by selectively removing undesirable
concepts from pretrained models. Nevertheless, the robustness and effectiveness
of existing unlearning techniques remain largely unexplored, particularly in
the presence of multi-modal adversarial inputs.
  To bridge this gap, we propose Recall, a novel adversarial framework
explicitly designed to compromise the robustness of unlearned IGMs. Unlike
existing approaches that predominantly rely on adversarial text prompts, Recall
exploits the intrinsic multi-modal conditioning capabilities of diffusion
models by efficiently optimizing adversarial image prompts with guidance from a
single semantically relevant reference image. Extensive experiments across ten
state-of-the-art unlearning methods and diverse tasks show that Recall
consistently outperforms existing baselines in terms of adversarial
effectiveness, computational efficiency, and semantic fidelity with the
original textual prompt. These findings reveal critical vulnerabilities in
current unlearning mechanisms and underscore the need for more robust solutions
to ensure the safety and reliability of generative models. Code and data are
publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [58] [Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey](https://arxiv.org/abs/2507.07148)
*Getamesay Haile Dagnaw,Yanming Zhu,Muhammad Hassan Maqsood,Wencheng Yang,Xingshuai Dong,Xuefei Yin,Alan Wee-Chung Liew*

Main category: cs.CV

TL;DR: The paper surveys explainable AI (XAI) methods for biomedical image analysis, proposing a modality-focused taxonomy and exploring underexamined areas like multimodal learning and vision-language models.


<details>
  <summary>Details</summary>
Motivation: The study aims to address gaps in XAI reviews, particularly in considering modality-specific challenges, multimodal paradigms, and practical guidance for biomedical image analysis.

Method: The authors systematically categorize XAI approaches, propose a taxonomy for imaging modalities, review emerging multimodal and vision-language methods, and provide metrics and frameworks for evaluation.

Result: The paper synthesizes XAI methods tailored to biomedical image analysis, highlights trends and challenges across modalities, and summarizes existing evaluation tools while identifying future directions.

Conclusion: This survey sets a detailed foundation for improving interpretability in deep learning models for biomedical imaging and highlights the need for continued research in multimodal and vision-language-based XAI.

Abstract: Explainable artificial intelligence (XAI) has become increasingly important
in biomedical image analysis to promote transparency, trust, and clinical
adoption of DL models. While several surveys have reviewed XAI techniques, they
often lack a modality-aware perspective, overlook recent advances in multimodal
and vision-language paradigms, and provide limited practical guidance. This
survey addresses this gap through a comprehensive and structured synthesis of
XAI methods tailored to biomedical image analysis.We systematically categorize
XAI methods, analyzing their underlying principles, strengths, and limitations
within biomedical contexts. A modality-centered taxonomy is proposed to align
XAI methods with specific imaging types, highlighting the distinct
interpretability challenges across modalities. We further examine the emerging
role of multimodal learning and vision-language models in explainable
biomedical AI, a topic largely underexplored in previous work. Our
contributions also include a summary of widely used evaluation metrics and
open-source frameworks, along with a critical discussion of persistent
challenges and future directions. This survey offers a timely and in-depth
foundation for advancing interpretable DL in biomedical image analysis.

</details>


### [59] [EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks](https://arxiv.org/abs/2507.07734)
*Michael Neumeier,Jules Lecomte,Nils Kazinski,Soubarna Banik,Bing Li,Axel von Arnim*

Main category: cs.CV

TL;DR: The paper proposes a high-rate two-stream Spiking Neural Network (SNN) for early human activity recognition, achieving higher accuracy and demonstrating its application in sports motion capture.


<details>
  <summary>Details</summary>
Motivation: Early human activity recognition is vital for safe and responsive interactions in human-robot and human-machine interfaces. Event-based vision sensors offer high temporal resolution and low latency, but existing approaches limit their early prediction potential.

Method: The authors developed a high-rate two-stream Spiking Neural Network (SNN) and integrated it into an early event-based recognition framework to improve early prediction and evaluation accuracy.

Result: The proposed SNN improves final recognition accuracy by 2% on the large-scale THU EACT-50 dataset and demonstrates better tracking for human motion capture in sports applications.

Conclusion: This study highlights the advantages of high-rate SNNs for event-based human activity recognition, showing improvements in prediction accuracy as well as practical implications for real-world tasks like sports.

Abstract: Recognizing human activities early is crucial for the safety and
responsiveness of human-robot and human-machine interfaces. Due to their high
temporal resolution and low latency, event-based vision sensors are a perfect
match for this early recognition demand. However, most existing processing
approaches accumulate events to low-rate frames or space-time voxels which
limits the early prediction capabilities. In contrast, spiking neural networks
(SNNs) can process the events at a high-rate for early predictions, but most
works still fall short on final accuracy. In this work, we introduce a
high-rate two-stream SNN which closes this gap by outperforming previous work
by 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark
the SNNs within a novel early event-based recognition framework by reporting
Top-1 and Top-5 recognition scores for growing observation time. Finally, we
exemplify the impact of these methods on a real-world task of early action
triggering for human motion capture in sports.

</details>


### [60] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

TL;DR: This paper explores the hallucination issue in multimodal large language models (MLLMs), especially arising from conflicts between different input modalities.


<details>
  <summary>Details</summary>
Motivation: The goal is to better understand and address hallucinations in MLLMs caused by inherent modality conflicts, which occur in real-world multi-modal inputs.

Method: Researchers formally define modality conflict, introduce a new dataset (MMMC), and test three approaches (prompt engineering, supervised fine-tuning, reinforcement learning) to mitigate hallucinations.

Result: The reinforcement learning method performs best, while supervised fine-tuning offers stable and promising results in reducing hallucinations.

Conclusion: The study introduces a previously unnoticed issue of modality conflict causing hallucinations in MLLMs and provides solutions and insights for enhancing their robustness.

Abstract: Despite the impressive capabilities of multimodal large language models
(MLLMs) in vision-language tasks, they are prone to hallucinations in
real-world scenarios. This paper investigates the hallucination phenomenon in
MLLMs from the perspective of modality conflict. Unlike existing works focusing
on the conflicts between model responses and inputs, we study the inherent
conflicts in inputs from different modalities that place MLLMs in a dilemma and
directly lead to hallucinations. We formally define the modality conflict and
construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this
phenomenon in vision-language tasks. Three methods based on prompt engineering,
supervised fine-tuning, and reinforcement learning are proposed to alleviate
the hallucination caused by modality conflict. Extensive experiments are
conducted on the MMMC dataset to analyze the merits and demerits of these
methods. Our results show that the reinforcement learning method achieves the
best performance in mitigating the hallucination under modality conflict, while
the supervised fine-tuning method shows promising and stable performance. Our
work sheds light on the unnoticed modality conflict that leads to
hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [61] [Multigranular Evaluation for Brain Visual Decoding](https://arxiv.org/abs/2507.07993)
*Weihao Xia,Cengiz Oztireli*

Main category: cs.CV

TL;DR: The paper introduces BASIC, a comprehensive framework for evaluating brain visual decoding methods with detailed structural, semantic, and contextual metrics.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation protocols for brain visual decoding lack precision, neuroscientific foundations, and the ability to capture fine-grained visual details.

Method: The paper proposes BASIC, a unified evaluation system with multigranular metrics, including segmentation-based structural fidelity, semantic structure analysis using multimodal large language models, and contextual coherence.

Result: BASIC provides a unified and comprehensive benchmarking tool applied to diverse visual decoding methods and datasets, revealing nuanced inter-model differences.

Conclusion: BASIC establishes a discriminative and interpretable foundation for evaluating and advancing brain visual decoding techniques.

Abstract: Existing evaluation protocols for brain visual decoding predominantly rely on
coarse metrics that obscure inter-model differences, lack neuroscientific
foundation, and fail to capture fine-grained visual distinctions. To address
these limitations, we introduce BASIC, a unified, multigranular evaluation
framework that jointly quantifies structural fidelity, inferential alignment,
and contextual coherence between decoded and ground truth images. For the
structural level, we introduce a hierarchical suite of segmentation-based
metrics, including foreground, semantic, instance, and component masks,
anchored in granularity-aware correspondence across mask structures. For the
semantic level, we extract structured scene representations encompassing
objects, attributes, and relationships using multimodal large language models,
enabling detailed, scalable, and context-rich comparisons with ground-truth
stimuli. We benchmark a diverse set of visual decoding methods across multiple
stimulus-neuroimaging datasets within this unified evaluation framework.
Together, these criteria provide a more discriminative, interpretable, and
comprehensive foundation for measuring brain visual decoding methods.

</details>


### [62] [Aerial Maritime Vessel Detection and Identification](https://arxiv.org/abs/2507.07153)
*Antonella Barisic Kulas,Frano Petric,Stjepan Bogdan*

Main category: cs.CV

TL;DR: The study develops a UAV-based maritime surveillance system for identifying target vessels in GNSS-denied environments, utilizing YOLOv8 object detection, feature matching, and hue histogram analysis.


<details>
  <summary>Details</summary>
Motivation: To address the need for autonomous maritime surveillance in GNSS-denied environments for applications like search and rescue or threat detection.

Method: The authors use YOLOv8 object detection to detect vessels, followed by feature matching and hue histogram distance analysis for target identification. Localization is achieved via simple geometric principles.

Result: The system was demonstrated during MBZIRC2023 in real-world experiments, successfully integrating GNSS-denied navigation and showcasing its efficiency in target detection and localization.

Conclusion: The approach offers an effective UAV-based solution for GNSS-denied target vessel identification, with high potential for practical applications in maritime operations.

Abstract: Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.

</details>


### [63] [CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation](https://arxiv.org/abs/2507.07154)
*Desheng Li,Chaoliang Liu,Zhiyong Xiao*

Main category: cs.CV

TL;DR: CL-Polyp proposes a contrastive learning-enhanced framework for polyp segmentation, achieving superior segmentation accuracy using self-supervised techniques and lightweight modules, without needing extra labeled data.


<details>
  <summary>Details</summary>
Motivation: Provide accurate polyp segmentation for early colorectal cancer diagnosis while overcoming limitations of existing deep learning methods that rely heavily on additional labeled data and task similarity.

Method: The study introduces CL-Polyp, leveraging contrastive learning for feature enhancement and two modules: MASPP for multi-scale feature fusion and CA module for attaining better boundary reconstruction.

Result: Experimental evaluations on five benchmark datasets demonstrate superior performance of CL-Polyp over existing methods, showing consistent IoU improvements, particularly 0.011 and 0.020 on Kvasir-SEG and CVC-ClinicDB, respectively.

Conclusion: CL-Polyp enhances clinical polyp segmentation through innovative self-supervised learning and efficient module designs, offering a generalizable solution without dependency on extra labeled data.

Abstract: Accurate segmentation of polyps from colonoscopy images is crucial for the
early diagnosis and treatment of colorectal cancer. Most existing deep
learning-based polyp segmentation methods adopt an Encoder-Decoder
architecture, and some utilize multi-task frameworks that incorporate auxiliary
tasks such as classification to enhance segmentation performance. However,
these approaches often require additional labeled data and rely on task
similarity, which can limit their generalizability. To address these
challenges, we propose CL-Polyp, a contrastive learning-enhanced polyp
segmentation network. Our method leverages contrastive learning to improve the
encoder's ability to extract discriminative features by contrasting positive
and negative sample pairs derived from polyp images. This self-supervised
strategy enhances visual representation without requiring additional
annotations. In addition, we introduce two lightweight and effective modules:
the Modified Atrous Spatial Pyramid Pooling (MASPP) module for better
multi-scale feature fusion, and the Channel Concatenate and Element Add (CA)
module to fuse low-level and upsampled features for improved boundary
reconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,
CVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp
consistently outperforms state-of-the-art methods. Specifically, it improves
the IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,
respectively, validating its effectiveness in clinical polyp segmentation
tasks.

</details>


### [64] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

TL;DR: The paper presents a framework for decoding visual experiences from EEG by using semantic captions generated by a language model, which leads to state-of-the-art performance in visual decoding.


<details>
  <summary>Details</summary>
Motivation: To utilize EEG, which is accessible and temporally precise, in decoding visual experiences despite its limitations in spatial information, and to improve interpretability in neuroscience and AI.

Method: A transformer-based EEG encoder maps brain signals to multilevel semantic captions (generated by a language model) using contrastive learning, and these captions condition a latent diffusion model for image generation.

Result: The framework achieved state-of-the-art results on the EEGCVPR dataset, demonstrated interpretable alignments with brain activity, and highlighted dominant EEG-caption associations.

Conclusion: Using text-mediated frameworks enables cognitively aligned and interpretable visual decoding from EEG signals.

Abstract: Decoding visual experience from brain signals offers exciting possibilities
for neuroscience and interpretable AI. While EEG is accessible and temporally
precise, its limitations in spatial detail hinder image reconstruction. Our
model bypasses direct EEG-to-image generation by aligning EEG signals with
multilevel semantic captions -- ranging from object-level to abstract themes --
generated by a large language model. A transformer-based EEG encoder maps brain
activity to these captions through contrastive learning. During inference,
caption embeddings retrieved via projection heads condition a pretrained latent
diffusion model for image generation. This text-mediated framework yields
state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable
alignment to known neurocognitive pathways. Dominant EEG-caption associations
reflected the importance of different semantic levels extracted from perceived
images. Saliency maps and t-SNE projections reveal semantic topography across
the scalp. Our model demonstrates how structured semantic mediation enables
cognitively aligned visual decoding from EEG.

</details>


### [65] [A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality](https://arxiv.org/abs/2507.07202)
*Mohamed Elmoghany,Ryan Rossi,Seunghyun Yoon,Subhojyoti Mukherjee,Eslam Bakr,Puneet Mathur,Gang Wu,Viet Dac Lai,Nedim Lipka,Ruiyi Zhang,Varun Manjunatha,Chien Nguyen,Daksh Dangi,Abel Salinas,Mohammad Taesiri,Hongjie Chen,Xiaolei Huang,Joe Barrow,Nesreen Ahmed,Hoda Eldardiry,Namyong Park,Yu Wang,Jaemin Cho,Anh Totti Nguyen,Zhengzhong Tu,Thien Nguyen,Dinesh Manocha,Mohamed Elhoseiny,Franck Dernoncourt*

Main category: cs.CV

TL;DR: The paper addresses challenges in generating long-form videos, including consistency, coherence, and detail, by analyzing 32 existing papers and proposing a taxonomy.


<details>
  <summary>Details</summary>
Motivation: Current video generative models struggle to maintain character consistency, motion coherence, and high temporal diversity in long-form videos.

Method: The authors conducted a study of 32 papers, identifying effective architectural components, training strategies, and building a taxonomy categorizing these models.

Result: The study yielded a taxonomy and comparative tables to classify and evaluate video generative methods by design and performance.

Conclusion: Comprehensive analysis and taxonomy provide deeper insights into advancing long-form video generation addressing current limitations.

Abstract: Despite the significant progress that has been made in video generative
models, existing state-of-the-art methods can only produce videos lasting 5-16
seconds, often labeled "long-form videos". Furthermore, videos exceeding 16
seconds struggle to maintain consistent character appearances and scene layouts
throughout the narrative. In particular, multi-subject long videos still fail
to preserve character consistency and motion coherence. While some methods can
generate videos up to 150 seconds long, they often suffer from frame redundancy
and low temporal diversity. Recent work has attempted to produce long-form
videos featuring multiple characters, narrative coherence, and high-fidelity
detail. We comprehensively studied 32 papers on video generation to identify
key architectural components and training strategies that consistently yield
these qualities. We also construct a comprehensive novel taxonomy of existing
methods and present comparative tables that categorize papers by their
architectural designs and performance characteristics.

</details>


### [66] [Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement](https://arxiv.org/abs/2507.07230)
*Priyank Pathak,Yogesh S. Rawat*

Main category: cs.CV

TL;DR: The paper introduces CSCI, an RGB-only method leveraging color information for Clothes-Changing Re-Identification (CC-ReID), eliminating the need for additional annotations or models.


<details>
  <summary>Details</summary>
Motivation: Current CC-ReID methods are resource-intensive, requiring additional models or annotations to learn clothing-invariant features. The authors aim to develop a lightweight and annotation-free alternative using color information.

Method: The proposed method, Colors See, Colors Ignore (CSCI), uses raw RGB images to separate color-related biases from identity cues. It introduces S2A self-attention to avoid information leakage between color and identity cues.

Result: CSCI achieved improvements of Top-1 2.9% (LTCC), 5.0% (PRCC) for image-based ReID, and 1.0% (CCVID), 2.5% (MeVID) for video-based ReID without additional supervision.

Conclusion: The study demonstrates the effectiveness of using color as a proxy for mitigating appearance bias in CC-ReID, offering a resource-efficient alternative to existing methods.

Abstract: Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals
across different locations and times, irrespective of clothing. Existing
methods often rely on additional models or annotations to learn robust,
clothing-invariant features, making them resource-intensive. In contrast, we
explore the use of color - specifically foreground and background colors - as a
lightweight, annotation-free proxy for mitigating appearance bias in ReID
models. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that
leverages color information directly from raw images or video frames. CSCI
efficiently captures color-related appearance bias ('Color See') while
disentangling it from identity-relevant ReID features ('Color Ignore'). To
achieve this, we introduce S2A self-attention, a novel self-attention to
prevent information leak between color and identity cues within the feature
space. Our analysis shows a strong correspondence between learned color
embeddings and clothing attributes, validating color as an effective proxy when
explicit clothing labels are unavailable. We demonstrate the effectiveness of
CSCI on both image and video ReID with extensive experiments on four CC-ReID
datasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for
image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID
without relying on additional supervision. Our results highlight the potential
of color as a cost-effective solution for addressing appearance bias in
CC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.

</details>


### [67] [Automated Video Segmentation Machine Learning Pipeline](https://arxiv.org/abs/2507.07242)
*Johannes Merz,Lucien Fostier*

Main category: cs.CV

TL;DR: The paper introduces a machine learning-based automated pipeline for creating consistent masks in video segmentation to enhance VFX production efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and resource-intensive process of mask generation in VFX production.

Method: Machine learning techniques are employed for text-prompt-based object detection, refined image segmentation per frame, and robust video tracking for temporal stability. The system is deployed via containerization with structured outputs.

Result: Artists quickly adopted the pipeline, which reduced manual effort, accelerated composite creation, and provided detailed segmentation data.

Conclusion: The automated pipeline improves manual labor efficiency, speeds up production processes, and introduces better segmentation for more efficient VFX workflows.

Abstract: Visual effects (VFX) production often struggles with slow, resource-intensive
mask generation. This paper presents an automated video segmentation pipeline
that creates temporally consistent instance masks. It employs machine learning
for: (1) flexible object detection via text prompts, (2) refined per-frame
image segmentation and (3) robust video tracking to ensure temporal stability.
Deployed using containerization and leveraging a structured output format, the
pipeline was quickly adopted by our artists. It significantly reduces manual
effort, speeds up the creation of preliminary composites, and provides
comprehensive segmentation data, thereby enhancing overall VFX production
efficiency.

</details>


### [68] [DisenQ: Disentangling Q-Former for Activity-Biometrics](https://arxiv.org/abs/2507.07262)
*Shehreen Azad,Yogesh S Rawat*

Main category: cs.CV

TL;DR: The paper proposes a framework for identifying individuals across different activities using text-based guidance, achieving state-of-the-art results on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current biometric methods struggle with challenges like motion dynamics and appearance variations, making feature learning complex. Additional visual data like pose or silhouette is often unreliable.

Method: The authors propose a multimodal language-guided framework named DisenQ (Disentangling Q-Former), which uses structured textual supervision to disentangle biometric, motion, and non-biometric features.

Result: The framework achieves state-of-the-art performance on three activity-based video benchmarks and performs competitively in traditional video-based identification tasks.

Conclusion: The proposed method effectively separates identity cues from motion and appearance variations, demonstrating strong generalization and improved accuracy in both controlled and real-world scenarios.

Abstract: In this work, we address activity-biometrics, which involves identifying
individuals across diverse set of activities. Unlike traditional person
identification, this setting introduces additional challenges as identity cues
become entangled with motion dynamics and appearance variations, making
biometrics feature learning more complex. While additional visual data like
pose and/or silhouette help, they often struggle from extraction inaccuracies.
To overcome this, we propose a multimodal language-guided framework that
replaces reliance on additional visual data with structured textual
supervision. At its core, we introduce \textbf{DisenQ} (\textbf{Disen}tangling
\textbf{Q}-Former), a unified querying transformer that disentangles
biometrics, motion, and non-biometrics features by leveraging structured
language guidance. This ensures identity cues remain independent of appearance
and motion variations, preventing misidentifications. We evaluate our approach
on three activity-based video benchmarks, achieving state-of-the-art
performance. Additionally, we demonstrate strong generalization to complex
real-world scenario with competitive performance on a traditional video-based
identification benchmark, showing the effectiveness of our framework.

</details>


### [69] [LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation](https://arxiv.org/abs/2507.07274)
*Ananya Raval,Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: The paper introduces LinguaMark, a benchmark for evaluating multilingual Visual Question Answering capabilities of Large Multimodal Models, focusing on bias, relevancy, and faithfulness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on assessing multilingual capabilities in Large Multimodal Models, which often yield biased outputs across languages.

Method: The authors created LinguaMark, a benchmark consisting of 6,875 image-text pairs covering 11 languages and 5 social attributes, and evaluated models on Bias, Answer Relevancy, and Faithfulness.

Result: Findings show closed-source models generally outperform open-source ones, with Qwen2.5 demonstrating noteworthy multilingual generalization.

Conclusion: The release of the LinguaMark benchmark and evaluation code aims to advance research on multilingual evaluations in multimodal AI models.

Abstract: Large Multimodal Models (LMMs) are typically trained on vast corpora of
image-text data but are often limited in linguistic coverage, leading to biased
and unfair outputs across languages. While prior work has explored multimodal
evaluation, less emphasis has been placed on assessing multilingual
capabilities. In this work, we introduce LinguaMark, a benchmark designed to
evaluate state-of-the-art LMMs on a multilingual Visual Question Answering
(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages
and five social attributes. We evaluate models using three key metrics: Bias,
Answer Relevancy, and Faithfulness. Our findings reveal that closed-source
models generally achieve the highest overall performance. Both closed-source
(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform
competitively across social attributes, and Qwen2.5 demonstrates strong
generalization across multiple languages. We release our benchmark and
evaluation code to encourage reproducibility and further research.

</details>


### [70] [MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning](https://arxiv.org/abs/2507.07297)
*Chengfei Wu,Ronald Seoh,Bingxuan Li,Liqiang Zhang,Fengrong Han,Dan Goldwasser*

Main category: cs.CV

TL;DR: The paper presents MagiC, a benchmark to test vision-language models' grounded reasoning, focusing on answer accuracy, reasoning quality, and alignment with visual evidence using unique metrics and evaluations.


<details>
  <summary>Details</summary>
Motivation: Addressing the uncertainty about whether vision-language models perform genuine reasoning or rely on biases, by proposing a benchmark specifically evaluating grounded multimodal cognition.

Method: The benchmark includes 5,500 weakly supervised and 900 human-curated QA examples with annotations (e.g., answers, rationales, bounding box groundings), evaluates 15 models across dimensions, and introduces new metrics like MagiScore and StepSense.

Result: MagiC evaluates models for answer correctness, reasoning validity, grounding fidelity, and self-correction, highlighting their robustness against adversarial cues and introspective abilities.

Conclusion: The approach identifies key limitations and opportunities for improving grounded reasoning in vision-language models, contributing new benchmarks and diagnostics to the field.

Abstract: Recent advances in large vision-language models have led to impressive
performance in visual question answering and multimodal reasoning. However, it
remains unclear whether these models genuinely perform grounded visual
reasoning or rely on superficial patterns and dataset biases. In this work, we
introduce MagiC, a comprehensive benchmark designed to evaluate grounded
multimodal cognition, assessing not only answer accuracy but also the quality
of step-by-step reasoning and its alignment with relevant visual evidence. Our
benchmark includes approximately 5,500 weakly supervised QA examples generated
from strong model outputs and 900 human-curated examples with fine-grained
annotations, including answers, rationales, and bounding box groundings. We
evaluate 15 vision-language models ranging from 7B to 70B parameters across
four dimensions: final answer correctness, reasoning validity, grounding
fidelity, and self-correction ability. MagiC further includes diagnostic
settings to probe model robustness under adversarial visual cues and assess
their capacity for introspective error correction. We introduce new metrics
such as MagiScore and StepSense, and provide comprehensive analyses that reveal
key limitations and opportunities in current approaches to grounded visual
reasoning.

</details>


### [71] [ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation](https://arxiv.org/abs/2507.07317)
*Sherry X. Chen,Yi Wei,Luowei Zhou,Suren Kumar*

Main category: cs.CV

TL;DR: The paper introduces ADIEE for creating a large dataset for training a Vision-Language Model (VLM) scorer for instruction-guided image editing, achieving superior performance compared to existing open-source and proprietary models.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models (VLMs) for evaluating instruction-guided image editing face alignment challenges, lack of transparency, high costs, and absence of substantial public training datasets.

Method: ADIEE (Automated Dataset for Image Editing Evaluation) is proposed to systematically create a large training dataset of over 100K samples. A modified LLaVA-NeXT-8B model with numeric scoring capabilities is fine-tuned using this dataset.

Result: The trained scorer surpasses all open-source and proprietary VLMs, showing a 17.24% improvement in correlation with human ratings and better pair-wise comparison accuracy on benchmarks like AURORA-Bench and GenAI-Bench.

Conclusion: The ADIEE-based scorer emerges as a leading automated evaluation tool for instruction-guided image editing, enabling more efficient model fine-tuning and application in enhancing other image editing models.

Abstract: Recent advances in instruction-guided image editing underscore the need for
effective automated evaluation. While Vision-Language Models (VLMs) have been
explored as judges, open-source models struggle with alignment, and proprietary
models lack transparency and cost efficiency. Additionally, no public training
datasets exist to fine-tune open-source VLMs, only small benchmarks with
diverse evaluation schemes. To address this, we introduce ADIEE, an automated
dataset creation approach which is then used to train a scoring model for
instruction-guided image editing evaluation. We generate a large-scale dataset
with over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified
to decode a numeric score from a custom token. The resulting scorer outperforms
all open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a
0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,
and improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench
and 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the
state-of-the-art. The scorer can act as a reward model, enabling automated best
edit selection and model fine-tuning. Notably, the proposed scorer can boost
MagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43
(+8.98%).

</details>


### [72] [Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory](https://arxiv.org/abs/2507.07333)
*Hui Pang,Sunil Hadap,Violetta Shevchenko,Rahul Suresh,Amin Banitalebi-Dehkordi*

Main category: cs.CV

TL;DR: The paper presents a novel method to improve virtual try-on (VTO) applications for the beauty industry by facilitating realistic blending of foundation and skin tones through a fast and scalable system.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of realistic foundation-skin tone color blending in VTO applications while ensuring scalability across diverse product ranges.

Method: A method approximating the Kubelka-Munk (KM) theory for efficient image synthesis and a framework that relies on e-commerce product information for scalability.

Result: The proposed framework achieves realistic blending of foundation and skin tones, validated by real-world makeup images, and outperforms existing techniques.

Conclusion: The proposed method enhances VTO applications with realistic and scalable foundation blending capabilities, advancing the beauty industry's digital innovation.

Abstract: Augmented reality is revolutionizing beauty industry with virtual try-on
(VTO) applications, which empowers users to try a wide variety of products
using their phones without the hassle of physically putting on real products. A
critical technical challenge in foundation VTO applications is the accurate
synthesis of foundation-skin tone color blending while maintaining the
scalability of the method across diverse product ranges. In this work, we
propose a novel method to approximate well-established Kubelka-Munk (KM) theory
for faster image synthesis while preserving foundation-skin tone color blending
realism. Additionally, we build a scalable end-to-end framework for realistic
foundation makeup VTO solely depending on the product information available on
e-commerce sites. We validate our method using real-world makeup images,
demonstrating that our framework outperforms other techniques.

</details>


### [73] [Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.07340)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: This paper addresses the issue of large vision-language models' inability to maintain consistent character and object identity in visual storytelling across frames, proposing a contrastive reinforcement learning method that significantly improves performance.


<details>
  <summary>Details</summary>
Motivation: To resolve the issue of inconsistent references and referential hallucinations in visual storytelling systems, caused by a lack of explicit training on connecting entities across frames.

Method: A contrastive reinforcement learning approach using synthetic negative examples to extend the Story Reasoning dataset, coupled with a dual-component reward function focusing on grounding and entity re-identification, was employed to fine-tune the Qwen Storyteller model.

Result: The model achieved improvements in grounding mAP (+14.8%), F1 (+17.1%), pronoun grounding accuracy (except for 'its'), and increased cross-frame character/object persistence (+13.7%). The percentage of well-structured stories increased significantly (+23.3%).

Conclusion: The proposed method effectively enhances the ability of visual storytelling models to maintain cross-frame consistency of entities, improving both referential accuracy and the overall coherence of generated stories.

Abstract: Visual storytelling systems, particularly large vision-language models,
struggle to maintain character and object identity across frames,
  often failing to recognize when entities in different images represent the
same individuals or objects,
  leading to inconsistent references and referential hallucinations.
  This occurs because models lack explicit training on when to establish entity
connections across frames.
  We propose a contrastive reinforcement learning approach that trains models
to discriminate between coherent image sequences
  and stories from unrelated images.
  We extend the Story Reasoning dataset with synthetic negative examples to
teach appropriate entity connection behavior.
  We employ Direct Preference Optimization with a dual-component reward
function that promotes grounding and re-identification of entities
  in real stories while penalizing incorrect entity connections in synthetic
contexts.
  Using this contrastive framework, we fine-tune Qwen Storyteller (based on
Qwen2.5-VL 7B).
  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1
from 0.35 to 0.41 (+17.1%).
  Pronoun grounding accuracy improved across all pronoun types except ``its'',
  and cross-frame character and object persistence increased
  across all frame counts, with entities appearing in 5 or more frames
advancing from 29.3% to 33.3% (+13.7%).
  Well-structured stories, containing the chain-of-thought and grounded story,
increased from 79.1% to 97.5% (+23.3%).

</details>


### [74] [PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency](https://arxiv.org/abs/2507.07374)
*Haotian Wang,Aoran Xiao,Xiaoqin Zhang,Meng Yang,Shijian Lu*

Main category: cs.CV

TL;DR: PacGDC is a label-efficient method for generalizable depth completion that uses depth foundation models and data synthesis to create pseudo labels with varied scene scales, achieving strong performance with minimal annotations.


<details>
  <summary>Details</summary>
Motivation: To reduce the labor-intensive process of collecting large-scale metric depth labels, enabling robust perception for unseen environments using a generalizable depth completion approach.

Method: PacGDC uses a data synthesis pipeline with multiple depth foundation models to manipulate scene scales, create pseudo depth labels, and ensure projection consistency. It further employs interpolation, relocation strategies, and unlabeled images to diversify data coverage.

Result: Experiments demonstrate that PacGDC achieves notable generalizability across diverse benchmarks and scenarios, excelling under zero-shot and few-shot settings in various scene semantics, scales, and depth sparsity patterns.

Conclusion: PacGDC provides an efficient solution to expand data diversity for depth completion tasks with reduced annotation effort, leveraging foundation models to achieve robust generalization in unseen environments.

Abstract: Generalizable depth completion enables the acquisition of dense metric depth
maps for unseen environments, offering robust perception capabilities for
various downstream tasks. However, training such models typically requires
large-scale datasets with metric depth labels, which are often labor-intensive
to collect. This paper presents PacGDC, a label-efficient technique that
enhances data diversity with minimal annotation effort for generalizable depth
completion. PacGDC builds on novel insights into inherent ambiguities and
consistencies in object shapes and positions during 2D-to-3D projection,
allowing the synthesis of numerous pseudo geometries for the same visual scene.
This process greatly broadens available geometries by manipulating scene scales
of the corresponding depth maps. To leverage this property, we propose a new
data synthesis pipeline that uses multiple depth foundation models as scale
manipulators. These models robustly provide pseudo depth labels with varied
scene scales, affecting both local objects and global layouts, while ensuring
projection consistency that supports generalization. To further diversify
geometries, we incorporate interpolation and relocation strategies, as well as
unlabeled images, extending the data coverage beyond the individual use of
foundation models. Extensive experiments show that PacGDC achieves remarkable
generalizability across multiple benchmarks, excelling in diverse scene
semantics/scales and depth sparsity/patterns under both zero-shot and few-shot
settings. Code: https://github.com/Wang-xjtu/PacGDC.

</details>


### [75] [Adaptive Particle-Based Shape Modeling for Anatomical Surface Correspondence](https://arxiv.org/abs/2507.07379)
*Hong Xu,Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: This paper enhances particle-based shape modeling by introducing adaptive mechanisms using a novel neighborhood correspondence loss and a geodesic correspondence algorithm, improving representation of anatomical variability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of self-adaptivity in particle-based shape modeling methods, which hinders accurate representation of complex anatomical structures.

Method: Proposed two mechanisms: a novel neighborhood correspondence loss for adaptivity and a geodesic correspondence algorithm to regularize and ensure consistency of particle configurations.

Result: Evaluated on challenging datasets, the proposed approach improves surface representation accuracy and correspondence metrics, while offering a detailed adaptivity-correspondence trade-off analysis.

Conclusion: The new methods enable more accurate and adaptive modeling of anatomical shape variability, outperforming existing methods in terms of accuracy and consistency.

Abstract: Particle-based shape modeling (PSM) is a family of approaches that
automatically quantifies shape variability across anatomical cohorts by
positioning particles (pseudo landmarks) on shape surfaces in a consistent
configuration. Recent advances incorporate implicit radial basis function
representations as self-supervised signals to better capture the complex
geometric properties of anatomical structures. However, these methods still
lack self-adaptivity -- that is, the ability to automatically adjust particle
configurations to local geometric features of each surface, which is essential
for accurately representing complex anatomical variability. This paper
introduces two mechanisms to increase surface adaptivity while maintaining
consistent particle configurations: (1) a novel neighborhood correspondence
loss to enable high adaptivity and (2) a geodesic correspondence algorithm that
regularizes optimization to enforce geodesic neighborhood consistency. We
evaluate the efficacy and scalability of our approach on challenging datasets,
providing a detailed analysis of the adaptivity-correspondence trade-off and
benchmarking against existing methods on surface representation accuracy and
correspondence metrics.

</details>


### [76] [Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos](https://arxiv.org/abs/2507.07381)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.CV

TL;DR: This paper introduces a new lightweight module, MSAGSM, for event spotting in sports videos and presents a new dataset for table tennis, the TTA dataset, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing PES models, which have restricted temporal receptive fields and spatial adaptability, and to improve the performance of recognizing fine-grained actions in sports videos.

Method: The authors propose MSAGSM, which combines multi-scale temporal dilations with multi-head spatial attention, providing better modeling of short- and long-term dependencies. They also introduce the TTA dataset for table tennis event recognition.

Result: MSAGSM achieves consistent performance improvements across five PES benchmarks, setting new state-of-the-art results while maintaining minimal computational overhead.

Conclusion: The proposed MSAGSM module and the introduction of the TTA dataset mark a significant advancement in PES, improving both methodology and benchmark resources for the community.

Abstract: Precise Event Spotting (PES) in sports videos requires frame-level
recognition of fine-grained actions from single-camera footage. Existing PES
models typically incorporate lightweight temporal modules such as Gate Shift
Module (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with
temporal context. However, these modules are limited in both temporal receptive
field and spatial adaptability. We propose a Multi-Scale Attention Gate Shift
Module (MSAGSM) that enhances GSM with multi-scale temporal dilations and
multi-head spatial attention, enabling efficient modeling of both short- and
long-term dependencies while focusing on salient regions. MSAGSM is a
lightweight plug-and-play module that can be easily integrated with various 2D
backbones. To further advance the field, we introduce the Table Tennis
Australia (TTA) dataset-the first PES benchmark for table tennis-containing
over 4800 precisely annotated events. Extensive experiments across five PES
benchmarks demonstrate that MSAGSM consistently improves performance with
minimal overhead, setting new state-of-the-art results.

</details>


### [77] [KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos](https://arxiv.org/abs/2507.07393)
*Jinseong Kim,Junghoon Song,Gyeongseon Baek,Byeongjoon Noh*

Main category: cs.CV

TL;DR: KeyRe-ID improves video-based person re-identification using keypoints and achieves state-of-the-art results on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of video-based person re-identification by leveraging human keypoints for better representation learning.

Method: KeyRe-ID utilizes a global branch with Transformer-based temporal aggregation for holistic representation and a local branch for part-aware features based on dynamically segmented body regions from keypoints.

Result: The framework achieves 91.73% mAP and 97.32% Rank-1 accuracy on MARS, 96.00% Rank-1 and 100.0% Rank-5 accuracy on iLIDS-VID benchmarks.

Conclusion: KeyRe-ID advances person re-identification by combining global and local spatiotemporal features, and its performance sets a new benchmark in the domain.

Abstract: We propose \textbf{KeyRe-ID}, a keypoint-guided video-based person
re-identification framework consisting of global and local branches that
leverage human keypoints for enhanced spatiotemporal representation learning.
The global branch captures holistic identity semantics through
Transformer-based temporal aggregation, while the local branch dynamically
segments body regions based on keypoints to generate fine-grained, part-aware
features. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate
state-of-the-art performance, achieving 91.73\% mAP and 97.32\% Rank-1 accuracy
on MARS, and 96.00\% Rank-1 and 100.0\% Rank-5 accuracy on iLIDS-VID. The code
for this work will be publicly available on GitHub upon publication.

</details>


### [78] [Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer](https://arxiv.org/abs/2507.07394)
*Zhimin Zhang,Bi'an Du,Caoyuan Ma,Zheng Wang,Wei Hu*

Main category: cs.CV

TL;DR: The paper proposes a motion transfer framework preserving animal-specific habitual behaviors using generative models and category-specific encoders, also incorporating large language models for unseen species.


<details>
  <summary>Details</summary>
Motivation: Current motion transfer methods fail to preserve unique habitual behaviors in animals when transferring motion across species categories.

Method: The paper introduces a habit-preservation module and a category-specific habit encoder within a generative framework. It leverages large language models (LLMs) for motion transfer to unobserved species.

Result: Experiments on the newly introduced DeformingThings4D-skl dataset demonstrate that the proposed model outperforms existing methods.

Conclusion: The framework successfully bridges the gap in habit-preserved motion transfer across animal categories and accommodates unseen species.

Abstract: Animal motion embodies species-specific behavioral habits, making the
transfer of motion across categories a critical yet complex task for
applications in animation and virtual reality. Existing motion transfer
methods, primarily focused on human motion, emphasize skeletal alignment
(motion retargeting) or stylistic consistency (motion style transfer), often
neglecting the preservation of distinct habitual behaviors in animals. To
bridge this gap, we propose a novel habit-preserved motion transfer framework
for cross-category animal motion. Built upon a generative framework, our model
introduces a habit-preservation module with category-specific habit encoder,
allowing it to learn motion priors that capture distinctive habitual
characteristics. Furthermore, we integrate a large language model (LLM) to
facilitate the motion transfer to previously unobserved species. To evaluate
the effectiveness of our approach, we introduce the DeformingThings4D-skl
dataset, a quadruped dataset with skeletal bindings, and conduct extensive
experiments and quantitative analyses, which validate the superiority of our
proposed model.

</details>


### [79] [Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections](https://arxiv.org/abs/2507.07395)
*Yongtang Bao,Chengjie Tang,Yuze Wang,Haojie Li*

Main category: cs.CV

TL;DR: The paper introduces Seg-Wild, a method leveraging 3D Gaussian Splatting for enhanced scene reconstruction and segmentation of unconstrained photo collections, addressing issues like transient occlusions and inconsistent lighting.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges involved in reconstructing and segmenting scenes from readily accessible unconstrained photo collections, which suffer from factors like inconsistent lighting and transient occlusions that hinder segmentation quality.

Method: The proposed approach, Seg-Wild, uses interactive segmentation based on 3D Gaussian Splatting. Each 3D Gaussian has multi-dimensional feature embeddings, and a Spiky 3D Gaussian Cutter (SGC) is introduced to refine segmentation by removing abnormal projections.

Result: Experimental evaluations demonstrate that Seg-Wild outperforms previous methods in segmentation accuracy and reconstruction quality for unconstrained photo collections.

Conclusion: Seg-Wild effectively handles challenges such as transient occlusions and inconsistent lighting, improving segmentation and reconstruction of in-the-wild scenes gathered from the Internet. The method is validated through a newly designed benchmark and experimental comparisons.

Abstract: Reconstructing and segmenting scenes from unconstrained photo collections
obtained from the Internet is a novel but challenging task. Unconstrained photo
collections are easier to get than well-captured photo collections. These
unconstrained images suffer from inconsistent lighting and transient
occlusions, which makes segmentation challenging. Previous segmentation methods
cannot address transient occlusions or accurately restore the scene's lighting
conditions. Therefore, we propose Seg-Wild, an interactive segmentation method
based on 3D Gaussian Splatting for unconstrained image collections, suitable
for in-the-wild scenes. We integrate multi-dimensional feature embeddings for
each 3D Gaussian and calculate the feature similarity between the feature
embeddings and the segmentation target to achieve interactive segmentation in
the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to
smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and
calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We
also designed a benchmark to evaluate segmentation quality in in-the-wild
scenes. Experimental results demonstrate that compared to previous methods,
Seg-Wild achieves better segmentation results and reconstruction quality. Our
code will be available at https://github.com/Sugar0725/Seg-Wild.

</details>


### [80] [EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2507.07410)
*Xinan Zhang,Muhammad Zubair Irshad,Anthony Yezzi,Yi-Chang Tsai,Zsolt Kira*

Main category: cs.CV

TL;DR: EscherNet++ is a novel masked fine-tuned diffusion model designed for zero-shot novel view synthesis and amodal image completion, outperforming traditional multi-stage methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing novel view synthesis approaches, such as complex pipelines and inefficient cross-view dependencies.

Method: Apply input-level and feature-level masking for an end-to-end fine-tuned diffusion model with generalization to arbitrary query views.

Result: Achieves state-of-the-art results, increasing PSNR by 3.9 and Volume IoU by 0.28 in occlusion tasks, while reducing reconstruction time by 95%.

Conclusion: EscherNet++ offers a scalable and efficient solution for novel view synthesis and 3D reconstruction, enhancing practicality in real-world applications.

Abstract: We propose EscherNet++, a masked fine-tuned diffusion model that can
synthesize novel views of objects in a zero-shot manner with amodal completion
ability. Existing approaches utilize multiple stages and complex pipelines to
first hallucinate missing parts of the image and then perform novel view
synthesis, which fail to consider cross-view dependencies and require redundant
storage and computing for separate stages. Instead, we apply masked fine-tuning
including input-level and feature-level masking to enable an end-to-end model
with the improved ability to synthesize novel views and conduct amodal
completion. In addition, we empirically integrate our model with other
feed-forward image-to-mesh models without extra training and achieve
competitive results with reconstruction time decreased by 95%, thanks to its
ability to synthesize arbitrary query views. Our method's scalable nature
further enhances fast 3D reconstruction. Despite fine-tuning on a smaller
dataset and batch size, our method achieves state-of-the-art results, improving
PSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings,
while also generalizing to real-world occluded reconstruction.

</details>


### [81] [EPIC: Efficient Prompt Interaction for Text-Image Classification](https://arxiv.org/abs/2507.07415)
*Xinyao Yu,Hao Sun,Zeyu Ling,Ziwei Niu,Zhenjia Bai,Rui Qin,Yen-Wei Chen,Lanfen Lin*

Main category: cs.CV

TL;DR: A new prompt-based strategy, EPIC, is introduced for text-image classification, improving efficiency and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: The rising computational cost of fine-tuning large-scale pre-trained multimodal models requires more efficient methods for aligning vision and language modalities.

Method: EPIC employs temporal prompts on intermediate layers and utilizes similarity-based interaction to enable effective information exchange between modalities.

Result: EPIC achieves reduced cost with only 1% of trainable parameters of the foundation model, while showing strong performance on UPMC-Food101 and SNLI-VE datasets, and comparable results on MM-IMDB.

Conclusion: EPIC is an efficient and effective prompt-based method for addressing computational challenges in multimodal tasks.

Abstract: In recent years, large-scale pre-trained multimodal models (LMMs) generally
emerge to integrate the vision and language modalities, achieving considerable
success in multimodal tasks, such as text-image classification. The growing
size of LMMs, however, results in a significant computational cost for
fine-tuning these models for downstream tasks. Hence, prompt-based interaction
strategy is studied to align modalities more efficiently. In this context, we
propose a novel efficient prompt-based multimodal interaction strategy, namely
Efficient Prompt Interaction for text-image Classification (EPIC).
Specifically, we utilize temporal prompts on intermediate layers, and integrate
different modalities with similarity-based prompt interaction, to leverage
sufficient information exchange between modalities. Utilizing this approach,
our method achieves reduced computational resource consumption and fewer
trainable parameters (about 1\% of the foundation model) compared to other
fine-tuning strategies. Furthermore, it demonstrates superior performance on
the UPMC-Food101 and SNLI-VE datasets, while achieving comparable performance
on the MM-IMDB dataset.

</details>


### [82] [Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.07424)
*Jingjing Jiang,Chao Ma,Xurui Song,Hanwang Zhang,Jun Luo*

Main category: cs.CV

TL;DR: This paper introduces Corvid, an MLLM designed to address limitations in complex reasoning by integrating a hybrid vision encoder and an advanced CoT training method, achieving superior performance in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs face challenges in complex reasoning, especially in decision-making and problem-solving tasks; the study aims to enhance these reasoning capabilities.

Method: The authors developed Corvid by incorporating a hybrid vision encoder, a GateMixer for cross-modal alignment, a new CoT instruction-following dataset (MCoT-Instruct-287K), and a two-stage training process to boost reasoning. They also introduced an inference-time scaling strategy to balance reasoning levels.

Result: Corvid outperforms existing open-source MLLMs and state-of-the-art models with similar parameter scales, particularly excelling in mathematical reasoning and science problem-solving.

Conclusion: Corvid represents a significant advancement in MLLMs by improving structured reasoning capabilities through architectural innovations, enhanced datasets, and fine-tuned training approaches, potentially broadening the applicability of MLLMs in complex multimodal tasks.

Abstract: Recent advancements in multimodal large language models (MLLMs) have
demonstrated exceptional performance in multimodal perception and
understanding. However, leading open-source MLLMs exhibit significant
limitations in complex and structured reasoning, particularly in tasks
requiring deep reasoning for decision-making and problem-solving. In this work,
we present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning
capabilities. Architecturally, Corvid incorporates a hybrid vision encoder for
informative visual representation and a meticulously designed connector
(GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT
reasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality
multimodal CoT instruction-following dataset, refined and standardized from
diverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid
with a two-stage CoT-formatted training approach to progressively enhance its
step-by-step reasoning abilities. Furthermore, we propose an effective
inference-time scaling strategy that enables Corvid to mitigate over-reasoning
and under-reasoning through self-verification. Extensive experiments
demonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art
MLLMs with similar parameter scales, with notable strengths in mathematical
reasoning and science problem-solving. Project page:
https://mm-vl.github.io/corvid.

</details>


### [83] [A Comprehensive Survey on Deep Learning Solutions for 3D Flood Mapping](https://arxiv.org/abs/2506.13201)
*Wenfeng Jia,Bin Liang,Yuxi Liu,Muhammad Arif Khan,Lihong Zheng*

Main category: cs.CV

TL;DR: This paper surveys deep learning-based 3D flood mapping, contrasting it with traditional 2D approaches, and emphasizes advancements in accuracy and applications for disaster management.


<details>
  <summary>Details</summary>
Motivation: Flooding is a worsening global issue due to climate change and urbanization, necessitating advanced solutions such as 3D flood mapping to enhance disaster management.

Method: The paper categorizes deep learning techniques into task decomposition and end-to-end approaches, evaluates architectures for accuracy and efficiency, and examines diverse data sources like DEMs, satellite imagery, and rainfall for 3D flood mapping.

Result: Key progress and challenges are reviewed, demonstrating how deep learning improves prediction accuracy and computational efficiency but faces issues like data scarcity and model interpretability.

Conclusion: The paper suggests future directions, such as developing better datasets and models, and emphasizes how deep learning in 3D flood mapping can contribute to improved policy and flood management strategies.

Abstract: Flooding remains a major global challenge, worsened by climate change and
urbanization, demanding advanced solutions for effective disaster management.
While traditional 2D flood mapping techniques provide limited insights, 3D
flood mapping, powered by deep learning (DL), offers enhanced capabilities by
integrating flood extent and depth. This paper presents a comprehensive survey
of deep learning-based 3D flood mapping, emphasizing its advancements over 2D
maps by integrating flood extent and depth for effective disaster management
and urban planning. The survey categorizes deep learning techniques into task
decomposition and end-to-end approaches, applicable to both static and dynamic
flood features. We compare key DL architectures, highlighting their respective
roles in enhancing prediction accuracy and computational efficiency.
Additionally, this work explores diverse data sources such as digital elevation
models, satellite imagery, rainfall, and simulated data, outlining their roles
in 3D flood mapping. The applications reviewed range from real-time flood
prediction to long-term urban planning and risk assessment. However,
significant challenges persist, including data scarcity, model
interpretability, and integration with traditional hydrodynamic models. This
survey concludes by suggesting future directions to address these limitations,
focusing on enhanced datasets, improved models, and policy implications for
flood management. This survey aims to guide researchers and practitioners in
leveraging DL techniques for more robust and reliable 3D flood mapping,
fostering improved flood management strategies.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [84] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: The paper identifies memory and communication bottlenecks in AI systems like LLMs and proposes a modular architecture using CXL and XLink for improved scalability, throughput, and flexibility.


<details>
  <summary>Details</summary>
Motivation: AI models like LLMs and RAG demand high memory, bandwidth, and scalability, which current GPU-centric architectures fail to handle efficiently.

Method: The authors propose a modular data center architecture utilizing Compute Express Link (CXL) with hybrid CXL-over-XLink interconnects, hierarchical memory models, and lightweight implementations for scalability.

Result: The proposed architecture shows enhancements in scalability, throughput, and flexibility through evaluations.

Conclusion: Addressing the demands of modern AI workloads requires innovative hardware solutions like modular architectures to overcome GPU-centric limitations and communication bottlenecks.

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [85] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: The paper introduces a distributed training framework for language and vision models that works over unreliable network connections while preserving model accuracy and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Distributed training frameworks often rely on reliable connections, but this assumption inflates latency and limits scalability. A need for a principled solution that handles genuine packet loss without compromising accuracy and convergence was not addressed in the existing frameworks.

Method: The framework uses a two-stage approach: (1) unbiased gradient aggregation where each worker reconstructs consistent gradient estimates from available packets, and (2) bounded-drift parameter broadcasts that limit inter-worker model discrepancy to O(1) even after numerous iterations.

Result: Tests on the LLAMA2 7B model with 64 GPUs showed that the framework could handle 10% random packet loss with only a 0.8% perplexity change, demonstrating analytical bounds' practical applicability.

Conclusion: The proposed framework bridges the gap between datacenter communication efficiency and the accuracy demands of large-model training, facilitating robust learning on less reliable or commodity networks.

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [86] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: This paper evaluates how to efficiently store semantic data on different types of distributed ledger technologies, concluding that private DLTs are the most efficient and hybrids balance auditability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of efficiently storing semantic data on distributed ledger technologies (DLTs), which are crucial for enabling secure and trustworthy decentralized data exchanges in data spaces.

Method: The authors conducted a systematic evaluation comparing different types of DLTs (public, private, and hybrid) using a real-world knowledge graph. They analyzed performance, storage efficiency, resource consumption, and capabilities to update and query semantic data.

Result: The evaluation revealed that private DLTs are highly efficient for handling semantic data, while hybrid DLTs offer a balance between public auditability and operational efficiency.

Conclusion: Private DLTs are optimal for semantic data storage and management in terms of efficiency, whereas hybrid DLTs provide a good compromise where both efficiency and auditability are needed.

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [87] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: The paper analyzes collective communication behaviors in distributed machine learning workloads to address network congestion and packet loss issues.


<details>
  <summary>Details</summary>
Motivation: Investigate traffic patterns of collective communication (e.g., AllReduce) in distributed machine learning to mitigate network performance impacts caused by congestion and packet loss.

Method: Used Nvidia Collective Communication Library logging for detailed data collection and adjusted variables such as parallelism and node count to study their impact on communication behaviors.

Result: The results provided insights into traffic types, transfer sizes, and behavior differences, particularly for the DeepSeek V3 model, highlighting the impact of network anomalies.

Conclusion: Revisiting collective communication frameworks and network designs is necessary to better handle the analyzed workloads in light of network irregularities.

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [88] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: As LLMs require processing extremely long token sequences with real-time constraints, the researchers introduce Helix Parallelism to boost efficiency by combining KV and Tensor Parallelism with optimized GPU usage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiency challenges in scaling LLMs to multi-million-token histories under tight Token-to-Token Latency constraints, particularly related to bottlenecks in FFN weight access and KV cache reads.

Method: Helix Parallelism is proposed as a hybrid approach combining KV parallelism for attention and GPU reuse for TP or TPxEP during FFN computation, with additional lightweight communication (Helix HOP-B) to reduce inefficiencies.

Result: Helix achieves up to 1.5x reduction in TTL for fixed batch sizes and supports extremely large batches (up to 32x larger) under the same latency budget, pushing the boundaries for real-time inference with ultra-long-sequence LLMs.

Conclusion: The work showcases how Helix Parallelism addresses bottlenecks in scaling LLMs for long sequences, achieving improved GPU efficiency, lower latency, and significant batch size capabilities for practical real-time inference.

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [89] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: Ampere proposes a method to reduce on-device computation, device-server communication, and improve model accuracy in federated learning systems.


<details>
  <summary>Details</summary>
Motivation: Federated and Split Federated Learning systems face challenges like high on-device computation cost, communication overhead, and reduced accuracy on non-IID data.

Method: Ampere employs unidirectional inter-block training and a lightweight auxiliary network to reduce gradient transfers and intermediate exchanges; it also consolidates activations for better handling of non-IID data.

Result: Ampere showed up to 13.26% accuracy improvement, 94.6% reduction in training time, 99.1% reduction in communication overhead, and 93.13% reduction in on-device computation.

Conclusion: Ampere effectively addresses major limitations of existing SFL systems, showcasing significant improvements in accuracy, efficiency, and handling of heterogeneity.

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [90] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: The paper presents M$^2$-MFP, a framework designed to predict memory failures in cloud infrastructure by using a Binary Spatial Feature Extractor and dual-path temporal modeling.


<details>
  <summary>Details</summary>
Motivation: To improve hardware reliability in modern IT infrastructure by addressing limitations in existing memory failure prediction methods, which have limited generalizability or suboptimal performance.

Method: The framework converts memory error logs into multi-level binary matrices, extracts high-order features using a Binary Spatial Feature Extractor (BSFE), and applies a dual-path temporal model for prediction using both aggregation and interpretable rule-generation trees.

Result: The proposed method outperforms state-of-the-art approaches on benchmark datasets and real-world applications, demonstrating increased accuracy and reliability.

Conclusion: M$^2$-MFP significantly enhances memory failure prediction capabilities, improving the reliability and availability of cloud infrastructures.

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [91] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: This paper introduces mini-MuMMI, a scaled-down version of a workflow management infrastructure for orchestrating machine learning-driven multiscale models. It allows such simulations to run on modest computational systems instead of requiring massive parallel computing setups.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the complexity of modeling multiscale biomolecular interactions, which requires bridging time and length scales while managing overwhelming computational demands, traditionally achievable only on large HPC systems.

Method: The authors developed and introduced mini-MuMMI, a curated and smaller-scale version of the larger MuMMI infrastructure. It is tailored to enable multiscale simulations on modest computational platforms, like smaller HPC systems or even laptops.

Result: The authors successfully demonstrate the utility of mini-MuMMI by exploring specific molecular interactions, such as RAS-RAF membrane interactions. They also analyze its potential to generalize multiscale workflows beyond this particular domain.

Conclusion: Mini-MuMMI expands the accessibility of multiscale machine learning-driven modeling by reducing the resource barrier. It holds potential for broader applications across various fields beyond molecular dynamics and biomolecular modeling.

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [92] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: The paper introduces KVFlow, a workflow-aware KV cache management framework designed to improve efficiency in LLM agentic workflows by optimizing cache eviction and prefetching strategies.


<details>
  <summary>Details</summary>
Motivation: Current LLM systems using prefix caching suffer from inefficiencies due to the simplistic Least Recently Used (LRU) cache eviction policy, which does not anticipate usage patterns, leading to frequent cache misses.

Method: KVFlow uses an Agent Step Graph to estimate agents' execution timing and prioritizes KV cache retention based on temporal proximity to reuse. Additionally, it incorporates KV prefetching to proactively load tensors between CPU and GPU.

Result: KVFlow demonstrates significant performance improvements, achieving up to 1.83x speedup for single workflows with large prompts and up to 2.19x speedup for concurrent workflows.

Conclusion: KVFlow offers a robust cache management framework, significantly enhancing efficiency for agentic workloads and overcoming limitations of existing cache eviction policies.

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [93] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: The paper introduces MARLISE, a reinforcement learning-based approach for dynamic in-place scaling of resources in edge-cloud systems, outperforming traditional heuristic methods.


<details>
  <summary>Details</summary>
Motivation: Traditional static scaling methods fail to efficiently handle the dynamic and unpredictable workloads in modern edge-cloud systems, leading to suboptimal resource utilization and performance.

Method: The authors propose MARLISE, based on Multi-Agent Reinforcement Learning, utilizing Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms for dynamic, reactive control of in-place resource scaling.

Result: MARLISE demonstrates superior performance in managing resource elasticity, ensuring low microservice response times, and achieving better resource efficiency compared to heuristic methods.

Conclusion: The study validates MARLISE as an effective solution for enhancing resource utilization and performance in edge-cloud systems, addressing challenges of dynamic workload scaling.

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [94] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: Reactive autoscaling in Kubernetes struggles with GPU workloads under dynamic traffic. KIS-S combines a GPU-aware simulator with a PPO-based autoscaler for latency-aware scaling, improving efficiency and reward metrics across diverse traffic patterns.


<details>
  <summary>Details</summary>
Motivation: Address challenges in autoscaling GPU inference workloads in Kubernetes due to the shortcomings of default mechanisms, such as HPA's reactive and threshold-based scaling approaches.

Method: Developed KIS-S, comprising KISim (a GPU-aware Kubernetes simulator) and KIScaler (a PPO-based autoscaler) that learns efficient policies via simulation without retraining.

Result: KIScaler achieves a 75.2% increase in average reward, reduces P95 latency by up to 6.7x compared to CPU baseline, and generalizes across traffic patterns without requiring retraining.

Conclusion: KIS-S bridges the gap between traditional reactive autoscaling and optimized GPU workload orchestration, achieving better latency and scalability in dynamic environments.

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [95] [Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate](https://arxiv.org/abs/2507.07129)
*A. Bochkov*

Main category: cs.LG

TL;DR: The paper proposes a constructive approach to developing large language models (LLMs) using deterministic input embeddings, enabling innovative scaling through modular composition and layer-wise growth.


<details>
  <summary>Details</summary>
Motivation: Existing large language model training methods are resource-heavy and inflexible, necessitating a more efficient and modular approach.

Method: The authors use frozen Unicode glyph embeddings as universal docking ports for modular combination of specialist models and propose layer-wise, incremental training of Transformers.

Result: Specialist models, when averaged post-training, outperform individual models on reasoning benchmarks. Layer-wise growth of Transformers shows stable convergence and better reasoning ability with increased depth.

Conclusion: This approach shifts AI development towards modular and incremental methods, offering benefits like efficiency, continual learning, and easier democratization of AI system building.

Abstract: The prevailing paradigm for scaling large language models (LLMs) involves
monolithic, end-to-end training, a resource-intensive process that lacks
flexibility. This paper explores an alternative, constructive approach to model
development, built upon the foundation of non-trainable, deterministic input
embeddings. In prior [1], we established that high-level semantic reasoning can
emerge in Transformers using frozen embeddings derived from the visual
structure of Unicode glyphs. Here, we demonstrate that this fixed
representational substrate acts as a universal "docking port," enabling two
powerful and efficient scaling paradigms: seamless modular composition and
progressive layer-wise growth.
  First, we show that specialist models trained on disparate datasets (e.g.,
Russian and Chinese text) can be merged into a single, more capable
Mixture-of-Experts (MoE) model, post-training, with zero architectural
modification. This is achieved by simply averaging their output logits. The
resulting MoE model exhibits immediate performance improvements on reasoning
benchmarks like MMLU, surpassing its constituent experts without catastrophic
forgetting. Second, we introduce a layer-wise constructive training
methodology, where a deep Transformer is "grown" by progressively stacking and
training one layer at a time. This method demonstrates stable convergence and a
clear correlation between model depth and the emergence of complex reasoning
abilities, such as those required for SQuAD.
  Our findings suggest a paradigm shift from monolithic optimization towards a
more biological or constructive model of AI development, where complexity is
built incrementally and modules can be composed freely. This opens new avenues
for resource-efficient scaling, continual learning, and a more democratized
ecosystem for building powerful AI systems. We release all code and models to
facilitate further research.

</details>


### [96] [FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval](https://arxiv.org/abs/2507.07135)
*François Gardères,Shizhe Chen,Camille-Sovanneary Gauthier,Jean Ponce*

Main category: cs.LG

TL;DR: The paper introduces FACap, a large-scale fashion-domain dataset for composed image retrieval (CIR), and FashionBLIP-2, a model tailored for fine-grained fashion-specific retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: CIR methods face challenges in domains like fashion due to a rich vocabulary and lack of annotated datasets, limiting their capability to understand fine-grained details.

Method: The authors automatically construct the FACap dataset using web-sourced images, vision-language models (VLMs), and large language models (LLMs) for text annotations. They also propose FashionBLIP-2, which fine-tunes BLIP-2 with adapters and multi-head query-candidate matching.

Result: FashionBLIP-2, when trained on FACap, and fine-tuned further, outperforms existing fashion CIR methods, demonstrating superior performance on benchmarks and enhanced datasets like Fashion IQ and enhFashionIQ.

Conclusion: FACap and FashionBLIP-2 collectively advance the CIR task in the fashion domain, particularly for e-commerce scenarios, by addressing dataset limitations and leveraging detailed text annotations for better retrieval performance.

Abstract: The composed image retrieval (CIR) task is to retrieve target images given a
reference image and a modification text. Recent methods for CIR leverage large
pretrained vision-language models (VLMs) and achieve good performance on
general-domain concepts like color and texture. However, they still struggle
with application domains like fashion, because the rich and diverse vocabulary
used in fashion requires specific fine-grained vision and language
understanding. An additional difficulty is the lack of large-scale fashion
datasets with detailed and relevant annotations, due to the expensive cost of
manual annotation by specialists. To address these challenges, we introduce
FACap, a large-scale, automatically constructed fashion-domain CIR dataset. It
leverages web-sourced fashion images and a two-stage annotation pipeline
powered by a VLM and a large language model (LLM) to generate accurate and
detailed modification texts. Then, we propose a new CIR model FashionBLIP-2,
which fine-tunes the general-domain BLIP-2 model on FACap with lightweight
adapters and multi-head query-candidate matching to better account for
fine-grained fashion-specific information. FashionBLIP-2 is evaluated with and
without additional fine-tuning on the Fashion IQ benchmark and the enhanced
evaluation dataset enhFashionIQ, leveraging our pipeline to obtain
higher-quality annotations. Experimental results show that the combination of
FashionBLIP-2 and pretraining with FACap significantly improves the model's
performance in fashion CIR especially for retrieval with fine-grained
modification texts, demonstrating the value of our dataset and approach in a
highly demanding environment such as e-commerce websites. Code is available at
https://fgxaos.github.io/facap-paper-website/.

</details>


### [97] [Scale leads to compositional generalization](https://arxiv.org/abs/2507.07207)
*Florian Redhardt,Yassir Akram,Simon Schug*

Main category: cs.LG

TL;DR: The study demonstrates that neural networks can achieve compositional generalization with data scaling and appropriate task coverage, and hidden activations can reflect compositional properties.


<details>
  <summary>Details</summary>
Motivation: To investigate whether neural networks can effectively capture compositional task structure and identify the factors influencing this capability.

Method: The authors examined standard neural networks on tasks with compositional structure, analyzed generalization through data/model scaling, and used mathematical proofs to establish performance bounds.

Result: Scaling data/model size and covering task distribution enable compositional generalization. Hidden activations reveal linear decodability of task components, exposing coherence in failure cases of text-to-image models.

Conclusion: Standard neural networks can generalize over compositional structures when datasets and tasks are sufficiently structured, and hidden layer analysis serves as an insightful diagnostic tool.

Abstract: Can neural networks systematically capture discrete, compositional task
structure despite their continuous, distributed nature? The impressive
capabilities of large-scale neural networks suggest that the answer to this
question is yes. However, even for the most capable models, there are still
frequent failure cases that raise doubts about their compositionality. Here, we
seek to understand what it takes for a standard neural network to generalize
over tasks that share compositional structure. We find that simply scaling data
and model size leads to compositional generalization. We show that this holds
across different task encodings as long as the training distribution
sufficiently covers the task space. In line with this finding, we prove that
standard multilayer perceptrons can approximate a general class of
compositional task families to arbitrary precision using only a linear number
of neurons with respect to the number of task modules. Finally, we uncover that
if networks successfully compositionally generalize, the constituents of a task
can be linearly decoded from their hidden activations. We show that this metric
correlates with failures of text-to-image generation models to compose known
concepts.

</details>


### [98] [Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language Model World Knowledge](https://arxiv.org/abs/2507.07137)
*Eric Yeats,Darryl Hannan,Henry Kvinge,Timothy Doster,Scott Mahan*

Main category: cs.LG

TL;DR: The paper focuses on a tool called autoeval-dmun for evaluating the effectiveness and implications of machine unlearning (MU) in diffusion models.


<details>
  <summary>Details</summary>
Motivation: To address challenges in proving the effectiveness of machine unlearning in diffusion models and to assess its impact on surrounding concepts while maintaining deployment readiness.

Method: The authors propose autoeval-dmun, a tool that uses (vision-) language models to extract structured knowledge, identify nearby concepts affected by unlearning, and employ adversarial prompts to evaluate resilience.

Result: Findings indicate that language models can identify semantic relationships correlated to unlearning damage and create synthetic prompts that circumvent unlearning, revealing flaws in existing unlearning methods.

Conclusion: Autonomous evaluation tools like autoeval-dmun are essential for rigorously assessing the balance between successful unlearning and potential collateral performance damage in diffusion models.

Abstract: Machine unlearning (MU) is a promising cost-effective method to cleanse
undesired information (generated concepts, biases, or patterns) from
foundational diffusion models. While MU is orders of magnitude less costly than
retraining a diffusion model without the undesired information, it can be
challenging and labor-intensive to prove that the information has been fully
removed from the model. Moreover, MU can damage diffusion model performance on
surrounding concepts that one would like to retain, making it unclear if the
diffusion model is still fit for deployment. We introduce autoeval-dmun, an
automated tool which leverages (vision-) language models to thoroughly assess
unlearning in diffusion models. Given a target concept, autoeval-dmun extracts
structured, relevant world knowledge from the language model to identify nearby
concepts which are likely damaged by unlearning and to circumvent unlearning
with adversarial prompts. We use our automated tool to evaluate popular
diffusion model unlearning methods, revealing that language models (1) impose
semantic orderings of nearby concepts which correlate well with unlearning
damage and (2) effectively circumvent unlearning with synthetic adversarial
prompts.

</details>


### [99] [Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention](https://arxiv.org/abs/2507.07247)
*Zhengyu Tian,Anantha Padmanaban Krishna Kumar,Hemant Krishnakumar,Reza Rawassizadeh*

Main category: cs.LG

TL;DR: The paper benchmarks eight attention mechanisms using key metrics during GPT-2 training, finding that optimized kernel implementations like Flash Attention are energy-efficient solutions while noting that training time significantly impacts overall energy use.


<details>
  <summary>Details</summary>
Motivation: Efficient attention mechanisms are crucial due to resource-heavy demands in large language and visual models, and there is limited evaluation regarding their energy consumption and hardware resources.

Method: The authors benchmark eight attention mechanisms by measuring training time, GPU memory usage, FLOPS, CPU usage, and power consumption during GPT-2 training.

Result: Optimized attention implementations, such as Flash Attention, LSH Attention, and MLA, demonstrated the best energy efficiency. Training time and energy use are equally critical factors.

Conclusion: Energy-aware benchmarking is vital for attention design. Optimized kernel implementations should be prioritized for resource efficiency in model training.

Abstract: As large language models (LLMs) and visual language models (VLMs) grow in
scale and application, attention mechanisms have become a central computational
bottleneck due to their high memory and time complexity. While many efficient
attention variants have been proposed, there remains a lack of rigorous
evaluation on their actual energy usage and hardware resource demands during
training. In this work, we benchmark eight attention mechanisms in training
GPT-2 architecture, measuring key metrics including training time, GPU memory
usage, FLOPS, CPU usage, and power consumption. Our results reveal that
attention mechanisms with optimized kernel implementations, including Flash
Attention, Locality-Sensitive Hashing (LSH) Attention, and Multi-Head Latent
Attention (MLA), achieve the best energy efficiency. We further show that lower
GPU power alone does not guarantee reduced energy use, as training time plays
an equally important role. Our study highlights the importance of energy-aware
benchmarking in attention design and provides a practical insight for selecting
resource-efficient mechanisms. All our codes are available at GitHub.

</details>


### [100] [GNNs Meet Sequence Models Along the Shortest-Path: an Expressive Method for Link Prediction](https://arxiv.org/abs/2507.07138)
*Francesco Ferrini,Veronica Lachi,Antonio Longa,Bruno Lepri,Andrea Passerini*

Main category: cs.LG

TL;DR: SP4LP, a new framework, enhances link prediction by combining GNNs with sequence modeling over shortest paths to capture multi-hop relational patterns effectively.


<details>
  <summary>Details</summary>
Motivation: Address the inability of GNNs to incorporate link-specific structural patterns for accurate link prediction due to their node-centric design.

Method: SP4LP integrates GNN-based node encodings with a sequence model applied to shortest path structures between node pairs, emphasizing efficiency and multi-hop relational patterns.

Result: SP4LP achieves state-of-the-art results across link prediction benchmarks, demonstrating higher expressiveness than standard GNNs and structural feature methods.

Conclusion: SP4LP establishes itself as an efficient and principled approach for link prediction, enhancing both empirical results and theoretical expressiveness beyond other methods.

Abstract: Graph Neural Networks (GNNs) often struggle to capture the link-specific
structural patterns crucial for accurate link prediction, as their node-centric
message-passing schemes overlook the subgraph structures connecting a pair of
nodes. Existing methods to inject such structural context either incur high
computational cost or rely on simplistic heuristics (e.g., common neighbor
counts) that fail to model multi-hop dependencies. We introduce SP4LP (Shortest
Path for Link Prediction), a novel framework that combines GNN-based node
encodings with sequence modeling over shortest paths. Specifically, SP4LP first
applies a GNN to compute representations for all nodes, then extracts the
shortest path between each candidate node pair and processes the resulting
sequence of node embeddings using a sequence model. This design enables SP4LP
to capture expressive multi-hop relational patterns with computational
efficiency. Empirically, SP4LP achieves state-of-the-art performance across
link prediction benchmarks. Theoretically, we prove that SP4LP is strictly more
expressive than standard message-passing GNNs and several state-of-the-art
structural features methods, establishing it as a general and principled
approach for link prediction in graphs.

</details>


### [101] [Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts](https://arxiv.org/abs/2507.07140)
*Samin Yeasar Arnob,Zhan Su,Minseon Kim,Oleksiy Ostapenko,Riyasat Ohib,Esra'a Saleh,Doina Precup,Lucas Caccia,Alessandro Sordoni*

Main category: cs.LG

TL;DR: The study explores sparse adapters for modular architectures, showing these simple methods outperform LoRA and full fine-tuning in task merging.


<details>
  <summary>Details</summary>
Motivation: To enable adaptable modular architectures capable of enhancing performance across diverse downstream NLP tasks without requiring further fine-tuning.

Method: The paper introduces a simpler approach to training sparse adapters by selectively training subsets of weights and compares their performance against LoRA and full fine-tuning.

Result: Sparse adapters demonstrate superior in-distribution performance post-merging across up to 20 NLP tasks compared to LoRA or full model merging.

Conclusion: Sparse adapters are a promising alternative to LoRA for modular architectures despite challenges in achieving strong held-out performance.

Abstract: Merging parameter-efficient task experts has recently gained growing
attention as a way to build modular architectures that can be rapidly adapted
on the fly for specific downstream tasks, without requiring additional
fine-tuning. Typically, LoRA serves as the foundational building block of such
parameter-efficient modular architectures, leveraging low-rank weight
structures to reduce the number of trainable parameters. In this paper, we
study the properties of sparse adapters, which train only a subset of weights
in the base neural network, as potential building blocks of modular
architectures. First, we propose a simple method for training highly effective
sparse adapters, which is conceptually simpler than existing methods in the
literature and surprisingly outperforms both LoRA and full fine-tuning in our
setting. Next, we investigate the merging properties of these sparse adapters
by merging adapters for up to 20 natural language processing tasks, thus
scaling beyond what is usually studied in the literature. Our findings
demonstrate that sparse adapters yield superior in-distribution performance
post-merging compared to LoRA or full model merging. Achieving strong held-out
performance remains a challenge for all methods considered.

</details>


### [102] [Str-GCL: Structural Commonsense Driven Graph Contrastive Learning](https://arxiv.org/abs/2507.07141)
*Dongxiao He,Yongqi Huang,Jitao Zhao,Xiaobao Wang,Zhen Wang*

Main category: cs.LG

TL;DR: The paper introduces Str-GCL, a framework that incorporates structural commonsense into Graph Contrastive Learning (GCL) using first-order logic rules, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current GCL methods overlook structural commonsense embedded in graphs, which is crucial for effective representation learning, due to the lack of explicit information and guidance in general graphs.

Method: The paper proposes Str-GCL, which uses first-order logic rules to model structural commonsense through topological and attribute-based rules, alongside a representation alignment mechanism, without altering the original graph.

Result: Str-GCL outperforms existing GCL methods in effectively capturing structural commonsense for graph representation learning based on extensive experiments.

Conclusion: Explicitly integrating structural commonsense into GCL frameworks is viable and effective, as shown by the improved performance of Str-GCL, paving the way for new perspectives in graph representation learning.

Abstract: Graph Contrastive Learning (GCL) is a widely adopted approach in
self-supervised graph representation learning, applying contrastive objectives
to produce effective representations. However, current GCL methods primarily
focus on capturing implicit semantic relationships, often overlooking the
structural commonsense embedded within the graph's structure and attributes,
which contains underlying knowledge crucial for effective representation
learning. Due to the lack of explicit information and clear guidance in general
graph, identifying and integrating such structural commonsense in GCL poses a
significant challenge. To address this gap, we propose a novel framework called
Structural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL).
Str-GCL leverages first-order logic rules to represent structural commonsense
and explicitly integrates them into the GCL framework. It introduces
topological and attribute-based rules without altering the original graph and
employs a representation alignment mechanism to guide the encoder in
effectively capturing this commonsense. To the best of our knowledge, this is
the first attempt to directly incorporate structural commonsense into GCL.
Extensive experiments demonstrate that Str-GCL outperforms existing GCL
methods, providing a new perspective on leveraging structural commonsense in
graph representation learning.

</details>


### [103] [TRIP: A Nonparametric Test to Diagnose Biased Feature Importance Scores](https://arxiv.org/abs/2507.07276)
*Aaron Foote,Danny Krizanc*

Main category: cs.LG

TL;DR: Permutation feature importance can be misleading with dependent features due to model extrapolation issues. The paper introduces TRIP, a test that detects unreliable importance scores.


<details>
  <summary>Details</summary>
Motivation: Understanding feature importance in predictions is crucial for interpreting machine learning models. Existing methods, like permutation feature importance, struggle with dependent features.

Method: The researchers propose TRIP, a minimal-assumption test that identifies unreliable feature importance scores caused by model extrapolation under feature dependency. Techniques are provided to extend its application to high-dimensional datasets.

Result: TRIP reliably detects when permutation feature importance scores fail, validating its effectiveness on simulated and real-world data.

Conclusion: TRIP enhances the reliability of feature importance interpretations, addressing shortcomings of permutation methods under feature dependency scenarios.

Abstract: Along with accurate prediction, understanding the contribution of each
feature to the making of the prediction, i.e., the importance of the feature,
is a desirable and arguably necessary component of a machine learning model.
For a complex model such as a random forest, such importances are not innate --
as they are, e.g., with linear regression. Efficient methods have been created
to provide such capabilities, with one of the most popular among them being
permutation feature importance due to its efficiency, model-agnostic nature,
and perceived intuitiveness. However, permutation feature importance has been
shown to be misleading in the presence of dependent features as a result of the
creation of unrealistic observations when permuting the dependent features. In
this work, we develop TRIP (Test for Reliable Interpretation via Permutation),
a test requiring minimal assumptions that is able to detect unreliable
permutation feature importance scores that are the result of model
extrapolation. To build on this, we demonstrate how the test can be
complemented in order to allow its use in high dimensional settings. Through
testing on simulated data and applications, our results show that the test can
be used to reliably detect when permutation feature importance scores are
unreliable.

</details>


### [104] [Understanding Malware Propagation Dynamics through Scientific Machine Learning](https://arxiv.org/abs/2507.07143)
*Karthik Pappu,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: The paper explores how hybrid physics-informed models improve malware propagation prediction, outperforming traditional and neural models.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding and prediction of malware propagation dynamics, especially for adaptive threats, using improved modeling that captures nonlinear feedback mechanisms.

Method: The study evaluates three approaches: traditional Ordinary Differential Equations (ODEs), Universal Differential Equations (UDEs), and Neural ODEs, using data from the Code Red worm outbreak.

Result: The UDE approach reduced prediction error by 44% compared to traditional and neural baselines, while also preserving model interpretability.

Conclusion: Hybrid physics-informed models offer superior prediction accuracy and deeper insights into malware dynamics, supporting better cybersecurity strategies and defenses.

Abstract: Accurately modeling malware propagation is essential for designing effective
cybersecurity defenses, particularly against adaptive threats that evolve in
real time. While traditional epidemiological models and recent neural
approaches offer useful foundations, they often fail to fully capture the
nonlinear feedback mechanisms present in real-world networks. In this work, we
apply scientific machine learning to malware modeling by evaluating three
approaches: classical Ordinary Differential Equations (ODEs), Universal
Differential Equations (UDEs), and Neural ODEs. Using data from the Code Red
worm outbreak, we show that the UDE approach substantially reduces prediction
error compared to both traditional and neural baselines by 44%, while
preserving interpretability. We introduce a symbolic recovery method that
transforms the learned neural feedback into explicit mathematical expressions,
revealing suppression mechanisms such as network saturation, security response,
and malware variant evolution. Our results demonstrate that hybrid
physics-informed models can outperform both purely analytical and purely neural
approaches, offering improved predictive accuracy and deeper insight into the
dynamics of malware spread. These findings support the development of early
warning systems, efficient outbreak response strategies, and targeted cyber
defense interventions.

</details>


### [105] [Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning](https://arxiv.org/abs/2507.07359)
*Zheyu Zhang,Jiayuan Dong,Jie Liu,Xun Huan*

Main category: cs.LG

TL;DR: GO-CBED is a new Bayesian framework for experimental design targeting specific causal research goals. It employs a transformer-based network for efficient decision-making and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods in causal experimental design aim to infer full causal models, which can be inefficient when only specific causal queries are of interest.

Method: The paper introduces GO-CBED, which maximizes expected information gain for user-specified targets, combining a variational lower bound estimator, transformer-based policy network, and normalizing flow-based variational posteriors.

Result: GO-CBED achieves superior performance in causal reasoning and discovery tasks, especially with limited experiments or in complex contexts.

Conclusion: The approach successfully aligns experimental design with research-specific goals, demonstrating efficiency and effectiveness in a variety of tasks.

Abstract: We present GO-CBED, a goal-oriented Bayesian framework for sequential causal
experimental design. Unlike conventional approaches that select interventions
aimed at inferring the full causal model, GO-CBED directly maximizes the
expected information gain (EIG) on user-specified causal quantities of
interest, enabling more targeted and efficient experimentation. The framework
is both non-myopic, optimizing over entire intervention sequences, and
goal-oriented, targeting only model aspects relevant to the causal query. To
address the intractability of exact EIG computation, we introduce a variational
lower bound estimator, optimized jointly through a transformer-based policy
network and normalizing flow-based variational posteriors. The resulting policy
enables real-time decision-making via an amortized network. We demonstrate that
GO-CBED consistently outperforms existing baselines across various causal
reasoning and discovery tasks-including synthetic structural causal models and
semi-synthetic gene regulatory networks-particularly in settings with limited
experimental budgets and complex causal mechanisms. Our results highlight the
benefits of aligning experimental design objectives with specific research
goals and of forward-looking sequential planning.

</details>


### [106] [CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs](https://arxiv.org/abs/2507.07145)
*Zhaojing Zhou,Xunchao Li,Minghao Li,Handi Zhang,Haoshuang Wang,Wenbin Chang,Yiqun Liu,Qingqing Dang,Dianhai Yu,Yanjun Ma,Haifeng Wang*

Main category: cs.LG

TL;DR: This paper introduces Convolutional Code Quantization (CCQ), an approach to compress LLMs to 2.0-2.75 bits while maintaining high accuracy and performance.


<details>
  <summary>Details</summary>
Motivation: The scaling of Large Language Models significantly increases inference costs and deployment challenges. Existing quantization methods below 3-bit levels face issues like accuracy and efficiency degradation, necessitating a better solution.

Method: CCQ employs a hardware-aware encoding/decoding strategy using Convolutional Code, Hybrid Encoding, and Code Cluster. It constructs a lookup-free codebook-to-weight mapping, optimizing inference.

Result: The method compresses models like DeepSeek-V3 (671B parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment and eliminating inter-card communication.

Conclusion: CCQ offers a practical and efficient approach for using LLMs at extremely low-bit quantization levels, balancing accuracy and computational resource requirements effectively.

Abstract: The rapid scaling of Large Language Models (LLMs) elevates inference costs
and compounds substantial deployment barriers. While quantization to 8 or 4
bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and
efficiency degradation. We propose Convolutional Code Quantization (CCQ), an
inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits
with minimal accuracy loss. Departing from error-prone scalar quantization or
slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding
and decoding solution with Convolutional Code, Hybrid Encoding, and Code
Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a
lookup-free encoding space, enabling a linear mapping between the codebook and
weight vectors, thereby optimizing inference performance. Meanwhile, by drawing
on the concept of data mapping from vector quantization, we minimize the
performance degradation of the model under extremely low-bit conditions.
Experiments demonstrate that CCQ achieves outstanding performance on LLMs
across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to
184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE
4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B
model and inference engine have been open-sourced.

</details>


### [107] [An attention-aware GNN-based input defender against multi-turn jailbreak on LLMs](https://arxiv.org/abs/2507.07146)
*Zixuan Huang,Kecheng Huang,Lihao Yin,Bowei He,Huiling Zhen,Mingxuan Yuan,Zili Shao*

Main category: cs.LG

TL;DR: The paper introduces G-Guard, an advanced graph-based model that defends against multi-turn jailbreak attacks on large language models by analyzing relationships in multi-turn conversations and harmful keywords.


<details>
  <summary>Details</summary>
Motivation: As LLMs grow popular, they face security vulnerabilities like jailbreak attacks, particularly multi-turn ones, which are harder to detect and mitigate due to their escalating nature.

Method: The study introduces G-Guard, an attention-aware input classifier using Graph Neural Networks (GNN). It builds an entity graph to link harmful keywords across multi-turn queries and employs an attention-aware mechanism for query augmentation, benefiting classification accuracy.

Result: G-Guard demonstrated superior performance over baseline methods across multiple datasets, showing robust defense capabilities against multi-turn jailbreak attacks.

Conclusion: G-Guard effectively enhances the security of LLMs by accurately classifying and defending against multi-turn jailbreak attacks through innovative graph-based analysis and attention-aware mechanisms.

Abstract: Large Language Models (LLMs) have gained widespread popularity and are
increasingly integrated into various applications. However, their capabilities
can be exploited for both benign and harmful purposes. Despite rigorous
training and fine-tuning for safety, LLMs remain vulnerable to jailbreak
attacks. Recently, multi-turn attacks have emerged, exacerbating the issue.
Unlike single-turn attacks, multi-turn attacks gradually escalate the dialogue,
making them more difficult to detect and mitigate, even after they are
identified.
  In this study, we propose G-Guard, an innovative attention-aware GNN-based
input classifier designed to defend against multi-turn jailbreak attacks on
LLMs. G-Guard constructs an entity graph for multi-turn queries, explicitly
capturing relationships between harmful keywords and queries even when those
keywords appear only in previous queries. Additionally, we introduce an
attention-aware augmentation mechanism that retrieves the most similar
single-turn query based on the multi-turn conversation. This retrieved query is
treated as a labeled node in the graph, enhancing the ability of GNN to
classify whether the current query is harmful. Evaluation results demonstrate
that G-Guard outperforms all baselines across all datasets and evaluation
metrics.

</details>


### [108] [An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces and Applications](https://arxiv.org/abs/2507.07826)
*Erfan Mirzaei,Andreas Maurer,Vladimir R. Kostic,Massimiliano Pontil*

Main category: cs.LG

TL;DR: The paper introduces data-dependent Bernstein inequalities for vector-valued processes in Hilbert space, improving risk bounds for stationary and non-stationary processes.


<details>
  <summary>Details</summary>
Motivation: Statistical learning often faces challenges with non-independent and non-identically distributed data. This paper aims to address such challenges using advanced mathematical techniques.

Method: The authors derive data-dependent Bernstein inequalities for vector-valued processes in Hilbert space, focusing on correlation decay and applying the results to covariance operator estimation and operator learning.

Result: The study achieves new risk bounds for covariance operator estimation in the Hilbert-Schmidt norm and for operator learning in dynamical systems.

Conclusion: The results demonstrate improved estimation techniques for complex data dependencies, supported by both theoretical derivations and numerical experiments.

Abstract: Learning from non-independent and non-identically distributed data poses a
persistent challenge in statistical learning. In this study, we introduce
data-dependent Bernstein inequalities tailored for vector-valued processes in
Hilbert space. Our inequalities apply to both stationary and non-stationary
processes and exploit the potential rapid decay of correlations between
temporally separated variables to improve estimation. We demonstrate the
utility of these bounds by applying them to covariance operator estimation in
the Hilbert-Schmidt norm and to operator learning in dynamical systems,
achieving novel risk bounds. Finally, we perform numerical experiments to
illustrate the practical implications of these bounds in both contexts.

</details>


### [109] [Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation](https://arxiv.org/abs/2507.07147)
*Sua Lee,Kyubum Shin,Jung Ho Park*

Main category: cs.LG

TL;DR: This paper introduces DeMul, a description-free multi-prompt learning technique for adapting vision language models (VLMs) to downstream tasks, achieving improved performance on recognition datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of variability and reliability in existing prompt learning methods that extract text-based descriptions from large language models (LLMs).

Method: The proposed method, DeMul, eliminates the need for extracting text-based descriptions and instead directly integrates knowledge from LLMs into continuous vector prompts. Multi-prompt weighting is also utilized to reflect the importance of different prompts during training.

Result: DeMul achieves superior performance across 11 recognition datasets, demonstrating its effectiveness in adapting VLMs to downstream tasks.

Conclusion: DeMul enhances the adaptability of pre-trained VLMs by using robust, description-free, and weighted multi-prompt learning, proving its efficacy in handling diverse and unseen data.

Abstract: Recent advances in pre-trained Vision Language Models (VLM) have shown
promising potential for effectively adapting to downstream tasks through prompt
learning, without the need for additional annotated paired datasets. To
supplement the text information in VLM trained on correlations with vision
data, new approaches leveraging Large Language Models (LLM) in prompts have
been proposed, enhancing robustness to unseen and diverse data. Existing
methods typically extract text-based responses (i.e., descriptions) from LLM to
incorporate into prompts; however, this approach suffers from high variability
and low reliability. In this work, we propose Description-free Multi-prompt
Learning(DeMul), a novel method that eliminates the process of extracting
descriptions and instead directly distills knowledge from LLM into prompts. By
adopting a description-free approach, prompts can encapsulate richer semantics
while still being represented as continuous vectors for optimization, thereby
eliminating the need for discrete pre-defined templates. Additionally, in a
multi-prompt setting, we empirically demonstrate the potential of prompt
weighting in reflecting the importance of different prompts during training.
Experimental results show that our approach achieves superior performance
across 11 recognition datasets.

</details>


### [110] [Pre-Trained AI Model Assisted Online Decision-Making under Missing Covariates: A Theoretical Perspective](https://arxiv.org/abs/2507.07852)
*Haichen Hu,David Simchi-Levi*

Main category: cs.LG

TL;DR: The paper examines how pre-trained AI models for imputing missing covariates affect regret in sequential decision-making tasks, proposing 'model elasticity' as a key metric.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the impact of pre-trained AI models on decision-making when data is missing and to develop methods to mitigate the resulting regret.

Method: Introduced 'model elasticity' to analyze sensitivity and utilized orthogonal statistical learning and doubly robust regression for sequential model calibration under a MAR setting.

Result: Sequential calibration of pre-trained models significantly improves imputation quality and reduces regret.

Conclusion: Model elasticity offers a unified framework to optimize the integration of pre-trained models, enhancing sequential decision-making across various domains.

Abstract: We study a sequential contextual decision-making problem in which certain
covariates are missing but can be imputed using a pre-trained AI model. From a
theoretical perspective, we analyze how the presence of such a model influences
the regret of the decision-making process. We introduce a novel notion called
"model elasticity", which quantifies the sensitivity of the reward function to
the discrepancy between the true covariate and its imputed counterpart. This
concept provides a unified way to characterize the regret incurred due to model
imputation, regardless of the underlying missingness mechanism. More
surprisingly, we show that under the missing at random (MAR) setting, it is
possible to sequentially calibrate the pre-trained model using tools from
orthogonal statistical learning and doubly robust regression. This calibration
significantly improves the quality of the imputed covariates, leading to much
better regret guarantees. Our analysis highlights the practical value of having
an accurate pre-trained model in sequential decision-making tasks and suggests
that model elasticity may serve as a fundamental metric for understanding and
improving the integration of pre-trained models in a wide range of data-driven
decision-making problems.

</details>


### [111] [Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting with Conditional Guided Flow Matching](https://arxiv.org/abs/2507.07192)
*Huibo Xu,Runlong Yu,Likang Wu,Xianquan Wang,Qi Liu*

Main category: cs.LG

TL;DR: Conditional Guided Flow Matching (CGFM) improves time series forecasting by leveraging error information from auxiliary models, surpassing state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting needs advancements to overcome limitations in rigid generative models and limited error utilization.

Method: CGFM incorporates auxiliary model outputs, historical data conditions, and expands probability path space using two-sided conditional probability paths.

Result: CGFM consistently outperforms state-of-the-art forecasting models in experimental evaluations.

Conclusion: This approach effectively addresses existing challenges and significantly advances the capability and accuracy of forecasting methods.

Abstract: Diffusion models, a type of generative model, have shown promise in time
series forecasting. But they face limitations like rigid source distributions
and limited sampling paths, which hinder their performance. Flow matching
offers faster generation, higher-quality outputs, and greater flexibility,
while also possessing the ability to utilize valuable information from the
prediction errors of prior models, which were previously inaccessible yet
critically important. To address these challenges and fully unlock the untapped
potential of flow matching, we propose Conditional Guided Flow Matching (CGFM).
CGFM extends flow matching by incorporating the outputs of an auxiliary model,
enabling a previously unattainable capability in the field: learning from the
errors of the auxiliary model. For time series forecasting tasks, it integrates
historical data as conditions and guidance, constructs two-sided conditional
probability paths, and uses a general affine path to expand the space of
probability paths, ultimately leading to improved predictions. Extensive
experiments show that CGFM consistently enhances and outperforms
state-of-the-art models, highlighting its effectiveness in advancing
forecasting methods.

</details>


### [112] [Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning](https://arxiv.org/abs/2507.07197)
*Elia Piccoli,Malio Li,Giacomo Carfì,Vincenzo Lomonaco,Davide Bacciu*

Main category: cs.LG

TL;DR: The paper introduces Weight Sharing Attention (WSA), a method to integrate embeddings from multiple pre-trained models for enhanced state representation in reinforcement learning (RL).


<details>
  <summary>Details</summary>
Motivation: Although pre-trained models have shown promising representation capabilities in fields like NLP and vision, their integration with RL remains limited. RL agents typically lack prior knowledge and rely on either learning from scratch or being tied to large foundational models. A more flexible and efficient architecture for combining pre-trained embeddings in RL is needed.

Method: The proposed Weight Sharing Attention (WSA) architecture combines embeddings from multiple pre-trained models into a unified and enriched state representation. It aims to achieve a balance between computational efficiency and model performance.

Result: The WSA model demonstrates performance comparable to end-to-end models in several Atari games. Experiments also reveal its generalization abilities and how scaling the number of pre-trained models impacts the RL agent's performance.

Conclusion: WSA offers a promising approach to leverage multiple pre-trained models in RL, showing both efficiency and competitive effectiveness in complex environments.

Abstract: The recent focus and release of pre-trained models have been a key components
to several advancements in many fields (e.g. Natural Language Processing and
Computer Vision), as a matter of fact, pre-trained models learn disparate
latent embeddings sharing insightful representations. On the other hand,
Reinforcement Learning (RL) focuses on maximizing the cumulative reward
obtained via agent's interaction with the environment. RL agents do not have
any prior knowledge about the world, and they either learn from scratch an
end-to-end mapping between the observation and action spaces or, in more recent
works, are paired with monolithic and computationally expensive Foundational
Models. How to effectively combine and leverage the hidden information of
different pre-trained models simultaneously in RL is still an open and
understudied question. In this work, we propose Weight Sharing Attention (WSA),
a new architecture to combine embeddings of multiple pre-trained models to
shape an enriched state representation, balancing the tradeoff between
efficiency and performance. We run an extensive comparison between several
combination modes showing that WSA obtains comparable performance on multiple
Atari games compared to end-to-end models. Furthermore, we study the
generalization capabilities of this approach and analyze how scaling the number
of models influences agents' performance during and after training.

</details>


### [113] [Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data](https://arxiv.org/abs/2507.07589)
*Arpana Sinhal,Anay Sinhal,Amit Sinhal*

Main category: cs.LG

TL;DR: This study addresses stress detection in healthcare professionals using a multimodal dataset, advanced machine learning, and a systematic approach.


<details>
  <summary>Details</summary>
Motivation: High occupational stress in healthcare professionals, especially during the COVID-19 pandemic, necessitates effective and deployable real-time monitoring solutions.

Method: Created and preprocessed a multimodal physiological dataset using SMOTE for balance, and implemented machine learning models (Random Forest, XGBoost, MLP) combined through a Stacking Classifier.

Result: Developed a reproducible pipeline with improved stress-detection capabilities leveraging ensemble machine learning techniques.

Conclusion: The research provides a framework for deployable stress-monitoring systems with future focus on demographic diversity and edge-computing integration.

Abstract: Healthcare professionals, particularly nurses, face elevated occupational
stress, a concern amplified during the COVID-19 pandemic. While wearable
sensors offer promising avenues for real-time stress monitoring, existing
studies often lack comprehensive datasets and robust analytical frameworks.
This study addresses these gaps by introducing a multimodal dataset comprising
physiological signals, electrodermal activity, heart rate and skin temperature.
A systematic literature review identified limitations in prior stress-detection
methodologies, particularly in handling class imbalance and optimizing model
generalizability. To overcome these challenges, the dataset underwent
preprocessing with the Synthetic Minority Over sampling Technique (SMOTE),
ensuring balanced representation of stress states. Advanced machine learning
models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were
evaluated and combined into a Stacking Classifier to leverage their collective
predictive strengths. By using a publicly accessible dataset and a reproducible
analytical pipeline, this work advances the development of deployable
stress-monitoring systems, offering practical implications for safeguarding
healthcare workers' mental health. Future research directions include expanding
demographic diversity and exploring edge-computing implementations for low
latency stress alerts.

</details>


### [114] [Prospective Learning in Retrospect](https://arxiv.org/abs/2507.07965)
*Yuxin Bai,Cecelia Shuai,Ashwin De Silva,Siyu Yu,Pratik Chaudhari,Joshua T. Vogelstein*

Main category: cs.LG

TL;DR: Current AI approaches struggle with changing data and evolving goals. This paper enhances prospective learning to better handle these shifts, including applications in sequential decision-making like foraging.


<details>
  <summary>Details</summary>
Motivation: Traditional PAC learning fails to adapt to dynamic environments, limiting its effectiveness in real-world scenarios.

Method: Authors build upon the prospective learning framework, improving algorithms and extending its applications to scenarios like sequential decision-making.

Result: Preliminary results demonstrate improved algorithmic performance and applicability to tasks like foraging.

Conclusion: Enhanced prospective learning offers a promising approach to address dynamic environments and is applicable beyond traditional domains.

Abstract: In most real-world applications of artificial intelligence, the distributions
of the data and the goals of the learners tend to change over time. The
Probably Approximately Correct (PAC) learning framework, which underpins most
machine learning algorithms, fails to account for dynamic data distributions
and evolving objectives, often resulting in suboptimal performance. Prospective
learning is a recently introduced mathematical framework that overcomes some of
these limitations. We build on this framework to present preliminary results
that improve the algorithm and numerical results, and extend prospective
learning to sequential decision-making scenarios, specifically foraging. Code
is available at: https://github.com/neurodata/prolearn2.

</details>


### [115] [Reinforcement Learning with Action Chunking](https://arxiv.org/abs/2507.07969)
*Qiyang Li,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: The paper introduces Q-chunking, an approach to improve reinforcement learning (RL) for long-horizon tasks by adopting action chunking to enable efficient exploration and learning.


<details>
  <summary>Details</summary>
Motivation: Challenges like effective exploration and sample-efficient learning in offline-to-online RL make it necessary to explore how offline datasets can guide policies in sparse-reward environments.

Method: Q-chunking incorporates action chunking into TD-based RL algorithms, allowing the agent to operate in a 'chunked' action space and apply $n$-step TD learning for stability and efficiency.

Result: The method improves performance and efficiency, outperforming previous offline-to-online RL methods in long-horizon manipulation tasks with sparse rewards.

Conclusion: Q-chunking successfully leverages offline datasets for better exploration and learning, marking it as a promising approach for RL in challenging settings.

Abstract: We present Q-chunking, a simple yet effective recipe for improving
reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.
Our recipe is designed for the offline-to-online RL setting, where the goal is
to leverage an offline prior dataset to maximize the sample-efficiency of
online learning. Effective exploration and sample-efficient learning remain
central challenges in this setting, as it is not obvious how the offline data
should be utilized to acquire a good exploratory policy. Our key insight is
that action chunking, a technique popularized in imitation learning where
sequences of future actions are predicted rather than a single action at each
timestep, can be applied to temporal difference (TD)-based RL methods to
mitigate the exploration challenge. Q-chunking adopts action chunking by
directly running RL in a 'chunked' action space, enabling the agent to (1)
leverage temporally consistent behaviors from offline data for more effective
online exploration and (2) use unbiased $n$-step backups for more stable and
efficient TD learning. Our experimental results demonstrate that Q-chunking
exhibits strong offline performance and online sample efficiency, outperforming
prior best offline-to-online methods on a range of long-horizon, sparse-reward
manipulation tasks.

</details>


### [116] [Bias-Aware Mislabeling Detection via Decoupled Confident Learning](https://arxiv.org/abs/2507.07216)
*Yunyi Li,Maria De-Arteaga,Maytal Saar-Tsechansky*

Main category: cs.LG

TL;DR: This paper introduces Decoupled Confident Learning (DeCoLe), a framework to detect mislabeled data, particularly addressing label bias in datasets.


<details>
  <summary>Details</summary>
Motivation: Label bias, creating systematic errors in data labeling across social groups, undermines data quality and impacts critical domains like hate speech detection.

Method: The proposed method, DeCoLe, leverages a machine learning framework to detect mislabeled instances in data impacted by label bias, with theoretical justification and empirical validation.

Result: DeCoLe consistently outperforms other approaches in bias-aware mislabeling detection, with strong results in hate speech detection datasets.

Conclusion: DeCoLe provides an effective and integrable solution for detecting and addressing label bias, enhancing data reliability in organizational practices.

Abstract: Reliable data is a cornerstone of modern organizational systems. A notable
data integrity challenge stems from label bias, which refers to systematic
errors in a label, a covariate that is central to a quantitative analysis, such
that its quality differs across social groups. This type of bias has been
conceptually and empirically explored and is widely recognized as a pressing
issue across critical domains. However, effective methodologies for addressing
it remain scarce. In this work, we propose Decoupled Confident Learning
(DeCoLe), a principled machine learning based framework specifically designed
to detect mislabeled instances in datasets affected by label bias, enabling
bias aware mislabelling detection and facilitating data quality improvement. We
theoretically justify the effectiveness of DeCoLe and evaluate its performance
in the impactful context of hate speech detection, a domain where label bias is
a well documented challenge. Empirical results demonstrate that DeCoLe excels
at bias aware mislabeling detection, consistently outperforming alternative
approaches for label error detection. Our work identifies and addresses the
challenge of bias aware mislabeling detection and offers guidance on how DeCoLe
can be integrated into organizational data management practices as a powerful
tool to enhance data reliability.

</details>


### [117] [Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems](https://arxiv.org/abs/2507.07222)
*Minchan Jeong,J. Jon Ryu,Se-Young Yun,Gregory W. Wornell*

Main category: cs.LG

TL;DR: This paper develops a scalable method for learning the leading singular functions of the Koopman operator, avoiding unstable linear operations and improving downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for learning Koopman operator singular subspaces suffer from computational instability and scalability issues, particularly due to operations like SVD and matrix inversion.

Method: The authors introduce a deep learning-compatible, low-rank approximation-based method to learn the top-k singular functions of the Koopman operator, avoiding numerically unstable operations.

Result: The proposed method reliably and effectively learns singular subspaces, excelling in tasks such as eigen-analysis and multi-step prediction.

Conclusion: The work presents a robust approach for Koopman operator analysis that aligns with modern deep learning practices while addressing scalability and stability challenges.

Abstract: The Koopman operator provides a principled framework for analyzing nonlinear
dynamical systems through linear operator theory. Recent advances in dynamic
mode decomposition (DMD) have shown that trajectory data can be used to
identify dominant modes of a system in a data-driven manner. Building on this
idea, deep learning methods such as VAMPnet and DPNet have been proposed to
learn the leading singular subspaces of the Koopman operator. However, these
methods require backpropagation through potentially numerically unstable
operations on empirical second moment matrices, such as singular value
decomposition and matrix inversion, during objective computation, which can
introduce biased gradient estimates and hinder scalability to large systems. In
this work, we propose a scalable and conceptually simple method for learning
the top-k singular functions of the Koopman operator for stochastic dynamical
systems based on the idea of low-rank approximation. Our approach eliminates
the need for unstable linear algebraic operations and integrates easily into
modern deep learning pipelines. Empirical results demonstrate that the learned
singular subspaces are both reliable and effective for downstream tasks such as
eigen-analysis and multi-step prediction.

</details>


### [118] [An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation](https://arxiv.org/abs/2507.07236)
*Maya Kruse,Majid Afshar,Saksham Khatwani,Anoop Mayampurath,Guanhua Chen,Yanjun Gao*

Main category: cs.LG

TL;DR: This paper proposes MUSE, a method using subset ensembles of diverse LLMs for better calibration and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification is critical for high-stakes applications, and existing methods largely focus on single models, missing potential benefits of combining diverse models.

Method: MUSE uses Jensen-Shannon Divergence to select and aggregate subsets of complementary LLMs, improving uncertainty estimates.

Result: Experiments reveal better calibration and predictive performance compared to individual models or simple ensembles in binary prediction tasks.

Conclusion: Aggregating outputs from subsets of diverse LLMs enhances reliable uncertainty estimation, demonstrating the advantage of model diversity.

Abstract: Large language models (LLMs) often behave inconsistently across inputs,
indicating uncertainty and motivating the need for its quantification in
high-stakes settings. Prior work on calibration and uncertainty quantification
often focuses on individual models, overlooking the potential of model
diversity. We hypothesize that LLMs make complementary predictions due to
differences in training and the Zipfian nature of language, and that
aggregating their outputs leads to more reliable uncertainty estimates. To
leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a
simple information-theoretic method that uses Jensen-Shannon Divergence to
identify and aggregate well-calibrated subsets of LLMs. Experiments on binary
prediction tasks demonstrate improved calibration and predictive performance
compared to single-model and naive ensemble baselines.

</details>


### [119] [Towards Robust Surrogate Models: Benchmarking Machine Learning Approaches to Expediting Phase Field Simulations of Brittle Fracture](https://arxiv.org/abs/2507.07237)
*Erfan Hamdi,Emma Lejeune*

Main category: cs.LG

TL;DR: The paper introduces a challenging dataset based on Phase Field Modeling (PFM) simulations for fracture modeling to tackle complex benchmarks beyond simplistic cases. It evaluates baseline ML models like PINN, FNO, and UNet against the dataset, also analyzing the effect of ensembling strategies.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in fracture modeling benchmarks by introducing a more complex dataset suitable for advancing machine learning methods, which traditionally relied on overly simple cases that do not align with real-world fracture complexity. Modeling fracture efficiently is essential for multi-scale modeling and uncertainty quantification.

Method: The authors develop a dataset based on PFM simulations incorporating three energy decomposition methods, two boundary conditions, and 1,000 random initial crack configurations, comprising 6,000 simulations. They apply baseline ML models like PINN, FNO, UNet, and ensemble approaches to this dataset to evaluate prediction performance.

Result: The study demonstrates the strengths and shortcomings of popular machine learning models in approximating PFM, showing their potential as well as limitations. The dataset proves effective in serving as a challenging benchmark for ML modeling in fracture mechanics.

Conclusion: The paper highlights the relevance of the dataset as a standardized tool for advancing ML techniques in fracture mechanics. It underscores how the dataset allows for more accurate evaluations of machine learning approaches, contributing to closing the gap between ML capabilities and complex fracture phenomena.

Abstract: Data driven approaches have the potential to make modeling complex, nonlinear
physical phenomena significantly more computationally tractable. For example,
computational modeling of fracture is a core challenge where machine learning
techniques have the potential to provide a much needed speedup that would
enable progress in areas such as mutli-scale modeling and uncertainty
quantification. Currently, phase field modeling (PFM) of fracture is one such
approach that offers a convenient variational formulation to model crack
nucleation, branching and propagation. To date, machine learning techniques
have shown promise in approximating PFM simulations. However, most studies rely
on overly simple benchmarks that do not reflect the true complexity of the
fracture processes where PFM excels as a method. To address this gap, we
introduce a challenging dataset based on PFM simulations designed to benchmark
and advance ML methods for fracture modeling. This dataset includes three
energy decomposition methods, two boundary conditions, and 1,000 random initial
crack configurations for a total of 6,000 simulations. Each sample contains 100
time steps capturing the temporal evolution of the crack field. Alongside this
dataset, we also implement and evaluate Physics Informed Neural Networks
(PINN), Fourier Neural Operators (FNO) and UNet models as baselines, and
explore the impact of ensembling strategies on prediction accuracy. With this
combination of our dataset and baseline models drawn from the literature we aim
to provide a standardized and challenging benchmark for evaluating machine
learning approaches to solid mechanics. Our results highlight both the promise
and limitations of popular current models, and demonstrate the utility of this
dataset as a testbed for advancing machine learning in fracture mechanics
research.

</details>


### [120] [Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning](https://arxiv.org/abs/2507.07259)
*Giulio Rossolini,Fabio Brau,Alessandro Biondi,Battista Biggio,Giorgio Buttazzo*

Main category: cs.LG

TL;DR: This paper identifies a security vulnerability in distributed deep learning systems by showing that intercepted intermediate features can be exploited to create highly transferable adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore vulnerabilities in partitioned deep learning models deployed in distributed environments, specifically focusing on the risks of intermediate feature leakage.

Method: The proposed method involves analyzing intercepted intermediate features to reconstruct tensor shapes, then crafting surrogate architectures for feature distillation and generating effective adversarial attacks.

Result: Experimental results reveal that leveraging intermediate features significantly improves the transferability of adversarial attacks crafted by proxy models.

Conclusion: The study highlights the critical security risks posed by intermediate feature leakage in distributed deep learning systems and stresses the importance of designing more secure frameworks.

Abstract: As machine learning models become increasingly deployed across the edge of
internet of things environments, a partitioned deep learning paradigm in which
models are split across multiple computational nodes introduces a new dimension
of security risk. Unlike traditional inference setups, these distributed
pipelines span the model computation across heterogeneous nodes and
communication layers, thereby exposing a broader attack surface to potential
adversaries. Building on these motivations, this work explores a previously
overlooked vulnerability: even when both the edge and cloud components of the
model are inaccessible (i.e., black-box), an adversary who intercepts the
intermediate features transmitted between them can still pose a serious threat.
We demonstrate that, under these mild and realistic assumptions, an attacker
can craft highly transferable proxy models, making the entire deep learning
system significantly more vulnerable to evasion attacks. In particular, the
intercepted features can be effectively analyzed and leveraged to distill
surrogate models capable of crafting highly transferable adversarial examples
against the target model. To this end, we propose an exploitation strategy
specifically designed for distributed settings, which involves reconstructing
the original tensor shape from vectorized transmitted features using simple
statistical analysis, and adapting surrogate architectures accordingly to
enable effective feature distillation. A comprehensive and systematic
experimental evaluation has been conducted to demonstrate that surrogate models
trained with the proposed strategy, i.e., leveraging intermediate features,
tremendously improve the transferability of adversarial attacks. These findings
underscore the urgent need to account for intermediate feature leakage in the
design of secure distributed deep learning systems.

</details>


### [121] [Robust Multimodal Learning Framework For Intake Gesture Detection Using Contactless Radar and Wearable IMU Sensors](https://arxiv.org/abs/2507.07261)
*Chunzhuo Wang,Hans Hallez,Bart Vanrumste*

Main category: cs.LG

TL;DR: This paper proposes a new multimodal learning framework for food intake gesture detection using both IMU and radar sensors, achieving higher performance even with missing modality inputs.


<details>
  <summary>Details</summary>
Motivation: To improve food intake gesture detection for dietary monitoring and address the challenge of reduced robustness in multimodal learning when one sensing modality is unavailable.

Method: A robust multimodal temporal convolutional network with cross-modal attention (MM-TCN-CMA) was developed to integrate IMU and radar data, with evaluations on a newly created dataset showcasing 3,847 eating and drinking gestures.

Result: The proposed MM-TCN-CMA framework improved detection performance with a segmental F1-score increase of 4.3% and 5.2% over unimodal radar and IMU models, and maintained robustness with gains of 1.3% and 2.4% under missing modality inputs.

Conclusion: The MM-TCN-CMA framework effectively demonstrates that combining wearable IMU sensors and radar data enhances food intake gesture detection, paving the way for more robust dietary monitoring systems.

Abstract: Automated food intake gesture detection plays a vital role in dietary
monitoring, enabling objective and continuous tracking of eating behaviors to
support better health outcomes. Wrist-worn inertial measurement units (IMUs)
have been widely used for this task with promising results. More recently,
contactless radar sensors have also shown potential. This study explores
whether combining wearable and contactless sensing modalities through
multimodal learning can further improve detection performance. We also address
a major challenge in multimodal learning: reduced robustness when one modality
is missing. To this end, we propose a robust multimodal temporal convolutional
network with cross-modal attention (MM-TCN-CMA), designed to integrate IMU and
radar data, enhance gesture detection, and maintain performance under missing
modality conditions. A new dataset comprising 52 meal sessions (3,050 eating
gestures and 797 drinking gestures) from 52 participants is developed and made
publicly available. Experimental results show that the proposed framework
improves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU
models, respectively. Under missing modality scenarios, the framework still
achieves gains of 1.3% and 2.4% for missing radar and missing IMU inputs. This
is the first study to demonstrate a robust multimodal learning framework that
effectively fuses IMU and radar data for food intake gesture detection.

</details>


### [122] [Beyond the ATE: Interpretable Modelling of Treatment Effects over Dose and Time](https://arxiv.org/abs/2507.07271)
*Julianna Piskorz,Krzysztof Kacprzyk,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: The paper introduces a novel framework to model treatment effects over dose and time, moving beyond static metrics like ATE, by using interpretable trajectory modeling to enable actionable clinical insights.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like ATE in causal inference fail to capture the dynamic nature of treatment effects across dose and time, limiting insights particularly in fields like healthcare.

Method: The proposed method adapts SemanticODE for modeling treatment dynamics, decoupling trajectory estimation from defining clinically relevant properties, allowing for interpretability and flexible analysis.

Result: The approach demonstrates the ability to accurately and transparently model the dynamics of treatment effects, offering robust and editable models for practical and clinical use.

Conclusion: The framework enhances the understanding of treatment effects by providing actionable models and insights for causal analysis and decision-making in high-stakes domains.

Abstract: The Average Treatment Effect (ATE) is a foundational metric in causal
inference, widely used to assess intervention efficacy in randomized controlled
trials (RCTs). However, in many applications -- particularly in healthcare --
this static summary fails to capture the nuanced dynamics of treatment effects
that vary with both dose and time. We propose a framework for modelling
treatment effect trajectories as smooth surfaces over dose and time, enabling
the extraction of clinically actionable insights such as onset time, peak
effect, and duration of benefit. To ensure interpretability, robustness, and
verifiability -- key requirements in high-stakes domains -- we adapt
SemanticODE, a recent framework for interpretable trajectory modelling, to the
causal setting where treatment effects are never directly observed. Our
approach decouples the estimation of trajectory shape from the specification of
clinically relevant properties (e.g., maxima, inflection points), supporting
domain-informed priors, post-hoc editing, and transparent analysis. We show
that our method yields accurate, interpretable, and editable models of
treatment dynamics, facilitating both rigorous causal analysis and practical
decision-making.

</details>


### [123] [Natural Evolutionary Search meets Probabilistic Numerics](https://arxiv.org/abs/2507.07288)
*Pierre Osselin,Masaki Adachi,Xiaowen Dong,Michael A. Osborne*

Main category: cs.LG

TL;DR: The paper introduces Probabilistic Natural Evolutionary Strategy (ProbNES) algorithms, combining NES with Bayesian quadrature to enhance sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve the sample efficiency of Natural Evolution Strategies (NES), which are limited due to their reliance on random sampling and Monte Carlo estimates.

Method: Proposes ProbNES algorithms, integrating Bayesian quadrature into the NES framework to enhance optimization in black-box settings.

Result: ProbNES outperforms traditional NES, Bayesian Optimization (BO), and πBO in various optimization tasks, including benchmark problems, data-driven, hyperparameter tuning, and locomotion tasks.

Conclusion: ProbNES improves on NES by leveraging Bayesian quadrature, demonstrating superior performance and sample efficiency in several application domains.

Abstract: Zeroth-order local optimisation algorithms are essential for solving
real-valued black-box optimisation problems. Among these, Natural Evolution
Strategies (NES) represent a prominent class, particularly well-suited for
scenarios where prior distributions are available. By optimising the objective
function in the space of search distributions, NES algorithms naturally
integrate prior knowledge during initialisation, making them effective in
settings such as semi-supervised learning and user-prior belief frameworks.
However, due to their reliance on random sampling and Monte Carlo estimates,
NES algorithms can suffer from limited sample efficiency. In this paper, we
introduce a novel class of algorithms, termed Probabilistic Natural
Evolutionary Strategy Algorithms (ProbNES), which enhance the NES framework
with Bayesian quadrature. We show that ProbNES algorithms consistently
outperforms their non-probabilistic counterparts as well as global sample
efficient methods such as Bayesian Optimisation (BO) or $\pi$BO across a wide
range of tasks, including benchmark test functions, data-driven optimisation
tasks, user-informed hyperparameter tuning tasks and locomotion tasks.

</details>


### [124] [Estimating Dataset Dimension via Singular Metrics under the Manifold Hypothesis: Application to Inverse Problems](https://arxiv.org/abs/2507.07291)
*Paola Causin,Alessio Marta*

Main category: cs.LG

TL;DR: The paper presents a framework using Mixture of Variational Autoencoders (VAEs) and Riemannian geometry tools to address intrinsic dimension estimation, local coordinate construction, and learning mappings in high-dimensional manifold datasets.


<details>
  <summary>Details</summary>
Motivation: To fully exploit the low-dimensional manifolds in high-dimensional data and address intrinsic dimension estimation, local coordinate construction, and mappings, improving solutions for machine learning and inverse problems.

Method: The framework uses Mixture of Variational Autoencoders (VAEs) to estimate intrinsic dimensions, construct local coordinates via atlas creation, and incorporate the Riemannian pullback metric for analysis.

Result: Estimates of intrinsic dimensions via the pullback metric improve manifold parameterization, solutions to inverse problems (e.g., biomedical imaging), and enable efficient inference.

Conclusion: The method leverages intrinsic dimensionality for better manifold learning, improving reconstruction and inference efficiency while demonstrating utility in monitor model capacity through intrinsic dimensionality.

Abstract: High-dimensional datasets often exhibit low-dimensional geometric structures,
as suggested by the manifold hypothesis, which implies that data lie on a
smooth manifold embedded in a higher-dimensional ambient space. While this
insight underpins many advances in machine learning and inverse problems, fully
leveraging it requires to deal with three key tasks: estimating the intrinsic
dimension (ID) of the manifold, constructing appropriate local coordinates, and
learning mappings between ambient and manifold spaces. In this work, we propose
a framework that addresses all these challenges using a Mixture of Variational
Autoencoders (VAEs) and tools from Riemannian geometry. We specifically focus
on estimating the ID of datasets by analyzing the numerical rank of the VAE
decoder pullback metric. The estimated ID guides the construction of an atlas
of local charts using a mixture of invertible VAEs, enabling accurate manifold
parameterization and efficient inference. We how this approach enhances
solutions to ill-posed inverse problems, particularly in biomedical imaging, by
enforcing that reconstructions lie on the learned manifold. Lastly, we explore
the impact of network pruning on manifold geometry and reconstruction quality,
showing that the intrinsic dimension serves as an effective proxy for
monitoring model capacity.

</details>


### [125] [Discretization-independent multifidelity operator learning for partial differential equations](https://arxiv.org/abs/2507.07292)
*Jacob Hauck,Yanzhi Zhang*

Main category: cs.LG

TL;DR: The paper introduces a discretization-independent operator learning model to improve multifidelity learning, validated by theoretical and numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Address challenges in operator learning models, aiming for robust and efficient multifidelity approaches by achieving discretization independence.

Method: Develop a model with numerical operator learning techniques leveraging neural representations, providing theories for approximation and validation through numerical PDE experiments.

Result: Demonstrated improved accuracy, computational efficiency, and enhanced discretization independence through multifidelity training.

Conclusion: The proposed approach establishes a strong foundation for robust multifidelity operator learning models, suitable for diverse computational scenarios.

Abstract: We develop a new and general encode-approximate-reconstruct operator learning
model that leverages learned neural representations of bases for input and
output function distributions. We introduce the concepts of \textit{numerical
operator learning} and \textit{discretization independence}, which clarify the
relationship between theoretical formulations and practical realizations of
operator learning models. Our model is discretization-independent, making it
particularly effective for multifidelity learning. We establish theoretical
approximation guarantees, demonstrating uniform universal approximation under
strong assumptions on the input functions and statistical approximation under
weaker conditions. To our knowledge, this is the first comprehensive study that
investigates how discretization independence enables robust and efficient
multifidelity operator learning. We validate our method through extensive
numerical experiments involving both local and nonlocal PDEs, including
time-independent and time-dependent problems. The results show that
multifidelity training significantly improves accuracy and computational
efficiency. Moreover, multifidelity training further enhances empirical
discretization independence.

</details>


### [126] [AdeptHEQ-FL: Adaptive Homomorphic Encryption for Federated Learning of Hybrid Classical-Quantum Models with Dynamic Layer Sparing](https://arxiv.org/abs/2507.07316)
*Md Abrar Jahin,Taufikur Rahman Fuad,M. F. Mridha,Nafiz Fahad,Md. Jakir Hossen*

Main category: cs.LG

TL;DR: The paper introduces AdeptHEQ-FL, a hybrid classical-quantum federated learning (FL) framework tackling challenges such as model performance, privacy, and communication efficiency.


<details>
  <summary>Details</summary>
Motivation: To address key challenges in FL like balancing model performance, privacy, and communication efficiency, especially in non-IID decentralized setups, and integrate quantum computing capabilities.

Method: The framework combines hybrid CNN-PQC architecture, adaptive accuracy-weighted aggregation using differentially private techniques, selective homomorphic encryption for sensitive layers, and dynamic layer-wise adaptive freezing.

Result: Achieved 25.43% and 14.17% accuracy improvements compared to Standard-FedQNN and FHE-FedQNN on CIFAR-10. It also reduced communication overhead through layer-freezing strategies.

Conclusion: AdeptHEQ-FL showcases efficiency in balancing performance, privacy, and resource utilization, making it a versatile and practical approach for FL.

Abstract: Federated Learning (FL) faces inherent challenges in balancing model
performance, privacy preservation, and communication efficiency, especially in
non-IID decentralized environments. Recent approaches either sacrifice formal
privacy guarantees, incur high overheads, or overlook quantum-enhanced
expressivity. We introduce AdeptHEQ-FL, a unified hybrid classical-quantum FL
framework that integrates (i) a hybrid CNN-PQC architecture for expressive
decentralized learning, (ii) an adaptive accuracy-weighted aggregation scheme
leveraging differentially private validation accuracies, (iii) selective
homomorphic encryption (HE) for secure aggregation of sensitive model layers,
and (iv) dynamic layer-wise adaptive freezing to minimize communication
overhead while preserving quantum adaptability. We establish formal privacy
guarantees, provide convergence analysis, and conduct extensive experiments on
the CIFAR-10, SVHN, and Fashion-MNIST datasets. AdeptHEQ-FL achieves a $\approx
25.43\%$ and $\approx 14.17\%$ accuracy improvement over Standard-FedQNN and
FHE-FedQNN, respectively, on the CIFAR-10 dataset. Additionally, it reduces
communication overhead by freezing less important layers, demonstrating the
efficiency and practicality of our privacy-preserving, resource-aware design
for FL.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [127] [A Robust, Open-Source Framework for Spiking Neural Networks on Low-End FPGAs](https://arxiv.org/abs/2507.07284)
*Andrew Fan,Simon D. Levy*

Main category: cs.NE

TL;DR: This paper introduces an efficient framework for accelerating Spiking Neural Networks (SNNs) on low-end FPGAs and demonstrates its competitive performance with minimal resource usage.


<details>
  <summary>Details</summary>
Motivation: With the growing computational demands of traditional neural networks, SNNs offer an energy-efficient alternative. However, existing neuromorphic hardware is largely inaccessible, and FPGA-based implementations often require expensive hardware or restrict flexibility in network topology.

Method: The paper presents a two-part framework: first, a robust SNN acceleration architecture targeting low-end FPGAs, featuring a scalable synaptic array; second, a Pytorch-based compiler for SNN models. The design minimizes resource usage (6358 LUT, 40.5 BRAM) and supports any-to-any and fully connected topologies.

Result: The framework was implemented and tested on a low-end Xilinx Artix-7 FPGA running at 100 MHz. It achieved competitive performance in tasks like MNIST digit recognition (0.52 ms/img) and demonstrated accurate simulation for other toy SNN problems.

Conclusion: The proposed framework offers an accessible, resource-efficient, and flexible solution for SNN acceleration on low-end FPGAs, enabling wider community adoption and practical applications. Code and setup instructions make the work easily reproducible.

Abstract: As the demand for compute power in traditional neural networks has increased
significantly, spiking neural networks (SNNs) have emerged as a potential
solution to increasingly power-hungry neural networks. By operating on 0/1
spikes emitted by neurons instead of arithmetic multiply-and-accumulate
operations, SNNs propagate information temporally and spatially, allowing for
more efficient compute power. To this end, many architectures for accelerating
and simulating SNNs have been developed, including Loihi, TrueNorth, and
SpiNNaker. However, these chips are largely inaccessible to the wider
community. Field programmable gate arrays (FPGAs) have been explored to serve
as a middle ground between neuromorphic and non-neuromorphic hardware, but many
proposed architectures require expensive high-end FPGAs or target a single SNN
topology. This paper presents a framework consisting of a robust SNN
acceleration architecture and a Pytorch-based SNN model compiler. Targeting
any-to-any and/or fully connected SNNs, the FPGA architecture features a
synaptic array that tiles across the SNN to propagate spikes. The architecture
targets low-end FPGAs and requires very little (6358 LUT, 40.5 BRAM) resources.
The framework, tested on a low-end Xilinx Artix-7 FPGA at 100 MHz, achieves
competitive speed in recognizing MNIST digits (0.52 ms/img). Further
experiments also show accurate simulation of hand coded any-to-any spiking
neural networks on toy problems. All code and setup instructions are available
at
https://github.com/im-afan/snn-fpga}{\texttt{https://github.com/im-afan/snn-fpga.

</details>


### [128] [Homeostatic Adaptation of Optimal Population Codes under Metabolic Stress](https://arxiv.org/abs/2507.07874)
*Yi-Chun Hung,Gregory Schwartz,Emily A. Cooper,Emma Alexander*

Main category: cs.NE

TL;DR: This paper introduces a novel theoretical framework for population coding in neurons, considering energy limits and noise properties.


<details>
  <summary>Details</summary>
Motivation: To explain how neurons maintain firing rate homeostasis and adapt their coding properties under metabolic stress, a phenomenon not well modeled by existing approaches.

Method: Developing a theoretical framework using constraints approximating firing rate homeostasis and linking energy use to noise levels through biophysical simulations.

Result: The framework reveals an energy-dependent Poisson noise model and derivation of optimal coding strategies, validated with properties measurable in experiments.

Conclusion: The proposed model effectively explains how neurons adapt their tuning curves and maintain homeostasis under metabolic constraints, making it generalizable for studying optimal neural codes.

Abstract: Information processing in neural populations is inherently constrained by
metabolic resource limits and noise properties, with dynamics that are not
accurately described by existing mathematical models. Recent data, for example,
shows that neurons in mouse visual cortex go into a "low power mode" in which
they maintain firing rate homeostasis while expending less energy. This
adaptation leads to increased neuronal noise and tuning curve flattening in
response to metabolic stress. We have developed a theoretical population coding
framework that captures this behavior using two novel, surprisingly simple
constraints: an approximation of firing rate homeostasis and an energy limit
tied to noise levels via biophysical simulation. A key feature of our
contribution is an energy budget model directly connecting adenosine
triphosphate (ATP) use in cells to a fully explainable mathematical framework
that generalizes existing optimal population codes. Specifically, our
simulation provides an energy-dependent dispersed Poisson noise model, based on
the assumption that the cell will follow an optimal decay path to produce the
least-noisy spike rate that is possible at a given cellular energy budget. Each
state along this optimal path is associated with properties (resting potential
and leak conductance) which can be measured in electrophysiology experiments
and have been shown to change under prolonged caloric deprivation. We
analytically derive the optimal coding strategy for neurons under varying
energy budgets and coding goals, and show how our method uniquely captures how
populations of tuning curves adapt while maintaining homeostasis, as has been
observed empirically.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [129] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: The paper discusses propositional equivalence in programming, focusing on recent advancements in Guarded Kleene Algebra with Tests (G-KAT).


<details>
  <summary>Details</summary>
Motivation: General program equivalence is undecidable in its complete semantics, posing challenges in comparing programs effectively.

Method: The study employs abstractions using Guarded Kleene Algebra with Tests (G-KAT) to analyze propositional equivalence in programs.

Result: Propositional equivalence, which abstracts semantics, is shown to be decidable and feasible.

Conclusion: By using G-KAT, propositional program equivalence becomes tractable, offering insights into program analysis independent of detailed semantics.

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [130] [g2o vs. Ceres: Optimizing Scan Matching in Cartographer SLAM](https://arxiv.org/abs/2507.07142)
*Quanjie Qiu,MengCheng Lau*

Main category: cs.RO

TL;DR: This paper evaluates and compares g2o and Ceres solvers for SLAM performance in the Cartographer framework, demonstrating Ceres as superior in speed and map accuracy, while g2o excels in obstacle detection.


<details>
  <summary>Details</summary>
Motivation: The study aims to determine the relative strengths and weaknesses of g2o and Ceres solvers within the SLAM-focused Cartographer framework to improve mapping performance.

Method: Comparative experiments were conducted using the AgileX LIMO robot in real-world environments, measuring the speed, convergence efficiency, and mapping clarity of g2o versus Ceres solvers.

Result: Ceres outperformed g2o in speed, efficiency of convergence, and map accuracy. Conversely, g2o performed better in detecting localized obstacles.

Conclusion: Despite g2o’s strengths in specific tasks, Ceres is the more efficient and accurate choice for most SLAM mapping scenarios.

Abstract: This article presents a comparative analysis of g2o and Ceres solvers in
enhancing scan matching performance within the Cartographer framework.
Cartographer, a widely-used library for Simultaneous Localization and Mapping
(SLAM), relies on optimization algorithms to refine pose estimates and improve
map accuracy. The research aims to evaluate the performance, efficiency, and
accuracy of the g2o solver in comparison to the Ceres solver, which is the
default in Cartographer. In our experiments comparing Ceres and g2o within
Cartographer, Ceres outperformed g2o in terms of speed, convergence efficiency,
and overall map clarity. Ceres required fewer iterations and less time to
converge, producing more accurate and well-defined maps, especially in
real-world mapping scenarios with the AgileX LIMO robot. However, g2o excelled
in localized obstacle detection, highlighting its value in specific situations.

</details>


### [131] [Self-Wearing Adaptive Garments via Soft Robotic Unfurling](https://arxiv.org/abs/2507.07221)
*Nam Gyun Kim,William E. Heap,Yimeng Qin,Elvy B. Yao,Jee-Hwan Ryu,Allison M. Okamura*

Main category: cs.RO

TL;DR: The paper introduces the Self-Wearing Adaptive Garment (SWAG), a soft robotic dressing system that improves safety and efficiency in dressing assistance for individuals with limited mobility.


<details>
  <summary>Details</summary>
Motivation: To address challenges like excessive operation time, complex control strategies, and constrained user postures faced by existing robotic dressing systems using rigid manipulators.

Method: Developed the SWAG system that utilizes an unfurling and growth mechanism for autonomous dressing, eliminating skin-garment friction and adapting to human body contours.

Result: Demonstrated effective dressing assistance across various garment configurations, highlighting efficiency and adaptability in practice.

Conclusion: SWAG offers a promising alternative to conventional robotic dressing systems with improved safety, efficiency, and adaptability.

Abstract: Robotic dressing assistance has the potential to improve the quality of life
for individuals with limited mobility. Existing solutions predominantly rely on
rigid robotic manipulators, which have challenges in handling deformable
garments and ensuring safe physical interaction with the human body. Prior
robotic dressing methods require excessive operation times, complex control
strategies, and constrained user postures, limiting their practicality and
adaptability. This paper proposes a novel soft robotic dressing system, the
Self-Wearing Adaptive Garment (SWAG), which uses an unfurling and growth
mechanism to facilitate autonomous dressing. Unlike traditional approaches,the
SWAG conforms to the human body through an unfurling based deployment method,
eliminating skin-garment friction and enabling a safer and more efficient
dressing process. We present the working principles of the SWAG, introduce its
design and fabrication, and demonstrate its performance in dressing assistance.
The proposed system demonstrates effective garment application across various
garment configurations, presenting a promising alternative to conventional
robotic dressing assistance.

</details>


### [132] [3D Steering and Localization in Pipes and Burrows using an Externally Steered Soft Growing Robot](https://arxiv.org/abs/2507.07225)
*Yimeng Qin,Jared Grinberg,William Heap,Allison M. Okamura*

Main category: cs.RO

TL;DR: This paper introduces a steerable vine robot capable of navigating confined environments, such as pipes and burrows, with adaptability to sharp turns and complex paths.


<details>
  <summary>Details</summary>
Motivation: Existing robots struggle with navigating confined spaces like tunnels and pipes, especially those with sharp turns and branches, limiting their usability in such applications.

Method: The paper proposes a vine robot equipped with a tubular body and an external tip mount for steering. This setup allows for 3 degrees of freedom in maneuverability, enabling real-time 3D localization and sharp turns.

Result: The steerable vine robot achieved active branch selection in 3D space, navigation in pipes with radii as small as 2.5 cm, and sharp turns. It also featured real-time localization in GPS-denied environments.

Conclusion: This steerable vine robot proves effective for confined spaces, offering significant advancements in adaptability and functionality compared to traditional robotic designs.

Abstract: Navigation and inspection in confined environments, such as tunnels and
pipes, pose significant challenges for existing robots due to limitations in
maneuverability and adaptability to varying geometries. Vine robots, which are
soft growing continuum robots that extend their length through soft material
eversion at their tip, offer unique advantages due to their ability to navigate
tight spaces, adapt to complex paths, and minimize friction. However, existing
vine robot designs struggle with navigation in manmade and natural passageways,
with branches and sharp 3D turns. In this letter, we introduce a steerable vine
robot specifically designed for pipe and burrow environments. The robot
features a simple tubular body and an external tip mount that steers the vine
robot in three degrees of freedom by changing the growth direction and, when
necessary, bracing against the wall of the pipe or burrow. Our external tip
steering approach enables: (1) active branch selection in 3D space with a
maximum steerable angle of 51.7{\deg}, (2) navigation of pipe networks with
radii as small as 2.5 cm, (3) a compliant tip enabling navigation of sharp
turns, and (4) real-time 3D localization in GPS-denied environments using
tip-mounted sensors and continuum body odometry. We describe the forward
kinematics, characterize steerability, and demonstrate the system in a 3D pipe
system as well as a natural animal burrow.

</details>


### [133] [LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation](https://arxiv.org/abs/2507.07299)
*Sonia Raychaudhuri,Enrico Cancelli,Tommaso Campari,Lamberto Ballan,Manolis Savva,Angel X. Chang*

Main category: cs.RO

TL;DR: LangNav is a detailed benchmark dataset designed to evaluate how well vision-language models can navigate through semantic instructions involving object descriptions ranging from categories to spatial relations. A new method called Multi-Layered Feature Map (MLFM) is introduced, which outperforms existing navigation methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to fill the gap in language-focused evaluation frameworks for assessing how well embodied navigation agents ground natural language instructions.

Method: Introduced LangNav, an open-set dataset with manually checked descriptions for object identification, and LangNavBench, a corresponding benchmark for semantic navigation evaluation. Also proposed MLFM, a mapping-based navigation method designed to handle complex object descriptions and relations.

Result: LangNavBench provides a rigorous language-centric evaluation, and MLFM surpasses state-of-the-art baselines, especially in tasks involving small objects and spatial relations.

Conclusion: The work offers a comprehensive dataset and benchmark for evaluating language grounding in navigation tasks and proposes an effective method for improving navigation based on semantic instructions.

Abstract: Recent progress in large vision-language models has driven improvements in
language-based semantic navigation, where an embodied agent must reach a target
object described in natural language. Despite these advances, we still lack a
clear, language-focused benchmark for testing how well such agents ground the
words in their instructions. We address this gap with LangNav, an open-set
dataset specifically created to test an agent's ability to locate objects
described at different levels of detail, from broad category names to fine
attributes and object-object relations. Every description in LangNav was
manually checked, yielding a lower error rate than existing lifelong- and
semantic-navigation datasets. On top of LangNav we build LangNavBench, a
benchmark that measures how well current semantic-navigation methods understand
and act on these descriptions while moving toward their targets. LangNavBench
allows us to systematically compare models on their handling of attributes,
spatial and relational cues, and category hierarchies, offering the first
thorough, language-centric evaluation of embodied navigation systems. We also
present Multi-Layered Feature Map (MLFM), a method that builds a queryable
multi-layered semantic map, particularly effective when dealing with small
objects or instructions involving spatial relations. MLFM outperforms
state-of-the-art mapping-based navigation baselines on the LangNav dataset.

</details>


### [134] [Classifying Emergence in Robot Swarms: An Observer-Dependent Approach](https://arxiv.org/abs/2507.07315)
*Ricardo Vega,Cameron Nowzari*

Main category: cs.RO

TL;DR: The paper examines the concepts of emergence and swarms, proposing a framework to discuss them rigorously by distinguishing observable and latent states, ultimately arguing these concepts are subjective and shaped by observers.


<details>
  <summary>Details</summary>
Motivation: The lack of agreed-upon definitions for 'swarm' and 'emergence' creates confusion both for new researchers and for experts using these terms differently. A rigorous framework is needed to support understanding and robotic swarm system design.

Method: The authors propose a framework that separates externally observable states from latent, unobservable ones to rigorously compare and review existing definitions of swarms and emergence.

Result: The paper argues that swarms and emergence are subjective concepts shaped by the observer's perception rather than the intrinsic system properties. It highlights that a swarm's behavior is defined by the process generating it, not the behavior itself.

Conclusion: The study underscores that clear differentiation is crucial for understanding swarms and emergence, supporting precise communication for the design and deployment of robotic swarm systems.

Abstract: Emergence and swarms are widely discussed topics, yet no consensus exists on
their formal definitions. This lack of agreement makes it difficult not only
for new researchers to grasp these concepts, but also for experts who may use
the same terms to mean different things. Many attempts have been made to
objectively define 'swarm' or 'emergence,' with recent work highlighting the
role of the external observer. Still, several researchers argue that once an
observer's vantage point (e.g., scope, resolution, context) is established, the
terms can be made objective or measured quantitatively. In this note, we
propose a framework to discuss these ideas rigorously by separating externally
observable states from latent, unobservable ones. This allows us to compare and
contrast existing definitions of swarms and emergence on common ground. We
argue that these concepts are ultimately subjective-shaped less by the system
itself than by the perception and tacit knowledge of the observer.
Specifically, we suggest that a 'swarm' is not defined by its group behavior
alone, but by the process generating that behavior. Our broader goal is to
support the design and deployment of robotic swarm systems, highlighting the
critical distinction between multi-robot systems and true swarms.

</details>


### [135] [Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task](https://arxiv.org/abs/2507.07327)
*Brian B. Vuong,Josie Davidson,Sangheui Cheon,Kyujin Cho,Allison M. Okamura*

Main category: cs.RO

TL;DR: This study explored relocating haptic feedback to the wrist to improve force application in robotic surgeries, showing improved accuracy despite altered speed-accuracy tradeoffs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of occlusion and interruption caused by hand-based haptic feedback mechanisms for improving teleoperation task accuracy and performance in robot-assisted surgeries.

Method: Participants used the da Vinci Research Kit (dVRK) surgical robot to perform tissue palpation tasks under two conditions: with and without a wrist-worn haptic device. The device communicated tool-tissue interaction forces via soft pneumatic technology.

Result: Statistically significant lower force errors were observed with wrist-worn haptic feedback, indicating improved accuracy. However, movement times increased, suggesting participants adjusted to a different speed-accuracy tradeoff to accomplish the tasks.

Conclusion: Wrist-worn haptic feedback enhances accuracy in teleoperated surgical tasks, but it may influence speed as users shift along the speed-accuracy tradeoff curve. Relocating feedback to the wrist appears effective for improving surgical performance.

Abstract: Previous work has shown that the addition of haptic feedback to the hands can
improve awareness of tool-tissue interactions and enhance performance of
teleoperated tasks in robot-assisted minimally invasive surgery. However,
hand-based haptic feedback occludes direct interaction with the manipulanda of
surgeon console in teleoperated surgical robots. We propose relocating haptic
feedback to the wrist using a wearable haptic device so that haptic feedback
mechanisms do not need to be integrated into the manipulanda. However, it is
unknown if such feedback will be effective, given that it is not co-located
with the finger movements used for manipulation. To test if relocated haptic
feedback improves force application during teleoperated tasks using da Vinci
Research Kit (dVRK) surgical robot, participants learned to palpate a phantom
tissue to desired forces. A soft pneumatic wrist-worn haptic device with an
anchoring system renders tool-tissue interaction forces to the wrist of the
user. Participants performed the palpation task with and without wrist-worn
haptic feedback and were evaluated for the accuracy of applied forces.
Participants demonstrated statistically significant lower force error when
wrist-worn haptic feedback was provided. Participants also performed the
palpation task with longer movement times when provided wrist-worn haptic
feedback, indicating that the haptic feedback may have caused participants to
operate at a different point in the speed-accuracy tradeoff curve.

</details>


### [136] [UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots](https://arxiv.org/abs/2507.07356)
*Kangning Yin,Weishuai Zeng,Ke Fan,Zirui Wang,Qiang Zhang,Zheng Tian,Jingbo Wang,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: The paper introduces UniTracker, a new framework that uses a Conditional Variational Autoencoder (CVAE) to enhance whole-body control in humanoid robots.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving diverse, robust, and generalizable control in humanoid robots for operation in complex environments, as existing methods lose motion diversity and have limited behavior generalization.

Method: The authors integrate a CVAE into the student policy to model latent motion diversity, allowing expressive characteristics while maintaining robustness and adaptability even under partial observations.

Result: UniTracker delivers high fidelity and stability in whole-body motion tracking, outperforming traditional MLP-based DAgger baselines in simulation and real-world settings.

Conclusion: This framework offers a practical and scalable solution for expressive and generalizable humanoid robot control, with significant advancements over existing baselines.

Abstract: Humanoid robots must achieve diverse, robust, and generalizable whole-body
control to operate effectively in complex, human-centric environments. However,
existing methods, particularly those based on teacher-student frameworks often
suffer from a loss of motion diversity during policy distillation and exhibit
limited generalization to unseen behaviors. In this work, we present
UniTracker, a simplified yet powerful framework that integrates a Conditional
Variational Autoencoder (CVAE) into the student policy to explicitly model the
latent diversity of human motion. By leveraging a learned CVAE prior, our
method enables the student to retain expressive motion characteristics while
improving robustness and adaptability under partial observations. The result is
a single policy capable of tracking a wide spectrum of whole-body motions with
high fidelity and stability. Comprehensive experiments in both simulation and
real-world deployments demonstrate that UniTracker significantly outperforms
MLP-based DAgger baselines in motion quality, generalization to unseen
references, and deployment robustness, offering a practical and scalable
solution for expressive humanoid control.

</details>


### [137] [Data-driven Kinematic Modeling in Soft Robots: System Identification and Uncertainty Quantification](https://arxiv.org/abs/2507.07370)
*Zhanhong Jiang,Dylan Shah,Hsin-Jung Yang,Soumik Sarkar*

Main category: cs.RO

TL;DR: The paper addresses the challenge of precise kinematic modeling for soft robots by exploring machine learning methods and introducing a conformal prediction framework for uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the difficulty in precise kinematic modeling of soft robots, which arises from their nonlinear and complex behavior, and to address the lack of uncertainty quantification in current approaches.

Method: The authors investigate various linear and nonlinear machine learning models for kinematic modeling, evaluate their generalization performance, and develop a conformal prediction framework to quantify uncertainty in predictive positions.

Result: Nonlinear ensemble methods were found to perform most robustly, and the proposed conformal kinematic modeling framework provides theoretical guarantees for distribution-free prediction intervals.

Conclusion: The study contributes to soft robotics by improving modeling accuracy and incorporating uncertainty quantification, which could aid in better calibration and controller design.

Abstract: Precise kinematic modeling is critical in calibration and controller design
for soft robots, yet remains a challenging issue due to their highly nonlinear
and complex behaviors. To tackle the issue, numerous data-driven machine
learning approaches have been proposed for modeling nonlinear dynamics.
However, these models suffer from prediction uncertainty that can negatively
affect modeling accuracy, and uncertainty quantification for kinematic modeling
in soft robots is underexplored. In this work, using limited simulation and
real-world data, we first investigate multiple linear and nonlinear machine
learning models commonly used for kinematic modeling of soft robots. The
results reveal that nonlinear ensemble methods exhibit the most robust
generalization performance. We then develop a conformal kinematic modeling
framework for soft robots by utilizing split conformal prediction to quantify
predictive position uncertainty, ensuring distribution-free prediction
intervals with a theoretical guarantee.

</details>


### [138] [PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments](https://arxiv.org/abs/2507.07376)
*Hengrui Liu,Yi Feng,Qilong Zhang*

Main category: cs.RO

TL;DR: The paper introduces PILOC, a framework for multi-agent search and rescue (MASAR) that uses local perceptions, communication, and a novel pheromone inverse guidance mechanism to improve search efficiency and coordination.


<details>
  <summary>Details</summary>
Motivation: Dynamic and unpredictable environments create challenges in multi-agent search and rescue operations, necessitating strategies adapted to uncertainty and limited knowledge.

Method: The study introduces PILOC, which integrates deep reinforcement learning with a pheromone inverse guidance mechanism for indirect coordination and local communication to reduce reliance on global knowledge.

Result: Extensive experiments demonstrate that PILOC improves search efficiency, adaptability, and robustness, outperforming existing methods in dynamic and communication-constrained scenarios.

Conclusion: PILOC offers a promising approach for future MASAR applications by enhancing decentralized cooperation and dynamic target discovery using innovative coordination mechanisms.

Abstract: Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster
response, exploration, and reconnaissance. However, dynamic and unknown
environments pose significant challenges due to target unpredictability and
environmental uncertainty. To tackle these issues, we propose PILOC, a
framework that operates without global prior knowledge, leveraging local
perception and communication. It introduces a pheromone inverse guidance
mechanism to enable efficient coordination and dynamic target localization.
PILOC promotes decentralized cooperation through local communication,
significantly reducing reliance on global channels. Unlike conventional
heuristics, the pheromone mechanism is embedded into the observation space of
Deep Reinforcement Learning (DRL), supporting indirect agent coordination based
on environmental cues. We further integrate this strategy into a DRL-based
multi-agent architecture and conduct extensive experiments. Results show that
combining local communication with pheromone-based guidance significantly
boosts search efficiency, adaptability, and system robustness. Compared to
existing methods, PILOC performs better under dynamic and
communication-constrained scenarios, offering promising directions for future
MASAR applications.

</details>


### [139] [Towards Safe Autonomous Driving: A Real-Time Safeguarding Concept for Motion Planning Algorithms](https://arxiv.org/abs/2507.07444)
*Korbinian Moller,Rafael Neher,Marvin Seegert,Johannes Betz*

Main category: cs.RO

TL;DR: This paper proposes a new safeguarding concept for autonomous vehicle motion planning that includes a time safeguard, enhancing temporal consistency and safety assurances.


<details>
  <summary>Details</summary>
Motivation: To address challenges in ensuring functional safety of autonomous vehicle motion planning systems, especially in real-time embedded environments, and limited integration of online verification methods.

Method: Introduced a time safeguard to monitor planning outputs' temporal consistency, implemented as a modular framework on a real-time operating system with feasibility checks and cost-based plausibility metrics.

Result: The safeguarding module operates within real-time limits and detects unsafe trajectories effectively. Full integration and fallback strategies are still under development.

Conclusion: This work advances runtime trajectory verification by adding temporal safeguards, providing a foundation for further system testing and deployment on automotive-grade hardware.

Abstract: Ensuring the functional safety of motion planning modules in autonomous
vehicles remains a critical challenge, especially when dealing with complex or
learning-based software. Online verification has emerged as a promising
approach to monitor such systems at runtime, yet its integration into embedded
real-time environments remains limited. This work presents a safeguarding
concept for motion planning that extends prior approaches by introducing a time
safeguard. While existing methods focus on geometric and dynamic feasibility,
our approach additionally monitors the temporal consistency of planning outputs
to ensure timely system response. A prototypical implementation on a real-time
operating system evaluates trajectory candidates using constraint-based
feasibility checks and cost-based plausibility metrics. Preliminary results
show that the safeguarding module operates within real-time bounds and
effectively detects unsafe trajectories. However, the full integration of the
time safeguard logic and fallback strategies is ongoing. This study contributes
a modular and extensible framework for runtime trajectory verification and
highlights key aspects for deployment on automotive-grade hardware. Future work
includes completing the safeguarding logic and validating its effectiveness
through hardware-in-the-loop simulations and vehicle-based testing. The code is
available at: https://github.com/TUM-AVS/motion-planning-supervisor

</details>


### [140] [SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation](https://arxiv.org/abs/2507.07467)
*Juyeop Han,Lukas Lao Beyer,Guilherme V. Cavalheiro,Sertac Karaman*

Main category: cs.RO

TL;DR: The paper introduces a framework combining scene coordinate regression (SCR) for high-accuracy pose estimation and trajectory optimization for perception-aware indoor autonomous flight.


<details>
  <summary>Details</summary>
Motivation: To address drift issues in visual inertial odometry (VIO) and ensure bounded visual localization error for autonomous flight in GPS-denied indoor spaces.

Method: Couples evidential learning-based SCR for pose estimation with receding horizon trajectory optimization steering a camera towards reliable pixels, supported by a fixed-lag smoother integrating SCR and IMU data.

Result: Simulation results show significant error reduction in translation (54%, 15%) and rotation (40%, 31%) compared to baselines. Hardware-in-the-loop validates feasibility.

Conclusion: The perception-aware framework reliably boosts localization accuracy and is feasible for real-time use in autonomous indoor navigation.

Abstract: Autonomous flight in GPS denied indoor spaces requires trajectories that keep
visual localization error tightly bounded across varied missions. Whereas
visual inertial odometry (VIO) accumulates drift over time, scene coordinate
regression (SCR) yields drift-free, high accuracy absolute pose estimation. We
present a perception-aware framework that couples an evidential learning-based
SCR pose estimator with a receding horizon trajectory optimizer. The optimizer
steers the onboard camera toward pixels whose uncertainty predicts reliable
scene coordinates, while a fixed-lag smoother fuses the low rate SCR stream
with high rate IMU data to close the perception control loop in real time. In
simulation, our planner reduces translation (rotation) mean error by 54% / 15%
(40% / 31%) relative to yaw fixed and forward-looking baselines, respectively.
Moreover, hardware in the loop experiment validates the feasibility of our
proposed framework.

</details>


### [141] [A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation](https://arxiv.org/abs/1602.03104)
*Ayan Dutta,Prithviraj Dasgupta,Carl Nelson*

Main category: cs.RO

TL;DR: This paper proposes a graph-isomorphism-based algorithm to optimize modular robot system reconfiguration, emphasizing Pareto-optimal allocations and reduced time/energy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of efficiently reconfiguring modular robot systems into a user-specified target configuration, while minimizing time and energy utilization.

Method: The authors propose a novel algorithm utilizing graph isomorphism and a utility-based framework that prioritizes modules retaining their original configuration as much as possible. Analytical proofs of completeness and Pareto-optimality are provided.

Result: The algorithm performs quickly (milliseconds for 100 modules) and outperforms a market-based alternative in terms of planning time and message complexity under experimental conditions.

Conclusion: The proposed algorithm offers an effective, efficient, and analytically sound solution for modular robot reconfiguration, outperforming existing approaches in key metrics.

Abstract: We consider the problem of configuration formation in modular robot systems
where a set of modules that are initially in different configurations and
located at different locations are required to assume appropriate positions so
that they can get into a new, user-specified, target configuration. We propose
a novel algorithm based on graph isomorphism, where the modules select
locations or spots in the target configuration using a utility-based framework,
while retaining their original configuration to the greatest extent possible,
to reduce the time and energy required by the modules to assume the target
configuration. We have shown analytically that our proposed algorithm is
complete and guarantees a Pareto-optimal allocation. Experimental simulations
of our algorithm with different number of modules in different initial
configurations and located initially at different locations, show that the
planning time of our algorithm is nominal (order of msec. for 100 modules). We
have also compared our algorithm against a market-based allocation algorithm
and shown that our proposed algorithm performs better in terms of time and
number of messages exchanged.

</details>


### [142] [FiDTouch: A 3D Wearable Haptic Display for the Finger Pad](https://arxiv.org/abs/2507.07661)
*Daria Trinitatova,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FiDTouch is a wearable haptic device designed to enhance tactile feedback on the fingertip, aimed at improving user interaction in various fields like virtual reality and robotics.


<details>
  <summary>Details</summary>
Motivation: To improve tactile feedback and user experiences in diverse fields such as virtual reality, medical training, and human-robot interaction.

Method: Development of FiDTouch, a 3D wearable haptic device utilizing an inverted Delta robot mechanism for delivering precise and dynamic feedback to the fingertip.

Result: Evaluation of FiDTouch in a user study demonstrated its ability to deliver accurate and dynamic tactile stimuli like skin stretch and spatial contact.

Conclusion: FiDTouch shows promise in enhancing user immersion and efficiency, benefiting applications in virtual reality, medical training, and human-computer interaction.

Abstract: The applications of fingertip haptic devices have spread to various fields
from revolutionizing virtual reality and medical training simulations to
facilitating remote robotic operations, proposing great potential for enhancing
user experiences, improving training outcomes, and new forms of interaction. In
this work, we present FiDTouch, a 3D wearable haptic device that delivers
cutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin
stretch, and vibrotactile feedback. The application of a tiny inverted Delta
robot in the mechanism design allows providing accurate contact and fast
changing dynamic stimuli to the finger pad surface. The performance of the
developed display was evaluated in a two-stage user study of the perception of
static spatial contact stimuli and skin stretch stimuli generated on the finger
pad. The proposed display, by providing users with precise touch and force
stimuli, can enhance user immersion and efficiency in the fields of
human-computer and human-robot interactions.

</details>


### [143] [Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots](https://arxiv.org/abs/2507.07714)
*Julio Garrido,Javier Vales,Diego Silva-Muñiz,Enrique Riveiro,Pablo López-Matencio,Josué Rivera-Andrade*

Main category: cs.RO

TL;DR: This paper focuses on using motor torque data and an adaptive Gaussian Mixture Model for real-time anomaly detection in Cable-Driven Parallel Robots (CDPRs), achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To ensure the safety and performance of CDPRs during load manipulation tasks by detecting anomalies such as wind gusts or cable impacts, using only motor torque data and avoiding additional sensors.

Method: An adaptive, unsupervised outlier detection algorithm based on Gaussian Mixture Models (GMMs) is used. A brief calibration period trains the model on anomaly-free data, after which real-time torque data is evaluated using Mahalanobis distance and statistically-derived thresholds. Periodic model updates adapt to changing conditions.

Result: The method achieved a 100% true positive rate and a 95.4% true negative rate on 14 long-duration test sessions, with a 1-second detection latency. It showed higher robustness compared to power threshold and non-adaptive GMM methods.

Conclusion: The proposed GMM-based anomaly detection method is effective, adaptive, and robust for detecting anomalies in CDPRs, providing a reliable solution without additional sensors.

Abstract: Cable-Driven Parallel Robots (CDPRs) are increasingly used for load
manipulation tasks involving predefined toolpaths with intermediate stops. At
each stop, where the platform maintains a fixed pose and the motors keep the
cables under tension, the system must evaluate whether it is safe to proceed by
detecting anomalies that could compromise performance (e.g., wind gusts or
cable impacts). This paper investigates whether anomalies can be detected using
only motor torque data, without additional sensors. It introduces an adaptive,
unsupervised outlier detection algorithm based on Gaussian Mixture Models
(GMMs) to identify anomalies from torque signals. The method starts with a
brief calibration period, just a few seconds, during which a GMM is fit on
known anomaly-free data. Real-time torque measurements are then evaluated using
Mahalanobis distance from the GMM, with statistically derived thresholds
triggering anomaly flags. Model parameters are periodically updated using the
latest segments identified as anomaly-free to adapt to changing conditions.
Validation includes 14 long-duration test sessions simulating varied wind
intensities. The proposed method achieves a 100% true positive rate and 95.4%
average true negative rate, with 1-second detection latency. Comparative
evaluation against power threshold and non-adaptive GMM methods indicates
higher robustness to drift and environmental variation.

</details>


### [144] [Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics](https://arxiv.org/abs/2507.07718)
*Alberto Rota,Ke Fan,Elena De Momi*

Main category: cs.RO

TL;DR: The study develops a haptic-based VR simulator for surgical robotics training, demonstrating its effectiveness in improving performance and skill transfer.


<details>
  <summary>Details</summary>
Motivation: To improve the clinical performance of aspiring surgeons by creating a comprehensive and robust training approach through advancements in surgical robotics training.

Method: A haptic-enhanced VR simulator was developed with 8 surgical tasks integrated with a physics engine. High-level haptic interfaces assist in guiding hand/wrist motions and provide performance scores.

Result: The use of this advanced simulator in training showed improved performance in practice, with increased skill transfer to unassisted, real-world surgical scenarios.

Conclusion: Integrating haptic-enhanced robotic assistance in surgical training fosters better skill acquisition and transition to clinical practice.

Abstract: The integration of high-level assistance algorithms in surgical robotics
training curricula may be beneficial in establishing a more comprehensive and
robust skillset for aspiring surgeons, improving their clinical performance as
a consequence. This work presents the development and validation of a
haptic-enhanced Virtual Reality simulator for surgical robotics training,
featuring 8 surgical tasks that the trainee can interact with thanks to the
embedded physics engine. This virtual simulated environment is augmented by the
introduction of high-level haptic interfaces for robotic assistance that aim at
re-directing the motion of the trainee's hands and wrists toward targets or
away from obstacles, and providing a quantitative performance score after the
execution of each training exercise.An experimental study shows that the
introduction of enhanced robotic assistance into a surgical robotics training
curriculum improves performance during the training process and, crucially,
promotes the transfer of the acquired skills to an unassisted surgical
scenario, like the clinical one.

</details>


### [145] [Distributed Surface Inspection via Operational Modal Analysis by a Swarm of Miniaturized Vibration-Sensing Robots](https://arxiv.org/abs/2507.07724)
*Thiemen Siemensma,Niels de Boer,Bahar Haghighat*

Main category: cs.RO

TL;DR: This paper proposes using a swarm of mini vibration-sensing robots for structural inspection and damage localization, validated through simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional static sensor networks are limited in structural coverage for applications like structural monitoring. Deploying a swarm of mobile, vibration-sensing robots could overcome these limitations.

Method: The study uses finite element analysis to simulate realistic vibration data of a steel surface and integrates it into a robotic simulator. It deploys Gaussian process estimators for guiding exploration and operational modal analysis for damage detection.

Result: Simulation studies across 10 randomized scenarios demonstrate that the swarm is effective in gathering vibration samples and detecting structural damage across varied conditions.

Conclusion: The approach validates the potential of miniaturized robot swarms for cost-effective and accurate vibration-based structural inspections.

Abstract: Robot swarms offer the potential to serve a variety of distributed sensing
applications. An interesting real-world application that stands to benefit
significantly from deployment of swarms is structural monitoring, where
traditional sensor networks face challenges in structural coverage due to their
static nature. This paper investigates the deployment of a swarm of
miniaturized vibration sensing robots to inspect and localize structural
damages on a surface section within a high-fidelity simulation environment. In
particular, we consider a 1 m x 1 m x 3 mm steel surface section and utilize
finite element analysis using Abaqus to obtain realistic structural vibration
data. The resulting vibration data is imported into the physics-based robotic
simulator Webots, where we simulate the dynamics of our surface inspecting
robot swarm. We employ (i) Gaussian process estimators to guide the robots'
exploration as they collect vibration samples across the surface and (ii)
operational modal analysis to detect structural damages by estimating and
comparing existing and intact structural vibration patterns. We analyze the
influence of exploration radii on estimation uncertainty and assess the
effectiveness of our method across 10 randomized scenarios, where the number,
locations, surface area, and depth of structural damages vary. Our simulation
studies validate the efficacy of our miniaturized robot swarm for
vibration-based structural inspection.

</details>


### [146] [On the capabilities of LLMs for classifying and segmenting time series of fruit picking motions into primitive actions](https://arxiv.org/abs/2507.07745)
*Eleni Konstantinidou,Nikolaos Kounalakis,Nikolaos Efstathopoulos,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: This paper studies how Large Language Models (LLMs) can classify and segment complex motions into primitive actions, specifically in fruit-picking operations, using fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of LLMs in simplifying Learning by Demonstration (LbD) by classifying complex motions for real-life applications, like fruit picking.

Method: The authors use three fine-tuning approaches applied to datasets collected from a UR10e robot during a fruit-picking scenario to evaluate LLMs' capabilities.

Result: The study compares the efficiency and applicability of different fine-tuned LLM approaches in segmenting and classifying primitive actions for fruit-picking.

Conclusion: LLMs show potential for practical deployment in tasks requiring motion classification and segmentation, making them advantageous for real-world uses in areas like robotics.

Abstract: Despite their recent introduction to human society, Large Language Models
(LLMs) have significantly affected the way we tackle mental challenges in our
everyday lives. From optimizing our linguistic communication to assisting us in
making important decisions, LLMs, such as ChatGPT, are notably reducing our
cognitive load by gradually taking on an increasing share of our mental
activities. In the context of Learning by Demonstration (LbD), classifying and
segmenting complex motions into primitive actions, such as pushing, pulling,
twisting etc, is considered to be a key-step towards encoding a task. In this
work, we investigate the capabilities of LLMs to undertake this task,
considering a finite set of predefined primitive actions found in fruit picking
operations. By utilizing LLMs instead of simple supervised learning or analytic
methods, we aim at making the method easily applicable and deployable in a
real-life scenario. Three different fine-tuning approaches are investigated,
compared on datasets captured kinesthetically, using a UR10e robot, during a
fruit-picking scenario.

</details>


### [147] [IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End for Visual SLAM in Challenging Environments](https://arxiv.org/abs/2507.07752)
*Thanh Nguyen Canh,Bao Nguyen Quoc,Haolan Zhang,Bupesh Rethinam Veeraiah,Xiem HoangVan,Nak Young Chong*

Main category: cs.RO

TL;DR: IRAF-SLAM is a vSLAM system that enhances performance under challenging environmental conditions using adaptive front-end strategies.


<details>
  <summary>Details</summary>
Motivation: vSLAM systems often fail in real-world scenarios with dynamic objects, low texture, and illumination changes. The authors aim to address the vulnerability of existing vSLAM methods to these challenges.

Method: The paper introduces IRAF-SLAM, which combines image enhancement, adaptive feature extraction, and adaptive feature culling to optimize tracking and mapping under diverse and changing lighting conditions.

Result: Experimental evaluations show that IRAF-SLAM significantly reduces tracking failures and achieves better trajectory accuracy than existing approaches across TUM-VI and EuRoC datasets.

Conclusion: IRAF-SLAM improves vSLAM robustness to illumination changes without adding significant computational load, showing the value of its adaptive techniques for real-world scenarios.

Abstract: Robust Visual SLAM (vSLAM) is essential for autonomous systems operating in
real-world environments, where challenges such as dynamic objects, low texture,
and critically, varying illumination conditions often degrade performance.
Existing feature-based SLAM systems rely on fixed front-end parameters, making
them vulnerable to sudden lighting changes and unstable feature tracking. To
address these challenges, we propose ``IRAF-SLAM'', an Illumination-Robust and
Adaptive Feature-Culling front-end designed to enhance vSLAM resilience in
complex and challenging environments. Our approach introduces: (1) an image
enhancement scheme to preprocess and adjust image quality under varying
lighting conditions; (2) an adaptive feature extraction mechanism that
dynamically adjusts detection sensitivity based on image entropy, pixel
intensity, and gradient analysis; and (3) a feature culling strategy that
filters out unreliable feature points using density distribution analysis and a
lighting impact factor. Comprehensive evaluations on the TUM-VI and European
Robotics Challenge (EuRoC) datasets demonstrate that IRAF-SLAM significantly
reduces tracking failures and achieves superior trajectory accuracy compared to
state-of-the-art vSLAM methods under adverse illumination conditions. These
results highlight the effectiveness of adaptive front-end strategies in
improving vSLAM robustness without incurring significant computational
overhead. The implementation of IRAF-SLAM is publicly available at
https://thanhnguyencanh. github.io/IRAF-SLAM/.

</details>


### [148] [Collaborative Human-Robot Surgery for Mandibular Angle Split Osteotomy: Optical Tracking based Approach](https://arxiv.org/abs/2507.07794)
*Zhe Han,Huanyu Tian,Tom Vercauteren,Da Liu,Changsheng Li,Xingguang Duan*

Main category: cs.RO

TL;DR: The paper proposes a human-robot collaborative system for performing Mandibular Angle Split Osteotomy (MASO) with high precision.


<details>
  <summary>Details</summary>
Motivation: To enhance the precision and safety of MASO while reducing dependence on surgeon experience and the need for invasive stabilization methods like skull clamps.

Method: The approach combines task decomposition (robot-led positional and orientation control, and surgeon-led force control) with an optical tracking system that uses a dental occlusal splint for non-invasive patient movement tracking. A calibration and registration method ensures precise surgical navigation.

Result: Drilling experiments on a realistic phantom model achieved an average error of 1.85mm between the planned and actual points, demonstrating high precision.

Conclusion: The proposed system shows promise in enhancing surgical accuracy in MASO while addressing limitations of traditional techniques, making it safer and more reliable.

Abstract: Mandibular Angle Split Osteotomy (MASO) is a significant procedure in oral
and maxillofacial surgery. Despite advances in technique and instrumentation,
its success still relies heavily on the surgeon's experience. In this work, a
human-robot collaborative system is proposed to perform MASO according to a
preoperative plan and under guidance of a surgeon. A task decomposition
methodology is used to divide the collaborative surgical procedure into three
subtasks: (1) positional control and (2) orientation control, both led by the
robot for precise alignment; and (3) force-control, managed by surgeon to
ensure safety. Additionally, to achieve patient tracking without the need for a
skull clamp, an optical tracking system (OTS) is utilized. Movement of the
patient mandibular is measured with an optical-based tracker mounted on a
dental occlusal splint. A registration method and Robot-OTS calibration method
are introduced to achieve reliable navigation within our framework. The
experiments of drilling were conducted on the realistic phantom model, which
demonstrated that the average error between the planned and actual drilling
points is 1.85mm.

</details>


### [149] [Beyond Robustness: Learning Unknown Dynamic Load Adaptation for Quadruped Locomotion on Rough Terrain](https://arxiv.org/abs/2507.07825)
*Leixin Chang,Yuxuan Nai,Hua Chen,Liangjing Yang*

Main category: cs.RO

TL;DR: The paper focuses on enabling quadruped robots to handle unknown dynamic loads effectively by addressing challenges in modeling, sensing, and stabilizing. The authors propose an RL-based control system and validate it through simulations.


<details>
  <summary>Details</summary>
Motivation: Address challenges posed by unknown dynamic loads in quadruped robots, requiring solutions for modeling, sensing, and stabilizing loads.

Method: Introduced load characteristics modeling and integrated it with Reinforcement Learning (RL) to infer and interact with load dynamics.

Result: Demonstrated effectiveness in comparative simulations, showcasing better load resistance, stabilization, and performance on rough terrains than other methods.

Conclusion: The proposed method is an effective solution for quadruped robots handling dynamic loads, verified through sim-to-real applications and outperforming alternatives.

Abstract: Unknown dynamic load carrying is one important practical application for
quadruped robots. Such a problem is non-trivial, posing three major challenges
in quadruped locomotion control. First, how to model or represent the dynamics
of the load in a generic manner. Second, how to make the robot capture the
dynamics without any external sensing. Third, how to enable the robot to
interact with load handling the mutual effect and stabilizing the load. In this
work, we propose a general load modeling approach called load characteristics
modeling to capture the dynamics of the load. We integrate this proposed
modeling technique and leverage recent advances in Reinforcement Learning (RL)
based locomotion control to enable the robot to infer the dynamics of load
movement and interact with the load indirectly to stabilize it and realize the
sim-to-real deployment to verify its effectiveness in real scenarios. We
conduct extensive comparative simulation experiments to validate the
effectiveness and superiority of our proposed method. Results show that our
method outperforms other methods in sudden load resistance, load stabilizing
and locomotion with heavy load on rough terrain.
\href{https://leixinjonaschang.github.io/leggedloadadapt.github.io/}{Project
Page}.

</details>


### [150] [Perceptual Distortions and Autonomous Representation Learning in a Minimal Robotic System](https://arxiv.org/abs/2507.07845)
*David Warutumo,Ciira wa Maina*

Main category: cs.RO

TL;DR: This paper explores how sensory distortions impact autonomous representation learning using a minimal robotic system, showing the emergence of structured perceptual spaces aiding navigation.


<details>
  <summary>Details</summary>
Motivation: Understanding how sensory imperfections impact perception and navigation in autonomous agents, and uncovering emergent representational structures, improves knowledge in robotics and artificial life domains.

Method: A simulated two-wheeled robot with distance sensors and a compass navigates a square environment. Perceptual distortions are analyzed based on sensor data during random exploration.

Result: Despite distorted sensory data, emergent structures in perceptual space correlated with the physical environment, facilitating autonomous navigation.

Conclusion: Autonomous agents can develop structured representations that aid their navigation despite perceptual distortions, advancing embodied cognition and artificial life research.

Abstract: Autonomous agents, particularly in the field of robotics, rely on sensory
information to perceive and navigate their environment. However, these sensory
inputs are often imperfect, leading to distortions in the agent's internal
representation of the world. This paper investigates the nature of these
perceptual distortions and how they influence autonomous representation
learning using a minimal robotic system. We utilize a simulated two-wheeled
robot equipped with distance sensors and a compass, operating within a simple
square environment. Through analysis of the robot's sensor data during random
exploration, we demonstrate how a distorted perceptual space emerges. Despite
these distortions, we identify emergent structures within the perceptual space
that correlate with the physical environment, revealing how the robot
autonomously learns a structured representation for navigation without explicit
spatial information. This work contributes to the understanding of embodied
cognition, minimal agency, and the role of perception in self-generated
navigation strategies in artificial life.

</details>


### [151] [ROS Help Desk: GenAI Powered, User-Centric Framework for ROS Error Diagnosis and Debugging](https://arxiv.org/abs/2507.07846)
*Kavindie Katuwandeniya,Samith Rajapaksha Jayasekara Widhanapathirana*

Main category: cs.RO

TL;DR: A tool, ROS Help Desk, is introduced to simplify error diagnosis in robotics systems, catering to users of varying expertise.


<details>
  <summary>Details</summary>
Motivation: Robotics systems are increasingly integrated into daily life, but the complexity of robotic system frameworks like ROS poses challenges for users in understanding system status and diagnosing errors.

Method: The approach includes ROS Help Desk's user-centric debugging tools, proactive error detection capabilities, and multimodal data processing for system state understanding across multi-sensor data like lidar and RGB.

Result: Tests with artificially induced errors show the system accurately and proactively diagnoses issues, thereby reducing maintenance downtime.

Conclusion: ROS Help Desk aids in reducing maintenance time and fostering improved human-robot collaboration by simplifying and expediting error diagnosis and resolution.

Abstract: As the robotics systems increasingly integrate into daily life, from smart
home assistants to the new-wave of industrial automation systems (Industry
4.0), there's an increasing need to bridge the gap between complex robotic
systems and everyday users. The Robot Operating System (ROS) is a flexible
framework often utilised in writing robot software, providing tools and
libraries for building complex robotic systems. However, ROS's distributed
architecture and technical messaging system create barriers for understanding
robot status and diagnosing errors. This gap can lead to extended maintenance
downtimes, as users with limited ROS knowledge may struggle to quickly diagnose
and resolve system issues. Moreover, this deficit in expertise often delays
proactive maintenance and troubleshooting, further increasing the frequency and
duration of system interruptions. ROS Help Desk provides intuitive error
explanations and debugging support, dynamically customized to users of varying
expertise levels. It features user-centric debugging tools that simplify error
diagnosis, implements proactive error detection capabilities to reduce
downtime, and integrates multimodal data processing for comprehensive system
state understanding across multi-sensor data (e.g., lidar, RGB). Testing
qualitatively and quantitatively with artificially induced errors demonstrates
the system's ability to proactively and accurately diagnose problems,
ultimately reducing maintenance time and fostering more effective human-robot
collaboration.

</details>


### [152] [Improving AEBS Validation Through Objective Intervention Classification Leveraging the Prediction Divergence Principle](https://arxiv.org/abs/2507.07872)
*Daniel Betschinske,Steven Peters*

Main category: cs.RO

TL;DR: The paper introduces a rule-based approach using the Prediction Divergence Principle (PDP) to enhance classification in the safety validation of automatic emergency braking systems (AEBS), addressing challenges associated with human labeling and uncertainty in open-loop resimulations.


<details>
  <summary>Details</summary>
Motivation: Safety validation of AEBS has challenges in distinguishing false positives from true positives due to uncertainties in simulations and driver interventions, leading to reliance on subjective and potentially biased human labeling.

Method: The study proposes a rule-based classification method based on the Prediction Divergence Principle (PDP) and applies it to a simplified AEBS to evaluate its effectiveness.

Result: The proposed PDP-based method demonstrates strengths, limitations, and system requirements for implementation, and combining it with human labeling shows potential for improved transparency and consistency in classification.

Conclusion: The paper concludes that the proposed method can complement human labeling practices, paving the way for more reliable and reproducible AEBS validation while outlining future improvements.

Abstract: The safety validation of automatic emergency braking system (AEBS) requires
accurately distinguishing between false positive (FP) and true positive (TP)
system activations. While simulations allow straightforward differentiation by
comparing scenarios with and without interventions, analyzing activations from
open-loop resimulations - such as those from field operational testing (FOT) -
is more complex. This complexity arises from scenario parameter uncertainty and
the influence of driver interventions in the recorded data. Human labeling is
frequently used to address these challenges, relying on subjective assessments
of intervention necessity or situational criticality, potentially introducing
biases and limitations. This work proposes a rule-based classification approach
leveraging the Prediction Divergence Principle (PDP) to address those issues.
Applied to a simplified AEBS, the proposed method reveals key strengths,
limitations, and system requirements for effective implementation. The findings
suggest that combining this approach with human labeling may enhance the
transparency and consistency of classification, thereby improving the overall
validation process. While the rule set for classification derived in this work
adopts a conservative approach, the paper outlines future directions for
refinement and broader applicability. Finally, this work highlights the
potential of such methods to complement existing practices, paving the way for
more reliable and reproducible AEBS validation frameworks.

</details>


### [153] [UniTac: Whole-Robot Touch Sensing Without Tactile Sensors](https://arxiv.org/abs/2507.07980)
*Wanjia Fu,Hongyu Li,Ivy X. He,Stefanie Tellex,Srinath Sridhar*

Main category: cs.RO

TL;DR: UniTac introduces a touch-sensing approach for robots using only their proprioceptive joint sensors, allowing contact localization without any additional sensors.


<details>
  <summary>Details</summary>
Motivation: Most commercial robots lack tactile skins, making tasks like contact localization difficult. This paper aims to democratize touch sensing for robots.

Method: The approach utilizes data-driven models and the robot's proprioceptive joint sensors for whole-body touch-sensing. No extra sensors are added.

Result: UniTac achieves contact localization on the Franka robot arm (8.0 cm accuracy) and the Spot quadruped (7.2 cm accuracy) at high-frequency rates (~2,000 Hz).

Conclusion: UniTac enables affordable, effective touch-sensing capabilities for robots, benefiting HRI researchers by providing a scalable solution using existing hardware.

Abstract: Robots can better interact with humans and unstructured environments through
touch sensing. However, most commercial robots are not equipped with tactile
skins, making it challenging to achieve even basic touch-sensing functions,
such as contact localization. We present UniTac, a data-driven whole-body
touch-sensing approach that uses only proprioceptive joint sensors and does not
require the installation of additional sensors. Our approach enables a robot
equipped solely with joint sensors to localize contacts. Our goal is to
democratize touch sensing and provide an off-the-shelf tool for HRI researchers
to provide their robots with touch-sensing capabilities. We validate our
approach on two platforms: the Franka robot arm and the Spot quadruped. On
Franka, we can localize contact to within 8.0 centimeters, and on Spot, we can
localize to within 7.2 centimeters at around 2,000 Hz on an RTX 3090 GPU
without adding any additional sensors to the robot. Project website:
https://ivl.cs.brown.edu/research/unitac.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [154] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: The paper introduces a robust German dataset for sentiment analysis tailored to software engineering, validated through high interrater agreement and domain-specific evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis tools largely cater to English datasets, leaving a gap for German-speaking software engineering communities.

Method: Developed a dataset of 5,949 unique statements from a German developer forum, annotated with one of six basic emotions by four linguistically compatible annotators to ensure high validity.

Result: Achieved high interrater agreement in annotation, confirming the dataset’s reliability. Evaluation with current tools highlighted the lack of domain-specific solutions for German sentiment analysis.

Conclusion: The dataset robustly supports sentiment analysis in German-speaking software development, highlighting the need for optimized tools and broader application opportunities.

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [155] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: The paper presents a tool-supported approach to automate the derivation of explainability requirements and explanation generation from user reviews, evaluating it with an industrial dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of systematically deriving structured explainability requirements and explanations from user feedback, a gap in ensuring transparency, trust, and regulatory compliance in software systems.

Method: The authors introduce an automated approach combined with an evaluation on 58 annotated user reviews from an industrial automation manufacturer, comparing AI and human outputs for relevance and style.

Result: The evaluation reveals that AI-generated requirements lack human-level relevance and correctness, but AI explanations are often preferred for their clarity despite issues with correctness.

Conclusion: The study underscores the potential of automated tools in generating explainability-related artifacts while emphasizing the necessity of human validation for correctness. It also provides a dataset to foster further research in this domain.

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [156] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: The paper introduces a novel approach for enhancing engineering workflows through Industry 4.0 technologies, employing AAS, BPMN, and a copy-on-write infrastructure.


<details>
  <summary>Details</summary>
Motivation: To integrate Industry 4.0 technologies for automating plant and process engineering workflows effectively, leveraging interoperable Digital Twins and engineering automation.

Method: The authors utilize Asset Administration Shell (AAS) combined with BPMN for structured workflow definition, proposing a distributed copy-on-write infrastructure for scalability and security.

Result: A prototype of a workflow management system is introduced, capable of automating AAS operations and engineering processes while optimizing efficiency and traceability.

Conclusion: The proposed infrastructure and management prototype facilitate cross-organizational collaboration, showcasing improved engineering automation and data exchange through Industry 4.0 integrations.

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [157] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: The paper investigates how developers use requirements and design artifacts to generate code with large language models (LLMs), finding that requirements are too abstract and must be decomposed and enriched for effective use in prompts.


<details>
  <summary>Details</summary>
Motivation: Examine the interplay between requirements engineering (RE) and code generation with the rise of LLMs, given the claim that LLMs could revolutionize traditional software engineering.

Method: The researchers conducted interviews with 18 industry practitioners from 14 companies to explore how they utilize requirements and design data when prompting LLMs for code generation.

Result: The study found that requirements are too abstract for direct use with LLMs. Developers need to manually break down and supplement requirements with design and architecture information to create effective prompts.

Conclusion: Key RE tasks remain indispensable even when LLMs are employed for code generation, emphasizing that automated requirements-centric software engineering still requires human input.

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [158] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: The paper conducts a systematic literature review on prompt engineering (PE) for requirements engineering (RE) aimed at addressing issues of LLM uncertainty and lack of control.


<details>
  <summary>Details</summary>
Motivation: To address the uncertainty and lack of control in applying large language models (LLMs) for requirements engineering (RE) tasks, limiting their trustworthy implementation.

Method: Conducted a systematic review of prompt engineering literature using secondary-study protocols, analyzing 35 out of 867 screened studies to create a taxonomy linking technique patterns and RE tasks.

Result: Identified technique-task linkages, current limitations, and research gaps. Developed a roadmap for transitioning ad-hoc prototypes into reproducible workflows for practitioners.

Conclusion: Prompt engineering can significantly enhance RE tasks if systematic, reproducible strategies are developed, addressing the fragmented nature of current methods.

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [159] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: The paper investigates how Retrieval-Augmented Generation (RAG) models can semi-automate requirements engineering in the space industry by processing mission documents, categorizing them, retrieving relevant content, and generating requirements using large language models, showcasing potential benefits like reduced manual effort and better compliance.


<details>
  <summary>Details</summary>
Motivation: Smaller space organizations face challenges in deriving actionable requirements from unstructured documents due to the complexity and rigor of the space industry's standards, creating a need for innovative approaches to requirements engineering.

Method: The authors present a modular, AI-driven approach that preprocesses mission documents, classifies content into semantic categories, retrieves relevant standards-based information, and creates draft requirements using large language models (LLMs).

Result: Applying the proposed approach to a real-world mission document showed that it reduced manual effort, enhanced the coverage of relevant requirements, and supported compliance alignment, as tested in collaboration with Starbound Space Solutions.

Conclusion: This study demonstrates the feasibility of using AI to support requirements engineering in the space domain, potentially lowering barriers for smaller organizations and outlining a roadmap for broader AI integration in RE workflows.

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [160] [Astrocyte-Mediated Higher-Order Control of Synaptic Plasticity](https://arxiv.org/abs/2507.07693)
*Gustavo Menesse,Ana P. Millán,Joaquín J. Torres*

Main category: q-bio.NC

TL;DR: The paper introduces a model integrating astrocyte modulation in tripartite synapses to explore its role in stabilizing neural circuit dynamics.


<details>
  <summary>Details</summary>
Motivation: Analyzing the role of astrocyte-driven higher-order interactions in synaptic dynamics and their interplay with traditional node-driven short-term plasticity.

Method: Developed a higher-order model incorporating astrocyte-mediated mechanisms into the dynamics of neurotransmitter release; tested within a recurrent motif of leaky integrate-and-fire neurons.

Result: Higher-order modulation by astrocyte mechanisms stabilized circuit dynamics and expanded parameter space for stimulus-driven neural activity.

Conclusion: Astrocytes reshape effective connectivity, enhancing information processing and stabilizing neural circuits via higher-order interactions.

Abstract: The dynamics of higher-order topological signals are increasingly recognized
as a key aspect of the activity of complex systems. A paradigmatic example are
synaptic dynamics: synaptic efficacy changes over time driven by different
mechanisms. Beyond traditional node-driven short-term plasticity mechanisms,
the role of astrocyte modulation through higher-order interactions, in the
so-called tripartite synapse, is increasingly recognized. However, the
competition and interplay between node-driven and higher-order mechanisms have
yet to be considered. Here, we introduce a simple higher-order model of the
tripartite synapse accounting for astrocyte-synapse-neuron interactions in
short-term plasticity. In the model, astrocyte gliotransmission and
pre-synaptic intrinsic facilitation mechanisms jointly modulate the probability
of neurotransmitter release at the synapse, generalizing previous short-term
plasticity models. We investigate the implications of such mechanisms in a
minimal recurrent motif -- a directed ring of three excitatory leaky
integrate-and-fire neurons -- where one neuron receives external stimulation
that propagates through the circuit. Due to its strong recurrence, the circuit
is highly prone to self-sustained activity, which can make it insensitive to
external input. By introducing higher-order interactions among different
synapses through astrocyte modulation, we show that higher-order modulation
robustly stabilizes circuit dynamics and expands the parameter space that
supports stimulus-driven activity. Our findings highlight a plausible mechanism
by which astrocytes can reshape effective connectivity and enhance information
processing through higher-order structural interactions -- even in the simplest
recurrent circuits.

</details>


### [161] [OMiSO: Adaptive optimization of state-dependent brain stimulation to shape neural population states](https://arxiv.org/abs/2507.07858)
*Yuki Minai,Joana Soldado-Magraner,Byron M. Yu,Matthew A. Smith*

Main category: q-bio.NC

TL;DR: The paper introduces Online MicroStimulation Optimization (OMiSO), a framework that uses pre-stimulation brain state data to improve the precision of brain stimulation techniques.


<details>
  <summary>Details</summary>
Motivation: There is a need to enhance brain stimulation methods to achieve precise and causal manipulation of neural population activity for scientific and clinical advancements.

Method: OMiSO leverages pre-stimulation brain state information to optimally select stimulation parameters and adaptively refines these choices based on observed responses during stimulation.

Result: Experimental testing of OMiSO in monkeys demonstrated its superiority over existing methods by achieving more precise neural activity states.

Conclusion: OMiSO enhances the accuracy of brain stimulation technologies, advancing tools for neuroscience research and potential treatments for brain disorders.

Abstract: The coordinated activity of neural populations underlies myriad brain
functions. Manipulating this activity using brain stimulation techniques has
great potential for scientific and clinical applications, as it provides a tool
to causally influence brain function. The state of the brain affects how neural
populations respond to incoming sensory stimuli. Thus, taking into account
pre-stimulation neural population activity may be crucial to achieve a desired
causal manipulation using stimulation. In this work, we propose Online
MicroStimulation Optimization (OMiSO), a brain stimulation framework that
leverages brain state information to find stimulation parameters that can drive
neural population activity toward specified states. OMiSO includes two key
advances: i) it leverages the pre-stimulation brain state to choose optimal
stimulation parameters, and ii) it adaptively refines the choice of those
parameters by considering newly-observed stimulation responses. We tested OMiSO
by applying intracortical electrical microstimulation in a monkey and found
that it outperformed competing methods that do not incorporate these advances.
Taken together, OMiSO provides greater accuracy in achieving specified activity
states, thereby advancing neuromodulation technologies for understanding the
brain and for treating brain disorders.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [162] [Class conditional conformal prediction for multiple inputs by p-value aggregation](https://arxiv.org/abs/2507.07150)
*Jean-Baptiste Fermanian,Mohamed Hebiri,Joseph Salmon*

Main category: stat.ML

TL;DR: This paper introduces an enhanced conformal prediction method for handling multiple observations of a single instance in classification tasks, preserving statistical guarantees while reducing predictive label set sizes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve uncertainty quantification in classification scenarios where multiple observations, such as images of the same plant or animal from citizen science platforms, are available.

Method: The method aggregates conformal p-values from multiple inputs using an abstract scoring function and exploits the exact p-value distribution to refine standard prediction strategies.

Result: The proposed method outperforms alternatives in preserving class-conditional coverage guarantees while reducing prediction set size, as shown via simulations and real data, including on Pl@ntNet.

Conclusion: The paper presents a generalized framework for leveraging multiple inputs in conformal prediction, enhancing its applicability in real-world scenarios like citizen science imaging.

Abstract: Conformal prediction methods are statistical tools designed to quantify
uncertainty and generate predictive sets with guaranteed coverage
probabilities. This work introduces an innovative refinement to these methods
for classification tasks, specifically tailored for scenarios where multiple
observations (multi-inputs) of a single instance are available at prediction
time. Our approach is particularly motivated by applications in citizen
science, where multiple images of the same plant or animal are captured by
individuals. Our method integrates the information from each observation into
conformal prediction, enabling a reduction in the size of the predicted label
set while preserving the required class-conditional coverage guarantee. The
approach is based on the aggregation of conformal p-values computed from each
observation of a multi-input. By exploiting the exact distribution of these
p-values, we propose a general aggregation framework using an abstract scoring
function, encompassing many classical statistical tools. Knowledge of this
distribution also enables refined versions of standard strategies, such as
majority voting. We evaluate our method on simulated and real data, with a
particular focus on Pl@ntNet, a prominent citizen science platform that
facilitates the collection and identification of plant species through
user-submitted images.

</details>


### [163] [Topological Machine Learning with Unreduced Persistence Diagrams](https://arxiv.org/abs/2507.07156)
*Nicole Abreu,Parker B. Edwards,Francis Motta*

Main category: stat.ML

TL;DR: The paper finds that machine learning pipelines can utilize unreduced boundary matrices for training, potentially saving computational time while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Supervised machine learning pipelines using persistence diagrams ignore much of the contained information and require computationally expensive steps. Exploring alternative methods is necessary.

Method: The authors generated topological feature vectors from unreduced boundary matrices in persistence diagrams and compared their performance with pipelines using fully-reduced diagrams across various tasks.

Result: Pipelines trained on unreduced diagrams performed on par or better than those trained on fully-reduced diagrams for some tasks, indicating their potential usefulness.

Conclusion: Topological machine learning pipelines could benefit in terms of efficiency and performance by leveraging unreduced boundary matrices instead of fully-reduced persistence diagrams.

Abstract: Supervised machine learning pipelines trained on features derived from
persistent homology have been experimentally observed to ignore much of the
information contained in a persistence diagram. Computing persistence diagrams
is often the most computationally demanding step in such a pipeline, however.
To explore this, we introduce several methods to generate topological feature
vectors from unreduced boundary matrices. We compared the performance of
pipelines trained on vectorizations of unreduced PDs to vectorizations of
fully-reduced PDs across several data and task types. Our results indicate that
models trained on PDs built from unreduced diagrams can perform on par and even
outperform those trained on fully-reduced diagrams on some tasks. This
observation suggests that machine learning pipelines which incorporate
topology-based features may benefit in terms of computational cost and
performance by utilizing information contained in unreduced boundary matrices.

</details>


### [164] [Bayesian Double Descent](https://arxiv.org/abs/2507.07338)
*Nick Polson,Vadim Sokolov*

Main category: stat.ML

TL;DR: The paper examines the phenomenon of double descent in over-parameterized models from a Bayesian perspective, showing its alignment with Bayesian principles and Occam's razor.


<details>
  <summary>Details</summary>
Motivation: Explore double descent in the context of Bayesian models to understand its implications and reconcile it with traditional statistical principles like Occam's razor.

Method: Analyze double descent in over-parameterized models, interpret its characteristics with Bayesian principles, and provide an example using Bayesian model selection in neural networks.

Result: Double descent has a natural Bayesian interpretation, showing compatibility with Occam’s razor despite the complexity and over-parameterization of models.

Conclusion: Double descent in over-parameterized models aligns with Bayesian principles and Occam's razor, opening new directions for research and extending our understanding of risk characteristics in machine learning.

Abstract: Double descent is a phenomenon of over-parameterized statistical models. Our
goal is to view double descent from a Bayesian perspective. Over-parameterized
models such as deep neural networks have an interesting re-descending property
in their risk characteristics. This is a recent phenomenon in machine learning
and has been the subject of many studies. As the complexity of the model
increases, there is a U-shaped region corresponding to the traditional
bias-variance trade-off, but then as the number of parameters equals the number
of observations and the model becomes one of interpolation, the risk can become
infinite and then, in the over-parameterized region, it re-descends -- the
double descent effect. We show that this has a natural Bayesian interpretation.
Moreover, we show that it is not in conflict with the traditional Occam's razor
that Bayesian models possess, in that they tend to prefer simpler models when
possible. We illustrate the approach with an example of Bayesian model
selection in neural networks. Finally, we conclude with directions for future
research.

</details>


### [165] [Hess-MC2: Sequential Monte Carlo Squared using Hessian Information and Second Order Proposals](https://arxiv.org/abs/2507.07461)
*Joshua Murphy,Conor Rosato,Andrew Millard,Lee Devlin,Paul Horridge,Simon Maskell*

Main category: stat.ML

TL;DR: This paper incorporates second-order derivative information to improve Sequential Monte Carlo Squared (SMC$^2$) methods for Bayesian inference, increasing efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the computational efficiency and accuracy challenges in Bayesian inference using SMC$^2$, by improving proposal distributions with advanced gradient and curvature information.

Method: Extend the SMC$^2$ framework by integrating second-order proposal distributions, leveraging both the gradient (first-order) and curvature (second-order).

Result: Experimental results on synthetic models show improved step-size selection and more accurate posterior approximation compared to existing methods.

Conclusion: Second-order proposals within SMC$^2$ enhance computational performance and posterior exploration, making it effective for HPC-based Bayesian inference.

Abstract: When performing Bayesian inference using Sequential Monte Carlo (SMC)
methods, two considerations arise: the accuracy of the posterior approximation
and computational efficiency. To address computational demands, Sequential
Monte Carlo Squared (SMC$^2$) is well-suited for high-performance computing
(HPC) environments. The design of the proposal distribution within SMC$^2$ can
improve accuracy and exploration of the posterior as poor proposals may lead to
high variance in importance weights and particle degeneracy. The
Metropolis-Adjusted Langevin Algorithm (MALA) uses gradient information so that
particles preferentially explore regions of higher probability. In this paper,
we extend this idea by incorporating second-order information, specifically the
Hessian of the log-target. While second-order proposals have been explored
previously in particle Markov Chain Monte Carlo (p-MCMC) methods, we are the
first to introduce them within the SMC$^2$ framework. Second-order proposals
not only use the gradient (first-order derivative), but also the curvature
(second-order derivative) of the target distribution. Experimental results on
synthetic models highlight the benefits of our approach in terms of step-size
selection and posterior approximation accuracy when compared to other
proposals.

</details>


### [166] [Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting](https://arxiv.org/abs/2507.07469)
*Haojie Liu,Zihan Lin*

Main category: stat.ML

TL;DR: The paper introduces Galerkin-ARIMA, a spline-based advancement over ARIMA, capturing nonlinear dependencies and improving forecasting efficiency.


<details>
  <summary>Details</summary>
Motivation: ARIMA models are limited by linearity assumptions and inefficiencies with large, complex datasets.

Method: The ARIMA's AR component is replaced with a spline-based function using Galerkin projection. The paper derives an OLS estimator for this new setup, ensuring unbiasedness and consistency.

Result: The proposed method combines classical time-series modeling with nonparametric regression, achieving improved forecasting and computational efficiency.

Conclusion: Galerkin-ARIMA enhances ARIMA by addressing nonlinearities while maintaining foundational elements, making it effective for complex forecasting tasks.

Abstract: Time-series models like ARIMA remain widely used for forecasting but limited
to linear assumptions and high computational cost in large and complex
datasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA
and replace it with a flexible spline-based function estimated by Galerkin
projection. This enables the model to capture nonlinear dependencies in lagged
values and retain the MA component and Gaussian noise assumption. We derive a
closed-form OLS estimator for the Galerkin coefficients and show the model is
asymptotically unbiased and consistent under standard conditions. Our method
bridges classical time-series modeling and nonparametric regression, which
offering improved forecasting performance and computational efficiency.

</details>


### [167] [A Unified Empirical Risk Minimization Framework for Flexible N-Tuples Weak Supervision](https://arxiv.org/abs/2507.07771)
*Shuying Huang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: stat.ML

TL;DR: The paper proposes a general framework for N-tuples learning that unifies various weakly supervised scenarios and integrates pointwise unlabeled data to improve learning performance.


<details>
  <summary>Details</summary>
Motivation: To reduce the dependency on labor-intensive annotations in supervised learning while extending the applicability and theoretical foundation of N-tuples learning.

Method: The authors present a unified probabilistic formulation for N-tuples and pointwise unlabeled data, derive an unbiased empirical risk estimator, establish a generalization error bound, and implement correction functions to prevent overfitting.

Result: The proposed framework demonstrates improved generalization and effectiveness across multiple weakly-supervised learning tasks, particularly when leveraging pointwise unlabeled data.

Conclusion: This study provides a theoretical and practical foundation for N-tuples learning, allowing its application in diverse weakly supervised scenarios and improving performance through pointwise unlabeled data.

Abstract: To alleviate the annotation burden in supervised learning, N-tuples learning
has recently emerged as a powerful weakly-supervised method. While existing
N-tuples learning approaches extend pairwise learning to higher-order
comparisons and accommodate various real-world scenarios, they often rely on
task-specific designs and lack a unified theoretical foundation. In this paper,
we propose a general N-tuples learning framework based on empirical risk
minimization, which systematically integrates pointwise unlabeled data to
enhance learning performance. This paper first unifies the data generation
processes of N-tuples and pointwise unlabeled data under a shared probabilistic
formulation. Based on this unified view, we derive an unbiased empirical risk
estimator that generalizes a broad class of existing N-tuples models. We
further establish a generalization error bound for theoretical support. To
demonstrate the flexibility of the framework, we instantiate it in four
representative weakly supervised scenarios, each recoverable as a special case
of our general model. Additionally, to address overfitting issues arising from
negative risk terms, we adopt correction functions to adjust the empirical
risk. Extensive experiments on benchmark datasets validate the effectiveness of
the proposed framework and demonstrate that leveraging pointwise unlabeled data
consistently improves generalization across various N-tuples learning tasks.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [168] [A statistical physics framework for optimal learning](https://arxiv.org/abs/2507.07907)
*Francesca Mignacco,Francesco Mori*

Main category: cond-mat.dis-nn

TL;DR: The paper combines statistical physics and control theory to derive optimal learning protocols, simplifying complex dynamics in neural network training.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in understanding and designing optimal learning strategies due to evolving meta-parameters and nonlinear dynamics.

Method: A theoretical framework is introduced that leverages statistical physics and control theory, deriving ordinary differential equations to model high-dimensional neural learning processes.

Result: Derivation of interpretable learning strategies for curriculum design, dropout regularization, and noise schedules that optimize generalization error.

Conclusion: The framework provides a foundation for principled design of learning protocols and advances meta-learning theory using statistical physics.

Abstract: Learning is a complex dynamical process shaped by a range of interconnected
decisions. Careful design of hyperparameter schedules for artificial neural
networks or efficient allocation of cognitive resources by biological learners
can dramatically affect performance. Yet, theoretical understanding of optimal
learning strategies remains sparse, especially due to the intricate interplay
between evolving meta-parameters and nonlinear learning dynamics. The search
for optimal protocols is further hindered by the high dimensionality of the
learning space, often resulting in predominantly heuristic, difficult to
interpret, and computationally demanding solutions. Here, we combine
statistical physics with control theory in a unified theoretical framework to
identify optimal protocols in prototypical neural network models. In the
high-dimensional limit, we derive closed-form ordinary differential equations
that track online stochastic gradient descent through low-dimensional order
parameters. We formulate the design of learning protocols as an optimal control
problem directly on the dynamics of the order parameters with the goal of
minimizing the generalization error at the end of training. This framework
encompasses a variety of learning scenarios, optimization constraints, and
control budgets. We apply it to representative cases, including optimal
curricula, adaptive dropout regularization and noise schedules in denoising
autoencoders. We find nontrivial yet interpretable strategies highlighting how
optimal protocols mediate crucial learning tradeoffs, such as maximizing
alignment with informative input directions while minimizing noise fitting.
Finally, we show how to apply our framework to real datasets. Our results
establish a principled foundation for understanding and designing optimal
learning protocols and suggest a path toward a theory of meta-learning grounded
in statistical physics.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [169] [Late Fusion Multi-task Learning for Semiparametric Inference with Nuisance Parameters](https://arxiv.org/abs/2507.07941)
*Sohom Bhattacharya,Yongzhuo Chen,Muxuan Liang*

Main category: stat.ME

TL;DR: The paper presents a two-step late fusion framework for multi-task learning applied to semiparametric models, focusing on heterogeneous treatment effect estimation across datasets, demonstrating increased robustness, privacy preservation, and improved convergence rates.


<details>
  <summary>Details</summary>
Motivation: With the rising prevalence of large and diverse datasets, the need arises to integrate information from multiple sources to enhance parameter estimation, particularly in applications like heterogeneous treatment effect estimation.

Method: The framework uses a two-step approach: first by employing double machine-learning estimators for individual tasks, and then by adaptively aggregating them to utilize task similarities and address differences. It also introduces a multi-task learning method for estimating nuisance parameters.

Result: This framework achieves faster convergence rates compared to individual task learning when tasks have shared parametric components. The approach also retains privacy by avoiding data sharing and is effective even with moderate sample sizes.

Conclusion: The proposed method offers a robust, privacy-preserving, and theoretically grounded solution for multi-task learning across heterogeneous datasets, validated through simulations and real-world applications, showcasing its practicality and efficiency.

Abstract: In the age of large and heterogeneous datasets, the integration of
information from diverse sources is essential to improve parameter estimation.
Multi-task learning offers a powerful approach by enabling simultaneous
learning across related tasks. In this work, we introduce a late fusion
framework for multi-task learning with semiparametric models that involve
infinite-dimensional nuisance parameters, focusing on applications such as
heterogeneous treatment effect estimation across multiple data sources,
including electronic health records from different hospitals or clinical trial
data. Our framework is two-step: first, initial double machine-learning
estimators are obtained through individual task learning; second, these
estimators are adaptively aggregated to exploit task similarities while
remaining robust to task-specific differences. In particular, the framework
avoids individual level data sharing, preserving privacy. Additionally, we
propose a novel multi-task learning method for nuisance parameter estimation,
which further enhances parameter estimation when nuisance parameters exhibit
similarity across tasks. We establish theoretical guarantees for the method,
demonstrating faster convergence rates compared to individual task learning
when tasks share similar parametric components. Extensive simulations and real
data applications complement the theoretical findings of our work while
highlight the effectiveness of our framework even in moderate sample sizes.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [170] [Feature-free regression kriging](https://arxiv.org/abs/2507.07382)
*Peng Luo,Yilong Wu,Yongze Song*

Main category: physics.soc-ph

TL;DR: The paper introduces the Feature-Free Regression Kriging (FFRK) method, which uses geospatial features without external explanatory variables to improve spatial interpolation accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges with conventional geostatistical methods like Ordinary Kriging and Regression Kriging, which struggle with spatial nonstationarity and rely on hard-to-acquire explanatory variables.

Method: The proposed method, Feature-Free Regression Kriging (FFRK), extracts geospatial features like local dependence, heterogeneity, and geosimilarity to construct prediction models without external explanatory data.

Result: Experiments on heavy metals prediction in Australia demonstrate that FFRK consistently outperforms traditional Kriging techniques and machine learning models relying on explanatory variables.

Conclusion: FFRK enhances spatial interpolation accuracy and generalization by addressing spatial nonstationarity and eliminating reliance on external explanatory variables, showing the value of geospatial feature characterization.

Abstract: Spatial interpolation is a crucial task in geography. As perhaps the most
widely used interpolation methods, geostatistical models -- such as Ordinary
Kriging (OK) -- assume spatial stationarity, which makes it difficult to
capture the nonstationary characteristics of geographic variables. A common
solution is trend surface modeling (e.g., Regression Kriging, RK), which relies
on external explanatory variables to model the trend and then applies
geostatistical interpolation to the residuals. However, this approach requires
high-quality and readily available explanatory variables, which are often
lacking in many spatial interpolation scenarios -- such as estimating heavy
metal concentrations underground. This study proposes a Feature-Free Regression
Kriging (FFRK) method, which automatically extracts geospatial features --
including local dependence, local heterogeneity, and geosimilarity -- to
construct a regression-based trend surface without requiring external
explanatory variables. We conducted experiments on the spatial distribution
prediction of three heavy metals in a mining area in Australia. In comparison
with 17 classical interpolation methods, the results indicate that FFRK, which
does not incorporate any explanatory variables and relies solely on extracted
geospatial features, consistently outperforms both conventional Kriging
techniques and machine learning models that depend on explanatory variables.
This approach effectively addresses spatial nonstationarity while reducing the
cost of acquiring explanatory variables, improving both prediction accuracy and
generalization ability. This finding suggests that an accurate characterization
of geospatial features based on domain knowledge can significantly enhance
spatial prediction performance -- potentially yielding greater improvements
than merely adopting more advanced statistical models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [171] [Pluri-perspectivism in Human-robot Co-creativity with Older Adults](https://arxiv.org/abs/2507.07550)
*Marianne Bossema,Rob Saunders,Aske Plaat,Somaya Ben Allouch*

Main category: cs.HC

TL;DR: The paper investigates how the concept of pluriperspectivism can enhance human-robot cocreation, presenting a five-dimensional model informed by artist interviews.


<details>
  <summary>Details</summary>
Motivation: To explore how the principle of pluriperspectivism—a core aspect of human creativity—can inform and improve human-robot cocreation.

Method: A five-dimensional model was proposed based on literature review and interviews with 10 visual artists and 8 art educators, analyzing how pluriperspectivism aids creative practice.

Result: The study revealed ways robots could potentially enhance human creativity through adaptive, context-sensitive behavior informed by pluriperspectivism.

Conclusion: Pluriperspectivism holds promise for enhancing cocreative experiences between humans and robots, and future integration with vision-language models (VLMs) could further support this goal.

Abstract: This position paper explores pluriperspectivism as a core element of human
creative experience and its relevance to humanrobot cocreativity We propose a
layered fivedimensional model to guide the design of cocreative behaviors and
the analysis of interaction dynamics This model is based on literature and
results from an interview study we conducted with 10 visual artists and 8 arts
educators examining how pluriperspectivism supports creative practice The
findings of this study provide insight in how robots could enhance human
creativity through adaptive contextsensitive behavior demonstrating the
potential of pluriperspectivism This paper outlines future directions for
integrating pluriperspectivism with visionlanguage models VLMs to support
context sensitivity in cocreative robots

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [172] [A novel approach for classifying Monoamine Neurotransmitters by applying Machine Learning on UV plasmonic-engineered Auto Fluorescence Time Decay Series (AFTDS)](https://arxiv.org/abs/2507.07227)
*Mohammad Mohammadi,Sima Najafzadehkhoei,George Vega Yon,Yunshan Wang*

Main category: q-bio.BM

TL;DR: This paper integrates advanced plasmonic substrates and machine learning to achieve high-precision detection of neurotransmitters. Aluminum concave nanocubes (AlCNCs) amplify fluorescence signals, combined with ML algorithms like LSTM for improved classification.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address limitations in detecting and differentiating biomolecules with high sensitivity and specificity, leveraging plasmonic nanomaterials and ML.

Method: The study uses AlCNCs as plasmonic substrates to enhance fluorescence and employs ML algorithms, particularly Long Short-Term Memory networks, for time-dependent fluorescence data analysis.

Result: The results include up to a 12-fold fluorescence amplification with AlCNCs and ML-driven classification accuracy surpassing 89%.

Conclusion: This hybrid approach demonstrates the potential of combining AlCNC-enhanced fluorescence with ML algorithms for probe-free, label-free biomolecular detection. It can significantly impact biomedical diagnostics and neuroscience.

Abstract: This study introduces a hybrid approach integrating advanced plasmonic
nanomaterials and machine learning (ML) for high-precision biomolecule
detection. We leverage aluminum concave nanocubes (AlCNCs) as an innovative
plasmonic substrate to enhance the native fluorescence of neurotransmitters,
including dopamine (DA), norepinephrine (NE), and 3,4-Dihydroxyphenylacetic
acid (DOPAC). AlCNCs amplify weak fluorescence signals, enabling probe-free,
label-free detection and differentiation of these molecules with great
sensitivity and specificity. To further improve classification accuracy, we
employ ML algorithms, with Long Short-Term Memory (LSTM) networks playing a
central role in analyzing time-dependent fluorescence data. Comparative
evaluations with k-Nearest Neighbors (KNN) and Random Forest (RF) demonstrate
the superior performance of LSTM in distinguishing neurotransmitters. The
results reveal that AlCNC substrates provide up to a 12-fold enhancement in
fluorescence intensity for DA, 9-fold for NE, and 7-fold for DOPAC compared to
silicon substrates. At the same time, ML algorithms achieve classification
accuracy exceeding 89%. This interdisciplinary methodology bridges the gap
between nanotechnology and ML, showcasing the synergistic potential of
AlCNC-enhanced native fluorescence and ML in biosensing. The framework paves
the way for probe-free, label-free biomolecule profiling, offering
transformative implications for biomedical diagnostics and neuroscience
research.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [173] [Robust signal decompositions on the circle](https://arxiv.org/abs/2507.07007)
*Aral Kose,Daniel Liberzon*

Main category: math.OC

TL;DR: This paper explores decomposing a piecewise constant function into a sum of indicator functions for circular disks, addressing challenges like unknown landmark locations and robustness.


<details>
  <summary>Details</summary>
Motivation: To enable agents on a circular path to estimate the number and location of landmarks, facilitating tasks like motion planning and obstacle avoidance.

Method: The authors introduce concepts of robustness and degrees of freedom to analyze decompositions and provide a method for generating and characterizing robust decompositions.

Result: The study presents a framework to characterize and compute robust decompositions, including bounds for decompositions with maximum degrees of freedom.

Conclusion: The method provides a systematic way to handle uncertain data and compute robust decompositions, improving functionality in proximity sensing and landmark estimation.

Abstract: We consider the problem of decomposing a piecewise constant function on the
circle into a sum of indicator functions of closed circular disks in the plane,
whose number and location are not a priori known. This represents a situation
where an agent moving on the circle is able to sense its proximity to some
landmarks, and the goal is to estimate the number of these landmarks and their
possible locations -- which can in turn enable control tasks such as motion
planning and obstacle avoidance. Moreover, the exact values of the function at
its discontinuities (which correspond to disk boundaries for the individual
indicator functions) are not assumed to be known to the agent. We introduce
suitable notions of robustness and degrees of freedom to single out those
decompositions that are more desirable, or more likely, given this non-precise
data collected by the agent. We provide a characterization of robust
decompositions and give a procedure for generating all such decompositions.
When the given function admits a robust decomposition, we compute the number of
possible robust decompositions and derive bounds for the number of
decompositions maximizing the degrees of freedom.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [174] [Toolchain for Faster Iterations in Quantum Software Development](https://arxiv.org/abs/2507.07448)
*Otso Kinanen,Andrés D. Muñoz-Moller,Vlad Stirbu,Tommi Mikkonen*

Main category: quant-ph

TL;DR: This paper explores a remote computational approach to optimize quantum software workflows, achieving execution speedups and better accessibility for developers.


<details>
  <summary>Details</summary>
Motivation: Quantum computing has transformative potential but faces hurdles like limited hardware, intensive computational demands when simulating on classical systems, and complex technology stacks, creating inefficiencies for developers.

Method: The approach leverages remote computational resources to streamline quantum software development by integrating an easy-to-use plug-and-play Jupyter notebook kernel, enabling faster execution and support for larger quantum circuits.

Result: The proposed solution achieved up to 5× faster execution runtime and supported qubit ranges from 21 to 29 qubits.

Conclusion: Remote computational resources can significantly enhance quantum software development, simplifying workflows and enabling iterative development of complex quantum circuits.

Abstract: Quantum computing proposes a revolutionary paradigm that can radically
transform numerous scientific and industrial application domains. To realize
this promise, these new capabilities need software solutions that are able to
effectively harness its power. However, developers may face significant
challenges when developing and executing quantum software due to the limited
availability of quantum computer hardware, high computational demands of
simulating quantum computers on classical systems, and complicated technology
stack to enable currently available accelerators into development environments.
These limitations make it difficult for the developer to create an efficient
workflow for quantum software development. In this paper, we investigate the
potential of using remote computational capabilities in an efficient manner to
improve the workflow of quantum software developers, by lowering the barrier of
moving between local execution and computationally more efficient remote
hardware and offering speedup in execution with simulator surroundings. The
goal is to allow the development of more complex circuits and to support an
iterative software development approach. In our experiment, with the solution
presented in this paper, we have obtained up to 5 times faster circuit
execution runtime, and enabled qubit ranges from 21 to 29 qubits with a simple
plug-and-play kernel for the Jupyter notebook.

</details>


### [175] [Quantum Executor: A Unified Interface for Quantum Computing](https://arxiv.org/abs/2507.07597)
*Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: quant-ph

TL;DR: This paper presents Quantum Executor, a backend-agnostic engine facilitating seamless quantum software experimentation and deployment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing demand for robust, scalable, and versatile tools for quantum software experimentation across heterogeneous platforms.

Method: The authors developed Quantum Executor, which uses a declarative and modular interface. It supports asynchronous, distributed execution, customizable strategies, and offers a unified API to manage quantum experiments.

Result: The engine's applicability is demonstrated via two example scenarios: automated benchmarking and hybrid validation, showcasing its ability to streamline quantum software development.

Conclusion: Quantum Executor enhances interoperability and quantum experiment management, but the paper acknowledges limitations and provides a roadmap for future improvements.

Abstract: As quantum computing evolves from theoretical promise to practical
deployment, the demand for robust, portable, and scalable tools for quantum
software experimentation is growing. This paper introduces Quantum Executor, a
backend-agnostic execution engine designed to orchestrate quantum experiments
across heterogeneous platforms. Quantum Executor provides a declarative and
modular interface that decouples experiment design from backend execution,
enabling seamless interoperability and code reuse across diverse quantum and
classical resources. Key features include support for asynchronous and
distributed execution, customizable execution strategies and a unified API for
managing quantum experiments. We illustrate its applicability through two
life-like usage scenarios such as automated benchmarking and hybrid validation,
discussing its capacity to streamline quantum development. We conclude by
discussing current limitations and outlining a roadmap for future enhancements.

</details>


### [176] [ProvideQ: A Quantum Optimization Toolbox](https://arxiv.org/abs/2507.07649)
*Domenik Eichhorn,Nick Poser,Maximilian Schweikart,Ina Schaefer*

Main category: quant-ph

TL;DR: Hybrid solvers merging classical and quantum computing show theoretical promise but face practical challenges due to missing integration frameworks. ProvideQ toolbox is introduced to enable adaptable Meta-Solver strategies that split problems effectively using both computation paradigms.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of integrating quantum computing with classical optimization frameworks for tackling combinatorial optimization problems.

Method: ProvideQ toolbox utilizes Meta-Solver strategies, which split problems into classical and quantum subroutines. It features a configuration tool to facilitate interactive decomposition and seamless integration of quantum circuits with classical methods.

Result: The paper demonstrates the toolbox's technical design and its application to various real-world problems, showing the viability of Meta-Solver strategies using current quantum computing hardware.

Conclusion: While Meta-Solver strategies via ProvideQ prove effective even on existing hardware, improving quantum hardware is necessary to achieve competitive performance compared to classical solutions.

Abstract: Hybrid solvers for combinatorial optimization problems combine the advantages
of classical and quantum computing to overcome difficult computational
challenges. Although their theoretical performance seems promising, their
practical applicability is challenging due to the lack of a technological stack
that can seamlessly integrate quantum solutions with existing classical
optimization frameworks. We tackle this challenge by introducing the ProvideQ
toolbox, a software tool that enables users to easily adapt and configure
hybrid solvers via Meta-Solver strategies. A Meta-Solver strategy implements
decomposition techniques, which splits problems into classical and quantum
subroutines. The ProvideQ toolbox enables the interactive creation of such
decompositions via a Meta-Solver configuration tool. It combines
well-established classical optimization techniques with quantum circuits that
are seamlessly executable on multiple backends. This paper introduces the
technical details of the ProvideQ toolbox, explains its architecture, and
demonstrates possible applications for several real-world use cases. Our proof
of concept shows that Meta-Solver strategies already enable the application of
quantum subroutines today, however, more sophisticated hardware is required to
make their performance competitive.

</details>
