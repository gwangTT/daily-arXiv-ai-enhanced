<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 86]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 24]
- [cs.SE](#cs.SE) [Total: 11]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 4]
- [nlin.CG](#nlin.CG) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CY](#cs.CY) [Total: 5]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.SC](#cs.SC) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CR](#cs.CR) [Total: 6]
- [stat.CO](#stat.CO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 8]
- [math.OC](#math.OC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Study on the Application of Artificial Intelligence in Ecological Design](https://arxiv.org/abs/2507.11595)
*Hengyue Zhao*

Main category: cs.AI

TL;DR: The paper explores the shift from human dominance to interdependence with nature through AI-assisted ecological design, highlighting its potential in sustainable ecosystems.


<details>
  <summary>Details</summary>
Motivation: Investigate whether AI can mediate a shift toward a more interdependent relationship between humans and nature.

Method: Analyzing case studies where artists and designers use AI for ecological purposes, and discussing an AI-assisted water remediation prototype.

Result: AI shows promise in enhancing ecological design, linking scientific, artistic, and environmental goals through innovative methods like reinforcement learning and phytoremediation.

Conclusion: AI can serve as a valuable tool in fostering sustainable ecosystems and advancing ecological design paradigms, offering pathways for future innovation.

Abstract: This paper asks whether our relationship with nature can move from human
dominance to genuine interdependence, and whether artificial intelligence (AI)
can mediate that shift. We examine a new ecological-design paradigm in which AI
interacts with non-human life forms. Through case studies we show how artists
and designers apply AI for data analysis, image recognition, and ecological
restoration, producing results that differ from conventional media. We argue
that AI not only expands creative methods but also reframes the theory and
practice of ecological design. Building on the author's prototype for
AI-assisted water remediation, the study proposes design pathways that couple
reinforcement learning with plant-based phytoremediation. The findings
highlight AI's potential to link scientific insight, artistic practice, and
environmental stewardship, offering a roadmap for future research on
sustainable, technology-enabled ecosystems.

</details>


### [2] [General Modular Harness for LLM Agents in Multi-Turn Gaming Environments](https://arxiv.org/abs/2507.11633)
*Yuxuan Zhang,Haoyang Yu,Lanxiang Hu,Haojian Jin,Hao Zhang*

Main category: cs.AI

TL;DR: The paper presents a modular framework for improving LLM agents in various gaming environments without requiring domain-specific modifications.


<details>
  <summary>Details</summary>
Motivation: To design a unified system that enhances the versatility and performance of LLM agents across a range of dynamic and interactive tasks.

Method: The authors introduce a harness composed of perception, memory, and reasoning modules that interfaces with LLM or VLM backbones, tested on game suites to evaluate its effectiveness.

Result: Experiments show significant performance improvements with the harness and highlight how different modules contribute in specific scenarios, such as memory excelling in long puzzles and perception being crucial in noisy visual environments.

Conclusion: The modular harness effectively advances general-purpose agents, leveraging the commonality of gaming tasks to reflect everyday problem-solving.

Abstract: We introduce a modular harness design for LLM agents that composes of
perception, memory, and reasoning components, enabling a single LLM or VLM
backbone to tackle a wide spectrum of multi turn gaming environments without
domain-specific engineering. Using classic and modern game suites as
low-barrier, high-diversity testbeds, our framework provides a unified workflow
for analyzing how each module affects performance across dynamic interactive
settings. Extensive experiments demonstrate that the harness lifts gameplay
performance consistently over un-harnessed baselines and reveals distinct
contribution patterns, for example, memory dominates in long-horizon puzzles
while perception is critical in vision noisy arcades. These findings highlight
the effectiveness of our modular harness design in advancing general-purpose
agent, given the familiarity and ubiquity of games in everyday human
experience.

</details>


### [3] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Main category: cs.AI

TL;DR: The paper explores using Multimodal Large Language Models (MLLMs) as verifiers for agent tasks, and addresses their inherent 'agreement bias' through a novel self-grounded verification (SGV) approach, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of extending AI verifier systems to domains without well-defined success criteria, leveraging MLLMs' understanding and reasoning capabilities.

Method: The authors introduce Self-Grounded Verification (SGV), where MLLMs first generate independent task priors and then evaluate agent trajectories based on these self-derived priors.

Result: SGV-enhanced MLLM verifiers showed up to 20-point improvements in accuracy and surpassed state-of-the-art task completion benchmarks across multiple domains and agent types.

Conclusion: Self-Grounded Verification (SGV) mitigates MLLMs' agreement bias, significantly improving their reliability as task verifiers, and showcases their potential for real-time supervision of AI agents in diverse applications.

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


### [4] [ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making](https://arxiv.org/abs/2507.11733)
*Srikanth Vemula*

Main category: cs.AI

TL;DR: The study introduces ClarifAI, an approach to improve AI transparency in decision-making using Case-Based Reasoning and ontology-driven methods.


<details>
  <summary>Details</summary>
Motivation: To address the need for better transparency and interpretability in AI-powered applications, especially in critical decision-making scenarios.

Method: ClarifAI integrates Case-Based Reasoning with ontology methodologies to offer comprehensive explanation mechanisms and design principles.

Result: The approach enhances AI interpretability across various sectors and showcases its applicability in high-stakes environments.

Conclusion: ClarifAI significantly contributes to advancing AI interpretability, offering practical solutions for deployment in critical decision-making scenarios.

Abstract: This Study introduces Clarity and Reasoning Interface for Artificial
Intelligence(ClarifAI), a novel approach designed to augment the transparency
and interpretability of artificial intelligence (AI) in the realm of improved
decision making. Leveraging the Case-Based Reasoning (CBR) methodology and
integrating an ontology-driven approach, ClarifAI aims to meet the intricate
explanatory demands of various stakeholders involved in AI-powered
applications. The paper elaborates on ClarifAI's theoretical foundations,
combining CBR and ontologies to furnish exhaustive explanation mechanisms. It
further elaborates on the design principles and architectural blueprint,
highlighting ClarifAI's potential to enhance AI interpretability across
different sectors and its applicability in high-stake environments. This
research delineates the significant role of ClariAI in advancing the
interpretability of AI systems, paving the way for its deployment in critical
decision-making processes.

</details>


### [5] [Auto-Formulating Dynamic Programming Problems with Large Language Models](https://arxiv.org/abs/2507.11737)
*Chenyu Zhou,Jingyuan Yang,Linwei Xin,Yitian Chen,Ziyan He,Dongdong Ge*

Main category: cs.AI

TL;DR: The paper introduces DP-Bench for evaluating dynamic programming (DP) problems and proposes the Dynamic Programming Language Model (DPLM), which excels using the novel DualReflect synthetic data generation pipeline.


<details>
  <summary>Details</summary>
Motivation: Dynamic programming is hard to model without expertise, and existing Large Language Models (LLMs) aren't directly applicable due to unique characteristics of DP problems like stochastic transitions and limited data.

Method: The authors developed DP-Bench to systematically analyze DP problems and introduced DPLM, a 7B-parameter model relying on DualReflect, a synthetic data generation pipeline combining forward and backward generation.

Result: DPLM matches state-of-the-art LLMs on most tasks and outperforms them on hard problems, thanks to DualReflect's balance between diversity (forward generation) and reliability (backward generation).

Conclusion: The paper demonstrates the value of combining forward and backward synthetic data generation for training LLMs specialized in DP problems, offering a significant step forward in automating this domain.

Abstract: Dynamic programming (DP) is a fundamental method in operations research, but
formulating DP models has traditionally required expert knowledge of both the
problem context and DP techniques. Large Language Models (LLMs) offer the
potential to automate this process. However, DP problems pose unique challenges
due to their inherently stochastic transitions and the limited availability of
training data. These factors make it difficult to directly apply existing
LLM-based models or frameworks developed for other optimization problems, such
as linear or integer programming. We introduce DP-Bench, the first benchmark
covering a wide range of textbook-level DP problems to enable systematic
evaluation. We present Dynamic Programming Language Model (DPLM), a
7B-parameter specialized model that achieves performance comparable to
state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on
hard problems. Central to DPLM's effectiveness is DualReflect, our novel
synthetic data generation pipeline, designed to scale up training data from a
limited set of initial examples. DualReflect combines forward generation for
diversity and backward generation for reliability. Our results reveal a key
insight: backward generation is favored in low-data regimes for its strong
correctness guarantees, while forward generation, though lacking such
guarantees, becomes increasingly valuable at scale for introducing diverse
formulations. This trade-off highlights the complementary strengths of both
approaches and the importance of combining them.

</details>


### [6] [Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11787)
*Chandrashekar Muniyappa,Eunjin Kim*

Main category: cs.AI

TL;DR: The paper surveys the use of Swarm Intelligence algorithms in searching documents based on semantic similarity and discusses future research directions.


<details>
  <summary>Details</summary>
Motivation: There is increasing interest in leveraging the natural behaviors of animals and insects, mimicked in Swarm Intelligence algorithms, to address challenges such as document semantic similarity search.

Method: The study involves reviewing recent advancements in Swarm Intelligence applied to semantic similarity document searches and analyzing the effectiveness of these techniques.

Result: Swarm Intelligence has proven to be effective for solving semantic similarity problems in document searches, showcasing diverse applications.

Conclusion: Surveying the SI methodologies provides a baseline for understanding current capabilities and identifies opportunities for advancing semantic similarity search in future studies.

Abstract: Swarm Intelligence (SI) is gaining a lot of popularity in artificial
intelligence, where the natural behavior of animals and insects is observed and
translated into computer algorithms called swarm computing to solve real-world
problems. Due to their effectiveness, they are applied in solving various
computer optimization problems. This survey will review all the latest
developments in Searching for documents based on semantic similarity using
Swarm Intelligence algorithms and recommend future research directions.

</details>


### [7] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: The paper explores the use of GPU parallelism in depth-first search algorithms, introducing a cost-bounded method to optimize performance and maintain admissibility.


<details>
  <summary>Details</summary>
Motivation: To leverage the parallel processing capabilities of GPUs and enhance search algorithms with batching techniques, especially in depth-first search.

Method: The authors propose Cost-Bounded Depth-First Search (CB-DFS) that uses GPU batching in depth-first search. They create extended algorithms like Batch IDA* and Batch BTS based on IDA* and Budgeted Tree Search, ensuring optimality.

Result: Experiments on the 3x3 Rubik’s Cube and 4x4 sliding tile puzzle demonstrate efficient GPU batching in DFS, along with extensive analysis on hyperparameters, neural network heuristics, and hardware resources.

Conclusion: The proposed methods successfully integrate GPU parallelism into DFS, providing a foundation for future optimization while preserving traditional guarantees like optimality.

Abstract: The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>


### [8] [Aime: Towards Fully-Autonomous Multi-Agent Framework](https://arxiv.org/abs/2507.11988)
*Yexuan Shi,Mingyu Wang,Yunxiang Cao,Hongjie Lai,Junjian Lan,Xin Han,Yu Wang,Jie Geng,Zhenan Li,Zihao Xia,Xiang Chen,Chen Li,Jian Xu,Wenbo Duan,Yuanshuo Zhu*

Main category: cs.AI

TL;DR: Aime addresses limitations in traditional MAS frameworks by introducing dynamic planning, reactive execution, and adaptive agent instantiation, yielding superior adaptability and performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the rigidity and inefficiency of traditional MAS paradigms, specifically their static plan execution and limited adaptability in dynamic environments.

Method: Aime incorporates three key components: a Dynamic Planner for real-time strategy adjustment, an Actor Factory for on-demand agent creation, and a Progress Management Module for centralized state awareness.

Result: Empirical evaluations on benchmarks in reasoning, software engineering, and web navigation show that Aime outperforms specialized state-of-the-art agents in adaptability and task success rate.

Conclusion: Aime is a robust and effective framework for enhancing multi-agent systems, enabling superior collaboration and adaptability in dynamic scenarios.

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are
emerging as a powerful paradigm for solving complex, multifaceted problems.
However, the potential of these systems is often constrained by the prevalent
plan-and-execute framework, which suffers from critical limitations: rigid plan
execution, static agent capabilities, and inefficient communication. These
weaknesses hinder their adaptability and robustness in dynamic environments.
This paper introduces Aime, a novel multi-agent framework designed to overcome
these challenges through dynamic, reactive planning and execution. Aime
replaces the conventional static workflow with a fluid and adaptive
architecture. Its core innovations include: (1) a Dynamic Planner that
continuously refines the overall strategy based on real-time execution
feedback; (2) an Actor Factory that implements Dynamic Actor instantiation,
assembling specialized agents on-demand with tailored tools and knowledge; and
(3) a centralized Progress Management Module that serves as a single source of
truth for coherent, system-wide state awareness. We empirically evaluated Aime
on a diverse suite of benchmarks spanning general reasoning (GAIA), software
engineering (SWE-bench Verified), and live web navigation (WebVoyager). The
results demonstrate that Aime consistently outperforms even highly specialized
state-of-the-art agents in their respective domains. Its superior adaptability
and task success rate establish Aime as a more resilient and effective
foundation for multi-agent collaboration.

</details>


### [9] [BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2507.12207)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: BuildEvo utilizes Large Language Models (LLMs) to create accurate and explainable energy prediction heuristics, significantly improving generalization and transparency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of traditional heuristics and advanced models in building energy forecasting, specifically their lack of precision, transparency, and neglect of physical principles.

Method: BuildEvo establishes an evolutionary framework where LLMs autonomously design and refine energy prediction heuristics by integrating physical insights derived from building characteristics and operational data.

Result: BuildEvo demonstrates state-of-the-art accuracy and generalization on energy prediction benchmarks, offering interpretability and improved trustworthiness.

Conclusion: The framework enhances the automated creation of reliable and physically informed prediction heuristics, fostering trust in models for complex building energy systems.

Abstract: Accurate building energy forecasting is essential, yet traditional heuristics
often lack precision, while advanced models can be opaque and struggle with
generalization by neglecting physical principles. This paper introduces
BuildEvo, a novel framework that uses Large Language Models (LLMs) to
automatically design effective and interpretable energy prediction heuristics.
Within an evolutionary process, BuildEvo guides LLMs to construct and enhance
heuristics by systematically incorporating physical insights from building
characteristics and operational data (e.g., from the Building Data Genome
Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on
benchmarks, offering improved generalization and transparent prediction logic.
This work advances the automated design of robust, physically grounded
heuristics, promoting trustworthy models for complex energy systems.

</details>


### [10] [Understanding visual attention beehind bee-inspired UAV navigation](https://arxiv.org/abs/2507.11992)
*Pranav Rajbhandari,Abhi Veda,Matthew Garratt,Mandayam Srinivasan,Sridhar Ravi*

Main category: cs.AI

TL;DR: The paper investigates using optic flow as sensory input for UAV navigation, inspired by honeybee behavior. A Reinforcement Learning model mimics insect-like navigation in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: To explore how biological systems like honeybees navigate complex spaces using limited computational resources and optic flow, and apply these insights to UAV navigation.

Method: A Reinforcement Learning agent is trained to navigate obstacle-filled tunnels using optic flow as the sole sensory input. The attentional focus of the trained agent is analyzed to understand its decision-making process.

Result: The agent primarily focuses on regions of optic flow discontinuity and high magnitude, enabling navigation by avoiding obstacles while maintaining a centered path in the environment.

Conclusion: The navigation strategy observed in the trained agents resembles the behavior of flying insects and may serve as the basis for developing a simple control law for UAVs and similar physical systems.

Abstract: Bio-inspired design is often used in autonomous UAV navigation due to the
capacity of biological systems for flight and obstacle avoidance despite
limited sensory and computational capabilities. In particular, honeybees mainly
use the sensory input of optic flow, the apparent motion of objects in their
visual field, to navigate cluttered environments. In our work, we train a
Reinforcement Learning agent to navigate a tunnel with obstacles using only
optic flow as sensory input. We inspect the attention patterns of trained
agents to determine the regions of optic flow on which they primarily base
their motor decisions. We find that agents trained in this way pay most
attention to regions of discontinuity in optic flow, as well as regions with
large optic flow magnitude. The trained agents appear to navigate a cluttered
tunnel by avoiding the obstacles that produce large optic flow, while
maintaining a centered position in their environment, which resembles the
behavior seen in flying insects. This pattern persists across independently
trained agents, which suggests that this could be a good strategy for
developing a simple explicit control law for physical UAVs.

</details>


### [11] [Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs](https://arxiv.org/abs/2507.12110)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: Proposes TPE-MARL for optimizing decision-making in traffic with autonomous vehicles, improving exploration-exploitation trade-off in multi-agent settings.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning struggles with exploration-exploitation trade-offs, particularly in multi-agent scenarios like traffic systems.

Method: Introduces a game topology tensor for dynamic traffic flows and integrates it with QMIX, utilizing visit counts and agent mutual information.

Result: Simulations show enhanced performance in traffic metrics and vehicle behavior, with rationality matching or exceeding human drivers.

Conclusion: TPE-MARL successfully balances exploration and exploitation, optimizing cooperative vehicle decision-making in mixed and autonomous traffic.

Abstract: The exploration-exploitation trade-off constitutes one of the fundamental
challenges in reinforcement learning (RL), which is exacerbated in multi-agent
reinforcement learning (MARL) due to the exponential growth of joint
state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)
method for optimizing cooperative decision-making of connected and autonomous
vehicles (CAVs) in mixed traffic. This work presents two primary contributions:
First, we construct a game topology tensor for dynamic traffic flow,
effectively compressing high-dimensional traffic state information and decrease
the search space for MARL algorithms. Second, building upon the designed game
topology tensor and using QMIX as the backbone RL algorithm, we establish a
topology-enhanced MARL framework incorporating visit counts and agent mutual
information. Extensive simulations across varying traffic densities and CAV
penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations
encompassing training dynamics, exploration patterns, macroscopic traffic
performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL
successfully balances exploration and exploitation. Consequently, it exhibits
superior performance in terms of traffic efficiency, safety, decision
smoothness, and task completion. Furthermore, the algorithm demonstrates
decision-making rationality comparable to or exceeding that of human drivers in
both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is
available at
\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.

</details>


### [12] [Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation](https://arxiv.org/abs/2507.12186)
*Edward Kim,Hanna Kurniawati*

Main category: cs.AI

TL;DR: The paper introduces an anytime online method to solve POMDPs efficiently by deep, meaningful history sampling and gradual policy updates, with proven performance bound advantages over conventional approaches.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving large-scale POMDPs under dynamically evolving environments, particularly under conditions of sparse sampling in online planning.

Method: Develops the Partially Observable Reference Policy Programming algorithm that ensures gradual policy updates while deeply sampling future histories and provides theoretical guarantees regarding performance loss bounds.

Result: Empirical evaluations show the proposed solver significantly outperforms existing online benchmarks in complex scenarios, such as a helicopter emergency situation in Corsica.

Conclusion: The proposed solver offers both theoretical performance guarantees and practical efficiency, making it superior to current methods for large-scale dynamic POMDP problems.

Abstract: This paper proposes Partially Observable Reference Policy Programming, a
novel anytime online approximate POMDP solver which samples meaningful future
histories very deeply while simultaneously forcing a gradual policy update. We
provide theoretical guarantees for the algorithm's underlying scheme which say
that the performance loss is bounded by the average of the sampling
approximation errors rather than the usual maximum, a crucial requirement given
the sampling sparsity of online planning. Empirical evaluations on two
large-scale problems with dynamically evolving environments -- including a
helicopter emergency scenario in the Corsica region requiring approximately 150
planning steps -- corroborate the theoretical results and indicate that our
solver considerably outperforms current online benchmarks.

</details>


### [13] [Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning](https://arxiv.org/abs/2507.12215)
*Yuhao Chen,Shuochen Liu,Yuanjie Lyu,Chao Zhang,Jiayao Shi,Tong Xu*

Main category: cs.AI

TL;DR: This paper proposes a specialized training framework for Large Language Models in playing Chinese Chess to address their limitations in spatial strategic reasoning. A novel model, Xiangqi-R1, showcases improved legality and strategic accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance the spatial strategic reasoning capabilities of Large Language Models in complex and fully observable board games like Chinese Chess.

Method: The paper introduces a multi-stage training framework that includes fine-tuning for legal moves, strategic decision-making enhancements, and reinforcement learning using Group Relative Policy Optimization with multi-dimensional reward signals.

Result: The presented model, Xiangqi-R1, outperforms general-purpose LLMs with an 18% improvement in move legality and a 22% gain in strategic analysis accuracy for board moves in Chinese Chess.

Conclusion: The results demonstrate the potential for creating general strategic intelligence systems in spatially complex domains using specialized training frameworks, moving beyond traditional general-purpose LLM capabilities.

Abstract: Game playing has long served as a fundamental benchmark for evaluating
Artificial General Intelligence (AGI). While Large Language Models (LLMs) have
demonstrated impressive capabilities in general reasoning, their effectiveness
in spatial strategic reasoning, which is critical for complex and fully
observable board games, remains insufficiently explored. In this work, we adopt
Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate
rules and spatial complexity. To advance LLMs' strategic competence in such
environments, we propose a training framework tailored to Xiangqi, built upon a
large-scale dataset of five million board-move pairs enhanced with expert
annotations and engine evaluations. Building on this foundation, we introduce
Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning
for legal move prediction to capture basic spatial rules, (2) incorporating
strategic annotations to improve decision-making, and (3) applying
reinforcement learning via Group Relative Policy Optimization (GRPO) with
multi-dimensional reward signals to enhance reasoning stability. Our
Experimental results indicate that, despite their size and power,
general-purpose LLMs struggle to achieve satisfactory performance in these
tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an
18% rise in move legality and a 22% boost in analysis accuracy. Our results
point to a promising path for creating general strategic intelligence in
spatially complex areas.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Double Duty: FPGA Architecture to Enable Concurrent LUT and Adder Chain Usage](https://arxiv.org/abs/2507.11709)
*Junius Pun,Xilai Dai,Grace Zgheib,Mahesh A. Iyer,Andrew Boutros,Vaughn Betz,Mohamed S. Abdelfattah*

Main category: cs.AR

TL;DR: The paper introduces the Double Duty logic block architecture for FPGAs to enable concurrent use of adders and LUTs within logic elements, achieving up to 21.6% area reduction in adder-intensive circuits without delay impact.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in modern FPGA logic block architectures that restrict the independent and concurrent use of adders and LUTs, which hinders area optimization in arithmetic-intensive applications.

Method: The authors propose modifications to the logic block architecture by reusing 4 existing inputs to bypass LUTs and connect directly to adder chains, modeling these changes through circuit and CAD tools.

Result: The Double Duty architecture achieves 21.6% area reduction in adder-heavy circuits, and 9.3% and 8.2% reductions in general benchmarks, without impacting critical path delay.

Conclusion: The proposed architecture enhances FPGA flexibility and arithmetic density, improving area-delay product by 9.7%, showing that adopting input bypassing for adder chains is a viable optimization strategy.

Abstract: Flexibility and customization are key strengths of Field-Programmable Gate
Arrays (FPGAs) when compared to other computing devices. For instance, FPGAs
can efficiently implement arbitrary-precision arithmetic operations, and can
perform aggressive synthesis optimizations to eliminate ineffectual operations.
Motivated by sparsity and mixed-precision in deep neural networks (DNNs), we
investigate how to optimize the current logic block architecture to increase
its arithmetic density. We find that modern FPGA logic block architectures
prevent the independent use of adder chains, and instead only allow adder chain
inputs to be fed by look-up table (LUT) outputs. This only allows one of the
two primitives -- either adders or LUTs -- to be used independently in one
logic element and prevents their concurrent use, hampering area optimizations.
In this work, we propose the Double Duty logic block architecture to enable the
concurrent use of the adders and LUTs within a logic element. Without adding
expensive logic cluster inputs, we use 4 of the existing inputs to bypass the
LUTs and connect directly to the adder chain inputs. We accurately model our
changes at both the circuit and CAD levels using open-source FPGA development
tools. Our experimental evaluation on a Stratix-10-like architecture
demonstrates area reductions of 21.6% on adder-intensive circuits from the
Kratos benchmarks, and 9.3% and 8.2% on the more general Koios and VTR
benchmarks respectively. These area improvements come without an impact to
critical path delay, demonstrating that higher density is feasible on modern
FPGA architectures by adding more flexibility in how the adder chain is used.
Averaged across all circuits from our three evaluated benchmark set, our Double
Duty FPGA architecture improves area-delay product by 9.7%.

</details>


### [15] [MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments](https://arxiv.org/abs/2507.12028)
*Soheil Mahdizadeh,Elyas Oustad,Mohsen Ansari*

Main category: cs.AR

TL;DR: The paper introduces MOFCO, a task offloading algorithm tailored to three-layer fog computing, aiming to reduce system costs amid user mobility.


<details>
  <summary>Details</summary>
Motivation: User mobility in fog computing leads to costly service migrations and system performance degradation, creating the need for efficient task offloading methods.

Method: The authors model the task offloading as a Mixed-Integer Nonlinear Programming problem and utilize a heuristic-enhanced evolutionary game theory approach for efficient solutions.

Result: Through simulations with realistic user mobility patterns, MOFCO demonstrates an average system cost reduction of 19% and up to 43% in certain scenarios compared to existing approaches.

Conclusion: MOFCO effectively addresses mobility and migration challenges in fog computing, enhancing task offloading while significantly reducing system cost.

Abstract: Task offloading in three-layer fog computing environments presents a critical
challenge due to user equipment (UE) mobility, which frequently triggers costly
service migrations and degrades overall system performance. This paper
addresses this problem by proposing MOFCO, a novel Mobility- and
Migration-aware Task Offloading algorithm for Fog Computing environments. The
proposed method formulates task offloading and resource allocation as a
Mixed-Integer Nonlinear Programming (MINLP) problem and employs a
heuristic-aided evolutionary game theory approach to solve it efficiently. To
evaluate MOFCO, we simulate mobile users using SUMO, providing realistic
mobility patterns. Experimental results show that MOFCO reduces system cost,
defined as a combination of latency and energy consumption, by an average of
19% and up to 43% in certain scenarios compared to state-of-the-art methods.

</details>


### [16] [High-Performance Pipelined NTT Accelerators with Homogeneous Digit-Serial Modulo Arithmetic](https://arxiv.org/abs/2507.12418)
*George Alexakis,Dimitrios Schoinianakis,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: The paper proposes a design for optimized NTT accelerators using digit-serial arithmetic and redundant data representation to enhance performance in hardware implementations for FHE.


<details>
  <summary>Details</summary>
Motivation: Efficient NTT computation is essential for improving the performance of fully homomorphic encryption (FHE). However, current hardware implementations face challenges around low clock frequencies and increased hardware costs due to large moduli requirements.

Method: The paper combines digit-serial modular arithmetic with redundant data representation to create a modular pipelined NTT accelerator. This design eliminates the need for intermediate (de)serialization and optimizes performance through regular pipelining and parallelism.

Result: The proposed architecture achieves higher clock frequencies, lower hardware complexity, and outperforms existing state-of-the-art NTT implementations under comparable constraints.

Conclusion: This work introduces a novel NTT accelerator that leverages innovative arithmetic methods, enabling efficient, high-performance hardware for realistic FHE applications while minimizing complexity.

Abstract: The Number Theoretic Transform (NTT) is a fundamental operation in
privacy-preserving technologies, particularly within fully homomorphic
encryption (FHE). The efficiency of NTT computation directly impacts the
overall performance of FHE, making hardware acceleration a critical technology
that will enable realistic FHE applications. Custom accelerators, in FPGAs or
ASICs, offer significant performance advantages due to their ability to exploit
massive parallelism and specialized optimizations. However, the operation of
NTT over large moduli requires large word-length modulo arithmetic that limits
achievable clock frequencies in hardware and increases hardware area costs. To
overcome such deficits, digit-serial arithmetic has been explored for modular
multiplication and addition independently. The goal of this work is to leverage
digit-serial modulo arithmetic combined with appropriate redundant data
representation to design modular pipelined NTT accelerators that operate
uniformly on arbitrary small digits, without the need for intermediate
(de)serialization. The proposed architecture enables high clock frequencies
through regular pipelining while maintaining parallelism. Experimental results
demonstrate that the proposed approach outperforms state-of-the-art
implementations and reduces hardware complexity under equal performance and
input-output bandwidth constraints.

</details>


### [17] [Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length](https://arxiv.org/abs/2507.12442)
*Saptarshi Mitra,Rachid Karami,Haocheng Xu,Sitao Huang,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: The paper compares Transformer, State Space Models (SSMs), and hybrid architectures for long-context inference on consumer GPUs, finding SSMs to be superior in processing longer sequences efficiently.


<details>
  <summary>Details</summary>
Motivation: The need for machine intelligence systems capable of handling lengthy, context-rich inputs efficiently on local devices is growing, but Transformer architectures struggle due to quadratic complexity and high memory requirements.

Method: The authors systematically benchmark and analyze selected Transformer, SSM, and hybrid models' performances for long-context inference, focusing on consumer and embedded GPUs, including operator-level performance analysis.

Result: SSMs outperform Transformers in long-context processing, handling sequences up to 220K tokens (~4x longer) on a 24GB GPU. SSMs are faster for lengthy contexts (~57K tokens), reversing shorter-context performance trends where Transformers excel.

Conclusion: SSMs are shown to be a superior choice for long-context tasks. Optimizing hardware-aware SSM kernels and system co-design can further enhance performance, and the open-sourced framework will enable further research in this domain.

Abstract: The demand for machine intelligence capable of processing continuous,
long-context inputs on local devices is growing rapidly. However, the quadratic
complexity and memory requirements of traditional Transformer architectures
make them inefficient and often unusable for these tasks. This has spurred a
paradigm shift towards new architectures like State Space Models (SSMs) and
hybrids, which promise near-linear scaling. While most current research focuses
on the accuracy and theoretical throughput of these models, a systematic
performance characterization on practical consumer hardware is critically
needed to guide system-level optimization and unlock new applications.
  To address this gap, we present a comprehensive, comparative benchmarking of
carefully selected Transformer, SSM, and hybrid models specifically for
long-context inference on consumer and embedded GPUs. Our analysis reveals that
SSMs are not only viable but superior for this domain, capable of processing
sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than
comparable Transformers. While Transformers may be up to 1.8x faster at short
sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x
faster at very long contexts (~57K tokens). Our operator-level analysis reveals
that custom, hardware-aware SSM kernels dominate the inference runtime,
accounting for over 55% of latency on edge platforms, identifying them as a
primary target for future hardware acceleration. We also provide detailed,
device-specific characterization results to guide system co-design for the
edge. To foster further research, we will open-source our characterization
framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [18] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

TL;DR: The paper investigates large language models' (LLMs) subjective nature in evaluating Japanese science fiction stories, revealing varied aesthetic patterns and biases.


<details>
  <summary>Details</summary>
Motivation: To study the potential for LLMs to function as subjective literary critics, exploring their aesthetic preferences, evaluation patterns, and biases.

Method: Translated ten Japanese science fiction stories into English and evaluated them with six state-of-the-art LLMs in seven independent sessions. Used PCA, clustering, and TF-IDF analysis to assess evaluation consistency, patterns, and vocabularies.

Result: Revealed significant evaluation variances (consistencies ranging from 1.00 to 0.35), five distinct evaluation patterns, and diverse vocabulary usage across LLMs for literary judgment.

Conclusion: LLMs exhibit individual aesthetic preferences and implicit value systems, likening them to human critical schools rather than neutral evaluators.

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [19] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: This paper introduces MapIQ, a benchmark dataset for evaluating multimodal large language models (MLLMs) on various types of maps with diverse themes and tasks.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the limited research focus on diverse map types and analytical tasks in the study of MLLMs' visual question-answering capabilities, particularly beyond choropleth maps.

Method: The authors created a dataset named MapIQ, comprising 14,706 question-answer pairs spanning three map types across six thematic topics. They evaluated MLLMs on six visual analytical tasks, compared their performance against humans, and analyzed the impact of map design changes.

Result: Experiments revealed insights into the robustness and sensitivity of MLLMs to map design variations and highlighted their reliance on internal geographic knowledge.

Conclusion: MapIQ serves as a comprehensive benchmark to assess and improve the performance of MLLMs on diverse map types, offering directions for advancing Map-VQA research and addressing design-related challenges.

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [20] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

TL;DR: This paper explores the use of few-shot and incremental learning for Persian sentiment analysis, achieving 96% accuracy using multilingual models like mDeBERTa and XLM-RoBERTa.


<details>
  <summary>Details</summary>
Motivation: There is a need to develop efficient sentiment analysis systems for low-resource languages like Persian by leveraging knowledge from high-resource languages.

Method: The authors fine-tuned three multilingual models (XLM-RoBERTa, mDeBERTa, DistilBERT) using few-shot and incremental learning with small, diverse datasets in Persian.

Result: mDeBERTa and XLM-RoBERTa models achieved high performance, with 96% accuracy for Persian sentiment analysis.

Conclusion: The results demonstrate the effectiveness of combining few-shot and incremental learning with multilingual pre-trained models for low-resource language applications.

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [21] [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)
*Yuhong Zhang,Jialu Li,Shilai Yang,Yuchen Xu,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.CL

TL;DR: The paper explores the comparison of human and LLMs' reading comprehension using graph-based semantic representations, validated through eye-tracking data.


<details>
  <summary>Details</summary>
Motivation: To understand how both humans and LLMs process reading comprehension, specifically in functional tasks like inference and information retrieval, and to build on limitations from prior studies that analyzed only individual words.

Method: A graph-based semantic text representation created by an LLM AI agent is used, grouping words into nodes and edges. These were analyzed alongside human eye-tracking fixation data to draw comparisons.

Result: LLMs show high consistency in language understanding when analyzed via graph topological structures, validated against human eye-tracking data distribution.

Conclusion: The study enhances understanding of human-LLM language comprehension and supports the development of effective co-learning strategies between humans and AI.

Abstract: Reading comprehension is a fundamental skill in human cognitive development.
With the advancement of Large Language Models (LLMs), there is a growing need
to compare how humans and LLMs understand language across different contexts
and apply this understanding to functional tasks such as inference, emotion
interpretation, and information retrieval. Our previous work used LLMs and
human biomarkers to study the reading comprehension process. The results showed
that the biomarkers corresponding to words with high and low relevance to the
inference target, as labeled by the LLMs, exhibited distinct patterns,
particularly when validated using eye-tracking data. However, focusing solely
on individual words limited the depth of understanding, which made the
conclusions somewhat simplistic despite their potential significance. This
study used an LLM-based AI agent to group words from a reading passage into
nodes and edges, forming a graph-based text representation based on semantic
meaning and question-oriented prompts. We then compare the distribution of eye
fixations on important nodes and edges. Our findings indicate that LLMs exhibit
high consistency in language understanding at the level of graph topological
structure. These results build on our previous findings and offer insights into
effective human-AI co-learning strategies.

</details>


### [22] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

TL;DR: The paper introduces PgM, a method to improve multimodal learning by splitting and learning representations into uni-modal and paired-modal features.


<details>
  <summary>Details</summary>
Motivation: Enhance the understanding and integration of uni-modal and paired-modal features to improve multimodal learning outcomes.

Method: Proposed the PgM framework, which includes a modal partitioner to separate features, along with distinct learning and decoding components for uni-modal and paired-modal features.

Result: PgM showed effective performance across four multimodal tasks, demonstrated transferability, and provided feature distribution insights.

Conclusion: PgM thoroughly learns and adjusts uni-modal and paired-modal representations, making it valuable for various multimodal tasks and adaptable to existing models.

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [23] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Main category: cs.CL

TL;DR: This paper explores the limitations of existing Large Language Models (LLMs) for VHDL (a hardware description language) tasks like code generation and summarization, and proposes a novel approach called Chain-of-Descriptions (CoDes) to enhance performance in these tasks.


<details>
  <summary>Details</summary>
Motivation: Despite the widespread use of LLMs for general code-related tasks, their performance remains inadequate for hardware description languages like VHDL. This gap highlights the need for tailored techniques to address the domain-specific challenges in Electronic Design Automation.

Method: The authors introduced the Chain-of-Descriptions (CoDes) approach, which breaks tasks into intermediate descriptive steps derived from either the problem statement (for code generation) or the VHDL code itself (for summarization). These steps are combined with the original input to refine the prompt for the LLM.

Result: The CoDes approach demonstrates significant improvements in performance metrics across two datasets, VHDL-Eval and VHDL-Xform, outperforming standard prompting strategies in generating and summarizing VHDL code.

Conclusion: The proposed CoDes method not only advances LLM performance for VHDL-related tasks but also provides a foundation for future research to further specialize and improve LLMs within this domain.

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [24] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

TL;DR: The ExpliCIT-QA system enhances tabular question answering by combining multimodal input processing, step-by-step reasoning, automated code execution, and clear explanations, aiming for transparency and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the explainability and transparency gap in TableVQA systems, which is crucial for applications in sensitive areas like finance and healthcare.

Method: ExpliCIT-QA employs a modular design including multimodal table understanding, natural language reasoning, Python/Pandas code generation with error feedback, code execution, and natural language explanation.

Result: ExpliCIT-QA demonstrated improved interpretability and performance on the TableVQA-Bench benchmark compared to existing baselines.

Conclusion: This system advances the explainability and auditability of TableVQA systems, facilitating their adoption in critical domains where reliable and transparent results are necessary.

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [25] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

TL;DR: The paper presents a novel approach to understand Python data science notebooks by creating information flow graphs and execution dependencies using a method called Capture and Resolve Assisted Bounding Strategy (CRABS).


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle to fully comprehend realistic Python notebooks due to hallucinations and long-context issues, leading to the need for better notebook analysis methods.

Method: The proposed method, CRABS, combines shallow syntactic parsing with LLM-based zero-shot learning to accurately interpret notebook cells' inputs and outputs, creating detailed information flow and dependency graphs.

Result: When tested on 50 annotated Kaggle notebooks, CRABS achieved an F1 score of 98% for cell-to-cell information flows and 99% for transitive execution dependencies, resolving 98% of ambiguities left by syntactic analysis.

Conclusion: CRABS effectively addresses the limitations of LLMs in notebook comprehension by combining syntactic analysis with LLM-assisted ambiguity resolution, enabling precise understanding of notebook data and execution flows.

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [26] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

TL;DR: The paper describes AI Wizards' approach to categorizing sentences as subjective or objective in news articles, using sentiment-augmented transformer models in various languages.


<details>
  <summary>Details</summary>
Motivation: To develop a robust system for subjective vs. objective sentence classification, particularly in unseen languages, and to address challenges like class imbalance.

Method: The authors enhanced transformer-based models by integrating sentiment scores into sentence representations and utilized decision threshold calibration to mitigate class imbalance.

Result: Their approach significantly improved performance, particularly on the subjective F1 score, leading to high rankings and a 1st place finish for Greek (Macro F1 = 0.51).

Conclusion: Augmenting transformer classifiers with sentiment features and addressing class imbalance can considerably improve results in multilingual and zero-shot settings.

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [27] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: The study evaluates how LLMs handle factual and counterfactual data using attention heads, finding that attention behavior is domain-sensitive and driven by general rather than selective suppression.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs balance factual accuracy against contradictory contexts and reconcile findings from previous studies using mechanistic interpretability.

Method: The study examines attention head strength and factual output ratios, evaluates hypotheses about suppression mechanisms, and analyzes domain specificity in LLMs’ attention patterns.

Result: Findings indicate that attention heads suppress counterfactuals via general mechanisms rather than selectively, and their behavior is influenced by domain and model size, with larger models showing specialized patterns.

Conclusion: Attention heads manage factual output through broad suppression mechanisms, and their behavior varies across domains and model scales, contributing to understanding LLM interpretability.

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [28] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
*Yash Ingle,Pruthwik Mishra*

Main category: cs.CL

TL;DR: The paper focuses on the language identification task in NLP, particularly addressing the challenges in Indian languages by providing a new dataset and robust baseline models.


<details>
  <summary>Details</summary>
Motivation: The language identification task is fundamental for NLP applications, especially in complex scenarios like noisy, short, and code-mixed texts involving Indian languages with unique challenges such as script sharing.

Method: The authors created a dataset of 230K sentences spanning English and 22 Indian languages, with most data newly developed, and implemented state-of-the-art machine learning and deep learning techniques to build baseline models.

Result: The resulting dataset and baseline models are comparable to state-of-the-art methodologies, providing reliable tools for language identification research.

Conclusion: This research enhances language identification capabilities by addressing the unique complexities of Indian languages and offering both data and models to advance the field.

Abstract: The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

</details>


### [29] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: The paper introduces a new framework to enable autoregressive language models to predict multiple tokens at once, increasing inference speed without compromising quality.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of autoregressive language models generating tokens sequentially, which hampers speed and parallelism despite textual certainty during later stages.

Method: The framework employs a masked-input setup, gated LoRA for multi-token prediction, a lightweight sampler, auxiliary training losses, and speculative generation to enhance token prediction and generation speed.

Result: The approach significantly speeds up language model tasks, achieving up to 5x acceleration in code/math generation and 2.5x in general tasks without quality loss.

Conclusion: The proposed innovations enable faster, efficient multi-token predictions in autoregressive models while maintaining output quality.

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [30] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
*Junhong Ye,Xu Yuan,Xinying Qiu*

Main category: cs.CL

TL;DR: The paper studies methods to improve PII recognition through cross-domain transfer, data fusion, and sample-efficient learning.


<details>
  <summary>Details</summary>
Motivation: PII recognition is crucial for automated anonymization, and the paper aims to explore transferability and efficiency in learning across domains.

Method: The study evaluates models using corpora from healthcare, legal, and biographical texts across four dimensions: in-domain performance, cross-domain transferability, fusion, and few-shot learning.

Result: Legal-text data transfers effectively to biographies, medical data resists transfers, and benefits of fusion are domain-specific. High recognition can be achieved with limited data in less specialized domains.

Conclusion: PII recognition effectiveness varies by domain, and sample-efficient methods work well in less specialized contexts.

Abstract: Accurate recognition of personally identifiable information (PII) is central
to automated text anonymization. This paper investigates the effectiveness of
cross-domain model transfer, multi-domain data fusion, and sample-efficient
learning for PII recognition. Using annotated corpora from healthcare (I2B2),
legal (TAB), and biography (Wikipedia), we evaluate models across four
dimensions: in-domain performance, cross-domain transferability, fusion, and
few-shot learning. Results show legal-domain data transfers well to
biographical texts, while medical domains resist incoming transfer. Fusion
benefits are domain-specific, and high-quality recognition is achievable with
only 10% of training data in low-specialization domains.

</details>


### [31] [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)
*Xiangyu Yang,Xinying Qiu*

Main category: cs.CL

TL;DR: The paper explores a unified framework, COLA-GEC, to enhance grammatical error correction (GEC) and acceptability judgment (COLA) by mutual knowledge sharing, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: GEC and COLA are interrelated tasks but often developed separately. The paper aims to improve both tasks via a collaborative approach that leverages shared grammatical knowledge.

Method: The framework combines GEC datasets to enhance COLA models and employs grammatical acceptability signals in GEC training using a dynamic loss function.

Result: The proposed approach delivers state-of-the-art performance on several multilingual benchmarks and identifies challenges in punctuation error correction.

Conclusion: COLA-GEC successfully bridges GEC and COLA, improving grammatical modeling through mutual knowledge transfer, with potential for future refinements.

Abstract: Grammatical Error Correction (GEC) and grammatical acceptability judgment
(COLA) are core tasks in natural language processing, sharing foundational
grammatical knowledge yet typically evolving independently. This paper
introduces COLA-GEC, a novel bidirectional framework that enhances both tasks
through mutual knowledge transfer. First, we augment grammatical acceptability
models using GEC datasets, significantly improving their performance across
multiple languages. Second, we integrate grammatical acceptability signals into
GEC model training via a dynamic loss function, effectively guiding corrections
toward grammatically acceptable outputs. Our approach achieves state-of-the-art
results on several multilingual benchmarks. Comprehensive error analysis
highlights remaining challenges, particularly in punctuation error correction,
providing insights for future improvements in grammatical modeling.

</details>


### [32] [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)
*Tianyou Huang,Xinglu Chen,Jingshen Zhang,Xinying Qiu,Ruiying Niu*

Main category: cs.CL

TL;DR: The paper presents DualReward, a reinforcement learning system for creating distractors in cloze tests, using a dual reward structure with adaptive scaling.


<details>
  <summary>Details</summary>
Motivation: To improve automatic distractor generation for cloze tests by addressing the limitations of static models and supervised learning approaches.

Method: The DualReward framework uses a dual reward structure with adaptive scaling to balance learning from human-created distractors and exploring model-generated ones. It dynamically adjusts reward signals based on the model performance and confidence.

Result: The framework outperforms state-of-the-art baselines, with modest benefits on homogeneous datasets (CLOTH-F) and significant improvements (3.48-3.86% in P@1) on diverse datasets (MCQ).

Conclusion: DualReward serves as a flexible and effective solution for balancing reliable distractor creation and exploring novel options, especially in varied and cross-domain scenarios.

Abstract: This paper introduces DualReward, a novel reinforcement learning framework
for automatic distractor generation in cloze tests. Unlike conventional
approaches that rely primarily on supervised learning or static generative
models, our method employs a dual reward structure with adaptive scaling that
differentiates between human-created gold standard distractors and
model-generated candidates. The framework dynamically adjusts reward signal
intensity based on model performance and confidence. We evaluate our approach
on both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,
demonstrating consistent improvements over state-of-the-art baselines.
Experimental results show that our adaptive reward scaling mechanism provides
modest but consistent benefits on homogeneous datasets (CLOTH-F) and more
substantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data
(MCQ), suggesting its particular effectiveness for handling varied question
types and domains. Our work offers a flexible framework that effectively
balances learning from reliable human examples while exploring novel,
high-quality distractors for automated test generation.

</details>


### [33] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: BlockBPE introduces a GPU-optimized tokenization implementation for byte-pair encoding, addressing inefficiencies in widely-used CPU-bound tokenizers, achieving higher throughput for batch inference.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of CPU-bound tokenization implementations, particularly their inefficiency for batch inference workflows on GPUs, and address the high complexity of widely used methods reliant on Regex pre-tokenization.

Method: BlockBPE eliminates the use of Regex pre-tokenization, ensuring high parallelization of token merges within thread blocks. This leads to reduced runtime complexity from $O(n \log n)$ to $O(nd)$ where $d \ll n$.

Result: BlockBPE achieves up to 2x higher throughput compared to tiktoken and up to 2.5x compared to HuggingFace Tokenizers in high-batch inference workloads.

Conclusion: BlockBPE provides a faster, more efficient GPU-based tokenization approach for byte-pair encoding, offering significant throughput improvements over existing implementations while maintaining nearly linear-time complexity under realistic conditions.

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [34] [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
*Jiachen Zhao,Jing Huang,Zhengxuan Wu,David Bau,Weiyan Shi*

Main category: cs.CL

TL;DR: This paper investigates the internal distinction between LLMs' harmfulness understanding and refusal mechanisms and develops a practical safety application using this knowledge.


<details>
  <summary>Details</summary>
Motivation: Current LLMs refuse to engage with harmful instructions, but it is unclear if this refusal behavior stems from a true understanding of harmfulness or simple programmed responses.

Method: The authors identify two separate dimensions in the LLMs' mechanisms—'refusal direction' and 'harmfulness direction'—and analyze how steering along these directions affects model behavior. They use causal evidence to demonstrate the distinct nature of these dimensions and propose a new safeguard mechanism called 'Latent Guard.'

Result: The study finds that a latent, internal understanding of harmfulness in LLMs is distinct and more robust than their external refusal behavior. Latent Guard, derived from this understanding, is shown to detect unsafe inputs effectively and handle jailbreaks better than dedicated fine-tuned models.

Conclusion: LLMs possess an intrinsic and robust concept of harmfulness, separate from refusal behaviors. This internal mechanism can be leveraged as an effective safeguard against unsafe instructions and adversarial attacks.

Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand
harmfulness beyond just refusing? Prior work has shown that LLMs' refusal
behaviors can be mediated by a one-dimensional subspace, i.e., a refusal
direction. In this work, we identify a new dimension to analyze safety
mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a
separate concept from refusal. There exists a harmfulness direction that is
distinct from the refusal direction. As causal evidence, steering along the
harmfulness direction can lead LLMs to interpret harmless instructions as
harmful, but steering along the refusal direction tends to elicit refusal
responses directly without reversing the model's judgment on harmfulness.
Furthermore, using our identified harmfulness concept, we find that certain
jailbreak methods work by reducing the refusal signals without reversing the
model's internal belief of harmfulness. We also find that adversarially
finetuning models to accept harmful instructions has minimal impact on the
model's internal belief of harmfulness. These insights lead to a practical
safety application: The model's latent harmfulness representation can serve as
an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing
over-refusals that is robust to finetuning attacks. For instance, our Latent
Guard achieves performance comparable to or better than Llama Guard 3 8B, a
dedicated finetuned safeguard model, across different jailbreak methods. Our
findings suggest that LLMs' internal understanding of harmfulness is more
robust than their refusal decision to diverse input instructions, offering a
new perspective to study AI safety

</details>


### [35] [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)
*Bo Zeng,Chenyang Lyu,Sinuo Liu,Mingyan Zeng,Minghao Wu,Xuanfan Ni,Tianqi Shi,Yu Zhao,Yefeng Liu,Chenyu Zhu,Ruizhe Li,Jiahui Geng,Qing Li,Yu Tong,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: The paper introduces Marco-Bench-MIF, a multilingual framework for evaluating instruction-following capabilities in LLMs, across 30 languages with localized adaptations, addressing linguistic and cultural constraints.


<details>
  <summary>Details</summary>
Motivation: Current evaluations for instruction-following in LLMs are overly English-centric or rely on simple translations, which fail to capture linguistic and cultural nuances in multilingual settings.

Method: The researchers extended the IFEval dataset by curating a multilingual version (Marco-Bench-MIF) with localization and employed a hybrid pipeline of translation and human verification to adapt content for 30 languages.

Result: They evaluated 20+ LLMs and found significant accuracy gaps across resource languages (25-35%), performance dependency on scale (45-60%), and identified inaccuracies (7-22%) in machine-translated benchmarks.

Conclusion: Marco-Bench-MIF highlights the importance of localized datasets for accurately evaluating LLMs’ multilingual capabilities and uncovers persistent challenges in preserving linguistic consistency and handling compositional constraints.

Abstract: Instruction-following capability has become a major ability to be evaluated
for Large Language Models (LLMs). However, existing datasets, such as IFEval,
are either predominantly monolingual and centered on English or simply machine
translated to other languages, limiting their applicability in multilingual
contexts. In this paper, we present an carefully-curated extension of IFEval to
a localized multilingual version named Marco-Bench-MIF, covering 30 languages
with varying levels of localization. Our benchmark addresses linguistic
constraints (e.g., modifying capitalization requirements for Chinese) and
cultural references (e.g., substituting region-specific company names in
prompts) via a hybrid pipeline combining translation with verification. Through
comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)
25-35% accuracy gap between high/low-resource languages, (2) model scales
largely impact performance by 45-60% yet persists script-specific challenges,
and (3) machine-translated data underestimates accuracy by7-22% versus
localized data. Our analysis identifies challenges in multilingual instruction
following, including keyword consistency preservation and compositional
constraint adherence across languages. Our Marco-Bench-MIF is available at
https://github.com/AIDC-AI/Marco-Bench-MIF.

</details>


### [36] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: This paper surveys the use of deep learning, including multimodal large language models, in geometry problem solving.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview and practical reference on applying deep learning to solving geometry problems, promoting further advancements.

Method: A detailed review of tasks, methods, evaluation metrics, and future directions in deep learning for geometry problem solving.

Result: A structured summary of research advancements in the field and a curated GitHub list of relevant papers for continued updates.

Conclusion: The survey highlights achievements, challenges, and actionable directions, aiming to foster progress in deep learning applications for geometry problem solving.

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [37] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: The paper introduces PolyChartQA, a multilingual benchmark for question answering on charts, covering 10 languages with 22,606 charts and 26,151 QA pairs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of multilingual resources in chart understanding benchmarks, which are primarily English-centric, limiting global applicability.

Method: The proposed decoupled pipeline separates chart data and rendering code, enabling language translation and consistent multilingual chart generation. State-of-the-art LLM-based translation and quality control measures are used.

Result: Experiments show a large performance discrepancy between English and other languages, particularly for low-resource languages with non-Latin scripts.

Conclusion: PolyChartQA lays the groundwork for developing vision-language models with a global, multilingual perspective.

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [38] [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)
*Yi Zhao,Zuchao Li,Hai Zhao,Baoyuan Qi,Guoming Liu*

Main category: cs.CL

TL;DR: The paper presents DAC, a dynamic attention-aware approach for task-agnostic prompt compression, combining entropy and attention information to enhance prompt efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations of existing prompt compression methods by considering attention dependencies and entropy shifts during the compression process.

Method: The authors propose DAC, which integrates entropy and attention metrics to dynamically adapt compression based on entropy variations, ensuring optimal compression for long-context scenarios.

Result: DAC shows consistent and significant improvements across diverse tasks and large language models (LLMs) in experimental benchmarks such as LongBench, GSM8K, and BBH.

Conclusion: Dynamic integration of attention and entropy metrics offers robust prompt compression, enhancing computational efficiency and preserving task performance across varied scenarios.

Abstract: Task-agnostic prompt compression leverages the redundancy in natural language
to reduce computational overhead and enhance information density within
prompts, especially in long-context scenarios. Existing methods predominantly
rely on information entropy as the metric to compress lexical units, aiming to
achieve minimal information loss. However, these approaches overlook two
critical aspects: (i) the importance of attention-critical tokens at the
algorithmic level, and (ii) shifts in information entropy during the
compression process. Motivated by these challenges, we propose a dynamic
attention-aware approach for task-agnostic prompt compression (DAC). This
approach effectively integrates entropy and attention information, dynamically
sensing entropy shifts during compression to achieve fine-grained prompt
compression. Extensive experiments across various domains, including LongBench,
GSM8K, and BBH, show that DAC consistently yields robust and substantial
improvements across a diverse range of tasks and LLMs, offering compelling
evidence of its efficacy.

</details>


### [39] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Main category: cs.CL

TL;DR: The paper introduces the IAM framework to optimize attention computation and KV cache usage in LLMs, leveraging similarities in attention matrices across models.


<details>
  <summary>Details</summary>
Motivation: Address the efficiency challenges in LLMs related to resource consumption, especially when managing long contexts, by utilizing external information for optimization.

Method: Proposed the IAM framework to map attention between small and large LLMs, accelerating computations by exploiting the similarity of attention matrices across different models.

Result: IAM accelerates prefill by 15% and reduces KV cache usage by 22.1%, with minimal impact on performance, demonstrating effectiveness and generalizability across different models.

Conclusion: IAM effectively optimizes LLM efficiency, is compatible with existing KV cache optimization methods, and serves as a robust and versatile tool for LLM advancements.

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [40] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: The paper introduces a multi-stage query-based framework for WikiData QA, focusing on enhancements for multi-hop and temporal reasoning. It includes a novel entity linking technique and demonstrates improved performance using smaller language models.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with handling multi-hop reasoning and temporal questions effectively in question-answering tasks.

Method: A multi-stage query-based framework is proposed, incorporating advanced entity linking and predicate matching methods using chain-of-thought (CoT) reasoning.

Result: The framework improves robustness and performance on multi-hop and temporal question-answering benchmarks, also showcasing the effectiveness of small language models.

Conclusion: Query-based approaches offer modularity and efficacy, making them promising for addressing complex reasoning tasks with smaller computational resources.

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [41] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Main category: cs.CL

TL;DR: The paper presents a Power-of-Two (PoT) quantization framework for Large Language Models (LLMs), achieving superior accuracy in low-precision formats and faster inference speeds compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Large Language Models require significant computational resources for deployment, creating a need for methods like PoT quantization to reduce these demands effectively while maintaining accuracy.

Method: The authors propose a PoT quantization framework that uses a robust two-step post-training algorithm: initializing quantization scales with reliable starting points and refining them with a minimal calibration set.

Result: The framework surpasses state-of-the-art accuracy in 2- and 3-bit low precision formats and achieves inference speed-ups of 3.67x on NVIDIA V100 and 1.63x on NVIDIA RTX 4090 compared to traditional methods.

Conclusion: The proposed PoT quantization delivers enhanced accuracy and efficiency, making it a promising approach for deploying LLMs with reduced computational overheads.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [42] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
*Ziyu Ge,Gabriel Chua,Leanne Tan,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: The paper introduces a framework for improving toxicity-preserving translation in low-resource languages, using Singlish as a case study.


<details>
  <summary>Details</summary>
Motivation: Current translation systems struggle with preserving local slang, code-mixing, and culturally significant toxic language, especially for low-resource language pairs.

Method: The authors propose a two-stage framework: (1) few-shot prompt engineering with human-validated examples to capture nuanced expression and (2) benchmarking large language models using semantic similarity through direct and back-translation.

Result: The framework is shown to be efficient and effective through human evaluation, improving the cultural sensitivity and translation quality for low-resource languages.

Conclusion: This work highlights the need for sociolinguistic nuance in translation systems and supports multicultural LLMs for culturally sensitive applications, using Singlish as a benchmark for inclusive NLP research.

Abstract: As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

</details>


### [43] [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)
*Yuki Sakamoto,Takahisa Uchida,Hiroshi Ishiguro*

Main category: cs.CL

TL;DR: This paper studies how value similarity affects relationships among AI agents using large language models (LLMs). It finds that agents with higher value similarity exhibit greater trust and interpersonal closeness.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the human principle of trust and closeness through value similarity also applies to LLM-based artificial societies.

Method: Two experiments were conducted. A preliminary experiment evaluated how well values could be controlled in LLMs using different model and prompt designs. The main experiment paired agents with specific values and analyzed their mutual trust and closeness after dialogue in English and Japanese.

Result: The study found that LLM agent pairs with higher value similarity demonstrated greater mutual trust and interpersonal closeness, regardless of language.

Conclusion: LLM agent simulations offer a valid testbed for social science theories and help uncover how values influence relationships, paving the way for new theories in social sciences.

Abstract: Large language models (LLMs) have emerged as powerful tools for simulating
complex social phenomena using human-like agents with specific traits. In human
societies, value similarity is important for building trust and close
relationships; however, it remains unexplored whether this principle holds true
in artificial societies comprising LLM agents. Therefore, this study
investigates the influence of value similarity on relationship-building among
LLM agents through two experiments. First, in a preliminary experiment, we
evaluated the controllability of values in LLMs to identify the most effective
model and prompt design for controlling the values. Subsequently, in the main
experiment, we generated pairs of LLM agents imbued with specific values and
analyzed their mutual evaluations of trust and interpersonal closeness
following a dialogue. The experiments were conducted in English and Japanese to
investigate language dependence. The results confirmed that pairs of agents
with higher value similarity exhibited greater mutual trust and interpersonal
closeness. Our findings demonstrate that the LLM agent simulation serves as a
valid testbed for social science theories, contributes to elucidating the
mechanisms by which values influence relationship building, and provides a
foundation for inspiring new theories and insights into the social sciences.

</details>


### [44] [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)
*Lukas Ellinger,Miriam Anschütz,Georg Groh*

Main category: cs.CL

TL;DR: The paper studies how simplification of homonym definitions impacts quality, revealing risks of information loss and misunderstanding for different target groups.


<details>
  <summary>Details</summary>
Motivation: The aim is to address how definitions by LLMs vary for different target groups and the challenges in balancing simplicity with completeness, especially for homonyms.

Method: The study tested LLMs using novel datasets and human annotations to analyze the effect of simplification on definition quality, focusing on different user groups.

Result: Simplification worsens the completeness of homonym definitions, increasing risks of misunderstanding. Fine-tuning significantly improved definition quality.

Conclusion: Balancing simplicity and completeness in NLP tools is crucial to ensure effective and context-aware definitions for various learners, emphasizing the need for fine-tuning LLMs.

Abstract: Large Language Models (LLMs) can provide accurate word definitions and
explanations for any context. However, the scope of the definition changes for
different target groups, like children or language learners. This is especially
relevant for homonyms, words with multiple meanings, where oversimplification
might risk information loss by omitting key senses, potentially misleading
users who trust LLM outputs. We investigate how simplification impacts homonym
definition quality across three target groups: Normal, Simple, and ELI5. Using
two novel evaluation datasets spanning multiple languages, we test DeepSeek v3,
Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge
and human annotations. Our results show that simplification drastically
degrades definition completeness by neglecting polysemy, increasing the risk of
misunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization
substantially improves homonym response quality across all prompt types. These
findings highlight the need to balance simplicity and completeness in
educational NLP to ensure reliable, context-aware definitions for all learners.

</details>


### [45] [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)
*Josip Jukić*

Main category: cs.CL

TL;DR: The paper addresses challenges in data and parameter efficiency for neural language models, by analyzing representations, introducing new optimization techniques, and employing innovative training methods.


<details>
  <summary>Details</summary>
Motivation: To improve robustness, generalization, and efficiency in neural language models by addressing data and parameter constraints.

Method: The paper utilizes representation smoothness (Jacobians, Hessians), active learning, parameter-efficient fine-tuning, and in-context weak supervision for optimization.

Result: Introducing these techniques improved performance, stability, and efficiency across NLP tasks, especially in low-resource and dynamic settings.

Conclusion: The research demonstrated that combining representation smoothness analysis, active learning, and weak supervision significantly enhances neural language model efficiency and robustness.

Abstract: This thesis addresses challenges related to data and parameter efficiency in
neural language models, with a focus on representation analysis and the
introduction of new optimization techniques. The first part examines the
properties and dynamics of language representations within neural models,
emphasizing their significance in enhancing robustness and generalization. It
proposes innovative approaches based on representation smoothness, including
regularization strategies that utilize Jacobian and Hessian matrices to
stabilize training and mitigate sensitivity to input perturbations. The second
part focuses on methods to significantly enhance data and parameter efficiency
by integrating active learning strategies with parameter-efficient fine-tuning,
guided by insights from representation smoothness analysis. It presents
smoothness-informed early-stopping techniques designed to eliminate the need
for labeled validation sets and proposes innovative combinations of active
learning and parameter-efficient fine-tuning to reduce labeling efforts and
computational resources. Extensive experimental evaluations across various NLP
tasks demonstrate that these combined approaches substantially outperform
traditional methods in terms of performance, stability, and efficiency. The
third part explores weak supervision techniques enhanced by in-context learning
to effectively utilize unlabeled data, further reducing dependence on extensive
labeling. It shows that using in-context learning as a mechanism for weak
supervision enables models to better generalize from limited labeled data by
leveraging unlabeled examples more effectively during training. Comprehensive
empirical evaluations confirm significant gains in model accuracy,
adaptability, and robustness, especially in low-resource settings and dynamic
data environments.

</details>


### [46] [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)
*Anca Dinu,Andra-Maria Florescu,Alina Resceanu*

Main category: cs.CL

TL;DR: This paper introduces a creativity test to evaluate linguistic creativity in humans and Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To assess and compare the linguistic creativity capabilities between humans and LLMs using a systematic test and metrics.

Method: Developed a test focusing on word generation and metaphorical language, administered it to humans and LLMs, and evaluated with OCSAI using Originality, Elaboration, and Flexibility criteria.

Result: LLMs outperformed humans across all criteria and most tasks, with minor differences in answer uniqueness.

Conclusion: Humans lean towards extending creativity (E-creativity), while LLMs prefer fixed creativity (F-creativity).

Abstract: The following paper introduces a general linguistic creativity test for
humans and Large Language Models (LLMs). The test consists of various tasks
aimed at assessing their ability to generate new original words and phrases
based on word formation processes (derivation and compounding) and on
metaphorical language use. We administered the test to 24 humans and to an
equal number of LLMs, and we automatically evaluated their answers using OCSAI
tool for three criteria: Originality, Elaboration, and Flexibility. The results
show that LLMs not only outperformed humans in all the assessed criteria, but
did better in six out of the eight test tasks. We then computed the uniqueness
of the individual answers, which showed some minor differences between humans
and LLMs. Finally, we performed a short manual analysis of the dataset, which
revealed that humans are more inclined towards E(extending)-creativity, while
LLMs favor F(ixed)-creativity.

</details>


### [47] [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)
*Anthony G Cohn,Robert E Blackwell*

Main category: cs.CL

TL;DR: This paper examines the capabilities of 28 large language models (LLMs) in reasoning about cardinal directions (CDs) using a template-based benchmark. The investigation revealed limitations even in advanced models when identifying accurate CDs.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work stems from the need to assess and improve the reasoning capabilities of LLMs, particularly in determining cardinal directions in varied scenarios.

Method: The authors designed a benchmark using template-based scenarios that introduce variety in locomotion, agent perspective, and questioning style. They tested the reasoning performance of 28 LLMs using this framework.

Result: Even newly developed Large Reasoning Models struggled to reliably identify correct cardinal directions in all scenarios presented in the benchmark, revealing the limits of current LLM capabilities.

Conclusion: The study highlights gaps in current LLM reasoning abilities regarding spatial direction and suggests areas for future improvement. Earlier work from COSIT-24 is extended in this paper.

Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason
about cardinal directions (CDs) using a benchmark generated from a set of
templates, extensively testing an LLM's ability to determine the correct CD
given a particular scenario. The templates allow for a number of degrees of
variation such as means of locomotion of the agent involved, and whether set in
the first, second or third person. Even the newer Large Reasoning Models are
unable to reliably determine the correct CD for all questions. This paper
summarises and extends earlier work presented at COSIT-24.

</details>


### [48] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Main category: cs.CL

TL;DR: The research presents a stylometric pipeline using spaCy for preprocessing and n-gram feature extraction, and light-gradient boosting machines for classification to distinguish machine-generated texts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop an effective and computationally inexpensive method to detect AI-generated texts using a modular, explainable framework.

Method: The team built a stylometric pipeline leveraging spaCy models for text preprocessing and feature extraction, alongside a light-gradient boosting machine classifier trained on a large corpus of AI-generated texts.

Result: The developed pipeline achieved enhanced classification capacity by optimizing parameters and leveraging over 500,000 machine-generated texts for training.

Conclusion: The study highlights a non-neural, low-cost yet explainable approach for binary AI detection, demonstrating its efficacy in identifying machine-generated text.

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [49] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
*Giuliano Martinelli,Tommaso Bonomo,Pere-Lluís Huguet Cabot,Roberto Navigli*

Main category: cs.CL

TL;DR: The paper introduces BOOKCOREF, a benchmark for evaluating coreference resolution in book-scale texts, addressing gaps in existing methods and resources for long-document analysis.


<details>
  <summary>Details</summary>
Motivation: Current coreference resolution benchmarks like LitBank are insufficient for evaluating system performance at a book scale, leaving a gap in understanding how systems handle mentions spanning hundreds of thousands of tokens.

Method: The authors developed an automated pipeline to generate high-quality coreference annotations for full narrative texts and used it to construct BOOKCOREF, a dataset with an average document length exceeding 200,000 tokens. Experiments were conducted to validate the pipeline and assess the dataset's utility.

Result: Coreference systems evaluated on BOOKCOREF improved performance by up to +20 CoNLL-F1 points, but they revealed significant challenges in handling book-scale documents compared to smaller ones.

Conclusion: The paper highlights the need for advancing coreference resolution systems capable of handling book-scale documents and provides resources (data and code) to drive future research in this domain.

Abstract: Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [50] [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)
*Tosin Adewumi,Foteini Simistira Liwicki,Marcus Liwicki,Viktor Gardelli,Lama Alkhaled,Hamam Mokayed*

Main category: cs.CL

TL;DR: The MEGA method combining Socratic discussion, Chain of Thought, simplified gamification, and formative feedback improves university students' mathematics learning with LLMs, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Students often struggle with mathematics due to ineffective teaching methods, which can dissuade them from Math-related fields; improving pedagogy is crucial.

Method: A within-group study randomly assigned math questions to university students, comparing MEGA against traditional CoT methods using GSM8K and MATH datasets with LLMs GPT4o and Claude 3.5 Sonnet.

Result: MEGA achieved higher student approval (47.5% vs. 26.67%) for explaining difficult math problems, especially in the MATH dataset, surpassing CoT methods.

Conclusion: MEGA provides a superior approach for tackling challenging math problems, demonstrating its potential as an enhanced pedagogical tool leveraging LLMs.

Abstract: This paper presents an intervention study on the effects of the combined
methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)
simplified gamification and (4) formative feedback on university students'
Maths learning driven by large language models (LLMs). We call our approach
Mathematics Explanations through Games by AI LLMs (MEGA). Some students
struggle with Maths and as a result avoid Math-related discipline or subjects
despite the importance of Maths across many fields, including signal
processing. Oftentimes, students' Maths difficulties stem from suboptimal
pedagogy. We compared the MEGA method to the traditional step-by-step (CoT)
method to ascertain which is better by using a within-group design after
randomly assigning questions for the participants, who are university students.
Samples (n=60) were randomly drawn from each of the two test sets of the Grade
School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)
datasets, based on the error margin of 11%, the confidence level of 90%, and a
manageable number of samples for the student evaluators. These samples were
used to evaluate two capable LLMs at length (Generative Pretrained Transformer
4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for
capability. The results showed that students agree in more instances that the
MEGA method is experienced as better for learning for both datasets. It is even
much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH
dataset, indicating that MEGA is better at explaining difficult Maths problems.

</details>


### [51] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Main category: cs.CL

TL;DR: The paper proposes a principled evaluation framework for text data augmentation using large language models (LLMs), focusing on semantic preservation, scalability, and iteration.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of data sparsity in NLP, especially in low-resource settings, and mitigate issues like redundancy and semantic instability in existing text augmentation techniques.

Method: The paper introduces a framework with two components: Scalability Analysis to measure semantic consistency during increased augmentation and IASR to evaluate semantic drift during iterative cycles of augmentation. It is tested with state-of-the-art LLMs.

Result: Empirical evaluations show that GPT-3.5 Turbo achieves the best trade-off among semantic fidelity, diversity, and efficiency. When applied to BERTopic for few-shot labeling, their approach improved topic granularity by 400% and removed topic overlaps.

Conclusion: The proposed framework effectively evaluates and optimizes LLM-based text augmentation, demonstrating significant practical benefits in natural language processing applications.

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [52] [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)
*Pavel Šindelář,Ondřej Bojar*

Main category: cs.CL

TL;DR: ELOQUENT introduces "Sensemaking," a task to evaluate language models through a classroom-inspired approach: creating questions, answering them, and evaluating answers. The 2025 edition involved multiple teams and languages, highlighting issues and strengths in models.


<details>
  <summary>Details</summary>
Motivation: To develop standardized and high-level criteria for rigorously testing generative language models' ability in making coherent sense out of a given text.

Method: A three-step approach: (1) Teacher systems generate questions, (2) Student systems provide answers based on input, and (3) Evaluator systems score these answers. These steps were assessed using multilingual sources and automated/manual evaluation techniques.

Result: 4 teams participated in the 2025 Sensemaking challenge, showing progress in answering accuracy but encountering issues like restricted input dependency and flawed evaluation systems, especially in adversarial tests.

Conclusion: Generative models show promise but face challenges in ensuring input-based answers and reliable evaluation, requiring improved strategies for creating and scoring questions effectively.

Abstract: ELOQUENT is a set of shared tasks that aims to create easily testable
high-level criteria for evaluating generative language models. Sensemaking is
one such shared task.
  In Sensemaking, we try to assess how well generative models ``make sense out
of a given text'' in three steps inspired by exams in a classroom setting: (1)
Teacher systems should prepare a set of questions, (2) Student systems should
answer these questions, and (3) Evaluator systems should score these answers,
all adhering rather strictly to a given set of input materials.
  We report on the 2025 edition of Sensemaking, where we had 7 sources of test
materials (fact-checking analyses of statements, textbooks, transcribed
recordings of a lecture, and educational videos) spanning English, German,
Ukrainian, and Czech languages.
  This year, 4 teams participated, providing us with 2 Teacher submissions, 2
Student submissions, and 2 Evaluator submissions. We added baselines for
Teacher and Student using commercial large language model systems. We devised a
fully automatic evaluation procedure, which we compare to a minimalistic manual
evaluation.
  We were able to make some interesting observations. For the first task, the
creation of questions, better evaluation strategies will still have to be
devised because it is difficult to discern the quality of the various candidate
question sets. In the second task, question answering, the LLMs examined
overall perform acceptably, but restricting their answers to the given input
texts remains problematic. In the third task, evaluation of question answers,
our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm
erroneously rate both garbled question-answer pairs and answers to mixed-up
questions as acceptable.

</details>


### [53] [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)
*Michael Carl,Takanori Mizowaki,Aishvarya Ray,Masaru Yamada,Devi Sri Bandaru,Xinyue Ren*

Main category: cs.CL

TL;DR: The study introduces a Behavioural Translation Style Space (BTSS), analyzing physical translation actions and cognitive processes through gaze and keystroke data.


<details>
  <summary>Details</summary>
Motivation: To better understand and model human translation behaviour by linking observable actions like eye and finger movements to higher-order cognitive and emotional processes.

Method: Keystroke and gaze data were analyzed to identify hidden mental processing structure, leading to the development of a hierarchical, multi-layered BTSS that organizes behavioural translation patterns.

Result: The BTSS was successfully constructed to capture and organize behavioural and cognitive patterns during human translation.

Conclusion: BTSS provides a foundation for creating computational translation agents that simulate human translation dynamics, linking physical actions with cognitive and affective states.

Abstract: The paper introduces a Behavioural Translation Style Space (BTSS) that
describes possible behavioural translation patterns. The suggested BTSS is
organized as a hierarchical structure that entails various embedded processing
layers. We posit that observable translation behaviour - i.e., eye and finger
movements - is fundamental when executing the physical act of translation but
it is caused and shaped by higher-order cognitive processes and affective
translation states. We analyse records of keystrokes and gaze data as
indicators of the hidden mental processing structure and organize the
behavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the
basis for a computational translation agent to simulate the temporal dynamics
of affect, automatized behaviour and cognition during human translation
production.

</details>


### [54] [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)
*Reuben Smit,Retief Louw,Herman Kamper*

Main category: cs.CL

TL;DR: The paper explores an ASR-free, few-shot technique for assessing isolated word reading in low-resource settings using child and adult speech templates encoded by SSL models, but notes performance limitations on child speech.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of improving reading assessment in low-resource settings without relying on automatic speech recognition (ASR), particularly for younger speakers.

Method: The authors employ few-shot learning to compare child speech inputs to adult-provided speech templates using intermediate SSL model features. They experimented with features like discretisation and barycentre averaging on an Afrikaans speech dataset.

Result: While experiments showed reasonable accuracy with adults, accuracy dropped significantly when processing child speech, even when child templates were used.

Conclusion: Although SSL representations are effective for some low-resource speech tasks, they currently face limitations when applied to processing child speech in this few-shot framework.

Abstract: We explore an ASR-free method for isolated word reading assessment in
low-resource settings. Our few-shot approach compares input child speech to a
small set of adult-provided reference templates. Inputs and templates are
encoded using intermediate layers from large self-supervised learned (SSL)
models. Using an Afrikaans child speech benchmark, we investigate design
options such as discretising SSL features and barycentre averaging of the
templates. Idealised experiments show reasonable performance for adults, but a
substantial drop for child speech input, even with child templates. Despite the
success of employing SSL representations in low-resource speech tasks, our work
highlights the limitations of SSL representations for processing child data
when used in a few-shot classification system.

</details>


### [55] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
*Shilin Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: The paper proposes a multi-grained fusion method for ASR, combining token-level and phrase-level fusion with LLM, significantly enhancing keyword recognition without compromising general transcription accuracy.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end ASR systems struggle to correctly recognize keywords like proper nouns and user-specific entities, largely due to limitations in leveraging contextual information.

Method: They introduce a multi-grained fusion approach that integrates token-level and phrase-level fusion with LLMs, using a late-fusion strategy to balance acoustic and contextual information.

Result: The proposed method achieves state-of-the-art keyword recognition performance in both Chinese and English datasets while maintaining accuracy in general transcription tasks.

Conclusion: The multi-grained approach effectively combines the advantages of token and phrase-level fusion, demonstrating its potential in enhancing ASR for keywords. The framework's robustness is confirmed through ablation studies.

Abstract: While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

</details>


### [56] [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
*Yikang Liu,Wanyang Zhang,Yiming Wang,Jialong Tang,Pei Zhang,Baosong Yang,Fei Huang,Rui Wang,Hai Hu*

Main category: cs.CL

TL;DR: This paper introduces the translationese-index (T-index), a novel quantitative measure for grading and generalizing translationese by analyzing likelihood ratios from fine-tuned language models.


<details>
  <summary>Details</summary>
Motivation: The goal is to create a robust metric for translationese that complements existing machine translation quality estimation metrics like BLEU and COMET, as these may not fully capture translationese features.

Method: The T-index is computed using likelihood ratios from two contrastively fine-tuned language models, evaluated on synthetic and real-world translation data. The approach tests generalizability across domains and validity against human judgments.

Result: T-index effectively predicts translationese annotations and aligns with human ratings of translationese degrees (Pearson's r=0.568). It also shows low correlation with common MT quality metrics (BLEU, COMET), proving it as complementary.

Conclusion: T-index is efficient, generalizable, robust, and can serve as a useful complementary metric to existing machine translation evaluation metrics in assessing translationese.

Abstract: In this paper, we propose the first quantitative measure for translationese
-- the translationese-index (T-index) for graded and generalizable measurement
of translationese, computed from the likelihood ratios of two contrastively
fine-tuned language models (LMs). We use a synthesized dataset and a dataset
with translations in the wild to evaluate T-index's generalizability in
cross-domain settings and its validity against human judgments. Our results
show that T-index is both robust and efficient. T-index scored by two 0.5B LMs
fine-tuned on only 1-5k pairs of synthetic data can well capture translationese
in the wild. We find that the relative differences in T-indices between
translations can well predict pairwise translationese annotations obtained from
human annotators; and the absolute values of T-indices correlate well with
human ratings of degrees of translationese (Pearson's $r = 0.568$).
Additionally, the correlation between T-index and existing machine translation
(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting
that T-index is not covered by these metrics and can serve as a complementary
metric in MT QE.

</details>


### [57] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
*Johann Frei,Nils Feldhus,Lisa Raithel,Roland Roller,Alexander Meyer,Frank Kramer*

Main category: cs.CL

TL;DR: The paper introduces Infherno, an end-to-end framework utilizing LLM agents to translate clinical notes into structured FHIR resources, addressing common issues of generalizability and conformity.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of translating unstructured clinical notes into structured FHIR resources, including issues of limited generalizability and poor structural conformity in existing methods.

Method: The paper proposes Infherno, an end-to-end framework powered by LLM agents, code execution, and healthcare terminology tools, designed to adhere strictly to the FHIR schema.

Result: Infherno performs competitively with human baselines in accurately predicting FHIR resources from clinical text. It supports both custom and synthetic data as well as various model types.

Conclusion: Infherno represents a significant step toward improving clinical data integration and interoperability, offering a robust solution that could benefit healthcare institutions.

Abstract: For clinical data integration and healthcare services, the HL7 FHIR standard
has established itself as a desirable format for interoperability between
complex health data. Previous attempts at automating the translation from
free-form clinical notes into structured FHIR resources rely on modular,
rule-based systems or LLMs with instruction tuning and constrained decoding.
Since they frequently suffer from limited generalizability and structural
inconformity, we propose an end-to-end framework powered by LLM agents, code
execution, and healthcare terminology database tools to address these issues.
Our solution, called Infherno, is designed to adhere to the FHIR document
schema and competes well with a human baseline in predicting FHIR resources
from unstructured text. The implementation features a front end for custom and
synthetic data and both local and proprietary models, supporting clinical data
integration processes and interoperability across institutions.

</details>


### [58] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Main category: cs.CL

TL;DR: This paper introduces a new benchmark for text anomaly detection using embeddings from pre-trained language models, evaluating their effectiveness across various datasets and metrics.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized and comprehensive benchmarks for evaluating anomaly detection methods on text data has hindered the comparison and development of innovative approaches.

Method: The study utilizes embeddings from various pre-trained language models, evaluates them across multi-domain datasets, and employs several performance metrics (AUROC, AUPRC). Traditional anomaly detection algorithms are compared against deep learning models.

Result: The study finds that embedding quality significantly impacts anomaly detection performance. Shallow algorithms perform comparably to deep learning methods when leveraging embeddings. Additionally, cross-model performance matrices exhibit low-rank characteristics, allowing for efficient model evaluation and selection.

Conclusion: An open-source benchmark toolkit is provided, paving the way for scalable and robust anomaly detection in future research.

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [59] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
*Liu He,Yuanchao Li,Rui Feng,XinRan Han,Yin-Long Liu,Yuwei Yang,Zude Zhu,Jiahong Yuan*

Main category: cs.CL

TL;DR: The paper explores gender bias in Alzheimer's Disease (AD) speech perception, where male speech is more often identified as AD, especially in Chinese. Certain acoustic features like shimmer values correlate with this perception.


<details>
  <summary>Details</summary>
Motivation: The study aims to uncover and address gender bias in AD speech perception, as voicing differences between genders may lead to inaccuracies in identifying speech associated with AD.

Method: A perception experiment with 16 Chinese listeners was conducted to evaluate Chinese and Greek speech for signs of AD, alongside an acoustic analysis of speech features correlating with AD perception.

Result: Male speech was more frequently identified as AD, with the bias more evident in Chinese speech. Shimmer values in male speech were significant in AD perception, while speech portion negatively correlated with AD identification. Language itself was not a major factor.

Conclusion: The study highlights the influence of gender bias in AD speech perception, advocating for addressing this bias in AD detection models and expanding research on diverse linguistic contexts to ensure model reliability.

Abstract: Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

</details>


### [60] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CL

TL;DR: This paper proposes a multi-agent debate framework to improve ambiguity resolution for LLMs, achieving notable performance increases.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with user request ambiguities, hindering their potential for robust language understanding.

Method: The study devised a multi-agent debate framework involving three LLMs (Llama3-8B, Gemma2-9B, and Mistral-7B) on a custom ambiguity dataset to assess collaborative performance.

Result: The debate framework significantly improved the performance of Llama3-8B and Mistral-7B. Mistral-7B-led debates achieved a 76.7% success rate.

Conclusion: Structured debates are effective for enhancing the capabilities of LLMs, offering promising avenues for handling complex ambiguities and fostering clarity in interactions.

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and generating human language, contributing to more natural
interactions with complex systems. However, they face challenges such as
ambiguity in user requests processed by LLMs. To address these challenges, this
paper introduces and evaluates a multi-agent debate framework designed to
enhance detection and resolution capabilities beyond single models. The
framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and
Mistral-7B variants) and a dataset with diverse ambiguities. The debate
framework markedly enhanced the performance of Llama3-8B and Mistral-7B
variants over their individual baselines, with Mistral-7B-led debates achieving
a notable 76.7% success rate and proving particularly effective for complex
ambiguities and efficient consensus. While acknowledging varying model
responses to collaborative strategies, these findings underscore the debate
framework's value as a targeted method for augmenting LLM capabilities. This
work offers important insights for developing more robust and adaptive language
understanding systems by showing how structured debates can lead to improved
clarity in interactive systems.

</details>


### [61] [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei,Mohsen Mosleh*

Main category: cs.CL

TL;DR: The paper evaluates how large language models (LLMs) with web browsing capabilities can interpret social media profiles to predict user demographics, raising both promise and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study stems from understanding whether LLMs can analyze social media profiles with only usernames, given the lack of exploration in this area.

Method: The researchers used a synthetic dataset of 48 Twitter accounts and a survey dataset of 1,384 participants to test the ability of LLMs to retrieve and analyze social media data.

Result: LLMs demonstrated reasonable accuracy in predicting user demographics and successfully accessed social media content. Potential biases were observed in analyzing minimal activity profiles.

Conclusion: While the capability opens doors for computational social science, it raises ethical risks like misuse in targeted advertising and information operations. The authors recommend limiting public-facing applications while enabling controlled access for research.

Abstract: Large language models (LLMs) have traditionally relied on static training
data, limiting their knowledge to fixed snapshots. Recent advancements,
however, have equipped LLMs with web browsing capabilities, enabling real time
information retrieval and multi step reasoning over live web content. While
prior studies have demonstrated LLMs ability to access and analyze websites,
their capacity to directly retrieve and analyze social media data remains
unexplored. Here, we evaluate whether web browsing LLMs can infer demographic
attributes of social media users given only their usernames. Using a synthetic
dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international
participants, we show that these models can access social media content and
predict user demographics with reasonable accuracy. Analysis of the synthetic
dataset further reveals how LLMs parse and interpret social media profiles,
which may introduce gender and political biases against accounts with minimal
activity. While this capability holds promise for computational social science
in the post API era, it also raises risks of misuse particularly in information
operations and targeted advertising underscoring the need for safeguards. We
recommend that LLM providers restrict this capability in public facing
applications, while preserving controlled access for verified research
purposes.

</details>


### [62] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
*Yucheng Sun,Alessandro Stolfo,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: This paper explores using internal activations of language models to detect arithmetic errors, achieving over 90% accuracy with lightweight error detectors and showing potential for model self-correction.


<details>
  <summary>Details</summary>
Motivation: The study aims to address whether language models can internally recognize their own errors, particularly in arithmetic, without external validations.

Method: Simple probes were trained to decode both correct and predicted answers from hidden states, which were extended to error detectors and tested on complex arithmetic tasks using chain-of-thought reasoning.

Result: Probes demonstrated high accuracy in predicting model correctness and generalized well to complex arithmetic settings, revealing consistent internal representations.

Conclusion: Arithmetic errors can be detected through internal activations, enabling lightweight self-correction strategies for language models through selective re-prompting.

Abstract: We investigate whether internal activations in language models can be used to
detect arithmetic errors. Starting with a controlled setting of 3-digit
addition, we show that simple probes can accurately decode both the model's
predicted output and the correct answer from hidden states, regardless of
whether the model's output is correct. Building on this, we train lightweight
error detectors that predict model correctness with over 90% accuracy. We then
extend our analysis to structured chain-of-thought traces on addition-only
GSM8K problems and find that probes trained on simple arithmetic generalize
well to this more complex setting, revealing consistent internal
representations. Finally, we demonstrate that these probes can guide selective
re-prompting of erroneous reasoning steps, improving task accuracy with minimal
disruption to correct outputs. Our findings suggest that arithmetic errors can
be anticipated from internal activations alone, and that simple probes offer a
viable path toward lightweight model self-correction.

</details>


### [63] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
*Chandana Cheerla*

Main category: cs.CL

TL;DR: The paper presents an enhanced Retrieval-Augmented Generation (RAG) framework optimized for enterprise data, achieving significant improvements in precision, recall, and qualitative evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Organizations need more effective ways to process and utilize proprietary enterprise data for decision-making, but current LLMs and conventional RAG frameworks face limits with structured/semi-structured data and context handling.

Method: The proposed method enhances RAG with dense embedding retrieval (all-mpnet-base-v2), BM25 hybrid strategies, metadata-aware filtering (SpaCy NER), cross-encoder reranking, semantic chunking, and quantized indexing. It also incorporates human feedback and conversation memory.

Result: Experimental results on enterprise datasets show a 15% increase in Precision@5, a 13% boost in Recall@5, and a 16% improvement in Mean Reciprocal Rank. It also achieved higher faithfulness, completeness, and relevance scores on a Likert scale.

Conclusion: The newly devised RAG framework is effective for enterprise tasks, offering precise, relevant, and comprehensive responses. Future work focuses on accommodating multimodal data and agent-based retrieval capabilities, with source code provided for wider adoption.

Abstract: Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [64] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CL

TL;DR: The paper explores using chain-of-thought (CoT) activations instead of CoT text to predict the safety of final responses from reasoning language models. It demonstrates that model activations outperform text-based methods in reliability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the alignment risks posed by reasoning language models, such as the introduction of harmful content in CoT and final outputs, and to improve safety monitoring during text generation.

Method: The study evaluates various monitoring approaches like human review, language models, and classifiers, focusing on either CoT text or CoT activations. It uses a simple linear probe on CoT activations to predict response misalignment.

Result: Linear probes on CoT activations significantly outperform text-based approaches in identifying safe or unsafe responses, showing effectiveness even on early segments of CoT and generalizing across model sizes, types, and safety benchmarks.

Conclusion: Lightweight probes on CoT activations offer a promising method for real-time safety monitoring and early intervention, being more reliable and accurate compared to text-based methods.

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [65] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: The paper introduces Spherical Sliced Wasserstein Autoencoder (S2WTM) to mitigate posterior collapse in neural topic models, generating improved topic coherence and diversity.


<details>
  <summary>Details</summary>
Motivation: To address posterior collapse in VAE-based neural topic models and enhance modeling of hyperspherical structures for better latent representations.

Method: Propose Spherical Sliced Wasserstein Autoencoder (S2WTM), which uses the Spherical Sliced-Wasserstein distance to align aggregated posterior with a hyperspherical prior.

Result: S2WTM surpasses state-of-the-art topic models in generating coherent and diverse topics and enhancing downstream task performance.

Conclusion: Leveraging hyperspherical priors and Sliced-Wasserstein alignment improves neural topic modeling and mitigates posterior collapse effectively.

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [66] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Main category: cs.CL

TL;DR: The paper introduces BETR, a data selection method optimizing model pretraining by matching data to benchmark tasks, yielding substantial performance gains and insights into scaling laws.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of explicitly optimizing pretraining data selection towards benchmark tasks on model performance and data efficiency.

Method: Proposed BETR, where pretraining documents are ranked and filtered based on their similarity to benchmark examples using embedding and a lightweight classifier, followed by scaling law analysis across 500 models.

Result: BETR achieved a compute multiplier of 2.1x over DCLM-Baseline (4.7x over unfiltered data) and enhanced performance in 9 out of 10 tasks across scales, also generalizing well to unseen benchmarks.

Conclusion: Explicitly aligning pretraining data with evaluation benchmarks optimizes model capabilities, with larger models benefiting from less filtering. Data selection methods must adapt to model scale for optimal results.

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Main category: cs.CV

TL;DR: The paper develops a hardware optimization framework for DAT, reducing memory access complexities without losing significant accuracy.


<details>
  <summary>Details</summary>
Motivation: Deformable Attention Transformers (DAT) excel in performance, but their data-dependent sampling leads to irregular memory access patterns, making hardware deployment inefficient.

Method: The authors propose a NAS-based slicing strategy to automatically divide input features into uniform patches, avoiding memory conflicts, and use an FPGA-based system to validate its performance.

Result: The method has a negligible 0.2% accuracy drop on ImageNet-1K and reduces DRAM access to 18% versus existing methods on Xilinx FPGA.

Conclusion: The proposed hardware-friendly framework optimizes DAT efficiency for edge-side hardware without significantly compromising accuracy or performance.

Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [68] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Main category: cs.CV

TL;DR: The paper introduces a novel Deformable Dynamic Convolution Network (DDCN) for scalable and accurate spatio-temporal traffic prediction, addressing limitations of Graph Neural Networks (GNNs).


<details>
  <summary>Details</summary>
Motivation: To address the challenges of modeling heterogeneity in traffic patterns, improve scalability in large-scale datasets, and overcome the limitations of GNNs requiring predefined adjacency matrices.

Method: DDCN uses deformable filters that dynamically adapt based on offsets, incorporates a transformer-style CNN decomposed into an encoder-decoder structure, and enhances spatial and spatio-temporal attention blocks for feature emphasis.

Result: In experiments on four real-world datasets, DDCN demonstrated competitive performance in traffic prediction, validating the effectiveness of CNN-based methods.

Conclusion: DDCN offers a scalable, accurate, and efficient solution for spatio-temporal traffic prediction, showcasing the potential of convolutional networks in this domain.

Abstract: Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [69] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

TL;DR: The paper introduces Inversion-DPO, an alignment framework for diffusion models (DMs) that improves performance without the need for a reward model.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods for diffusion models require computationally expensive reward modeling, which reduces efficiency and accuracy.

Method: The proposed method, Inversion-DPO, reformulates Direct Preference Optimization (DPO) with DDIM inversion to eliminate reliance on reward models by inverting winning and losing samples back to noise.

Result: Inversion-DPO achieves significant performance improvements in both text-to-image and compositional image generation tasks, surpassing existing post-training methods.

Conclusion: The framework enhances training precision and efficiency, demonstrating its potential to align diffusion models for complex generation tasks effectively.

Abstract: Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [70] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Main category: cs.CV

TL;DR: The paper introduces ST-VFM, a framework that adapts Vision Foundation Models (VFMs) for spatio-temporal forecasting by addressing their lack of temporal modeling and modality gaps through dual-branch architecture and reprogramming strategies.


<details>
  <summary>Details</summary>
Motivation: The authors aim to leverage the spatial pattern capabilities of Vision Foundation Models for spatio-temporal forecasting, addressing the limitations of current approaches using large language models that fail to capture complex spatio-temporal correlations.

Method: They use a dual-branch architecture with raw input and auxiliary temporal signals, and introduce pre-VFM and post-VFM reprogramming stages to embed temporal contexts and enhance inter-branch interactions while utilizing VFMs without altering their backbones.

Result: ST-VFM surpasses state-of-the-art baselines across ten datasets, proving robustness and effectiveness across various VFMs and demonstrating strong potential as a general framework.

Conclusion: By effectively adapting VFMs for spatio-temporal forecasting through innovative design, ST-VFM sets a new benchmark in the field, overcoming limitations of existing models and exhibiting versatility across applications.

Abstract: Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [71] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

TL;DR: Underwater image restoration is challenging due to complex distortions, and traditional GAN models struggle to address heterogeneous scenarios. xOp-GAN introduces multiple expert generators tailored to different image quality subsets and uses a discriminator for optimal result selection during inference, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Current GAN-based methods for underwater image restoration fail to address diverse visual distortions adequately due to the limitations of single generator networks.

Method: The paper proposes xOp-GAN, a GAN model featuring multiple expert generator networks, each trained on specific subsets of images with distinct quality metrics. During inference, the discriminator evaluates restoration quality and selects the best result.

Result: xOp-GAN attained up to 25.16 dB PSNR on the LSUI dataset, significantly outperforming conventional single-regressor models while maintaining reduced complexity.

Conclusion: xOp-GAN's use of multiple generators and discriminator for inference demonstrates its effectiveness in heterogeneous underwater image restoration, setting a new benchmark for performance and efficiency.

Abstract: The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [72] [Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation](https://arxiv.org/abs/2507.11571)
*Varun Velankar*

Main category: cs.CV

TL;DR: The paper reviews studies on age prediction using gait features, conducts experiments, and suggests methods to achieve accurate predictions, highlighting the use of neural networks for high precision.


<details>
  <summary>Details</summary>
Motivation: The study aims to leverage gait analysis for age estimation with applications in fields like healthcare, security, and human-computer interaction.

Method: The authors analyze existing studies, perform correlation tests on large datasets, fine-tune neural networks, apply visualization methods like Grad-CAM, and compare multiple machine learning models.

Result: Convolutional neural networks achieve up to 96% accuracy, with Grad-CAM showing focus on specific gait regions. Fusion and refined methods reduce prediction error to below three years.

Conclusion: The study combines meta-analysis and experimentation to establish performance baselines, offering practical guidelines for improving gait-based age estimation in real-world scenarios.

Abstract: Estimating a person's age from their gait has important applications in
healthcare, security and human-computer interaction. In this work, we review
fifty-nine studies involving over seventy-five thousand subjects recorded with
video, wearable and radar sensors. We observe that convolutional neural
networks produce an average error of about 4.2 years, inertial-sensor models
about 4.5 years and multi-sensor fusion as low as 3.4 years, with notable
differences between lab and real-world data. We then analyse sixty-three
thousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population
dataset to quantify correlations between age and five key metrics: stride
length, walking speed, step cadence, step-time variability and joint-angle
entropy, with correlation coefficients of at least 0.27. Next, we fine-tune a
ResNet34 model and apply Grad-CAM to reveal that the network attends to the
knee and pelvic regions, consistent with known age-related gait changes.
Finally, on a one hundred thousand sample subset of the VersatileGait database,
we compare support vector machines, decision trees, random forests, multilayer
perceptrons and convolutional neural networks, finding that deep networks
achieve up to 96 percent accuracy while processing each sample in under 0.1
seconds. By combining a broad meta-analysis with new large-scale experiments
and interpretable visualizations, we establish solid performance baselines and
practical guidelines for reducing gait-age error below three years in
real-world scenarios.

</details>


### [73] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Main category: cs.CV

TL;DR: This paper introduces a modified computer vision model, PPGNet-Cat, to identify individual feral cats using camera trap images for wildlife monitoring.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address the negative environmental impact of feral cats in Australia, recognizing their status as invasive species and the need for effective monitoring.

Method: The research adapted a part-pose guided network (PPGNet), employed previously for Amur tiger re-ID, with specific changes for feral cats images. It also explored contrastive learning methods like ArcFace loss.

Result: PPGNet-Cat achieved high identification accuracy for feral cats with a mAP of 0.86 and rank-1 accuracy of 0.95.

Conclusion: PPGNet-Cat represents a competitive and effective re-ID model for monitoring feral cats, with significant performance outcomes in individual identification.

Abstract: Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [74] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

TL;DR: The paper introduces SketchDNN, a CAD sketch generative model utilizing a novel continuous-discrete diffusion process for enhanced quality.


<details>
  <summary>Details</summary>
Motivation: To address challenges in CAD sketch generation, particularly the heterogeneity of parameterizations and permutation invariance of primitives, and to improve synthesis quality.

Method: Developed SketchDNN using a Gaussian-Softmax diffusion process for unified modeling of continuous and discrete variables in CAD sketches.

Result: SketchDNN achieved significant quality improvements, reducing FID from 16.04 to 7.80 and NLL from 84.8 to 81.33 on the SketchGraphs dataset.

Conclusion: The innovative Gaussian-Softmax diffusion enables state-of-the-art CAD sketch generation with enhanced modeling capabilities for continuous and discrete data.

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [75] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: The paper investigates using Variational Autoencoders (VAEs) for image-based lymph node metastasis (LNM) staging in rectal cancer, achieving high performance using MRI data.


<details>
  <summary>Details</summary>
Motivation: Radiological methods relying on lymph node size, shape, and texture have limited accuracy in diagnosing lymph node metastasis (LNM) in rectal cancer.

Method: The study used a Variational Autoencoder (VAE) as a feature encoder to reconstruct images and generate meaningful latent visual patterns. The model was evaluated using MRI data from 168 rectal cancer patients.

Result: The proposed VAE-based model ('VAE-MLP') achieved state-of-the-art results on MRI data, with cross-validated metrics: AUC 0.86 ± 0.05, Sensitivity 0.79 ± 0.06, and Specificity 0.85 ± 0.05.

Conclusion: Using a VAE for feature encoding simplifies the latent space and improves performance for lymph node diagnosis, demonstrating its utility for LNM staging in rectal cancer.

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [76] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: This paper explores the use of posture analysis to infer human intent via motion tracking in sports, achieving promising results for intent categorization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in mental state inference from posture for applications like fatigue diagnosis and injury prevention, while overcoming data sensitivity issues.

Method: The study focuses on cricket, utilizing activity video analysis to predict aggressive or defensive intents based on posture signals and leveraging weak supervision for validation.

Result: The method demonstrates over 75% F1 score and 80% AUC-ROC in classifying shot intent in cricket videos.

Conclusion: Posture-based approaches provide strong insights into intent inference despite noisy data, with applications in sports analytics and broader human behavior analysis domains.

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [77] [VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization](https://arxiv.org/abs/2507.11653)
*Hannah Shafferman,Annika Thomas,Jouko Kinnari,Michael Ricard,Jose Nino,Jonathan How*

Main category: cs.CV

TL;DR: The paper introduces VISTA, a novel localization framework for autonomous navigation in unstructured environments, overcoming challenges like appearance changes, spatial aliasing, and occlusions. The method achieves superior recall and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of global localization in unstructured environments where traditional place recognition methods struggle due to factors like appearance changes, spatial aliasing, and occlusions.

Method: The proposed framework, VISTA, integrates an object-based segmentation and tracking pipeline as the front-end, and a submap correspondence search that exploits geometric consistencies for reference frame alignment.

Result: VISTA achieves up to a 69% improvement in recall over baselines and features a highly compact object-based map (0.6% the size of traditional methods), making it suitable for real-time use on resource-constrained platforms.

Conclusion: VISTA is a robust and memory-efficient solution for monocular global localization, functioning across diverse viewpoints and environmental changes without requiring extensive training, thereby enabling consistent and real-time localization.

Abstract: Global localization is critical for autonomous navigation, particularly in
scenarios where an agent must localize within a map generated in a different
session or by another agent, as agents often have no prior knowledge about the
correlation between reference frames. However, this task remains challenging in
unstructured environments due to appearance changes induced by viewpoint
variation, seasonal changes, spatial aliasing, and occlusions -- known failure
modes for traditional place recognition methods. To address these challenges,
we propose VISTA (View-Invariant Segmentation-Based Tracking for Frame
Alignment), a novel open-set, monocular global localization framework that
combines: 1) a front-end, object-based, segmentation and tracking pipeline,
followed by 2) a submap correspondence search, which exploits geometric
consistencies between environment maps to align vehicle reference frames. VISTA
enables consistent localization across diverse camera viewpoints and seasonal
changes, without requiring any domain-specific training or finetuning. We
evaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a
69% improvement in recall over baseline methods. Furthermore, we maintain a
compact object-based map that is only 0.6% the size of the most
memory-conservative baseline, making our approach capable of real-time
implementation on resource-constrained platforms.

</details>


### [78] [Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis](https://arxiv.org/abs/2507.11730)
*Maciej Szankin,Vidhyananth Venkatasamy,Lihang Ying*

Main category: cs.CV

TL;DR: The paper evaluates the performance of Vision-Language Models (VLMs) against traditional OCR methods for billboard text visibility in outdoor conditions, while introducing a weather-augmented benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying billboard text visibility under real-world conditions like outdoor scene complexity and weather-induced distortions.

Method: Benchmarked VLMs (e.g., Qwen 2.5 VL 3B) against a CNN-based OCR (PaddleOCRv4) on datasets (ICDAR 2015, SVT) with added synthetic weather distortions.

Result: VLMs demonstrated strong holistic scene reasoning, while CNN-based OCR showed competitive accuracy for cropped text with lower computational cost.

Conclusion: Although VLMs are promising in understanding complex scenes, CNN-based OCR remains effective for edge deployments, highlighting a tradeoff between accuracy and resource usage. Publicly released benchmarks enable further research.

Abstract: Outdoor advertisements remain a critical medium for modern marketing, yet
accurately verifying billboard text visibility under real-world conditions is
still challenging. Traditional Optical Character Recognition (OCR) pipelines
excel at cropped text recognition but often struggle with complex outdoor
scenes, varying fonts, and weather-induced visual noise. Recently, multimodal
Vision-Language Models (VLMs) have emerged as promising alternatives, offering
end-to-end scene understanding with no explicit detection step. This work
systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,
InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline
(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with
synthetic weather distortions to simulate realistic degradation. Our results
reveal that while selected VLMs excel at holistic scene reasoning, lightweight
CNN pipelines still achieve competitive accuracy for cropped text at a fraction
of the computational cost-an important consideration for edge deployment. To
foster future research, we release our weather-augmented benchmark and
evaluation code publicly.

</details>


### [79] [Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning](https://arxiv.org/abs/2507.11761)
*Fan Shi,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: This paper introduces the Unified Conditional Generative Solver (UCGS) to address multiple abstract visual reasoning (AVR) tasks with a unified framework, eliminating the need for task-specific retraining.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a unified framework for AVR tasks, avoiding the inefficiencies of task-specific solver designs, retraining, and architectural changes when addressing new tasks.

Method: The paper reformulates AVR tasks as predicting target image predictability in problem panels and proposes a conditional generative model that can be trained once for solving multiple AVR tasks.

Result: Experiments demonstrate that UCGS can solve various AVR tasks and even achieve zero-shot reasoning on unseen tasks during testing, showing strong generalization capabilities.

Conclusion: UCGS provides an efficient, unified approach to handling multiple AVR tasks while generalizing to unseen tasks, showcasing its potential to replace task-specific frameworks.

Abstract: Abstract visual reasoning (AVR) enables humans to quickly discover and
generalize abstract rules to new scenarios. Designing intelligent systems with
human-like AVR abilities has been a long-standing topic in the artificial
intelligence community. Deep AVR solvers have recently achieved remarkable
success in various AVR tasks. However, they usually use task-specific designs
or parameters in different tasks. In such a paradigm, solving new tasks often
means retraining the model, and sometimes retuning the model architectures,
which increases the cost of solving AVR problems. In contrast to task-specific
approaches, this paper proposes a novel Unified Conditional Generative Solver
(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we
prove that some well-known AVR tasks can be reformulated as the problem of
estimating the predictability of target images in problem panels. Then, we
illustrate that, under the proposed framework, training one conditional
generative model can solve various AVR tasks. The experiments show that with a
single round of multi-task training, UCGS demonstrates abstract reasoning
ability across various AVR tasks. Especially, UCGS exhibits the ability of
zero-shot reasoning, enabling it to perform abstract reasoning on problems from
unseen AVR tasks in the testing phase.

</details>


### [80] [CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning](https://arxiv.org/abs/2507.11834)
*Peiwen Xia,Tangfei Liao,Wei Zhu,Danhuai Zhao,Jianjun Ke,Kaihao Zhang,Tong Lu,Tao Wang*

Main category: cs.CV

TL;DR: CorrMoE is a novel framework for improving image correspondence robustness under cross-domain and cross-scene variations.


<details>
  <summary>Details</summary>
Motivation: Address challenges in achieving reliable image correspondences under diverse scene structures and domain variations, which existing methods often overlook.

Method: Introduces a De-stylization Dual Branch for style mixing to counter domain shifts and a Bi-Fusion Mixture of Experts module for adaptive feature integration under scene diversity.

Result: Demonstrated superior accuracy and generalization in correspondence pruning over state-of-the-art methods on benchmark datasets.

Conclusion: CorrMoE enhances robustness and accuracy in image correspondence tasks, addressing domain shifts and scene diversity effectively.

Abstract: Establishing reliable correspondences between image pairs is a fundamental
task in computer vision, underpinning applications such as 3D reconstruction
and visual localization. Although recent methods have made progress in pruning
outliers from dense correspondence sets, they often hypothesize consistent
visual domains and overlook the challenges posed by diverse scene structures.
In this paper, we propose CorrMoE, a novel correspondence pruning framework
that enhances robustness under cross-domain and cross-scene variations. To
address domain shift, we introduce a De-stylization Dual Branch, performing
style mixing on both implicit and explicit graph features to mitigate the
adverse influence of domain-specific representations. For scene diversity, we
design a Bi-Fusion Mixture of Experts module that adaptively integrates
multi-perspective features through linear-complexity attention and dynamic
expert routing. Extensive experiments on benchmark datasets demonstrate that
CorrMoE achieves superior accuracy and generalization compared to
state-of-the-art methods. The code and pre-trained models are available at
https://github.com/peiwenxia/CorrMoE.

</details>


### [81] [ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification](https://arxiv.org/abs/2507.11845)
*Kexuan Shi,Zhuang Qi,Jingjing Zhu,Lei Meng,Yaochen Zhang,Haibei Huang,Xiangxu Meng*

Main category: cs.CV

TL;DR: The paper introduces ProtoConNet, a method for open-set few-shot image classification, leveraging contextual information to enhance feature diversity and class distinction.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for open-set few-shot classification rely on single-image visual representations and often ignore the advantages of contextual information. This leads to limitations in handling unknown data in diverse environments.

Method: The proposed method, ProtoConNet, uses three modules: (1) Clustering-based Data Selection (CDS) for mining diverse patterns while preserving key features; (2) Contextual-Enhanced Semantic Refinement (CSR) to build and use a context dictionary for robust representations; and (3) Prototypical Alignment (PA) to align image representations with class prototypes and amplify feature distances for better discrimination.

Result: Experimental evaluations on two datasets demonstrated that ProtoConNet achieved superior performance in representation learning and open-set sample identification, outperforming existing methods.

Conclusion: ProtoConNet successfully integrates contextual information to enhance diversity in feature spaces, improve representation learning, and boost the ability to distinguish known and unknown classes in few-shot scenarios.

Abstract: Open-set few-shot image classification aims to train models using a small
amount of labeled data, enabling them to achieve good generalization when
confronted with unknown environments. Existing methods mainly use visual
information from a single image to learn class representations to distinguish
known from unknown categories. However, these methods often overlook the
benefits of integrating rich contextual information. To address this issue,
this paper proposes a prototypical augmentation and alignment method, termed
ProtoConNet, which incorporates background information from different samples
to enhance the diversity of the feature space, breaking the spurious
associations between context and image subjects in few-shot scenarios.
Specifically, it consists of three main modules: the clustering-based data
selection (CDS) module mines diverse data patterns while preserving core
features; the contextual-enhanced semantic refinement (CSR) module builds a
context dictionary to integrate into image representations, which boosts the
model's robustness in various scenarios; and the prototypical alignment (PA)
module reduces the gap between image representations and class prototypes,
amplifying feature distances for known and unknown classes. Experimental
results from two datasets verified that ProtoConNet enhances the effectiveness
of representation learning in few-shot scenarios and identifies open-set
samples, making it superior to existing methods.

</details>


### [82] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: The paper proposes GRACE, a novel method for dynamic facial expression recognition (DFER) using token-level cross-modal alignment to improve spatiotemporal emotion detection.


<details>
  <summary>Details</summary>
Motivation: Existing methods in DFER still struggle to effectively utilize subtle emotional cues from text descriptions and lack robust mechanisms to focus on emotion-relevant facial dynamics.

Method: The proposed GRACE method introduces Coarse-to-fine Affective Text Enhancement (CATE) for refining semantic text, integrates a motion-difference weighting mechanism for filtering facial dynamics, and employs entropy-regularized optimal transport for token-level cross-modal alignment.

Result: Experimental findings show that GRACE delivers significant performance enhancements on three benchmark datasets, particularly in scenarios with ambiguous or imbalanced emotions, achieving state-of-the-art results in UAR and WAR metrics.

Conclusion: GRACE successfully addresses key limitations in DFER by integrating refined textual descriptions and precise alignment of emotionally salient features, advancing the field with new SOTA benchmarks.

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [83] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Main category: cs.CV

TL;DR: Semantic segmentation performance often suffers from loss of high-frequency information due to downsampling. This paper introduces Spatial Frequency Modulation (SFM), which modulates high-frequency signals to prevent information distortion in downsampling and then retrieves them during upsampling.


<details>
  <summary>Details</summary>
Motivation: High-frequency details, critical for semantic segmentation, are distorted during downsampling processes due to aliasing and the limitations posed by the Nyquist-Shannon Sampling Theorem.

Method: The SFM method involves modulating high-frequency features to lower frequencies with adaptive resampling (ARS) during downsampling and demodulating them with Multi-Scale Adaptive Upsampling (MSAU) to recover lost details. It integrates efficiently with a variety of neural architectures.

Result: The proposed approach demonstrates reduced aliasing effects and successful retention of fine details. It shows improved performance in semantic segmentation and other tasks such as image classification, adversarial robustness, and panoptic segmentation.

Conclusion: SFM effectively preserves high-frequency information during downsampling, validating its potential for broad application across multiple tasks and architectures.

Abstract: High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and sparsely resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to transformers. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [84] [SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring](https://arxiv.org/abs/2507.11910)
*Kaustav Chanda,Aayush Atul Verma,Arpitsinh Vaghela,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: The paper introduces SEPose, a synthetic dataset for event-based human pose estimation in traffic monitoring, designed to improve response times under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: There is a lack of data covering distracted walking or unusual pedestrian movements under diverse scenarios, which hinders the development of effective monitoring systems.

Method: The authors developed SEPose, a synthetic dataset with 350K annotated pedestrian poses using dynamic vision sensors in the CARLA simulator, encompassing diverse traffic, weather, and lighting conditions.

Result: State-of-the-art models like RVT and YOLOv8 were trained on SEPose and tested on real event-based data, showing strong sim-to-real generalization.

Conclusion: SEPose addresses data scarcity for event-based systems, offering a robust dataset for improving pedestrian monitoring under various conditions.

Abstract: Event-based sensors have emerged as a promising solution for addressing
challenging conditions in pedestrian and traffic monitoring systems. Their
low-latency and high dynamic range allow for improved response time in
safety-critical situations caused by distracted walking or other unusual
movements. However, the availability of data covering such scenarios remains
limited. To address this gap, we present SEPose -- a comprehensive synthetic
event-based human pose estimation dataset for fixed pedestrian perception
generated using dynamic vision sensors in the CARLA simulator. With nearly 350K
annotated pedestrians with body pose keypoints from the perspective of fixed
traffic cameras, SEPose is a comprehensive synthetic multi-person pose
estimation dataset that spans busy and light crowds and traffic across diverse
lighting and weather conditions in 4-way intersections in urban, suburban, and
rural environments. We train existing state-of-the-art models such as RVT and
YOLOv8 on our dataset and evaluate them on real event-based data to demonstrate
the sim-to-real generalization capabilities of the proposed dataset.

</details>


### [85] [Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark](https://arxiv.org/abs/2507.11931)
*Jingqian Wu,Peiqi Duan,Zongqiang Wang,Changwei Wang,Boxin Shi,Edmund Y. Lam*

Main category: cs.CV

TL;DR: Event-assisted frameworks like Dark-EvGS improve 3D radiance field reconstruction in challenging low-light conditions through advanced image rendering and color matching techniques.


<details>
  <summary>Details</summary>
Motivation: Conventional cameras struggle in low-light environments due to motion blur and dynamic range limitations. Event cameras with their high-speed and high-dynamic range properties provide a possible solution, especially when combined with methods like 3D Gaussian Splatting (GS).

Method: Dark-EvGS incorporates event cameras within a 3D GS framework for bright image reconstruction. It introduces triplet-level supervision for enhanced rendering and a color tone matching block for consistent output. The authors also present a novel dataset to further the evaluation of their method.

Result: Experimental results show that Dark-EvGS outperforms existing methods in synthesizing high-quality, color-consistent bright frames under low-light conditions.

Conclusion: Dark-EvGS overcomes the challenges of low-light radiance field reconstruction using event cameras and offers practical advancements for 3D image synthesis with improved sharpness and consistency. This framework demonstrates superior performance over existing solutions.

Abstract: In low-light environments, conventional cameras often struggle to capture
clear multi-view images of objects due to dynamic range limitations and motion
blur caused by long exposure. Event cameras, with their high-dynamic range and
high-speed properties, have the potential to mitigate these issues.
Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,
facilitating bright frame synthesis from multiple viewpoints in low-light
conditions. However, naively using an event-assisted 3D GS approach still faced
challenges because, in low light, events are noisy, frames lack quality, and
the color tone may be inconsistent. To address these issues, we propose
Dark-EvGS, the first event-assisted 3D GS framework that enables the
reconstruction of bright frames from arbitrary viewpoints along the camera
trajectory. Triplet-level supervision is proposed to gain holistic knowledge,
granular details, and sharp scene rendering. The color tone matching block is
proposed to guarantee the color consistency of the rendered frames.
Furthermore, we introduce the first real-captured dataset for the event-guided
bright frame synthesis task via 3D GS-based radiance field reconstruction.
Experiments demonstrate that our method achieves better results than existing
methods, conquering radiance field reconstruction under challenging low-light
conditions. The code and sample data are included in the supplementary
material.

</details>


### [86] [Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs](https://arxiv.org/abs/2507.11932)
*Mohammad Shahab Sepehri,Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: The paper presents Hyperphantasia, a benchmark to assess the mental visualization capabilities of Multimodal Large Language Models (MLLMs), revealing their limitations compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks assessing active mental visualization capabilities in MLLMs, crucial for problem-solving and other cognitive tasks.

Method: Introduction of the Hyperphantasia benchmark with procedurally generated visual puzzles at three difficulty levels, followed by model evaluation and reinforcement learning explorations.

Result: State-of-the-art MLLMs show gaps in mental visualization when compared to human performance.

Conclusion: Robust mental visualization in MLLMs is still an unresolved challenge, despite some partial capabilities in pattern recognition.

Abstract: Mental visualization, the ability to construct and manipulate visual
representations internally, is a core component of human cognition and plays a
vital role in tasks involving reasoning, prediction, and abstraction. Despite
the rapid progress of Multimodal Large Language Models (MLLMs), current
benchmarks primarily assess passive visual perception, offering limited insight
into the more active capability of internally constructing visual patterns to
support problem solving. Yet mental visualization is a critical cognitive skill
in humans, supporting abilities such as spatial navigation, predicting physical
trajectories, and solving complex visual problems through imaginative
simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic
benchmark designed to evaluate the mental visualization abilities of MLLMs
through four carefully constructed puzzles. Each task is procedurally generated
and presented at three difficulty levels, enabling controlled analysis of model
performance across increasing complexity. Our comprehensive evaluation of
state-of-the-art models reveals a substantial gap between the performance of
humans and MLLMs. Additionally, we explore the potential of reinforcement
learning to improve visual simulation capabilities. Our findings suggest that
while some models exhibit partial competence in recognizing visual patterns,
robust mental visualization remains an open challenge for current MLLMs.

</details>


### [87] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: RaDL is a framework that improves text-to-image generation for handling multiple instances within a single image prompt by enhancing positional accuracy, and considering attributes and relationships between instances.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in accurately generating multiple instances in an image with preserved individual attributes and relationship consistency, which existing methods fail to fully address.

Method: RaDL uses learnable parameters to refine instance-specific attributes and applies Relation Attention based on action verbs from the global prompt to create relation-aware features.

Result: RaDL outperforms existing techniques in benchmarks like COCO-Position, COCO-MIG, and DrawBench, showing superior results in positional accuracy, handling multi-attributes, and managing relationships between instances.

Conclusion: RaDL provides a significant advancement in generating coherent and attribute-aware multi-instance images, addressing specific shortcomings of earlier methods.

Abstract: With recent advancements in text-to-image (T2I) models, effectively
generating multiple instances within a single image prompt has become a crucial
challenge. Existing methods, while successful in generating positions of
individual instances, often struggle to account for relationship discrepancy
and multiple attributes leakage. To address these limitations, this paper
proposes the relation-aware disentangled learning (RaDL) framework. RaDL
enhances instance-specific attributes through learnable parameters and
generates relation-aware image features via Relation Attention, utilizing
action verbs extracted from the global prompt. Through extensive evaluations on
benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that
RaDL outperforms existing methods, showing significant improvements in
positional accuracy, multiple attributes consideration, and the relationships
between instances. Our results present RaDL as the solution for generating
images that consider both the relationships and multiple attributes of each
instance within the multi-instance image.

</details>


### [88] [Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation](https://arxiv.org/abs/2507.11955)
*Yuhang Zhang,Zhengyu Zhang,Muxin Liao,Shishun Tian,Wenbin Zou,Lu Zhang,Chen Xu*

Main category: cs.CV

TL;DR: This paper introduces a novel framework, PPAR, to enhance semantic segmentation generalization using domain-invariant class prototypes and leveraging CLIP's generalization ability.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of achieving high generalizability in semantic segmentation for unseen target domains in real-world applications.

Method: PPAR uses two prototypes (Original and Visual Text Prototypes derived from CLIP), employs a progressive alignment strategy for adaptation, and incorporates a reweighting mechanism to reduce the impact of irrelevant features.

Result: PPAR achieves state-of-the-art performance across multiple benchmarks, showcasing its ability to generalize effectively.

Conclusion: The framework addresses key limitations of existing methods, improves alignment and adaptation difficulties, and aligns with theoretical domain generalization principles, making it highly effective for unseen domains.

Abstract: Generalizable semantic segmentation aims to perform well on unseen target
domains, a critical challenge due to real-world applications requiring high
generalizability. Class-wise prototypes, representing class centroids, serve as
domain-invariant cues that benefit generalization due to their stability and
semantic consistency. However, this approach faces three challenges. First,
existing methods often adopt coarse prototypical alignment strategies, which
may hinder performance. Second, naive prototypes computed by averaging source
batch features are prone to overfitting and may be negatively affected by
unrelated source data. Third, most methods treat all source samples equally,
ignoring the fact that different features have varying adaptation difficulties.
To address these limitations, we propose a novel framework for generalizable
semantic segmentation: Prototypical Progressive Alignment and Reweighting
(PPAR), leveraging the strong generalization ability of the CLIP model.
Specifically, we define two prototypes: the Original Text Prototype (OTP) and
Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for
alignment. We then introduce a progressive alignment strategy that aligns
features in an easy-to-difficult manner, reducing domain gaps gradually.
Furthermore, we propose a prototypical reweighting mechanism that estimates the
reliability of source data and adjusts its contribution, mitigating the effect
of irrelevant or harmful features (i.e., reducing negative transfer). We also
provide a theoretical analysis showing the alignment between our method and
domain generalization theory. Extensive experiments across multiple benchmarks
demonstrate that PPAR achieves state-of-the-art performance, validating its
effectiveness.

</details>


### [89] [Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos](https://arxiv.org/abs/2507.11967)
*Yuchi Ishikawa,Shota Nakada,Hokuto Munakata,Kazuhiro Saito,Tatsuya Komatsu,Yoshimitsu Aoki*

Main category: cs.CV

TL;DR: The paper introduces LG-CAV-MAE, a model leveraging text as a third modality to improve audio-visual representation learning.


<details>
  <summary>Details</summary>
Motivation: There is a need for better integration of audio, visual, and text data in representation learning models to achieve higher performance and rely less on manual annotations.

Method: The researchers employed a pretrained text encoder with contrastive audio-visual masked autoencoders, automatically generating high-quality audio-visual-text triplets using a video captioning model and CLAP-filtering in unlabeled videos.

Result: LG-CAV-MAE demonstrated significant improvement in audio-visual retrieval (5.6% in recall@10) and classification tasks (3.2%).

Conclusion: Integrating text as a modality into audio-visual models leads to superior performance in learning and retrieval tasks while reducing dependency on labeled data.

Abstract: In this paper, we propose Language-Guided Contrastive Audio-Visual Masked
Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning.
LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual
masked autoencoders, enabling the model to learn across audio, visual and text
modalities. To train LG-CAV-MAE, we introduce an automatic method to generate
audio-visual-text triplets from unlabeled videos. We first generate frame-level
captions using an image captioning model and then apply CLAP-based filtering to
ensure strong alignment between audio and captions. This approach yields
high-quality audio-visual-text triplets without requiring manual annotations.
We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an
audio-visual classification task. Our method significantly outperforms existing
approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks
and a 3.2% improvement for the classification task.

</details>


### [90] [Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation](https://arxiv.org/abs/2507.11968)
*Sahid Hossain Mustakim,S M Jishanul Islam,Ummay Maria Muna,Montasir Chowdhury,Mohammed Jawwadul Islam,Sadia Ahmmed,Tashfia Sikder,Syed Tasdid Azam Dhrubo,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: This paper evaluates the safety of Multimodal Large Language Models (MLLMs) under combined attacks and introduces the SVMA dataset alongside the ChimeraBreak tri-modal attack strategy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the underexplored robustness of MLLMs in short-form video content moderation, especially against combined multimodal attacks.

Method: The authors develop the SVMA dataset featuring adversarial short-form videos and propose ChimeraBreak, which challenges MLLMs across visual, auditory, and semantic modalities.

Result: Extensive experiments demonstrate significant vulnerabilities in state-of-the-art MLLMs, with high Attack Success Rates revealing biases in content classification.

Conclusion: The findings emphasize model susceptibility and offer datasets and benchmarks to enhance the safety and robustness of MLLMs for tri-modal applications.

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used for content
moderation, yet their robustness in short-form video contexts remains
underexplored. Current safety evaluations often rely on unimodal attacks,
failing to address combined attack vulnerabilities. In this paper, we introduce
a comprehensive framework for evaluating the tri-modal safety of MLLMs. First,
we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising
diverse short-form videos with human-guided synthetic adversarial attacks.
Second, we propose ChimeraBreak, a novel tri-modal attack strategy that
simultaneously challenges visual, auditory, and semantic reasoning pathways.
Extensive experiments on state-of-the-art MLLMs reveal significant
vulnerabilities with high Attack Success Rates (ASR). Our findings uncover
distinct failure modes, showing model biases toward misclassifying benign or
policy-violating content. We assess results using LLM-as-a-judge, demonstrating
attack reasoning efficacy. Our dataset and findings provide crucial insights
for developing more robust and safe MLLMs.

</details>


### [91] [GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2507.11969)
*Zhaohong Huang,Yuxin Zhang,Jingjing Xie,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: The paper introduces GS-Bias, a new test-time adaptation method enhancing efficiency and performance for vision-language models by using learnable global and spatial biases.


<details>
  <summary>Details</summary>
Motivation: Current methods for test-time adaptation in vision-language models are either inefficient or unstable in balancing performance and computational overhead related to tuning text prompts or visual feature enhancement.

Method: The proposed GS-Bias employs two learnable biases, global bias and spatial bias, integrated directly into the logits of the pretrained models without requiring full backpropagation. This innovative approach leverages augmented view consistency and spatial semantic coherence.

Result: The proposed method shows state-of-the-art performance on 15 benchmarks, improving cross-dataset generalization by 2.23% and domain generalization by 2.72%, while using just 6.5% of the memory required by previous methods (e.g., TPT on ImageNet).

Conclusion: GS-Bias achieves a significant balance of efficiency and performance in TTA for vision-language models, overcoming the limitations of existing methods with minimal computational requirements.

Abstract: Recent advances in test-time adaptation (TTA) for Vision-Language Models
(VLMs) have garnered increasing attention, particularly through the use of
multiple augmented views of a single image to boost zero-shot generalization.
Unfortunately, existing methods fail to strike a satisfactory balance between
performance and efficiency, either due to excessive overhead of tuning text
prompts or unstable benefits from handcrafted, training-free visual feature
enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),
an efficient and effective TTA paradigm that incorporates two learnable biases
during TTA, unfolded as the global bias and spatial bias. Particularly, the
global bias captures the global semantic features of a test image by learning
consistency across augmented views, while spatial bias learns the semantic
coherence between regions in the image's spatial visual representation. It is
worth highlighting that these two sets of biases are directly added to the
logits outputed by the pretrained VLMs, which circumvent the full
backpropagation through VLM that hinders the efficiency of existing TTA
methods. This endows GS-Bias with extremely high efficiency while achieving
state-of-the-art performance on 15 benchmark datasets. For example, it achieves
a 2.23% improvement over TPT in cross-dataset generalization and a 2.72%
improvement in domain generalization, while requiring only 6.5% of TPT's memory
usage on ImageNet.

</details>


### [92] [EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models](https://arxiv.org/abs/2507.11980)
*Jiajian Xie,Shengyu Zhang,Zhou Zhao,Fan Wu,Fei Wu*

Main category: cs.CV

TL;DR: The paper introduces EC-Diff, a method to enhance hybrid edge-cloud collaborative frameworks for diffusion models by balancing inference time and generation quality, achieving faster processing with consistent quality.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for image and video synthesis have increased in size and latency, creating a need for hybrid edge-cloud frameworks. However, excessive or insufficient cloud denoising steps create issues with inference time and generation quality.

Method: EC-Diff introduces a K-step noise approximation strategy to reduce cloud inference frequency using noise gradients, paired with a two-stage greedy search algorithm to optimize noise approximation and the cloud-edge switching point.

Result: EC-Diff improves generation quality over edge-only inference and achieves up to double the inference speedup compared to cloud-only inference.

Conclusion: The proposed EC-Diff method balances fast inference and high-quality generation in hybrid edge-cloud frameworks, proving to be an efficient solution for diffusion models.

Abstract: Diffusion Models have shown remarkable proficiency in image and video
synthesis. As model size and latency increase limit user experience, hybrid
edge-cloud collaborative framework was recently proposed to realize fast
inference and high-quality generation, where the cloud model initiates
high-quality semantic planning and the edge model expedites later-stage
refinement. However, excessive cloud denoising prolongs inference time, while
insufficient steps cause semantic ambiguity, leading to inconsistency in edge
model output. To address these challenges, we propose EC-Diff that accelerates
cloud inference through gradient-based noise estimation while identifying the
optimal point for cloud-edge handoff to maintain generation quality.
Specifically, we design a K-step noise approximation strategy to reduce cloud
inference frequency by using noise gradients between steps and applying cloud
inference periodically to adjust errors. Then we design a two-stage greedy
search algorithm to efficiently find the optimal parameters for noise
approximation and edge model switching. Extensive experiments demonstrate that
our method significantly enhances generation quality compared to edge
inference, while achieving up to an average $2\times$ speedup in inference
compared to cloud inference. Video samples and source code are available at
https://ec-diff.github.io/.

</details>


### [93] [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/abs/2507.11985)
*Jiahao Xia,Yike Wu,Wenjian Huang,Jianguo Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: The paper introduces Masked Part Autoencoder (MPAE), an unsupervised approach to discovering meaningful object parts in images, even in challenging scenarios and across categories.


<details>
  <summary>Details</summary>
Motivation: Part-based features are important for image understanding, but current methods are limited due to the lack of fine-grained labels or robustness across various categories and scenarios.

Method: The proposed MPAE learns part descriptors and feature maps, uses masked images to align parts better with their shapes, and incorporates looser constraints to improve robustness and adaptability.

Result: MPAE outperforms existing methods by robustly identifying meaningful parts even under occlusion and across diverse categories. Extensive experiments validate its efficacy.

Conclusion: MPAE expands the application of unsupervised part discovery to broader and more complex scenarios, providing a solid foundation for addressing occlusion and part similarities across categories.

Abstract: Part-level features are crucial for image understanding, but few studies
focus on them because of the lack of fine-grained labels. Although unsupervised
part discovery can eliminate the reliance on labels, most of them cannot
maintain robustness across various categories and scenarios, which restricts
their application range. To overcome this limitation, we present a more
effective paradigm for unsupervised part discovery, named Masked Part
Autoencoder (MPAE). It first learns part descriptors as well as a feature map
from the inputs and produces patch features from a masked version of the
original images. Then, the masked regions are filled with the learned part
descriptors based on the similarity between the local features and descriptors.
By restoring these masked patches using the part descriptors, they become
better aligned with their part shapes, guided by appearance features from
unmasked patches. Finally, MPAE robustly discovers meaningful parts that
closely match the actual object shapes, even in complex scenarios. Moreover,
several looser yet more effective constraints are proposed to enable MPAE to
identify the presence of parts across various scenarios and categories in an
unsupervised manner. This provides the foundation for addressing challenges
posed by occlusion and for exploring part similarity across multiple
categories. Extensive experiments demonstrate that our method robustly
discovers meaningful parts across various categories and scenarios. The code is
available at the project https://github.com/Jiahao-UTS/MPAE.

</details>


### [94] [Style Composition within Distinct LoRA modules for Traditional Art](https://arxiv.org/abs/2507.11986)
*Jaehyun Lee,Wonhark Park,Wonsik Shin,Hyunho Lee,Hyoung Min Na,Nojun Kwak*

Main category: cs.CV

TL;DR: The paper introduces a diffusion-based method for precise region-specific style control in text-to-image synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with blending multiple styles regionally, as their latent spaces and interpolation capabilities are limited.

Method: The authors propose a zero-shot diffusion pipeline using spatial masks to blend styles during the denoising process of style-specialized models. Additionally, depth-map conditioning is applied.

Result: The proposed method enables successful, user-controlled regional style mixing, as evidenced by qualitative and quantitative experiments.

Conclusion: The approach preserves individual style fidelity while achieving coherent, region-specific style blending, advancing text-to-image synthesis capabilities.

Abstract: Diffusion-based text-to-image models have achieved remarkable results in
synthesizing diverse images from text prompts and can capture specific artistic
styles via style personalization. However, their entangled latent space and
lack of smooth interpolation make it difficult to apply distinct painting
techniques in a controlled, regional manner, often causing one style to
dominate. To overcome this, we propose a zero-shot diffusion pipeline that
naturally blends multiple styles by performing style composition on the
denoised latents predicted during the flow-matching denoising process of
separately trained, style-specialized models. We leverage the fact that
lower-noise latents carry stronger stylistic information and fuse them across
heterogeneous diffusion pipelines using spatial masks, enabling precise,
region-specific style control. This mechanism preserves the fidelity of each
individual style while allowing user-guided mixing. Furthermore, to ensure
structural coherence across different models, we incorporate depth-map
conditioning via ControlNet into the diffusion framework. Qualitative and
quantitative experiments demonstrate that our method successfully achieves
region-specific style mixing according to the given masks.

</details>


### [95] [ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2507.11990)
*Hyun-Jun Jin,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: This paper introduces ID-EA, a framework that enhances identity preservation in personalized portrait generation using text-to-image diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current Textual Inversion methods in text-to-image models face challenges in maintaining consistent facial identity due to semantic misalignments between text and visual embedding spaces.

Method: The ID-EA framework uses two components: (1) ID-Enhancer to refine identity embeddings by merging textual and visual embeddings, and (2) ID-Adapter to adjust text conditions and cross-attention mechanisms within the UNet model.

Result: ID-EA significantly improves identity preservation metrics and achieves higher computational efficiency, producing personalized portraits 15 times faster than existing techniques.

Conclusion: ID-EA effectively aligns textual embeddings with visual identity embeddings, offering high-fidelity personalized portrait generation while boosting identity consistency and computational efficiency.

Abstract: Recently, personalized portrait generation with a text-to-image diffusion
model has significantly advanced with Textual Inversion, emerging as a
promising approach for creating high-fidelity personalized images. Despite its
potential, current Textual Inversion methods struggle to maintain consistent
facial identity due to semantic misalignments between textual and visual
embedding spaces regarding identity. We introduce ID-EA, a novel framework that
guides text embeddings to align with visual identity embeddings, thereby
improving identity preservation in a personalized generation. ID-EA comprises
two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned
Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings
with a textual ID anchor, refining visual identity embeddings derived from a
face recognition model using representative text embeddings. Then, the
ID-Adapter leverages the identity-enhanced embedding to adapt the text
condition, ensuring identity preservation by adjusting the cross-attention
module in the pre-trained UNet model. This process encourages the text features
to find the most related visual clues across the foreground snippets. Extensive
quantitative and qualitative evaluations demonstrate that ID-EA substantially
outperforms state-of-the-art methods in identity preservation metrics while
achieving remarkable computational efficiency, generating personalized
portraits approximately 15 times faster than existing approaches.

</details>


### [96] [SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation](https://arxiv.org/abs/2507.12027)
*Beining Xu,Siting Zhu,Hesheng Wang*

Main category: cs.CV

TL;DR: SGLoc is a localization method using 3D Gaussian Splatting and semantic information to estimate camera poses without prior pose information. It achieves accurate global localization on datasets like 12scenes and 7scenes.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of pose estimation without relying on initial pose priors by leveraging semantic relationships between 2D images and 3D scene representations.

Method: Utilizes a multi-level pose regression strategy along with semantic-based retrieval to align 2D images with regions of a 3D Gaussian Splatting map for coarse and refined pose estimation.

Result: SGLoc outperforms baseline methods on 12scenes and 7scenes datasets, demonstrating accurate and reliable global localization capabilities.

Conclusion: SGLoc effectively estimates camera poses without pose priors by leveraging semantic information and has strong scalability with publicly available code.

Abstract: We propose SGLoc, a novel localization system that directly regresses camera
poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic
information. Our method utilizes the semantic relationship between 2D image and
3D scene representation to estimate the 6DoF pose without prior pose
information. In this system, we introduce a multi-level pose regression
strategy that progressively estimates and refines the pose of query image from
the global 3DGS map, without requiring initial pose priors. Moreover, we
introduce a semantic-based global retrieval algorithm that establishes
correspondences between 2D (image) and 3D (3DGS map). By matching the extracted
scene semantic descriptors of 2D query image and 3DGS semantic representation,
we align the image with the local region of the global 3DGS map, thereby
obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by
iteratively optimizing the difference between the query image and the rendered
image from 3DGS. Our SGLoc demonstrates superior performance over baselines on
12scenes and 7scenes datasets, showing excellent capabilities in global
localization without initial pose prior. Code will be available at
https://github.com/IRMVLab/SGLoc.

</details>


### [97] [SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation](https://arxiv.org/abs/2507.11994)
*Jun Yin,Fei Wu,Yupeng Ren,Jisheng Huang,Qiankun Li,Heng jin,Jianhai Fu,Chanjie Cui*

Main category: cs.CV

TL;DR: The paper introduces SAMST, a semi-supervised method for remote sensing semantic segmentation that combines the Segment Anything Model (SAM) for refining pseudo-labels, improving model performance on datasets with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Public remote sensing datasets are often constrained by inconsistent resolution and land cover definitions, necessitating better methods for utilizing large pools of unlabeled data.

Method: SAMST employs a self-training process with supervised models and refines pseudo-labels using SAM. It uses three modules—Threshold Filter, Prompt Generation, and Label Refinement—for pseudo-label enhancement.

Result: Experiments on the Potsdam dataset confirm that SAMST improves pseudo-label accuracy and delivers better overall performance in remote sensing segmentation tasks.

Conclusion: SAMST successfully addresses the limitations of labeled data in remote sensing segmentation by integrating large and small model strengths, offering a viable solution to enhance performance.

Abstract: Public remote sensing datasets often face limitations in universality due to
resolution variability and inconsistent land cover category definitions. To
harness the vast pool of unlabeled remote sensing data, we propose SAMST, a
semi-supervised semantic segmentation method. SAMST leverages the strengths of
the Segment Anything Model (SAM) in zero-shot generalization and boundary
detection. SAMST iteratively refines pseudo-labels through two main components:
supervised model self-training using both labeled and pseudo-labeled data, and
a SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three
modules: a Threshold Filter Module for preprocessing, a Prompt Generation
Module for extracting connected regions and generating prompts for SAM, and a
Label Refinement Module for final label stitching. By integrating the
generalization power of large models with the training efficiency of small
models, SAMST improves pseudo-label accuracy, thereby enhancing overall model
performance. Experiments on the Potsdam dataset validate the effectiveness and
feasibility of SAMST, demonstrating its potential to address the challenges
posed by limited labeled data in remote sensing semantic segmentation.

</details>


### [98] [Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics](https://arxiv.org/abs/2507.12083)
*Muleilan Pei,Shaoshuai Shi,Xuesong Chen,Xu Liu,Shaojie Shen*

Main category: cs.CV

TL;DR: The paper introduces a motion forecasting approach for traffic agents that integrates reasoning about intentions before predicting trajectories using a novel reward-driven IRL method.


<details>
  <summary>Details</summary>
Motivation: To improve autonomous driving safety by addressing the challenge of accurate motion forecasting, particularly by incorporating behavior intention reasoning instead of directly predicting trajectories.

Method: Proposes a novel query-centric Inverse Reinforcement Learning (IRL) approach, creating a unified vectorized representation of agents and environments, reasoning intentions through policy rollouts, and generating trajectories using a hierarchical DETR-like decoder.

Result: The method achieves competitive performance in trajectory prediction on the Argoverse and nuScenes datasets, enhancing prediction confidence.

Conclusion: The proposed reasoning and forecasting strategy effectively improves both the interpretability and accuracy of motion forecasting, offering a significant advance over existing state-of-the-art approaches.

Abstract: Motion forecasting for on-road traffic agents presents both a significant
challenge and a critical necessity for ensuring safety in autonomous driving
systems. In contrast to most existing data-driven approaches that directly
predict future trajectories, we rethink this task from a planning perspective,
advocating a "First Reasoning, Then Forecasting" strategy that explicitly
incorporates behavior intentions as spatial guidance for trajectory prediction.
To achieve this, we introduce an interpretable, reward-driven intention
reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)
scheme. Our method first encodes traffic agents and scene elements into a
unified vectorized representation, then aggregates contextual features through
a query-centric paradigm. This enables the derivation of a reward distribution,
a compact yet informative representation of the target agent's behavior within
the given scene context via IRL. Guided by this reward heuristic, we perform
policy rollouts to reason about multiple plausible intentions, providing
valuable priors for subsequent trajectory generation. Finally, we develop a
hierarchical DETR-like decoder integrated with bidirectional selective state
space models to produce accurate future trajectories along with their
associated probabilities. Extensive experiments on the large-scale Argoverse
and nuScenes motion forecasting datasets demonstrate that our approach
significantly enhances trajectory prediction confidence, achieving highly
competitive performance relative to state-of-the-art methods.

</details>


### [99] [AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation](https://arxiv.org/abs/2507.12001)
*Hao Li,Ju Dai,Feng Zhou,Kaida Ning,Lei Li,Junjun Pan*

Main category: cs.CV

TL;DR: The paper addresses challenges in fine-grained stylized 3D facial expression manipulation by introducing the AUBlendSet dataset and AUBlendNet network, leveraging facial action units (AUs) for diverse identities.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in stylized 3D facial expression manipulation due to the lack of suitable datasets and tools.

Method: The paper introduces AUBlendSet, a dataset based on 32 facial action units (AUs) across 500 identities, and proposes AUBlendNet, a network learning AU-Blendshape basis vectors for stylized manipulation.

Result: Experiments demonstrate AUBlendSet and AUBlendNet's effectiveness in tasks like stylized expression manipulation, speech-driven animation, and emotion recognition augmentation.

Conclusion: AUBlendSet and AUBlendNet provide foundational tools for continuous 3D facial expression manipulation, emphasizing their utility and innovation in advancing 3D facial animation.

Abstract: While 3D facial animation has made impressive progress, challenges still
exist in realizing fine-grained stylized 3D facial expression manipulation due
to the lack of appropriate datasets. In this paper, we introduce the
AUBlendSet, a 3D facial dataset based on AU-Blendshape representation for
fine-grained facial expression manipulation across identities. AUBlendSet is a
blendshape data collection based on 32 standard facial action units (AUs)
across 500 identities, along with an additional set of facial postures
annotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to
learn AU-Blendshape basis vectors for different character styles. AUBlendNet
predicts, in parallel, the AU-Blendshape basis vectors of the corresponding
style for a given identity mesh, thereby achieving stylized 3D emotional facial
manipulation. We comprehensively validate the effectiveness of AUBlendSet and
AUBlendNet through tasks such as stylized facial expression manipulation,
speech-driven emotional facial animation, and emotion recognition data
augmentation. Through a series of qualitative and quantitative experiments, we
demonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D
facial animation tasks. To the best of our knowledge, AUBlendSet is the first
dataset, and AUBlendNet is the first network for continuous 3D facial
expression manipulation for any identity through facial AUs. Our source code is
available at https://github.com/wslh852/AUBlendNet.git.

</details>


### [100] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Main category: cs.CV

TL;DR: The paper introduces AutoVDC, a framework using Vision-Language Models (VLMs) to detect and correct annotation errors in vision datasets for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of laborious and costly manual dataset annotation reviews in autonomous driving by automating the error detection process.

Method: Utilizing Vision-Language Models (VLMs) in the AutoVDC framework to identify erroneous annotations within datasets (e.g., KITTI and nuImages), also testing effectiveness through dataset variants and model fine-tuning.

Result: High performance in error detection and data cleaning across datasets, proving the approach's effectiveness.

Conclusion: The framework can significantly enhance dataset reliability and accuracy for large-scale autonomous driving systems, reducing manual intervention.

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [101] [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/abs/2507.12006)
*Linwei Chen,Lin Gu,Ying Fu*

Main category: cs.CV

TL;DR: This paper introduces Frequency-Dynamic Attention Modulation (FDAM) to address frequency vanishing in Vision Transformers (ViTs), resulting in improved performance across tasks like segmentation and object detection.


<details>
  <summary>Details</summary>
Motivation: Existing Vision Transformers are hindered by frequency vanishing caused by their stacked-layer architecture, leading to loss of critical details in representation.

Method: The FDAM strategy modulates the frequency response of ViTs using two techniques: Attention Inversion (AttInv), which introduces high-pass filtering, and Frequency Dynamic Scaling (FreqScale) for fine-tuning response functions.

Result: FDAM delivers consistent improvements in performance across models (e.g., SegFormer and DeiT) and tasks (e.g., semantic segmentation and object detection), including state-of-the-art results in remote sensing detection.

Conclusion: FDAM effectively mitigates representation collapse in ViTs, enhancing their flexibility and robustness while achieving superior performance across diverse computer vision challenges.

Abstract: Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.

</details>


### [102] [Dual form Complementary Masking for Domain-Adaptive Image Segmentation](https://arxiv.org/abs/2507.12008)
*Jiawen Wang,Yinda Chen,Xiaoyu Liu,Che Liu,Dong Liu,Jianqing Gao,Zhiwei Xiong*

Main category: cs.CV

TL;DR: The paper introduces MaskTwins, a framework for unsupervised domain adaptation using masked reconstruction to uncover domain-invariant features, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Current research on Masked Image Modeling in Unsupervised Domain Adaptation lacks a theoretical foundation, limiting its capability to extract domain-agnostic features.

Method: The authors reframe masked reconstruction as a sparse signal reconstruction problem and propose MaskTwins, enforcing consistency via complementary masks in the main training pipeline.

Result: MaskTwins demonstrates superior performance in natural and biological image segmentation tasks compared to baseline methods by extracting domain-invariant features.

Conclusion: MaskTwins provides a novel approach for domain-adaptive segmentation without pre-training, leveraging domain-invariant structural patterns effectively.

Abstract: Recent works have correlated Masked Image Modeling (MIM) with consistency
regularization in Unsupervised Domain Adaptation (UDA). However, they merely
treat masking as a special form of deformation on the input images and neglect
the theoretical analysis, which leads to a superficial understanding of masked
reconstruction and insufficient exploitation of its potential in enhancing
feature extraction and representation learning. In this paper, we reframe
masked reconstruction as a sparse signal reconstruction problem and
theoretically prove that the dual form of complementary masks possesses
superior capabilities in extracting domain-agnostic image features. Based on
this compelling insight, we propose MaskTwins, a simple yet effective UDA
framework that integrates masked reconstruction directly into the main training
pipeline. MaskTwins uncovers intrinsic structural patterns that persist across
disparate domains by enforcing consistency between predictions of images masked
in complementary ways, enabling domain generalization in an end-to-end manner.
Extensive experiments verify the superiority of MaskTwins over baseline methods
in natural and biological image segmentation. These results demonstrate the
significant advantages of MaskTwins in extracting domain-invariant features
without the need for separate pre-training, offering a new paradigm for
domain-adaptive segmentation.

</details>


### [103] [Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli](https://arxiv.org/abs/2507.12009)
*Florian David,Michael Chan,Elenor Morgenroth,Patrik Vuilleumier,Dimitri Van De Ville*

Main category: cs.CV

TL;DR: The paper proposes an end-to-end neural model to interpret fMRI data in relation to visual stimuli and explores the contributions of specific brain regions using saliency maps.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand the encoding and decoding dynamics of brain activity in response to naturalistic stimuli, leveraging fMRI data and neural networks.

Method: The authors use an encoder-decoder architecture with temporal convolutional layers to map between temporal film frame data and fMRI data, predict voxel activity, reconstruct visual stimuli, and analyze saliency maps to identify key brain regions.

Result: The model successfully predicts visual cortex activity, reconstructs visual stimuli from neural activity, and highlights specific brain regions crucial for visual decoding (middle occipital, fusiform, and calcarine areas).

Conclusion: The method effectively aligns deep learning insights with brain functions, suggesting its potential to deepen understanding of visual processing during exposure to films.

Abstract: We propose an end-to-end deep neural encoder-decoder model to encode and
decode brain activity in response to naturalistic stimuli using functional
magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input
from consecutive film frames, we employ temporal convolutional layers in our
architecture, which effectively allows to bridge the temporal resolution gap
between natural movie stimuli and fMRI acquisitions. Our model predicts
activity of voxels in and around the visual cortex and performs reconstruction
of corresponding visual inputs from neural activity. Finally, we investigate
brain regions contributing to visual decoding through saliency maps. We find
that the most contributing regions are the middle occipital area, the fusiform
area, and the calcarine, respectively employed in shape perception, complex
recognition (in particular face perception), and basic visual features such as
edges and contrasts. These functions being strongly solicited are in line with
the decoder's capability to reconstruct edges, faces, and contrasts. All in
all, this suggests the possibility to probe our understanding of visual
processing in films using as a proxy the behaviour of deep learning models such
as the one proposed in this paper.

</details>


### [104] [SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection](https://arxiv.org/abs/2507.12017)
*Xiwei Zhang,Chunjin Yang,Yiming Xiao,Runtong Zhang,Fanman Meng*

Main category: cs.CV

TL;DR: This paper introduces a domain adaptation method for object detection from RGB to infrared by decoupling and coupling domain-invariant and domain-specific features using spectral decomposition.


<details>
  <summary>Details</summary>
Motivation: The challenge is adapting object detection models from RGB to infrared domains while addressing subdomain variances like daytime, nighttime, and foggy scenes within RGB data.

Method: The paper proposes the SS-DC framework with a Spectral Adaptive Idempotent Decoupling (SAID) module for spectral decomposition and a spatial-spectral coupling method for feature integration.

Result: Experiments demonstrate improved baseline performance and superior results over existing methods on RGB-IR datasets, including better benchmarks on the FLIR-ADAS dataset.

Conclusion: Decoupling and coupling domain-specific and invariant features via spectral and spatial methods is effective for domain adaptation in RGB-IR object detection.

Abstract: Unsupervised domain adaptive object detection (UDAOD) from the visible domain
to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB
domain as a unified domain and neglect the multiple subdomains within it, such
as daytime, nighttime, and foggy scenes. We argue that decoupling the
domain-invariant (DI) and domain-specific (DS) features across these multiple
subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper
proposes a new SS-DC framework based on a decoupling-coupling strategy. In
terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)
module in the aspect of spectral decomposition. Due to the style and content
information being highly embedded in different frequency bands, this module can
decouple DI and DS components more accurately and interpretably. A novel filter
bank-based spectral processing paradigm and a self-distillation-driven
decoupling loss are proposed to improve the spectral domain decoupling. In
terms of coupling, a new spatial-spectral coupling method is proposed, which
realizes joint coupling through spatial and spectral DI feature pyramids.
Meanwhile, this paper introduces DS from decoupling to reduce the domain bias.
Extensive experiments demonstrate that our method can significantly improve the
baseline performance and outperform existing UDAOD methods on multiple RGB-IR
datasets, including a new experimental protocol proposed in this paper based on
the FLIR-ADAS dataset.

</details>


### [105] [Dataset Ownership Verification for Pre-trained Masked Models](https://arxiv.org/abs/2507.12022)
*Yuechen Xie,Jie Song,Yicheng Shan,Xiaoyan Zhang,Yuanyu Wan,Shengxuming Zhang,Jiarui Duan,Mingli Song*

Main category: cs.CV

TL;DR: The paper introduces DOV4MM—a novel methodology for verifying ownership of datasets used in pre-training masked models.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge of protecting open-source datasets, particularly in verifying ownership of masked models.

Method: DOV4MM assesses reconstruction difficulty of masked information in embedding space to detect if a model utilized a specific dataset for pre-training.

Result: Evaluation on ten masked image models (ImageNet-1K) and four masked language models (WikiText-103) showed DOV4MM surpasses prior approaches with statistical significance.

Conclusion: DOV4MM effectively assists dataset owners in safeguarding rights by introducing a robust verification method for masked modeling.

Abstract: High-quality open-source datasets have emerged as a pivotal catalyst driving
the swift advancement of deep learning, while facing the looming threat of
potential exploitation. Protecting these datasets is of paramount importance
for the interests of their owners. The verification of dataset ownership has
evolved into a crucial approach in this domain; however, existing verification
techniques are predominantly tailored to supervised models and contrastive
pre-trained models, rendering them ill-suited for direct application to the
increasingly prevalent masked models. In this work, we introduce the inaugural
methodology addressing this critical, yet unresolved challenge, termed Dataset
Ownership Verification for Masked Modeling (DOV4MM). The central objective is
to ascertain whether a suspicious black-box model has been pre-trained on a
particular unlabeled dataset, thereby assisting dataset owners in safeguarding
their rights. DOV4MM is grounded in our empirical observation that when a model
is pre-trained on the target dataset, the difficulty of reconstructing masked
information within the embedding space exhibits a marked contrast to models not
pre-trained on that dataset. We validated the efficacy of DOV4MM through ten
masked image models on ImageNet-1K and four masked language models on
WikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,
with a $p$-value considerably below 0.05, surpassing all prior approaches. Code
is available at https://github.com/xieyc99/DOV4MM.

</details>


### [106] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Main category: cs.CV

TL;DR: This paper introduces MVAR, a model for multivariate air pollutant forecasting, which improves data efficiency, utilizes meteorological forecasts, and accounts for pollutant interactions and spatial responses.


<details>
  <summary>Details</summary>
Motivation: Current air pollutants forecasting methods often deal with single pollutants and overlook interactions between various pollutants and their different spatial behaviors.

Method: The paper proposes the MultiVariate AutoRegressive model (MVAR) with a unique training paradigm for long-term forecasting and a Meteorological Coupled Spatial Transformer block that integrates meteorological forecasts while learning pollutant interactions.

Result: Experiments with a newly constructed dataset spanning 6 pollutants and 75 cities demonstrate that MVAR outperforms existing methods and highlights its advanced capabilities.

Conclusion: MVAR provides a more accurate and efficient method for multivariate air pollutant forecasting, addressing key gaps in forecasting practices and delivering significant advancements for pollution management.

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [107] [3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering](https://arxiv.org/abs/2507.12026)
*Rongtao Xu,Han Gao,Mingming Yu,Dong An,Shunpeng Chen,Changwei Wang,Li Guo,Xiaodan Liang,Shibiao Xu*

Main category: cs.CV

TL;DR: The paper introduces 3D-MoRe, a framework for generating large-scale 3D-language datasets using foundational models, which enhances performance on indoor scene tasks like question answering and object description.


<details>
  <summary>Details</summary>
Motivation: The need for diverse and scalable indoor scene data for tasks such as dense captioning and question answering.

Method: The framework integrates multi-modal embedding, cross-modal interaction, and a language model decoder to process text instructions and 3D scene data. Data augmentation and semantic filtering ensure quality.

Result: 3D-MoRe generates 62,000 QA pairs and 73,000 object descriptions with improved scores on ScanQA (+2.15% CIDEr) and ScanRefer (+1.84% CIDEr@0.5) tasks.

Conclusion: 3D-MoRe demonstrates significant performance enhancements and will share its code and datasets publicly to advance community research.

Abstract: With the growing need for diverse and scalable data in indoor scene tasks,
such as question answering and dense captioning, we propose 3D-MoRe, a novel
paradigm designed to generate large-scale 3D-language datasets by leveraging
the strengths of foundational models. The framework integrates key components,
including multi-modal embedding, cross-modal interaction, and a language model
decoder, to process natural language instructions and 3D scene data. This
approach facilitates enhanced reasoning and response generation in complex 3D
environments. Using the ScanNet 3D scene dataset, along with text annotations
from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs
and 73,000 object descriptions across 1,513 scenes. We also employ various data
augmentation techniques and implement semantic filtering to ensure high-quality
data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms
state-of-the-art baselines, with the CIDEr score improving by 2.15\%.
Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5
by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated
datasets will be publicly released to benefit the community, and both can be
accessed on the https://3D-MoRe.github.io.

</details>


### [108] [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](https://arxiv.org/abs/2507.12029)
*Xinhang Wan,Jiyuan Liu,Qian Qu,Suyuan Liu,Chuyu Zhang,Fangdi Wang,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: The paper introduces a novel framework, IICMVNCD, for novel class discovery (NCD) using multi-view data, addressing limitations in existing methods' data dependency and label sensitivity.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome two key limitations in existing NCD methods: the focus on single-view data and the instability caused by reliance on pseudo-labels.

Method: The proposed framework employs matrix factorization for intra-view analysis and uses inter-view relationships to guide novel class clustering, incorporating dynamic view weight adjustments.

Result: Experimental results confirm the effectiveness of the IICMVNCD framework in clustering novel classes using multi-view data.

Conclusion: The IICMVNCD framework is a pioneering step in multi-view NCD research, showcasing improved clustering stability and addressing challenges in multi-view datasets.

Abstract: In this paper, we address the problem of novel class discovery (NCD), which
aims to cluster novel classes by leveraging knowledge from disjoint known
classes. While recent advances have made significant progress in this area,
existing NCD methods face two major limitations. First, they primarily focus on
single-view data (e.g., images), overlooking the increasingly common multi-view
data, such as multi-omics datasets used in disease diagnosis. Second, their
reliance on pseudo-labels to supervise novel class clustering often results in
unstable performance, as pseudo-label quality is highly sensitive to factors
such as data noise and feature dimensionality. To address these challenges, we
propose a novel framework named Intra-view and Inter-view Correlation Guided
Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to
explore NCD in multi-view setting so far. Specifically, at the intra-view
level, leveraging the distributional similarity between known and novel
classes, we employ matrix factorization to decompose features into
view-specific shared base matrices and factor matrices. The base matrices
capture distributional consistency among the two datasets, while the factor
matrices model pairwise relationships between samples. At the inter-view level,
we utilize view relationships among known classes to guide the clustering of
novel classes. This includes generating predicted labels through the weighted
fusion of factor matrices and dynamically adjusting view weights of known
classes based on the supervision loss, which are then transferred to novel
class learning. Experimental results validate the effectiveness of our proposed
approach.

</details>


### [109] [MoViAD: Modular Visual Anomaly Detection](https://arxiv.org/abs/2507.12049)
*Manuel Barusco,Francesco Borsatti,Arianna Stropeni,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: MoViAD is a modular library designed for the Visual Anomaly Detection (VAD) field, offering tools and frameworks for a variety of scenarios and deployment challenges.


<details>
  <summary>Details</summary>
Motivation: The task of VAD is impeded by limited anomalous data and the need for unsupervised learning. This paper aims to simplify research and practical deployment in VAD by introducing a library to streamline access to models, tools, and datasets.

Method: The MoViAD library encompasses modular support enabling state-of-the-art VAD, addressing continual learning, semi-supervised learning, and other scenarios. It also considers Edge and IoT needs through quantization, compression, and efficient deployment functionalities.

Result: The library demonstrates robust support for VAD with modular backbones, evaluation metrics, and usable tooling for efficiency and deployment analysis.

Conclusion: MoViAD simplifies the deployment and experimentation in the VAD field for engineers and researchers, facilitating the development of custom solutions or new methods in anomaly detection.

Abstract: VAD is a critical field in machine learning focused on identifying deviations
from normal patterns in images, often challenged by the scarcity of anomalous
data and the need for unsupervised training. To accelerate research and
deployment in this domain, we introduce MoViAD, a comprehensive and highly
modular library designed to provide fast and easy access to state-of-the-art
VAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array
of scenarios, including continual, semi-supervised, few-shots, noisy, and many
more. In addition, it addresses practical deployment challenges through
dedicated Edge and IoT settings, offering optimized models and backbones, along
with quantization and compression utilities for efficient on-device execution
and distributed inference. MoViAD integrates a selection of backbones, robust
evaluation VAD metrics (pixel-level and image-level) and useful profiling tools
for efficiency analysis. The library is designed for fast, effortless
deployment, enabling machine learning engineers to easily use it for their
specific setup with custom models, datasets, and backbones. At the same time,
it offers the flexibility and extensibility researchers need to develop and
experiment with new methods.

</details>


### [110] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: This paper introduces InstructFLIP, a framework that employs vision-language models (VLMs) to improve face anti-spoofing (FAS) by leveraging textual guidance to enhance generalization across diverse domains.


<details>
  <summary>Details</summary>
Motivation: To overcome two challenges in face anti-spoofing: the limited semantic understanding of attack types and the redundancy in training across multiple domains.

Method: The authors propose InstructFLIP, which uses a meta-domain strategy and instruction tuning of VLMs. These instructions are decoupled into content (semantic aspects of spoofing) and style (variations due to environment or camera).

Result: Extensive experiments show that InstructFLIP surpasses state-of-the-art models in accuracy and reduces training redundancy across domains.

Conclusion: InstructFLIP effectively enhances generalization in face anti-spoofing while addressing both major challenges, proving its robustness and efficiency across domains.

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [111] [MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning](https://arxiv.org/abs/2507.12062)
*Hongxu Ma,Guanshuo Wang,Fufu Yu,Qiong Jia,Shouhong Ding*

Main category: cs.CV

TL;DR: This paper introduces MS-DETR, a framework designed for video moment retrieval and highlight detection by leveraging temporal motion and spatial semantics relationships for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of harnessing temporal motion and spatial semantics relationships in video moment retrieval and highlight detection tasks.

Method: MS-DETR employs an encoder-decoder architecture where the encoder models intra-modal motion and semantics correlations and the decoder leverages task-wise correlations across motion and semantics dimensions. Data augmentation and contrastive denoising techniques are also applied.

Result: MS-DETR achieves superior performance compared to state-of-the-art models across four benchmarks for video moment retrieval and highlight detection.

Conclusion: The proposed MS-DETR framework effectively captures motion-semantics features, overcoming data sparsity issues and advancing the performance of MR and HD tasks.

Abstract: Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint
specific moments and assess clip-wise relevance based on the text query. While
DETR-based joint frameworks have made significant strides, there remains
untapped potential in harnessing the intricate relationships between temporal
motion and spatial semantics within video content. In this paper, we propose
the Motion-Semantics DETR (MS-DETR), a framework that captures rich
motion-semantics features through unified learning for MR/HD tasks. The encoder
first explicitly models disentangled intra-modal correlations within motion and
semantics dimensions, guided by the given text queries. Subsequently, the
decoder utilizes the task-wise correlation across temporal motion and spatial
semantics dimensions to enable precise query-guided localization for MR and
refined highlight boundary delineation for HD. Furthermore, we observe the
inherent sparsity dilemma within the motion and semantics dimensions of MR/HD
datasets. To address this issue, we enrich the corpus from both dimensions by
generation strategies and propose contrastive denoising learning to ensure the
above components learn robustly and effectively. Extensive experiments on four
MR/HD benchmarks demonstrate that our method outperforms existing
state-of-the-art models by a margin. Our code is available at
https://github.com/snailma0229/MS-DETR.git.

</details>


### [112] [YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association](https://arxiv.org/abs/2507.12087)
*Xiang Yu,Xinyao Liu,Guang Liang*

Main category: cs.CV

TL;DR: The paper presents a solution for tracking small, agile objects, like birds, from UAVs. It employs a detection-training framework named SliceTrain and improved tracking mechanisms achieving state-of-the-art performance in a related challenge.


<details>
  <summary>Details</summary>
Motivation: Tracking birds using UAVs is challenging due to scarce appearance features, complex motion dynamics, and occlusion from flocking behavior.

Method: The paper introduces the SliceTrain framework for enhanced training of small object detection and a robust tracker combining EMA mechanism, bounding box expansion, and distance penalty metrics into the OC-SORT framework.

Result: Their method achieved state-of-the-art results, with an SO-HOTA score of 55.205 in the SMOT4SB "Finding Birds" challenge.

Conclusion: The proposed innovations effectively address detection and tracking challenges, providing significant real-world advancements for multi-object tracking tasks. Code will be shared publicly.

Abstract: Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned
Aerial Vehicle (UAV) perspective is a highly challenging computer vision task.
The difficulty stems from three main sources: the extreme scarcity of target
appearance features, the complex motion entanglement caused by the combined
dynamics of the camera and the targets themselves, and the frequent occlusions
and identity ambiguity arising from dense flocking behavior. This paper details
our championship-winning solution in the MVA 2025 "Finding Birds" Small
Multi-Object Tracking Challenge (SMOT4SB), which adopts the
tracking-by-detection paradigm with targeted innovations at both the detection
and association levels. On the detection side, we propose a systematic training
enhancement framework named \textbf{SliceTrain}. This framework, through the
synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic
augmentation, effectively addresses the problem of insufficient learning for
small objects in high-resolution image training. On the tracking side, we
designed a robust tracker that is completely independent of appearance
information. By integrating a \textbf{motion direction maintenance (EMA)}
mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding
box expansion and distance penalty} into the OC-SORT framework, our tracker can
stably handle irregular motion and maintain target identities. Our method
achieves state-of-the-art performance on the SMOT4SB public test set, reaching
an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness
and advancement of our framework in solving complex real-world SMOT problems.
The source code will be made available at
https://github.com/Salvatore-Love/YOLOv8-SMOT.

</details>


### [113] [BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images](https://arxiv.org/abs/2507.12095)
*Davide Di Nucci,Matteo Tomei,Guido Borghi,Luca Ciuffreda,Roberto Vezzani,Rita Cucchiara*

Main category: cs.CV

TL;DR: The paper introduces a method to improve 3D vehicle reconstruction under sparse view inputs using depth maps, robust pose estimation, and optimized Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the need for accurate 3D vehicle reconstructions for applications like vehicle inspection and urban planning, and the limitations of existing methods relying on dense views.

Method: The paper enhances Gaussian Splatting with a selective photometric loss and uses the DUSt3R architecture for improving camera pose estimations. It also employs a novel dataset for evaluation featuring synthetic and real-world vehicle data.

Result: Experimental findings show state-of-the-art performance on multiple benchmarks, achieving high-quality reconstructions even with limited input views.

Conclusion: The proposed approach advances sparse-view 3D vehicle reconstruction, demonstrating robustness and effectiveness while tackling real-world challenges.

Abstract: Accurate 3D reconstruction of vehicles is vital for applications such as
vehicle inspection, predictive maintenance, and urban planning. Existing
methods like Neural Radiance Fields and Gaussian Splatting have shown
impressive results but remain limited by their reliance on dense input views,
which hinders real-world applicability. This paper addresses the challenge of
reconstructing vehicles from sparse-view inputs, leveraging depth maps and a
robust pose estimation architecture to synthesize novel views and augment
training data. Specifically, we enhance Gaussian Splatting by integrating a
selective photometric loss, applied only to high-confidence pixels, and
replacing standard Structure-from-Motion pipelines with the DUSt3R architecture
to improve camera pose estimation. Furthermore, we present a novel dataset
featuring both synthetic and real-world public transportation vehicles,
enabling extensive evaluation of our approach. Experimental results demonstrate
state-of-the-art performance across multiple benchmarks, showcasing the
method's ability to achieve high-quality reconstructions even under constrained
input conditions.

</details>


### [114] [DeepShade: Enable Shade Simulation by Text-conditioned Image Generation](https://arxiv.org/abs/2507.12103)
*Longchao Da,Xiangrui Liu,Mithun Shivakoti,Thirulogasankar Pranav Kutralingam,Yezhou Yang,Hua Wei*

Main category: cs.CV

TL;DR: The paper introduces DeepShade, a model to predict urban shade variations for improving heatwave-era route planning. Shade data is generated using Blender and satellite imagery.


<details>
  <summary>Details</summary>
Motivation: Current routing systems fail to incorporate critical shade data for urban planning amid intensifying heatwaves.

Method: The authors constructed a detailed dataset using 3D Blender simulations and combined it with satellite images. They developed DeepShade, a diffusion-based model leveraging RGB and edge details for enhanced shade synthesis.

Result: The DeepShade framework improves shade image generation by incorporating textual and temporal conditions, successfully calculating shade ratios for route planning in Tempe, Arizona.

Conclusion: This work enhances urban planning in extreme heat conditions and demonstrates real-world applications for mitigating thermal stress.

Abstract: Heatwaves pose a significant threat to public health, especially as global
warming intensifies. However, current routing systems (e.g., online maps) fail
to incorporate shade information due to the difficulty of estimating shades
directly from noisy satellite imagery and the limited availability of training
data for generative models. In this paper, we address these challenges through
two main contributions. First, we build an extensive dataset covering diverse
longitude-latitude regions, varying levels of building density, and different
urban layouts. Leveraging Blender-based 3D simulations alongside building
outlines, we capture building shadows under various solar zenith angles
throughout the year and at different times of day. These simulated shadows are
aligned with satellite images, providing a rich resource for learning shade
patterns. Second, we propose the DeepShade, a diffusion-based model designed to
learn and synthesize shade variations over time. It emphasizes the nuance of
edge features by jointly considering RGB with the Canny edge layer, and
incorporates contrastive learning to capture the temporal change rules of
shade. Then, by conditioning on textual descriptions of known conditions (e.g.,
time of day, solar angles), our framework provides improved performance in
generating shade images. We demonstrate the utility of our approach by using
our shade predictions to calculate shade ratios for real-world route planning
in Tempe, Arizona. We believe this work will benefit society by providing a
reference for urban planning in extreme heat weather and its potential
practical applications in the environment.

</details>


### [115] [Out-of-distribution data supervision towards biomedical semantic segmentation](https://arxiv.org/abs/2507.12105)
*Yiquan Gao,Duohui Xu*

Main category: cs.CV

TL;DR: This paper introduces Med-OoD, a novel framework leveraging Out-of-Distribution (OoD) data for biomedical segmentation, significantly improving performance without external data or architectural modifications.


<details>
  <summary>Details</summary>
Motivation: Biomedical segmentation networks struggle with foreground and background misclassification due to limited and imperfect datasets.

Method: The proposed approach, Med-OoD, integrates OoD data into segmentation networks without requiring external data, feature regularization, or additional annotations.

Result: Med-OoD reduces misclassification and achieves notable performance improvements, with 76.1% mIoU on the Lizard dataset when trained entirely using OoD data.

Conclusion: This innovative training approach demonstrates the potential of OoD data in transforming medical segmentation and invites further exploration into alternative learning paradigms.

Abstract: Biomedical segmentation networks easily suffer from the unexpected
misclassification between foreground and background objects when learning on
limited and imperfect medical datasets. Inspired by the strong power of
Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric
framework, Med-OoD to address this issue by introducing OoD data supervision
into fully-supervised biomedical segmentation with none of the following needs:
(i) external data sources, (ii) feature regularization objectives, (iii)
additional annotations. Our method can be seamlessly integrated into
segmentation networks without any modification on the architectures. Extensive
experiments show that Med-OoD largely prevents various segmentation networks
from the pixel misclassification on medical images and achieves considerable
performance improvements on Lizard dataset. We also present an emerging
learning paradigm of training a medical segmentation network completely using
OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU
as test result. We hope this learning paradigm will attract people to rethink
the roles of OoD data. Code is made available at
https://github.com/StudioYG/Med-OoD.

</details>


### [116] [Non-Adaptive Adversarial Face Generation](https://arxiv.org/abs/2507.12107)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Minsu Kim,Jae Hong Seo*

Main category: cs.CV

TL;DR: The paper presents a novel method for generating adversarial facial images to fool face recognition systems (FRSs). This method, leveraging attribute-based subspheres in FRS feature space, achieves a high success rate using a single non-adaptive query.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the risks that adversarial attacks pose to the reliability of face recognition systems, particularly in identity verification use cases.

Method: The proposed method involves exploiting the structural trait of attributed subspheres in FRS feature space, avoiding the need for iterative, transfer-based, or surrogate model approaches. It executes a single, non-adaptive query using 100 synthetic images.

Result: The method achieves over a 93% success rate against AWS's CompareFaces API at its default setting, showcasing its efficiency in adversarial attacks.

Conclusion: The approach effectively generates adversarial faces with a minimal number of queries, creating potential threats to FRSs while eliminating dependence on transferability or iterative processes.

Abstract: Adversarial attacks on face recognition systems (FRSs) pose serious security
and privacy threats, especially when these systems are used for identity
verification. In this paper, we propose a novel method for generating
adversarial faces-synthetic facial images that are visually distinct yet
recognized as a target identity by the FRS. Unlike iterative optimization-based
approaches (e.g., gradient descent or other iterative solvers), our method
leverages the structural characteristics of the FRS feature space. We figure
out that individuals sharing the same attribute (e.g., gender or race) form an
attributed subsphere. By utilizing such subspheres, our method achieves both
non-adaptiveness and a remarkably small number of queries. This eliminates the
need for relying on transferability and open-source surrogate models, which
have been a typical strategy when repeated adaptive queries to commercial FRSs
are impossible. Despite requiring only a single non-adaptive query consisting
of 100 face images, our method achieves a high success rate of over 93% against
AWS's CompareFaces API at its default threshold. Furthermore, unlike many
existing attacks that perturb a given image, our method can deliberately
produce adversarial faces that impersonate the target identity while exhibiting
high-level attributes chosen by the adversary.

</details>


### [117] [LidarPainter: One-Step Away From Any Lidar View To Novel Guidance](https://arxiv.org/abs/2507.12114)
*Yuzhou Ji,Ke Ma,Hong Cai,Anchun Zhang,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: LidarPainter improves driving scene reconstruction using a real-time diffusion model for LiDAR data.


<details>
  <summary>Details</summary>
Motivation: To address issues like inconsistency and resource inefficiency in current driving scene reconstruction methods.

Method: A diffusion model called LidarPainter that recovers driving views from sparse LiDAR data and corrupted renderings in one step.

Result: LidarPainter exceeds state-of-the-art methods in speed, quality, and resource efficiency; it is 7x faster than alternatives and uses one fifth of the GPU memory.

Conclusion: LidarPainter is a superior tool for real-time, high-fidelity driving scene reconstruction and enables stylized scene generation through text prompts.

Abstract: Dynamic driving scene reconstruction is of great importance in fields like
digital twin system and autonomous driving simulation. However, unacceptable
degradation occurs when the view deviates from the input trajectory, leading to
corrupted background and vehicle models. To improve reconstruction quality on
novel trajectory, existing methods are subject to various limitations including
inconsistency, deformation, and time consumption. This paper proposes
LidarPainter, a one-step diffusion model that recovers consistent driving views
from sparse LiDAR condition and artifact-corrupted renderings in real-time,
enabling high-fidelity lane shifts in driving scene reconstruction. Extensive
experiments show that LidarPainter outperforms state-of-the-art methods in
speed, quality and resource efficiency, specifically 7 x faster than
StreetCrafter with only one fifth of GPU memory required. LidarPainter also
supports stylized generation using text prompts such as "foggy" and "night",
allowing for a diverse expansion of the existing asset library.

</details>


### [118] [Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph](https://arxiv.org/abs/2507.12123)
*Sergey Linok,Gleb Naumov*

Main category: cs.CV

TL;DR: The paper introduces OVIGo-3DHSG, a method for indoor object grounding using a hierarchical 3D scene graph and language models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of representing and querying extensive indoor environments for effective object grounding.

Method: Uses RGB-D data and open-vocabulary models to construct a hierarchical scene graph, integrating spatial relations, combined with large language models for reasoning.

Result: Demonstrates efficient scene understanding and robust object grounding in complex multi-floor indoor scenarios.

Conclusion: OVIGo-3DHSG shows promising potential for spatial reasoning and understanding of indoor spaces, outperforming current methods.

Abstract: We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects
using 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor
environment over a Hierarchical Scene Graph derived from sequences of RGB-D
frames utilizing a set of open-vocabulary foundation models and sensor data
processing. The hierarchical representation explicitly models spatial relations
across floors, rooms, locations, and objects. To effectively address complex
queries involving spatial reference to other objects, we integrate the
hierarchical scene graph with a Large Language Model for multistep reasoning.
This integration leverages inter-layer (e.g., room-to-object) and intra-layer
(e.g., object-to-object) connections, enhancing spatial contextual
understanding. We investigate the semantic and geometry accuracy of
hierarchical representation on Habitat Matterport 3D Semantic multi-floor
scenes. Our approach demonstrates efficient scene comprehension and robust
object grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates
strong potential for applications requiring spatial reasoning and understanding
of indoor environments. Related materials can be found at
https://github.com/linukc/OVIGo-3DHSG.

</details>


### [119] [Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers](https://arxiv.org/abs/2507.12125)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin Li,Yu-Ming Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: The paper introduces BSPF-ViT, an efficient pruning strategy for Vision Transformers, achieving better accuracy and reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for making Vision Transformers efficient often degrade performance by pruning tokens independently and not considering token interactions.

Method: The approach, BSPF-ViT, prunes Q/K tokens jointly while evaluating token interactions and compresses retained tokens through similarity fusion, preserving essential information.

Result: BSPF-ViT improves ImageNet classification accuracy by 1.3% on DeiT-T and 2.0% on DeiT-S, reducing computational overhead by 50% and achieving a 40% speedup across various ViTs.

Conclusion: BSPF-ViT offers a robust solution for efficient ViTs by addressing token pruning limitations, delivering better accuracy and efficiency compared to state-of-the-art methods.

Abstract: Vision Transformer (ViT) has achieved impressive results across various
vision tasks, yet its high computational cost limits practical applications.
Recent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning
unimportant tokens. However, these techniques often sacrifice accuracy by
independently pruning query (Q) and key (K) tokens, leading to performance
degradation due to overlooked token interactions. To address this limitation,
we introduce a novel {\bf Block-based Symmetric Pruning and Fusion} for
efficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.
Unlike previous methods that consider only a single direction, our approach
evaluates each token and its neighbors to decide which tokens to retain by
taking token interaction into account. The retained tokens are compressed
through a similarity fusion step, preserving key information while reducing
computational costs. The shared weights of Q/K tokens create a symmetric
attention matrix, allowing pruning only the upper triangular part for speed up.
BSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning
levels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%
on DeiT-S, while reducing computational overhead by 50%. It achieves 40%
speedup with improved accuracy across various ViTs.

</details>


### [120] [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/abs/2507.12135)
*Junyu Lou,Xiaorui Zhao,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: The paper proposes the BPAM framework, combining bilateral grids with pixel-adaptive MLPs for efficient and effective image enhancement while overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Current bilateral grid-based methods are restricted to linear transformations, and MLP-based methods struggle with localized variations despite their non-linear potential.

Method: The proposed BPAM framework dynamically generates MLP parameters per pixel via bilateral grids, enriched by a novel grid decomposition strategy that uses multi-channel guidance maps to extract parameters from subgrids.

Result: Extensive experiments showed that the BPAM framework outperforms state-of-the-art methods in image enhancement, ensuring real-time processing capabilities.

Conclusion: BPAM successfully integrates spatial and intensity modeling with non-linear adaptability, offering a better and more efficient solution for image enhancement.

Abstract: Deep learning-based bilateral grid processing has emerged as a promising
solution for image enhancement, inherently encoding spatial and intensity
information while enabling efficient full-resolution processing through slicing
operations. However, existing approaches are limited to linear affine
transformations, hindering their ability to model complex color relationships.
Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,
traditional MLP-based methods employ globally shared parameters, which is hard
to deal with localized variations. To overcome these dual challenges, we
propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)
framework. Our approach synergizes the spatial modeling of bilateral grids with
the non-linear capabilities of MLPs. Specifically, we generate bilateral grids
containing MLP parameters, where each pixel dynamically retrieves its unique
transformation parameters and obtain a distinct MLP for color mapping based on
spatial coordinates and intensity values. In addition, we propose a novel grid
decomposition strategy that categorizes MLP parameters into distinct types
stored in separate subgrids. Multi-channel guidance maps are used to extract
category-specific parameters from corresponding subgrids, ensuring effective
utilization of color information during slicing while guiding precise parameter
generation. Extensive experiments on public datasets demonstrate that our
method outperforms state-of-the-art methods in performance while maintaining
real-time processing capabilities.

</details>


### [121] [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/abs/2507.12137)
*Jiawei Xu,Kai Deng,Zexin Fan,Shenlong Wang,Jin Xie,Jian Yang*

Main category: cs.CV

TL;DR: AD-GS introduces a self-supervised framework for rendering dynamic urban driving scenes without manual annotations, delivering high-quality results competitive with annotation-dependent methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of rendering dynamic urban driving scenes accurately without relying on expensive manual object annotations.

Method: AD-GS uses a novel self-supervised motion model combining locality-aware B-spline curves and global-aware trigonometric functions. It segments scenes automatically and employs dynamic Gaussians alongside bidirectional temporal visibility masks.

Result: The framework significantly surpassed state-of-the-art annotation-free rendering methods and matched the performance of annotation-dependent techniques.

Conclusion: AD-GS proves that high-quality rendering of driving scenes can be achieved through self-supervised methods, eliminating the need for manual annotations.

Abstract: Modeling and rendering dynamic urban driving scenes is crucial for
self-driving simulation. Current high-quality methods typically rely on costly
manual object tracklet annotations, while self-supervised approaches fail to
capture dynamic object motions accurately and decompose scenes properly,
resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised
framework for high-quality free-viewpoint rendering of driving scenes from a
single log. At its core is a novel learnable motion model that integrates
locality-aware B-spline curves with global-aware trigonometric functions,
enabling flexible yet precise dynamic object modeling. Rather than requiring
comprehensive semantic labeling, AD-GS automatically segments scenes into
objects and background with the simplified pseudo 2D segmentation, representing
objects using dynamic Gaussians and bidirectional temporal visibility masks.
Further, our model incorporates visibility reasoning and physically rigid
regularization to enhance robustness. Extensive evaluations demonstrate that
our annotation-free model significantly outperforms current state-of-the-art
annotation-free methods and is competitive with annotation-dependent
approaches.

</details>


### [122] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: The paper presents a data-driven approach to model a neural prior over human body poses using normalizing flows and the RealNVP method.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve human body pose modeling by creating a more flexible, principled probabilistic model compared to heuristic or less expressive methods.

Method: The method involves using normalizing flows with the RealNVP framework to model poses in 6D rotation format, and addressing manifold challenges via an inverted Gram-Schmidt process during training.

Result: The results demonstrate the effectiveness of the learned prior through qualitative, quantitative evaluations, and ablation studies to analyze its impact on human motion capture and reconstruction.

Conclusion: The study establishes a robust probabilistic foundation for integrating pose priors into various human motion capture and reconstruction systems.

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [123] [Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation](https://arxiv.org/abs/2507.12157)
*Edwin Arkel Rios,Fernando Mikael,Oswin Gosal,Femiloye Oyerinde,Hao-Chun Liang,Bo-Cheng Lai,Min-Chun Hu*

Main category: cs.CV

TL;DR: This paper proposes TGDA, a training framework that enables training fine-grained image recognition (FGIR) systems from scratch, eliminating dependence on pretrained models while ensuring high performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of pretrained models in FGIR, such as reduced adaptability in resource-constrained settings, and to enable the development of task-specific architectures.

Method: The TGDA framework combines data-aware augmentation and weak supervision through a fine-grained-aware teacher model using knowledge distillation. This approach facilitates training specialized architectures like low-resolution LRNets and efficient ViTFS Vision Transformers.

Result: TGDA achieved state-of-the-art or better accuracy across FGIR benchmarks, significantly improving efficiency. In low-resolution settings, LRNets improved accuracy by up to 23% while using far fewer parameters and computational resources. ViTFS-T matched pretrained ViT B-16 performance with drastically fewer resources.

Conclusion: TGDA demonstrates the feasibility of training FGIR systems from scratch while surpassing pretrained counterparts in efficiency and accuracy, paving the way for adaptable, resource-efficient FGIR architectures.

Abstract: Fine-grained image recognition (FGIR) aims to distinguish visually similar
sub-categories within a broader class, such as identifying bird species. While
most existing FGIR methods rely on backbones pretrained on large-scale datasets
like ImageNet, this dependence limits adaptability to resource-constrained
environments and hinders the development of task-specific architectures
tailored to the unique challenges of FGIR.
  In this work, we challenge the conventional reliance on pretrained models by
demonstrating that high-performance FGIR systems can be trained entirely from
scratch. We introduce a novel training framework, TGDA, that integrates
data-aware augmentation with weak supervision via a fine-grained-aware teacher
model, implemented through knowledge distillation. This framework unlocks the
design of task-specific and hardware-aware architectures, including LRNets for
low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for
efficient inference.
  Extensive experiments across three FGIR benchmarks over diverse settings
involving low-resolution and high-resolution inputs show that our method
consistently matches or surpasses state-of-the-art pretrained counterparts. In
particular, in the low-resolution setting, LRNets trained with TGDA improve
accuracy by up to 23\% over prior methods while requiring up to 20.6x less
parameters, lower FLOPs, and significantly less training data. Similarly,
ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k
while using 15.3x fewer trainable parameters and requiring orders of magnitudes
less data. These results highlight TGDA's potential as an adaptable alternative
to pretraining, paving the way for more efficient fine-grained vision systems.

</details>


### [124] [Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification](https://arxiv.org/abs/2507.12177)
*Zahid Ullah,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: This study proposes a novel double ensembling framework combining pre-trained deep learning models and fine-tuned machine learning models to enhance brain tumor classification in MRI, overcoming limitations in manual diagnosis.


<details>
  <summary>Details</summary>
Motivation: MRI is effective for detecting tumors, but manual diagnosis is prone to errors caused by human limitations like fatigue and expertise gaps. This study aims to address these challenges by leveraging advanced computational frameworks.

Method: The method incorporates preprocessing, data augmentation, transfer learning from deep convolutional neural networks and vision transformers, and hyperparameter fine-tuning of machine learning classifiers. Double ensembling is applied for feature extraction and classification, tested on three brain tumor datasets.

Result: The proposed approach outperforms state-of-the-art methods in brain tumor classification. Ensemble techniques combined with hyperparameter fine-tuning significantly enhance performance, and an ablation study confirms the contribution of each component.

Conclusion: The double ensembling framework demonstrates improved diagnostic precision in brain tumor classification from MRI images. The combination of advanced deep learning, machine learning, and fine-tuning techniques offers a robust solution to limitations in manual evaluation.

Abstract: Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable
tool for detecting tumors due to its capability to produce detailed images that
reveal their presence. However, the accuracy of diagnosis can be compromised
when human specialists evaluate these images. Factors such as fatigue, limited
expertise, and insufficient image detail can lead to errors. For example, small
tumors might go unnoticed, or overlap with healthy brain regions could result
in misidentification. To address these challenges and enhance diagnostic
precision, this study proposes a novel double ensembling framework, consisting
of ensembled pre-trained deep learning (DL) models for feature extraction and
ensembled fine-tuned hyperparameter machine learning (ML) models to efficiently
classify brain tumors. Specifically, our method includes extensive
preprocessing and augmentation, transfer learning concepts by utilizing various
pre-trained deep convolutional neural networks and vision transformer networks
to extract deep features from brain MRI, and fine-tune hyperparameters of ML
classifiers. Our experiments utilized three different publicly available Kaggle
MRI brain tumor datasets to evaluate the pre-trained DL feature extractor
models, ML classifiers, and the effectiveness of an ensemble of deep features
along with an ensemble of ML classifiers for brain tumor classification. Our
results indicate that the proposed feature fusion and classifier fusion improve
upon the state of the art, with hyperparameter fine-tuning providing a
significant enhancement over the ensemble method. Additionally, we present an
ablation study to illustrate how each component contributes to accurate brain
tumor classification.

</details>


### [125] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Main category: cs.CV

TL;DR: The paper introduces a wavelet-based low-light stereo image enhancement method that decouples feature space to separately handle illumination and texture enhancement, using innovative modules for cross-view interaction and texture improvement.


<details>
  <summary>Details</summary>
Motivation: Existing methods for low-light image enhancement often bundle all degradation factors into a single latent space, resulting in entangled features and shortcut learning. This paper aims to address this limitation.

Method: The authors employed a multi-level wavelet decomposition technique to separate low and high-frequency components. Illumination adjustments are applied to the low-frequency component, while high-frequency branches handle texture enhancement. A novel high-frequency guided cross-view interaction module (HF-CIM) and detail and texture enhancement module (DTEM) utilizing cross-attention are introduced.

Result: Experiments on both real and synthetic datasets demonstrate substantial improvements in light adjustment and high-frequency detail recovery.

Conclusion: The proposed method effectively improves low-light image enhancement by decoupling feature spaces and leveraging stereo view cues, yielding noticeable advantages in both illumination and texture restoration.

Abstract: Low-light images suffer from complex degradation, and existing enhancement
methods often encode all degradation factors within a single latent space. This
leads to highly entangled features and strong black-box characteristics, making
the model prone to shortcut learning. To mitigate the above issues, this paper
proposes a wavelet-based low-light stereo image enhancement method with feature
space decoupling. Our insight comes from the following findings: (1) Wavelet
transform enables the independent processing of low-frequency and
high-frequency information. (2) Illumination adjustment can be achieved by
adjusting the low-frequency component of a low-light image, extracted through
multi-level wavelet decomposition. Thus, by using wavelet transform the feature
space is decomposed into a low-frequency branch for illumination adjustment and
multiple high-frequency branches for texture enhancement. Additionally, stereo
low-light image enhancement can extract useful cues from another view to
improve enhancement. To this end, we propose a novel high-frequency guided
cross-view interaction module (HF-CIM) that operates within high-frequency
branches rather than across the entire feature space, effectively extracting
valuable image details from the other view. Furthermore, to enhance the
high-frequency information, a detail and texture enhancement module (DTEM) is
proposed based on cross-attention mechanism. The model is trained on a dataset
consisting of images with uniform illumination and images with non-uniform
illumination. Experimental results on both real and synthetic images indicate
that our algorithm offers significant advantages in light adjustment while
effectively recovering high-frequency information. The code and dataset are
publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [126] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Main category: cs.CV

TL;DR: The paper proposes advanced techniques to preserve and restore Indian monuments using computer vision and machine learning.


<details>
  <summary>Details</summary>
Motivation: To improve cultural heritage protection with automated methods that merge tradition and innovation.

Method: Three approaches: Fractal Convolution (for segmentation), Self-Sensitive Tile Filling (specialized for terracotta temples), and Super Resolution strategy (image upscaling).

Result: Enhanced region-filling, detailed tiling, and image quality preservation at affordable costs through automation.

Conclusion: The study ensures efficient and aesthetically superior solutions for cultural heritage protection while maintaining the balance between tradition and innovation.

Abstract: Modern digitised approaches have dramatically changed the preservation and
restoration of cultural treasures, integrating computer scientists into
multidisciplinary projects with ease. Machine learning, deep learning, and
computer vision techniques have revolutionised developing sectors like 3D
reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and
image processing with the integration of computer scientists into
multidisciplinary initiatives. We suggest three cutting-edge techniques in
recognition of the special qualities of Indian monuments, which are famous for
their architectural skill and aesthetic appeal. First is the Fractal
Convolution methodology, a segmentation method based on image processing that
successfully reveals subtle architectural patterns within these irreplaceable
cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling
(SSTF) method created especially for West Bengal's mesmerising Bankura
Terracotta Temples with a brand-new data augmentation method called MosaicSlice
on the third. Furthermore, we delve deeper into the Super Resolution strategy
to upscale the images without losing significant amount of quality. Our methods
allow for the development of seamless region-filling and highly detailed tiles
while maintaining authenticity using a novel data augmentation strategy within
affordable costs introducing automation. By providing effective solutions that
preserve the delicate balance between tradition and innovation, this study
improves the subject and eventually ensures unrivalled efficiency and aesthetic
excellence in cultural heritage protection. The suggested approaches advance
the field into an era of unmatched efficiency and aesthetic quality while
carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [127] [RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models](https://arxiv.org/abs/2507.12201)
*Yiqi Tian,Pengfei Jin,Mingze Yuan,Na Li,Bo Zeng,Quanzheng Li*

Main category: cs.CV

TL;DR: Diffusion models' sampling issues (like hallucinations) are addressed by RODS, a method that reduces errors using optimization techniques and geometric cues.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve the problem of hallucinations in diffusion models caused by score approximation inaccuracies during sampling.

Method: The authors reinterpret diffusion sampling through optimization and propose RODS, which utilizes geometric loss landscape cues to detect and correct high-risk sampling steps, enforcing smoother sampling trajectories and adaptive perturbations.

Result: Experiments show that RODS improves diffusion model performance and robustness on datasets like AFHQv2, FFHQ, and 11k-hands, detecting over 70% of hallucinations and correcting 25% of them, without creating new issues.

Conclusion: RODS successfully enhances both fidelity and robustness of diffusion model sampling processes, achieving its goals with minimal inference cost and no retraining requirements.

Abstract: Diffusion models have achieved state-of-the-art performance in generative
modeling, yet their sampling procedures remain vulnerable to hallucinations,
often stemming from inaccuracies in score approximation. In this work, we
reinterpret diffusion sampling through the lens of optimization and introduce
RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that
detects and corrects high-risk sampling steps using geometric cues from the
loss landscape. RODS enforces smoother sampling trajectories and adaptively
adjusts perturbations, reducing hallucinations without retraining and at
minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands
demonstrate that RODS improves both sampling fidelity and robustness, detecting
over 70% of hallucinated samples and correcting more than 25%, all while
avoiding the introduction of new artifacts.

</details>


### [128] [MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM](https://arxiv.org/abs/2507.12232)
*Tao Chen,Jingyi Zhang,Decheng Liu,Chunlei Peng*

Main category: cs.CV

TL;DR: The paper introduces DD-VQA+, an extended dataset, and MGFFD-VLM, a novel framework for better detecting and explaining forgeries in faces using visual language models, surpassing prior methods in performance and accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing forgery detection methods using VLMs, particularly their underutilization of face quality-related attributes and lack of effective training strategies.

Method: The authors extend the VQA dataset to DD-VQA+ with richer attributes and diverse samples, and propose the MGFFD-VLM framework. This incorporates an Attribute-Driven Hybrid LoRA Strategy, Multi-Granularity Prompt Learning, Forgery-Aware Training Strategy, and additional forgery-related auxiliary losses.

Result: Experimental results indicate the proposed method improves both forgery classification accuracy and interpretability, achieving better results than existing techniques.

Conclusion: The study advances forgery detection by enriching datasets and introducing innovative training strategies, thereby improving performance and interpretable analysis of forged faces.

Abstract: Recent studies have utilized visual large language models (VLMs) to answer
not only "Is this face a forgery?" but also "Why is the face a forgery?" These
studies introduced forgery-related attributes, such as forgery location and
type, to construct deepfake VQA datasets and train VLMs, achieving high
accuracy while providing human-understandable explanatory text descriptions.
However, these methods still have limitations. For example, they do not fully
leverage face quality-related attributes, which are often abnormal in forged
faces, and they lack effective training strategies for forgery-aware VLMs. In
this paper, we extend the VQA dataset to create DD-VQA+, which features a
richer set of attributes and a more diverse range of samples. Furthermore, we
introduce a novel forgery detection framework, MGFFD-VLM, which integrates an
Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual
Large Language Models (VLMs). Additionally, our framework incorporates
Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By
transforming classification and forgery segmentation results into prompts, our
method not only improves forgery classification but also enhances
interpretability. To further boost detection performance, we design multiple
forgery-related auxiliary losses. Experimental results demonstrate that our
approach surpasses existing methods in both text-based forgery judgment and
analysis, achieving superior accuracy.

</details>


### [129] [Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models](https://arxiv.org/abs/2507.12236)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: This paper explores using generative text-to-image diffusion models for zero-shot phrase grounding in medical imaging, outperforming conventional discriminative methods.


<details>
  <summary>Details</summary>
Motivation: To improve disease localization in medical imaging through natural language phrase grounding by leveraging generative models.

Method: The authors fine-tune generative text-to-image diffusion models using a domain-specific language model, CXR-BERT, and introduce a post-processing technique called Bimodal Bias Merging (BBM) for refining cross-attention maps.

Result: The generative model setup doubled mIoU scores compared to current discriminative methods, and BBM provided enhanced localization accuracy.

Conclusion: Generative models offer a superior paradigm for phrase grounding in medical imaging, demonstrating robustness and potential for clinical applications.

Abstract: Phrase grounding, i.e., mapping natural language phrases to specific image
regions, holds significant potential for disease localization in medical
imaging through clinical reports. While current state-of-the-art methods rely
on discriminative, self-supervised contrastive models, we demonstrate that
generative text-to-image diffusion models, leveraging cross-attention maps, can
achieve superior zero-shot phrase grounding performance. Contrary to prior
assumptions, we show that fine-tuning diffusion models with a frozen,
domain-specific language model, such as CXR-BERT, substantially outperforms
domain-agnostic counterparts. This setup achieves remarkable improvements, with
mIoU scores doubling those of current discriminative methods. These findings
highlight the underexplored potential of generative models for phrase grounding
tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),
a novel post-processing technique that aligns text and image biases to identify
regions of high certainty. BBM refines cross-attention maps, achieving even
greater localization accuracy. Our results establish generative approaches as a
more effective paradigm for phrase grounding in the medical imaging domain,
paving the way for more robust and interpretable applications in clinical
practice. The source code and model weights are available at
https://github.com/Felix-012/generate_to_ground.

</details>


### [130] [Calisthenics Skills Temporal Video Segmentation](https://arxiv.org/abs/2507.12245)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: This paper introduces a dataset for temporal segmentation of static calisthenics skills from videos and demonstrates a baseline solution, highlighting feasibility and areas for improvement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of tools for automated temporal video segmentation of static calisthenics skills to assist athletes and judges.

Method: The authors propose a dataset of annotated videos showing static calisthenics skills along with a baseline method for temporal segmentation of these skills.

Result: The baseline method shows that automated segmentation of static skills from videos is feasible but requires further refinement and improvement.

Conclusion: This work establishes a foundational step towards the creation of automated tools for evaluating static calisthenics skills, emphasizing the utility and areas for development.

Abstract: Calisthenics is a fast-growing bodyweight discipline that consists of
different categories, one of which is focused on skills. Skills in calisthenics
encompass both static and dynamic elements performed by athletes. The
evaluation of static skills is based on their difficulty level and the duration
of the hold. Automated tools able to recognize isometric skills from a video by
segmenting them to estimate their duration would be desirable to assist
athletes in their training and judges during competitions. Although the video
understanding literature on action recognition through body pose analysis is
rich, no previous work has specifically addressed the problem of calisthenics
skill temporal video segmentation. This study aims to provide an initial step
towards the implementation of automated tools within the field of Calisthenics.
To advance knowledge in this context, we propose a dataset of video footage of
static calisthenics skills performed by athletes. Each video is annotated with
a temporal segmentation which determines the extent of each skill. We hence
report the results of a baseline approach to address the problem of skill
temporal segmentation on the proposed dataset. The results highlight the
feasibility of the proposed problem, while there is still room for improvement.

</details>


### [131] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida Nezović,Jalal Romano,Nada Marić,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: This paper evaluates the comparative performance of CNN implementations in Keras, PyTorch, and JAX for medical image classification using the PathMNIST dataset.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comparative analysis among deep learning frameworks (Keras, PyTorch, and JAX) specifically for medical imaging tasks.

Method: Comprehensive assessment of CNN models across frameworks using training efficiency, classification accuracy, and inference speed as benchmarks, with the PathMNIST dataset as the test case.

Result: The study identifies trade-offs between computational speed and model accuracy among different frameworks.

Conclusion: Results provide useful insights for selecting the optimal framework in real-world medical image analysis applications.

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [132] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno Bücker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Main category: cs.CV

TL;DR: The paper develops a deep learning method using day-1 chest X-rays of extremely preterm infants for predicting bronchopulmonary dysplasia (BPD) outcome, showing improved accuracy with domain-specific pretraining and data augmentations.


<details>
  <summary>Details</summary>
Motivation: Many preterm infants suffer from bronchopulmonary dysplasia (BPD), a chronic lung disease linked to oxygen dependence and lasting complications. Reliable early prediction of BPD outcomes can help reduce risks associated with interventions.

Method: The study trained a fine-tuned ResNet-50 model on domain-specific pretraining for adult chest radiographs. Techniques like progressive layer freezing, discriminative learning rates, CutMix augmentation, and linear probing were applied to prevent overfitting and enhance predictions.

Result: The model achieved an AUROC of 0.78, balanced accuracy of 0.69, and F1-score of 0.67 for moderate/severe BPD prediction. Domain-specific pretraining outperformed ImageNet initialization, highlighting its relevance to improving outcomes.

Conclusion: The findings emphasize the importance of domain-specific pretraining and advanced techniques in enabling accurate BPD predictions using routine radiographs, while also demonstrating feasibility for real-world and federated implementations.

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [133] [FADE: Adversarial Concept Erasure in Flow Models](https://arxiv.org/abs/2507.12283)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wang,Ze Niu,Dacheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: The paper introduces the FADE method to safely erase specified concepts (e.g., stereotypes or private identities) from text-to-image diffusion models while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: To address privacy and fairness concerns in text-to-image diffusion models by removing sensitive or harmful concepts from their outputs.

Method: FADE uses a trajectory-aware fine-tuning approach combined with an adversarial objective to erase concepts while preserving image fidelity. It minimizes mutual information between the erased concept and model outputs.

Result: FADE outperforms existing methods in concept removal and image quality by 5-10% on benchmarks like Stable Diffusion and FLUX.

Conclusion: FADE offers a reliable solution for erasing harmful or sensitive concepts in diffusion models, paving the way for safer and more fair generative modeling.

Abstract: Diffusion models have demonstrated remarkable image generation capabilities,
but also pose risks in privacy and fairness by memorizing sensitive concepts or
perpetuating biases. We propose a novel \textbf{concept erasure} method for
text-to-image diffusion models, designed to remove specified concepts (e.g., a
private individual or a harmful stereotype) from the model's generative
repertoire. Our method, termed \textbf{FADE} (Fair Adversarial Diffusion
Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial
objective to ensure the concept is reliably removed while preserving overall
model fidelity. Theoretically, we prove a formal guarantee that our approach
minimizes the mutual information between the erased concept and the model's
outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable
Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,
explicit content, and style erasure tasks from MACE). FADE achieves
state-of-the-art concept removal performance, surpassing recent baselines like
ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.
Notably, FADE improves the harmonic mean of concept removal and fidelity by
5--10\% over the best prior method. We also conduct an ablation study to
validate each component of FADE, confirming that our adversarial and
trajectory-preserving objectives each contribute to its superior performance.
Our work sets a new standard for safe and fair generative modeling by
unlearning specified concepts without retraining from scratch.

</details>


### [134] [Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation](https://arxiv.org/abs/2507.12292)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: The paper introduces a method for calisthenics skill classification that avoids pose estimation costs by using depth estimation and athlete localization.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of traditional methods in real-time applications and mobile devices by addressing computational inefficiencies in pose estimation.

Method: The researchers employ Depth Anything V2 for depth estimation and YOLOv10 for athlete localization, segmenting athletes from backgrounds without pose estimation.

Result: Their approach achieves faster inference, being 38.3x quicker and showing improved classification accuracy compared to skeleton-based methods.

Conclusion: The proposed method enhances efficiency and accuracy, offering modular flexibility for future upgrades and broad applicability.

Abstract: Calisthenics skill classification is the computer vision task of inferring
the skill performed by an athlete from images, enabling automatic performance
assessment and personalized analytics. Traditional methods for calisthenics
skill recognition are based on pose estimation methods to determine the
position of skeletal data from images, which is later fed to a classification
algorithm to infer the performed skill. Despite the progress in human pose
estimation algorithms, they still involve high computational costs, long
inference times, and complex setups, which limit the applicability of such
approaches in real-time applications or mobile devices. This work proposes a
direct approach to calisthenics skill recognition, which leverages depth
estimation and athlete patch retrieval to avoid the computationally expensive
human pose estimation module. Using Depth Anything V2 for depth estimation and
YOLOv10 for athlete localization, we segment the subject from the background
rather than relying on traditional pose estimation techniques. This strategy
increases efficiency, reduces inference time, and improves classification
accuracy. Our approach significantly outperforms skeleton-based methods,
achieving 38.3x faster inference with RGB image patches and improved
classification accuracy with depth patches (0.837 vs. 0.815). Beyond these
performance gains, the modular design of our pipeline allows for flexible
replacement of components, enabling future enhancements and adaptation to
real-world applications.

</details>


### [135] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: This paper introduces Discrete Latent Code (DLC), a new representation for conditioning diffusion models to achieve state-of-the-art image generation fidelity, improve compositionality, and enable out-of-distribution sample creation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in diffusion models' representations for improving sample fidelity, ease of generation, and compositionality to allow generation of out-of-training samples.

Method: The authors propose DLCs, discrete tokens derived from Simplicial Embeddings trained with a self-supervised learning objective, for conditioning diffusion models.

Result: Experiments demonstrate that diffusion models trained with DLCs outperform state-of-the-art results in unconditional image generation on ImageNet and enable compositional image generation beyond the training distribution.

Conclusion: DLCs improve generation fidelity, enable out-of-distribution sample creation, and can leverage text diffusion language models for innovative text-to-image generation.

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [136] [Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors](https://arxiv.org/abs/2507.12336)
*Subin Jeon,In Cho,Junyoung Hong,Seon Joo Kim*

Main category: cs.CV

TL;DR: The paper presents KeyDiff3D, a framework for unsupervised monocular 3D keypoints estimation from single images using geometric priors in a pretrained multi-view diffusion model.


<details>
  <summary>Details</summary>
Motivation: 3D keypoints estimation often requires expensive manual annotations or calibrated multi-view images; this paper seeks to achieve similar outcomes using only single-view images.

Method: KeyDiff3D leverages a multi-view diffusion model to generate supervision signals, extract 2D multi-view features, and create explicit 3D feature volumes.

Result: KeyDiff3D shows high accuracy and generalization in 3D keypoints estimation across datasets like Human3.6M and Stanford Dogs and enables single-image-based 3D object manipulation.

Conclusion: The framework innovatively transforms implicit 3D priors into explicit features, achieving accurate monocular 3D keypoints estimation without reliance on expensive annotated datasets.

Abstract: This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D
keypoints estimation that accurately predicts 3D keypoints from a single image.
While previous methods rely on manual annotations or calibrated multi-view
images, both of which are expensive to collect, our method enables monocular 3D
keypoints estimation using only a collection of single-view images. To achieve
this, we leverage powerful geometric priors embedded in a pretrained multi-view
diffusion model. In our framework, this model generates multi-view images from
a single image, serving as a supervision signal to provide 3D geometric cues to
our model. We also use the diffusion model as a powerful 2D multi-view feature
extractor and construct 3D feature volumes from its intermediate
representations. This transforms implicit 3D priors learned by the diffusion
model into explicit 3D features. Beyond accurate keypoints estimation, we
further introduce a pipeline that enables manipulation of 3D objects generated
by the diffusion model. Experimental results on diverse aspects and datasets,
including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain
datasets, highlight the effectiveness of our method in terms of accuracy,
generalization, and its ability to enable manipulation of 3D objects generated
by the diffusion model from a single image.

</details>


### [137] [Improving Lightweight Weed Detection via Knowledge Distillation](https://arxiv.org/abs/2507.12344)
*Ahmet Oğuz Saltık,Max Voigt,Sourav Modak,Mike Beckworth,Anthony Stein*

Main category: cs.CV

TL;DR: The paper investigates methods to improve lightweight object detection models' precision for weed identification in real-time agricultural systems using knowledge distillation techniques.


<details>
  <summary>Details</summary>
Motivation: The aim is to overcome challenges in deploying accurate weed detection models on resource-limited platforms while distinguishing visually similar weed species.

Method: The study uses YOLO11x as a teacher model and YOLO11n as a student and reference model, employing Channel-wise Knowledge Distillation (CWD) and Masked Generative Distillation (MGD) for knowledge transfer.

Result: Their experiments show significant improvement in detection accuracy, with 2.5% mAP50 improvement using CWD and 1.9% using MGD, alongside successful real-time deployment tests on Jetson Orin Nano and Raspberry Pi 5.

Conclusion: CWD and MGD are effective methods to enhance weed detection models for precision agriculture without increasing complexity, offering practical real-world implementation potential.

Abstract: Weed detection is a critical component of precision agriculture, facilitating
targeted herbicide application and reducing environmental impact. However,
deploying accurate object detection models on resource-limited platforms
remains challenging, particularly when differentiating visually similar weed
species commonly encountered in plant phenotyping applications. In this work,
we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative
Distillation (MGD) to enhance the performance of lightweight models for
real-time smart spraying systems. Utilizing YOLO11x as the teacher model and
YOLO11n as both reference and student, both CWD and MGD effectively transfer
knowledge from the teacher to the student model. Our experiments, conducted on
a real-world dataset comprising sugar beet crops and four weed types (Cirsium,
Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50
across all classes. The distilled CWD student model achieves a notable
improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without
increasing model complexity. Additionally, we validate real-time deployment
feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and
Raspberry Pi 5 embedded devices, performing five independent runs to evaluate
performance stability across random seeds. These findings confirm CWD and MGD
as an effective, efficient, and practical approach for improving deep
learning-based weed detection accuracy in precision agriculture and plant
phenotyping scenarios.

</details>


### [138] [Cluster Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2507.12359)
*Nikolaos Giakoumoglou,Tania Stathaki*

Main category: cs.CV

TL;DR: This paper introduces "Cluster Contrast (CueCo)," an approach that combines contrastive learning and clustering for unsupervised visual representation learning, achieving high classification accuracy on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance unsupervised visual representation learning by leveraging the strengths of both contrastive learning and clustering.

Method: The paper proposes CueCo, which uses two neural networks (query and key) updated via a slow-moving average, combines contrastive loss for inter-class separation, and clustering objectives for intra-class compactness.

Result: CueCo achieves classification accuracies of 91.40% on CIFAR-10, 68.56% on CIFAR-100, and 78.65% on ImageNet-100 using a ResNet-18 backbone.

Conclusion: CueCo successfully balances the scattering and alignment of feature representations, establishing a promising new direction for unsupervised representation learning.

Abstract: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised
visual representation learning that effectively combines the strengths of
contrastive learning and clustering methods. Inspired by recent advancements,
CueCo is designed to simultaneously scatter and align feature representations
within the feature space. This method utilizes two neural networks, a query and
a key, where the key network is updated through a slow-moving average of the
query outputs. CueCo employs a contrastive loss to push dissimilar features
apart, enhancing inter-class separation, and a clustering objective to pull
together features of the same cluster, promoting intra-class compactness. Our
method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on
CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18
backbone. By integrating contrastive learning with clustering, CueCo sets a new
direction for advancing unsupervised visual representation learning.

</details>


### [139] [Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.12382)
*Kaiwen Huang,Yi Zhou,Huazhu Fu,Yizhe Zhang,Chen Gong,Tao Zhou*

Main category: cs.CV

TL;DR: The paper presents a semi-supervised image segmentation framework (Text-SemiSeg) that leverages textual information to improve 3D medical imaging tasks.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of labeled data in medical imaging and leverage textual data for enhanced visual semantic understanding.

Method: The approach consists of three modules: (1) Text-enhanced Multiplanar Representation (TMR) for text-visual interaction, (2) Category-aware Semantic Alignment (CSA) for cross-modal alignment, and (3) Dynamic Cognitive Augmentation (DCA) for reducing labeled-unlabeled data discrepancy.

Result: Experiments on three public datasets show that Text-SemiSeg outperforms baseline methods and enhances visual features using textual information.

Conclusion: The proposed framework effectively utilizes textual data to enhance 3D medical image segmentation, demonstrating robustness and superior performance against other methods.

Abstract: Semi-supervised medical image segmentation is a crucial technique for
alleviating the high cost of data annotation. When labeled data is limited,
textual information can provide additional context to enhance visual semantic
understanding. However, research exploring the use of textual data to enhance
visual semantic embeddings in 3D medical imaging tasks remains scarce. In this
paper, we propose a novel text-driven multiplanar visual interaction framework
for semi-supervised medical image segmentation (termed Text-SemiSeg), which
consists of three main modules: Text-enhanced Multiplanar Representation (TMR),
Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation
(DCA). Specifically, TMR facilitates text-visual interaction through planar
mapping, thereby enhancing the category awareness of visual features. CSA
performs cross-modal semantic alignment between the text features with
introduced learnable variables and the intermediate layer of visual features.
DCA reduces the distribution discrepancy between labeled and unlabeled data
through their interaction, thus improving the model's robustness. Finally,
experiments on three public datasets demonstrate that our model effectively
enhances visual features with textual information and outperforms other
methods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.

</details>


### [140] [OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments](https://arxiv.org/abs/2507.12396)
*Hayat Ullah,Abbas Khan,Arslan Munir,Hari Kalva*

Main category: cs.CV

TL;DR: The paper introduces two new surveillance image benchmarks, OD-VIRAT Large and OD-VIRAT Tiny, containing annotated instances for challenging tasks in human-object detection, and evaluations of modern object detection models on these datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to create robust and reliable surveillance systems that can operate effectively in real-world scenarios by developing benchmarks for evaluating object detection models on challenging surveillance imagery.

Method: The authors developed two benchmarks, OD-VIRAT Large and OD-VIRAT Tiny, covering 10 surveillance scenes with millions of annotated instances. They also benchmarked modern object detection architectures such as YOLOX, RetinaNet, and others on these collections.

Result: The benchmarks provide rich annotations and were used to assess the performance of state-of-the-art object detection models under challenging real-world conditions like occlusions, complex backgrounds, and small-scale objects.

Conclusion: The two datasets and benchmarking results are expected to provide insights for improving and developing more efficient object detection algorithms suitable for surveillance applications.

Abstract: Realistic human surveillance datasets are crucial for training and evaluating
computer vision models under real-world conditions, facilitating the
development of robust algorithms for human and human-interacting object
detection in complex environments. These datasets need to offer diverse and
challenging data to enable a comprehensive assessment of model performance and
the creation of more reliable surveillance systems for public safety. To this
end, we present two visual object detection benchmarks named OD-VIRAT Large and
OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance
imagery. The video sequences in both benchmarks cover 10 different scenes of
human surveillance recorded from significant height and distance. The proposed
benchmarks offer rich annotations of bounding boxes and categories, where
OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and
OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also
focuses on benchmarking state-of-the-art object detection architectures,
including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object
detection-specific variant of VIRAT dataset. To the best of our knowledge, it
is the first work to examine the performance of these recently published
state-of-the-art object detection architectures on realistic surveillance
imagery under challenging conditions such as complex backgrounds, occluded
objects, and small-scale objects. The proposed benchmarking and experimental
settings will help in providing insights concerning the performance of selected
object detection models and set the base for developing more efficient and
robust object detection architectures.

</details>


### [141] [QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval](https://arxiv.org/abs/2507.12416)
*Jaehyun Kwak,Ramahdani Muhammad Izaaz Inhar,Se-Young Yun,Sung-Ju Lee*

Main category: cs.CV

TL;DR: The paper addresses false negatives in Composed Image Retrieval (CIR) by introducing Query-Relevant Retrieval through Hard Negative Sampling (QuRe), achieving state-of-the-art performance and better alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods overlook the relevance of non-target images, leading to reduced user satisfaction due to false negatives caused by current contrastive learning approaches.

Method: The authors propose QuRe, which optimizes a reward model to reduce false negatives and employs a novel hard negative sampling strategy to filter irrelevant images effectively. They also introduce a new dataset, HP-FashionIQ, to evaluate models based on human satisfaction.

Result: QuRe significantly outperforms existing methods on FashionIQ and CIRR datasets and aligns better with human preferences on the newly created HP-FashionIQ dataset.

Conclusion: QuRe enhances CIR by addressing false negatives, improving user-relevant retrieval, and achieving state-of-the-art results with a method that aligns closely with human satisfaction.

Abstract: Composed Image Retrieval (CIR) retrieves relevant images based on a reference
image and accompanying text describing desired modifications. However, existing
CIR methods only focus on retrieving the target image and disregard the
relevance of other images. This limitation arises because most methods
employing contrastive learning-which treats the target image as positive and
all other images in the batch as negatives-can inadvertently include false
negatives. This may result in retrieving irrelevant images, reducing user
satisfaction even when the target image is retrieved. To address this issue, we
propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which
optimizes a reward model objective to reduce false negatives. Additionally, we
introduce a hard negative sampling strategy that selects images positioned
between two steep drops in relevance scores following the target image, to
effectively filter false negatives. In order to evaluate CIR models on their
alignment with human satisfaction, we create Human-Preference FashionIQ
(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond
target retrieval. Extensive experiments demonstrate that QuRe achieves
state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting
the strongest alignment with human preferences on the HP-FashionIQ dataset. The
source code is available at https://github.com/jackwaky/QuRe.

</details>


### [142] [InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization](https://arxiv.org/abs/2507.12420)
*Haoyuan Liu,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: This paper introduces InterpIoU, a new loss function for bounding box regression in object detection, addressing deficiencies in IoU-based methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current IoU-based losses, such as sensitivity to box characteristics and suboptimal performance on small object detection.

Method: InterpIoU uses interpolated boxes to provide meaningful gradients in non-overlapping cases, and Dynamic InterpIoU adjusts interpolation based on IoU values to adapt to diverse object scenarios.

Result: The proposed methods outperform state-of-the-art IoU-based losses on COCO, VisDrone, and PASCAL VOC datasets, particularly excelling in small object detection.

Conclusion: InterpIoU and its dynamic variant resolve existing limitations in IoU-based losses, offering superior bounding box regression capabilities.

Abstract: Bounding box regression (BBR) is fundamental to object detection, where the
regression loss is crucial for accurate localization. Existing IoU-based losses
often incorporate handcrafted geometric penalties to address IoU's
non-differentiability in non-overlapping cases and enhance BBR performance.
However, these penalties are sensitive to box shape, size, and distribution,
often leading to suboptimal optimization for small objects and undesired
behaviors such as bounding box enlargement due to misalignment with the IoU
objective. To address these limitations, we propose InterpIoU, a novel loss
function that replaces handcrafted geometric penalties with a term based on the
IoU between interpolated boxes and the target. By using interpolated boxes to
bridge the gap between predictions and ground truth, InterpIoU provides
meaningful gradients in non-overlapping cases and inherently avoids the box
enlargement issue caused by misaligned penalties. Simulation results further
show that IoU itself serves as an ideal regression target, while existing
geometric penalties are both unnecessary and suboptimal. Building on InterpIoU,
we introduce Dynamic InterpIoU, which dynamically adjusts interpolation
coefficients based on IoU values, enhancing adaptability to scenarios with
diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC
show that our methods consistently outperform state-of-the-art IoU-based losses
across various detection frameworks, with particularly notable improvements in
small object detection, confirming their effectiveness.

</details>


### [143] [DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition](https://arxiv.org/abs/2507.12426)
*Hayat Ullah,Muhammad Ali Shafique,Abbas Khan,Arslan Munir*

Main category: cs.CV

TL;DR: This paper proposes DVFL-Net, a lightweight video recognition model, using knowledge distillation and spatial-temporal feature modulation to achieve high efficiency with strong accuracy for real-time human action recognition.


<details>
  <summary>Details</summary>
Motivation: Video recognition methods have shifted from CNNs to Transformer architectures for improved performance. However, Transformers are computationally expensive for dense video data. This paper aims to develop an efficient and effective model for on-device deployment.

Method: The authors propose DVFL-Net, which employs knowledge distillation and spatial-temporal focal modulation to transfer both local and global knowledge from a larger teacher model to a compact student model. The framework uses forward KL divergence to enhance knowledge transfer.

Result: DVFL-Net outperforms existing methods on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400 benchmarks. It achieves a balance between accuracy and computational efficiency with reduced memory usage and FLOPs.

Conclusion: DVFL-Net is a practical and efficient solution for real-time human action recognition, providing a strong trade-off between performance and computational demands, making it suitable for on-device applications.

Abstract: The landscape of video recognition has evolved significantly, shifting from
traditional Convolutional Neural Networks (CNNs) to Transformer-based
architectures for improved accuracy. While 3D CNNs have been effective at
capturing spatiotemporal dynamics, recent Transformer models leverage
self-attention to model long-range spatial and temporal dependencies. Despite
achieving state-of-the-art performance on major benchmarks, Transformers remain
computationally expensive, particularly with dense video data. To address this,
we propose a lightweight Video Focal Modulation Network, DVFL-Net, which
distills spatiotemporal knowledge from a large pre-trained teacher into a
compact nano student model, enabling efficient on-device deployment. DVFL-Net
utilizes knowledge distillation and spatial-temporal feature modulation to
significantly reduce computation while preserving high recognition performance.
We employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal
focal modulation to effectively transfer both local and global context from the
Video-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate
DVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it
against recent state-of-the-art methods in Human Action Recognition (HAR).
Additionally, we conduct a detailed ablation study analyzing the impact of
forward KL divergence. The results confirm the superiority of DVFL-Net in
achieving an optimal balance between performance and efficiency, demonstrating
lower memory usage, reduced GFLOPs, and strong accuracy, making it a practical
solution for real-time HAR applications.

</details>


### [144] [Traffic-Aware Pedestrian Intention Prediction](https://arxiv.org/abs/2507.12433)
*Fahimeh Orvati Nia,Hai Lin*

Main category: cs.CV

TL;DR: The paper proposes a new model, TA-STGCN, for accurate pedestrian intention estimation in autonomous vehicles, incorporating traffic signals and spatial-temporal context, and achieving a 4.75% higher accuracy on the PIE dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing pedestrian intention prediction models that neglect critical factors like dynamic traffic signals and contextual scene information.

Method: The paper introduces TA-STGCN, a Traffic-Aware Spatio-Temporal Graph Convolutional Network, which integrates traffic signal states and bounding box size to model spatial and temporal dependencies.

Result: The proposed model improves prediction accuracy by 4.75% compared to the baseline when evaluated on the PIE dataset.

Conclusion: TA-STGCN effectively improves pedestrian intention prediction by incorporating traffic-aware spatial and temporal features, offering significant advancements for safe autonomous navigation.

Abstract: Accurate pedestrian intention estimation is crucial for the safe navigation
of autonomous vehicles (AVs) and hence attracts a lot of research attention.
However, current models often fail to adequately consider dynamic traffic
signals and contextual scene information, which are critical for real-world
applications. This paper presents a Traffic-Aware Spatio-Temporal Graph
Convolutional Network (TA-STGCN) that integrates traffic signs and their states
(Red, Yellow, Green) into pedestrian intention prediction. Our approach
introduces the integration of dynamic traffic signal states and bounding box
size as key features, allowing the model to capture both spatial and temporal
dependencies in complex urban environments. The model surpasses existing
methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy
compared to the baseline model on the PIE dataset, demonstrating its
effectiveness in improving pedestrian intention prediction.

</details>


### [145] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: The paper introduces DAM-QA, a framework leveraging region-aware capabilities of DAM for text-rich Visual Question Answering (VQA), achieving superior performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome challenges in VQA tasks, particularly those involving dense text in images, where fine-grained text extraction is essential for providing correct answers.

Method: The DAM-QA framework integrates mechanisms to aggregate answers from multiple regional views of image content, leveraging the Describe Anything Model (DAM) for its region-level descriptive capabilities.

Result: DAM-QA significantly outperforms the baseline DAM by over 7 points on the DocVQA benchmark and achieves competitive performance with fewer parameters compared to strong generalist VLMs.

Conclusion: The results demonstrate the effectiveness of region-aware vision-language models like DAM when adapted for text-rich VQA tasks, illustrating DAM-QA's potential to improve performance in such scenarios.

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [146] [Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios](https://arxiv.org/abs/2507.12449)
*Van-Hoang-Anh Phan,Chi-Tam Nguyen,Doan-Trung Au,Thanh-Danh Phan,Minh-Thien Duong,My-Ha Le*

Main category: cs.CV

TL;DR: The paper proposes an efficient obstacle avoidance system for autonomous vehicles using YOLOv11 and a Frenet-Pure Pursuit-based planner.


<details>
  <summary>Details</summary>
Motivation: To improve the safety and navigation capabilities of autonomous vehicles in complex environments by addressing obstacle avoidance challenges.

Method: The system integrates a camera-only perception module using YOLOv11 for object detection and monocular depth estimation with Depth Anything V2, combined with Frenet-Pure Pursuit-based planning.

Result: The system was evaluated in diverse university campus scenarios, showing effectiveness in identifying and avoiding obstacles with enhanced navigation.

Conclusion: The proposed method demonstrates robust, efficient, and accurate obstacle avoidance in real-world conditions, improving autonomous vehicle safety and navigation.

Abstract: Obstacle avoidance is essential for ensuring the safety of autonomous
vehicles. Accurate perception and motion planning are crucial to enabling
vehicles to navigate complex environments while avoiding collisions. In this
paper, we propose an efficient obstacle avoidance pipeline that leverages a
camera-only perception module and a Frenet-Pure Pursuit-based planning
strategy. By integrating advancements in computer vision, the system utilizes
YOLOv11 for object detection and state-of-the-art monocular depth estimation
models, such as Depth Anything V2, to estimate object distances. A comparative
analysis of these models provides valuable insights into their accuracy,
efficiency, and robustness in real-world conditions. The system is evaluated in
diverse scenarios on a university campus, demonstrating its effectiveness in
handling various obstacles and enhancing autonomous navigation. The video
presenting the results of the obstacle avoidance experiments is available at:
https://www.youtube.com/watch?v=FoXiO5S_tA8

</details>


### [147] [Mitigating Object Hallucinations via Sentence-Level Early Intervention](https://arxiv.org/abs/2507.12455)
*Shangpin Peng,Senqiao Yang,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: The study introduces SENTINEL, a novel method to significantly reduce hallucinations in multimodal large language models (MLLMs) through sentence-level early intervention.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work lies in addressing the significant issue of hallucinations in MLLMs, which involve producing fabricated content that contradicts visual input, while avoiding the high computational costs or data mismatches of existing solutions.

Method: The proposed framework, SENTINEL, generates in-domain preference pairs by cross-checking model outputs with open-vocabulary detectors and categorizing them based on hallucination. It trains the model using a context-aware preference loss (C-DPO) to specifically target sentence-level hallucination instances.

Result: SENTINEL reduces hallucinations by over 90% compared to the baseline model and surpasses the performance of the previous state-of-the-art on both hallucination and general capability benchmarks.

Conclusion: The study concludes that SENTINEL offers an effective and efficient solution to hallucination problems in MLLMs, demonstrating both strong performance improvements and the potential for broad applicability.

Abstract: Multimodal large language models (MLLMs) have revolutionized cross-modal
understanding but continue to struggle with hallucinations - fabricated content
contradicting visual inputs. Existing hallucination mitigation methods either
incur prohibitive computational costs or introduce distribution mismatches
between training data and model outputs. We identify a critical insight:
hallucinations predominantly emerge at the early stages of text generation and
propagate through subsequent outputs. To address this, we propose **SENTINEL**
(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain
pr**E**ference **L**earning), a framework that eliminates dependency on human
annotations. Specifically, we first bootstrap high-quality in-domain preference
pairs by iteratively sampling model outputs, validating object existence
through cross-checking with two open-vocabulary detectors, and classifying
sentences into hallucinated/non-hallucinated categories. Subsequently, we use
context-coherent positive samples and hallucinated negative samples to build
context-aware preference data iteratively. Finally, we train models using a
context-aware preference loss (C-DPO) that emphasizes discriminative learning
at the sentence level where hallucinations initially manifest. Experimental
results show that SENTINEL can reduce hallucinations by over 90\% compared to
the original model and outperforms the previous state-of-the-art method on both
hallucination benchmarks and general capabilities benchmarks, demonstrating its
superiority and generalization ability. The models, datasets, and code are
available at https://github.com/pspdada/SENTINEL.

</details>


### [148] [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](https://arxiv.org/abs/2507.12461)
*Trong-Thang Pham,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: This paper introduces RadGazeIntent, a transformer-based deep learning approach that models radiologists' intention-driven gaze behaviors during medical image interpretation.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing models that fail to interpret the intent behind radiologists' gaze movements when analyzing medical images.

Method: The paper develops RadGazeIntent, a transformer-based architecture that processes temporal and spatial gaze data to model radiologists’ diagnostic intentions. The authors also create intention-labeled datasets derived from eye-tracking data.

Result: RadGazeIntent outperforms baseline methods in predicting which findings radiologists are examining at specific moments across multiple intention-labeled datasets.

Conclusion: RadGazeIntent successfully captures and predicts the diagnostic intent behind radiologists’ gaze patterns, offering a novel step in gaze tracking and medical image interpretation.

Abstract: Radiologists rely on eye movements to navigate and interpret medical images.
A trained radiologist possesses knowledge about the potential diseases that may
be present in the images and, when searching, follows a mental checklist to
locate them using their gaze. This is a key observation, yet existing models
fail to capture the underlying intent behind each fixation. In this paper, we
introduce a deep learning-based approach, RadGazeIntent, designed to model this
behavior: having an intention to find something and actively searching for it.
Our transformer-based architecture processes both the temporal and spatial
dimensions of gaze data, transforming fine-grained fixation features into
coarse, meaningful representations of diagnostic intent to interpret
radiologists' goals. To capture the nuances of radiologists' varied
intention-driven behaviors, we process existing medical eye-tracking datasets
to create three intention-labeled subsets: RadSeq (Systematic Sequential
Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid
Pattern). Experimental results demonstrate RadGazeIntent's ability to predict
which findings radiologists are examining at specific moments, outperforming
baseline methods across all intention-labeled datasets.

</details>


### [149] [SpatialTrackerV2: 3D Point Tracking Made Easy](https://arxiv.org/abs/2507.12462)
*Yuxi Xiao,Jianyuan Wang,Nan Xue,Nikita Karaev,Yuri Makarov,Bingyi Kang,Xing Zhu,Hujun Bao,Yujun Shen,Xiaowei Zhou*

Main category: cs.CV

TL;DR: SpatialTrackerV2 is a feed-forward 3D point tracker for monocular videos that integrates geometry, ego-motion, and object motion tracking to achieve high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To address limitations in modular 3D tracking pipelines and unify key components like point tracking, depth estimation, and camera pose estimation into an end-to-end system.

Method: It uses a fully differentiable and end-to-end architecture that decomposes 3D motion into scene geometry, camera ego-motion, and object motion. It facilitates training across diverse datasets ranging from synthetic to unlabeled footage.

Result: SpatialTrackerV2 achieves a 30% improvement over existing 3D tracking methods and matches leading 3D reconstruction accuracy while being 50x faster.

Conclusion: The unified approach demonstrates high scalability and performance, providing a significant advancement in 3D point tracking for various datasets and applications.

Abstract: We present SpatialTrackerV2, a feed-forward 3D point tracking method for
monocular videos. Going beyond modular pipelines built on off-the-shelf
components for 3D tracking, our approach unifies the intrinsic connections
between point tracking, monocular depth, and camera pose estimation into a
high-performing and feedforward 3D point tracker. It decomposes world-space 3D
motion into scene geometry, camera ego-motion, and pixel-wise object motion,
with a fully differentiable and end-to-end architecture, allowing scalable
training across a wide range of datasets, including synthetic sequences, posed
RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and
motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms
existing 3D tracking methods by 30%, and matches the accuracy of leading
dynamic 3D reconstruction approaches while running 50$\times$ faster.

</details>


### [150] [MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding](https://arxiv.org/abs/2507.12463)
*Renjie Li,Ruijie Ye,Mingyang Wu,Hao Frank Yang,Zhiwen Fan,Hezhen Hu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: The paper introduces MMHU, a large-scale benchmark for analyzing human behavior in autonomous driving, featuring diverse data and annotations.


<details>
  <summary>Details</summary>
Motivation: Understanding human behaviors in transportation is critical for developing safe driving systems, but existing research lacks a comprehensive evaluation benchmark.

Method: The authors developed MMHU, a dataset featuring 57k clips, 1.73M frames, and annotations like human motion, text descriptions, and behavior labels, gathered from diverse sources and annotated using a human-in-the-loop pipeline.

Result: The paper provides dataset analysis and benchmarks for tasks like motion prediction, generation, and human behavior question answering, showcasing MMHU's utility.

Conclusion: MMHU serves as a valuable resource for evaluating and advancing human behavior understanding in autonomous driving systems.

Abstract: Humans are integral components of the transportation ecosystem, and
understanding their behaviors is crucial to facilitating the development of
safe driving systems. Although recent progress has explored various aspects of
human behavior$\unicode{x2014}$such as motion, trajectories, and
intention$\unicode{x2014}$a comprehensive benchmark for evaluating human
behavior understanding in autonomous driving remains unavailable. In this work,
we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis
featuring rich annotations, such as human motion and trajectories, text
description for human motions, human intention, and critical behavior labels
relevant to driving safety. Our dataset encompasses 57k human motion clips and
1.73M frames gathered from diverse sources, including established driving
datasets such as Waymo, in-the-wild videos from YouTube, and self-collected
data. A human-in-the-loop annotation pipeline is developed to generate rich
behavior captions. We provide a thorough dataset analysis and benchmark
multiple tasks$\unicode{x2014}$ranging from motion prediction to motion
generation and human behavior question answering$\unicode{x2014}$thereby
offering a broad evaluation suite. Project page :
https://MMHU-Benchmark.github.io.

</details>


### [151] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. Götze,Carsten Marr,Steffen Schneider*

Main category: cs.CV

TL;DR: The paper introduces CytoSAE, a sparse autoencoder for hematology, capable of identifying morphologically relevant visual concepts in single-cell blood images.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of interpretability tools for transformer-based models in the medical imaging field, particularly focusing on cellular-level insights in hematology.

Method: They developed CytoSAE, trained it on a dataset of 40,000 single-cell blood images, and validated its performance across multiple datasets, including out-of-domain examples.

Result: CytoSAE generalizes well to diverse datasets, identifies disease-specific and patient-specific concepts, and achieves classification performance on par with state-of-the-art methods, while providing improved explainability.

Conclusion: CytoSAE successfully combines performance with explainability, paving the way for more interpretable AI solutions in medical hematology imaging.

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


### [152] [PhysX: Physical-Grounded 3D Asset Generation](https://arxiv.org/abs/2507.12465)
*Ziang Cao,Zhaoxi Chen,Linag Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces PhysX, an end-to-end system for physical-grounded 3D asset generation, addressing the lack of physics annotation in models used for real-world applications.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generative models overlook physical properties, limiting their application in areas like simulation and embodied AI.

Method: The authors present PhysXNet, a physics-annotated dataset created via a human-in-the-loop pipeline, and PhysXGen, a feed-forward framework with a dual-branch architecture to enhance physical accuracy.

Result: Experiments show that PhysX achieves superior and generalizable performance in creating 3D assets with physical plausibility while maintaining geometry quality.

Conclusion: PhysX bridges the gap between geometrical modeling and physical annotation in 3D generation, providing tools and datasets for advancing generative physical AI research.

Abstract: 3D modeling is moving from virtual to physical. Existing 3D generation
primarily emphasizes geometries and textures while neglecting physical-grounded
modeling. Consequently, despite the rapid development of 3D generative models,
the synthesized 3D assets often overlook rich and important physical
properties, hampering their real-world application in physical domains like
simulation and embodied AI. As an initial attempt to address this challenge, we
propose \textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset
generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we
present PhysXNet - the first physics-grounded 3D dataset systematically
annotated across five foundational dimensions: absolute scale, material,
affordance, kinematics, and function description. In particular, we devise a
scalable human-in-the-loop annotation pipeline based on vision-language models,
which enables efficient creation of physics-first assets from raw 3D assets.2)
Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for
physics-grounded image-to-3D asset generation, injecting physical knowledge
into the pre-trained 3D structural space. Specifically, PhysXGen employs a
dual-branch architecture to explicitly model the latent correlations between 3D
structures and physical properties, thereby producing 3D assets with plausible
physical predictions while preserving the native geometry quality. Extensive
experiments validate the superior performance and promising generalization
capability of our framework. All the code, data, and models will be released to
facilitate future research in generative physical AI.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [153] [The AI Shadow War: SaaS vs. Edge Computing Architectures](https://arxiv.org/abs/2507.11545)
*Rhea Pritham Marpu,Kevin J McNamara,Preeti Gupta*

Main category: cs.DC

TL;DR: The paper contrasts cloud-based AI and edge AI, highlighting edge AI's efficiency, privacy, and market growth potential.


<details>
  <summary>Details</summary>
Motivation: To investigate the advantages and competitive positioning of decentralized edge AI over traditional centralized cloud-based models.

Method: Analyzed factors like computational capability, energy efficiency, and data privacy, highlighting innovations in edge AI such as test-time training and mixture-of-experts architectures.

Result: Edge AI is shown to be significantly more efficient, consuming 10,000x less power than cloud models, and has strong advantages in privacy, affordability, and latency.

Conclusion: Edge AI will drive technological growth and adoption, establishing hybrid edge-cloud ecosystems for a more decentralized and efficient future.

Abstract: The very DNA of AI architecture presents conflicting paths: centralized
cloud-based models (Software-as-a-Service) versus decentralized edge AI (local
processing on consumer devices). This paper analyzes the competitive
battleground across computational capability, energy efficiency, and data
privacy. Recent breakthroughs show edge AI challenging cloud systems on
performance, leveraging innovations like test-time training and
mixture-of-experts architectures. Crucially, edge AI boasts a 10,000x
efficiency advantage: modern ARM processors consume merely 100 microwatts
forinference versus 1 watt for equivalent cloud processing. Beyond efficiency,
edge AI secures data sovereignty by keeping processing local, dismantling
single points of failure in centralized architectures. This democratizes access
throughaffordable hardware, enables offline functionality, and reduces
environmental impact by eliminating data transmission costs. The edge AI market
projects explosive growth from $9 billion in 2025 to $49.6 billion by 2030
(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical
applications including personalized education, healthcare monitoring,
autonomous transport, and smart infrastructure rely on edge AI's ultra-low
latency (5-10ms versus 100-500ms for cloud). The convergence of architectural
innovation with fundamental physics confirms edge AI's distributed approach
aligns with efficient information processing, signaling the inevitable
emergence of hybrid edge-cloud ecosystems.

</details>


### [154] [A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing](https://arxiv.org/abs/2507.11560)
*Xin Wang,Xiao Huan Li,Xun Wang*

Main category: cs.DC

TL;DR: This paper proposes an algorithm for offloading computation-intensive AI tasks in industrial edge computing environments, achieving improved efficiency and reduced latency and energy consumption.


<details>
  <summary>Details</summary>
Motivation: Traditional cloud computing methods struggle to meet the real-time demands of AI tasks in IIoT environments due to latency and resource constraints.

Method: The paper introduces a framework where IIoT devices utilize a Multi-Agent Deep Deterministic Policy Gradient-based algorithm to dynamically offload tasks to edge servers optimized for different generative models.

Result: The MADDPG-MATO algorithm demonstrated superior performance, with reductions of 6.98% in latency, 7.12% in energy consumption, and a 3.72% increase in task completion rate.

Conclusion: The proposed method is robust and efficient, making it suitable for dynamic, high-load IIoT scenarios.

Abstract: The integration of the Industrial Internet of Things (IIoT) with Artificial
Intelligence-Generated Content (AIGC) offers new opportunities for smart
manufacturing, but it also introduces challenges related to
computation-intensive tasks and low-latency demands. Traditional generative
models based on cloud computing are difficult to meet the real-time
requirements of AIGC tasks in IIoT environments, and edge computing can
effectively reduce latency through task offloading. However, the dynamic nature
of AIGC tasks, model switching delays, and resource constraints impose higher
demands on edge computing environments. To address these challenges, this paper
proposes an AIGC task offloading framework tailored for IIoT edge computing
environments, considering the latency and energy consumption caused by AIGC
model switching for the first time. IIoT devices acted as multi-agent
collaboratively offload their dynamic AIGC tasks to the most appropriate edge
servers deployed with different generative models. A model aware AIGC task
offloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient
(MADDPG-MATO) is devised to minimize the latency and energy. Experimental
results show that MADDPG-MATO outperforms baseline algorithms, achieving an
average reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%
increase in task completion rate across four sets of experiments with model
numbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is
robust and efficient in dynamic, high-load IIoT environments.

</details>


### [155] [Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers](https://arxiv.org/abs/2507.11563)
*Giulio Attenni,Novella Bartolini*

Main category: cs.DC

TL;DR: The paper introduces an optimization model for environmentally-conscious job deployment in cloud environments focusing on sustainability metrics.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing demand for sustainable cloud services and the need to reduce the environmental impact while meeting sustainability requirements.

Method: An optimization model is developed that balances multiple environmental factors and incorporates sustainability indicators for data centers.

Result: A simulative case study shows that the proposed method is superior to strategies focusing on single sustainability factors.

Conclusion: Incorporating comprehensive sustainability metrics into cloud job deployment can effectively reduce the ecological footprint.

Abstract: This paper presents a theoretical discussion for environmentally-conscious
job deployment and migration in cloud environments, aiming to minimize the
environmental impact of resource provisioning while incorporating
sustainability requirements. As the demand for sustainable cloud services
grows, it is crucial for cloud customers to select data center operators based
on sustainability metrics and to accurately report the ecological footprint of
their services. To this end, we analyze sustainability reports and define
comprehensive environmental impact profiles for data centers, incorporating key
sustainability indicators. We formalize the problem as an optimization model,
balancing multiple environmental factors while respecting user preferences. A
simulative case study demonstrates the {potential} of our approach compared to
baseline strategies that optimize for single sustainability factors.

</details>


### [156] [PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training](https://arxiv.org/abs/2507.11683)
*Seth Ockerman,Amal Gueroudji,Tanwi Mallick,Yixuan He,Line Pouchard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: Spatiotemporal graph neural networks (ST-GNNs) often face memory constraints, limiting scalability. This paper introduces PyTorch Geometric Temporal Index (PGT-I) with distributed training and novel batching strategies to enable large-scale ST-GNN training efficiently.


<details>
  <summary>Details</summary>
Motivation: To overcome the memory limitations in training ST-GNNs on large datasets and enhance scalability of existing frameworks.

Method: Introduces PGT-I, which incorporates distributed data parallel training, index-batching to exploit spatiotemporal data structure, and distributed-index-batching to spread workload across GPUs.

Result: PGT-I allows training ST-GNNs on large datasets without graph partitioning, reduces peak memory usage by up to 89%, and achieves up to a 13.1x speedup over standard frameworks using 128 GPUs.

Conclusion: PGT-I effectively enables scalable and memory-efficient training of ST-GNNs, making it suitable for larger datasets and broader applications.

Abstract: Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for
modeling spatial and temporal data dependencies. However, their applications
have been limited primarily to small-scale datasets because of memory
constraints. While distributed training offers a solution, current frameworks
lack support for spatiotemporal models and overlook the properties of
spatiotemporal data. Informed by a scaling study on a large-scale workload, we
present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch
Geometric Temporal that integrates distributed data parallel training and two
novel strategies: index-batching and distributed-index-batching. Our index
techniques exploit spatiotemporal structure to construct snapshots dynamically
at runtime, significantly reducing memory overhead, while
distributed-index-batching extends this approach by enabling scalable
processing across multiple GPUs. Our techniques enable the first-ever training
of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing
peak memory usage by up to 89\% and achieving up to a 13.1x speedup over
standard DDP with 128 GPUs.

</details>


### [157] [Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI](https://arxiv.org/abs/2507.11830)
*Samyam Rajbhandari,Mert Hidayetoglu,Aurick Qiao,Ye Wang,Juncheng Yang,Jeff Rasley,Michael Wyatt,Yuxiong He*

Main category: cs.DC

TL;DR: Arctic Inference, an open-source vLLM plugin, offers faster, cost-effective enterprise AI inference by introducing Shift Parallelism and optimization techniques, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing AI inference systems which require trade-offs between latency, throughput, and cost.

Method: Introduced Shift Parallelism and integrated speculative decoding, SwiftKV compute reduction, and optimized embedding inference in the Arctic Inference plugin.

Result: Achieved up to 3.4x faster request completion, 1.75x faster generation, and 1.6M tokens/sec per GPU for embeddings compared to optimized deployments.

Conclusion: Arctic Inference establishes a state-of-the-art framework for enterprise AI inference with improved efficiency, now accessible to the community.

Abstract: Inference is now the dominant AI workload, yet existing systems force
trade-offs between latency, throughput, and cost. Arctic Inference, an
open-source vLLM plugin from Snowflake AI Research, introduces Shift
Parallelism, a dynamic parallelism strategy that adapts to real-world traffic
while integrating speculative decoding, SwiftKV compute reduction, and
optimized embedding inference. It achieves up to 3.4 times faster request
completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for
embeddings, outperforming both latency- and throughput-optimized deployments.
Already powering Snowflake Cortex AI, Arctic Inference delivers
state-of-the-art, cost-effective inference for enterprise AI and is now
available to the community.

</details>


### [158] [Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst](https://arxiv.org/abs/2507.11899)
*Saeid Aghasoleymani Najafabadi*

Main category: cs.DC

TL;DR: This paper examines adaptive load balancing strategies in cloud computing, focusing on algorithms evaluated with Cloud Analyst and highlighting performance across centralized and distributed setups.


<details>
  <summary>Details</summary>
Motivation: Workloads in cloud environments are dynamic and unpredictable, necessitating the development of adaptive and intelligent load balancing methods.

Method: The study employed the Cloud Analyst simulation tool to assess load balancing algorithms like Round Robin, Equally Spread, and Throttled in different resource scenarios.

Result: Results show Round Robin excels in single data centers, while Equally Spread and Throttled algorithms perform better in distributed environments by reducing response times and operational costs.

Conclusion: Strategic resource placement and advanced load balancing techniques are essential for optimizing performance, reducing costs, and meeting dynamic cloud demands.

Abstract: Load balancing plays a pivotal role in cloud computing, ensuring that
resources are optimally allocated to maintain high service quality and
operational efficiency. As workloads in cloud environments become increasingly
dynamic and unpredictable, load balancing strategies are evolving from
traditional static methods to more adaptive and intelligent approaches. In this
study, the Cloud Analyst simulation tool was used to evaluate the performance
of different load balancing algorithms under various scenarios, including both
centralized and distributed resource setups. The results highlight that while
the Round Robin algorithm yields slightly better processing times within a
single data center, Equally Spread and Throttled techniques perform
competitively, especially when network latency is considered. More importantly,
when resources are distributed across multiple data centers, response times are
significantly reduced, emphasizing the value of proximity and efficient load
distribution. In these distributed environments, Equally Spread and Throttled
algorithms not only maintain quick response times but also contribute to lower
operational costs. These findings demonstrate the necessity of strategic
resource placement and proactive infrastructure planning to balance performance
and cost. Adopting intelligent, dynamic load balancing and resource management
practices can help organizations meet evolving cloud demands, optimize costs,
and maintain a competitive advantage. Continuous evaluation and integration of
emerging technologies are crucial for sustaining effective and scalable cloud
operations.

</details>


### [159] [Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics](https://arxiv.org/abs/2507.11929)
*Minchen Yu,Yinghao Ren,Jiamu Zhao,Jiaqi Li*

Main category: cs.DC

TL;DR: The paper proposes an extensible design principle for serverless platforms to balance performance optimization and simplicity. Using a data analytics-focused prototype (Proteus), it introduces decision workflows for customization, resulting in performance enhancements and resource sharing benefits.


<details>
  <summary>Details</summary>
Motivation: Current serverless solutions face a dilemma between performance and simplicity. Optimizing for complex workloads with application-specific designs undermines the general use case, while relying on general platforms often yields unsatisfactory performance in specialized domains.

Method: The study proposes an extensible design where developers can extend and customize serverless platform behaviors. Proteus, a platform designed following this principle, introduces decision workflows enabling optimization of control-plane behaviors for domain-specific needs.

Result: Preliminary results demonstrate that the Proteus prototype is effective in optimizing execution performance of analytical queries. It also enhances fine-grained resource sharing, making it suitable for diverse serverless applications.

Conclusion: By enabling system extensibility through decision workflows, Proteus bridges the gap between optimization for complex tasks and maintaining general-purpose simplicity in serverless environments. This demonstrates the feasibility of the proposed design principle.

Abstract: Serverless computing has attracted a broad range of applications due to its
ease of use and resource elasticity. However, developing serverless
applications often poses a dilemma -- relying on general-purpose serverless
platforms can fall short of delivering satisfactory performance for complex
workloads, whereas building application-specific serverless systems undermines
the simplicity and generality. In this paper, we propose an extensible design
principle for serverless computing. We argue that a platform should enable
developers to extend system behaviors for domain-specialized optimizations
while retaining a shared, easy-to-use serverless environment. We take data
analytics as a representative serverless use case and realize this design
principle in Proteus. Proteus introduces a novel abstraction of decision
workflows, allowing developers to customize control-plane behaviors for
improved application performance. Preliminary results show that Proteus's
prototype effectively optimizes analytical query execution and supports
fine-grained resource sharing across diverse applications.

</details>


### [160] [NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning](https://arxiv.org/abs/2507.11978)
*Jiacheng Huang,Zimin Li,Yinghui Li,Haojie Wang*

Main category: cs.DC

TL;DR: The paper introduces NineToothed, a DSL that simplifies the creation of compute kernels for deep learning by using serial semantics and tensor-oriented metaprogramming (TOM), automating parallel code generation with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Current DSLs like Triton require expertise in parallel programming, making it hard to develop and maintain high-performance compute kernels. The authors aim to overcome this barrier by introducing a programming model supporting serial semantics.

Method: NineToothed employs a domain-specific language with tensor-oriented metaprogramming (TOM) and an arrange-and-apply paradigm for tiled computations. It features a code generator that automatically converts serial code into high-performance parallel code.

Result: The evaluation demonstrates that NineToothed simplifies kernel development while achieving performance comparable to Triton.

Conclusion: NineToothed reduces development complexity for deep learning workloads by enabling serial programming and automating parallel code generation, allowing for simplicity without significant performance trade-offs.

Abstract: The emergence of deep learning domain-specific languages (DSLs) has
substantially reduced the obstacles in developing high-performance,
cross-platform compute kernels. However, current DSLs, such as Triton, still
demand that developers possess expertise in parallel programming and expose
them to many low-level details. This requirement complicates the development
process and adds to the difficulty of maintaining compute kernels.
Consequently, developing a new programming model that supports serial
programming for deep learning workloads is crucial.
  This paper introduces NineToothed, a domain-specific language that offers
serial semantics for machine learning programming. Through the automatic
transformation of serial code into parallel code, NineToothed significantly
streamlines the development process while causing minimal performance
degradation. NineToothed encompasses (1) a language with tensor-oriented
metaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the
expression of tiled computations without the need to manage low-level details
and (2) a code generator for generating high-performance parallel code. Our
evaluation results indicate that NineToothed can greatly simplify compute
kernel development while maintaining performance comparable to that of Triton.

</details>


### [161] [ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum](https://arxiv.org/abs/2507.12032)
*Brian-Frederik Jahnke,René Brinkhege,Jan Peter Meyer,Daniel Tebernum,Falk Howar*

Main category: cs.DC

TL;DR: The paper introduces ARRC, an explainable recommender system for optimizing resource allocation in edge-cloud systems, which reduces complexity, improves utilization, and supports maintainability without disrupting workflows.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of resource optimization across the edge-cloud continuum, tackling issues of overprovisioning, operational complexity, and the lack of explainability and maintainability in existing systems.

Method: The authors propose ARRC, an explainable cross-layer recommender system that integrates into operator workflows. It utilizes specialized, auditable agents coordinated through a shared interface to provide resource recommendations and their rationale transparently.

Result: Empirical results show that ARRC reduces operator workload by over 50%, improves compute utilization by up to 7.7 times, and maintains error rates below 5%, achieving these outcomes through operator-approved, incremental changes in a multi-region industrial deployment.

Conclusion: ARRC demonstrates that explainable, recommendation-based systems can enhance efficiency, transparency, and maintainability at production scale, promoting best practices for robust edge-cloud resource management.

Abstract: Achieving sustainable, explainable, and maintainable automation for resource
optimization is a core challenge across the edge-cloud continuum. Persistent
overprovisioning and operational complexity often stem from heterogeneous
platforms and layered abstractions, while systems lacking explainability and
maintainability become fragile, impede safe recovery, and accumulate technical
debt. Existing solutions are frequently reactive, limited to single abstraction
layers, or require intrusive platform changes, leaving efficiency and
maintainability gains unrealized.
  This paper addresses safe, transparent, and low-effort resource optimization
in dynamic, multi-tenant edge-cloud systems, without disrupting operator
workflows or increasing technical debt. We introduce ARRC, a recommender system
rooted in software engineering design principles, which delivers explainable,
cross-layer resource recommendations directly into operator workflows (such as
tickets and GitOps pull requests). ARRC encapsulates optimization logic in
specialized, auditable agents coordinated via a shared interface, supporting
maintainability and extensibility through transparency and the ability to
inspect both recommendations and their rationale.
  Empirical evaluation in a multi-region industrial deployment shows that ARRC
reduces operator workload by over 50%, improves compute utilization by up to
7.7x, and maintains error rates below 5%, with most benefits achieved through
incremental, operator-approved changes. This demonstrates that explainable,
recommendation-based architectures can achieve sustainable efficiency and
maintainability improvements at production scale.
  ARRC provides an empirically evaluated framework for integrating explainable,
workflow-driven automation into resource management, intended to advance best
practices for robust, maintainable, and transparent edge-cloud continuum
platforms.

</details>


### [162] [Distributed Algorithms for Potential Problems](https://arxiv.org/abs/2507.12038)
*Alkida Balliu,Thomas Boudier,Francesco d'Amore,Dennis Olivetti,Gustav Schmid,Jukka Suomela*

Main category: cs.DC

TL;DR: The paper introduces a fast algorithm for solving local potential problems in distributed graph settings, specifically targeting locally optimal cuts in bounded-degree graphs with improved round complexity.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding the distributed round complexity for locally optimal cut problems in bounded-degree graphs.

Method: The authors propose and analyze a distributed algorithm capable of solving local potential problems efficiently in bounded-degree graphs, achieving logarithmic round complexity.

Result: It was proven that all local potential problems, including the locally optimal cut, can be resolved within $\log^{O(1)} n$ rounds in both deterministic and randomized LOCAL models.

Conclusion: This work establishes new upper bounds for the complexity of locally optimal cuts, clarifying deterministic round requirements and improving efficiency in solving these types of problems.

Abstract: In this work we present a fast distributed algorithm for local potential
problems: these are graph problems where the task is to find a locally optimal
solution where no node can unilaterally improve the utility in its local
neighborhood by changing its own label. A simple example of such a problem is
the task of finding a locally optimal cut, i.e., a cut where for each node at
least half of its incident edges are cut edges. The distributed round
complexity of locally optimal cut has been wide open; the problem is known to
require $\Omega(\log n)$ rounds in the deterministic LOCAL model and
$\Omega(\log \log n)$ rounds in the randomized LOCAL model, but the only known
upper bound is the trivial brute-force solution of $O(n)$ rounds. Locally
optimal cut in bounded-degree graphs is perhaps the simplest example of a
locally checkable labeling problem for which there is still such a large gap
between current upper and lower bounds. We show that in bounded-degree graphs,
all local potential problems, including locally optimal cut, can be solved in
$\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.
In particular, the deterministic round complexity of the locally optimal cut
problem is now settled to $\log^{\Theta(1)} n$.

</details>


### [163] [Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso](https://arxiv.org/abs/2507.12106)
*Antonio Salis,Gabriele Troina,Gianluca Boanelli,Marco Ottaviano,Paola Fortini,Soraya Versace*

Main category: cs.DC

TL;DR: The paper discusses how emerging technologies like IoT and machine learning can aid sustainable management of urban green spaces, focusing on a Smart Green City case in Campobasso, Italy.


<details>
  <summary>Details</summary>
Motivation: To enhance health and quality of life in urban areas by improving management of public green spaces using ecosystem services.

Method: Combining IoT sensors, machine learning-based predictive models, and a cloud-based decision support system to monitor and manage urban green spaces in real-time.

Result: The proposed system optimizes irrigation decisions, provides alerts to maintain parameters within thresholds, and supports sustainable governance of green spaces.

Conclusion: Digitalization and innovative technologies like IoT and predictive modeling significantly enhance the management of urban green spaces, contributing to environmental resilience and better urban living conditions.

Abstract: The efficient design and management of public green spaces is a key factor in
promoting the health and well-being of urban population, as emphasized by the
WHO, UNEP, and EEA. These areas serve as the "green lungs" of the urban
ecosystem, playing a vital role in enhancing quality of life thanks to the
provision of ecosystem services. In this context, the Smart Green City use case
in Campobasso municipality, funded by the Italian Ministry of Enterprises
(MIMIT), emerges as an innovative model for the sustainable management of green
urban areas through the adoption of an advanced system of emerging technologies
integrated and interoperable. The project integrates IoT systems and
data-driven governance platforms, enabling real-time monitoring of the health
status of trees and green areas via a Decision Support System (DSS). It also
facilitates the collection and analysis of data from diverse sources, including
weather conditions, air quality, soil moisture, pollution levels. The resulting
cloud-based platform supports a holistic real time decision making for green
urban managers, technical experts and operational staff. It enables intelligent
control and management of urban green spaces using Tree Talker sensors,
integrated with soil moisture and water potential monitoring systems. Thanks to
predictive models based on machine learning algorithms and real time data
provided by IoT sensors, irrigation of public parks can be optimized by
providing suggestions on when and how much water to apply. Customized alerts
layers are also activated warning users when monitored parameters, such as soil
temperature, humidity, or water potential, exceed predefined thresholds. This
Use Case demonstrates how digitalization, IoT sensors fusion and technological
innovation can support sustainable urban governance, fostering environmental
resilience and improving citizens quality of life.

</details>


### [164] [Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage](https://arxiv.org/abs/2507.12205)
*Junqing Lin,Jingwei Sun,Mingge Lu,Guangzhong Sun*

Main category: cs.DC

TL;DR: EC-SpMV is a GPU-optimized approach to improve Sparse Matrix-Vector Multiplication (SpMV) for sparse Large Language Models (LLMs), achieving significant speedups and reduced storage overhead.


<details>
  <summary>Details</summary>
Motivation: The current SpMV kernels and sparse matrix formats underperform with sparse LLMs due to their inability to leverage the specialized sparsity patterns, creating inefficiencies in terms of performance and storage.

Method: EC-SpMV proposes two innovations: a hierarchical block extraction algorithm for capturing block structures in sparse LLMs and a novel compressed sparse format (EC-CSR) that uses delta indexing to enhance memory efficiency and reduce storage needs.

Result: EC-SpMV delivers up to 6.44x speedup over existing SpMV libraries and decreases storage overhead by as much as 55.4%, validated on real sparse weight matrices from LLaMA and OPT models.

Conclusion: EC-SpMV effectively addresses the inefficiencies of existing SpMV approaches for sparse LLM inference, demonstrating remarkable performance and storage improvements through its novel algorithm and format.

Abstract: Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance
bottleneck in the local deployment of sparse Large Language Models (LLMs),
where inference predominantly operates on workloads during the decoder phase
with a batch size of one. Existing SpMV kernels and sparse matrix formats,
originally designed for scientific computing, fail to exploit the unique
structure patterns inherent in sparse LLMs, resulting in suboptimal performance
and excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized
SpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a
hierarchical block extraction algorithm that captures multiple granularities of
block structures within sparse LLMs, and (2) a novel compressed sparse format
(EC-CSR) that employs delta indexing to reduce storage overhead and enhance
memory access efficiency. Evaluated on real sparse weight matrices from LLaMA
and OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV
libraries and reduces storage overhead by up to 55.4% compared to CSR.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [165] [Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming](https://arxiv.org/abs/2507.11547)
*Yingxue Zhao,Qianyi Chen,Haoran Li,Haosu Zhou,Hamid Reza Attar,Tobias Pfaff,Tailin Wu,Nan Li*

Main category: cs.LG

TL;DR: The study introduces RUGNN, a graph neural network model for predicting sheet material deformation during forming processes with improved accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional AI-based methods for predicting material forming processes struggle to model complex 3D spatial relationships and temporal dynamics effectively.

Method: The proposed RUGNN model combines Gated Recurrent Units (GRUs) for temporal dynamics and U-Net-inspired graph mechanisms for spatial dependencies, alongside a novel 'node-to-surface' contact representation method.

Result: RUGNN validates its predictions against cold and hot forming simulations of aluminium alloys, showcasing superior accuracy and efficiency over baseline GNN models.

Conclusion: RUGNN offers reliable and precise manufacturability predictions, enhancing design processes for material forming applications.

Abstract: In recent years, various artificial intelligence-based surrogate models have
been proposed to provide rapid manufacturability predictions of material
forming processes. However, traditional AI-based surrogate models, typically
built with scalar or image-based neural networks, are limited in their ability
to capture complex 3D spatial relationships and to operate in a
permutation-invariant manner. To overcome these issues, emerging graph-based
surrogate models are developed using graph neural networks. This study
developed a new graph neural network surrogate model named Recurrent U
Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate
predictions of sheet material deformation fields across multiple forming
timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model
temporal dynamics and a U-Net inspired graph-based downsample/upsample
mechanism to handle spatial long-range dependencies. A novel 'node-to-surface'
contact representation method was proposed, offering significant improvements
in computational efficiency for large-scale contact interactions. The RUGNN
model was validated using a cold forming case study and a more complex hot
forming case study using aluminium alloys. Results demonstrate that the RUGNN
model provides accurate deformation predictions closely matching ground truth
FE simulations and outperforming several baseline GNN architectures. Model
tuning was also performed to identify suitable hyperparameters, training
strategies, and input feature representations. These results demonstrate that
RUGNN is a reliable approach to support sheet material forming design by
enabling accurate manufacturability predictions.

</details>


### [166] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.LG

TL;DR: This paper introduces SurgeryLSTM, a temporal ML model enhancing LOS prediction for spine surgeries with high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve predictions of patient LOS in elective spine surgeries while addressing the need for models with temporal awareness and explainability.

Method: Traditional ML models were compared to SurgeryLSTM, a masked BiLSTM with attention mechanism, and performance was assessed using R² and explainable AI techniques.

Result: SurgeryLSTM demonstrated superior predictive accuracy (R²=0.86) over competing models and highlighted impactful LOS predictors like bone disorder, kidney disease, and lumbar fusion.

Conclusion: SurgeryLSTM is an effective, interpretable solution for LOS prediction, suggesting its utility in clinical settings to improve planning and personalized patient care.

Abstract: Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [167] [Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators](https://arxiv.org/abs/2507.11574)
*Kazuma Kobayashi,Shailesh Garg,Farid Ahmed,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: The paper introduces the Conformalized Monte Carlo Operator (CMCO), a framework for providing calibrated, distribution-free uncertainty quantification in neural operators. It achieves reliable uncertainty estimates without retraining and supports real-time inference across diverse domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical challenge of robust uncertainty quantification required for the safe deployment of deep learning models in real-time virtual sensors, especially in high-stakes, noisy, and sparse data environments.

Method: The authors propose CMCO, which combines Monte Carlo dropout with split conformal prediction within a single DeepONet architecture. This enables spatially resolved uncertainty estimates efficiently, without relying on retraining, ensembling, or custom loss functions.

Result: Through evaluations in applications such as turbulent flow, elastoplastic deformation, and cosmic radiation dose estimation, CMCO consistently demonstrates reliable uncertainty quantification with near-nominal empirical coverage, even in complex scenarios with high spatial gradients.

Conclusion: CMCO provides a general-purpose uncertainty quantification solution with minimal computational overhead, making it valuable for high-stakes domains like digital twins, sensor fusion, and safety-critical systems in scientific machine learning.

Abstract: Robust uncertainty quantification (UQ) remains a critical barrier to the safe
deployment of deep learning in real-time virtual sensing, particularly in
high-stakes domains where sparse, noisy, or non-collocated sensor data are the
norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework
that transforms neural operator-based virtual sensing with calibrated,
distribution-free prediction intervals. By unifying Monte Carlo dropout with
split conformal prediction in a single DeepONet architecture, CMCO achieves
spatially resolved uncertainty estimates without retraining, ensembling, or
custom loss design. Our method addresses a longstanding challenge: how to endow
operator learning with efficient and reliable UQ across heterogeneous domains.
Through rigorous evaluation on three distinct applications: turbulent flow,
elastoplastic deformation, and global cosmic radiation dose estimation-CMCO
consistently attains near-nominal empirical coverage, even in settings with
strong spatial gradients and proxy-based sensing. This breakthrough offers a
general-purpose, plug-and-play UQ solution for neural operators, unlocking
real-time, trustworthy inference in digital twins, sensor fusion, and
safety-critical monitoring. By bridging theory and deployment with minimal
computational overhead, CMCO establishes a new foundation for scalable,
generalizable, and uncertainty-aware scientific machine learning.

</details>


### [168] [Einstein Fields: A Neural Perspective To Computational General Relativity](https://arxiv.org/abs/2507.11589)
*Sandeep Suresh Cranganore,Andrei Bodnar,Arturs Berzins,Johannes Brandstetter*

Main category: cs.LG

TL;DR: The paper introduces Einstein Fields, a neural representation for compressing computationally intensive 4D numerical relativity simulations into compact neural network weights.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiencies and storage limitations in numerical relativity simulations.

Method: The authors propose Neural Tensor Fields that encode the spacetime geometry of general relativity, leveraging automatic differentiation and compact neural network representations.

Result: Einstein Fields demonstrate advantages such as mesh-agnostic modeling, storage efficiency, derivative accuracy, and continuum representation of 4D spacetime.

Conclusion: The approach paves the way for scalable, expressive methods in numerical relativity and is supported by an open-source JAX-based library for broader accessibility.

Abstract: We introduce Einstein Fields, a neural representation that is designed to
compress computationally intensive four-dimensional numerical relativity
simulations into compact implicit neural network weights. By modeling the
\emph{metric}, which is the core tensor field of general relativity, Einstein
Fields enable the derivation of physical quantities via automatic
differentiation. However, unlike conventional neural fields (e.g., signed
distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural
Tensor Fields} with the key difference that when encoding the spacetime
geometry of general relativity into neural field representations, dynamics
emerge naturally as a byproduct. Einstein Fields show remarkable potential,
including continuum modeling of 4D spacetime, mesh-agnosticity, storage
efficiency, derivative accuracy, and ease of use. We address these challenges
across several canonical test beds of general relativity and release an open
source JAX-based library, paving the way for more scalable and expressive
approaches to numerical relativity. Code is made available at
https://github.com/AndreiB137/EinFields

</details>


### [169] [Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques](https://arxiv.org/abs/2507.11590)
*Raju Challagundla,Mohsen Dorodchi,Pu Wang,Minwoo Lee*

Main category: cs.LG

TL;DR: The paper surveys recent advancements in synthetic tabular data generation, emphasizing practical objectives and proposing a novel taxonomy and benchmark framework.


<details>
  <summary>Details</summary>
Motivation: Stringent privacy regulations and limited access to real-world data necessitate alternatives like synthetic tabular data generation, which is crucial in several domains.

Method: The paper introduces a taxonomy of generation objectives and proposes a benchmark framework linking technical methods to practical applications.

Result: The survey highlights methods preserving feature relationships while ensuring statistical fidelity and privacy, offering actionable guidance for methodological design.

Conclusion: The work acts as a roadmap for future research and practical implementation of synthetic tabular data in sensitive environments.

Abstract: As privacy regulations become more stringent and access to real-world data
becomes increasingly constrained, synthetic data generation has emerged as a
vital solution, especially for tabular datasets, which are central to domains
like finance, healthcare and the social sciences. This survey presents a
comprehensive and focused review of recent advances in synthetic tabular data
generation, emphasizing methods that preserve complex feature relationships,
maintain statistical fidelity, and satisfy privacy requirements. A key
contribution of this work is the introduction of a novel taxonomy based on
practical generation objectives, including intended downstream applications,
privacy guarantees, and data utility, directly informing methodological design
and evaluation strategies. Therefore, this review prioritizes the actionable
goals that drive synthetic data creation, including conditional generation and
risk-sensitive modeling. Additionally, the survey proposes a benchmark
framework to align technical innovation with real-world demands. By bridging
theoretical foundations with practical deployment, this work serves as both a
roadmap for future research and a guide for implementing synthetic tabular data
in privacy-critical environments.

</details>


### [170] [Reinforcement Learning from Adversarial Preferences in Tabular MDPs](https://arxiv.org/abs/2507.11706)
*Taira Tsuchiya,Shinji Ito,Haipeng Luo*

Main category: cs.LG

TL;DR: The paper introduces and analyzes Preference-based Markov Decision Processes (PbMDPs) under adversarial preferences utilizing Borda scores, establishing lower regret bounds and proposing computationally efficient algorithms.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of traditional episodic MDPs with adversarial losses by introducing a preference-based framework, which better reflects scenarios where learners only observe comparative preferences rather than direct numerical rewards.

Method: They establish regret lower bounds for PbMDPs with Borda scores using a preliminary analysis of episodic adversarial MDPs. They also propose two algorithms: 1) a global optimization approach using online linear optimization, and 2) a policy optimization algorithm with improved computational efficiency.

Result: The global optimization algorithm achieves a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3} T^{2/3})$ under known transitions, while the policy optimization algorithm achieves roughly $\tilde{O}((H^6 S K^5)^{1/3} T^{2/3})$, which is further extended to the unknown-transition setting.

Conclusion: PbMDPs are a promising framework for handling adversarial preferences in MDPs. The proposed algorithms, despite some computational challenges, achieve theoretical guarantees for regret bounds and extend to settings with unknown transitions.

Abstract: We introduce a new framework of episodic tabular Markov decision processes
(MDPs) with adversarial preferences, which we refer to as preference-based MDPs
(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the
numerical value of the loss is directly observed, in PbMDPs the learner instead
observes preferences between two candidate arms, which represent the choices
being compared. In this work, we focus specifically on the setting where the
reward functions are determined by Borda scores. We begin by establishing a
regret lower bound for PbMDPs with Borda scores. As a preliminary step, we
present a simple instance to prove a lower bound of $\Omega(\sqrt{HSAT})$ for
episodic MDPs with adversarial losses, where $H$ is the number of steps per
episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is
the number of episodes. Leveraging this construction, we then derive a regret
lower bound of $\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda
scores, where $K$ is the number of arms. Next, we develop algorithms that
achieve a regret bound of order $T^{2/3}$. We first propose a global
optimization approach based on online linear optimization over the set of all
occupancy measures, achieving a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3}
T^{2/3} )$ under known transitions. However, this approach suffers from
suboptimal dependence on the potentially large number of states $S$ and
computational inefficiency. To address this, we propose a policy optimization
algorithm whose regret is roughly bounded by $\tilde{O}( (H^6 S K^5)^{1/3}
T^{2/3} )$ under known transitions, and further extend the result to the
unknown-transition setting.

</details>


### [171] [Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification](https://arxiv.org/abs/2507.11620)
*Steven Dillmann,Juan Rafael Martínez-Galarza*

Main category: cs.LG

TL;DR: This paper introduces a novel method for analyzing irregular event time series using tensor representations and sparse autoencoders, tested on X-ray astronomy data.


<details>
  <summary>Details</summary>
Motivation: Event time series are irregular and complex, making it difficult for conventional methods to extract meaningful patterns in fields like astrophysics and healthcare.

Method: The authors developed two- and three-dimensional tensor representations for event time series, combined with sparse autoencoders to learn latent representations.

Result: The technique was validated on real-world X-ray astronomy data, effectively identifying temporal and spectral patterns and categorizing X-ray transients.

Conclusion: The proposed framework extends to various scientific and industrial applications, offering a flexible solution for analyzing irregular event time series.

Abstract: Event time series are sequences of discrete events occurring at irregular
time intervals, each associated with a domain-specific observational modality.
They are common in domains such as high-energy astrophysics, computational
social science, cybersecurity, finance, healthcare, neuroscience, and
seismology. Their unstructured and irregular structure poses significant
challenges for extracting meaningful patterns and identifying salient phenomena
using conventional techniques. We propose novel two- and three-dimensional
tensor representations for event time series, coupled with sparse autoencoders
that learn physically meaningful latent representations. These embeddings
support a variety of downstream tasks, including anomaly detection,
similarity-based retrieval, semantic clustering, and unsupervised
classification. We demonstrate our approach on a real-world dataset from X-ray
astronomy, showing that these representations successfully capture temporal and
spectral signatures and isolate diverse classes of X-ray transients. Our
framework offers a flexible, scalable, and generalizable solution for analyzing
complex, irregular event time series across scientific and industrial domains.

</details>


### [172] [Deep Generative Methods and Tire Architecture Design](https://arxiv.org/abs/2507.11639)
*Fouad Oubari,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: This study evaluates five generative models (VAE, GAN, multimodal VAE, DDPM, MDM) for industrial tire architecture generation across three industrial design scenarios using geometry-aware metrics, introducing a new categorical inpainting method. Diffusion models outperform others overall.


<details>
  <summary>Details</summary>
Motivation: To address the lack of guidance for selecting suitable generative models for complex manufacturing design tasks, specifically for industrial tire architectures.

Method: Five generative models were compared for unconditional generation, component-conditioned generation, and dimension-constrained generation. Metrics tailored for industrial needs were used, and a novel categorical inpainting technique for diffusion models was developed.

Result: Diffusion models showed the best overall performance, with MDM excelling in-distribution and DDPM better handling out-of-distribution constraints. The masking-trained VAE outperformed the multimodal variant (MMVAE+).

Conclusion: Diffusion models are the top choice for industrial design tasks, but specific strengths also exist for other models depending on the scenario.

Abstract: As deep generative models proliferate across the AI landscape, industrial
practitioners still face critical yet unanswered questions about which deep
generative models best suit complex manufacturing design tasks. This work
addresses this question through a complete study of five representative models
(Variational Autoencoder, Generative Adversarial Network, multimodal
Variational Autoencoder, Denoising Diffusion Probabilistic Model, and
Multinomial Diffusion Model) on industrial tire architecture generation. Our
evaluation spans three key industrial scenarios: (i) unconditional generation
of complete multi-component designs, (ii) component-conditioned generation
(reconstructing architectures from partial observations), and (iii)
dimension-constrained generation (creating designs that satisfy specific
dimensional requirements). To enable discrete diffusion models to handle
conditional scenarios, we introduce categorical inpainting, a mask-aware
reverse diffusion process that preserves known labels without requiring
additional training. Our evaluation employs geometry-aware metrics specifically
calibrated for industrial requirements, quantifying spatial coherence,
component interaction, structural connectivity, and perceptual fidelity. Our
findings reveal that diffusion models achieve the strongest overall
performance; a masking-trained VAE nonetheless outperforms the multimodal
variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics,
and within the diffusion family MDM leads in-distribution whereas DDPM
generalises better to out-of-distribution dimensional constraints.

</details>


### [173] [Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation](https://arxiv.org/abs/2507.11645)
*Ahmed Salah,David Yevick*

Main category: cs.LG

TL;DR: The paper introduces practical metrics to predict and understand grokking, a phenomenon of delayed generalization in neural networks.


<details>
  <summary>Details</summary>
Motivation: To identify and anticipate grokking behavior in neural networks, which reveals delayed generalization after high training accuracy is achieved.

Method: The study proposed metrics like Dropout Robustness Curve (DRC), test accuracy variance, neuron sparsity measures, and embedding similarity, analyzing their changes during grokking.

Result: The metrics showed patterns such as a local maximum in test accuracy variance during grokking and shifts in neuron activity and embedding distributions.

Conclusion: The proposed metrics are effective in forecasting grokking and offer insights into its origin and dynamics in neural networks.

Abstract: Grokking refers to delayed generalization in which the increase in test
accuracy of a neural network occurs appreciably after the improvement in
training accuracy This paper introduces several practical metrics including
variance under dropout, robustness, embedding similarity, and sparsity
measures, that can forecast grokking behavior. Specifically, the resilience of
neural networks to noise during inference is estimated from a Dropout
Robustness Curve (DRC) obtained from the variation of the accuracy with the
dropout rate as the model transitions from memorization to generalization. The
variance of the test accuracy under stochastic dropout across training
checkpoints further exhibits a local maximum during the grokking. Additionally,
the percentage of inactive neurons decreases during generalization, while the
embeddings tend to a bimodal distribution independent of initialization that
correlates with the observed cosine similarity patterns and dataset symmetries.
These metrics additionally provide valuable insight into the origin and
behaviour of grokking.

</details>


### [174] [Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning](https://arxiv.org/abs/2507.11732)
*Shiyu Chen,Cencheng Shen,Youngser Park,Carey E. Priebe*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) often suffer from limitations due to random initial features. This paper proposes a method to improve GNNs using one-hot Graph Encoder Embedding (GEE) for better feature initialization, achieving state-of-the-art results in both unsupervised and supervised tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the issue of suboptimal performance in GNNs caused by random or minimally informed feature initialization, which leads to slower convergence and reduced efficiency.

Method: The authors introduce one-hot Graph Encoder Embedding (GEE) to initialize high-quality node features, integrating it into GNNs to create a GEE-powered GNN (GG), along with an enhanced variant GG-C for node classification tasks.

Result: In node clustering, GG achieves state-of-the-art performance across all tested datasets and faster convergence. GG-C further boosts node classification accuracy by combining GG and GEE outputs, outperforming other methods.

Conclusion: Principled, structure-aware feature initialization using GEE enhances GNN performance, demonstrating improved accuracy and faster convergence in various graph learning tasks.

Abstract: Graph neural networks (GNNs) have emerged as a powerful framework for a wide
range of node-level graph learning tasks. However, their performance is often
constrained by reliance on random or minimally informed initial feature
representations, which can lead to slow convergence and suboptimal solutions.
In this paper, we leverage a statistically grounded method, one-hot graph
encoder embedding (GEE), to generate high-quality initial node features that
enhance the end-to-end training of GNNs. We refer to this integrated framework
as the GEE-powered GNN (GG), and demonstrate its effectiveness through
extensive simulations and real-world experiments across both unsupervised and
supervised settings. In node clustering, GG consistently achieves
state-of-the-art performance, ranking first across all evaluated real-world
datasets, while exhibiting faster convergence compared to the standard GNN. For
node classification, we further propose an enhanced variant, GG-C, which
concatenates the outputs of GG and GEE and outperforms competing baselines.
These results confirm the importance of principled, structure-aware feature
initialization in realizing the full potential of GNNs.

</details>


### [175] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Garth V. Crosby*

Main category: cs.LG

TL;DR: The paper introduces a Zero-Knowledge Proof (ZKP) based protocol for privacy-preserving evaluation in Federated Learning (FL) by proving local loss values are below a threshold without revealing exact metrics.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in the evaluation phase of Federated Learning, where shared performance metrics can inadvertently leak sensitive information.

Method: The authors designed a ZKP-based protocol to enable participants to prove their local model's loss is below a threshold value, which is implemented using self-contained modules that do not rely on external APIs. Experimental evaluations were conducted using CNNs on MNIST and MLPs on HAR datasets.

Result: The proposed method demonstrated feasibility in terms of computational overhead, communication cost, and verifiability during experiments using the specified datasets.

Conclusion: The approach successfully allows for privacy-preserving and verifiable evaluation in FL without exposing sensitive performance data, expanding the utility and security of FL frameworks.

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [176] [STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics](https://arxiv.org/abs/2507.11660)
*Joao F. Rocha,Ke Xu,Xingzhi Sun,Ananya Krishna,Dhananjay Bhaskar,Blanche Mongeon,Morgan Craig,Mark Gerstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: This paper introduces STAGED, a novel framework integrating agent-based modeling with deep learning to better understand cellular dynamics.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in traditional data-driven approaches and leverage spatial transcriptomics for studying complex, dynamic cellular interactions.

Method: The STAGED framework uses graph ODE networks with shared weights and an attention mechanism to model intercellular communication and intracellular gene regulation dynamics.

Result: STAGED learns dynamic cellular interaction strengths and captures cellular state changes and intercellular communication more adaptively and accurately.

Conclusion: STAGED provides an innovative method for modeling complex cell-cell interactions, improving our understanding of cellular dynamics through data-driven analysis of spatial transcriptomics.

Abstract: The advent of single-cell technology has significantly improved our
understanding of cellular states and subpopulations in various tissues under
normal and diseased conditions by employing data-driven approaches such as
clustering and trajectory inference. However, these methods consider cells as
independent data points of population distributions. With spatial
transcriptomics, we can represent cellular organization, along with dynamic
cell-cell interactions that lead to changes in cell state. Still, key
computational advances are necessary to enable the data-driven learning of such
complex interactive cellular dynamics. While agent-based modeling (ABM)
provides a powerful framework, traditional approaches rely on handcrafted rules
derived from domain knowledge rather than data-driven approaches. To address
this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)
integrating ABM with deep learning to model intercellular communication, and
its effect on the intracellular gene regulatory network. Using graph ODE
networks (GDEs) with shared weights per cell type, our approach represents
genes as vertices and interactions as directed edges, dynamically learning
their strengths through a designed attention mechanism. Trained to match
continuous trajectories of simulated as well as inferred trajectories from
spatial transcriptomics data, the model captures both intercellular and
intracellular interactions, enabling a more adaptive and accurate
representation of cellular dynamics.

</details>


### [177] [Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update](https://arxiv.org/abs/2507.11847)
*Yu-Jie Zhang,Sheng-An Xu,Peng Zhao,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper addresses the generalized linear bandit (GLB) problem, introducing an efficient algorithm with nearly optimal regret bound and low computational cost.


<details>
  <summary>Details</summary>
Motivation: The GLB problem involves modeling complex reward distributions beyond the classical linear framework, but existing methods struggle to balance computational and statistical efficiency.

Method: The authors develop a tight confidence set for the online mirror descent (OMD) estimator using mix loss analysis, achieving constant-time updates with high statistical efficiency.

Result: The proposed method achieves nearly optimal regret bounds while maintaining $
mathcal{O}(1)$ time and space complexities per round, outperforming traditional approaches in efficiency.

Conclusion: Through novel analysis, the algorithm bridges the computational-statistical efficiency gap for GLBs, offering practical utility in real-world applications of multi-armed bandits.

Abstract: We study the generalized linear bandit (GLB) problem, a contextual
multi-armed bandit framework that extends the classical linear model by
incorporating a non-linear link function, thereby modeling a broad class of
reward distributions such as Bernoulli and Poisson. While GLBs are widely
applicable to real-world scenarios, their non-linear nature introduces
significant challenges in achieving both computational and statistical
efficiency. Existing methods typically trade off between two objectives, either
incurring high per-round costs for optimal regret guarantees or compromising
statistical efficiency to enable constant-time updates. In this paper, we
propose a jointly efficient algorithm that attains a nearly optimal regret
bound with $\mathcal{O}(1)$ time and space complexities per round. The core of
our method is a tight confidence set for the online mirror descent (OMD)
estimator, which is derived through a novel analysis that leverages the notion
of mix loss from online prediction. The analysis shows that our OMD estimator,
even with its one-pass updates, achieves statistical efficiency comparable to
maximum likelihood estimation, thereby leading to a jointly efficient
optimistic method.

</details>


### [178] [Composing Linear Layers from Irreducibles](https://arxiv.org/abs/2507.11688)
*Travis Pence,Daisuke Yamada,Vikas Singh*

Main category: cs.LG

TL;DR: The paper explores the compositional structures in linear layers, utilizing geometric primitives (bivectors and rotors) to create efficient transformations and explaining their potential within deep models.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental building blocks in large models and investigate whether linear transformations can be synthesized using minimal geometric primitives.

Method: Using Clifford algebra, the authors express linear layers as compositions of bivectors and introduce a differentiable algorithm for decomposing them into rotors. This method reduces the parameter count from O(d^2) to O(log^2 d).

Result: Rotor-based layers are applied to attention mechanisms in LLMs, where they demonstrate comparable performance to established methods like block-Hadamard and low-rank approximations.

Conclusion: This approach offers an algebraic framework for understanding higher-level compositional structures in large models, opening avenues for theoretical exploration and efficiency improvements.

Abstract: Contemporary large models often exhibit behaviors suggesting the presence of
low-level primitives that compose into modules with richer functionality, but
these fundamental building blocks remain poorly understood. We investigate this
compositional structure in linear layers by asking: can we identify/synthesize
linear transformations from a minimal set of geometric primitives? Using
Clifford algebra, we show that linear layers can be expressed as compositions
of bivectors -- geometric objects encoding oriented planes -- and introduce a
differentiable algorithm that decomposes them into products of rotors. This
construction uses only O(log^2 d) parameters, versus O(d^2) required by dense
matrices. Applied to the key, query, and value projections in LLM attention
layers, our rotor-based layers match the performance of strong baselines such
as block-Hadamard and low-rank approximations. Our findings provide an
algebraic perspective on how these geometric primitives can compose into
higher-level functions within deep models.

</details>


### [179] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio,Pietro Marsella,Ben Pan,Simon Guo,Silas Alberti*

Main category: cs.LG

TL;DR: The paper introduces Kevin, an AI model for optimizing CUDA kernel generation using multi-turn RL, which greatly improves performance and correctness in kernel output.


<details>
  <summary>Details</summary>
Motivation: Writing efficient GPU kernels is challenging and iterative, making it suitable for leverages like Reinforcement Learning (RL) to improve performance with verifiable rewards.

Method: The study develops Kevin - a multi-turn RL framework tailored for CUDA kernel generation by addressing real-world challenges like reward attribution and learning from long trajectories.

Result: Kevin achieves significant improvements in generating CUDA kernels, increasing correctness from 56% to 82%, and mean speedup from 0.53x to 1.10x compared to baseline methods.

Conclusion: Kevin demonstrates substantial gains in CUDA kernel optimization and outputs, with better results through iterative refinement, offering a viable solution for real-world GPU kernel development.

Abstract: Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [180] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Main category: cs.LG

TL;DR: Coreset selection methods can reduce training data while retaining performance, but their impact on dataset biases affecting model robustness needs scrutiny.


<details>
  <summary>Details</summary>
Motivation: To analyze how reducing dataset size with coreset methods influences biases and model robustness.

Method: Experimental analysis across multiple benchmarks, metrics, policies, and coreset sizes to explore sample difficulty and bias impacts.

Result: Embedding-based selection methods pose less risk of amplifying biases, but prioritizing difficult samples may not ensure robustness.

Conclusion: Coreset selection can lower bias but doesn't consistently lead to robust downstream models.

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [181] [Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption](https://arxiv.org/abs/2507.11702)
*Hein de Wilde,Ali Mohammed Mansoor Alsahag,Pierre Blanchet*

Main category: cs.LG

TL;DR: The paper addresses leaf-fall disruptions to UK rail operations costing £300M annually, by proposing an LSTM-based leaf-fall prediction model leveraging satellite data for improved scheduling of mitigation measures.


<details>
  <summary>Details</summary>
Motivation: Leaf-fall disruptions severely impact UK rail networks, costing millions of pounds annually, requiring better systems to anticipate leaf-fall timings for optimal mitigation scheduling.

Method: An LSTM network was trained using ground-truth leaf-fall data alongside multispectral and meteorological satellite data for scalable and reliable leaf-fall predictions.

Result: The system demonstrated errors of 6.32 days for predicting the start and 9.31 days for predicting the end of leaf-fall, significantly improving upon previous methodologies.

Conclusion: The model provides scalable and reliable leaf-fall timing predictions, promising optimization of mitigation schedules for railways and contributing to ecological insights.

Abstract: Railroad traffic disruption as a result of leaf-fall cost the UK rail
industry over 300 million per year and measures to mitigate such disruptions
are employed on a large scale, with 1.67 million kilometers of track being
treated in the UK in 2021 alone. Therefore, the ability to anticipate the
timing of leaf-fall would offer substantial benefits for rail network
operators, enabling the efficient scheduling of such mitigation measures.
However, current methodologies for predicting leaf-fall exhibit considerable
limitations in terms of scalability and reliability. This study endeavors to
devise a prediction system that leverages specialized prediction methods and
the latest satellite data sources to generate both scalable and reliable
insights into leaf-fall timings. An LSTM network trained on ground-truth
leaf-falling data combined with multispectral and meteorological satellite data
demonstrated a root-mean-square error of 6.32 days for predicting the start of
leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which
improves upon previous work on the topic, offers promising opportunities for
the optimization of leaf mitigation measures in the railway industry and the
improvement of our understanding of complex ecological systems.

</details>


### [182] [Robust Causal Discovery in Real-World Time Series with Power-Laws](https://arxiv.org/abs/2507.12257)
*Matteo Tusoni,Giuseppe Masi,Andrea Coletta,Aldo Glielmo,Viviana Arrigoni,Novella Bartolini*

Main category: cs.LG

TL;DR: The paper proposes a robust method for causal discovery leveraging power-law spectral features, achieving superior performance on synthetic and real-world data.


<details>
  <summary>Details</summary>
Motivation: Causal discovery in stochastic time series is sensitive to noise, leading to misleading inferences in real-world applications.

Method: The authors use power-law spectral features to develop a robust causal discovery method that amplifies genuine causal signals.

Result: Compared to existing algorithms, the proposed method showed improved robustness and performance on synthetic and real-world datasets with known causal structures.

Conclusion: Incorporating power-law spectral features can significantly enhance robustness and reliability in causal discovery of time series data.

Abstract: Exploring causal relationships in stochastic time series is a challenging yet
crucial task with a vast range of applications, including finance, economics,
neuroscience, and climate science. Many algorithms for Causal Discovery (CD)
have been proposed, but they often exhibit a high sensitivity to noise,
resulting in misleading causal inferences when applied to real data. In this
paper, we observe that the frequency spectra of typical real-world time series
follow a power-law distribution, notably due to an inherent self-organizing
behavior. Leveraging this insight, we build a robust CD method based on the
extraction of power -law spectral features that amplify genuine causal signals.
Our method consistently outperforms state-of-the-art alternatives on both
synthetic benchmarks and real-world datasets with known causal structures,
demonstrating its robustness and practical relevance.

</details>


### [183] [Subgraph Generation for Generalizing on Out-of-Distribution Links](https://arxiv.org/abs/2507.11710)
*Jay Revolinsky,Harry Shomer,Jiliang Tang*

Main category: cs.LG

TL;DR: The paper introduces FLEX, a graph generative model (GGM) framework designed to improve link prediction performance in out-of-distribution (OOD) scenarios through structurally-conditioned graph generation and adversarial co-training.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in graph-based learning, where existing graph neural networks (GNNs) excel at link prediction but assume all data samples come from the same distribution, limiting their applicability in OOD scenarios.

Method: FLEX utilizes two key mechanisms: structurally-conditioned graph generation and adversarial co-training between a graph auto-encoder and a GNN to ensure structural alignment between distributions.

Result: Experiments on both synthetic and real-world OOD settings demonstrate that FLEX significantly enhances link prediction performance and provides insights into how graph data augmentation affects link structures.

Conclusion: FLEX shows the potential to generalize link prediction tasks to OOD scenarios without requiring expert knowledge or any prior adjustments, positioning it as a flexible and robust framework for various applications.

Abstract: Graphs Neural Networks (GNNs) demonstrate high-performance on the link
prediction (LP) task. However, these models often rely on all dataset samples
being drawn from the same distribution. In addition, graph generative models
(GGMs) show a pronounced ability to generate novel output graphs. Despite this,
GGM applications remain largely limited to domain-specific tasks. To bridge
this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)
structurally-conditioned graph generation, and (2) adversarial co-training
between an auto-encoder and GNN. As such, FLEX ensures structural-alignment
between sample distributions to enhance link-prediction performance in
out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert
knowledge to function in different OOD scenarios. Numerous experiments are
conducted in synthetic and real-world OOD settings to demonstrate FLEX's
performance-enhancing ability, with further analysis for understanding the
effects of graph data augmentation on link structures. The source code is
available here: https://github.com/revolins/FlexOOD.

</details>


### [184] [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](https://arxiv.org/abs/2507.12262)
*Zachary James,Joseph Guinness*

Main category: cs.LG

TL;DR: This paper introduces a Gaussian process framework leveraging nonstationary kernels and neural networks to enhance model expressiveness and scalability, achieving higher accuracy on various datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of stationary kernels in Gaussian processes, which struggle to represent complex datasets due to limited expressiveness.

Method: The paper proposes coupling Gaussian processes with nonstationary kernels whose parameters are modeled as outputs of a neural network, trained jointly using gradient-based methods.

Result: The proposed method outperforms stationary and hierarchical models on machine learning datasets in accuracy and log-score metrics. It also effectively recovers nonstationary parameters in spatial datasets.

Conclusion: The approach provides a flexible, accurate, and adaptable framework for Gaussian process modeling with nonstationary kernels, demonstrating wide applicability and improved predictive performance.

Abstract: Gaussian processes have become a popular tool for nonparametric regression
because of their flexibility and uncertainty quantification. However, they
often use stationary kernels, which limit the expressiveness of the model and
may be unsuitable for many datasets. We propose a framework that uses
nonstationary kernels whose parameters vary across the feature space, modeling
these parameters as the output of a neural network that takes the features as
input. The neural network and Gaussian process are trained jointly using the
chain rule to calculate derivatives. Our method clearly describes the behavior
of the nonstationary parameters and is compatible with approximation methods
for scaling to large datasets. It is flexible and easily adapts to different
nonstationary kernels without needing to redesign the optimization procedure.
Our methods are implemented with the GPyTorch library and can be readily
modified. We test a nonstationary variance and noise variant of our method on
several machine learning datasets and find that it achieves better accuracy and
log-score than both a stationary model and a hierarchical model approximated
with variational inference. Similar results are observed for a model with only
nonstationary variance. We also demonstrate our approach's ability to recover
the nonstationary parameters of a spatial dataset.

</details>


### [185] [Globalization for Scalable Short-term Load Forecasting](https://arxiv.org/abs/2507.11729)
*Amirhossein Ahmadi,Hamidreza Zareipour,Henry Leung*

Main category: cs.LG

TL;DR: The paper focuses on improving load forecasting in power networks using global forecasting models (GFMs) to address challenges like generalizability, data drift, and scalability. It introduces clustering techniques to enhance prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional local forecasting models struggle with issues such as overfitting, scalability, and handling data drift in load forecasting. This limitation drives the exploration of more generalized and robust approaches for forecasting.

Method: The study explores global forecasting models, feature-transforming and target-transforming approaches, and introduces time series clustering methods to address data heterogeneity and balance global and local dynamics.

Result: Results reveal that global target-transforming models outperform local models, especially when enhanced with clustering techniques, while global feature-transforming models face challenges requiring effective clustering management.

Conclusion: Global forecasting models, especially with target-transformation and clustering techniques, significantly improve load forecasting. Proper management of data heterogeneity is essential to balance global and local dynamics.

Abstract: Forecasting load in power transmission networks is essential across various
hierarchical levels, from the system level down to individual points of
delivery (PoD). While intuitive and locally accurate, traditional local
forecasting models (LFMs) face significant limitations, particularly in
handling generalizability, overfitting, data drift, and the cold start problem.
These methods also struggle with scalability, becoming computationally
expensive and less efficient as the network's size and data volume grow. In
contrast, global forecasting models (GFMs) offer a new approach to enhance
prediction generalizability, scalability, accuracy, and robustness through
globalization and cross-learning. This paper investigates global load
forecasting in the presence of data drifts, highlighting the impact of
different modeling techniques and data heterogeneity. We explore
feature-transforming and target-transforming models, demonstrating how
globalization, data heterogeneity, and data drift affect each differently. In
addition, we examine the role of globalization in peak load forecasting and its
potential for hierarchical forecasting. To address data heterogeneity and the
balance between globality and locality, we propose separate time series
clustering (TSC) methods, introducing model-based TSC for feature-transforming
models and new weighted instance-based TSC for target-transforming models.
Through extensive experiments on a real-world dataset of Alberta's electricity
load, we demonstrate that global target-transforming models consistently
outperform their local counterparts, especially when enriched with global
features and clustering techniques. In contrast, global feature-transforming
models face challenges in balancing local and global dynamics, often requiring
TSC to manage data heterogeneity effectively.

</details>


### [186] [ROC-n-reroll: How verifier imperfection affects test-time scaling](https://arxiv.org/abs/2507.12399)
*Florian E. Dorner,Yatong Chen,André F. Cruz,Fanny Yang*

Main category: cs.LG

TL;DR: This paper explores test-time scaling for language models and investigates how verifier imperfections impact performance, backed by a theoretical understanding using ROC curve geometry.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding regarding how verifier imperfections affect test-time scaling techniques like Best-of-N (BoN) and rejection sampling.

Method: The paper analyzes these methods by proving how their instance-level accuracy is influenced by the geometric properties of the verifier's ROC curve. Theoretical results are validated on GSM8K datasets using various Llama and Qwen models.

Result: The analysis shows that rejection sampling performance is influenced by local ROC curve geometry while BoN relies on global properties. Both methods converge to the same accuracy when infinite compute is available.

Conclusion: Verifier imperfections significantly influence test-time scaling methods, and their scaling behavior is governed by specific ROC curve properties. Both methods achieve the same accuracy in extreme compute scenarios.

Abstract: Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.

</details>


### [187] [Sparse Identification of Nonlinear Dynamics with Conformal Prediction](https://arxiv.org/abs/2507.11739)
*Urban Fasel*

Main category: cs.LG

TL;DR: The paper presents an enhanced uncertainty quantification method for Sparse Identification of Nonlinear Dynamics (SINDy) using Conformal Prediction.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification in SINDy models is crucial for reliability, especially in safety-critical applications.

Method: The method integrates Conformal Prediction with Ensemble-SINDy to provide prediction intervals, model selection based on feature importance, and uncertainty quantification for model coefficients.

Result: The approach effectively achieves target coverage for predictions, quantifies feature importance, and improves robustness in coefficient uncertainty even in non-Gaussian noise conditions.

Conclusion: Conformal prediction enhances SINDy by providing reliable uncertainty quantification, making it more robust for chaotic and stochastic dynamics modeling.

Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for
discovering nonlinear dynamical system models from data. Quantifying
uncertainty in SINDy models is essential for assessing their reliability,
particularly in safety-critical applications. While various uncertainty
quantification methods exist for SINDy, including Bayesian and ensemble
approaches, this work explores the integration of Conformal Prediction, a
framework that can provide valid prediction intervals with coverage guarantees
based on minimal assumptions like data exchangeability. We introduce three
applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)
quantifying uncertainty in time series prediction, (2) model selection based on
library feature importance, and (3) quantifying the uncertainty of identified
model coefficients using feature conformal prediction. We demonstrate the three
applications on stochastic predator-prey dynamics and several chaotic dynamical
systems. We show that conformal prediction methods integrated with E-SINDy can
reliably achieve desired target coverage for time series forecasting,
effectively quantify feature importance, and produce more robust uncertainty
intervals for model coefficients, even under non-Gaussian noise, compared to
standard E-SINDy coefficient estimates.

</details>


### [188] [A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction](https://arxiv.org/abs/2507.11757)
*Yuehua Song,Yong Gao*

Main category: cs.LG

TL;DR: The paper introduces a novel Graph Neural Network-based model (GiG) that combines transductive and inductive learning paradigms to improve drug-target interaction prediction, demonstrating superior performance over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Accurate drug-target interaction prediction is crucial for enhancing drug discovery and target validation processes, but existing machine learning models struggle to integrate diverse features effectively.

Method: The paper proposes the Graph-in-Graph (GiG) model, which uses meta-nodes to represent drug and target molecular structures within a drug-target interaction graph. This approach combines transductive and inductive learning to better explore interaction data.

Result: Experimental evaluation shows that the GiG model outperforms current methods across multiple metrics, validating the advantages of its design and feature integration capabilities.

Conclusion: The GiG model provides a more effective framework for drug-target interaction prediction by leveraging diverse features at both molecular and network levels, demonstrating potential for advancing drug discovery techniques.

Abstract: Accurately predicting drug-target interactions (DTIs) is pivotal for
advancing drug discovery and target validation techniques. While machine
learning approaches including those that are based on Graph Neural Networks
(GNN) have achieved notable success in DTI prediction, many of them have
difficulties in effectively integrating the diverse features of drugs, targets
and their interactions. To address this limitation, we introduce a novel
framework to take advantage of the power of both transductive learning and
inductive learning so that features at molecular level and drug-target
interaction network level can be exploited. Within this framework is a
GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and
target molecular structures as meta-nodes in a drug-target interaction graph,
enabling a detailed exploration of their intricate relationships. To evaluate
the proposed model, we have compiled a special benchmark comprising drug
SMILES, protein sequences, and their interaction data, which is interesting in
its own right. Our experimental results demonstrate that the GiG model
significantly outperforms existing approaches across all evaluation metrics,
highlighting the benefits of integrating different learning paradigms and
interaction data.

</details>


### [189] [Torsional-GFN: a conditional conformation generator for small molecules](https://arxiv.org/abs/2507.11759)
*Alexandra Volokhova,Léna Néhale Ezzine,Piotr Gaiński,Luca Scimeca,Emmanuel Bengio,Prudencio Tossou,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: The paper introduces Torsional-GFN, a generative model for sampling molecular conformations proportionally to their Boltzmann distribution with promising results in scalability and generalization.


<details>
  <summary>Details</summary>
Motivation: There is a need for stable molecular conformation generation in drug discovery to improve efficiency in sampling from the Boltzmann distribution, as existing methods like molecular dynamics can be computationally expensive.

Method: The authors propose Torsional-GFN, a conditional generative flow network that uses a reward function to train and sample torsion angle rotations conditioned on a molecule's graph structure and local properties.

Result: The model successfully samples conformations proportional to the Boltzmann distribution, works for multiple molecules with one model, and generalizes to unseen bond lengths and angles derived from molecular dynamics simulations.

Conclusion: Torsional-GFN provides a scalable and effective approach to molecular conformation generation, with potential applications in larger systems and unseen molecules while incorporating local structure generation.

Abstract: Generating stable molecular conformations is crucial in several drug
discovery applications, such as estimating the binding affinity of a molecule
to a target. Recently, generative machine learning methods have emerged as a
promising, more efficient method than molecular dynamics for sampling of
conformations from the Boltzmann distribution. In this paper, we introduce
Torsional-GFN, a conditional GFlowNet specifically designed to sample
conformations of molecules proportionally to their Boltzmann distribution,
using only a reward function as training signal. Conditioned on a molecular
graph and its local structure (bond lengths and angles), Torsional-GFN samples
rotations of its torsion angles. Our results demonstrate that Torsional-GFN is
able to sample conformations approximately proportional to the Boltzmann
distribution for multiple molecules with a single model, and allows for
zero-shot generalization to unseen bond lengths and angles coming from the MD
simulations for such molecules. Our work presents a promising avenue for
scaling the proposed approach to larger molecular systems, achieving zero-shot
generalization to unseen molecules, and including the generation of the local
structure into the GFlowNet model.

</details>


### [190] [Scaling laws for activation steering with Llama 2 models and refusal mechanisms](https://arxiv.org/abs/2507.11771)
*Sheikh Abdur Raheem Ali,Justin Xu,Ivory Yang,Jasmine Xinze Li,Ayse Arslan,Clark Benham*

Main category: cs.LG

TL;DR: The paper explores the effectiveness of Contrastive Activation Addition (CAA) on Llama 2 models, finding that its efficacy varies by layer and decreases with model size.


<details>
  <summary>Details</summary>
Motivation: With large language models improving, there is uncertainty around the efficacy of less widely deployed techniques like CAA for controlling model outputs. This study investigates how CAA performs at different scales of Llama 2 models.

Method: The authors use contrastive pairs to identify directions in the model's residual stream vector space, which are added during the forward pass. They test CAA's impact on the models' response behaviors using answer-matching questions.

Result: The study finds that CAA is most effective at early-mid layers and its effectiveness diminishes with increasing model size. Negative steering has a stronger effect than positive steering across all models.

Conclusion: This research highlights the limitations of CAA's scalability and provides insights into optimizing its use across different layers and model sizes.

Abstract: As large language models (LLMs) evolve in complexity and capability, the
efficacy of less widely deployed alignment techniques are uncertain. Building
on previous work on activation steering and contrastive activation addition
(CAA), this paper explores the effectiveness of CAA with model scale using the
family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable
'directions' in the model's residual stream vector space using contrastive
pairs (for example, hate to love) and adding this direction to the residual
stream during the forward pass. It directly manipulates the residual stream and
aims to extract features from language models to better control their outputs.
Using answer matching questions centered around the refusal behavior, we found
that 1) CAA is most effective when applied at early-mid layers. 2) The
effectiveness of CAA diminishes with model size. 3) Negative steering has more
pronounced effects than positive steering across all model sizes.

</details>


### [191] [Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network](https://arxiv.org/abs/2507.11776)
*Merel Kampere,Ali Mohammed Mansoor Alsahag*

Main category: cs.LG

TL;DR: This paper predicts delays in the Dutch railway network using an XGBoost Classifier and network features, concluding that existing approaches show limited predictive performance.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address the lack of research in predicting delays by analyzing broader network-wide patterns, which are critical for mitigating ripple effects in the busy Dutch railway network.

Method: The paper adapts a methodology originally applied to the US air network, integrates Node Centrality Measures, and compares classifiers including XGBoost, RandomForest, DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression.

Result: The study finds limited predictive performance, especially in scenarios where testing does not occur simultaneously with training data, indicating the need for context-specific model improvements.

Conclusion: While current models have limitations, this research advances understanding of delay prediction in transportation networks and lays a foundation for developing more effective future models.

Abstract: The Dutch railway network is one of the busiest in the world, with delays
being a prominent concern for the principal passenger railway operator NS. This
research addresses a gap in delay prediction studies within the Dutch railway
network by employing an XGBoost Classifier with a focus on topological
features. Current research predominantly emphasizes short-term predictions and
neglects the broader network-wide patterns essential for mitigating ripple
effects. This research implements and improves an existing methodology,
originally designed to forecast the evolution of the fast-changing US air
network, to predict delays in the Dutch Railways. By integrating Node
Centrality Measures and comparing multiple classifiers like RandomForest,
DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is
to predict delayed trajectories. However, the results reveal limited
performance, especially in non-simultaneous testing scenarios, suggesting the
necessity for more context-specific adaptations. Regardless, this research
contributes to the understanding of transportation network evaluation and
proposes future directions for developing more robust predictive models for
delays.

</details>


### [192] [Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation](https://arxiv.org/abs/2507.11789)
*Alessandro Palma,Sergei Rybakov,Leon Hetzel,Stephan Günnemann,Fabian J. Theis*

Main category: cs.LG

TL;DR: FlatVI is a novel training framework that optimizes latent space geometry in deep generative models for single-cell RNA sequencing, ensuring geodesic paths align better with Euclidean geometry.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation where linear interpolations in the latent space of variational autoencoders often fail to align with geodesic paths on the data manifold, especially problematic in modeling cellular state transitions.

Method: FlatVI regularizes the latent manifold of discrete-likelihood variational autoencoders, encouraging straight lines in latent space to align more closely with geodesic interpolations on single-cell RNA manifolds.

Result: Experiments on synthetic and single-cell RNA sequencing data show that FlatVI improves trajectory reconstruction and manifold interpolation, demonstrating theoretical soundness and practical enhancement.

Conclusion: FlatVI enhances compatibility of latent space structures with downstream applications that assume Euclidean geometry, offering a more robust framework for modeling cellular transitions in single-cell RNA sequencing.

Abstract: Latent space interpolations are a powerful tool for navigating deep
generative models in applied settings. An example is single-cell RNA
sequencing, where existing methods model cellular state transitions as latent
space interpolations with variational autoencoders, often assuming linear
shifts and Euclidean geometry. However, unless explicitly enforced, linear
interpolations in the latent space may not correspond to geodesic paths on the
data manifold, limiting methods that assume Euclidean geometry in the data
representations. We introduce FlatVI, a novel training framework that
regularises the latent manifold of discrete-likelihood variational autoencoders
towards Euclidean geometry, specifically tailored for modelling single-cell
count data. By encouraging straight lines in the latent space to approximate
geodesic interpolations on the decoded single-cell manifold, FlatVI enhances
compatibility with downstream approaches that assume Euclidean latent geometry.
Experiments on synthetic data support the theoretical soundness of our
approach, while applications to time-resolved single-cell RNA sequencing data
demonstrate improved trajectory reconstruction and manifold interpolation.

</details>


### [193] [CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels](https://arxiv.org/abs/2507.11807)
*Ruofan Hu,Dongyu Zhang,Huayi Zhang,Elke Rundensteiner*

Main category: cs.LG

TL;DR: The paper introduces CLID-MU, a meta-learning approach that operates without relying on clean labeled datasets for noisy label scenarios. It achieves improved performance by analyzing the consistency of data structures across different feature layers.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the dependency on clean labeled meta-datasets in existing noisy label training methods, which is challenging to satisfy in practice.

Method: CLID-MU evaluates model performance based on the alignment of data structures across the last hidden and final layers, using this alignment to guide model training without clean labels.

Result: Experiments on various benchmark datasets show that CLID-MU outperforms existing state-of-the-art methods under both synthetic and real-world noisy label conditions.

Conclusion: CLID-MU provides an effective solution for meta-learning in noisy label scenarios, eliminating the need for clean labeled datasets and achieving better results in diverse noise conditions.

Abstract: Learning with noisy labels (LNL) is essential for training deep neural
networks with imperfect data. Meta-learning approaches have achieved success by
using a clean unbiased labeled set to train a robust model. However, this
approach heavily depends on the availability of a clean labeled meta-dataset,
which is difficult to obtain in practice. In this work, we thus tackle the
challenge of meta-learning for noisy label scenarios without relying on a clean
labeled dataset. Our approach leverages the data itself while bypassing the
need for labels. Building on the insight that clean samples effectively
preserve the consistency of related data structures across the last hidden and
the final layer, whereas noisy samples disrupt this consistency, we design the
Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU).
CLID-MU leverages the alignment of data structures across these diverse feature
spaces to evaluate model performance and use this alignment to guide training.
Experiments on benchmark datasets with varying amounts of labels under both
synthetic and real-world noise demonstrate that CLID-MU outperforms
state-of-the-art methods. The code is released at
https://github.com/ruofanhu/CLID-MU.

</details>


### [194] [SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling](https://arxiv.org/abs/2507.11818)
*Andrei Rekesh,Miruna Cretu,Dmytro Shevchuk,Vignesh Ram Somnath,Pietro Liò,Robert A. Batey,Mike Tyers,Michał Koziarski,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: The paper introduces SynCoGen, a framework for generating synthesizable 3D molecules based on masked graph diffusion and flow matching, achieving state-of-the-art performance across different generative tasks for small molecules.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring synthesizability in generative small molecule design, especially considering geometry-based conditional generation beyond 2D molecular graph representations.

Method: The proposed SynCoGen utilizes masked graph diffusion and flow matching to sample molecular building blocks, chemical reactions, and atomic coordinates. Training is supported by SynSpace, a dataset with 600K synthesis-aware building block graphs and 3.3M conformers.

Result: SynCoGen shows state-of-the-art results in generating graphs and conformers for small molecules and performs competitively in protein ligand generation tasks for drug discovery.

Conclusion: SynCoGen offers a robust foundation for applications in non-autoregressive molecular generation, such as analog expansion, lead optimization, and structure conditioning, addressing key challenges in molecule design.

Abstract: Ensuring synthesizability in generative small molecule design remains a major
challenge. While recent developments in synthesizable molecule generation have
demonstrated promising results, these efforts have been largely confined to 2D
molecular graph representations, limiting the ability to perform geometry-based
conditional generation. In this work, we present SynCoGen (Synthesizable
Co-Generation), a single framework that combines simultaneous masked graph
diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen
samples from the joint distribution of molecular building blocks, chemical
reactions, and atomic coordinates. To train the model, we curated SynSpace, a
dataset containing over 600K synthesis-aware building block graphs and 3.3M
conformers. SynCoGen achieves state-of-the-art performance in unconditional
small molecule graph and conformer generation, and the model delivers
competitive performance in zero-shot molecular linker design for protein ligand
generation in drug discovery. Overall, this multimodal formulation represents a
foundation for future applications enabled by non-autoregressive molecular
generation, including analog expansion, lead optimization, and direct structure
conditioning.

</details>


### [195] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: MNIST-Gen introduces an adaptive framework for creating custom MNIST-style datasets tailored to specific research applications, reducing manual workload significantly.


<details>
  <summary>Details</summary>
Motivation: Standard datasets like MNIST are inadequate for specialized applications such as tree or food classification. Creating custom datasets is often too resource-intensive for many researchers.

Method: The system employs CLIP-based semantic analysis, reinforcement learning, and human feedback, complemented by hierarchical semantic categorization and category-theory-inspired modularity.

Result: MNIST-Gen produces novel datasets (Tree-MNIST and Food-MNIST) with 85% automatic categorization accuracy and 80% time savings compared to manual effort.

Conclusion: MNIST-Gen is effective in generating domain-specific datasets, offering researchers a practical, intelligent tool to streamline dataset creation for specialized tasks.

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [196] [HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction](https://arxiv.org/abs/2507.11836)
*Jian Gao,Jianshe Wu,JingYi Ding*

Main category: cs.LG

TL;DR: HyperEvent reframes dynamic link prediction as hyper-event recognition, outperforming state-of-the-art methods on multiple datasets with enhanced accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to account for the structural cohesion of related events within complex dynamic graphs.

Method: HyperEvent dynamically constructs association sequences using event correlation vectors to evaluate hyper-events and predicts query events accordingly. It uses a parallel training algorithm for scalability.

Result: HyperEvent surpasses state-of-the-art on 4 out of 5 datasets, improving accuracy and efficiency, achieving a 6.95% improvement in MRR on the Flight dataset while reducing training time.

Conclusion: HyperEvent offers an effective and scalable solution for dynamic link prediction by capturing hyper-event structures in dynamic graphs.

Abstract: Dynamic link prediction in continuous-time dynamic graphs is a fundamental
task for modeling evolving complex systems. Existing node-centric and
event-centric methods focus on individual interactions or atomic states,
failing to capture the structural cohesion of composite hyper-events, groups of
causally related events. To address this, we propose HyperEvent, a framework
reframing dynamic link prediction as hyper-event recognition. Central to
HyperEvent is the dynamic construction of an association sequence using event
correlation vectors. These vectors quantify pairwise dependencies between the
query event and relevant historical events, thereby characterizing the
structural cohesion of a potential hyper-event. The framework predicts the
occurrence of the query event by evaluating whether it collectively forms a
valid hyper-event with these historical events. Notably, HyperEvent outperforms
state-of-the-art methods on 4 out of 5 datasets in the official leaderboard.
For scalability, we further introduce an efficient parallel training algorithm
that segments large event streams to enable concurrent training. Experiments
validate HyperEvent's superior accuracy and efficiency on large-scale graphs.
Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank
over state-of-the-art baseline on the large-scale Flight dataset while
utilizing only 10.17% of the training time.

</details>


### [197] [Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](https://arxiv.org/abs/2507.11839)
*Chengyue Gong,Xinshi Chen,Yuxuan Zhang,Yuxuan Song,Hao Zhou,Wenzhi Xiao*

Main category: cs.LG

TL;DR: Protenix-Mini offers a compact solution to streamline protein structure prediction through architectural design and sampling process refinements, achieving faster computations and reducing complexity with minimal accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency and heavy computational demands in biomolecular structure prediction by balancing model accuracy and efficiency.

Method: Protenix-Mini optimizes the structure prediction pipeline by replacing multi-step samplers with a two-step ODE approach, pruning unimportant architectural components, and using an ESM module to reduce preprocessing time.

Result: Protenix-Mini reduces model complexity and computational overhead while maintaining high prediction accuracy, with only a negligible 1-5% drop in performance compared to its full-scale counterpart.

Conclusion: Protenix-Mini is a highly efficient model for protein structure prediction, favoring applications where computational constraints and accuracy are both critical.

Abstract: Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion transformer blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
pruning and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.

</details>


### [198] [Online Training and Pruning of Deep Reinforcement Learning Networks](https://arxiv.org/abs/2507.11975)
*Valentin Frank Ingmar Guenter,Athanasios Sideris*

Main category: cs.LG

TL;DR: This paper explores applying neural network pruning techniques to reinforcement learning, introducing a method called XiNet to improve computational efficiency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Scaling deep reinforcement learning networks enhances performance but imposes high computational and memory requirements, highlighting the need for efficient pruning methods tailored for RL.

Method: The authors introduce XiNet, which integrates neural network pruning and training in RL algorithms using a stochastic optimization approach with variational Bernoulli distributions to prune inactive units.

Result: Experimental results on MuJoCo benchmarks and Soft Actor-Critic RL agents demonstrate that the proposed method enables significant network pruning with negligible performance loss.

Conclusion: Pruning during training results in more efficient and better-performing RL agents, outperforming results achieved by training smaller networks from scratch.

Abstract: Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms
has been shown to enhance performance when feature extraction networks are used
but the gained performance comes at the significant expense of increased
computational and memory complexity. Neural network pruning methods have
successfully addressed this challenge in supervised learning. However, their
application to RL is underexplored. We propose an approach to integrate
simultaneous training and pruning within advanced RL methods, in particular to
RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our
networks (XiNet) are trained to solve stochastic optimization problems over the
RL networks' weights and the parameters of variational Bernoulli distributions
for 0/1 Random Variables $\xi$ scaling each unit in the networks. The
stochastic problem formulation induces regularization terms that promote
convergence of the variational parameters to 0 when a unit contributes little
to the performance. In this case, the corresponding structure is rendered
permanently inactive and pruned from its network. We propose a cost-aware,
sparsity-promoting regularization scheme, tailored to the DenseNet architecture
of OFENets expressing the parameter complexity of involved networks in terms of
the parameters of the RVs in these networks. Then, when matching this cost with
the regularization terms, the many hyperparameters associated with them are
automatically selected, effectively combining the RL objectives and network
compression. We evaluate our method on continuous control benchmarks (MuJoCo)
and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned
considerably with minimal loss in performance. Furthermore, our results confirm
that pruning large networks during training produces more efficient and higher
performing RL agents rather than training smaller networks from scratch.

</details>


### [199] [OrdShap: Feature Position Importance for Sequential Black-Box Models](https://arxiv.org/abs/2507.11855)
*Davin Hill,Brian L. Hill,Aria Masoomi,Vijay S. Nori,Robert E. Tillman,Jennifer Dy*

Main category: cs.LG

TL;DR: The paper introduces OrdShap, a novel model interpretability method that isolates feature value and position effects in sequential data predictions.


<details>
  <summary>Details</summary>
Motivation: Existing feature attribution methods for sequential models fail to disentangle the effects of feature values and their sequence positions, leading to limited interpretability.

Method: OrdShap leverages a game-theoretic framework (connected to Sanchez-Bergantiños values) to isolate and quantify how feature position impacts model predictions.

Result: Experiments on health, natural language, and synthetic data demonstrate OrdShap's ability to attribute feature values and positions effectively, enhancing understanding of model behavior.

Conclusion: OrdShap offers a theoretically sound and practical tool for understanding sequential models by addressing limitations in traditional attribution methods.

Abstract: Sequential deep learning models excel in domains with temporal or sequential
dependencies, but their complexity necessitates post-hoc feature attribution
methods for understanding their predictions. While existing techniques quantify
feature importance, they inherently assume fixed feature ordering - conflating
the effects of (1) feature values and (2) their positions within input
sequences. To address this gap, we introduce OrdShap, a novel attribution
method that disentangles these effects by quantifying how a model's predictions
change in response to permuting feature position. We establish a game-theoretic
connection between OrdShap and Sanchez-Berganti\~nos values, providing a
theoretically grounded approach to position-sensitive attribution. Empirical
results from health, natural language, and synthetic datasets highlight
OrdShap's effectiveness in capturing feature value and feature position
attributions, and provide deeper insight into model behavior.

</details>


### [200] [A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers](https://arxiv.org/abs/2507.11865)
*Hanwen Dai,Chang Gao,Fang He,Congyuan Ji,Yanni Yang*

Main category: cs.LG

TL;DR: This paper develops a novel deep learning framework called pi-DDPG to optimize driver participation in a Discount Express service for ride-hailing platforms, addressing issues of high stochasticity, matching opacity, and limited data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to solve the challenge of optimizing driver participation in Discount Express services on ride-hailing integrations while handling limited historical data and obtaining reliable early-stage performance in dynamic and uncertain conditions.

Method: The proposed method, pi-DDPG, is a policy-improved deep reinforcement learning framework. It features a refiner module for enhanced early-stage performance, a convolutional LSTM model to capture spatiotemporal dynamics, and prioritized experience replay for efficient learning.

Result: The pi-DDPG framework outperforms alternatives in terms of learning efficiency and reduces training losses during the early stages, as shown through experiments using a simulator based on real-world datasets.

Conclusion: The pi-DDPG framework is effective in dynamically managing driver participation in Discount Express services, improving both learning performance and early-stage reliability for ride-hailing platforms.

Abstract: The rapid expansion of platform integration has emerged as an effective
solution to mitigate market fragmentation by consolidating multiple
ride-hailing platforms into a single application. To address heterogeneous
passenger preferences, third-party integrators provide Discount Express service
delivered by express drivers at lower trip fares. For the individual platform,
encouraging broader participation of drivers in Discount Express services has
the potential to expand the accessible demand pool and improve matching
efficiency, but often at the cost of reduced profit margins. This study aims to
dynamically manage drivers' acceptance of Discount Express from the perspective
of individual platforms. The lack of historical data under the new business
model necessitates online learning. However, early-stage exploration through
trial and error can be costly in practice, highlighting the need for reliable
early-stage performance in real-world deployment. To address these challenges,
this study formulates the decision regarding the proportion of drivers'
acceptance behavior as a continuous control task. In response to the high
stochasticity, the opaque matching mechanisms employed by third-party
integrator, and the limited availability of historical data, we propose a
policy-improved deep deterministic policy gradient (pi-DDPG) framework. The
proposed framework incorporates a refiner module to boost policy performance
during the early training phase, leverages a convolutional long short-term
memory network to effectively capture complex spatiotemporal patterns, and
adopts a prioritized experience replay mechanism to enhance learning
efficiency. A simulator based on a real-world dataset is developed to validate
the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate
that pi-DDPG achieves superior learning efficiency and significantly reduces
early-stage training losses.

</details>


### [201] [Imbalanced Regression Pipeline Recommendation](https://arxiv.org/abs/2507.11901)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: This paper introduces the Meta-IR framework, which uses meta-classifiers to recommend optimal pipelines for imbalanced regression problems, outperforming AutoML frameworks and other baseline configurations.


<details>
  <summary>Details</summary>
Motivation: Imbalanced data, while well-studied in classification, presents challenges for regression due to the rarity of target values, creating a need for effective solutions in preprocessing and learning model selection.

Method: The Meta-IR framework trains meta-classifiers to recommend the best combination of resampling strategy and learning model using meta-features. Two formulations, Independent and Chained, are proposed; the Chained approach models the relationship between resampling and learning algorithms.

Result: The Chained formulation showed superior performance, demonstrating that intrinsic relationships between resampling methods and learning models can be leveraged. Meta-IR also outperformed AutoML frameworks, as well as 42 combinations of six learning and seven resampling methods.

Conclusion: Meta-IR is an effective zero-shot meta-learning framework for handling imbalanced regression problems, demonstrating better performance compared to competitors and baselines.

Abstract: Imbalanced problems are prevalent in various real-world scenarios and are
extensively explored in classification tasks. However, they also present
challenges for regression tasks due to the rarity of certain target values. A
common alternative is to employ balancing algorithms in preprocessing to
address dataset imbalance. However, due to the variety of resampling methods
and learning models, determining the optimal solution requires testing many
combinations. Furthermore, the learning model, dataset, and evaluation metric
affect the best strategies. This work proposes the Meta-learning for Imbalanced
Regression (Meta-IR) framework, which diverges from existing literature by
training meta-classifiers to recommend the best pipeline composed of the
resampling strategy and learning model per task in a zero-shot fashion. The
meta-classifiers are trained using a set of meta-features to learn how to map
the meta-features to the classes indicating the best pipeline. We propose two
formulations: Independent and Chained. Independent trains the meta-classifiers
to separately indicate the best learning algorithm and resampling strategy.
Chained involves a sequential procedure where the output of one meta-classifier
is used as input for another to model intrinsic relationship factors. The
Chained scenario showed superior performance, suggesting a relationship between
the learning algorithm and the resampling strategy per task. Compared with
AutoML frameworks, Meta-IR obtained better results. Moreover, compared with
baselines of six learning algorithms and six resampling algorithms plus no
resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of
them. The code, data, and further information of the experiments can be found
on GitHub: https://github.com/JusciAvelino/Meta-IR.

</details>


### [202] [Resampling strategies for imbalanced regression: a survey and empirical analysis](https://arxiv.org/abs/2507.11902)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: This paper addresses the issues of imbalanced data in regression tasks by conducting an extensive experimental study and proposing a taxonomy for balancing approaches.


<details>
  <summary>Details</summary>
Motivation: To solve the underexplored problem of imbalanced data in regression tasks, which significantly affects predictive performance.

Method: An experimental study involving balancing and predictive models with evaluation metrics specifically tailored for imbalanced regression contexts. A taxonomy was also developed based on regression models, learning processes, and evaluation metrics.

Result: The study provides detailed insights into the effects of imbalanced regression strategies, showcasing their benefits for model learning and proposing directions for future research.

Conclusion: Balancing strategies bring significant advantages to predictive models in regression problems, and further exploration in this field is recommended. The study's resources are made open-source for community use.

Abstract: Imbalanced problems can arise in different real-world situations, and to
address this, certain strategies in the form of resampling or balancing
algorithms are proposed. This issue has largely been studied in the context of
classification, and yet, the same problem features in regression tasks, where
target values are continuous. This work presents an extensive experimental
study comprising various balancing and predictive models, and wich uses metrics
to capture important elements for the user and to evaluate the predictive model
in an imbalanced regression data context. It also proposes a taxonomy for
imbalanced regression approaches based on three crucial criteria: regression
model, learning process, and evaluation metrics. The study offers new insights
into the use of such strategies, highlighting the advantages they bring to each
model's learning process, and indicating directions for further studies. The
code, data and further information related to the experiments performed herein
can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.

</details>


### [203] [From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning](https://arxiv.org/abs/2507.11926)
*Max Hopkins,Sihan Liu,Christopher Ye,Yuichi Yoshida*

Main category: cs.LG

TL;DR: This paper addresses the challenge of replicable exploration in reinforcement learning (RL). It proposes an algorithm that achieves replicability and efficiency with a sample complexity of $\tilde{O}(S^2A)$. This bridges the gap between generative and episodic environments, disproving the notion that exploration is inherently expensive for replicable RL.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the replicability crisis across empirical sciences and RL. There is a need to develop replicable and data-efficient RL algorithms, especially in settings where agents must explore shifting environments.

Method: The authors propose a replicable RL algorithm tailored for low-horizon tabular Markov Decision Processes (MDPs). They prove a sample complexity of $\tilde{O}(S^2A)$, and also establish matching lower bounds under the generative and episodic settings.

Result: The algorithm achieves replicable learning in RL with $\tilde{O}(S^2A)$ samples, outperforming prior approaches that required significantly higher samples without generative models.

Conclusion: Replicable exploration in RL is not inherently expensive, as previously conjectured. Sample-efficient replicable RL is indeed feasible, and the proposed algorithm is nearly optimal with respect to state-space complexity.

Abstract: The epidemic failure of replicability across empirical science and machine
learning has recently motivated the formal study of replicable learning
algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from
a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the
design of data-efficient replicable algorithms is now more or less understood.
In contrast, there remain significant gaps in our knowledge for control
settings like reinforcement learning where an agent must interact directly with
a shifting environment. Karbasi et. al show that with access to a generative
model of an environment with $S$ states and $A$ actions (the RL 'batch
setting'), replicably learning a near-optimal policy costs only
$\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a
generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the
substantial difficulty of environment exploration. This gap raises a key
question in the broader theory of replicability: Is replicable exploration
inherently more expensive than batch learning? Is sample-efficient replicable
RL even possible?
  In this work, we (nearly) resolve this problem (for low-horizon tabular
MDPs): exploration is not a significant barrier to replicable learning! Our
main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging
the gap between the generative and episodic settings. We complement this with a
matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under
the common parallel sampling assumption) and an unconditional lower bound in
the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of
our algorithm with respect to the state space $S$.

</details>


### [204] [Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning](https://arxiv.org/abs/2507.11928)
*Abhishek Sriram,Neal Tuffy*

Main category: cs.LG

TL;DR: The paper introduces a machine learning-accelerated RF power amplifier design optimization framework that cuts down simulation requirements by 65% while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the need for reducing computational and simulation time in RF power amplifier design while ensuring target accuracy.

Method: Combines MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to reduce simulation points and predict performance using harmonic balance simulations.

Result: Achieved a 65% reduction in simulation workload with high predictive accuracy (average R² of 0.901) for 15 PA operating modes and significant time savings (58.24%-77.78%).

Conclusion: The framework enables rapid and efficient RF PA design iterations, maintaining accuracy while significantly reducing simulation requirements and time through intelligent parameter space exploration.

Abstract: This paper presents a machine learning-accelerated optimization framework for
RF power amplifier design that reduces simulation requirements by 65% while
maintaining $\pm0.3$ to $\pm0.4$ dBm accuracy. The proposed method combines
MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to
intelligently explore multidimensional parameter spaces. Instead of
exhaustively simulating all parameter combinations to achieve target P2dB
compression specifications, our approach strategically selects approximately
35% of critical simulation points. The framework processes ADS netlists,
executes harmonic balance simulations on the reduced dataset, and trains a
CatBoost model to predict P2dB performance across the entire design space.
Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with
the system ranking parameter combinations by their likelihood of meeting target
specifications. The integrated solution delivers 58.24% to 77.78% reduction in
simulation time through automated GUI-based workflows, enabling rapid design
iterations without compromising accuracy standards required for production RF
circuits.

</details>


### [205] [Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection](https://arxiv.org/abs/2507.11997)
*Tairan Huang,Yili Wang*

Main category: cs.LG

TL;DR: The paper introduces MLED, a framework combining Large Language Models (LLMs) and Graph Neural Networks (GNNs) to enhance graph-based fraud detection.


<details>
  <summary>Details</summary>
Motivation: Current graph fraud detection methods often neglect the semantic richness in textual data, and multimodal fusion of text embeddings and graph structures is challenging.

Method: The authors propose MLED, which leverages LLMs to extract external knowledge from text and integrates this with graph structures using type-level and relation-level enhancements.

Result: MLED achieves state-of-the-art performance on four real-world datasets and can be applied to existing graph fraud detection methods.

Conclusion: MLED effectively combines LLMs and GNNs for graph fraud detection, highlighting the value of integrating textual and graph-based data.

Abstract: Graph fraud detection has garnered significant attention as Graph Neural
Networks (GNNs) have proven effective in modeling complex relationships within
multimodal data. However, existing graph fraud detection methods typically use
preprocessed node embeddings and predefined graph structures to reveal
fraudsters, which ignore the rich semantic cues contained in raw textual
information. Although Large Language Models (LLMs) exhibit powerful
capabilities in processing textual information, it remains a significant
challenge to perform multimodal fusion of processed textual embeddings with
graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM
\textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In
MLED, we utilize LLMs to extract external knowledge from textual information to
enhance graph fraud detection methods. To integrate LLMs with graph structure
information and enhance the ability to distinguish fraudsters, we design a
multi-level LLM enhanced framework including type-level enhancer and
relation-level enhancer. One is to enhance the difference between the
fraudsters and the benign entities, the other is to enhance the importance of
the fraudsters in different relations. The experiments on four real-world
datasets show that MLED achieves state-of-the-art performance in graph fraud
detection as a generalized framework that can be applied to existing methods.

</details>


### [206] [Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing](https://arxiv.org/abs/2507.12002)
*Alice Zhang,Callihan Bertley,Dawei Liang,Edison Thomaz*

Main category: cs.LG

TL;DR: The paper introduces a computational method to identify in-person verbal conversations using audio and inertial data from smartwatches, achieving high accuracy in both controlled and semi-naturalistic studies.


<details>
  <summary>Details</summary>
Motivation: Understanding and detecting human social interactions, particularly verbal conversations, can facilitate advancements in behavior analysis and societal insights. Existing methods face challenges in acoustically-challenging scenarios, which this approach aims to address using wearable devices.

Method: The authors leveraged audio and inertial data from smartwatches and applied machine learning and deep learning models. They employed three fusion methods to combine the data, integrating verbal cues and non-verbal gestures. They tested their method in both lab and semi-naturalistic studies.

Result: The framework achieved a macro F1-score of 82.0±3.0% under lab conditions and 77.2±1.8% in semi-naturalistic settings. Evaluations across activities and sampling rates highlighted the effectiveness of multimodal sensing.

Conclusion: Fusing audio and inertial data provides a robust solution to detect verbal conversations, even in acoustically-challenging environments. This method highlights the importance of multimodal sensing for comprehensive social interaction analysis.

Abstract: Social interactions play a crucial role in shaping human behavior,
relationships, and societies. It encompasses various forms of communication,
such as verbal conversation, non-verbal gestures, facial expressions, and body
language. In this work, we develop a novel computational approach to detect a
foundational aspect of human social interactions, in-person verbal
conversations, by leveraging audio and inertial data captured with a commodity
smartwatch in acoustically-challenging scenarios. To evaluate our approach, we
conducted a lab study with 11 participants and a semi-naturalistic study with
24 participants. We analyzed machine learning and deep learning models with 3
different fusion methods, showing the advantages of fusing audio and inertial
data to consider not only verbal cues but also non-verbal gestures in
conversations. Furthermore, we perform a comprehensive set of evaluations
across activities and sampling rates to demonstrate the benefits of multimodal
sensing in specific contexts. Overall, our framework achieved 82.0$\pm$3.0%
macro F1-score when detecting conversations in the lab and 77.2$\pm$1.8% in the
semi-naturalistic setting.

</details>


### [207] [DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning](https://arxiv.org/abs/2507.12011)
*Yao Lu,Hongyu Gao,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: The paper presents a method named DUSE to address the lack of labeled data in automatic modulation recognition (AMR) by leveraging uncertainty scoring and active learning for effective sample expansion.


<details>
  <summary>Details</summary>
Motivation: AMR models require significant labeled data for training, but practical scenarios often lack sufficient annotated data due to high manual annotation costs and limited data availability.

Method: The DUSE framework uses uncertainty scoring to filter valuable samples from relevant datasets and refines the scoring function through an active learning strategy.

Result: Extensive experiments show that DUSE consistently surpasses eight baseline methods in coreset selection under both balanced and imbalanced class conditions, and it generalizes well to architectures it hasn't seen before.

Conclusion: DUSE is an effective data expansion framework addressing data scarcity problems in AMR, demonstrating superior performance and cross-model compatibility.

Abstract: Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.

</details>


### [208] [Granular feedback merits sophisticated aggregation](https://arxiv.org/abs/2507.12041)
*Anmol Kagrecha,Henrik Marklund,Potsawee Manakul,Richard Zeckhauser,Benjamin Van Roy*

Main category: cs.LG

TL;DR: The paper investigates whether sophisticated methods can better estimate a population’s distribution of feedback compared to regularized averaging, especially as feedback becomes more granular.


<details>
  <summary>Details</summary>
Motivation: There is a need to accurately estimate a population's feedback distribution, especially when collecting feedback from smaller groups due to cost constraints. Granular feedback is key for improving informativeness.

Method: The paper compares regularized averaging methods to more sophisticated computational techniques in predicting feedback distributions at varying levels of granularity, using empirical data on social attitudes.

Result: It finds that for granular (e.g., five-point) feedback, sophisticated methods significantly outperform regularized averaging, needing only about half as many individuals to achieve similar performance.

Conclusion: Sophisticated methods are more beneficial as feedback becomes granular, allowing for more accurate population predictions with fewer individuals compared to traditional methods.

Abstract: Human feedback is increasingly used across diverse applications like training
AI models, developing recommender systems, and measuring public opinion -- with
granular feedback often being preferred over binary feedback for its greater
informativeness. While it is easy to accurately estimate a population's
distribution of feedback given feedback from a large number of individuals,
cost constraints typically necessitate using smaller groups. A simple method to
approximate the population distribution is regularized averaging: compute the
empirical distribution and regularize it toward a prior. Can we do better? As
we will discuss, the answer to this question depends on feedback granularity.
  Suppose one wants to predict a population's distribution of feedback using
feedback from a limited number of individuals. We show that, as feedback
granularity increases, one can substantially improve upon predictions of
regularized averaging by combining individuals' feedback in ways more
sophisticated than regularized averaging.
  Our empirical analysis using questions on social attitudes confirms this
pattern. In particular, with binary feedback, sophistication barely reduces the
number of individuals required to attain a fixed level of performance. By
contrast, with five-point feedback, sophisticated methods match the performance
of regularized averaging with about half as many individuals.

</details>


### [209] [Information-Theoretic Generalization Bounds of Replay-based Continual Learning](https://arxiv.org/abs/2507.12043)
*Wen Wen,Tieliang Gong,Yunjiao Zhang,Zeyu Gao,Weizhan Zhang,Yong-Jin Liu*

Main category: cs.LG

TL;DR: The paper establishes a theoretical framework for replay-based continual learning (CL), exploring generalization behavior and offering information-theoretic insights that improve understanding and empirical outcomes.


<details>
  <summary>Details</summary>
Motivation: To address the limited theoretical understanding of the generalization behavior of replay-based CL methods while they avoid catastrophic forgetting.

Method: The paper derives information-theoretic bounds to analyze the interactions of memory buffers and current task data, and employs stochastic gradient Langevin dynamics as an illustrative example.

Result: The derived bounds effectively capture generalization dynamics, demonstrating the empirical benefits of using limited exemplars for improving generalization and mitigating forgetting.

Conclusion: Replay-based CL benefits from theoretical guarantees, and using a limited replay approach can achieve better trade-offs between generalization and avoiding catastrophic forgetting.

Abstract: Continual learning (CL) has emerged as a dominant paradigm for acquiring
knowledge from sequential tasks while avoiding catastrophic forgetting.
Although many CL methods have been proposed to show impressive empirical
performance, the theoretical understanding of their generalization behavior
remains limited, particularly for replay-based approaches. In this paper, we
establish a unified theoretical framework for replay-based CL, deriving a
series of information-theoretic bounds that explicitly characterize how the
memory buffer interacts with the current task to affect generalization.
Specifically, our hypothesis-based bounds reveal that utilizing the limited
exemplars of previous tasks alongside the current task data, rather than
exhaustive replay, facilitates improved generalization while effectively
mitigating catastrophic forgetting. Furthermore, our prediction-based bounds
yield tighter and computationally tractable upper bounds of the generalization
gap through the use of low-dimensional variables. Our analysis is general and
broadly applicable to a wide range of learning algorithms, exemplified by
stochastic gradient Langevin dynamics (SGLD) as a representative method.
Comprehensive experimental evaluations demonstrate the effectiveness of our
derived bounds in capturing the generalization dynamics in replay-based CL
settings.

</details>


### [210] [FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling](https://arxiv.org/abs/2507.12053)
*Seanglidet Yean,Jiazu Zhou,Bu-Sung Lee,Markus Schläpfer*

Main category: cs.LG

TL;DR: The paper introduces a cGAN-based approach for generating human mobility flows, integrating dynamic urban parameters and minimal reliance on calibration data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current models in capturing evolving factors like population density and land use, crucial for urban planning and transportation optimization.

Method: The study utilizes conditional generative adversarial networks (cGANs) to merge historical data with adaptive urban parameters such as dynamic region sizes and land use archetypes.

Result: The model's efficacy is validated using mobile phone data from Singapore, showcasing improved performance compared to existing methods.

Conclusion: The proposed approach efficiently generates precise mobility flows with adjustable granularity, demonstrating potential for sustainable urban planning without extensive calibration data.

Abstract: The mobility patterns of people in cities evolve alongside changes in land
use and population. This makes it crucial for urban planners to simulate and
analyze human mobility patterns for purposes such as transportation
optimization and sustainable urban development. Existing generative models
borrowed from machine learning rely heavily on historical trajectories and
often overlook evolving factors like changes in population density and land
use. Mechanistic approaches incorporate population density and facility
distribution but assume static scenarios, limiting their utility for future
projections where historical data for calibration is unavailable. This study
introduces a novel, data-driven approach for generating origin-destination
mobility flows tailored to simulated urban scenarios. Our method leverages
adaptive factors such as dynamic region sizes and land use archetypes, and it
utilizes conditional generative adversarial networks (cGANs) to blend
historical data with these adaptive parameters. The approach facilitates rapid
mobility flow generation with adjustable spatial granularity based on regions
of interest, without requiring extensive calibration data or complex behavior
modeling. The promising performance of our approach is demonstrated by its
application to mobile phone data from Singapore, and by its comparison with
existing methods.

</details>


### [211] [Emergence of Quantised Representations Isolated to Anisotropic Functions](https://arxiv.org/abs/2507.12070)
*George Bird*

Main category: cs.LG

TL;DR: This paper introduces a method to study representation alignment and discovers that algebraic symmetries in network primitives affect the structure of representations. Discrete activation functions lead to discrete representations, while continuous ones maintain continuity.


<details>
  <summary>Details</summary>
Motivation: Current approaches lack tools to comprehensively analyze how functional form choices impact the representational structure and alignment in neural models. This paper aims to address this gap.

Method: A new approach based on the Spotlight Resonance method is developed to investigate representational alignment. An ablation study involving variations in activation functions explores their impact on representational forms.

Result: Discrete algebraic permutation-equivariant activation functions induce discretized representations, while continuous algebraic orthogonal-equivariant ones maintain continuous structures. These findings show how functional forms introduce unintended inductive biases, including representation quantization.

Conclusion: Functional form choices in neural networks significantly influence representations, and this mechanism might underpin various interpretability phenomena. These insights could help design better tools and models, preventing detrimental representation collapse.

Abstract: This paper describes a novel methodology for determining representational
alignment, developed upon the existing Spotlight Resonance method. Using this,
it is found that algebraic symmetries of network primitives are a strong
predictor for task-agnostic structure in representations. Particularly, this
new tool is used to gain insight into how discrete representations can form and
arrange in autoencoder models, through an ablation study where only the
activation function is altered. Representations are found to tend to discretise
when the activation functions are defined through a discrete algebraic
permutation-equivariant symmetry. In contrast, they remain continuous under a
continuous algebraic orthogonal-equivariant definition. These findings
corroborate the hypothesis that functional form choices can carry unintended
inductive biases which produce task-independent artefactual structures in
representations, particularly that contemporary forms induce discretisation of
otherwise continuous structure -- a quantisation effect. Moreover, this
supports a general causal model for one mode in which discrete representations
may form, and could constitute a prerequisite for downstream interpretability
phenomena, including grandmother neurons, discrete coding schemes, general
linear features and possibly Superposition. Hence, this tool and proposed
mechanism for the influence of functional form on representations may provide
several insights into emergent interpretability research. Finally, preliminary
results indicate that quantisation of representations appears to correlate with
a measurable increase in reconstruction error, reinforcing previous conjectures
that this collapse can be detrimental.

</details>


### [212] [Measuring Informativeness Gap of (Mis)Calibrated Predictors](https://arxiv.org/abs/2507.12094)
*Yiding Feng,Wei Tang*

Main category: cs.LG

TL;DR: This paper introduces the concept of informativeness gap to compare predictive models' usefulness in decision-making tasks, generalizing and extending prior notions and providing a relaxed version of Earth Mover’s Distance to measure informativeness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of comparing the usefulness of potentially miscalibrated predictors in downstream decision tasks, especially in cases beyond perfect calibration.

Method: The paper defines the informativeness gap between predictors and develops a relaxed Earth Mover's Distance (EMD) inspired measure to quantify informativeness. It provides theoretical guidelines, dual characterizations, and models for efficient estimation.

Result: The informativeness measure satisfies soundness and completeness, ensuring its theoretical validity. It can be practically estimated with prediction-only access and provides novel findings for comparing calibrated predictors.

Conclusion: The informativeness gap and the resulting measure form a robust framework for understanding and comparing predictors’ usefulness in decision-making, generalizing existing measures and improving their flexibility and applicability.

Abstract: In many applications, decision-makers must choose between multiple predictive
models that may all be miscalibrated. Which model (i.e., predictor) is more
"useful" in downstream decision tasks? To answer this, our first contribution
introduces the notion of the informativeness gap between any two predictors,
defined as the maximum normalized payoff advantage one predictor offers over
the other across all decision-making tasks. Our framework strictly generalizes
several existing notions: it subsumes U-Calibration [KLST-23] and Calibration
Decision Loss [HW-24], which compare a miscalibrated predictor to its
calibrated counterpart, and it recovers Blackwell informativeness [Bla-51,
Bla-53] as a special case when both predictors are perfectly calibrated. Our
second contribution is a dual characterization of the informativeness gap,
which gives rise to a natural informativeness measure that can be viewed as a
relaxed variant of the earth mover's distance (EMD) between two prediction
distributions. We show that this measure satisfies natural desiderata: it is
complete and sound, and it can be estimated sample-efficiently in the
prediction-only access setting. Along the way, we also obtain novel
combinatorial structural results when applying this measure to perfectly
calibrated predictors.

</details>


### [213] [Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks](https://arxiv.org/abs/2507.12127)
*Ngoc Duy Pham,Thusitha Dayaratne,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.LG

TL;DR: This paper explores federated learning for spectrum sensing, addressing challenges like data scarcity and security vulnerabilities, and proposes novel solutions with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The increasing number of wireless devices and limited spectrum availability drive the need for dynamic spectrum allocation solutions. Centralized machine learning faces privacy and regulatory challenges, making distributed methods like federated learning more appealing.

Method: The paper introduces a semi-supervised federated learning method to handle unlabeled data and proposes a vaccination-inspired defense mechanism to mitigate data poisoning attacks in federated learning-based spectrum sensing.

Result: The proposed solutions achieve near-perfect accuracy on unlabeled data and effectively defend against data poisoning attacks, even when a large number of participants are malicious.

Conclusion: This work demonstrates the effectiveness of federated learning for improving spectrum sensing while addressing data scarcity and security issues, paving the way for robust distributed spectrum allocation systems.

Abstract: Advancements in wireless and mobile technologies, including 5G advanced and
the envisioned 6G, are driving exponential growth in wireless devices. However,
this rapid expansion exacerbates spectrum scarcity, posing a critical
challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and
dynamically sharing spectrum--has emerged as an essential solution to address
this issue. While machine learning (ML) models hold significant potential for
improving spectrum sensing, their adoption in centralized ML-based DSA systems
is limited by privacy concerns, bandwidth constraints, and regulatory
challenges. To overcome these limitations, distributed ML-based approaches such
as Federated Learning (FL) offer promising alternatives. This work addresses
two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of
labeled data for training FL models in practical spectrum sensing scenarios is
tackled with a semi-supervised FL approach, combined with energy detection,
enabling model training on unlabeled datasets. Second, we examine the security
vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our
analysis highlights the shortcomings of existing majority-based defenses in
countering such attacks. To address these vulnerabilities, we propose a novel
defense mechanism inspired by vaccination, which effectively mitigates data
poisoning attacks without relying on majority-based assumptions. Extensive
experiments on both synthetic and real-world datasets validate our solutions,
demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets
and maintain Byzantine robustness against both targeted and untargeted data
poisoning attacks, even when a significant proportion of participants are
malicious.

</details>


### [214] [HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD](https://arxiv.org/abs/2507.12133)
*Hanwen Liu,Yuhe Huang,Yifeng Gong,Yanjie Zhai,Jiaxuan Lu*

Main category: cs.LG

TL;DR: The paper introduces HyDRA, a hybrid RF architecture for device recognition in wireless security, achieving high accuracy and efficient real-time performance.


<details>
  <summary>Details</summary>
Motivation: Enhance wireless communication security through device recognition by addressing limitations of existing methods with a robust, adaptable non-cryptographic approach.

Method: HyDRA integrates Optimized Variational Mode Decomposition (fixed center frequencies & closed-form solutions) with fusion architectures combining CNNs, Transformers, and Mamba components.

Result: Tests on public datasets show SOTA accuracy in closed-set tasks and robust open-set performance while achieving low power, millisecond-level inference on NVIDIA Jetson Xavier NX.

Conclusion: HyDRA offers a practical, efficient solution for enhancing wireless authentication with high performance in both open and closed-set classifications.

Abstract: Device recognition is vital for security in wireless communication systems,
particularly for applications like access control. Radio Frequency Fingerprint
Identification (RFFI) offers a non-cryptographic solution by exploiting
hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid
Dual-mode RF Architecture that integrates an optimized Variational Mode
Decomposition (VMD) with a novel architecture based on the fusion of
Convolutional Neural Networks (CNNs), Transformers, and Mamba components,
designed to support both closed-set and open-set classification tasks. The
optimized VMD enhances preprocessing efficiency and classification accuracy by
fixing center frequencies and using closed-form solutions. HyDRA employs the
Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and
the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting
to varying conditions. Evaluation on public datasets demonstrates
state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance
in our proposed open-set classification method, effectively identifying
unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves
millisecond-level inference speed with low power consumption, providing a
practical solution for real-time wireless authentication in real-world
environments.

</details>


### [215] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: This paper introduces RiemannLoRA, a novel framework for parameter-efficient fine-tuning of large language models that improves convergence speed and final performance by treating LoRA matrices as elements on a smooth manifold.


<details>
  <summary>Details</summary>
Motivation: Challenges in Low-Rank Adaptation (LoRA) include overparameterization of matrix factorization and the lack of optimal initialization strategies during fine-tuning of large language models.

Method: The proposed approach views fixed-rank LoRA matrices as a smooth manifold and uses Riemannian optimization techniques to address overparameterization and provide initialization, leveraging numerical linear algebra best practices.

Result: RiemannLoRA consistently enhances both performance metrics and convergence speed in experimental setups with large language models and diffusion model architectures.

Conclusion: The framework offers a computationally efficient and numerically stable solution that advances the state-of-the-art in fine-tuning techniques for large-scale machine learning models.

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [216] [FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale](https://arxiv.org/abs/2507.12144)
*Boris Bonev,Thorsten Kurth,Ankur Mahesh,Mauro Bisson,Jean Kossaifi,Karthik Kashinath,Anima Anandkumar,William D. Collins,Michael S. Pritchard,Alexander Keller*

Main category: cs.LG

TL;DR: FourCastNet 3 is a geometric machine learning approach for global weather modeling, offering faster and more accurate probabilistic forecasting than traditional methods while maintaining stability and scalability.


<details>
  <summary>Details</summary>
Motivation: To improve meteorological forecasting and early warning systems through scalable, efficient, and highly accurate probabilistic ensemble predictions on global weather models.

Method: Implemented a convolutional neural network architecture tailored for spherical geometry, leveraging novel training paradigms for model- and data-parallelism on large-scale GPUs, enabling fast and accurate weather forecasts.

Result: FourCastNet 3 outperforms conventional and diffusion-based methods in accuracy, is up to 60 times faster, retains realistic spectra even for 60-day lead times, and generates 90-day global forecasts in under 20 seconds on a single GPU.

Conclusion: The approach demonstrates robust computational efficiency, medium-range probabilistic forecasting skill, and stability at subseasonal timescales, making it a strong candidate for enhancing meteorological forecasting and early warning systems.

Abstract: FourCastNet 3 advances global weather modeling by implementing a scalable,
geometric machine learning (ML) approach to probabilistic ensemble forecasting.
The approach is designed to respect spherical geometry and to accurately model
the spatially correlated probabilistic nature of the problem, resulting in
stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble
models and rivals the best diffusion-based methods, while producing forecasts 8
to 60 times faster than these approaches. In contrast to other ML approaches,
FourCastNet 3 demonstrates excellent probabilistic calibration and retains
realistic spectra, even at extended lead times of up to 60 days. All of these
advances are realized using a purely convolutional neural network architecture
tailored for spherical geometry. Scalable and efficient large-scale training on
1024 GPUs and more is enabled by a novel training paradigm for combined model-
and data-parallelism, inspired by domain decomposition methods in classical
numerical models. Additionally, FourCastNet 3 enables rapid inference on a
single GPU, producing a 90-day global forecast at 0.25{\deg}, 6-hourly
resolution in under 20 seconds. Its computational efficiency, medium-range
probabilistic skill, spectral fidelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting
and early warning systems through large ensemble predictions.

</details>


### [217] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Main category: cs.LG

TL;DR: PRISM is proposed to enhance edge deployment of foundation models by reducing communication overhead and computation, achieving significant efficiency gains with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Foundation models face challenges being deployed on edge devices due to computational and communication constraints, necessitating efficient and practical methods for edge adaptation.

Method: PRISM uses Segment Means for communication-efficient feature representation, restructures self-attention mechanisms, and incorporates partition-aware causal masking for distributed Transformer inference on edge devices.

Result: PRISM yielded up to 99.2% reduction in communication overhead and 51.24% reduction in computation for BERT on evaluated datasets, with minimal accuracy degradation.

Conclusion: PRISM provides a scalable, compute-efficient solution for deploying foundation models on resource-limited distributed environments.

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [218] [Multi-Component VAE with Gaussian Markov Random Field](https://arxiv.org/abs/2507.12165)
*Fouad Oubari,Mohamed El-Baha,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: This paper introduces the Gaussian Markov Random Field Multi-Component Variational AutoEncoder (GMRF MCVAE) to improve the coherence and representation of multi-component datasets in generative modeling.


<details>
  <summary>Details</summary>
Motivation: Current generative models for multi-component datasets often ignore complex interdependencies, leading to compromised structural coherence in outputs. This gap necessitates a framework for better cross-component relationship modeling.

Method: The paper integrates Gaussian Markov Random Fields into the prior and posterior distributions of a Multi-Component Variational AutoEncoder, specifically catering to cross-component interactions in generative tasks.

Result: GMRF MCVAE achieves state-of-the-art results on the synthetic Copula dataset, shows competitive performance on the PolyMNIST benchmark, and enhances structural coherence on the BIKED dataset.

Conclusion: GMRF MCVAE effectively models intricate cross-component relationships, making it well-suited for applications requiring robust and realistic multi-component dataset coherence.

Abstract: Multi-component datasets with intricate dependencies, like industrial
assemblies or multi-modal imaging, challenge current generative modeling
techniques. Existing Multi-component Variational AutoEncoders typically rely on
simplified aggregation strategies, neglecting critical nuances and consequently
compromising structural coherence across generated components. To explicitly
address this gap, we introduce the Gaussian Markov Random Field Multi-Component
Variational AutoEncoder , a novel generative framework embedding Gaussian
Markov Random Fields into both prior and posterior distributions. This design
choice explicitly models cross-component relationships, enabling richer
representation and faithful reproduction of complex interactions. Empirically,
our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula
dataset specifically constructed to evaluate intricate component relationships,
demonstrates competitive results on the PolyMNIST benchmark, and significantly
enhances structural coherence on the real-world BIKED dataset. Our results
indicate that the GMRF MCVAE is especially suited for practical applications
demanding robust and realistic modeling of multi-component coherence

</details>


### [219] [RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication](https://arxiv.org/abs/2507.12166)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Junting Chen,Zezhong Zhang,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: The paper introduces UrbanRadio3D, a 3D radio map dataset offering higher resolution and richer dimensions compared to existing datasets, along with RadioDiff-3D, a diffusion-model framework for more accurate 3D radio map construction.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing radio map methodologies, which often neglect key spatial and temporal parameters and fail to generalize due to static learning paradigms.

Method: UrbanRadio3D is constructed using ray tracing in urban environments, forming a massively scalable 3D RM dataset. The paper also proposes a U-Net with 3D convolutional operators and a diffusion-model-based framework, RadioDiff-3D.

Result: UrbanRadio3D dataset is over 37 times larger with height variations and richer metrics (Pathloss, DoA, ToA) than existing datasets. RadioDiff-3D achieves high construction accuracy in various environmental conditions.

Conclusion: The study provides a robust dataset and benchmark for advancing 3D environment-aware wireless communication solutions by addressing existing limitations.

Abstract: Radio maps (RMs) serve as a critical foundation for enabling
environment-aware wireless communication, as they provide the spatial
distribution of wireless channel characteristics. Despite recent progress in RM
construction using data-driven approaches, most existing methods focus solely
on pathloss prediction in a fixed 2D plane, neglecting key parameters such as
direction of arrival (DoA), time of arrival (ToA), and vertical spatial
variations. Such a limitation is primarily due to the reliance on static
learning paradigms, which hinder generalization beyond the training data
distribution. To address these challenges, we propose UrbanRadio3D, a
large-scale, high-resolution 3D RM dataset constructed via ray tracing in
realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than
previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,
forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than
prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet
with 3D convolutional operators is proposed. Moreover, we further introduce
RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D
convolutional architecture. RadioDiff-3D supports both radiation-aware
scenarios with known transmitter locations and radiation-unaware settings based
on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate
that RadioDiff-3D achieves superior performance in constructing rich,
high-dimensional radio maps under diverse environmental dynamics. This work
provides a foundational dataset and benchmark for future research in 3D
environment-aware communication. The dataset is available at
https://github.com/UNIC-Lab/UrbanRadio3D.

</details>


### [220] [Explainable Evidential Clustering](https://arxiv.org/abs/2507.12192)
*Victor F. Lopes de Souza,Karima Bakhti,Sofiane Ramdani,Denis Mottet,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: The paper introduces the Iterative Evidential Mistake Minimization (IEMM) algorithm to explain evidential clustering results and validates its effectiveness with up to 93% satisfactory explanations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of explaining evidential clustering results, which is crucial in high-stakes domains such as healthcare, given that traditional clustering methods struggle with uncertainty and imprecision.

Method: The study introduces representativity as a foundation for decision tree-based abductive explanations, adapts the idea for partial labeling using utility functions, defines evidential mistakeness as explanation cost, and develops the IEMM algorithm.

Result: The IEMM algorithm provided decision tree explanations tailored to evidential classifiers and achieved a 93% satisfaction rate in validation tests on both synthetic and real-world datasets.

Conclusion: The proposed IEMM algorithm offers interpretable and cautious decision tree explanations for evidential clustering, effectively addressing uncertainty and imprecision while considering decision-maker preferences.

Abstract: Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.

</details>


### [221] [Selective Quantization Tuning for ONNX Models](https://arxiv.org/abs/2507.12196)
*Nikolaos Louloudakis,Ajitha Rajan*

Main category: cs.LG

TL;DR: This paper introduces TuneQn, a tool for selective quantization of ONNX models to optimize size and accuracy for deployment on various hardware.


<details>
  <summary>Details</summary>
Motivation: Selective quantization is needed to improve the trade-off between model size, computation demands, and accuracy for deployment challenges on low-end hardware.

Method: The authors developed TuneQn, a suite enabling selective model quantization, profiling, deployment, and multi-objective optimization on CPU and GPU devices. It identifies optimal candidates using Pareto Front analysis and visualizes results.

Result: TuneQn achieved up to a 54.14% reduction in accuracy loss and a 72.9% model size reduction on ONNX models tested across different devices, demonstrating its effectiveness.

Conclusion: TuneQn is an effective utility for selective quantization, enhancing deployment options for ONNX models with significant improvements in performance metrics.

Abstract: Quantization is a process that reduces the precision of deep neural network
models to lower model size and computational demands, often at the cost of
accuracy. However, fully quantized models may exhibit sub-optimal performance
below acceptable levels and face deployment challenges on low-end hardware
accelerators due to practical constraints. To address these issues,
quantization can be selectively applied to only a subset of layers, but
selecting which layers to exclude is non-trivial. To this direction, we propose
TuneQn, a suite enabling selective quantization, deployment and execution of
ONNX models across various CPU and GPU devices, combined with profiling and
multi-objective optimization. TuneQn generates selectively quantized ONNX
models, deploys them on different hardware, measures performance on metrics
like accuracy and size, performs Pareto Front minimization to identify the best
model candidate and visualizes the results. To demonstrate the effectiveness of
TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings
across CPU and GPU devices. As a result, we demonstrated that our utility
effectively performs selective quantization and tuning, selecting ONNX model
candidates with up to a $54.14$% reduction in accuracy loss compared to the
fully quantized model, and up to a $72.9$% model size reduction compared to the
original model.

</details>


### [222] [Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation](https://arxiv.org/abs/2507.12218)
*Tomohisa Okazaki*

Main category: cs.LG

TL;DR: The paper develops a physics-informed linear model to analytically solve linear PDEs for forward and inverse problems, comparing physical and mathematical regularization techniques.


<details>
  <summary>Details</summary>
Motivation: To improve the analytical representation of solutions to PDEs and estimate coefficients or boundary conditions using observational data through an alternative to physics-informed neural networks.

Method: A physics-informed linear model (PILM) was created using linear combinations of basis functions to solve PDEs. It was tested on forward and inverse problems, including cases with uncertain boundary conditions, and applied to geodetic data.

Result: The PILM effectively handled forward and inverse problems and underdetermined systems. Bayesian-based mathematical regularization performed better than physical regularization in specific scenarios.

Conclusion: The PILM serves as a robust, analytically solvable framework for solving linear problems in physics-informed domains, bridging gaps in forward, inverse, and regularization approaches.

Abstract: Many physical systems are described by partial differential equations (PDEs),
and solving these equations and estimating their coefficients or boundary
conditions (BCs) from observational data play a crucial role in understanding
the associated phenomena. Recently, a machine learning approach known as
physics-informed neural network, which solves PDEs using neural networks by
minimizing the sum of residuals from the PDEs, BCs, and data, has gained
significant attention in the scientific community. In this study, we
investigate a physics-informed linear model (PILM) that uses linear
combinations of basis functions to represent solutions, thereby enabling an
analytical representation of optimal solutions. The PILM was formulated and
verified for illustrative forward and inverse problems including cases with
uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain
rates using geodetic data. Specifically, physical regularization that enforces
elastic equilibrium on the velocity fields was compared with mathematical
regularization that imposes smoothness constraints. From a Bayesian
perspective, mathematical regularization exhibited superior performance. The
PILM provides an analytically solvable framework applicable to linear forward
and inverse problems, underdetermined systems, and physical regularization.

</details>


### [223] [Optimizers Qualitatively Alter Solutions And We Should Leverage This](https://arxiv.org/abs/2507.12224)
*Razvan Pascanu,Clare Lyle,Ionut-Vlad Modoranu,Naima Elosegui Borras,Dan Alistarh,Petar Velickovic,Sarath Chandar,Soham De,James Martens*

Main category: cs.LG

TL;DR: While deep learning success has often focused on optimizers improving convergence rates, this paper argues the need to evaluate their role in shaping solution properties.


<details>
  <summary>Details</summary>
Motivation: Highlight the overlooked role of optimizers in influencing qualitative properties of DNN solutions and not just convergence speed.

Method: Discussion paper emphasizing the biases and inductive properties encoded by optimizers during DNN training.

Result: Arguments presented suggest optimizers impact the effective expressivity of DNN models and can enforce specific solution properties.

Conclusion: Calls for deeper study into optimizers' biases and designing them to intentionally shape model outcomes beyond training efficiency.

Abstract: Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not
guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a
primary source of skepticism regarding the feasibility of DNNs in the early
days of the field. The past decades of progress in deep learning have revealed
this skepticism to be misplaced, and a large body of empirical evidence shows
that sufficiently large DNNs following standard training protocols exhibit
well-behaved optimization dynamics that converge to performant solutions. This
success has biased the community to use convex optimization as a mental model
for learning, leading to a focus on training efficiency, either in terms of
required iteration, FLOPs or wall-clock time, when improving optimizers. We
argue that, while this perspective has proven extremely fruitful, another
perspective specific to DNNs has received considerably less attention: the
optimizer not only influences the rate of convergence, but also the qualitative
properties of the learned solutions. Restated, the optimizer can and will
encode inductive biases and change the effective expressivity of a given class
of models. Furthermore, we believe the optimizer can be an effective way of
encoding desiderata in the learning process. We contend that the community
should aim at understanding the biases of already existing methods, as well as
aim to build new optimizers with the explicit intent of inducing certain
properties of the solution, rather than solely judging them based on their
convergence rates. We hope our arguments will inspire research to improve our
understanding of how the learning process can impact the type of solution we
converge to, and lead to a greater recognition of optimizers design as a
critical lever that complements the roles of architecture and data in shaping
model outcomes.

</details>


### [224] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Main category: cs.LG

TL;DR: This paper proposes a method to remove sensitive information, like gender or race, from text representations using an orthogonal projection approach called $\overline{\mathrm{L}}$EOPARD.


<details>
  <summary>Details</summary>
Motivation: To ensure fairness in real-world applications by preventing neural models from inferring sensitive demographic attributes from text representations.

Method: The approach learns an orthogonal projection in the embedding space, making class-conditional feature distributions of specific concepts indistinguishable while preserving other semantic details.

Result: The proposed method, $\overline{\mathrm{L}}$EOPARD, achieves state-of-the-art results in nonlinear erasure of discrete attributes on common NLP benchmarks and effectively reduces classifier bias.

Conclusion: $\overline{\mathrm{L}}$EOPARD provides an effective way to ensure fairness in neural models by erasing sensitive concept information while maintaining semantic integrity.

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [225] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: RegCL is a proposed continual learning framework for multi-domain knowledge integration, avoiding catastrophic forgetting and maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: The need to address performance limitations and catastrophic forgetting in domain-specific adaptations of the SAM model, which limits scalability.

Method: RegCL merges the parameters of SAM's domain-specific adaptation modules through a weight optimization process in a continual learning setup.

Result: RegCL achieves effective multi-domain continual learning without requiring additional storage or increasing model size.

Conclusion: RegCL provides a scalable and efficient solution for integrating domain-specific knowledge into SAM for dynamic scenarios.

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [226] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: The paper introduces a novel prompt-based method for online continual learning (OCL) to address catastrophic forgetting in streaming data, outperforming state-of-the-art (SOTA) techniques while requiring fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing OCL methods struggle with privacy constraints and catastrophic forgetting. Current SOTA solutions either rely on memory-based approaches that conflict with data openness policies or parameter-heavy prompt-based methods with throughput issues.

Method: The authors propose a new prompt-based method with four main components: (1) a lightweight prompt generator, (2) trainable scaler-and-shifter for specific knowledge, (3) pre-trained model generalization preservation, and (4) a hard-soft updates mechanism.

Result: The proposed method outperforms existing SOTA methods across CIFAR100, ImageNet-R, ImageNet-A, and CUB datasets, requiring fewer parameters and achieving moderate training and inference times.

Conclusion: The study demonstrates the effectiveness of a lightweight, privacy-compliant prompt-based OCL method, presenting a promising solution to handle catastrophic forgetting in streaming data.

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


### [227] [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)
*Zihao Xue,Zhen Bi,Long Ma,Zhenlin Hu,Yan Wang,Zhenfang Liu,Qing Sheng,Jie Xiao,Jungang Lou*

Main category: cs.LG

TL;DR: The paper addresses security vulnerabilities in reinforcement learning-based reasoning models like Deepseek-R1, specifically in Chain-of-Thought (CoT) processes, by proposing the "Thought Purity" defense paradigm.


<details>
  <summary>Details</summary>
Motivation: To counteract critical security threats in large reasoning models, particularly in Chain-of-Thought processes, caused by adversarial attacks like Chain-of-Thought Attack (CoTA).

Method: The authors propose a defense called "Thought Purity" comprising three components: a safety-enhanced data processing system, reinforcement learning-based rule constraints, and adaptive monitoring.

Result: The proposed framework effectively mitigates CoTA vulnerabilities while maintaining task performance, offering a pioneering defensive solution for reasoning architectures.

Conclusion: The paper provides a robust solution for fortifying reasoning models against adversarial attacks, balancing security and functionality in advanced AI systems.

Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,
Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large
Language Models (LLMs) domain, their susceptibility to security threats remains
a critical vulnerability. This weakness is particularly evident in
Chain-of-Thought (CoT) generation processes, where adversarial methods like
backdoor prompt attacks can systematically subvert the model's core reasoning
mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this
vulnerability through exploiting prompt controllability, simultaneously
degrading both CoT safety and task performance with low-cost interventions. To
address this compounded security-performance vulnerability, we propose Thought
Purity (TP): a defense paradigm that systematically strengthens resistance to
malicious content while preserving operational efficacy. Our solution achieves
this through three synergistic components: (1) a safety-optimized data
processing pipeline (2) reinforcement learning-enhanced rule constraints (3)
adaptive monitoring metrics. Our approach establishes the first comprehensive
defense mechanism against CoTA vulnerabilities in reinforcement
learning-aligned reasoning systems, significantly advancing the
security-functionality equilibrium for next-generation AI architectures.

</details>


### [228] [Heat Kernel Goes Topological](https://arxiv.org/abs/2507.12380)
*Maximilian Krahn,Vikas Garg*

Main category: cs.LG

TL;DR: This paper introduces a new Laplacian operator on combinatorial complexes (CCs) for efficient topological neural network computations and demonstrates its strong performance in molecular and topological tasks.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiency in topological neural networks' higher-order message passing methods.

Method: Introduced a Laplacian operator on combinatorial complexes (CCs) to compute heat kernels efficiently, enabling multiscale, permutation-equivariant descriptors for integration into transformer architectures.

Result: The approach is maximally expressive, distinguishes arbitrary non-isomorphic CCs, and delivers superior computational efficiency and competitive or superior performance on molecular and topological benchmarks.

Conclusion: The study achieves scalable and expressive representations in topological deep learning, opening new possibilities in molecular classification and property prediction.

Abstract: Topological neural networks have emerged as powerful successors of graph
neural networks. However, they typically involve higher-order message passing,
which incurs significant computational expense. We circumvent this issue with a
novel topological framework that introduces a Laplacian operator on
combinatorial complexes (CCs), enabling efficient computation of heat kernels
that serve as node descriptors. Our approach captures multiscale information
and enables permutation-equivariant representations, allowing easy integration
into modern transformer-based architectures.
  Theoretically, the proposed method is maximally expressive because it can
distinguish arbitrary non-isomorphic CCs. Empirically, it significantly
outperforms existing topological methods in terms of computational efficiency.
Besides demonstrating competitive performance with the state-of-the-art
descriptors on standard molecular datasets, it exhibits superior capability in
distinguishing complex topological structures and avoiding blind spots on
topological benchmarks. Overall, this work advances topological deep learning
by providing expressive yet scalable representations, thereby opening up
exciting avenues for molecular classification and property prediction tasks.

</details>


### [229] [Improving Reinforcement Learning Sample-Efficiency using Local Approximation](https://arxiv.org/abs/2507.12383)
*Mohit Prashant,Arvind Easwaran*

Main category: cs.LG

TL;DR: The paper improves PAC bounds for RL in infinite-horizon MDP by leveraging relationships between states’ optimal values, reducing sample complexity by a logarithmic factor.


<details>
  <summary>Details</summary>
Motivation: Current sample-complexity bounds in reinforcement learning are not sharp enough, and there's an opportunity to optimize them by focusing on state dependencies.

Method: The authors approximate the primary MDP using smaller MDPs derived from subsets of the state-space to reduce learning effort while constructing a PAC-MDP algorithm for infinite-horizon settings.

Result: The sample complexity was reduced to O(SA log A) timesteps, and experimental comparisons showed significant improvements in performance.

Conclusion: Sharper PAC bounds and reduced sample complexity were achieved, leading to more efficient reinforcement learning algorithms in infinite-horizon settings.

Abstract: In this study, we derive Probably Approximately Correct (PAC) bounds on the
asymptotic sample-complexity for RL within the infinite-horizon Markov Decision
Process (MDP) setting that are sharper than those in existing literature. The
premise of our study is twofold: firstly, the further two states are from each
other, transition-wise, the less relevant the value of the first state is when
learning the $\epsilon$-optimal value of the second; secondly, the amount of
'effort', sample-complexity-wise, expended in learning the $\epsilon$-optimal
value of a state is independent of the number of samples required to learn the
$\epsilon$-optimal value of a second state that is a sufficient number of
transitions away from the first. Inversely, states within each other's vicinity
have values that are dependent on each other and will require a similar number
of samples to learn. By approximating the original MDP using smaller MDPs
constructed using subsets of the original's state-space, we are able to reduce
the sample-complexity by a logarithmic factor to $O(SA \log A)$ timesteps,
where $S$ and $A$ are the state and action space sizes. We are able to extend
these results to an infinite-horizon, model-free setting by constructing a
PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with
showing how significant the improvement is by comparing our algorithm against
prior work in an experimental setting.

</details>


### [230] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen,Guoyun Gao,Zhicheng Xu,Ruibin Mao,Xiaojuan Qi,X. Sharon Hu,Xunzhao Yin,Can Li*

Main category: cs.LG

TL;DR: This paper introduces a co-design approach using $MoS_2$ Flash-based analog CAM for tree-based models, improving interpretability, robustness, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the trustworthiness concerns in artificial intelligence, particularly regarding the interpretability and robustness of tree-based models, while mitigating their computational inefficiency and susceptibility to hardware variability and adversarial attacks.

Method: The researchers developed a co-design approach utilizing $MoS_2$ Flash-based analog CAM with soft decision boundaries to enable efficient and robust inference for soft tree-based models.

Result: The method achieved state-of-the-art accuracy and robustness against device variations and adversarial attacks. On the WDBC database, it demonstrated 96% accuracy, maintaining explainability, and showed minimal accuracy degradation (0.6%) on the MNIST dataset under challenging conditions.

Conclusion: This approach effectively combines specialized hardware and tree-based models to enhance AI's robustness, accuracy, and trustworthiness, presenting a promising path for AI hardware innovations.

Abstract: The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [231] [NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data](https://arxiv.org/abs/2507.12412)
*Dzung Dinh,Boqi Chen,Marc Niethammer,Junier Oliva*

Main category: cs.LG

TL;DR: The paper presents NOCTA, a method for cost-efficient feature acquisition in temporal prediction tasks, validated on medical datasets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in resource-constrained settings, like healthcare, where acquiring features for predictions incurs temporal, financial, or health-related costs.

Method: They propose NOCTA, which includes two estimators: a non-parametric nearest neighbor-based approach (NOCTA-NP) and a parametric utility-predicting approach (NOCTA-P) for feature acquisition.

Result: Experiments on synthetic and real-world datasets validate that NOCTA outperforms existing baselines in balancing cost and predictive performance.

Conclusion: NOCTA effectively addresses temporal and cost constraints in feature acquisition, demonstrating superior performance over baselines.

Abstract: In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.

</details>


### [232] [Mixture of Raytraced Experts](https://arxiv.org/abs/2507.12419)
*Andrea Perin,Giacomo Lagomarsini,Claudio Gallicchio,Giuseppe Nuti*

Main category: cs.LG

TL;DR: This paper introduces a novel Mixture of Experts (MoE) architecture called Mixture of Raytraced Experts, capable of dynamically forming computational graphs with variable depth and width for better accuracy over multiple cycles.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing MoE architectures that require fixed computation levels regardless of accuracy gained, and to develop a method for dynamically adjusting computational graphs to improve training efficiency and model expressiveness.

Method: The authors propose an iterative training approach that samples candidates, progressively builds sequences of experts, and eliminates the need for load-balancing mechanisms traditionally present in MoEs.

Result: Experiments demonstrate a reduction in training epochs by 10% to 40%, while achieving comparable or better accuracy compared to existing models.

Conclusion: The proposed architecture shows promise in advancing MoE designs to be more efficient and expressive, opening up new directions for research in dynamic computation and adaptive modeling paradigms.

Abstract: We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts
(MoE) architecture which can dynamically select sequences of experts, producing
computational graphs of variable width and depth. Existing MoE architectures
generally require a fixed amount of computation for a given sample. Our
approach, in contrast, yields predictions with increasing accuracy as the
computation cycles through the experts' sequence. We train our model by
iteratively sampling from a set of candidate experts, unfolding the sequence
akin to how Recurrent Neural Networks are trained. Our method does not require
load-balancing mechanisms, and preliminary experiments show a reduction in
training epochs of 10\% to 40\% with a comparable/higher accuracy. These
results point to new research directions in the field of MoEs, allowing the
design of potentially faster and more expressive models. The code is available
at https://github.com/nutig/RayTracing

</details>


### [233] [Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks](https://arxiv.org/abs/2507.12435)
*Yi Li,David Mccoy,Nolan Gunter,Kaitlyn Lee,Alejandro Schuler,Mark van der Laan*

Main category: cs.LG

TL;DR: The paper introduces Targeted Deep Architectures (TDA), a framework enabling valid causal inference and treatment effect estimation directly within deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches like Double Machine Learning and TMLE either fail to guarantee accurate causal inference or lack scalability for complex estimands, creating limitations in applying neural networks for causal inference.

Method: TDA partitions network parameters, freezes most of them, and uses iterative targeting gradient updates derived from influence functions to correct bias and produce asymptotically valid outputs directly within the network.

Result: The approach reduces bias and improves coverage in benchmarks like average treatment effect estimation (IHDP dataset) and simulated survival data compared to standard neural-network methods and prior TMLE implementations.

Conclusion: TDA provides a scalable, valid pathway for embedding causal inference into deep neural networks while retaining TMLE's robustness and efficiency for complex, multi-dimensional causal targets.

Abstract: Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.

</details>


### [234] [A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning](https://arxiv.org/abs/2507.12439)
*Daniel Commey,Rebecca A. Sarpong,Griffith S. Klogo,Winful Bagyl-Bac,Garth V. Crosby*

Main category: cs.LG

TL;DR: The paper proposes a proactive Bayesian incentive mechanism to mitigate data-poisoning attacks in federated learning by making malicious behavior economically irrational, achieving strong robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Current defenses against data-poisoning attacks in federated learning rely on reactive statistical methods that are computationally heavy and assume an honest majority. A proactive, efficient defense is needed to tackle these limitations.

Method: The authors introduce a Bayesian game-based incentive mechanism where the server uses a private validation dataset to evaluate update quality and issues payments. This approach ensures Individual Rationality (IR) and Incentive Compatibility (IC), disincentivizing malicious behavior economically.

Result: The proposed mechanism maintains 96.7% accuracy in scenarios with 50% label-flipping adversaries on MNIST, outperforming standard FedAvg by 51.7 percentage points. It demonstrates strong robustness in experiments on non-IID data.

Conclusion: The mechanism provides an economically viable, computationally lightweight defense against data-poisoning attacks, enhancing federated learning frameworks while ensuring sustainability and robustness.

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy. However, its
open-participation nature exposes it to data-poisoning attacks, in which
malicious actors submit corrupted model updates to degrade the global model.
Existing defenses are often reactive, relying on statistical aggregation rules
that can be computationally expensive and that typically assume an honest
majority. This paper introduces a proactive, economic defense: a lightweight
Bayesian incentive mechanism that makes malicious behavior economically
irrational. Each training round is modeled as a Bayesian game of incomplete
information in which the server, acting as the principal, uses a small, private
validation dataset to verify update quality before issuing payments. The design
satisfies Individual Rationality (IR) for benevolent clients, ensuring their
participation is profitable, and Incentive Compatibility (IC), making poisoning
an economically dominated strategy. Extensive experiments on non-IID partitions
of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping
adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3
percentage points lower than in a scenario with 30% label-flipping adversaries.
This outcome is 51.7 percentage points better than standard FedAvg, which
collapses under the same 50% attack. The mechanism is computationally light,
budget-bounded, and readily integrates into existing FL frameworks, offering a
practical route to economically robust and sustainable FL ecosystems.

</details>


### [235] [Cost-aware Stopping for Bayesian Optimization](https://arxiv.org/abs/2507.12453)
*Qian Xie,Linda Cai,Alexander Terenin,Peter I. Frazier,Ziv Scully*

Main category: cs.LG

TL;DR: The paper presents a new cost-aware stopping rule for Bayesian optimization with theoretical guarantees and strong experimental performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining when to stop expensive black-box function evaluations in Bayesian optimization in a cost-aware manner.

Method: The authors propose a cost-aware stopping rule grounded in theoretical connections to state-of-the-art acquisition functions (Pandora's Box Gittins Index and log expected improvement per cost) and provide guarantees on expected evaluation costs.

Result: The proposed stopping rule performs well on synthetic and empirical tasks, outperforming or matching other approaches in cost-adjusted simple regret metrics when combined with the PBGI acquisition function.

Conclusion: This work advances cost-aware Bayesian optimization by offering a practical and theoretically sound stopping rule that yields better trade-offs between evaluation cost and solution quality.

Abstract: In automated machine learning, scientific discovery, and other applications
of Bayesian optimization, deciding when to stop evaluating expensive black-box
functions is an important practical consideration. While several adaptive
stopping rules have been proposed, in the cost-aware setting they lack
guarantees ensuring they stop before incurring excessive function evaluation
costs. We propose a cost-aware stopping rule for Bayesian optimization that
adapts to varying evaluation costs and is free of heuristic tuning. Our rule is
grounded in a theoretical connection to state-of-the-art cost-aware acquisition
functions, namely the Pandora's Box Gittins Index (PBGI) and log expected
improvement per cost. We prove a theoretical guarantee bounding the expected
cumulative evaluation cost incurred by our stopping rule when paired with these
two acquisition functions. In experiments on synthetic and empirical tasks,
including hyperparameter optimization and neural architecture size search, we
show that combining our stopping rule with the PBGI acquisition function
consistently matches or outperforms other acquisition-function--stopping-rule
pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs
between solution quality and cumulative evaluation cost.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [236] [Emergent Heterogeneous Swarm Control Through Hebbian Learning](https://arxiv.org/abs/2507.11566)
*Fuda van Diggelen,Tugay Alperen Karagüzel,Andres Garcia Rincon,A. E. Eiben,Dario Floreano,Eliseo Ferrante*

Main category: cs.NE

TL;DR: The paper proposes Hebbian learning as a novel method for swarm robotics, enabling automatic emergence of heterogeneity and overcoming key challenges in heterogeneous control learning.


<details>
  <summary>Details</summary>
Motivation: To address challenges such as the micro-macro problem, scalability, and limited prior knowledge in learning heterogeneous control for swarm robotics.

Method: The authors rely on biologically inspired Hebbian learning rules, which use local information and require fewer parameters, to evolve swarm-level behaviors without extensive prior knowledge.

Result: Results show that Hebbian learning leads to natural emergence of heterogeneity, improves swarm capabilities through behavioral switching, and performs competitively compared to Multi Agent Reinforcement Learning benchmarks.

Conclusion: Hebbian learning is a promising approach for enabling heterogeneity in swarm robotics and offers a simpler, scalable, and effective alternative to existing approaches like Multi Agent Reinforcement Learning.

Abstract: In this paper, we introduce Hebbian learning as a novel method for swarm
robotics, enabling the automatic emergence of heterogeneity. Hebbian learning
presents a biologically inspired form of neural adaptation that solely relies
on local information. By doing so, we resolve several major challenges for
learning heterogeneous control: 1) Hebbian learning removes the complexity of
attributing emergent phenomena to single agents through local learning rules,
thus circumventing the micro-macro problem; 2) uniform Hebbian learning rules
across all swarm members limit the number of parameters needed, mitigating the
curse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian
learning rules based on swarm-level behaviour minimises the need for extensive
prior knowledge typically required for optimising heterogeneous swarms. This
work demonstrates that with Hebbian learning heterogeneity naturally emerges,
resulting in swarm-level behavioural switching and in significantly improved
swarm capabilities. It also demonstrates how the evolution of Hebbian learning
rules can be a valid alternative to Multi Agent Reinforcement Learning in
standard benchmarking tasks.

</details>


### [237] [Survey of Genetic and Differential Evolutionary Algorithm Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11751)
*Chandrashekar Muniyappa,Eunjin Kim*

Main category: cs.NE

TL;DR: The paper reviews advancements in using genetic and differential evolutionary algorithms to find semantically similar documents within large datasets.


<details>
  <summary>Details</summary>
Motivation: The challenge of identifying similar documents within vast volumes of data requires innovative computational approaches, especially given the rise of big data.

Method: The paper conducts a survey of distributed computing techniques, focusing on the application of genetic and differential evolutionary algorithms to semantic text similarity.

Result: The survey identifies the recent progress and techniques in document retrieval based on semantic similarity using advanced computing methods.

Conclusion: Genetic and differential evolutionary algorithms provide effective solutions for semantic text similarity search in big data contexts.

Abstract: Identifying similar documents within extensive volumes of data poses a
significant challenge. To tackle this issue, researchers have developed a
variety of effective distributed computing techniques. With the advancement of
computing power and the rise of big data, deep neural networks and evolutionary
computing algorithms such as genetic algorithms and differential evolution
algorithms have achieved greater success. This survey will explore the most
recent advancements in the search for documents based on their semantic text
similarity, focusing on genetic and differential evolutionary computing
algorithms.

</details>


### [238] [Simulated Language Acquisition in a Biologically Realistic Model of the Brain](https://arxiv.org/abs/2507.11788)
*Daniel Mitropolsky,Christos Papadimitriou*

Main category: cs.NE

TL;DR: The paper presents a simulated neuromorphic system that learns semantic and syntactic structures of any language from grounded sentences based on neuroscience principles.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how neuron spiking results in high-level cognitive phenomena such as language and planning.

Method: Developed a mathematical framework based on six neuroscience principles and implemented a neuromorphic system for language acquisition.

Result: The system, starting from scratch, learns word semantics, syntactic roles, word order, and generates novel sentences after limited exposure to grounded examples.

Conclusion: The system demonstrates how neuroscience principles can enable language learning and generation, with potential for further extensions and broader implications.

Abstract: Despite tremendous progress in neuroscience, we do not have a compelling
narrative for the precise way whereby the spiking of neurons in our brain
results in high-level cognitive phenomena such as planning and language. We
introduce a simple mathematical formulation of six basic and broadly accepted
principles of neuroscience: excitatory neurons, brain areas, random synapses,
Hebbian plasticity, local inhibition, and inter-area inhibition. We implement a
simulated neuromorphic system based on this formalism, which is capable of
basic language acquisition: Starting from a tabula rasa, the system learns, in
any language, the semantics of words, their syntactic role (verb versus noun),
and the word order of the language, including the ability to generate novel
sentences, through the exposure to a modest number of grounded sentences in the
same language. We discuss several possible extensions and implications of this
result.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [239] [Quantum circuits are just a phase](https://arxiv.org/abs/2507.11676)
*Chris Heunen,Louis Lemonnier,Christopher McNally,Alex Rice*

Main category: cs.PL

TL;DR: This paper proposes a novel quantum programming language that shifts focus from low-level circuit design to higher-level constructs like eigendecomposition and controlled unitaries, enabling clearer and more scalable quantum algorithm design.


<details>
  <summary>Details</summary>
Motivation: Current quantum programming languages operate at a low level of abstraction, making scalability and clarity difficult while hindering higher-level reasoning. The authors aim to address this limitation by introducing more abstract and expressive constructs.

Method: The authors developed a quantum programming language based on two key constructs: a global phase operation for capturing phase shifts and a quantum equivalent of the 'if let' construct for subspace selection. They established universality, designed denotational semantics, and implemented a prototype compiler for converting the language to quantum circuits.

Result: The proposed language is shown to express important quantum algorithms naturally and concisely. The authors proved universality by deriving a universal quantum gate set, implemented a sound compiler, and provided clean semantics linked to categorical quantum mechanics.

Conclusion: The language demonstrates a significant step toward abstract and structured quantum programming, offering both theoretical soundness and practical utility for designing quantum algorithms.

Abstract: Quantum programs today are written at a low level of abstraction - quantum
circuits akin to assembly languages - and even advanced quantum programming
languages essentially function as circuit description languages. This state of
affairs impedes scalability, clarity, and support for higher-level reasoning.
More abstract and expressive quantum programming constructs are needed.
  To this end, we introduce a novel yet simple quantum programming language for
generating unitaries from "just a phase"; we combine a (global) phase operation
that captures phase shifts with a quantum analogue of the "if let" construct
that captures subspace selection via pattern matching. This minimal language
lifts the focus from quantum gates to eigendecomposition, conjugation, and
controlled unitaries; common building blocks in quantum algorithm design.
  We demonstrate several aspects of the expressive power of our language in
several ways. Firstly, we establish that our representation is universal by
deriving a universal quantum gate set. Secondly, we show that important quantum
algorithms can be expressed naturally and concisely, including Grover's search
algorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal
Processing, and the Quantum Eigenvalue Transformation. Furthermore, we give
clean denotational semantics grounded in categorical quantum mechanics.
Finally, we implement a prototype compiler that efficiently translates terms of
our language to quantum circuits, and prove that it is sound with respect to
these semantics. Collectively, these contributions show that this construct
offers a principled and practical step toward more abstract and structured
quantum programming.

</details>


### [240] [Picat Through the Lens of Advent of Code](https://arxiv.org/abs/2507.11731)
*Neng-Fa Zhou,Cristian Grozea,Håkan Kjellerstrand,Oisín Mac Fhearaí*

Main category: cs.PL

TL;DR: The paper showcases solutions to Advent of Code 2024 problems using Picat, a versatile, multi-paradigm programming language.


<details>
  <summary>Details</summary>
Motivation: To highlight how Picat’s unique features are particularly effective for solving specific problem types such as reverse engineering and path-finding.

Method: The authors use Picat's logic-based programming features like SAT-based constraint solving, pattern matching, backtracking, and tabling to solve Advent of Code problems efficiently and concisely.

Result: Picat’s capabilities allow for concise, declarative, and efficient implementations of problems that are usually more labor-intensive in imperative programming languages.

Conclusion: Picat’s paradigm integration and specialized features make it highly suitable for solving complex algorithmic problems, demonstrating both its efficiency and versatility.

Abstract: Picat is a logic-based, multi-paradigm programming language that integrates
features from logic, functional, constraint, and imperative programming
paradigms. This paper presents solutions to several problems from the 2024
Advent of Code (AoC). While AoC problems are not designed for any specific
programming language, certain problem types, such as reverse engineering and
path-finding, are particularly well-suited to Picat due to its built-in
constraint solving, pattern matching, backtracking, and dynamic programming
with tabling. This paper demonstrates that Picat's features, especially its
SAT-based constraint solving and tabling, enable concise, declarative, and
highly efficient implementations of problems that would require significantly
more effort in imperative languages.

</details>


### [241] [Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers](https://arxiv.org/abs/2507.11827)
*Shaurya Gomber,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.PL

TL;DR: This paper proposes a universal transformer synthesis algorithm for numerical abstract interpretation, enabling adaptable, sound transformers across domains, guided by a novel gradient-based search strategy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing numerical abstract interpreters, which use fixed, hand-crafted transformers specific to each domain, thus restricting extensibility, compositional reasoning, and task-specific adaptability.

Method: The method involves introducing a universal synthesis algorithm for sound abstract transformers tailored to any polyhedral numerical domain and implementing a gradient-based search strategy, Adaptive Gradient Guidance, for efficient exploration of transformer spaces.

Result: The results show that the proposed framework, USTAD, can efficiently construct sound transformer families for various domains and achieve better precision than baselines by exploiting compositional reasoning and adaptive optimization.

Conclusion: The study concludes that the proposed universal transformer synthesis algorithm and USTAD framework enhance flexibility, precision, and efficiency in numerical abstract interpretation, demonstrating robust adaptability across domains.

Abstract: Numerical abstract interpretation is a widely used framework for the static
analysis of numerical programs. However, existing numerical abstract
interpreters rely on hand-crafted, instruction-specific transformers tailored
to each domain, with no general algorithm for handling common operations across
domains. This limits extensibility, prevents precise compositional reasoning
over instruction sequences, and forces all downstream tasks to use the same
fixed transformer regardless of their precision, efficiency, or task-specific
requirements. To address these limitations, we propose a universal transformer
synthesis algorithm that constructs a parametric family of sound abstract
transformers for any given polyhedral numerical domain and a concrete operator
from the class of Quadratic-Bounded Guarded Operators (QGO), which includes
both individual instructions and structured sequences. Each instantiation in
this family is sound by construction, enabling downstream analyses to adapt the
transformer to their particular needs. The space of transformers is
differentiable but complex. To efficiently explore this space of transformers,
we introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided
search strategy that steers the search process based on downstream analysis
objectives and runtime constraints. We implement these ideas in the USTAD
framework and evaluate their effectiveness across three numerical abstract
domains: Zones, Octagons, and Polyhedra. Our results demonstrate that the
universal synthesis algorithm successfully constructs sound families of
transformers across domains, and that USTAD achieves significant, tunable
precision gains over baselines by leveraging compositional reasoning and
efficient gradient-guided traversal of the transformer space.

</details>


### [242] [Towards Relational Contextual Equality Saturation](https://arxiv.org/abs/2507.11897)
*Tyler Hou,Shadaj Laddad,Joseph M. Hellerstein*

Main category: cs.PL

TL;DR: The paper explores extending equality saturation to contextual and relational settings, with a focus on the egglog framework.


<details>
  <summary>Details</summary>
Motivation: To advance program optimization techniques by integrating contextual reasoning and relational models into equality saturation frameworks.

Method: The authors build on existing contextual equality saturation work in egg, aiming to adapt and extend these concepts to relational equality saturation in egglog.

Result: This work outlines existing approaches, key applications, and identifies challenges for integrating contextual equality saturation with relational models in egglog.

Conclusion: The ongoing work in contextual and relational equality saturation presents promising directions for more sophisticated program optimization strategies.

Abstract: Equality saturation is a powerful technique for program optimization.
Contextual equality saturation extends this to support rewrite rules that are
conditioned on where a term appears in an expression. Existing work has brought
contextual reasoning to egg; in this paper, we share our ongoing work to extend
this to relational equality saturation in egglog. We summarize the existing
approaches to contextual equality saturation, outline its main applications,
and identify key challenges in combining this approach with relational models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [243] [HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways](https://arxiv.org/abs/2507.11621)
*Tianyi Wang,Yangyang Wang,Jie Pan,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: This study introduces a Hierarchical Cooperative On-Ramp Merging Control (HCOMC) framework to address traffic flow and safety challenges on highways with mixed CAV and HDV traffic.


<details>
  <summary>Details</summary>
Motivation: The problem of traffic congestion and accidents at highway on-ramp merging areas lacks effective control strategies adaptable to mixed traffic involving both CAVs and HDVs.

Method: The paper develops a HCOMC framework using longitudinal car-following models, lane-changing models, game theory, and multi-objective optimization. Simulations evaluate its performance under varying traffic densities and CAV penetration rates.

Result: The proposed HCOMC framework demonstrates significant advantages in enhancing safety, improving merging processes, boosting traffic efficiency, and reducing fuel consumption over existing methods.

Conclusion: The HCOMC framework effectively addresses on-ramp merging challenges for heterogeneous traffic, showcasing its potential for real-world application and a promising direction for future traffic systems.

Abstract: Highway on-ramp merging areas are common bottlenecks to traffic congestion
and accidents. Currently, a cooperative control strategy based on connected and
automated vehicles (CAVs) is a fundamental solution to this problem. While CAVs
are not fully widespread, it is necessary to propose a hierarchical cooperative
on-ramp merging control (HCOMC) framework for heterogeneous traffic flow on
two-lane highways to address this gap. This paper extends longitudinal
car-following models based on the intelligent driver model and lateral
lane-changing models using the quintic polynomial curve to account for
human-driven vehicles (HDVs) and CAVs, comprehensively considering human
factors and cooperative adaptive cruise control. Besides, this paper proposes a
HCOMC framework, consisting of a hierarchical cooperative planning model based
on the modified virtual vehicle model, a discretionary lane-changing model
based on game theory, and a multi-objective optimization model using the
elitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and
efficient merging process. Then, the performance of our HCOMC is analyzed under
different traffic densities and CAV penetration rates through simulation. The
findings underscore our HCOMC's pronounced comprehensive advantages in
enhancing the safety of group vehicles, stabilizing and expediting merging
process, optimizing traffic efficiency, and economizing fuel consumption
compared with benchmarks.

</details>


### [244] [A Roadmap for Climate-Relevant Robotics Research](https://arxiv.org/abs/2507.11623)
*Alan Papalia,Charles Dawson,Laurentiu L. Anton,Norhan Magdy Bayomi,Bianca Champenois,Jung-Hoon Cho,Levi Cai,Joseph DelPreto,Kristen Edwards,Bilha-Catherine Githinji,Cameron Hickert,Vindula Jayawardana,Matthew Kramer,Shreyaa Raghavan,David Russell,Shide Salimi,Jingnan Shi,Soumya Sudhakar,Yanwei Wang,Shouyi Wang,Luca Carlone,Vijay Kumar,Daniela Rus,John E. Fernandez,Cathy Wu,George Kantor,Derek Young,Hanumant Singh*

Main category: cs.RO

TL;DR: This paper proposes a roadmap for integrating robotics into climate-related domains, emphasizing collaboration and actionable opportunities.


<details>
  <summary>Details</summary>
Motivation: The robotics community seeks ways to address climate change, a critical global challenge.

Method: The roadmap highlights applications and collaborative opportunities in areas such as energy systems, transportation, agriculture, and environmental monitoring, employing physical robots and robotics algorithms.

Result: Specific, actionable problems at the intersection of robotics and climate research are identified, along with potential applications.

Conclusion: The paper aims to inspire new research and collaboration between roboticists and climate experts, encouraging the robotics community to address pressing climate issues.

Abstract: Climate change is one of the defining challenges of the 21st century, and
many in the robotics community are looking for ways to contribute. This paper
presents a roadmap for climate-relevant robotics research, identifying
high-impact opportunities for collaboration between roboticists and experts
across climate domains such as energy, the built environment, transportation,
industry, land use, and Earth sciences. These applications include problems
such as energy systems optimization, construction, precision agriculture,
building envelope retrofits, autonomous trucking, and large-scale environmental
monitoring. Critically, we include opportunities to apply not only physical
robots but also the broader robotics toolkit - including planning, perception,
control, and estimation algorithms - to climate-relevant problems. A central
goal of this roadmap is to inspire new research directions and collaboration by
highlighting specific, actionable problems at the intersection of robotics and
climate. This work represents a collaboration between robotics researchers and
domain experts in various climate disciplines, and it serves as an invitation
to the robotics community to bring their expertise to bear on urgent climate
priorities.

</details>


### [245] [CoNav Chair: Development and Evaluation of a Shared Control based Wheelchair for the Built Environment](https://arxiv.org/abs/2507.11716)
*Yifan Xu,Qianwei Wang,Jordan Lillie,Vineet Kamat,Carol Menassa,Clive D'Souza*

Main category: cs.RO

TL;DR: This paper introduces the CoNav Chair, a smart wheelchair with shared control navigation and obstacle avoidance, designed to enhance user safety, efficiency, and ease of use.


<details>
  <summary>Details</summary>
Motivation: The growing population of people with disabilities requires better mobility solutions that balance independence, social integration, and ease of navigation in constrained environments.

Method: The CoNav Chair was developed using the Robot Operating System (ROS) and tested across three navigation modes—manual, shared, and autonomous—with 21 unimpaired participants navigating an indoor environment.

Result: The shared control mode demonstrated fewer collisions, superior or comparable performance in efficiency and trajectory metrics, and positive user feedback on safety and usability compared to manual and autonomous modes.

Conclusion: The CoNav Chair shows promise as an effective and safe mobility tool, highlighting its potential for future usability evaluations with people who depend on powered wheelchairs.

Abstract: As the global population of people with disabilities (PWD) continues to grow,
so will the need for mobility solutions that promote independent living and
social integration. Wheelchairs are vital for the mobility of PWD in both
indoor and outdoor environments. The current SOTA in powered wheelchairs is
based on either manually controlled or fully autonomous modes of operation,
offering limited flexibility and often proving difficult to navigate in
spatially constrained environments. Moreover, research on robotic wheelchairs
has focused predominantly on complete autonomy or improved manual control;
approaches that can compromise efficiency and user trust. To overcome these
challenges, this paper introduces the CoNav Chair, a smart wheelchair based on
the Robot Operating System (ROS) and featuring shared control navigation and
obstacle avoidance capabilities that are intended to enhance navigational
efficiency, safety, and ease of use for the user. The paper outlines the CoNav
Chair's design and presents a preliminary usability evaluation comparing three
distinct navigation modes, namely, manual, shared, and fully autonomous,
conducted with 21 healthy, unimpaired participants traversing an indoor
building environment. Study findings indicated that the shared control
navigation framework had significantly fewer collisions and performed
comparably, if not superior to the autonomous and manual modes, on task
completion time, trajectory length, and smoothness; and was perceived as being
safer and more efficient based on user reported subjective assessments of
usability. Overall, the CoNav system demonstrated acceptable safety and
performance, laying the foundation for subsequent usability testing with end
users, namely, PWDs who rely on a powered wheelchair for mobility.

</details>


### [246] [Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies](https://arxiv.org/abs/2507.11770)
*Giang Nguyen,Mihai Pomarlan,Sascha Jongebloed,Nils Leusmann,Minh Nhat Vu,Michael Beetz*

Main category: cs.RO

TL;DR: The paper introduces a unified scene graph model to standardize disparate scene description formats (like MJCF, URDF, SDF) into the USD format, enabling better semantic integration with robot ontologies for cognitive control.


<details>
  <summary>Details</summary>
Motivation: Robotics faces challenges in integrating diverse and incompatible environmental data formats into usable knowledge for cognitive control.

Method: They developed a unified scene graph model to convert scene description formats into the USD format, semantically annotate them, and integrate with robot ontologies.

Result: The method was proven effective by converting procedural 3D environments into USD format, semantically annotating them, and translating them into a knowledge graph to answer competency questions in real time.

Conclusion: The proposed approach facilitates cognitive robotic decision-making through format standardization, semantic integration, and visualization tools for 3D environments.

Abstract: In robotics, the effective integration of environmental data into actionable
knowledge remains a significant challenge due to the variety and
incompatibility of data formats commonly used in scene descriptions, such as
MJCF, URDF, and SDF. This paper presents a novel approach that addresses these
challenges by developing a unified scene graph model that standardizes these
varied formats into the Universal Scene Description (USD) format. This
standardization facilitates the integration of these scene graphs with robot
ontologies through semantic reporting, enabling the translation of complex
environmental data into actionable knowledge essential for cognitive robotic
control. We evaluated our approach by converting procedural 3D environments
into USD format, which is then annotated semantically and translated into a
knowledge graph to effectively answer competency questions, demonstrating its
utility for real-time robotic decision-making. Additionally, we developed a
web-based visualization tool to support the semantic mapping process, providing
users with an intuitive interface to manage the 3D environment.

</details>


### [247] [The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey](https://arxiv.org/abs/2507.11840)
*Gaofeng Li,Ruize Wang,Peisen Xu,Qi Ye,Jiming Chen*

Main category: cs.RO

TL;DR: This survey provides an overview of the evolution of robotic manipulation, focusing on recent advances in data collection and skill-learning frameworks for embodied dexterous manipulation, and identifies key challenges restricting its progress.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the central challenge in robotics: achieving human-like dexterous manipulation.

Method: It surveys the evolution of robotic manipulation and highlights advances in data collection methods (simulation, human demonstrations, teleoperation) and skill-learning frameworks (imitation and reinforcement learning).

Result: The survey identifies three key challenges that are hindering further development in dexterous robotic manipulation.

Conclusion: Advances in embodied intelligence and skill-learning frameworks are promising, but addressing the identified challenges is crucial for further progress in robotic manipulation.

Abstract: Achieving human-like dexterous robotic manipulation remains a central goal
and a pivotal challenge in robotics. The development of Artificial Intelligence
(AI) has allowed rapid progress in robotic manipulation. This survey summarizes
the evolution of robotic manipulation from mechanical programming to embodied
intelligence, alongside the transition from simple grippers to multi-fingered
dexterous hands, outlining key characteristics and main challenges. Focusing on
the current stage of embodied dexterous manipulation, we highlight recent
advances in two critical areas: dexterous manipulation data collection (via
simulation, human demonstrations, and teleoperation) and skill-learning
frameworks (imitation and reinforcement learning). Then, based on the overview
of the existing data collection paradigm and learning framework, three key
challenges restricting the development of dexterous robotic manipulation are
summarized and discussed.

</details>


### [248] [Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers](https://arxiv.org/abs/2507.11852)
*Mohammed Hassanin,Mohammad Abu Alsheikh,Carlos C. N. Kuhn,Damith Herath,Dinh Thai Hoang,Ibrahim Radwan*

Main category: cs.RO

TL;DR: This paper reviews autonomous riding (AR) systems for micromobility vehicles like e-scooters and e-bikes, identifies challenges, and draws parallels with autonomous driving (AD) to suggest future research directions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for reliable autonomous riding technologies for two-wheeled micromobility vehicles, emphasizing their importance given increasing adoption and safety concerns.

Method: The authors conduct a systematic review of AR system components—perception, planning, and control—while analyzing them through the lens of autonomous driving technologies.

Result: Critical gaps in AR research are identified, including insufficient perception systems, limited support, and lack of attention from the research community. Promising directions, such as multimodal sensors and edge deep learning, are highlighted.

Conclusion: The review synthesizes insights from AD to accelerate safe and efficient AR system development, advancing urban micromobility solutions.

Abstract: The rapid adoption of micromobility solutions, particularly two-wheeled
vehicles like e-scooters and e-bikes, has created an urgent need for reliable
autonomous riding (AR) technologies. While autonomous driving (AD) systems have
matured significantly, AR presents unique challenges due to the inherent
instability of two-wheeled platforms, limited size, limited power, and
unpredictable environments, which pose very serious concerns about road users'
safety. This review provides a comprehensive analysis of AR systems by
systematically examining their core components, perception, planning, and
control, through the lens of AD technologies. We identify critical gaps in
current AR research, including a lack of comprehensive perception systems for
various AR tasks, limited industry and government support for such
developments, and insufficient attention from the research community. The
review analyses the gaps of AR from the perspective of AD to highlight
promising research directions, such as multimodal sensor techniques for
lightweight platforms and edge deep learning architectures. By synthesising
insights from AD research with the specific requirements of AR, this review
aims to accelerate the development of safe, efficient, and scalable autonomous
riding systems for future urban mobility.

</details>


### [249] [A Fast Method for Planning All Optimal Homotopic Configurations for Tethered Robots and Its Extended Applications](https://arxiv.org/abs/2507.11880)
*Jinyuan Liu,Minglei Fu,Ling Shi,Chenguang Yang,Wenan Zhang*

Main category: cs.RO

TL;DR: This paper introduces CDT-TCS, a novel algorithm for optimizing path planning in tethered robots operating in environments with tether constraints, leveraging topology and geometry theories.


<details>
  <summary>Details</summary>
Motivation: Tethered robots face challenges in motion planning due to constraints like tether entanglement and length limitations, particularly in specialized environments that require stable power and communication.

Method: The paper employs CDT Encoding as a homotopy invariant combined with geometric optimization to compute feasible configurations for tethered robots in 2D spaces, introducing three application-specific algorithms for distinct scenarios.

Result: Extensive simulations and real-world experiments showed that the algorithms outperform existing methods, validating their engineering practicality.

Conclusion: CDT-TCS and its associated algorithms provide effective and scalable solutions for tethered robot path planning, advancing the field with rigorously proven methods and practical validations.

Abstract: Tethered robots play a pivotal role in specialized environments such as
disaster response and underground exploration, where their stable power supply
and reliable communication offer unparalleled advantages. However, their motion
planning is severely constrained by tether length limitations and entanglement
risks, posing significant challenges to achieving optimal path planning. To
address these challenges, this study introduces CDT-TCS (Convex Dissection
Topology-based Tethered Configuration Search), a novel algorithm that leverages
CDT Encoding as a homotopy invariant to represent topological states of paths.
By integrating algebraic topology with geometric optimization, CDT-TCS
efficiently computes the complete set of optimal feasible configurations for
tethered robots at all positions in 2D environments through a single
computation. Building on this foundation, we further propose three
application-specific algorithms: i) CDT-TPP for optimal tethered path planning,
ii) CDT-TMV for multi-goal visiting with tether constraints, iii) CDT-UTPP for
distance-optimal path planning of untethered robots. All theoretical results
and propositions underlying these algorithms are rigorously proven and
thoroughly discussed in this paper. Extensive simulations demonstrate that the
proposed algorithms significantly outperform state-of-the-art methods in their
respective problem domains. Furthermore, real-world experiments on robotic
platforms validate the practicality and engineering value of the proposed
framework.

</details>


### [250] [NemeSys: An Online Underwater Explorer with Goal-Driven Adaptive Autonomy](https://arxiv.org/abs/2507.11889)
*Adnan Abdullah,Alankrit Gupta,Vaishnav Ramesh,Shivali Patel,Md Jahidul Islam*

Main category: cs.RO

TL;DR: This paper introduces NemeSys, an advanced autonomous underwater vehicle (AUV) system capable of real-time mission reconfiguration in challenging underwater environments using innovative signaling methods.


<details>
  <summary>Details</summary>
Motivation: The need for adaptive and responsive AUV systems in GPS-denied and communication-limited underwater settings has motivated the exploration of alternatives to static pre-programmed missions and inefficient existing communication methods.

Method: NemeSys employs compact optical and magnetoelectric signaling via floating buoys for mission updates. It features real-time control architecture and a semantic mission encoding framework to enable interactive exploration and adaptive task updates.

Result: Validation of NemeSys was conducted through analytical modeling, experimental setups, and real-world open-water trials, demonstrating its capability for online mission adaptation and semantic updates.

Conclusion: The study establishes NemeSys as a feasible solution for adaptive autonomy in dynamic underwater environments, enhancing AUV performance and responsiveness in uncertain conditions.

Abstract: Adaptive mission control and dynamic parameter reconfiguration are essential
for autonomous underwater vehicles (AUVs) operating in GPS-denied,
communication-limited marine environments. However, most current AUV platforms
execute static, pre-programmed missions or rely on tethered connections and
high-latency acoustic channels for mid-mission updates, significantly limiting
their adaptability and responsiveness. In this paper, we introduce NemeSys, a
novel AUV system designed to support real-time mission reconfiguration through
compact optical and magnetoelectric (OME) signaling facilitated by floating
buoys. We present the full system design, control architecture, and a semantic
mission encoding framework that enables interactive exploration and task
adaptation via low-bandwidth communication. The proposed system is validated
through analytical modeling, controlled experimental evaluations, and
open-water trials. Results confirm the feasibility of online mission adaptation
and semantic task updates, highlighting NemeSys as an online AUV platform for
goal-driven adaptive autonomy in dynamic and uncertain underwater environments.

</details>


### [251] [Hybrid Conformal Prediction-based Risk-Aware Model Predictive Planning in Dense, Uncertain Environments](https://arxiv.org/abs/2507.11920)
*Jeongyong Yang,KwangBin Lee,SooJean Han*

Main category: cs.RO

TL;DR: HyPRAP combines diverse prediction methods for efficient, risk-aware real-time path planning in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of real-time path planning in dense, uncertain environments with numerous dynamic obstacles.

Method: The HyPRAP framework uses hybrid models and a Prediction-based Collision Risk Index (P-CRI) to prioritize high-risk obstacles and incorporates hybrid conformal prediction for uncertainty quantification.

Result: Theoretical analysis and extensive simulations show HyPRAP improves both safety and computational efficiency and outperforms single predictor methods and naive risk assessments.

Conclusion: HyPRAP is suitable for dense environments, offering a balanced solution that combines risk awareness and computational efficacy.

Abstract: Real-time path planning in dense, uncertain environments remains a
challenging problem, as predicting the future motions of numerous dynamic
obstacles is computationally burdensome and unrealistic. To address this, we
introduce Hybrid Prediction-based Risk-Aware Planning (HyPRAP), a
prediction-based risk-aware path-planning framework which uses a hybrid
combination of models to predict local obstacle movement. HyPRAP uses a novel
Prediction-based Collision Risk Index (P-CRI) to evaluate the risk posed by
each obstacle, enabling the selective use of predictors based on whether the
agent prioritizes high predictive accuracy or low computational prediction
overhead. This selective routing enables the agent to focus on high-risk
obstacles while ignoring or simplifying low-risk ones, making it suitable for
environments with a large number of obstacles. Moreover, HyPRAP incorporates
uncertainty quantification through hybrid conformal prediction by deriving
confidence bounds simultaneously achieved by multiple predictions across
different models. Theoretical analysis demonstrates that HyPRAP effectively
balances safety and computational efficiency by leveraging the diversity of
prediction models. Extensive simulations validate these insights for more
general settings, confirming that HyPRAP performs better compared to single
predictor methods, and P-CRI performs better over naive proximity-based risk
assessment.

</details>


### [252] [A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning](https://arxiv.org/abs/2507.11938)
*Hao Chen,Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: The study presents a method using similarity matching for robotic grasping of unknown objects from a single view, improving robustness against noise compared to learning-based approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of learning-based approaches like GraspNet-1Billion, which struggle with robustness to sensing noise and environmental changes in grasping unknown objects.

Method: Utilize a similarity matching approach with three key steps: (1) Match visual features of observed objects to a database to find similar models. (2) Plan grasps using pre-known knowledge of matched models. (3) Fine-tune grasp quality locally. It integrates semantic, geometric, and dimensional features and introduces innovations like the C-FPFH descriptor and a plane-detection-based registration technique.

Result: The proposed method demonstrated robust single-view unknown-object grasping, minimizing the effects of noisy and partial observations via multi-level similarity matching and novel descriptors.

Conclusion: The paper provides an alternative to traditional deep learning approaches by introducing similarity matching for unknown-object grasping, showing robust outcomes under challenging observational conditions.

Abstract: Grasping unknown objects from a single view has remained a challenging topic
in robotics due to the uncertainty of partial observation. Recent advances in
large-scale models have led to benchmark solutions such as GraspNet-1Billion.
However, such learning-based approaches still face a critical limitation in
performance robustness for their sensitivity to sensing noise and environmental
changes. To address this bottleneck in achieving highly generalized grasping,
we abandon the traditional learning framework and introduce a new perspective:
similarity matching, where similar known objects are utilized to guide the
grasping of unknown target objects. We newly propose a method that robustly
achieves unknown-object grasping from a single viewpoint through three key
steps: 1) Leverage the visual features of the observed object to perform
similarity matching with an existing database containing various object models,
identifying potential candidates with high similarity; 2) Use the candidate
models with pre-existing grasping knowledge to plan imitative grasps for the
unknown target object; 3) Optimize the grasp quality through a local
fine-tuning process. To address the uncertainty caused by partial and noisy
observation, we propose a multi-level similarity matching framework that
integrates semantic, geometric, and dimensional features for comprehensive
evaluation. Especially, we introduce a novel point cloud geometric descriptor,
the C-FPFH descriptor, which facilitates accurate similarity assessment between
partial point clouds of observed objects and complete point clouds of database
models. In addition, we incorporate the use of large language models, introduce
the semi-oriented bounding box, and develop a novel point cloud registration
approach based on plane detection to enhance matching accuracy under
single-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.

</details>


### [253] [IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving](https://arxiv.org/abs/2507.11940)
*Kanghyun Ryu,Minjun Sung,Piyush Gupta,Jovin D'sa,Faizan M. Tariq,David Isele,Sangjae Bae*

Main category: cs.RO

TL;DR: The paper introduces IANN-MPPI control for autonomous vehicles, enhancing motion planning by incorporating interactions with surrounding agents in dense traffic.


<details>
  <summary>Details</summary>
Motivation: Motion planning in dense traffic often leads to overly conservative performance due to the inability of AVs to anticipate and respond to surrounding agents' behaviors.

Method: The proposed IANN-MPPI combines interaction-aware prediction with Model Predictive Path Integral control, sampling control sequences and anticipating reactions of surrounding agents.

Result: IANN-MPPI demonstrates efficient merging maneuvers in dense traffic scenarios, particularly aided by a spline-based prior for lane-changing behavior.

Conclusion: The integration of interaction-aware predictive modeling offers significant improvements in trajectory planning for AVs in dense traffic conditions.

Abstract: Motion planning for autonomous vehicles (AVs) in dense traffic is
challenging, often leading to overly conservative behavior and unmet planning
objectives. This challenge stems from the AVs' limited ability to anticipate
and respond to the interactive behavior of surrounding agents. Traditional
decoupled prediction and planning pipelines rely on non-interactive predictions
that overlook the fact that agents often adapt their behavior in response to
the AV's actions. To address this, we propose Interaction-Aware Neural
Network-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which
enables interactive trajectory planning by predicting how surrounding agents
may react to each control sequence sampled by MPPI. To improve performance in
structured lane environments, we introduce a spline-based prior for the MPPI
sampling distribution, enabling efficient lane-changing behavior. We evaluate
IANN-MPPI in a dense traffic merging scenario, demonstrating its ability to
perform efficient merging maneuvers. Our project website is available at
https://sites.google.com/berkeley.edu/iann-mppi

</details>


### [254] [A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming](https://arxiv.org/abs/2507.11974)
*Waseem Akram,Muhayy Ud Din,Lyes Saad Soud,Irfan Hussain*

Main category: cs.RO

TL;DR: This paper reviews the applications of Generative Artificial Intelligence (GAI) in aquaculture, highlighting its transformative impact on smarter and adaptive decision-making while discussing its limitations.


<details>
  <summary>Details</summary>
Motivation: To explore how GAI can revolutionize aquaculture under the Aquaculture 4.0 paradigm by enabling smarter decision-making and digital integration.

Method: A review of foundational architectures, experimental systems, pilot deployments, and real-world use cases of GAI in aquaculture. An updated taxonomy of GAI applications is provided, along with a discussion of technical and regulatory challenges.

Result: GAI's role in aquaculture is analyzed, particularly in underwater perception, digital twins, autonomous ROV planning, and across tasks like sensing, control, optimization, and more. Key limitations like data and trust issues are also identified.

Conclusion: GAI is highlighted as a critical enabler for smart and environmentally aligned aquaculture, with both opportunities and challenges to address.

Abstract: Generative Artificial Intelligence (GAI) has rapidly emerged as a
transformative force in aquaculture, enabling intelligent synthesis of
multimodal data, including text, images, audio, and simulation outputs for
smarter, more adaptive decision-making. As the aquaculture industry shifts
toward data-driven, automation and digital integration operations under the
Aquaculture 4.0 paradigm, GAI models offer novel opportunities across
environmental monitoring, robotics, disease diagnostics, infrastructure
planning, reporting, and market analysis. This review presents the first
comprehensive synthesis of GAI applications in aquaculture, encompassing
foundational architectures (e.g., diffusion models, transformers, and retrieval
augmented generation), experimental systems, pilot deployments, and real-world
use cases. We highlight GAI's growing role in enabling underwater perception,
digital twin modeling, and autonomous planning for remotely operated vehicle
(ROV) missions. We also provide an updated application taxonomy that spans
sensing, control, optimization, communication, and regulatory compliance.
Beyond technical capabilities, we analyze key limitations, including limited
data availability, real-time performance constraints, trust and explainability,
environmental costs, and regulatory uncertainty. This review positions GAI not
merely as a tool but as a critical enabler of smart, resilient, and
environmentally aligned aquaculture systems.

</details>


### [255] [Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers](https://arxiv.org/abs/2507.11991)
*Juanran Wang,Marc R. Schlichting,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: The paper develops deep generative models for autonomous vehicles to anticipate and mitigate collision risks at intersections.


<details>
  <summary>Details</summary>
Motivation: Improve autonomous vehicle safety in high-risk traffic zones like intersections using advanced generative models to account for contextual hazards.

Method: A denoising diffusion probabilistic model with 1000 steps trains to generate collision-causing noise scenarios, which is distilled into a faster single-step model for decision-making.

Result: The single-step model enables robust planners to anticipate and navigate potential failure cases, achieving lower failure and delay rates compared to baseline controllers.

Conclusion: Deep generative modeling provides an effective avenue to advance autonomous vehicle behaviors in challenging intersection contexts, demonstrating safety and efficiency improvements.

Abstract: High-risk traffic zones such as intersections are a major cause of
collisions. This study leverages deep generative models to enhance the safety
of autonomous vehicles in an intersection context. We train a 1000-step
denoising diffusion probabilistic model to generate collision-causing sensor
noise sequences for an autonomous vehicle navigating a four-way intersection
based on the current relative position and velocity of an intruder. Using the
generative adversarial architecture, the 1000-step model is distilled into a
single-step denoising diffusion model which demonstrates fast inference speed
while maintaining similar sampling quality. We demonstrate one possible
application of the single-step model in building a robust planner for the
autonomous vehicle. The planner uses the single-step model to efficiently
sample potential failure cases based on the currently measured traffic state to
inform its decision-making. Through simulation experiments, the robust planner
demonstrates significantly lower failure rate and delay rate compared with the
baseline Intelligent Driver Model controller.

</details>


### [256] [Robust Route Planning for Sidewalk Delivery Robots](https://arxiv.org/abs/2507.12067)
*Xing Tong,Michele D. Simoni*

Main category: cs.RO

TL;DR: The study focuses on improving route planning for sidewalk delivery robots under conditions of travel time uncertainty, employing robust optimization approaches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency and unreliability of sidewalk delivery robots caused by variable sidewalk conditions, pedestrian density, and obstacles.

Method: It combines optimization and simulation to model travel times and evaluates routing strategies based on uncertainty sets (budgeted, ellipsoidal, SVC-based) and a distributionally robust shortest path (DRSP) method. A case study in Stockholm is employed.

Result: Robust routing methods, particularly Ellipsoidal and DRSP approaches, showed significant improvements in operational reliability and delay reduction under varying sidewalk conditions.

Conclusion: Robust routing approaches are superior to conventional strategies for sidewalk delivery robots, especially under challenging conditions such as adverse weather or high pedestrian congestion.

Abstract: Sidewalk delivery robots are a promising solution for urban freight
distribution, reducing congestion compared to trucks and providing a safer,
higher-capacity alternative to drones. However, unreliable travel times on
sidewalks due to pedestrian density, obstacles, and varying infrastructure
conditions can significantly affect their efficiency. This study addresses the
robust route planning problem for sidewalk robots, explicitly accounting for
travel time uncertainty due to varying sidewalk conditions. Optimization is
integrated with simulation to reproduce the effect of obstacles and pedestrian
flows and generate realistic travel times. The study investigates three
different approaches to derive uncertainty sets, including budgeted,
ellipsoidal, and support vector clustering (SVC)-based methods, along with a
distributionally robust method to solve the shortest path (SP) problem. A
realistic case study reproducing pedestrian patterns in Stockholm's city center
is used to evaluate the efficiency of robust routing across various robot
designs and environmental conditions. The results show that, when compared to a
conventional SP, robust routing significantly enhances operational reliability
under variable sidewalk conditions. The Ellipsoidal and DRSP approaches
outperform the other methods, yielding the most efficient paths in terms of
average and worst-case delay. Sensitivity analyses reveal that robust
approaches consistently outperform the conventional SP, particularly for
sidewalk delivery robots that are wider, slower, and have more conservative
navigation behaviors. These benefits are even more pronounced in adverse
weather conditions and high pedestrian congestion scenarios.

</details>


### [257] [Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards](https://arxiv.org/abs/2507.12093)
*David Rapado-Rincon,Gert Kootstra*

Main category: cs.RO

TL;DR: Tree-SLAM is a semantic SLAM approach that uses RGB-D images and a cascade-graph method to map individual trees in orchards, achieving high accuracy despite unreliable GPS signals.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of creating accurate maps of individual trees in orchards, a task crucial for precision agriculture and autonomous robot operations. Standard methods fail due to unreliable GPS signals and repetitive tree appearances.

Method: Tree-SLAM employs RGB-D images to detect tree trunks using instance segmentation. It estimates tree locations and re-identifies them using a cascade-graph-based data association algorithm. These re-identifications are integrated with GPS, odometry, and tree observations in a factor graph framework.

Result: The system achieves a geo-localization error of 18 cm, less than 20% of tree planting distance. Tested across diverse scenarios, it demonstrates high accuracy and robustness, even with poor GPS signals.

Conclusion: Tree-SLAM offers a reliable and robust solution for mapping individual trees in orchards, improving precision agricultural tasks like targeted operations and monitoring, especially in challenging GPS-denied environments.

Abstract: Accurate mapping of individual trees is an important component for precision
agriculture in orchards, as it allows autonomous robots to perform tasks like
targeted operations or individual tree monitoring. However, creating these maps
is challenging because GPS signals are often unreliable under dense tree
canopies. Furthermore, standard Simultaneous Localization and Mapping (SLAM)
approaches struggle in orchards because the repetitive appearance of trees can
confuse the system, leading to mapping errors. To address this, we introduce
Tree-SLAM, a semantic SLAM approach tailored for creating maps of individual
trees in orchards. Utilizing RGB-D images, our method detects tree trunks with
an instance segmentation model, estimates their location and re-identifies them
using a cascade-graph-based data association algorithm. These re-identified
trunks serve as landmarks in a factor graph framework that integrates noisy GPS
signals, odometry, and trunk observations. The system produces maps of
individual trees with a geo-localization error as low as 18 cm, which is less
than 20\% of the planting distance. The proposed method was validated on
diverse datasets from apple and pear orchards across different seasons,
demonstrating high mapping accuracy and robustness in scenarios with unreliable
GPS signals.

</details>


### [258] [Leveraging Sidewalk Robots for Walkability-Related Analyses](https://arxiv.org/abs/2507.12148)
*Xing Tong,Michele D. Simoni,Kaj Munhoz Arfvidsson,Jonas Mårtensson*

Main category: cs.RO

TL;DR: This paper proposes using sensor-equipped sidewalk delivery robots for scalable, automated real-time data collection to assess walkability, analyzing sidewalk features and pedestrian behavior.


<details>
  <summary>Details</summary>
Motivation: Gathering detailed walkability-related data for urban development is challenging due to the cost and limited scalability of traditional methods.

Method: A sensor-equipped robot was deployed on Stockholm's sidewalk network, completing 101 trips and collecting data on sidewalk conditions, utilization, and pedestrian dynamics.

Result: Pedestrian movement is influenced by characteristics like width and surface irregularities; robot speed mirrors pedestrian behavior, indicating its utility as a proxy.

Conclusion: The study establishes that sidewalk delivery robots can continuously and effectively monitor sidewalk conditions and pedestrian patterns, contributing to the creation of better urban environments.

Abstract: Walkability is a key component of sustainable urban development, while
collecting detailed data on its related features remains challenging due to the
high costs and limited scalability of traditional methods. Sidewalk delivery
robots, increasingly deployed in urban environments, offer a promising solution
to these limitations. This paper explores how these robots can serve as mobile
data collection platforms, capturing sidewalk-level features related to
walkability in a scalable, automated, and real-time manner. A sensor-equipped
robot was deployed on a sidewalk network at KTH in Stockholm, completing 101
trips covering 900 segments. From the collected data, different typologies of
features are derived, including robot trip characteristics (e.g., speed,
duration), sidewalk conditions (e.g., width, surface unevenness), and sidewalk
utilization (e.g., pedestrian density). Their walkability-related implications
were investigated with a series of analyses. The results demonstrate that
pedestrian movement patterns are strongly influenced by sidewalk
characteristics, with higher density, reduced width, and surface irregularity
associated with slower and more variable trajectories. Notably, robot speed
closely mirrors pedestrian behavior, highlighting its potential as a proxy for
assessing pedestrian dynamics. The proposed framework enables continuous
monitoring of sidewalk conditions and pedestrian behavior, contributing to the
development of more walkable, inclusive, and responsive urban environments.

</details>


### [259] [Probabilistic Safety Verification for an Autonomous Ground Vehicle: A Situation Coverage Grid Approach](https://arxiv.org/abs/2507.12158)
*Nawshin Mannan Proma,Gricel Vázquez,Sepeedeh Shahbeigi,Arjun Badyal,Victoria Hodge*

Main category: cs.RO

TL;DR: The paper proposes a novel safety verification approach for autonomous ground vehicles combining systematic situation extraction, probabilistic modeling, and formal verification.


<details>
  <summary>Details</summary>
Motivation: Autonomous ground vehicles are used in safety-critical environments, necessitating rigorous safety verification for their deployment under diverse conditions.

Method: The approach utilizes situation coverage grids, probabilistic data collection, and generation of probabilistic models to encode system behavior and applies probabilistic model checking against temporal logic-based safety properties.

Result: This approach identifies high-risk scenarios, delivers quantitative safety assurances, and aligns with regulatory standards.

Conclusion: The presented methodology enhances the reliability and compliance of autonomous systems in critical applications.

Abstract: As industrial autonomous ground vehicles are increasingly deployed in
safety-critical environments, ensuring their safe operation under diverse
conditions is paramount. This paper presents a novel approach for their safety
verification based on systematic situation extraction, probabilistic modelling
and verification. We build upon the concept of a situation coverage grid, which
exhaustively enumerates environmental configurations relevant to the vehicle's
operation. This grid is augmented with quantitative probabilistic data
collected from situation-based system testing, capturing probabilistic
transitions between situations. We then generate a probabilistic model that
encodes the dynamics of both normal and unsafe system behaviour. Safety
properties extracted from hazard analysis and formalised in temporal logic are
verified through probabilistic model checking against this model. The results
demonstrate that our approach effectively identifies high-risk situations,
provides quantitative safety guarantees, and supports compliance with
regulatory standards, thereby contributing to the robust deployment of
autonomous systems.

</details>


### [260] [Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties](https://arxiv.org/abs/2507.12174)
*Zhenmin Huang,Yusen Xie,Benshan Ma,Shaojie Shen,Jun Ma*

Main category: cs.RO

TL;DR: The paper presents a game-theoretic interactive trajectory planning method to address intentional uncertainties in multi-agent interactions. It uses a novel framework to ensure efficiency and scalability, verified through simulations and experiments.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenges of trajectory planning in robotics involving multiple agents, particularly under intentional uncertainties and the inefficiencies of current game-theoretic methods.

Method: The authors model multi-agent interactions as a general Bayesian game, represented as a potential game under minor assumptions, and solve for Bayesian Nash equilibrium via a unified optimization problem using a distributed algorithm based on dual consensus ADMM.

Result: The proposed method effectively handles scenarios with intentional uncertainties, demonstrating better scalability compared to centralized and decentralized baselines, and enables real-time trajectory planning.

Conclusion: The method proves to be both efficient and scalable for multi-agent trajectory planning, significantly improving performance in uncertain game-theoretic settings.

Abstract: Trajectory planning involving multi-agent interactions has been a
long-standing challenge in the field of robotics, primarily burdened by the
inherent yet intricate interactions among agents. While game-theoretic methods
are widely acknowledged for their effectiveness in managing multi-agent
interactions, significant impediments persist when it comes to accommodating
the intentional uncertainties of agents. In the context of intentional
uncertainties, the heavy computational burdens associated with existing
game-theoretic methods are induced, leading to inefficiencies and poor
scalability. In this paper, we propose a novel game-theoretic interactive
trajectory planning method to effectively address the intentional uncertainties
of agents, and it demonstrates both high efficiency and enhanced scalability.
As the underpinning basis, we model the interactions between agents under
intentional uncertainties as a general Bayesian game, and we show that its
agent-form equivalence can be represented as a potential game under certain
minor assumptions. The existence and attainability of the optimal interactive
trajectories are illustrated, as the corresponding Bayesian Nash equilibrium
can be attained by optimizing a unified optimization problem. Additionally, we
present a distributed algorithm based on the dual consensus alternating
direction method of multipliers (ADMM) tailored to the parallel solving of the
problem, thereby significantly improving the scalability. The attendant
outcomes from simulations and experiments demonstrate that the proposed method
is effective across a range of scenarios characterized by general forms of
intentional uncertainties. Its scalability surpasses that of existing
centralized and decentralized baselines, allowing for real-time interactive
trajectory planning in uncertain game settings.

</details>


### [261] [UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization](https://arxiv.org/abs/2507.12194)
*Hongming Shen,Xun Chen,Yulin Hui,Zhenyu Wu,Wei Wang,Qiyang Lyu,Tianchen Deng,Danwei Wang*

Main category: cs.RO

TL;DR: The paper introduces UniLGL, a uniform method for LiDAR-based localization and mapping, achieving spatial, material, and sensor-type uniformity using innovative approaches.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-based localization (LGL) methods often neglect uniformity by focusing on partial information (e.g., geometric features) or being restricted to homogeneous LiDAR sensors.

Method: The method proposes encoding the full point cloud into spatial and intensity BEV images and employs a multi-BEV fusion network for feature extraction. A viewpoint invariance hypothesis is introduced to ensure compatibility across various LiDAR sensors.

Result: Extensive benchmarks in real-world environments show UniLGL outperforms State-of-the-Art methods in localization and mapping. It is successfully deployed on trucks and MAVs for industrial and field use cases.

Conclusion: UniLGL achieves robust, uniform, and high-precision localization across diverse environments, proving its effectiveness for both industrial and field applications.

Abstract: Existing LGL methods typically consider only partial information (e.g.,
geometric features) from LiDAR observations or are designed for homogeneous
LiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL
method is proposed, termed UniLGL, which simultaneously achieves spatial and
material uniformity, as well as sensor-type uniformity. The key idea of the
proposed method is to encode the complete point cloud, which contains both
geometric and material information, into a pair of BEV images (i.e., a spatial
BEV image and an intensity BEV image). An end-to-end multi-BEV fusion network
is designed to extract uniform features, equipping UniLGL with spatial and
material uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a
viewpoint invariance hypothesis is introduced, which replaces the conventional
translation equivariance assumption commonly used in existing LPR networks and
supervises UniLGL to achieve sensor-type uniformity in both global descriptors
and local feature representations. Finally, based on the mapping between local
features on the 2D BEV image and the point cloud, a robust global pose
estimator is derived that determines the global minimum of the global pose on
SE(3) without requiring additional registration. To validate the effectiveness
of the proposed uniform LGL, extensive benchmarks are conducted in real-world
environments, and the results show that the proposed UniLGL is demonstratively
competitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL
has been deployed on diverse platforms, including full-size trucks and agile
Micro Aerial Vehicles (MAVs), to enable high-precision localization and mapping
as well as multi-MAV collaborative exploration in port and forest environments,
demonstrating the applicability of UniLGL in industrial and field scenarios.

</details>


### [262] [Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot](https://arxiv.org/abs/2507.12273)
*Luca Garello,Francesca Cocchella,Alessandra Sciutti,Manuel Catalano,Francesco Rea*

Main category: cs.RO

TL;DR: The paper introduces Alter-Ego, an autonomous museum guide robot powered by advanced technologies such as LLMs and SLAM for interactive and navigational capabilities. Tested in a museum with 34 participants, the robot enhanced user experiences but exhibited some limitations.


<details>
  <summary>Details</summary>
Motivation: To improve user experiences in cultural and educational spaces using autonomous robots capable of navigation and real-time interaction.

Method: The robot, Alter-Ego, uses Large Language Models for Q&A interactions and SLAM for navigation. It was tested in a museum with 34 participants, incorporating qualitative and quantitative analyses of interactions and surveys.

Result: Alter-Ego was well-received by participants, enhancing user engagement in museum spaces. However, certain limitations in the robot’s comprehension and responsiveness were identified.

Conclusion: This study demonstrates the potential of AI-driven robotics in enhancing accessibility and education in cultural environments while also outlining the current challenges of deploying such systems in real-world scenarios.

Abstract: Autonomous robots are increasingly being tested into public spaces to enhance
user experiences, particularly in cultural and educational settings. This paper
presents the design, implementation, and evaluation of the autonomous museum
guide robot Alter-Ego equipped with advanced navigation and interactive
capabilities. The robot leverages state-of-the-art Large Language Models (LLMs)
to provide real-time, context aware question-and-answer (Q&A) interactions,
allowing visitors to engage in conversations about exhibits. It also employs
robust simultaneous localization and mapping (SLAM) techniques, enabling
seamless navigation through museum spaces and route adaptation based on user
requests. The system was tested in a real museum environment with 34
participants, combining qualitative analysis of visitor-robot conversations and
quantitative analysis of pre and post interaction surveys. Results showed that
the robot was generally well-received and contributed to an engaging museum
experience, despite some limitations in comprehension and responsiveness. This
study sheds light on HRI in cultural spaces, highlighting not only the
potential of AI-driven robotics to support accessibility and knowledge
acquisition, but also the current limitations and challenges of deploying such
technologies in complex, real-world environments.

</details>


### [263] [Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning](https://arxiv.org/abs/2507.12391)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: Large Language Models (LLMs) show potential in robotic path planning but struggle with scalability and spatial reasoning, especially in complex environments.


<details>
  <summary>Details</summary>
Motivation: Investigate how visual inputs enhance multimodal LLMs' capabilities in robotic path planning tasks.

Method: Benchmarking 15 multimodal LLMs' performance on 2D grid environments, comparing text-only and text-plus-visual inputs.

Result: Success rates were moderate for simpler grids with visual input showing some benefits; performance degraded on larger grids, scalability issues were prominent.

Conclusion: Current LLMs face challenges with spatial reasoning and scalability, indicating areas for improvement in multimodal robotic planning applications.

Abstract: Large Language Models (LLMs) show potential for enhancing robotic path
planning. This paper assesses visual input's utility for multimodal LLMs in
such tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on
generating valid and optimal paths in 2D grid environments, simulating
simplified robotic planning, comparing text-only versus text-plus-visual inputs
across varying model sizes and grid complexities. Our results indicate moderate
success rates on simpler small grids, where visual input or few-shot text
prompting offered some benefits. However, performance significantly degraded on
larger grids, highlighting a scalability challenge. While larger models
generally achieved higher average success, the visual modality was not
universally dominant over well-structured text for these multimodal systems,
and successful paths on simpler grids were generally of high quality. These
results indicate current limitations in robust spatial reasoning, constraint
adherence, and scalable multimodal integration, identifying areas for future
LLM development in robotic path planning.

</details>


### [264] [Regrasp Maps for Sequential Manipulation Planning](https://arxiv.org/abs/2507.12407)
*Svetlana Levit,Marc Toussaint*

Main category: cs.RO

TL;DR: The paper proposes enhancing a task and motion planning (TAMP) solver for regrasp manipulation problems in cluttered settings using regrasp maps.


<details>
  <summary>Details</summary>
Motivation: To address challenges in manipulation tasks requiring multiple reglaps in unknown, cluttered environments by improving the efficiency and robustness of current TAMP solvers.

Method: Utilize regrasp maps—state space abstractions for grasps in configuration space—to provide guesses for mode switches and constraints, iteratively refining these maps based on failures to guide the TAMP solver.

Result: The proposed approach improves the solver's ability to address challenging regrasp problems through robust search and iterative refinement of regrasp maps.

Conclusion: Incorporating regrasp maps into TAMP solvers enhances efficiency in solving complex multi-regrasp problems in constrained environments.

Abstract: We consider manipulation problems in constrained and cluttered settings,
which require several regrasps at unknown locations. We propose to inform an
optimization-based task and motion planning (TAMP) solver with possible regrasp
areas and grasp sequences to speed up the search. Our main idea is to use a
state space abstraction, a regrasp map, capturing the combinations of available
grasps in different parts of the configuration space, and allowing us to
provide the solver with guesses for the mode switches and additional
constraints for the object placements. By interleaving the creation of regrasp
maps, their adaptation based on failed refinements, and solving TAMP
(sub)problems, we are able to provide a robust search method for challenging
regrasp manipulation problems.

</details>


### [265] [Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement](https://arxiv.org/abs/2507.12431)
*Connor Burgess,Kyle Douin,Amir Kordijazi*

Main category: cs.RO

TL;DR: The paper introduces the Automated Contact Angle Tester (ACAT), a robotic system designed for measuring surface wettability on 3D-printed materials with precision and repeatability.


<details>
  <summary>Details</summary>
Motivation: To address limitations in manual contact angle testing by providing an automated, high-precision, and safe solution.

Method: The ACAT integrates a software-hardware architecture, including three core subsystems: an electrical system adhering to industrial standards, a Python-based software control system on a Raspberry Pi, and a mechanical 3-axis Cartesian robot with pneumatic actuation, all enclosed in a safety-certified frame.

Result: The ACAT system streamlines and automates surface wettability measurement while being ready for integration into larger smart manufacturing workflows.

Conclusion: ACAT is a robust, repeatable, and precise platform that paves the way for high-throughput automated material testing and future applications in manufacturing and material science workflows.

Abstract: The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work
cell developed to automate the measurement of surface wettability on 3D-printed
materials. Designed for precision, repeatability, and safety, ACAT addresses
the limitations of manual contact angle testing by combining programmable
robotics, precise liquid dispensing, and a modular software-hardware
architecture. The system is composed of three core subsystems: (1) an
electrical system including power, control, and safety circuits compliant with
industrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software
control system based on a Raspberry Pi and Python, featuring fault detection,
GPIO logic, and operator interfaces; and (3) a mechanical system that includes
a 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser
enclosed within a safety-certified frame. The ACAT enables high-throughput,
automated surface characterization and provides a robust platform for future
integration into smart manufacturing and materials discovery workflows. This
paper details the design methodology, implementation strategies, and system
integration required to develop the ACAT platform.

</details>


### [266] [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/abs/2507.12440)
*Ruihan Yang,Qinxi Yu,Yecheng Wu,Rui Yan,Borui Li,An-Chieh Cheng,Xueyan Zou,Yunhao Fang,Hongxu Yin,Sifei Liu,Song Han,Yao Lu,Xiaolong Wang*

Main category: cs.RO

TL;DR: This paper presents the use of egocentric human videos to train Vision-Language-Action (VLA) models for robotic manipulation, addressing limitations in robot data collection.


<details>
  <summary>Details</summary>
Motivation: The constraints of requiring robot hardware limit the scalability of data collection for imitation learning in robotics.

Method: The authors use egocentric human videos to train VLAs, predict human wrist and hand actions, and convert them into robot actions through inverse kinematics and retargeting. Fine-tuning with robot demonstrations further refines the model.

Result: The proposed EgoVLA, when evaluated on the Isaac Humanoid Manipulation Benchmark, demonstrates significant improvements over other methods and highlights the importance of human data for training.

Conclusion: Egocentric human videos provide scalable and rich data to train VLA models for robotic manipulation tasks, offering promising improvements in performance when combined with targeted robot demonstrations.

Abstract: Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [267] [Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems](https://arxiv.org/abs/2507.11671)
*Mst Shamima Aktar,Peng Liang,Muhammad Waseem,Amjed Tahir,Mojtaba Shahin,Muhammad Azeem Akbar,Arif Ali Khan,Aakash Ahmad,Musengamana Jean de Dieu,Ruiyin Li*

Main category: cs.SE

TL;DR: This paper proposes decision models to help quantum software developers select architectural patterns in six critical design areas to address challenges in quantum software systems design.


<details>
  <summary>Details</summary>
Motivation: Quantum software systems face complexity and lack of guidelines in selecting suitable architectural patterns and strategies.

Method: Decision models for six design areas were developed using data from a mining study (GitHub and Stack Exchange), literature review, and feedback from interviews with 16 quantum software practitioners.

Result: The proposed decision models were found to be effective in helping practitioners address design challenges, as per feedback from the evaluations.

Conclusion: The study provides tools to improve architecture design in quantum software systems and encourages further use and development in the community.

Abstract: Quantum software represents disruptive technologies in terms of
quantum-specific software systems, services, and applications - leverage the
principles of quantum mechanics via programmable quantum bits (Qubits) that
manipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.
Quantum software architecture enables quantum software developers to abstract
away implementation-specific details (i.e., mapping of Qubits and QuGates to
high-level architectural components and connectors). Architectural patterns and
strategies can provide reusable knowledge and best practices to engineer
quantum software systems effectively and efficiently. However, quantum software
practitioners face significant challenges in selecting and implementing
appropriate patterns and strategies due to the complexity of quantum software
systems and the lack of guidelines. To address these challenges, this study
proposes decision models for selecting patterns and strategies in six critical
design areas in quantum software systems: Communication, Decomposition, Data
Processing, Fault Tolerance, Integration and Optimization, and Algorithm
Implementation. These decision models are constructed based on data collected
from both a mining study (i.e., GitHub and Stack Exchange) and a Systematic
Literature Review, which were used to identify relevant patterns and strategies
with their involved Quality Attributes (QAs). We then conducted semi-structured
interviews with 16 quantum software practitioners to evaluate the familiarity,
understandability, completeness, and usefulness of the proposed decision
models. The results show that the proposed decision models can aid
practitioners in selecting suitable patterns and strategies to address the
challenges related to the architecture design of quantum software systems. The
dataset is available at [6], allowing the community to reproduce and build upon
our findings.

</details>


### [268] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint improves code quality analysis for Large Language Models (LLMs) by using instruction tuning on synthetic data, achieving strong generalization and competitive results compared to larger models.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with analyzing code quality due to limitations in adapting to evolving coding standards and practices.

Method: MetaLint formulates code quality tasks as detecting and fixing problematic code based on high-level specifications, leveraging instruction tuning on synthetic linter-generated data.

Result: MetaLint achieves 70.37% F-score in idiom detection and competitive localization results, demonstrating strong generalization and adaptability.

Conclusion: MetaLint offers a promising framework for adaptive and future-proof code quality analysis, competitive even against larger state-of-the-art models.

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [269] [REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps](https://arxiv.org/abs/2507.11689)
*Sergio Di Meglio,Valeria Pontillo,Luigi Libero Lucio Starace*

Main category: cs.SE

TL;DR: Analysis of 40 web applications found frequent violations of REST API design conventions, emphasizing the need for better API education in CS programs.


<details>
  <summary>Details</summary>
Motivation: To understand the typical quality of student code and improve education and hiring practices by analyzing how students adhere to API design rules.

Method: Analyzed 40 student-developed full-stack web applications using an automated static analysis to assess REST API design rule adherence.

Result: Found frequent violations such as missing hyphens (98%), incorrect pluralization (88%), and HTTP method misuse (83%).

Conclusion: Focused instruction on API design and the use of automated tools can improve student code quality and better prepare them for industry requirements.

Abstract: In Computer Science Bachelor's programs, software quality is often
underemphasized due to limited time and a focus on foundational skills, leaving
many students unprepared for industry expectations. To better understand the
typical quality of student code and inform both education and hiring practices,
we analyze 40 full-stack web applications developed in a third-year Web
Technologies course. Using an automated static analysis pipeline, we assess
adherence to REST API design rules. Results reveal frequent violations of
foundational conventions, such as missing hyphens in endpoint paths (98%),
incorrect pluralization (88%), and misuse of HTTP methods (83%). These findings
highlight the need for more focused instruction on API design and support the
adoption of automated tools to improve code quality in student projects.

</details>


### [270] [Extremal Testing for Network Software using LLMs](https://arxiv.org/abs/2507.11898)
*Rathin Singha,Harry Qian,Srinath Saikrishnan,Tracy Zhao,Ryan Beckett,Siva Kesava Reddy Kakarla,George Varghese*

Main category: cs.SE

TL;DR: The paper explores using large language models (LLMs) to automate extremal testing for network software, revealing bugs in implementations like HTTP, BGP, and DNS.


<details>
  <summary>Details</summary>
Motivation: The authors aim to automate the manual process of extremal testing in network software, which physicists often do manually, to uncover bugs and improve efficiency.

Method: They utilize LLMs to first generate input constraints and then create tests to violate these limits. Experiments were conducted on network protocols (e.g., HTTP, BGP, DNS), and centralized algorithms like shortest path computations.

Result: New bugs were discovered in HTTP, BGP, and DNS implementations. LLMs also effectively extended the methodology to other network software and generated filtering codes for invalid inputs.

Conclusion: LLM-driven extremal testing improves upon traditional Boundary Value Analysis, and the authors suggest using agentic AI for further automation.

Abstract: Physicists often manually consider extreme cases when testing a theory. In
this paper, we show how to automate extremal testing of network software using
LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS
name length limits); then ask the LLM to generate tests that violate the
constraints. We demonstrate how easy this process is by generating extremal
tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.
We show how this methodology extends to centralized network software such as
shortest path algorithms, and how LLMs can generate filtering code to reject
extremal input. We propose using agentic AI to further automate extremal
testing. LLM-generated extremal testing goes beyond an old technique in
software testing called Boundary Value Analysis.

</details>


### [271] [A Task Taxonomy for Conformance Checking](https://arxiv.org/abs/2507.11976)
*Jana-Rebecca Rehse,Michael Grohs,Finn Klessascheck,Lisa-Marie Klein,Tatiana von Landesberger,Luise Pufahl*

Main category: cs.SE

TL;DR: This paper discusses conformance checking, a subfield of process mining, emphasizing the need for systematic visualizations. A novel task taxonomy is proposed to categorize analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Organizations lack clarity on the analytical purposes of available visualizations for conformance checking, making it difficult to evaluate their usefulness effectively.

Method: The authors propose a task taxonomy for conformance checking, combining concepts from process mining and visual analytics.

Result: The taxonomy categorizes conformance checking tasks by various dimensions like goal, means, constraints, and data properties.

Conclusion: This taxonomy aids researchers in understanding analysis tasks better and fosters collaborations between process mining and visual analytics disciplines.

Abstract: Conformance checking is a sub-discipline of process mining, which compares
observed process traces with a process model to analyze whether the process
execution conforms with or deviates from the process design. Organizations can
leverage this analysis, for example to check whether their processes comply
with internal or external regulations or to identify potential improvements.
Gaining these insights requires suitable visualizations, which make complex
results accessible and actionable. So far, however, the development of
conformance checking visualizations has largely been left to tool vendors. As a
result, current tools offer a wide variety of visual representations for
conformance checking, but the analytical purposes they serve often remain
unclear. However, without a systematic understanding of these purposes, it is
difficult to evaluate the visualizations' usefulness. Such an evaluation hence
requires a deeper understanding of conformance checking as an analysis domain.
To this end, we propose a task taxonomy, which categorizes the tasks that can
occur when conducting conformance checking analyses. This taxonomy supports
researchers in determining the purpose of visualizations, specifying relevant
conformance checking tasks in terms of their goal, means, constraint type, data
characteristics, data target, and data cardinality. Combining concepts from
process mining and visual analytics, we address researchers from both
disciplines to enable and support closer collaborations.

</details>


### [272] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Main category: cs.SE

TL;DR: GitChameleon is a new dataset designed to evaluate AI systems' ability to generate Python code compatible with specific library versions using execution-based testing.


<details>
  <summary>Details</summary>
Motivation: The rapid changes in software library versions highlight the need for adaptable code generation methods that maintain functionality and backward compatibility.

Method: GitChameleon consists of 328 Python code completion problems tied to specific library versions, paired with executable unit tests to assess functional accuracy.

Result: Evaluations show enterprise AI systems struggle with this task, achieving only 48-51% success rates, revealing the challenge of version-conditioned code generation.

Conclusion: GitChameleon provides a dynamic benchmark to improve understanding and guide advancements in adaptive AI-driven code generation methods.

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


### [273] [LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation](https://arxiv.org/abs/2507.12084)
*Keke Gai,Haochen Liang,Jing Yu,Liehuang Zhu,Dusit Niyato*

Main category: cs.SE

TL;DR: The paper introduces LLAMA, an advanced smart contract fuzzing framework based on Large Language Models (LLMs), achieving superior performance in both coverage and vulnerability detection.


<details>
  <summary>Details</summary>
Motivation: Existing fuzzers underutilize mutation scheduling, a critical factor for fuzzing success, focusing more on seed scheduling and generation.

Method: LLAMA uses a combination of LLMs, evolutionary mutation strategies, and hybrid testing techniques, organized into hierarchical prompts, multi-feedback optimizations, and an evolutionary fuzzing engine.

Result: LLAMA achieved 91% instruction coverage, 90% branch coverage, and successfully detected 132 out of 148 known vulnerabilities during experiments.

Conclusion: The LLAMA framework significantly improves smart contract fuzzing effectiveness and practicality, demonstrating superior results against state-of-the-art techniques.

Abstract: Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing
remains an important approach to securing smart contracts. Even though mutation
scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers
have primarily explored seed scheduling and generation, while mutation
scheduling has been rarely addressed by prior work. In this work, we propose a
Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing
framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and
hybrid testing techniques. Key components of the proposed LLAMA include: (i) a
hierarchical prompting strategy that guides LLMs to generate semantically valid
initial seeds, coupled with a lightweight pre-fuzzing phase to select
high-potential inputs; (ii) a multi-feedback optimization mechanism that
simultaneously improves seed generation, seed selection, and mutation
scheduling by leveraging runtime coverage and dependency feedback; and (iii) an
evolutionary fuzzing engine that dynamically adjusts mutation operator
probabilities based on effectiveness, while incorporating symbolic execution to
escape stagnation and uncover deeper vulnerabilities. Our experiments
demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage
and vulnerability detection. Specifically, it achieves 91% instruction coverage
and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities
across diverse categories. These results highlight LLAMA's effectiveness,
adaptability, and practicality in real-world smart contract security testing
scenarios.

</details>


### [274] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: The paper introduces AI-driven intelligent pricing models to automate SaaS pricing analysis and management, addressing inefficiencies and reducing human errors.


<details>
  <summary>Details</summary>
Motivation: The manual management of SaaS pricing structures is time-consuming, error-prone, and lacks efficient tools for optimizing and scaling pricing models.

Method: The authors present AI4Pricing2Yaml, an LLM-driven tool using web scraping and large language models to convert static HTML pricing into intelligent, dynamic pricing formats.

Result: Validation on 30 distinct SaaS websites and 150 intelligent pricing models showed effective extraction of pricing elements, highlighting the tool's efficiency in automating the transformation process.

Conclusion: Automating intelligent pricing transformation has significant potential to enhance SaaS pricing management, but challenges like handling dynamic content and hallucinations need further research.

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [275] [An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment](https://arxiv.org/abs/2507.12118)
*Noe Zermeño,Cristina Zuheros,Lucas Daniel Del Rosso Calache,Francisco Herrera,Rosana Montes*

Main category: cs.SE

TL;DR: The paper introduces a methodology combining linguistic decision-making and design thinking for web usability evaluation through A/B testing, validated via a case study on Moodle platforms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing tools in web usability testing, particularly in accommodating real and fictional users for comprehensive evaluations.

Method: The proposed methodology uses user-centered approaches like design thinking and linguistic decision-making, integrating them into a system with A/B testing and usability tests such as the System Usability Scale.

Result: The methodology was applied in a case study involving three Moodle platforms at the University of Guadalajara, showcasing its practical utility and effectiveness.

Conclusion: The study presents a structured, user-centered approach to web usability evaluation that both enhances testing frameworks and incorporates real user involvement for robust results.

Abstract: In recent years, attention has increasingly focused on enhancing user
satisfaction with user interfaces, spanning both mobile applications and
websites. One fundamental aspect of human-machine interaction is the concept of
web usability. In order to assess web usability, the A/B testing technique
enables the comparison of data between two designs. Expanding the scope of
tests to include the designs being evaluated, in conjunction with the
involvement of both real and fictional users, presents a challenge for which
few online tools offer support. We propose a methodology for web usability
evaluation based on user-centered approaches such as design thinking and
linguistic decision-making, named Linguistic Decision-Making for Web Usability
Evaluation. This engages people in role-playing scenarios and conducts a number
of usability tests, including the widely recognized System Usability Scale. We
incorporate the methodology into a decision support system based on A/B
testing. We use real users in a case study to assess three Moodle platforms at
the University of Guadalajara, Mexico.

</details>


### [276] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: The paper introduces MERA Code, a benchmark to evaluate code quality and practical coding skills of LLMs, especially for Russian language-focused tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs mainly focus on natural language processing and high-level reasoning, ignoring executable code quality and real-world performance in production.

Method: They propose MERA Code, a benchmark containing 11 coding evaluation tasks across 8 programming languages, with a robust taxonomy, scoring system, open-source codebase, and leaderboard for assessment.

Result: MERA Code is used to evaluate open and frontier API models, uncovering their limitations in practical coding tasks, particularly in non-English languages like Russian.

Conclusion: The benchmark showcases the need for standardizing evaluation methodologies and aims to guide future research on improving LLMs' coding capabilities.

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [277] [SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?](https://arxiv.org/abs/2507.12415)
*Xinyi He,Qian Liu,Mingzhe Du,Lin Yan,Zhijie Fan,Yiming Huang,Zejian Yuan,Zejun Ma*

Main category: cs.SE

TL;DR: The paper introduces SWE-Perf, a benchmark for assessing LLMs' ability to optimize code performance in real-world repository contexts.


<details>
  <summary>Details</summary>
Motivation: Optimizing code performance is vital for production systems, but LLMs' potential in repository-level performance enhancement is underexplored.

Method: The authors developed SWE-Perf, a benchmark with 140 instances collected from GitHub performance-based pull requests, including codebases, tests, and patches for evaluation.

Result: A significant gap is observed between current LLMs and expert-level code optimization capabilities, exposing research opportunities.

Conclusion: SWE-Perf offers a foundation for systematically evaluating and advancing LLMs for performance optimization tasks.

Abstract: Code performance optimization is paramount in real-world software engineering
and critical for production-level systems. While Large Language Models (LLMs)
have demonstrated impressive capabilities in code generation and bug fixing,
their proficiency in enhancing code performance at the repository level remains
largely unexplored. To address this gap, we introduce SWE-Perf, the first
benchmark specifically designed to systematically evaluate LLMs on code
performance optimization tasks within authentic repository contexts. SWE-Perf
comprises 140 carefully curated instances, each derived from
performance-improving pull requests from popular GitHub repositories. Each
benchmark instance includes the relevant codebase, target functions,
performance-related tests, expert-authored patches, and executable
environments. Through a comprehensive evaluation of representative methods that
span file-level and repo-level approaches (e.g., Agentless and OpenHands), we
reveal a substantial capability gap between existing LLMs and expert-level
optimization performance, highlighting critical research opportunities in this
emerging field.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [278] [EEG-fused Digital Twin Brain for Autonomous Driving in Virtual Scenarios](https://arxiv.org/abs/2507.12263)
*Yubo Hou,Zhengxin Zhang,Ziyi Wang,Wenlian Lu,Jianfeng Feng,Taiping Zeng*

Main category: q-bio.NC

TL;DR: The paper proposes a Bayesian framework integrating EEG and structural MRI to enhance brain modeling and control systems for tasks like autonomous driving.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of using fMRI alone for brain modeling, such as poor temporal resolution, by combining EEG's high temporal precision with MRI's spatial accuracy.

Method: A Bayesian inference framework was developed to merge high-temporal-resolution EEG signals with structural MRI data, constructing biologically realistic digital twin brain models.

Result: The EEG-DTB model accurately generated EEG signals (0.88 resting-state correlation, 0.60 task-state) and improved steering predictions in the CARLA driving simulator.

Conclusion: This approach advances sensorimotor research and brain-inspired autonomous systems by integrating complementary imaging modalities for better perception-action understanding.

Abstract: Current methodologies typically integrate biophysical brain models with
functional magnetic resonance imaging(fMRI) data - while offering
millimeter-scale spatial resolution (0.5-2 mm^3 voxels), these approaches
suffer from limited temporal resolution (>0.5 Hz) for tracking rapid neural
dynamics during continuous tasks. Conversely, Electroencephalogram (EEG)
provides millisecond-scale temporal precision (<=1 ms sampling rate) for
real-time guidance of continuous task execution, albeit constrained by low
spatial resolution. To reconcile these complementary modalities, we present a
generalizable Bayesian inference framework that integrates
high-spatial-resolution structural MRI(sMRI) with high-temporal-resolution EEG
to construct a biologically realistic digital twin brain(DTB) model. The
framework establishes voxel-wise mappings between millisecond-scale EEG and
sMRI-derived spiking networks, while demonstrating its translational potential
through a brain-inspired autonomous driving simulation. Our EEG-DTB model
achieves capabilities: (1) Biologically-plausible EEG signal generation (0.88
resting-state,0.60 task-state correlation), with simulated signals in
task-state yielding steering predictions outperforming both chance and
empirical signals (p<0.05); (2) Successful autonomous driving in the CARLA
simulator using decoded steering angles. The proposed approach pioneers a new
paradigm for studying sensorimotor integration and for mechanistic studies of
perception-action cycles and the development of brain-inspired control systems.

</details>


### [279] [Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing through Non-invasive BCI](https://arxiv.org/abs/2507.12417)
*Weichen Dai,Yuxuan Huang,Li Zhu,Dongjun Liu,Yu Zhang,Qibin Zhao,Andrzej Cichocki,Fabio Babiloni,Ke Li,Jianyu Qiu,Gangyong Jia,Wanzeng Kong,Qing Wu*

Main category: q-bio.NC

TL;DR: The paper demonstrates decoding of 6D egocentric pose using EEG data during passive video viewing, showcasing how neural systems encode spatial information spontaneously and continuously.


<details>
  <summary>Details</summary>
Motivation: To understand large-scale neural dynamics involved in spatial representation during naturalistic, passive experiences, which is currently poorly understood.

Method: The study used electroencephalography (EEG) as a non-invasive brain-computer interface to decode fine-grained 6D egocentric spatial poses during passive viewing of egocentric videos, utilizing structured motion and a decoding model.

Result: EEG successfully decoded 6D spatial pose despite its limitations, with enhanced decoding performance at optimized temporal dynamics (100 ms/image) and identification of distinct neural encoding channels for position and orientation.

Conclusion: The findings challenge traditional distinctions between active and passive spatial cognition, showing spontaneous spatial mapping and advancing understanding of how sensory data transforms into structured internal representations.

Abstract: Humans possess a remarkable capacity for spatial cognition, allowing for
self-localization even in novel or unfamiliar environments. While hippocampal
neurons encoding position and orientation are well documented, the large-scale
neural dynamics supporting spatial representation, particularly during
naturalistic, passive experience, remain poorly understood. Here, we
demonstrate for the first time that non-invasive brain-computer interfaces
(BCIs) based on electroencephalography (EEG) can decode spontaneous,
fine-grained egocentric 6D pose, comprising three-dimensional position and
orientation, during passive viewing of egocentric video. Despite EEG's limited
spatial resolution and high signal noise, we find that spatially coherent
visual input (i.e., continuous and structured motion) reliably evokes decodable
spatial representations, aligning with participants' subjective sense of
spatial engagement. Decoding performance further improves when visual input is
presented at a frame rate of 100 ms per image, suggesting alignment with
intrinsic neural temporal dynamics. Using gradient-based backpropagation
through a neural decoding model, we identify distinct EEG channels contributing
to position -- and orientation specific -- components, revealing a distributed
yet complementary neural encoding scheme. These findings indicate that the
brain's spatial systems operate spontaneously and continuously, even under
passive conditions, challenging traditional distinctions between active and
passive spatial cognition. Our results offer a non-invasive window into the
automatic construction of egocentric spatial maps and advance our understanding
of how the human mind transforms everyday sensory experience into structured
internal representations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [280] [LLMs are Bayesian, in Expectation, not in Realization](https://arxiv.org/abs/2507.11768)
*Leon Chlon,Sarah Rashidi,Zein Khamis,MarcAntonio M. Awada*

Main category: stat.ML

TL;DR: The paper investigates martingale violations in transformers during in-context learning, and offers theoretical and empirical analyses to reconcile these issues with practical applications.


<details>
  <summary>Details</summary>
Motivation: To address the contradiction between observed martingale violations in transformers and the theoretical basis of Bayesian inference in exchangeable data, which impacts uncertainty quantification in critical use cases.

Method: Theoretical investigation to analyze positional encodings, excess risk, posterior convergence, and chain-of-thought length. Empirical validation using GPT-3 to examine martingale violations and entropy convergence.

Result: Key findings include martingale violations due to positional encodings, optimal risk order achievement by transformers, posterior convergence, derived optimal chain-of-thought length, and empirical validation demonstrating transformers approach theoretical entropy limits efficiently.

Conclusion: The approach provides a robust theoretical foundation and practical tools for calibrated uncertainty estimation and enhancement of computational efficiency in transformers, bridging a gap in in-context learning applications.

Abstract: Large language models demonstrate remarkable in-context learning
capabilities, adapting to new tasks without parameter updates. While this
phenomenon has been successfully modeled as implicit Bayesian inference, recent
empirical findings reveal a fundamental contradiction: transformers
systematically violate the martingale property, a cornerstone requirement of
Bayesian updating on exchangeable data. This violation challenges the
theoretical foundations underlying uncertainty quantification in critical
applications.
  Our theoretical analysis establishes four key results: (1) positional
encodings induce martingale violations of order $\Theta(\log n / n)$; (2)
transformers achieve information-theoretic optimality with excess risk
$O(n^{-1/2})$ in expectation over orderings; (3) the implicit posterior
representation converges to the true Bayesian posterior in the space of
sufficient statistics; and (4) we derive the optimal chain-of-thought length as
$k^* = \Theta(\sqrt{n}\log(1/\varepsilon))$ with explicit constants, providing
a principled approach to reduce inference costs while maintaining performance.
Empirical validation on GPT-3 confirms predictions (1)-(3), with transformers
reaching 99\% of theoretical entropy limits within 20 examples. Our framework
provides practical methods for extracting calibrated uncertainty estimates from
position-aware architectures and optimizing computational efficiency in
deployment.

</details>


### [281] [Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?](https://arxiv.org/abs/2507.11891)
*Shuangning Li,Chonghuan Wang,Jingyan Wang*

Main category: stat.ML

TL;DR: This paper investigates the bias caused by data-sharing interference in A/B experiments for recommendation algorithms, focusing on its impact on decision-making using a multi-armed bandit framework.


<details>
  <summary>Details</summary>
Motivation: To address the bias called 'symbiosis bias' in A/B experiments caused by shared data that trains both treatment and control algorithms, which can skew decision-making in selecting the optimal algorithm.

Method: Theoretical exploration under a multi-armed bandit framework to reveal scenarios where the bias impacts decision-making and algorithm selection, by analyzing the role of exploration versus exploitation.

Result: Identified conditions where symbiosis bias influences the sign alignment between the expected GTE estimate and the true GTE, highlighting exploration-exploitation balance as a key factor.

Conclusion: Understanding symbiosis bias is critical for improving decision-making in A/B experiments, particularly when the sign of GTE determines the optimal algorithm selection.

Abstract: We study A/B experiments that are designed to compare the performance of two
recommendation algorithms. Prior work has shown that the standard
difference-in-means estimator is biased in estimating the global treatment
effect (GTE) due to a particular form of interference between experimental
units. Specifically, units under the treatment and control algorithms
contribute to a shared pool of data that subsequently train both algorithms,
resulting in interference between the two groups. The bias arising from this
type of data sharing is known as "symbiosis bias". In this paper, we highlight
that, for decision-making purposes, the sign of the GTE often matters more than
its precise magnitude when selecting the better algorithm. We formalize this
insight under a multi-armed bandit framework and theoretically characterize
when the sign of the expected GTE estimate under data sharing aligns with or
contradicts the sign of the true GTE. Our analysis identifies the level of
exploration versus exploitation as a key determinant of how symbiosis bias
impacts algorithm selection.

</details>


### [282] [Newfluence: Boosting Model interpretability and Understanding in High Dimensions](https://arxiv.org/abs/2507.11895)
*Haolin Zou,Arnab Auddy,Yongchan Kwon,Kamiar Rahnama Rad,Arian Maleki*

Main category: stat.ML

TL;DR: The paper critiques the reliability of influence functions for interpreting high-dimensional AI models and introduces Newfluence as a more accurate alternative.


<details>
  <summary>Details</summary>
Motivation: Modern AI operates in high-dimensional settings, rendering traditional tools like influence functions inadequate.

Method: The authors conduct theoretical and empirical analyses of influence functions and propose Newfluence, a novel approximation method designed for high-dimensional settings.

Result: Newfluence offers improved accuracy and computational efficiency compared to influence functions in high-dimensional AI models.

Conclusion: Newfluence enhances the interpretability and diagnostic potential of AI models, addressing the limitations of traditional influence functions and extending applicability to techniques like Shapley values.

Abstract: The increasing complexity of machine learning (ML) and artificial
intelligence (AI) models has created a pressing need for tools that help
scientists, engineers, and policymakers interpret and refine model decisions
and predictions. Influence functions, originating from robust statistics, have
emerged as a popular approach for this purpose.
  However, the heuristic foundations of influence functions rely on
low-dimensional assumptions where the number of parameters $p$ is much smaller
than the number of observations $n$. In contrast, modern AI models often
operate in high-dimensional regimes with large $p$, challenging these
assumptions.
  In this paper, we examine the accuracy of influence functions in
high-dimensional settings. Our theoretical and empirical analyses reveal that
influence functions cannot reliably fulfill their intended purpose. We then
introduce an alternative approximation, called Newfluence, that maintains
similar computational efficiency while offering significantly improved
accuracy.
  Newfluence is expected to provide more accurate insights than many existing
methods for interpreting complex AI models and diagnosing their issues.
Moreover, the high-dimensional framework we develop in this paper can also be
applied to analyze other popular techniques, such as Shapley values.

</details>


### [283] [Incorporating Fairness Constraints into Archetypal Analysis](https://arxiv.org/abs/2507.12021)
*Aleix Alcacer,Irene Epifanio*

Main category: stat.ML

TL;DR: This paper introduces Fair Archetypal Analysis (FairAA) and FairKernelAA to improve fairness in unsupervised learning by minimizing encoding of sensitive attributes while preserving interpretability.


<details>
  <summary>Details</summary>
Motivation: Standard Archetypal Analysis may inadvertently encode sensitive attributes in data representations, causing fairness concerns, which necessitates solutions for equitable and interpretable data projections.

Method: FairAA incorporates fairness regularization into Archetypal Analysis, and FairKernelAA extends this approach nonlinearly for complex data distributions.

Result: Both FairAA and FairKernelAA reduce group separability without significantly sacrificing explained variance, verified on synthetic and real-world datasets.

Conclusion: FairAA achieves a balance between fairness and utility, positioning it as a responsible tool for sensitive applications in representation learning.

Abstract: Archetypal Analysis (AA) is an unsupervised learning method that represents
data as convex combinations of extreme patterns called archetypes. While AA
provides interpretable and low-dimensional representations, it can
inadvertently encode sensitive attributes, leading to fairness concerns. In
this work, we propose Fair Archetypal Analysis (FairAA), a modified formulation
that explicitly reduces the influence of sensitive group information in the
learned projections. We also introduce FairKernelAA, a nonlinear extension that
addresses fairness in more complex data distributions. Our approach
incorporates a fairness regularization term while preserving the structure and
interpretability of the archetypes. We evaluate FairAA and FairKernelAA on
synthetic datasets, including linear, nonlinear, and multi-group scenarios,
demonstrating their ability to reduce group separability -- as measured by mean
maximum discrepancy and linear separability -- without substantially
compromising explained variance. We further validate our methods on the
real-world ANSUR I dataset, confirming their robustness and practical utility.
The results show that FairAA achieves a favorable trade-off between utility and
fairness, making it a promising tool for responsible representation learning in
sensitive applications.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [284] [MaCE: General Mass Conserving Dynamics for Cellular Automata](https://arxiv.org/abs/2507.12306)
*Vassilis Papadopoulos,Etienne Guichard*

Main category: nlin.CG

TL;DR: This paper introduces MaCE (Mass-Conserving Evolution), a versatile method to ensure mass conservation in Cellular Automata, enhancing stability and producing intriguing behaviors.


<details>
  <summary>Details</summary>
Motivation: Cellular Automata are prone to instabilities like uncontrolled growth or extinction of patterns, limiting their use in modeling complex systems; this paper aims to address these limitations.

Method: MaCE, a mass-conserving evolution rule, is attached to existing Cellular Automata to stabilize their dynamics, making them more suitable for diverse experiments and applications.

Result: Experiments with Lenia, Neural-CAs, and discrete CAs demonstrate that MaCE enables stable, diverse, and resource-constrained behaviors, fostering intrinsic evolution and solitons formation.

Conclusion: MaCE enhances Cellular Automata's capabilities, suggesting new research avenues in dynamic systems modeling, especially under resource constraints.

Abstract: We present Mass-Conserving Evolution (MaCE), a general method for
implementing mass conservation in Cellular Automata (CA). MaCE is a simple
evolution rule that can be easily 'attached' to existing CAs to make them
mass-conserving, which tends to produce interesting behaviours more often, as
patterns can no longer explode or die out. We first show that MaCE is
numerically stable and admits a simple continuous limit. We then test MaCE on
Lenia, and through several experiments, we demonstrate that it produces a wide
variety of interesting behaviours, starting from the variety and abundance of
solitons up to hints of intrinsic evolution in resource-constrained
environments. Finally, we showcase the versatility of MaCE by applying it to
Neural-CAs and discrete CAs, and discuss promising research directions opened
up by this scheme.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [285] [Model averaging in the space of probability distributions](https://arxiv.org/abs/2507.11719)
*Emmanouil Androulakis,Georgios I. Papayiannis,Athanasios N. Yannacopoulos*

Main category: stat.ME

TL;DR: This paper addresses model averaging with measure-valued data using Wasserstein barycenters and introduces methods inspired by elastic net regularization for improved performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop robust and optimal aggregation schemes for model averaging in the context of probability distributions, particularly in applications like estimating distributions with heavy-tailed characteristics.

Method: The approach involves using Wasserstein barycenters for model aggregation and introducing elastic net regularization to enhance sparsity. The consistency of these schemes is theoretically analyzed using the $\\Gamma$-convergence framework.

Result: The methods were validated both through synthetic experiments with varied distributional characteristics and a real-world application to estimate insurance claim size distributions, showcasing reliable performance under stress conditions.

Conclusion: The proposed aggregation and regularization techniques effectively calibrate distribution models to empirical data, achieving sparse and consistent performance, even in cases of heavy-tailed data.

Abstract: This work investigates the problem of model averaging in the context of
measure-valued data. Specifically, we study aggregation schemes in the space of
probability distributions metrized in terms of the Wasserstein distance. The
resulting aggregate models, defined via Wasserstein barycenters, are optimally
calibrated to empirical data. To enhance model performance, we employ
regularization schemes motivated by the standard elastic net penalization,
which is shown to consistently yield models enjoying sparsity properties. The
consistency properties of the proposed averaging schemes with respect to sample
size are rigorously established using the variational framework of
$\Gamma$-convergence. The performance of the methods is evaluated through
carefully designed synthetic experiments that assess behavior across a range of
distributional characteristics and stress conditions. Finally, the proposed
approach is applied to a real-world dataset of insurance losses - characterized
by heavy-tailed behavior - to estimate the claim size distribution and the
associated tail risk.

</details>


### [286] [Fiducial Matching: Differentially Private Inference for Categorical Data](https://arxiv.org/abs/2507.11762)
*Ogonnaya Michael Romanus,Younes Boulaguiem,Roberto Molinari*

Main category: stat.ME

TL;DR: This paper explores statistical inference in the context of differential privacy by using a simulation-based fiducial framework to approximate distributions of estimates for categorical data.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the complexity introduced by the compounded randomness of sampling and differential privacy, which makes traditional statistical inference challenging in DP settings.

Method: A simulation-based matching technique grounded in the fiducial framework is employed to simulate the data generation process (including the DP step) and approximate the resulting statistical distributions.

Result: The proposed approach demonstrates validity in terms of coverage and achieves good performance for inferential tasks across simulated and real-world data settings.

Conclusion: The approach provides a feasible and effective method for statistical inference on categorical data within a differential privacy framework, offering practical insights for real-world applications.

Abstract: The task of statistical inference, which includes the building of confidence
intervals and tests for parameters and effects of interest to a researcher, is
still an open area of investigation in a differentially private (DP) setting.
Indeed, in addition to the randomness due to data sampling, DP delivers another
source of randomness consisting of the noise added to protect an individual's
data from being disclosed to a potential attacker. As a result of this
convolution of noises, in many cases it is too complicated to determine the
stochastic behavior of the statistics and parameters resulting from a DP
procedure. In this work, we contribute to this line of investigation by
employing a simulation-based matching approach, solved through tools from the
fiducial framework, which aims to replicate the data generation pipeline
(including the DP step) and retrieve an approximate distribution of the
estimates resulting from this pipeline. For this purpose, we focus on the
analysis of categorical (nominal) data that is common in national surveys, for
which sensitivity is naturally defined, and on additive privacy mechanisms. We
prove the validity of the proposed approach in terms of coverage and highlight
its good computational and statistical performance for different inferential
tasks in simulated and applied data settings.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [287] [Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations](https://arxiv.org/abs/2507.12117)
*Timothy Heightman,Edward Jiang,Ruth Mora-Soto,Maciej Lewenstein,Marcin Płodzień*

Main category: quant-ph

TL;DR: The paper develops a novel phase-space formalism for quantum machine learning that replaces traditional operator algebra with smoother function dynamics, reducing complexity.


<details>
  <summary>Details</summary>
Motivation: QML seeks to harness quantum properties like entanglement for data processing but encounters scalability issues with the state-vector representation.

Method: Introduces phase-space methods using quasi-probability functions, leveraging Stratonovich-Weyl correspondence to represent qubit systems on symplectic manifolds.

Result: Achieves linear scalability in the domain size with the number of qubits, potentially mitigating the curse of dimensionality.

Conclusion: The approach offers a scalable alternative for QML by modeling the dynamics over phase-space, setting a foundation for further variational methods.

Abstract: Quantum machine learning (QML) seeks to exploit the intrinsic properties of
quantum mechanical systems, including superposition, coherence, and quantum
entanglement for classical data processing. However, due to the exponential
growth of the Hilbert space, QML faces practical limits in classical
simulations with the state-vector representation of quantum system. On the
other hand, phase-space methods offer an alternative by encoding quantum states
as quasi-probability functions. Building on prior work in qubit phase-space and
the Stratonovich-Weyl (SW) correspondence, we construct a closed, composable
dynamical formalism for one- and many-qubit systems in phase-space. This
formalism replaces the operator algebra of the Pauli group with function
dynamics on symplectic manifolds, and recasts the curse of dimensionality in
terms of harmonic support on a domain that scales linearly with the number of
qubits. It opens a new route for QML based on variational modelling over
phase-space.

</details>


### [288] [BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search](https://arxiv.org/abs/2507.12189)
*Azhar Ikhtiarudin,Aditi Das,Param Thakkar,Akash Kundu*

Main category: quant-ph

TL;DR: This paper introduces BenchRL-QAS, a benchmarking framework to evaluate reinforcement learning (RL) algorithms for quantum architecture search (QAS) across diverse tasks and system sizes.


<details>
  <summary>Details</summary>
Motivation: To systematically assess and compare RL algorithms for QAS, given the growing importance of task-specific quantum circuit designs for various quantum computing applications.

Method: Benchmarked nine RL algorithms (value-based and policy-gradient) on quantum tasks (e.g., variational quantum eigensolver, quantum classification) in both noiseless and noisy environments, using weighted ranking metrics for comparison.

Result: Found that RL-based quantum classifiers outperform traditional variational classifiers and confirmed the "no free lunch" principle—no single RL algorithm works best across all tasks due to context-dependent performance.

Conclusion: Highlighted the necessity of context-sensitive RL algorithm selection and systematic benchmarking. BenchRL-QAS provides actionable insights for advancing quantum circuit synthesis and is publicly available for community use.

Abstract: We introduce BenchRL-QAS, a unified benchmarking framework for systematically
evaluating reinforcement learning (RL) algorithms in quantum architecture
search (QAS) across diverse variational quantum algorithm tasks and system
sizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including
both value-based and policy-gradient methods on representative quantum problems
such as variational quantum eigensolver, variational quantum state
diagonalization, quantum classification, and state preparation, spanning both
noiseless and realistic noisy regimes. We propose a weighted ranking metric
that balances accuracy, circuit depth, gate count, and computational
efficiency, enabling fair and comprehensive comparison. Our results first
reveal that RL-based quantum classifier outperforms baseline variational
classifiers. Then we conclude that no single RL algorithm is universally
optimal when considering a set of QAS tasks; algorithmic performance is highly
context-dependent, varying with task structure, qubit count, and noise. This
empirical finding provides strong evidence for the "no free lunch" principle in
RL-based quantum circuit design and highlights the necessity of tailored
algorithm selection and systematic benchmarking for advancing quantum circuit
synthesis. This work represents the most comprehensive RL-QAS benchmarking
effort to date, and BenchRL-QAS along with all experimental data are made
publicly available to support reproducibility and future research
https://github.com/azhar-ikhtiarudin/bench-rlqas.

</details>


### [289] [Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator](https://arxiv.org/abs/2507.12256)
*Monica Lăcătuş,Matthias Möller*

Main category: quant-ph

TL;DR: The paper develops a quantum surrogate circuit framework to emulate the collision operator in quantum lattice Boltzmann methods, achieving efficient implementation without ancilla qubits, probabilistic post-selection, or repeated executions.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenges faced by traditional computational fluid dynamics (CFD) tools in simulating turbulent flows at high Reynolds numbers and to leverage quantum computing's potential speed-ups for these complex simulations.

Method: The authors introduce a surrogate quantum circuit trained to approximate the physical properties of the BGK collision operator for the D2Q9 lattice. This four-qubit circuit is designed without additional ancilla qubits, probabilistic post-selection, or repeated executions.

Result: The proposed surrogate quantum circuit achieves efficient implementation requiring only 2,430 native gates and demonstrates performance comparable to benchmark flows like Taylor Green vortex decay and lid-driven cavity.

Conclusion: The framework showcases quantum parallelism in local operations, allowing scalable and physically accurate simulation of turbulent flows without dependency on grid resolution.

Abstract: Direct numerical simulation of turbulent flows at high Reynolds numbers
remains a major challenge for traditional computational fluid dynamics (CFD)
tools running on classical computer hardware. This has motivated growing
interest in quantum algorithms for CFD to enable flow simulations on quantum
computers. The reason being that these computers are expected to deliver
potential speed-ups for certain problems. One promising quantum CFD approach is
a fully quantum implementation of the lattice Boltzmann method called QLBM.
Although efficient quantum routines are now available for the streaming step,
implementing the nonlinear, irreversible collision step with a low depth
circuit that avoids additional ancilla qubits, probabilistic post-selection and
repeated executions remains a significant challenge. In this study, we address
this challenge by introducing a framework for learning a surrogate quantum
circuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision
operator for the D2Q9 lattice. The four qubit circuit is trained to respect the
physical properties of the BGK collision operator, including mass and momentum
conservation, D8 equivariance and scale equivariance. When compiled to the gate
set used by IBM Heron processor under the assumption of full qubit
connectivity, the 15 block SQC requires only 2,430 native gates and uses
neither ancilla qubits nor post-selection or repeated executions. Moreover, its
depth is independent of the grid resolution, as collision is a local operation
that can exploit quantum parallelism to its full extent. We validate the SQC on
two benchmark flows, the Taylor Green vortex decay and the lid driven cavity,
demonstrating that it accurately captures vortex dissipation and flow
recirculation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [290] [A Review of Generative AI in Computer Science Education: Challenges and Opportunities in Accuracy, Authenticity, and Assessment](https://arxiv.org/abs/2507.11543)
*Iman Reihanian,Yunfei Hou,Yu Chen,Yifei Zheng*

Main category: cs.CY

TL;DR: The paper reviews the implications of using Generative AI (e.g., ChatGPT) in computer science education, addressing challenges and opportunities in accuracy, authenticity, and assessment.


<details>
  <summary>Details</summary>
Motivation: To explore how Generative AI tools can be effectively integrated into computer science education, amid concerns like AI hallucinations, bias, and authorship authenticity.

Method: A literature review was conducted to identify challenges, opportunities, and proposed solutions in using Generative AI tools for academic purposes.

Result: Key findings include suggested adoption of hybrid assessment models, development of bias detection frameworks, and promotion of AI literacy among students and educators.

Conclusion: Integrating AI in education requires addressing ethical, technical, and pedagogical aspects, ensuring a balanced approach for fostering both integrity and creativity.

Abstract: This paper surveys the use of Generative AI tools, such as ChatGPT and
Claude, in computer science education, focusing on key aspects of accuracy,
authenticity, and assessment. Through a literature review, we highlight both
the challenges and opportunities these AI tools present. While Generative AI
improves efficiency and supports creative student work, it raises concerns such
as AI hallucinations, error propagation, bias, and blurred lines between
AI-assisted and student-authored content. Human oversight is crucial for
addressing these concerns. Existing literature recommends adopting hybrid
assessment models that combine AI with human evaluation, developing bias
detection frameworks, and promoting AI literacy for both students and
educators. Our findings suggest that the successful integration of AI requires
a balanced approach, considering ethical, pedagogical, and technical factors.
Future research may explore enhancing AI accuracy, preserving academic
integrity, and developing adaptive models that balance creativity with
precision.

</details>


### [291] [Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening](https://arxiv.org/abs/2507.11548)
*Kevin T Webster*

Main category: cs.CY

TL;DR: This paper audits generative AI systems for resume screening, uncovering both demographic bias and lack of substantive evaluative competence.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the critical need to investigate whether AI systems for resume screening are truly unbiased and competent for evaluative tasks, as often assumed.

Method: The study conducted a two-part audit of eight major AI platforms, assessing demographic biases in Experiment 1 and core evaluative competence in Experiment 2.

Result: Experiment 1 found racial and gender biases; Experiment 2 revealed that some seemingly unbiased models were incompetent, relying on superficial keyword matching.

Conclusion: The paper concludes with a recommendation to adopt a dual-validation framework auditing both bias and competence in AI hiring tools to ensure equity and efficacy.

Abstract: The increasing use of generative AI for resume screening is predicated on the
assumption that it offers an unbiased alternative to biased human
decision-making. However, this belief fails to address a critical question: are
these AI systems fundamentally competent at the evaluative tasks they are meant
to perform? This study investigates the question of competence through a
two-part audit of eight major AI platforms. Experiment 1 confirmed complex,
contextual racial and gender biases, with some models penalizing candidates
merely for the presence of demographic signals. Experiment 2, which evaluated
core competence, provided a critical insight: some models that appeared
unbiased were, in fact, incapable of performing a substantive evaluation,
relying instead on superficial keyword matching. This paper introduces the
"Illusion of Neutrality" to describe this phenomenon, where an apparent lack of
bias is merely a symptom of a model's inability to make meaningful judgments.
This study recommends that organizations and regulators adopt a dual-validation
framework, auditing AI hiring tools for both demographic bias and demonstrable
competence to ensure they are both equitable and effective.

</details>


### [292] [AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce](https://arxiv.org/abs/2507.11597)
*Richard Timpone,Yongwei Yang*

Main category: cs.CY

TL;DR: The paper evaluates how AI can augment data scientists using the Truth, Beauty, and Justice framework, pointing out benefits, risks, and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing influence of AI in transforming how data analysis and research are conducted, emphasizing the need to evaluate its benefits and risks.

Method: The authors use the Truth, Beauty, and Justice framework to assess analytic, generative, and agentic AI's potential in augmenting human researchers while stressing human-machine collaboration.

Result: While highlighting AI's potential to enhance data science tasks, the paper raises warnings about automation risks stemming from misuse or misunderstanding of new tools.

Conclusion: It advocates for AI tools to complement rather than replace data scientists, stressing the need for proper training and ethical application to achieve sustainable and effective research outcomes.

Abstract: AI is transforming research. It is being leveraged to construct surveys,
synthesize data, conduct analysis, and write summaries of the results. While
the promise is to create efficiencies and increase quality, the reality is not
always as clear cut. Leveraging our framework of Truth, Beauty, and Justice
(TBJ) which we use to evaluate AI, machine learning and computational models
for effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024),
we consider the potential and limitation of analytic, generative, and agentic
AI to augment data scientists or take on tasks traditionally done by human
analysts and researchers. While AI can be leveraged to assist analysts in their
tasks, we raise some warnings about push-button automation. Just as earlier
eras of survey analysis created some issues when the increased ease of using
statistical software allowed researchers to conduct analyses they did not fully
understand, the new AI tools may create similar but larger risks. We emphasize
a human-machine collaboration perspective (Daugherty and Wilson 2018)
throughout the data science workflow and particularly call out the vital role
that data scientists play under VUCA decision areas. We conclude by encouraging
the advance of AI tools to complement data scientists but advocate for
continued training and understanding of methods to ensure the substantive value
of research is fully achieved by applying, interpreting, and acting upon
results most effectively and ethically.

</details>


### [293] [Small Data Explainer -- The impact of small data methods in everyday life](https://arxiv.org/abs/2507.11773)
*Maren Hackenberg,Sophia G. Connor,Fabian Kabus,June Brawner,Ella Markham,Mahi Hardalupas,Areeq Chowdhury,Rolf Backofen,Anna Köttgen,Angelika Rohde,Nadine Binder,Harald Binder,the Collaborative Research Center 1597 Small Data*

Main category: cs.CY

TL;DR: The paper discusses leveraging artificial intelligence techniques for small data settings to address societal issues, contrasts small data with big data, and outlines solutions through diverse modeling techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how small data settings can benefit from AI breakthroughs, addressing societal issues like inclusivity and health benefits.

Method: It presents a conceptual overview contrasting small and big data and details techniques like knowledge-driven and data-driven modeling from multiple disciplines.

Result: The paper identifies feasible applications, common themes, and provides insights into the interplay of application settings and specific techniques.

Conclusion: It proposes an agenda for maximizing the potential of small data settings using AI advancements while emphasizing the importance of interdisciplinary approaches.

Abstract: The emergence of breakthrough artificial intelligence (AI) techniques has led
to a renewed focus on how small data settings, i.e., settings with limited
information, can benefit from such developments. This includes societal issues
such as how best to include under-represented groups in data-driven policy and
decision making, or the health benefits of assistive technologies such as
wearables. We provide a conceptual overview, in particular contrasting small
data with big data, and identify common themes from exemplary case studies and
application areas. Potential solutions are described in a more detailed
technical overview of current data analysis and modelling techniques,
highlighting contributions from different disciplines, such as knowledge-driven
modelling from statistics and data-driven modelling from computer science. By
linking application settings, conceptual contributions and specific techniques,
we highlight what is already feasible and suggest what an agenda for fully
leveraging small data might look like.

</details>


### [294] [The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models](https://arxiv.org/abs/2507.11544)
*Ann-Kathrin Dombrowski,Dillon Bowen,Adam Gleave,Chris Cundy*

Main category: cs.CY

TL;DR: This paper introduces the 'Safety Gap Toolkit,' an open-source framework to evaluate risks posed by removing safeguards in open-weight large language models (LLMs). Safeguard removal significantly enhances dangerous capabilities as model scale increases.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the systemic risks posed by open-weight LLMs, where safeguards can be subverted to unleash harmful capabilities, contrasting their vast benefits in innovation, privacy, and democratization.

Method: The study evaluates biochemical and cyber capabilities, refusal rates, and generation quality in open-weight models like Llama-3 and Qwen-2.5 (ranging from 0.5B to 405B parameters) using different safeguard removal techniques.

Result: Findings show a widening safety gap as model size increases, where the removal of safeguards greatly amplifies dangerous capabilities, especially in larger-scale models.

Conclusion: The paper emphasizes the need for tamper-resistant safeguards and offers the Safety Gap Toolkit as a resource to assess current models and encourage further research and contributions from the community.

Abstract: Open-weight large language models (LLMs) unlock huge benefits in innovation,
personalization, privacy, and democratization. However, their core advantage -
modifiability - opens the door to systemic risks: bad actors can trivially
subvert current safeguards, turning beneficial models into tools for harm. This
leads to a 'safety gap': the difference in dangerous capabilities between a
model with intact safeguards and one that has been stripped of those
safeguards. We open-source a toolkit to estimate the safety gap for
state-of-the-art open-weight models. As a case study, we evaluate biochemical
and cyber capabilities, refusal rates, and generation quality of models from
two families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to
405B) using different safeguard removal techniques. Our experiments reveal that
the safety gap widens as model scale increases and effective dangerous
capabilities grow substantially when safeguards are removed. We hope that the
Safety Gap Toolkit (https://github.com/AlignmentResearch/safety-gap) will serve
as an evaluation framework for common open-source models and as a motivation
for developing and testing tamper-resistant safeguards. We welcome
contributions to the toolkit from the community.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [295] [Enhancing Signal Proportion Estimation Through Leveraging Arbitrary Covariance Structures](https://arxiv.org/abs/2507.11922)
*Jingtian Bai,Xinge Jessie Jeng*

Main category: math.ST

TL;DR: The paper introduces a new method for estimating signal proportions that accommodates variable dependence, enhancing accuracy across diverse contexts.


<details>
  <summary>Details</summary>
Motivation: Traditional signal proportion estimators are limited as they assume independence among variables and specific sparsity conditions, which are often unrealistic.

Method: The authors extend a prior lower confidence bounds approach to include a principal factor approximation procedure for addressing variable dependence.

Result: Extensive simulations show that the new method improves estimation accuracy and detects weak signals better than state-of-the-art methods.

Conclusion: Incorporating covariance dependence information results in more accurate and reliable signal proportion estimates, with practical applications across diverse conditions.

Abstract: Accurately estimating the proportion of true signals among a large number of
variables is crucial for enhancing the precision and reliability of scientific
research. Traditional signal proportion estimators often assume independence
among variables and specific signal sparsity conditions, limiting their
applicability in real-world scenarios where such assumptions may not hold. This
paper introduces a novel signal proportion estimator that leverages arbitrary
covariance dependence information among variables, thereby improving
performance across a wide range of sparsity levels and dependence structures.
Building on previous work that provides lower confidence bounds for signal
proportions, we extend this approach by incorporating the principal factor
approximation procedure to account for variable dependence. Our theoretical
insights offer a deeper understanding of how signal sparsity, signal intensity,
and covariance dependence interact. By comparing the conditions for estimation
consistency before and after dependence adjustment, we highlight the advantages
of integrating dependence information across different contexts. This
theoretical foundation not only validates the effectiveness of the new
estimator but also guides its practical application, ensuring reliable use in
diverse scenarios. Through extensive simulations, we demonstrate that our
method outperforms state-of-the-art estimators in both estimation accuracy and
the detection of weaker signals that might otherwise go undetected.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [296] [MOFSimBench: Evaluating Universal Machine Learning Interatomic Potentials In Metal--Organic Framework Molecular Modeling](https://arxiv.org/abs/2507.11806)
*Hendrik Kraß,Ju Huang,Seyed Mohamad Moosavi*

Main category: cond-mat.mtrl-sci

TL;DR: This paper evaluates the effectiveness of universal machine learning interatomic potentials (uMLIPs) for modeling nanoporous materials like MOFs, using a new benchmark called MOFSimBench.


<details>
  <summary>Details</summary>
Motivation: Universal machine learning interatomic potentials (uMLIPs) offer quantum calculation-level accuracy and scalability for atomistic simulations, but their reliability for real-world applications, especially for complex materials like MOFs, is unclear.

Method: The authors introduced MOFSimBench, a benchmarking framework to assess uMLIPs on tasks like structural optimization, MD stability, bulk property prediction, and guest-host interaction modeling. They evaluated over 20 uMLIPs on a diverse set of nanoporous materials.

Result: Top-performing uMLIPs surpassed classical force fields and specialized ML potentials in all tasks. Data quality, including training set diversity and inclusion of out-of-equilibrium conformations, was more influential on performance than model architecture.

Conclusion: The study establishes uMLIPs as highly effective for nanoporous material modeling and emphasizes that training data quality is key to their success. The open-source MOFSimBench aims to guide adoption and foster further improvement of uMLIPs.

Abstract: Universal machine learning interatomic potentials (uMLIPs) have emerged as
powerful tools for accelerating atomistic simulations, offering scalable and
efficient modeling with accuracy close to quantum calculations. However, their
reliability and effectiveness in practical, real-world applications remain an
open question. Metal-organic frameworks (MOFs) and related nanoporous materials
are highly porous crystals with critical relevance in carbon capture, energy
storage, and catalysis applications. Modeling nanoporous materials presents
distinct challenges for uMLIPs due to their diverse chemistry, structural
complexity, including porosity and coordination bonds, and the absence from
existing training datasets. Here, we introduce MOFSimBench, a benchmark to
evaluate uMLIPs on key materials modeling tasks for nanoporous materials,
including structural optimization, molecular dynamics (MD) stability, the
prediction of bulk properties, such as bulk modulus and heat capacity, and
guest-host interactions. Evaluating over 20 models from various architectures
on a chemically and structurally diverse materials set, we find that
top-performing uMLIPs consistently outperform classical force fields and
fine-tuned machine learning potentials across all tasks, demonstrating their
readiness for deployment in nanoporous materials modeling. Our analysis
highlights that data quality, particularly the diversity of training sets and
inclusion of out-of-equilibrium conformations, plays a more critical role than
model architecture in determining performance across all evaluated uMLIPs. We
release our modular and extendable benchmarking framework at
https://github.com/AI4ChemS/mofsim-bench, providing an open resource to guide
the adoption for nanoporous materials modeling and further development of
uMLIPs.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [297] [The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist](https://arxiv.org/abs/2507.11810)
*Haoxuan Zhang,Ruochi Li,Yang Zhang,Ting Xiao,Jiangping Chen,Junhua Ding,Haihua Chen*

Main category: cs.DL

TL;DR: The paper investigates the evolving roles of Large Language Models (LLMs) in scientific innovation, categorizing them as Evaluators, Collaborators, and Scientists. It provides a taxonomy for understanding their capabilities, boundaries, and impacts on science.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in existing surveys regarding the understanding of LLMs' transformative potential in scientific innovation, given contemporary challenges like information overload and disciplinary silos.

Method: The survey establishes a hierarchical framework categorizing LLM roles (Evaluator, Collaborator, Scientist) and offers a taxonomy to delineate their contributions to structured research and discovery. It examines current methodologies, benchmarks, systems, and metrics.

Result: The research develops a unified taxonomy of LLM capabilities, clarifies interaction patterns between humans and AI, and provides systematic insights into their transformative role in science.

Conclusion: LLMs are reframed not merely as tools but as catalysts capable of possibly reshaping the foundational aspects of scientific inquiry, offering conceptual and practical guidance for future AI-driven research.

Abstract: Scientific innovation is undergoing a paradigm shift driven by the rapid
advancement of Large Language Models (LLMs). As science faces mounting
challenges including information overload, disciplinary silos, and diminishing
returns on conventional research methods, LLMs are emerging as powerful agents
capable not only of enhancing scientific workflows but also of participating in
and potentially leading the innovation process. Existing surveys mainly focus
on different perspectives, phrases, and tasks in scientific research and
discovery, while they have limitations in understanding the transformative
potential and role differentiation of LLM. This survey proposes a comprehensive
framework to categorize the evolving roles of LLMs in scientific innovation
across three hierarchical levels: Evaluator, Collaborator, and Scientist. We
distinguish between LLMs' contributions to structured scientific research
processes and open-ended scientific discovery, thereby offering a unified
taxonomy that clarifies capability boundaries, evaluation criteria, and
human-AI interaction patterns at each level. Through an extensive analysis of
current methodologies, benchmarks, systems, and evaluation metrics, this survey
delivers an in-depth and systematic synthesis on LLM-driven scientific
innovation. We present LLMs not only as tools for automating existing
processes, but also as catalysts capable of reshaping the epistemological
foundations of science itself. This survey offers conceptual clarity, practical
guidance, and theoretical foundations for future research, while also
highlighting open challenges and ethical considerations in the pursuit of
increasingly autonomous AI-driven science. Resources related to this survey can
be accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [298] [Image-Based Multi-Survey Classification of Light Curves with a Pre-Trained Vision Transformer](https://arxiv.org/abs/2507.11711)
*Daniel Moreno-Cartagena,Guillermo Cabrera-Vives,Alejandra M. Muñoz Arancibia,Pavlos Protopapas,Francisco Förster,Márcio Catelan,A. Bayo,Pablo A. Estévez,P. Sánchez-Sáez,Franz E. Bauer,M. Pavez-Herrera,L. Hernández-García,Gonzalo Rojas*

Main category: astro-ph.IM

TL;DR: The paper examines Swin Transformer V2 for photometric classification using light curves from ZTF and ATLAS, finding that joint processing yields optimal results.


<details>
  <summary>Details</summary>
Motivation: To improve photometric classification in multi-survey settings and optimize the integration of diverse survey data.

Method: Evaluation of Swin Transformer V2 using light curves from ZTF and ATLAS, exploring various data integration strategies.

Result: A multi-survey architecture that combines data from both surveys jointly outperforms other strategies.

Conclusion: Modeling survey-specific traits and cross-survey interactions is crucial for scalable time-domain astronomy classifiers.

Abstract: We explore the use of Swin Transformer V2, a pre-trained vision Transformer,
for photometric classification in a multi-survey setting by leveraging light
curves from the Zwicky Transient Facility (ZTF) and the Asteroid
Terrestrial-impact Last Alert System (ATLAS). We evaluate different strategies
for integrating data from these surveys and find that a multi-survey
architecture which processes them jointly achieves the best performance. These
results highlight the importance of modeling survey-specific characteristics
and cross-survey interactions, and provide guidance for building scalable
classifiers for future time-domain astronomy.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [299] [Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker](https://arxiv.org/abs/2507.12378)
*Rachna Saxena,Abhijeet Kumar,Suresh Shanmugam*

Main category: cs.IR

TL;DR: The paper proposes a scalable and efficient method for retrieval in multimodal question-answering systems using hybrid search and late interaction re-ranking, addressing challenges from current methods.


<details>
  <summary>Details</summary>
Motivation: Existing systems struggle to handle multimodal content (e.g., tables, charts) and suffer from inefficiencies like longer context length retrieval or computational overhead in multimodal large language models.

Method: The proposed method combines hybrid search (using metadata and embeddings) with a state-of-the-art late interaction re-ranker, enabling efficient retrieval. MLLMs are then prompted for answer generation based on contextualized matching pages.

Result: The framework demonstrates significant retrieval speed-ups and scalability without degrading performance quality, making it suitable for enterprise adoption.

Conclusion: The approach provides a practical solution for scalable and efficient retrieval in multimodal Q&A systems, addressing limitations of current methods and suitable for real-world use cases.

Abstract: Traditional information extraction systems face challenges with text only
language models as it does not consider infographics (visual elements of
information) such as tables, charts, images etc. often used to convey complex
information to readers. Multimodal LLM (MLLM) face challenges of finding needle
in the haystack problem i.e., either longer context length or substantial
number of documents as search space. Late interaction mechanism over visual
language models has shown state of the art performance in retrieval-based
vision augmented Q&A tasks. There are yet few challenges using it for RAG based
multi-modal Q&A. Firstly, many popular and widely adopted vector databases do
not support native multi-vector retrieval. Secondly, late interaction requires
computation which inflates space footprint and can hinder enterprise adoption.
Lastly, the current state of late interaction mechanism does not leverage the
approximate neighbor search indexing methods for large speed ups in retrieval
process. This paper explores a pragmatic approach to make vision retrieval
process scalable and efficient without compromising on performance quality. We
propose multi-step custom implementation utilizing widely adopted hybrid search
(metadata & embedding) and state of the art late interaction re-ranker to
retrieve best matching pages. Finally, MLLM are prompted as reader to generate
answers from contextualized best matching pages. Through experiments, we
observe that the proposed design is scalable (significant speed up) and stable
(without degrading performance quality), hence can be used as production
systems at enterprises.

</details>


### [300] [Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control](https://arxiv.org/abs/2507.12202)
*Anton Klenitskiy,Konstantin Polev,Daria Denisova,Alexey Vasilev,Dmitry Simakov,Gleb Gusev*

Main category: cs.IR

TL;DR: This paper explores using sparse autoencoders (SAE) with transformer-based models for sequential recommendations, promoting better interpretability and control over model behavior.


<details>
  <summary>Details</summary>
Motivation: Sequential recommendation models, often based on transformers, are hard to interpret as they function as black boxes. Understanding their internal mechanisms is crucial for better user interaction and control in real-world applications.

Method: The authors applied sparse autoencoders (SAE) to transformer's internal layers in sequential recommendation tasks. SAE learns sparse linear representations of the models' hidden states to extract interpretable and monosemantic features.

Result: SAE successfully extracted more understandable and specific directions from transformer hidden layers. Furthermore, these learned features enabled flexible adjustments of the model's recommendations to better align with user preferences in diverse contexts.

Conclusion: Sparse autoencoders offer a practical and effective approach for enhancing transparency, interpretability, and user-control in transformer-based sequential recommendation systems.

Abstract: Many current state-of-the-art models for sequential recommendations are based
on transformer architectures. Interpretation and explanation of such black box
models is an important research question, as a better understanding of their
internals can help understand, influence, and control their behavior, which is
very important in a variety of real-world applications. Recently sparse
autoencoders (SAE) have been shown to be a promising unsupervised approach for
extracting interpretable features from language models. These autoencoders
learn to reconstruct hidden states of the transformer's internal layers from
sparse linear combinations of directions in their activation space.
  This paper is focused on the application of SAE to the sequential
recommendation domain. We show that this approach can be successfully applied
to the transformer trained on a sequential recommendation task: learned
directions turn out to be more interpretable and monosemantic than the original
hidden state dimensions. Moreover, we demonstrate that the features learned by
SAE can be used to effectively and flexibly control the model's behavior,
providing end-users with a straightforward method to adjust their
recommendations to different custom scenarios and contexts.

</details>


### [301] [Looking for Fairness in Recommender Systems](https://arxiv.org/abs/2507.12242)
*Cécile Logé*

Main category: cs.IR

TL;DR: The abstract explores fairness in recommender systems and strategies for mitigating filter bubbles to ensure diverse and inclusive content recommendations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address fairness in recommender systems, focusing on the impact of filter bubbles on users, content creators, and society.

Method: It proposes defining performance metrics for diversity and modifying recommender systems to balance personalized recommendations with societal goals.

Result: Applying diversity metrics can help reduce manipulation, broaden content exposure for creators, and foster varied cultures.

Conclusion: Incorporating diversity-focused metrics allows recommender systems to promote fairness and inclusivity in content suggestions, benefiting users, creators, and society.

Abstract: Recommender systems can be found everywhere today, shaping our everyday
experience whenever we're consuming content, ordering food, buying groceries
online, or even just reading the news. Let's imagine we're in the process of
building a recommender system to make content suggestions to users on social
media. When thinking about fairness, it becomes clear there are several
perspectives to consider: the users asking for tailored suggestions, the
content creators hoping for some limelight, and society at large, navigating
the repercussions of algorithmic recommendations. A shared fairness concern
across all three is the emergence of filter bubbles, a side-effect that takes
place when recommender systems are almost "too good", making recommendations so
tailored that users become inadvertently confined to a narrow set of
opinions/themes and isolated from alternative ideas. From the user's
perspective, this is akin to manipulation. From the small content creator's
perspective, this is an obstacle preventing them access to a whole range of
potential fans. From society's perspective, the potential consequences are
far-reaching, influencing collective opinions, social behavior and political
decisions. How can our recommender system be fine-tuned to avoid the creation
of filter bubbles, and ensure a more inclusive and diverse content landscape?
Approaching this problem involves defining one (or more) performance metric to
represent diversity, and tweaking our recommender system's performance through
the lens of fairness. By incorporating this metric into our evaluation
framework, we aim to strike a balance between personalized recommendations and
the broader societal goal of fostering rich and varied cultures and points of
view.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [302] [A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy](https://arxiv.org/abs/2507.11853)
*J. Senthilnath,Jayasanker Jayabalan,Zhuoyi Lin,Aye Phyu Phyu Aung,Chen Hao,Kaixin Xu,Yeow Kheng Lim,F. C. Wellstood*

Main category: physics.ins-det

TL;DR: An advanced Spatial-Physics Informed Model (SPIM) is proposed to improve non-destructive testing in semiconductor packaging by addressing eddy current effects, misalignment issues, and enhancing magnetic field inversion.


<details>
  <summary>Details</summary>
Motivation: The complexity and depth of advanced semiconductor packaging layers pose significant challenges for non-destructive testing using Magnetic Field Imaging (MFI). Existing methods using Fast Fourier Transform (FFT) inadequately address eddy current effects and misalignment issues.

Method: The paper presents SPIM, which aligns sharp wire field signals to mitigate eddy currents, corrects skew misalignment in SQUID microscope imaging, and integrates the Biot-Savart Law with FFT for magnetic field inversion.

Result: SPIM enhances the sharpness of the I-channel by 0.3%, reduces Q-channel sharpness by 25%, and eliminates rotational and skew misalignments of 0.30 in real images.

Conclusion: SPIM demonstrates the effectiveness of combining spatial analysis and physics-driven models, refining magnetic imaging for advanced applications in semiconductor non-destructive testing.

Abstract: The development of advanced packaging is essential in the semiconductor
manufacturing industry. However, non-destructive testing (NDT) of advanced
packaging becomes increasingly challenging due to the depth and complexity of
the layers involved. In such a scenario, Magnetic field imaging (MFI) enables
the imaging of magnetic fields generated by currents. For MFI to be effective
in NDT, the magnetic fields must be converted into current density. This
conversion has typically relied solely on a Fast Fourier Transform (FFT) for
magnetic field inversion; however, the existing approach does not consider eddy
current effects or image misalignment in the test setup. In this paper, we
present a spatial-physics informed model (SPIM) designed for a 3D spiral sample
scanned using Superconducting QUantum Interference Device (SQUID) microscopy.
The SPIM encompasses three key components: i) magnetic image enhancement by
aligning all the "sharp" wire field signals to mitigate the eddy current effect
using both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii)
magnetic image alignment that addresses skew effects caused by any misalignment
of the scanning SQUID microscope relative to the wire segments; and (iii) an
inversion method for converting magnetic fields to magnetic currents by
integrating the Biot-Savart Law with FFT. The results show that the SPIM
improves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%.
Also, we were able to remove rotational and skew misalignments of 0.30 in a
real image. Overall, SPIM highlights the potential of combining spatial
analysis with physics-driven models in practical applications.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [303] [Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network](https://arxiv.org/abs/2507.11799)
*Shin-ichi Ito*

Main category: physics.comp-ph

TL;DR: The paper introduces an NN-based method to solve complex equations efficiently for modeling shrinkage-induced fragmentation, showing high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Address computational inefficiencies in traditional methods for solving shrinkage-induced fragmentation models.

Method: Leverage neural networks to directly map input parameters to probability density functions, bypassing numerical solutions to governing equations.

Result: Achieved lower computational costs with accuracy comparable to or better than conventional finite difference methods during validation on synthetic data.

Conclusion: The NN-based approach is computationally efficient and accurate, serving as a foundation for inverse analysis and adaptable to extended model structures.

Abstract: This paper presents a neural network (NN)-based solver for an
integro-differential equation that models shrinkage-induced fragmentation. The
proposed method directly maps input parameters to the corresponding probability
density function without numerically solving the governing equation, thereby
significantly reducing computational costs. Specifically, it enables efficient
evaluation of the density function in Monte Carlo simulations while maintaining
accuracy comparable to or even exceeding that of conventional finite difference
schemes. Validatation on synthetic data demonstrates both the method's
computational efficiency and predictive reliability. This study establishes a
foundation for the data-driven inverse analysis of fragmentation and suggests
the potential for extending the framework beyond pre-specified model
structures.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [304] [Syntax Repair as Language Intersection](https://arxiv.org/abs/2507.11873)
*Breandan Considine*

Main category: cs.FL

TL;DR: The paper proposes a novel approach to syntax error repair in context-free languages, defining it as a language intersection problem, and achieves state-of-the-art results on a Python syntax repair benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of repairing syntax errors in arbitrary context-free languages efficiently while ensuring valid repairs.

Method: The authors model syntax error repair as a language intersection problem, leveraging the Bar-Hillel construction from formal language theory and CFL reachability. They also use the Brzozowski derivative to create an enumeration algorithm.

Result: The proposed algorithm and implementation demonstrated state-of-the-art performance, surpassing existing solutions on a Python syntax repair benchmark.

Conclusion: Repairing syntax errors within a finite edit distance is feasible in polylogarithmic parallel time, providing both theoretical insights and practical advancements in error handling for context-free languages.

Abstract: We introduce a new technique for repairing syntax errors in arbitrary
context-free languages. This technique models syntax repair as a language
intersection problem by defining a finite language that provably generates
every syntactically valid repair within a given edit distance. Leveraging a
theoretical connection between the Bar-Hillel construction from formal language
theory and CFL reachability from program analysis, we show that repairability
in a finite number of typographic edits is polylogarithmic parallel time
decidable and provide an enumeration algorithm based on the Brzozowski
derivative. Finally, we evaluate this algorithm and its implementation,
demonstrating state-of-the-art results on a Python syntax repair benchmark.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [305] [Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts](https://arxiv.org/abs/2507.12404)
*Yeming Xian,Xiaoming Wang,Yanfa Yan*

Main category: physics.data-an

TL;DR: The study proposes a two-phase framework combining neural networks (NN), feature importance analysis, and symbolic regression (SR) to create interpretable and accurate catalysts descriptors for the oxygen evolution reaction (OER).


<details>
  <summary>Details</summary>
Motivation: The need to develop accurate and interpretable descriptors for oxide perovskite catalysts in OER while addressing challenges from high-dimensional inputs and small sample sizes.

Method: A two-phase approach combining NN, feature importance analysis, and SR: Phase I focused on improving known structural descriptors with small datasets, and Phase II explored a broader set of features, isolating LUMO energy as a significant descriptor.

Result: In Phase I, improved and interpreted the µ/t descriptor achieving low MAEs. In Phase II, identified LUMO energy as a key electronic descriptor, achieving further accuracy improvements and physical interpretability.

Conclusion: The combined use of NN and SR successfully allowed for the discovery of descriptors with both physical meaning and predictive accuracy, proving to be effective for materials informatics even in data-scarce conditions.

Abstract: Understanding and predicting the activity of oxide perovskite catalysts for
the oxygen evolution reaction (OER) requires descriptors that are both accurate
and physically interpretable. While symbolic regression (SR) offers a path to
discover such formulas, its performance degrades with high-dimensional inputs
and small datasets. We present a two-phase framework that combines neural
networks (NN), feature importance analysis, and symbolic regression (SR) to
discover interpretable descriptors for OER activity in oxide perovskites. In
Phase I, using a small dataset and seven structural features, we reproduce and
improve the known {\mu}/t descriptor by engineering composite features and
applying symbolic regression, achieving training and validation MAEs of 22.8
and 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce
dimensionality, and identify LUMO energy as a key electronic descriptor. A
final formula using {\mu}/t, {\mu}/RA, and LUMO energy achieves improved
accuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong
physical interpretability. Our results demonstrate that NN-guided symbolic
regression enables accurate, interpretable, and physically meaningful
descriptor discovery in data-scarce regimes, indicating interpretability need
not sacrifice accuracy for materials informatics.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [306] [Formal Verification of Neural Certificates Done Dynamically](https://arxiv.org/abs/2507.11987)
*Thomas A. Henzinger,Konstantin Kueffner,Emily Yu*

Main category: cs.SC

TL;DR: This paper introduces a runtime monitoring framework for on-the-fly verification of neural certificates, specifically ReLU-based control barrier functions, in cyber-physical systems, addressing scalability challenges in traditional static verification methods.


<details>
  <summary>Details</summary>
Motivation: Neural certificates, though effective as proofs of correctness in safety-critical systems, encounter scalability limitations during traditional formal verification due to exhaustive state-space exploration.

Method: The authors propose a runtime monitoring framework that observes the system during deployment, performing real-time verification of the certificate over a finite prediction horizon without needing access to the control policy.

Result: The approach is instantiated for ReLU-based control barrier functions and shown to detect safety violations and incorrect certificates effectively while maintaining minimal overhead.

Conclusion: The proposed monitoring framework serves as a lightweight and practical alternative to static verification, enabling efficient and timely assurance of system safety.

Abstract: Neural certificates have emerged as a powerful tool in cyber-physical systems
control, providing witnesses of correctness. These certificates, such as
barrier functions, often learned alongside control policies, once verified,
serve as mathematical proofs of system safety. However, traditional formal
verification of their defining conditions typically faces scalability
challenges due to exhaustive state-space exploration. To address this
challenge, we propose a lightweight runtime monitoring framework that
integrates real-time verification and does not require access to the underlying
control policy. Our monitor observes the system during deployment and performs
on-the-fly verification of the certificate over a lookahead region to ensure
safety within a finite prediction horizon. We instantiate this framework for
ReLU-based control barrier functions and demonstrate its practical
effectiveness in a case study. Our approach enables timely detection of safety
violations and incorrect certificates with minimal overhead, providing an
effective but lightweight alternative to the static verification of the
certificates.

</details>


### [307] [FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization](https://arxiv.org/abs/2507.12366)
*Yifei Zhou,Xuchu Huang,Chenyu Ni,Min Zhou,Zheyu Yan,Xunzhao Yin,Cheng Zhuo*

Main category: cs.SC

TL;DR: This paper introduces FactorHD, a novel model within Hyperdimensional Computing (HDC), tackling challenges in representing complex class-subclass relations.


<details>
  <summary>Details</summary>
Motivation: Current HDC models struggle with factorizing class-subclass relations, a key aspect for advanced neuro-symbolic AI systems.

Method: FactorHD employs symbolic encoding with a memorization clause and an efficient factorization algorithm to identify and eliminate redundant classes.

Result: FactorHD delivers exceptional speed (5667x faster at large representation sizes) and high accuracy (92.48% factorization on CIFAR-10) when integrated with ResNet-18.

Conclusion: FactorHD significantly enhances HDC's capability in handling complex class-subclass relations, marking a breakthrough in neuro-symbolic AI representation and factorization tasks.

Abstract: Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical
analysis and reasoning. Hyperdimensional Computing (HDC), a promising
brain-inspired computational model, is integral to neuro-symbolic AI. Various
HDC models have been proposed to represent class-instance and class-class
relations, but when representing the more complex class-subclass relation,
where multiple objects associate different levels of classes and subclasses,
they face challenges for factorization, a crucial task for neuro-symbolic AI
systems. In this article, we propose FactorHD, a novel HDC model capable of
representing and factorizing the complex class-subclass relation efficiently.
FactorHD features a symbolic encoding method that embeds an extra memorization
clause, preserving more information for multiple objects. In addition, it
employs an efficient factorization algorithm that selectively eliminates
redundant classes by identifying the memorization clause of the target class.
Such model significantly enhances computing efficiency and accuracy in
representing and factorizing multiple objects with class-subclass relation,
overcoming limitations of existing HDC models such as "superposition
catastrophe" and "the problem of 2". Evaluations show that FactorHD achieves
approximately 5667x speedup at a representation size of 10^9 compared to
existing HDC models. When integrated with the ResNet-18 neural network,
FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [308] [Neural Polar Decoders for Deletion Channels](https://arxiv.org/abs/2507.12329)
*Ziv Aharoni,Henry D. Pfister*

Main category: cs.IT

TL;DR: The paper introduces a neural polar decoder (NPD) for deletion channels with constant deletion rates, significantly reducing computational complexity compared to traditional polar decoders.


<details>
  <summary>Details</summary>
Motivation: Current polar decoders for deletion channels are computationally expensive, limiting their application to short-to-moderate block lengths. This paper aims to address the high complexity and extend their usability.

Method: A modified neural polar decoder architecture with four neural networks is proposed, where one network's structure is adjusted to handle deletion channels. It achieves a computational complexity of $O(AN\log N)$, where $A$ is a user-defined computational budget.

Result: Experimental evaluations demonstrate that the extended NPD supports deletion rates of $\delta \in \{0.01, 0.1\}$ with verification against ground truth trellis decoding. The reduced complexity allows for additional list decoding, enhancing performance.

Conclusion: The extended NPD offers a computationally efficient solution for decoding deletion channels and has potential applications in emerging areas, such as DNA storage.

Abstract: This paper introduces a neural polar decoder (NPD) for deletion channels with
a constant deletion rate. Existing polar decoders for deletion channels exhibit
high computational complexity of $O(N^4)$, where $N$ is the block length. This
limits the application of polar codes for deletion channels to
short-to-moderate block lengths. In this work, we demonstrate that employing
NPDs for deletion channels can reduce the computational complexity. First, we
extend the architecture of the NPD to support deletion channels. Specifically,
the NPD architecture consists of four neural networks (NNs), each replicating
fundamental successive cancellation (SC) decoder operations. To support
deletion channels, we change the architecture of only one. The computational
complexity of the NPD is $O(AN\log N)$, where the parameter $A$ represents a
computational budget determined by the user and is independent of the channel.
We evaluate the new extended NPD for deletion channels with deletion rates
$\delta\in\{0.01, 0.1\}$ and we verify the NPD with the ground truth given by
the trellis decoder by Tal et al. We further show that due to the reduced
complexity of the NPD, we are able to incorporate list decoding and further
improve performance. We believe that the extended NPD presented here could have
applications in future technologies like DNA storage.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [309] [Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions](https://arxiv.org/abs/2507.11783)
*Gayal Kuruppu,Neeraj Wagh,Yogatheesan Varatharajah*

Main category: eess.SP

TL;DR: This paper reviews the first-generation EEG foundation models (EEG-FMs) and explores their methods, findings, and limitations for robust EEG feature extraction.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in EEG analysis, such as the lack of robust pattern learning and reliance on annotated signals, through self-supervised EEG models.

Method: The authors conducted a systematic review of 10 early EEG-FMs, focusing on their methodologies and empirical results.

Result: Most EEG-FMs use transformer-based models and masked sequence reconstruction, but evaluations are inconsistent and limited.

Conclusion: Future EEG-FMs should focus on standardized evaluations, scalability, and collaboration with domain experts to advance real-world applications and adoption.

Abstract: Patterns of electrical brain activity recorded via electroencephalography
(EEG) offer immense value for scientific and clinical investigations. The
inability of supervised EEG encoders to learn robust EEG patterns and their
over-reliance on expensive signal annotations have sparked a transition towards
general-purpose self-supervised EEG encoders, i.e., EEG foundation models
(EEG-FMs), for robust and scalable EEG feature extraction. However, the
real-world readiness of early EEG-FMs and the rubric for long-term research
progress remain unclear. A systematic and comprehensive review of
first-generation EEG-FMs is therefore necessary to understand the current
state-of-the-art and identify key directions for future EEG-FMs. To that end,
this study reviews 10 early EEG-FMs and presents a critical synthesis of their
methodology, empirical findings, and outstanding research gaps. We find that
most EEG-FMs adopt a sequence-based modeling scheme that relies on
transformer-based backbones and the reconstruction of masked sequences for
self-supervision. However, model evaluations remain heterogeneous and largely
limited, making it challenging to assess their practical off-the-shelf utility.
In addition to adopting standardized and realistic evaluations, future work
should demonstrate more substantial scaling effects and make principled and
trustworthy choices throughout the EEG representation learning pipeline. We
believe that developing benchmarks, software tools, technical methodologies,
and applications in collaboration with domain experts may further advance the
translational utility and real-world adoption of EEG-FMs.

</details>


### [310] [DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi](https://arxiv.org/abs/2507.12132)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: The paper introduces Doppler radiance fields (DoRF), leveraging Wi-Fi CSI data to improve human activity recognition (HAR) generalizability via a 3D latent motion representation.


<details>
  <summary>Details</summary>
Motivation: Current Wi-Fi-based HAR struggles with issues of insufficient generalizability for practical use.

Method: The paper draws inspiration from neural radiance fields (NeRF) and reconstructs a 3D latent motion representation from 1D Doppler velocity projections extracted from Wi-Fi CSI. This latent representation is used to create Doppler radiance fields (DoRFs).

Result: DoRF enhances generalization accuracy and robustness of Wi-Fi-based HAR, effectively handling environmental variability.

Conclusion: The study demonstrates the feasibility of DoRFs in improving HAR generalizability, suggesting strong utility for real-world remote sensing applications.

Abstract: Wi-Fi Channel State Information (CSI) has gained increasing interest for
remote sensing applications. Recent studies show that Doppler velocity
projections extracted from CSI can enable human activity recognition (HAR) that
is robust to environmental changes and generalizes to new users. However,
despite these advances, generalizability still remains insufficient for
practical deployment. Inspired by neural radiance fields (NeRF), which learn a
volumetric representation of a 3D scene from 2D images, this work proposes a
novel approach to reconstruct an informative 3D latent motion representation
from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The
resulting latent representation is then used to construct a uniform Doppler
radiance field (DoRF) of the motion, providing a comprehensive view of the
performed activity and improving the robustness to environmental variability.
The results show that the proposed approach noticeably enhances the
generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential
of DoRFs for practical sensing applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [311] [Expanding ML-Documentation Standards For Better Security](https://arxiv.org/abs/2507.12003)
*Cara Ellen Appel*

Main category: cs.CR

TL;DR: The paper finds low awareness and poor practices in ML-security documentation and proposes an expanded framework for security-focused ML documentation.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the gaps in ML-security and improving the quality of ML-documentation, as current practices lack standardization and often ignore critical IT-security aspects.

Method: The paper conducts an extensive literature review to assess the current state of ML-security and documentation practices. It proposes an expanded method for documentation by adding security-relevant sections to existing standards like Model Cards and Datasheets for Datasets.

Result: Results show low quality and lack of standardization in ML documentation practices. The study introduces a novel framework for incorporating security requirements into ML-documentation.

Conclusion: The paper concludes that enhanced documentation, incorporating security considerations, is essential to address existing gaps and improve ML-security practices.

Abstract: This article presents the current state of ML-security and of the
documentation of ML-based systems, models and datasets in research and practice
based on an extensive review of the existing literature. It shows a generally
low awareness of security aspects among ML-practitioners and organizations and
an often unstandardized approach to documentation, leading to overall low
quality of ML-documentation. Existing standards are not regularly adopted in
practice and IT-security aspects are often not included in documentation. Due
to these factors, there is a clear need for improved security documentation in
ML, as one step towards addressing the existing gaps in ML-security. To achieve
this, we propose expanding existing documentation standards for
ML-documentation to include a security section with specific security relevant
information. Implementing this, a novel expanded method of documenting security
requirements in ML-documentation is presented, based on the existing Model
Cards and Datasheets for Datasets standards, but with the recommendation to
adopt these findings in all ML-documentation.

</details>


### [312] [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
*Brendan Murphy,Dillon Bowen,Shahrad Mohammadzadeh,Julius Broomfield,Adam Gleave,Kellin Pelrine*

Main category: cs.CR

TL;DR: The paper investigates vulnerabilities in AI models exposed by fine-tuning methods, showing how they can be transformed to perform harmful tasks effectively.


<details>
  <summary>Details</summary>
Motivation: Highlight the urgency of developing tamper-resistant safeguards for AI systems, given their increasing vulnerability to abuse through fine-tuning.

Method: Introduces jailbreak-tuning, an approach that exploits fine-tuning to bypass existing moderation safeguards and enable AI systems to perform detailed harmful tasks.

Result: Fine-tuned models were shown to successfully execute high-quality harmful requests, including cyberattacks and crimes, and exhibit increased vulnerability to stealthy attacks.

Conclusion: Organizations and policymakers should exercise caution with fine-tunable models, as releasing such systems could effectively release versions optimized for malicious use.

Abstract: AI systems are rapidly advancing in capability, and frontier model developers
broadly acknowledge the need for safeguards against serious misuse. However,
this paper demonstrates that fine-tuning, whether via open weights or closed
fine-tuning APIs, can produce helpful-only models. In contrast to prior work
which is blocked by modern moderation systems or achieved only partial removal
of safeguards or degraded output quality, our jailbreak-tuning method teaches
models to generate detailed, high-quality responses to arbitrary harmful
requests. For example, OpenAI, Google, and Anthropic models will fully comply
with requests for CBRN assistance, executing cyberattacks, and other criminal
activity. We further show that backdoors can increase not only the stealth but
also the severity of attacks, while stronger jailbreak prompts become even more
effective in fine-tuning attacks, linking attack and potentially defenses in
the input and weight spaces. Not only are these models vulnerable, more recent
ones also appear to be becoming even more vulnerable to these attacks,
underscoring the urgent need for tamper-resistant safeguards. Until such
safeguards are discovered, companies and policymakers should view the release
of any fine-tunable model as simultaneously releasing its evil twin: equally
capable as the original model, and usable for any malicious purpose within its
capabilities.

</details>


### [313] [Challenges in GenAI and Authentication: a scoping review](https://arxiv.org/abs/2507.11775)
*Wesley dos Reis Bezerra,Lais Machado Bezerra,Carlos Becker Westphall*

Main category: cs.CR

TL;DR: This paper analyzes 88 documents to explore how generative AI affects authentication and security, identifying challenges, gaps, and threats.


<details>
  <summary>Details</summary>
Motivation: To address the evolving challenges in authentication and security posed by generative AI advancements.

Method: A scoping review of 88 documents from major academic databases, analyzed through six guiding questions and individualized lenses.

Result: The study identifies significant gaps, threats, and challenges in handling generative AI's implications on various media types like images, text, audio, and video.

Conclusion: The analysis provides a foundation for future research focusing on authentication, generative AI, and associated security measures.

Abstract: Authentication and authenticity have been a security challenge since the
beginning of information sharing, especially in the context of digital
information. With the advancement of generative artificial intelligence, these
challenges have evolved, demanding a more up-to-date analysis of their impacts
on society and system security. This work presents a scoping review that
analyzed 88 documents from the IEEExplorer, Scopus, and ACM databases,
promoting an analysis of the resulting portfolio through six guiding questions
focusing on the most relevant work, challenges, attack surfaces, threats,
proposed solutions, and gaps. Finally, the portfolio articles are analyzed
through this guiding research lens and also receive individualized analysis.
The results consistently outline the challenges, gaps, and threats related to
images, text, audio, and video, thereby supporting new research in the areas of
authentication and generative artificial intelligence.

</details>


### [314] [Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification](https://arxiv.org/abs/2507.11943)
*Haiwei Lin,Shoko Imaizumi,Hitoshi Kiya*

Main category: cs.CR

TL;DR: The paper proposes a privacy-preserving mechanism to adapt pre-trained vision transformer (ViT) models using low-rank adaptation methods with frozen weights.


<details>
  <summary>Details</summary>
Motivation: To reduce the number of trainable parameters while ensuring competitive accuracy in privacy-preserving ViT model training.

Method: Trainable rank decomposition matrices are incorporated into each ViT layer, while keeping the patch embedding layer unfrozen to enhance efficiency.

Result: The approach maintains nearly identical accuracy to full-tuning while significantly lowering trainable parameter count.

Conclusion: This method provides an efficient balance between low parameter counts and high performance in privacy-preserving training for ViT models.

Abstract: We propose a low-rank adaptation method for training privacy-preserving
vision transformer (ViT) models that efficiently freezes pre-trained ViT model
weights. In the proposed method, trainable rank decomposition matrices are
injected into each layer of the ViT architecture, and moreover, the patch
embedding layer is not frozen, unlike in the case of the conventional low-rank
adaptation methods. The proposed method allows us not only to reduce the number
of trainable parameters but to also maintain almost the same accuracy as that
of full-time tuning.

</details>


### [315] [IDFace: Face Template Protection for Efficient and Secure Identification](https://arxiv.org/abs/2507.12050)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Dongsoo Kim,Junbum Shin,Jae Hong Seo*

Main category: cs.CR

TL;DR: This paper proposes IDFace, a homomorphic encryption (HE)-based secure and efficient face identification system that protects face templates while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: With the growing use of face recognition systems (FRS), protecting user privacy, particularly the security of face templates, has become critical. However, current HE-based approaches for template protection are inefficient and impractical for real-world applications.

Method: The authors introduced IDFace, leveraging two innovative techniques: (1) a template representation transformation to reduce the computational cost of matching tests, and (2) a space-efficient encoding to minimize unused encryption space and operational overhead.

Result: IDFace demonstrated the capability to identify a face template from a database of 1 million encrypted templates in 126 milliseconds, only doubling the computational overhead compared to plaintext systems.

Conclusion: IDFace successfully balances privacy protection and efficiency, providing a practical solution for securing face recognition systems through tailored use of homomorphic encryption.

Abstract: As face recognition systems (FRS) become more widely used, user privacy
becomes more important. A key privacy issue in FRS is protecting the user's
face template, as the characteristics of the user's face image can be recovered
from the template. Although recent advances in cryptographic tools such as
homomorphic encryption (HE) have provided opportunities for securing the FRS,
HE cannot be used directly with FRS in an efficient plug-and-play manner. In
particular, although HE is functionally complete for arbitrary programs, it is
basically designed for algebraic operations on encrypted data of predetermined
shape, such as a polynomial ring. Thus, a non-tailored combination of HE and
the system can yield very inefficient performance, and many previous HE-based
face template protection methods are hundreds of times slower than plain
systems without protection. In this study, we propose IDFace, a new HE-based
secure and efficient face identification method with template protection.
IDFace is designed on the basis of two novel techniques for efficient searching
on a (homomorphically encrypted) biometric database with an angular metric. The
first technique is a template representation transformation that sharply
reduces the unit cost for the matching test. The second is a space-efficient
encoding that reduces wasted space from the encryption algorithm, thus saving
the number of operations on encrypted templates. Through experiments, we show
that IDFace can identify a face template from among a database of 1M encrypted
templates in 126ms, showing only 2X overhead compared to the identification
over plaintexts.

</details>


### [316] [A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy](https://arxiv.org/abs/2507.12098)
*Xiang Li,Yifan Lin,Yuanzhe Zhang*

Main category: cs.CR

TL;DR: This paper proposes a privacy-centric framework for personalized advertising that integrates federated learning, differential privacy, secure computing, and anomaly detection to ensure both accuracy and privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy leakage and performance issues in personalized advertising systems.

Method: Introduces a framework combining federated learning, differential privacy, distributed feature extraction, dynamic privacy allocation, robust model aggregation, multi-party secure computing, and anomaly detection.

Result: Ensures a balance between recommendation accuracy, system efficiency, communication overhead, and privacy protection, as demonstrated by experimental results.

Conclusion: Provides a practical and theoretical solution to implement privacy-focused technologies in advertisement recommendation systems.

Abstract: To mitigate privacy leakage and performance issues in personalized
advertising, this paper proposes a framework that integrates federated learning
and differential privacy. The system combines distributed feature extraction,
dynamic privacy budget allocation, and robust model aggregation to balance
model accuracy, communication overhead, and privacy protection. Multi-party
secure computing and anomaly detection mechanisms further enhance system
resilience against malicious attacks. Experimental results demonstrate that the
framework achieves dual optimization of recommendation accuracy and system
efficiency while ensuring privacy, providing both a practical solution and a
theoretical foundation for applying privacy protection technologies in
advertisement recommendation.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [317] [Fast Variational Bayes for Large Spatial Data](https://arxiv.org/abs/2507.12251)
*Jiafang Song,Abhirup Datta*

Main category: stat.CO

TL;DR: The paper introduces spVarBayes, a computationally efficient variational Bayes method for large-scale geospatial data analysis, achieving comparable accuracy to spNNGP MCMC methods but with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: MCMC sampling methods for geospatial regression are computationally expensive, while variational methods need improvements in accuracy and speed. This paper seeks to address these limitations.

Method: The authors developed spVarBayes by incorporating calculus of variations, closed-form gradient updates, and linear response corrections to improve efficiency and accuracy. Covariates are included in the model, and the methods are implemented as an R-package.

Result: Simulation experiments show that spVarBayes rivals the accuracy of spNNGP MCMC while reducing computational costs. It outperforms existing variational methods in both accuracy and speed.

Conclusion: spVarBayes represents an effective approach to large-scale geospatial data analysis, providing fast and reliable inference consistent with MCMC methods. It is publicly available for use.

Abstract: Recent variational Bayes methods for geospatial regression, proposed as an
alternative to computationally expensive Markov chain Monte Carlo (MCMC)
sampling, have leveraged Nearest Neighbor Gaussian processes (NNGP) to achieve
scalability. Yet, these variational methods remain inferior in accuracy and
speed compared to spNNGP, the state-of-the-art MCMC-based software for NNGP. We
introduce spVarBayes, a suite of fast variational Bayesian approaches for
large-scale geospatial data analysis using NNGP. Our contributions are
primarily computational. We replace auto-differentiation with a combination of
calculus of variations, closed-form gradient updates, and linear response
corrections for improved variance estimation. We also accommodate covariates
(fixed effects) in the model and offer inference on the variance parameters.
Simulation experiments demonstrate that we achieve comparable accuracy to
spNNGP but with reduced computational costs, and considerably outperform
existing variational inference methods in terms of both accuracy and speed.
Analysis of a large forest canopy height dataset illustrates the practical
implementation of proposed methods and shows that the inference results are
consistent with those obtained from the MCMC approach. The proposed methods are
implemented in publicly available Github R-package spVarBayes.

</details>


### [318] [Surrogate modeling for uncertainty quantification in nonlinear dynamics](https://arxiv.org/abs/2507.12358)
*S. Marelli,S. Schär,B. Sudret*

Main category: stat.CO

TL;DR: This paper reviews surrogate modeling techniques for uncertainty quantification (UQ) in dynamical systems, focusing on time-dependent problems and relevant approaches.


<details>
  <summary>Details</summary>
Motivation: Uncertainty affects the performance and reliability of complex systems in engineering, and quantifying it requires efficient computational methods, especially for time-dependent responses.

Method: The paper classifies types of time-dependent problems and explores surrogate modeling techniques like principal component analysis (PCA) with polynomial chaos, time warping, and nonlinear autoregressive models (NARX), along with practical illustrations.

Result: The review highlights how each method effectively captures the behavior of dynamical systems under uncertainty, illustrated through examples to demonstrate their applicability.

Conclusion: Efficient surrogate models are critical for handling the computational challenges of time-dependent uncertainty quantification, with various promising approaches suited for different problem complexities.

Abstract: Predicting the behavior of complex systems in engineering often involves
significant uncertainty about operating conditions, such as external loads,
environmental effects, and manufacturing variability. As a result, uncertainty
quantification (UQ) has become a critical tool in modeling-based engineering,
providing methods to identify, characterize, and propagate uncertainty through
computational models. However, the stochastic nature of UQ typically requires
numerous evaluations of these models, which can be computationally expensive
and limit the scope of feasible analyses. To address this, surrogate models,
i.e., efficient functional approximations trained on a limited set of
simulations, have become central in modern UQ practice. This book chapter
presents a concise review of surrogate modeling techniques for UQ, with a focus
on the particularly challenging task of capturing the full time-dependent
response of dynamical systems. It introduces a classification of time-dependent
problems based on the complexity of input excitation and discusses
corresponding surrogate approaches, including combinations of principal
component analysis with polynomial chaos expansions, time warping techniques,
and nonlinear autoregressive models with exogenous inputs (NARX models). Each
method is illustrated with simple application examples to clarify the
underlying ideas and practical use.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [319] [Landmark Detection for Medical Images using a General-purpose Segmentation Model](https://arxiv.org/abs/2507.11551)
*Ekaterina Stansfield,Jennifer A. Mitterer,Abdulrahman Altahhan*

Main category: eess.IV

TL;DR: The paper proposes a novel pipeline combining YOLO and SAM models to address challenges in segmenting detailed anatomical landmarks in orthopaedic pelvic radiographs.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation models, such as SAM and MedSAM, lack the fine-grained precision necessary for identifying orthopaedic landmarks in pelvic radiographs.

Method: The authors integrated YOLO for object detection and bounding box generation with SAM for segmentation, enabling precise detection and segmentation of complex anatomical landmarks.

Result: The hybrid model successfully segmented 72 landmarks and 16 complex regions in orthopaedic pelvic radiographs, outperforming standalone approaches.

Conclusion: Combining YOLO and SAM enhances diagnostic segmentation in orthopaedics, making it possible to accurately detect and outline intricate pelvic structures.

Abstract: Radiographic images are a cornerstone of medical diagnostics in orthopaedics,
with anatomical landmark detection serving as a crucial intermediate step for
information extraction. General-purpose foundational segmentation models, such
as SAM (Segment Anything Model), do not support landmark segmentation out of
the box and require prompts to function. However, in medical imaging, the
prompts for landmarks are highly specific. Since SAM has not been trained to
recognize such landmarks, it cannot generate accurate landmark segmentations
for diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has
been trained to identify larger anatomical structures, such as organs and their
parts, and lacks the fine-grained precision required for orthopaedic pelvic
landmarks. To address this limitation, we propose leveraging another
general-purpose, non-foundational model: YOLO. YOLO excels in object detection
and can provide bounding boxes that serve as input prompts for SAM. While YOLO
is efficient at detection, it is significantly outperformed by SAM in
segmenting complex structures. In combination, these two models form a reliable
pipeline capable of segmenting not only a small pilot set of eight anatomical
landmarks but also an expanded set of 72 landmarks and 16 regions with complex
outlines, such as the femoral cortical bone and the pelvic inlet. By using
YOLO-generated bounding boxes to guide SAM, we trained the hybrid model to
accurately segment orthopaedic pelvic radiographs. Our results show that the
proposed combination of YOLO and SAM yields excellent performance in detecting
anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.

</details>


### [320] [3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation](https://arxiv.org/abs/2507.11557)
*Jiaxu Zheng,Meiman He,Xuhui Tang,Xiong Wang,Tuoyu Cao,Tianyi Zeng,Lichi Zhang,Chenyu You*

Main category: eess.IV

TL;DR: The paper introduces a 3D Wavelet Latent Diffusion Model (3D-WLDM) for improving the quality and alignment of synthesized CT images from MR scans in advanced diagnostic and therapeutic workflows.


<details>
  <summary>Details</summary>
Motivation: Existing methods in MR-to-CT synthesis for whole-body imaging struggle with spatial alignment and image quality, making them unreliable for clinical tasks. The paper aims to overcome these limitations.

Method: The authors propose a novel 3D-WLDM that operates in a learned latent space, integrates a Wavelet Residual Module for fine-scale feature reconstruction, employs structural disentanglement to prevent warping, and utilizes Dual Skip Connection Attention for enhanced high-resolution image synthesis.

Result: The proposed model demonstrates improved spatial alignment and image quality for CT images derived from MR scans, especially in the representation of bony structures and soft-tissue contrasts.

Conclusion: 3D-WLDM offers a significant advancement in MR-to-CT synthesis, enhancing image quality and anatomical alignment, thus making it more reliable for contemporary clinical workflows.

Abstract: Magnetic Resonance (MR) imaging plays an essential role in contemporary
clinical diagnostics. It is increasingly integrated into advanced therapeutic
workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance
(PET/MR) imaging and MR-only radiation therapy. These integrated approaches are
critically dependent on accurate estimation of radiation attenuation, which is
typically facilitated by synthesizing Computed Tomography (CT) images from MR
scans to generate attenuation maps. However, existing MR-to-CT synthesis
methods for whole-body imaging often suffer from poor spatial alignment between
the generated CT and input MR images, and insufficient image quality for
reliable use in downstream clinical tasks. In this paper, we present a novel 3D
Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by
performing modality translation in a learned latent space. By incorporating a
Wavelet Residual Module into the encoder-decoder architecture, we enhance the
capture and reconstruction of fine-scale features across image and latent
spaces. To preserve anatomical integrity during the diffusion process, we
disentangle structural and modality-specific characteristics and anchor the
structural component to prevent warping. We also introduce a Dual Skip
Connection Attention mechanism within the diffusion model, enabling the
generation of high-resolution CT images with improved representation of bony
structures and soft-tissue contrast.

</details>


### [321] [CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos](https://arxiv.org/abs/2507.11900)
*Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai*

Main category: eess.IV

TL;DR: The paper introduces CompressedVQA-HDR, a framework for evaluating HDR video quality using advanced neural models. It achieved top performance in a recent IEEE competition.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to improve the assessment of compressed HDR video quality, addressing the lack of generalization in current methods for diverse video types.

Method: The framework employs the Swin Transformer for full-reference (FR) models and SigLip 2 for no-reference (NR) models, with pre-training and mixed-dataset strategies to overcome limited HDR training data.

Result: Experimental evidence shows that the framework outperforms existing models, and it won first place in the FR track of the IEEE ICME 2025 challenge.

Conclusion: CompressedVQA-HDR demonstrates state-of-the-art capabilities for HDR video quality assessment, showcasing its effectiveness and generalization potential.

Abstract: Video compression is a standard procedure applied to all videos to minimize
storage and transmission demands while preserving visual quality as much as
possible. Therefore, evaluating the visual quality of compressed videos is
crucial for guiding the practical usage and further development of video
compression algorithms. Although numerous compressed video quality assessment
(VQA) methods have been proposed, they often lack the generalization capability
needed to handle the increasing diversity of video types, particularly high
dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an
effective VQA framework designed to address the challenges of HDR video quality
assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the
backbone networks for the proposed full-reference (FR) and no-reference (NR)
VQA models, respectively. For the FR model, we compute deep structural and
textural similarities between reference and distorted frames using
intermediate-layer features extracted from the Swin Transformer as its
quality-aware feature representation. For the NR model, we extract the global
mean of the final-layer feature maps from SigLip 2 as its quality-aware
representation. To mitigate the issue of limited HDR training data, we
pre-train the FR model on a large-scale standard dynamic range (SDR) VQA
dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ
an iterative mixed-dataset training strategy across multiple compressed VQA
datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental
results show that our models achieve state-of-the-art performance compared to
existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place
in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand
Challenge at IEEE ICME 2025. The code is available at
https://github.com/sunwei925/CompressedVQA-HDR.

</details>


### [322] [Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach](https://arxiv.org/abs/2507.11561)
*Lucas Erlacher,Samuel Ruipérez-Campillo,Holger Michel,Sven Wellmann,Thomas M. Sutter,Ece Ozkan,Julia E. Vogt*

Main category: eess.IV

TL;DR: The paper proposes a multi-view variational autoencoder (VAE) model to improve the diagnosis of pulmonary hypertension (PH) in newborns using echocardiographic videos.


<details>
  <summary>Details</summary>
Motivation: PH in newborns is challenging to diagnose accurately using traditional methods like echocardiography, which depends heavily on operator expertise.

Method: The authors developed and utilized a multi-view VAE to extract complex latent features from echocardiographic video data for PH prediction, comparing performance against single-view and supervised models.

Result: The proposed model demonstrated superior generalization and classification accuracy compared to single-view and supervised approaches.

Conclusion: Multi-view learning through VAE frameworks enhances robustness and accuracy in PH diagnosis for newborns.

Abstract: Pulmonary hypertension (PH) in newborns is a critical condition characterized
by elevated pressure in the pulmonary arteries, leading to right ventricular
strain and heart failure. While right heart catheterization (RHC) is the
diagnostic gold standard, echocardiography is preferred due to its non-invasive
nature, safety, and accessibility. However, its accuracy highly depends on the
operator, making PH assessment subjective. While automated detection methods
have been explored, most models focus on adults and rely on single-view
echocardiographic frames, limiting their performance in diagnosing PH in
newborns. While multi-view echocardiography has shown promise in improving PH
assessment, existing models struggle with generalizability. In this work, we
employ a multi-view variational autoencoder (VAE) for PH prediction using
echocardiographic videos. By leveraging the VAE framework, our model captures
complex latent representations, improving feature extraction and robustness. We
compare its performance against single-view and supervised learning approaches.
Our results show improved generalization and classification accuracy,
highlighting the effectiveness of multi-view learning for robust PH assessment
in newborns.

</details>


### [323] [Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?](https://arxiv.org/abs/2507.11569)
*Hanxue Gu,Yaqian Chen,Nicholas Konz,Qihang Li,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: This study assesses the efficacy of foundation model-based registration algorithms for challenging breast MRI tasks, finding that while models like SAM excel in global alignment under domain shifts, they fall short in capturing intricate fibroglandular details.


<details>
  <summary>Details</summary>
Motivation: To evaluate if foundation models, which show promise in simpler image registration tasks, can address the complex anatomical variations and challenges present in breast MRI registration.

Method: The research compares five pre-trained encoders (DINO-v2, SAM, MedSAM, SSLSAM, MedCLIP) across multiple breast MRI registration tasks that capture variations in imaging sequences, modalities, patient statuses, and time intervals.

Result: Foundation model-based algorithms, especially SAM, outperform traditional baselines for global breast alignment under domain shifts but fail at capturing fine details of fibroglandular tissue. Additionally, domain-specific pre-training does not consistently enhance performance.

Conclusion: While foundation models exhibit strong global alignment capabilities, further exploration is required to improve their ability to handle complex fine-structure details in breast MRI registration.

Abstract: Foundation models, pre-trained on large image datasets and capable of
capturing rich feature representations, have recently shown potential for
zero-shot image registration. However, their performance has mostly been tested
in the context of rigid or less complex structures, such as the brain or
abdominal organs, and it remains unclear whether these models can handle more
challenging, deformable anatomy. Breast MRI registration is particularly
difficult due to significant anatomical variation between patients, deformation
caused by patient positioning, and the presence of thin and complex internal
structure of fibroglandular tissue, where accurate alignment is crucial.
Whether foundation model-based registration algorithms can address this level
of complexity remains an open question. In this study, we provide a
comprehensive evaluation of foundation model-based registration algorithms for
breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM,
MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that
capture variations in different years and dates, sequences, modalities, and
patient disease status (lesion versus no lesion). Our results show that
foundation model-based algorithms such as SAM outperform traditional
registration baselines for overall breast alignment, especially under large
domain shifts, but struggle with capturing fine details of fibroglandular
tissue. Interestingly, additional pre-training or fine-tuning on medical or
breast-specific images in MedSAM and SSLSAM, does not improve registration
performance and may even decrease it in some cases. Further work is needed to
understand how domain-specific training influences registration and to explore
targeted strategies that improve both global alignment and fine structure
accuracy. We also publicly release our code at
\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.

</details>


### [324] [Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease](https://arxiv.org/abs/2507.12012)
*Matthias Perkonigg,Nina Bastati,Ahmed Ba-Ssalamah,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Xiaofei Zhou,Michael Trauner,Georg Langs*

Main category: eess.IV

TL;DR: This study leverages unsupervised machine learning to analyze liver tissue patterns in magnetic resonance images, identifying a vocabulary that can measure treatment response in liver disease patients.


<details>
  <summary>Details</summary>
Motivation: To develop advanced imaging tools that quantify changes in liver tissue for personalized treatment and the discovery of new therapies.

Method: Unsupervised machine learning with deep clustering networks is used to encode and cluster medical image patches, creating a low-dimensional tissue vocabulary that reflects treatment response.

Result: The tissue vocabulary identifies liver changes relevant to treatment, distinguishes placebo vs. treatment cohorts, and predicts biopsy features from imaging data. Results are validated on another cohort.

Conclusion: The proposed machine learning approach effectively quantifies liver tissue changes, aiding in understanding and optimizing treatment response in liver diseases.

Abstract: Quantifiable image patterns associated with disease progression and treatment
response are critical tools for guiding individual treatment, and for
developing novel therapies. Here, we show that unsupervised machine learning
can identify a pattern vocabulary of liver tissue in magnetic resonance images
that quantifies treatment response in diffuse liver disease. Deep clustering
networks simultaneously encode and cluster patches of medical images into a
low-dimensional latent space to establish a tissue vocabulary. The resulting
tissue types capture differential tissue change and its location in the liver
associated with treatment response. We demonstrate the utility of the
vocabulary on a randomized controlled trial cohort of non-alcoholic
steatohepatitis patients. First, we use the vocabulary to compare longitudinal
liver change in a placebo and a treatment cohort. Results show that the method
identifies specific liver tissue change pathways associated with treatment, and
enables a better separation between treatment groups than established
non-imaging measures. Moreover, we show that the vocabulary can predict biopsy
derived features from non-invasive imaging data. We validate the method on a
separate replication cohort to demonstrate the applicability of the proposed
method.

</details>


### [325] [Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis](https://arxiv.org/abs/2507.12092)
*Nataliia Molchanova,Alessandro Cagol,Mario Ocampo-Pineda,Po-Jui Lu,Matthias Weigel,Xinjie Chen,Erin Beck,Charidimos Tsagkas,Daniel Reich,Colin Vanden Bulcke,Anna Stolting,Serena Borrelli,Pietro Maggi,Adrien Depeursinge,Cristina Granziera,Henning Mueller,Pedro M. Gordaliza,Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: This paper proposes a standardized benchmark for detecting and segmenting cortical lesions in MS using MRI data with the nnU-Net framework, achieving significant detection performance and providing insights into model decision-making.


<details>
  <summary>Details</summary>
Motivation: Cortical lesions in multiple sclerosis have high diagnostic specificity and prognostic relevance but face challenges in routine application due to their subtle appearance, expert annotation difficulties, and lack of automated methods.

Method: The study used a multi-centric dataset of 656 MRI scans from 3T and 7T machines, employing expert-consensus annotations with the self-configuring nnU-Net framework for segmentation and adapted it for improved lesion detection.

Result: The model exhibited strong detection capabilities, achieving F1-scores of 0.64 in-distribution and 0.5 out-of-distribution, and provided an analysis of internal features and errors to understand AI behavior.

Conclusion: The study highlights the impact of data variability and protocol differences on model performance, offers recommendations for clinical adoption barriers, and ensures reproducibility by making models publicly accessible.

Abstract: Cortical lesions (CLs) have emerged as valuable biomarkers in multiple
sclerosis (MS), offering high diagnostic specificity and prognostic relevance.
However, their routine clinical integration remains limited due to subtle
magnetic resonance imaging (MRI) appearance, challenges in expert annotation,
and a lack of standardized automated methods. We propose a comprehensive
multi-centric benchmark of CL detection and segmentation in MRI. A total of 656
MRI scans, including clinical trial and research data from four institutions,
were acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with
expert-consensus annotations. We rely on the self-configuring nnU-Net
framework, designed for medical imaging segmentation, and propose adaptations
tailored to the improved CL detection. We evaluated model generalization
through out-of-distribution testing, demonstrating strong lesion detection
capabilities with an F1-score of 0.64 and 0.5 in and out of the domain,
respectively. We also analyze internal model features and model errors for a
better understanding of AI decision-making. Our study examines how data
variability, lesion ambiguity, and protocol differences impact model
performance, offering future recommendations to address these barriers to
clinical adoption. To reinforce the reproducibility, the implementation and
models will be publicly accessible and ready to use at
https://github.com/Medical-Image-Analysis-Laboratory/ and
https://doi.org/10.5281/zenodo.15911797.

</details>


### [326] [Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation](https://arxiv.org/abs/2507.12427)
*Ashkan Shakarami,Azade Farshad,Yousef Yeganeh,Lorenzo Nicole,Peter Schuffler,Stefano Ghidoni,Nassir Navab*

Main category: eess.IV

TL;DR: The paper introduces UTS, a segmentation framework using fixed-size tiles and a Multi-Level Vision Transformer, enhancing accuracy and efficiency in histopathology tasks.


<details>
  <summary>Details</summary>
Motivation: To reduce annotation effort, enhance computational efficiency, and preserve segmentation accuracy in histopathology analysis.

Method: A tile-based framework (UTS) and Multi-Level Vision Transformer (L-ViT) are proposed for segmenting histopathology images into predefined categories.

Result: The proposed approach outperforms U-Net and transformer-based baselines on over 386,000 image tiles.

Conclusion: UTS is effective for histopathology segmentation and is useful for clinically significant tasks like tumor quantification and surgical margin assessment.

Abstract: We propose UTS, a unit-based tissue segmentation framework for histopathology
that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the
segmentation unit. This approach reduces annotation effort and improves
computational efficiency without compromising accuracy. To implement this
approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits
the multi-level feature representation to capture both fine-grained morphology
and global tissue context. Trained to segment breast tissue into three
categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports
clinically relevant tasks such as tumor-stroma quantification and surgical
margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it
outperforms U-Net variants and transformer-based baselines. Code and Dataset
will be available at GitHub.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [327] [Designing Algorithms for Entropic Optimal Transport from an Optimisation Perspective](https://arxiv.org/abs/2507.12246)
*Vishwak Srinivasan,Qijia Jiang*

Main category: math.OC

TL;DR: This paper proposes optimization-based methods inspired by mirror descent for solving the entropic-regularised optimal transport problem, achieving rapid convergence and acceleration guarantees.


<details>
  <summary>Details</summary>
Motivation: To improve solving the entropic-regularised optimal transport problem by leveraging optimization techniques and achieving better convergence guarantees.

Method: Develop novel methods from an optimization perspective, including semi-dual problems and non-convex constrained problems, alongside a momentum-equipped approach for acceleration.

Result: Achieves non-asymptotic convergence rates under minimal assumptions, with a momentum method showing provable acceleration guarantees.

Conclusion: Optimization-based approaches enhance the efficiency of solving entropic-regularised optimal transport problems, also connecting to dynamical Schrödinger bridge challenges.

Abstract: In this work, we develop a collection of novel methods for the
entropic-regularised optimal transport problem, which are inspired by existing
mirror descent interpretations of the Sinkhorn algorithm used for solving this
problem. These are fundamentally proposed from an optimisation perspective:
either based on the associated semi-dual problem, or based on solving a
non-convex constrained problem over subset of joint distributions. This
optimisation viewpoint results in non-asymptotic rates of convergence for the
proposed methods under minimal assumptions on the problem structure. We also
propose a momentum-equipped method with provable accelerated guarantees through
this viewpoint, akin to those in the Euclidean setting. The broader framework
we develop based on optimisation over the joint distributions also finds an
analogue in the dynamical Schr\"{o}dinger bridge problem.

</details>


### [328] [Risk in Stochastic and Robust Model Predictive Path-Following Control for Vehicular Motion Planning](https://arxiv.org/abs/2304.12063)
*Leon Tolksdorf,Arturo Tejada,Nathan van de Wouw,Christian Birkner*

Main category: math.OC

TL;DR: The paper addresses defining and integrating risk as a constraint in autonomous vehicle motion planning using stochastic and robust model predictive controllers (SMPC & RMPC). It finds SMPC to be less conservative, more adaptable to risk tolerances, and capable of handling uncertainties better than RMPC.


<details>
  <summary>Details</summary>
Motivation: Risk representation and minimization in autonomous vehicles is crucial for safety certification and human-like driving behavior, yet lacks consensus on operational definitions.

Method: Introduced a stochastic risk measure as a constraint in both robust (RMPC) and stochastic (SMPC) nonlinear model predictive path-following controllers, comparing their behaviors through simulations under different risk tolerances and uncertainties.

Result: RMPC exhibited conservative behavior, larger following errors, and lacked human-like traits, particularly under moderate uncertainties. SMPC, in contrast, handled uncertainties better and allowed for adjustable risk tolerances.

Conclusion: SMPC is more effective and flexible for risk management in autonomous vehicle motion planning compared to RMPC, aligning better with human-like driving and safety expectations.

Abstract: In automated driving, risk describes potential harm to passengers of an
autonomous vehicle (AV) and other road users. Recent studies suggest that
human-like driving behavior emerges from embedding risk in AV motion planning
algorithms. Additionally, providing evidence that risk is minimized during the
AV operation is essential to vehicle safety certification. However, there has
yet to be a consensus on how to define and operationalize risk in motion
planning or how to bound or minimize it during operation. In this paper, we
define a stochastic risk measure and introduce it as a constraint into both
robust and stochastic nonlinear model predictive path-following controllers
(RMPC and SMPC respectively). We compare the vehicle's behavior arising from
employing SMPC and RMPC with respect to safety and path-following performance.
Further, the implementation of an automated driving example is provided,
showcasing the effects of different risk tolerances and uncertainty growths in
predictions of other road users for both cases. We find that the RMPC is
significantly more conservative than the SMPC, while also displaying greater
following errors towards references. Further, the RMPCs behavior cannot be
considered as human-like. Moreover, unlike SMPC, the RMPC cannot account for
different risk tolerances. The RMPC generates undesired driving behavior for
even moderate uncertainties, which are handled better by the SMPC.

</details>


### [329] [Improved Analysis for Sign-based Methods with Momentum Updates](https://arxiv.org/abs/2507.12091)
*Wei Jiang,Dingzhi Yu,Sifan Yang,Wenhao Yang,Lijun Zhang*

Main category: math.OC

TL;DR: The paper enhances sign-based optimization with momentum updates for better convergence rates, addressing large batch size and noise assumptions.


<details>
  <summary>Details</summary>
Motivation: Enhance sign-based optimization algorithms to operate effectively with constant batch sizes and no restrictive noise assumptions.

Method: Proposed refined analysis and methods for signSGD with momentum and majority vote adaptations in distributed settings under $l_2$-smoothness conditions.

Result: Achieved improved convergence rates compared to prior methods, validated by numerical experiments.

Conclusion: Enhanced sign-based methods with momentum are more efficient, achieving better convergence rates under less restrictive conditions.

Abstract: In this paper, we present enhanced analysis for sign-based optimization
algorithms with momentum updates. Traditional sign-based methods, under the
separable smoothness assumption, guarantee a convergence rate of
$\mathcal{O}(T^{-1/4})$, but they either require large batch sizes or assume
unimodal symmetric stochastic noise. To address these limitations, we
demonstrate that signSGD with momentum can achieve the same convergence rate
using constant batch sizes without additional assumptions. Our analysis, under
the standard $l_2$-smoothness condition, improves upon the result of the prior
momentum-based signSGD method by a factor of $\mathcal{O}(d^{1/2})$, where $d$
is the problem dimension. Furthermore, we explore sign-based methods with
majority vote in distributed settings and show that the proposed momentum-based
method yields convergence rates of $\mathcal{O}\left( d^{1/2}T^{-1/2} +
dn^{-1/2} \right)$ and $\mathcal{O}\left( \max \{ d^{1/4}T^{-1/4},
d^{1/10}T^{-1/5} \} \right)$, which outperform the previous results of
$\mathcal{O}\left( dT^{-1/4} + dn^{-1/2} \right)$ and $\mathcal{O}\left(
d^{3/8}T^{-1/8} \right)$, respectively. Numerical experiments further validate
the effectiveness of the proposed methods.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [330] [Approaching Optimality for Solving Dense Linear Systems with Low-Rank Structure](https://arxiv.org/abs/2507.11724)
*Michał Dereziński,Aaron Sidford*

Main category: cs.DS

TL;DR: The paper introduces high-accuracy randomized algorithms for solving linear systems and regression problems efficiently, particularly targeting cases where only a few singular values are problematic.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiency in solving well-conditioned linear systems and regression problems with special cases involving a limited number of problematic singular values.

Method: The algorithms use recursive preconditioning frameworks, matrix sketching, and tailored low-rank update formulas to achieve optimal performance.

Result: The algorithms achieve running times of $\tilde O(d^2 + k^\omega)$ for $d \times d$ systems and $\tilde O(\mathrm{nnz}(\mathbf{A}) + d^2 + k^\omega)$ for regression problems, improving upon prior approaches.

Conclusion: The methods push the boundaries of complexity limits for these problems and introduce the first nearly-linear time algorithm for approximate computation of nuclear norms in dense matrices.

Abstract: We provide new high-accuracy randomized algorithms for solving linear systems
and regression problems that are well-conditioned except for $k$ large singular
values. For solving such $d \times d$ positive definite system our algorithms
succeed whp. and run in time $\tilde O(d^2 + k^\omega)$. For solving such
regression problems in a matrix $\mathbf{A} \in \mathbb{R}^{n \times d}$ our
methods succeed whp. and run in time $\tilde O(\mathrm{nnz}(\mathbf{A}) + d^2 +
k^\omega)$ where $\omega$ is the matrix multiplication exponent and
$\mathrm{nnz}(\mathbf{A})$ is the number of non-zeros in $\mathbf{A}$. Our
methods nearly-match a natural complexity limit under dense inputs for these
problems and improve upon a trade-off in prior approaches that obtain running
times of either $\tilde O(d^{2.065}+k^\omega)$ or $\tilde O(d^2 +
dk^{\omega-1})$ for $d\times d$ systems. Moreover, we show how to obtain these
running times even under the weaker assumption that all but $k$ of the singular
values have a suitably bounded generalized mean. Consequently, we give the
first nearly-linear time algorithm for computing a multiplicative approximation
to the nuclear norm of an arbitrary dense matrix. Our algorithms are built on
three general recursive preconditioning frameworks, where matrix sketching and
low-rank update formulas are carefully tailored to the problems' structure.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [331] [CosmoFlow: Scale-Aware Representation Learning for Cosmology with Flow Matching](https://arxiv.org/abs/2507.11842)
*Sidharth Kannan,Tian Qiu,Carolina Cuesta-Lazaro,Haewon Jeong*

Main category: astro-ph.CO

TL;DR: Flow matching generative models, like CosmoFlow, can learn compact and interpretable latent representations of cold dark matter simulation data without supervision.


<details>
  <summary>Details</summary>
Motivation: To develop a method for learning compact latent representations of cosmological simulation data that preserve information for reconstruction, generation, and parameter inference.

Method: Introducing CosmoFlow, a flow matching based generative model, which creates 32x smaller latent representations of field level CDM simulation data without supervision.

Result: CosmoFlow produces semantically rich and interpretable latent representations, enabling data reconstruction, synthesis, and parameter inference across cosmological scales.

Conclusion: Flow matching generative models are effective for compact representation learning, enabling important applications in cosmology including data reconstruction and interpretation of latent features.

Abstract: Generative machine learning models have been demonstrated to be able to learn
low dimensional representations of data that preserve information required for
downstream tasks. In this work, we demonstrate that flow matching based
generative models can learn compact, semantically rich latent representations
of field level cold dark matter (CDM) simulation data without supervision. Our
model, CosmoFlow, learns representations 32x smaller than the raw field data,
usable for field level reconstruction, synthetic data generation, and parameter
inference. Our model also learns interpretable representations, in which
different latent channels correspond to features at different cosmological
scales.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [332] [SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics](https://arxiv.org/abs/2507.11588)
*Suyuan Zhao,Yizhen Luo,Ganbo Yang,Yan Zhong,Hao Zhou,Zaiqing Nie*

Main category: q-bio.GN

TL;DR: The paper introduces SToFM, a multi-scale Spatial Transcriptomics Foundation Model, to enhance ST data analysis by integrating macro-scale tissue morphology, micro-scale cellular environment, and gene-scale expression. SToFM excels in downstream tasks like tissue region segmentation and cell type annotation.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in processing and analyzing spatial transcriptomics data, which requires integrating information across various scales—macro, micro, gene—at vast complexity.

Method: The authors developed SToFM for multi-scale information extraction from ST slices and trained it using SE(2) Transformer on a large corpus of data: SToCorpus-88M.

Result: SToFM demonstrated exceptional performance in tasks like semantic segmentation of tissue regions and annotating cell types.

Conclusion: The efficacy of SToFM showcases advancements in spatial transcriptomics foundational models, paving the way for enhanced biological insights.

Abstract: Spatial Transcriptomics (ST) technologies provide biologists with rich
insights into single-cell biology by preserving spatial context of cells.
Building foundational models for ST can significantly enhance the analysis of
vast and complex data sources, unlocking new perspectives on the intricacies of
biological tissues. However, modeling ST data is inherently challenging due to
the need to extract multi-scale information from tissue slices containing vast
numbers of cells. This process requires integrating macro-scale tissue
morphology, micro-scale cellular microenvironment, and gene-scale gene
expression profile. To address this challenge, we propose SToFM, a multi-scale
Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale
information extraction on each ST slice, to construct a set of ST sub-slices
that aggregate macro-, micro- and gene-scale information. Then an SE(2)
Transformer is used to obtain high-quality cell representations from the
sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest
high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves
outstanding performance on a variety of downstream tasks, such as tissue region
semantic segmentation and cell type annotation, demonstrating its comprehensive
understanding of ST data

</details>


### [333] [RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery](https://arxiv.org/abs/2507.11950)
*Lauren Lui,Torben Nielsen*

Main category: q-bio.GN

TL;DR: RNAMunin, a machine learning model, identifies non-coding RNAs (ncRNAs) directly from genomic sequences without requiring transcriptomic data, making it computationally efficient even for large datasets.


<details>
  <summary>Details</summary>
Motivation: There is a lack of effective methods to identify ncRNAs from genomic data alone, crucial for understanding complete organism regulation.

Method: RNAMunin uses approximately 1 million parameters and is trained on Rfam datasets from 60 Gbp of long read metagenomes to detect ncRNAs based solely on genomic sequences.

Result: RNAMunin successfully identifies ncRNAs in large-scale genomic datasets and operates efficiently without requiring transcription data.

Conclusion: RNAMunin provides a scalable, fast, and efficient solution for detecting ncRNAs, filling a gap in bioinformatics and expanding our understanding of microbial genomic regulation.

Abstract: Functional annotation of microbial genomes is often biased toward
protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs
(ncRNAs) that are critical for regulating bacterial and archaeal physiology,
stress response and metabolism. Identifying ncRNAs directly from genomic
sequence is a paramount challenge in bioinformatics and biology, essential for
understanding the complete regulatory potential of an organism. This paper
presents RNAMunin, a machine learning (ML) model that is capable of finding
ncRNAs using genomic sequence alone. It is also computationally viable for
large sequence datasets such as long read metagenomic assemblies with contigs
totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from
approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary
samples. We know of no other model that can detect ncRNAs based solely on
genomic sequence at this scale. Since RNAMunin only requires genomic sequence
as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do
not need transcriptomics data. We wrote this manuscript in a narrative style in
order to best convey how RNAMunin was developed and how it works in detail.
Unlike almost all current ML models, at approximately 1M parameters, RNAMunin
is very small and very fast.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [334] [Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing](https://arxiv.org/abs/2507.11780)
*Justin Whitehouse,Morgane Austern,Vasilis Syrgkanis*

Main category: econ.EM

TL;DR: The paper develops a statistically efficient estimator based on softmax smoothing to construct confidence intervals for the optimal treatment policy value in causal inference, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for constructing confidence intervals for optimal treatment policy values in causal inference to guide individualized treatment strategies. Existing methods either impose unrealistic assumptions or are computationally demanding.

Method: The authors revisit smoothing non-differentiable functionals using softmax smoothing to develop an estimator that carefully controls bias and remainders, allowing for efficient parameter estimation.

Result: The proposed estimator achieves √n convergence rates without relying on parametric assumptions, unrealistic convergence rates, or strong margin assumptions.

Conclusion: The method provides a statistically efficient and computationally feasible solution for constructing confidence intervals of optimal treatment policy values, broadening applicability in causal inference.

Abstract: Constructing confidence intervals for the value of an optimal treatment
policy is an important problem in causal inference. Insight into the optimal
policy value can guide the development of reward-maximizing, individualized
treatment regimes. However, because the functional that defines the optimal
value is non-differentiable, standard semi-parametric approaches for performing
inference fail to be directly applicable. Existing approaches for handling this
non-differentiability fall roughly into two camps. In one camp are estimators
based on constructing smooth approximations of the optimal value. These
approaches are computationally lightweight, but typically place unrealistic
parametric assumptions on outcome regressions. In another camp are approaches
that directly de-bias the non-smooth objective. These approaches don't place
parametric assumptions on nuisance functions, but they either require the
computation of intractably-many nuisance estimates, assume unrealistic
$L^\infty$ nuisance convergence rates, or make strong margin assumptions that
prohibit non-response to a treatment. In this paper, we revisit the problem of
constructing smooth approximations of non-differentiable functionals. By
carefully controlling first-order bias and second-order remainders, we show
that a softmax smoothing-based estimator can be used to estimate parameters
that are specified as a maximum of scores involving nuisance components. In
particular, this includes the value of the optimal treatment policy as a
special case. Our estimator obtains $\sqrt{n}$ convergence rates, avoids
parametric restrictions/unrealistic margin assumptions, and is often
statistically efficient.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [335] [Multimodal Coordinated Online Behavior: Trade-offs and Strategies](https://arxiv.org/abs/2507.12108)
*Lorenzo Mannocci,Stefano Cresci,Matteo Magnani,Anna Monreale,Maurizio Tesconi*

Main category: cs.SI

TL;DR: This paper explores multimodal approaches to detect coordinated online behavior more effectively, contrasting them with monomodal methods.


<details>
  <summary>Details</summary>
Motivation: Understand the limitations of current detection methods for coordinated behavior in digital platforms and propose better approaches.

Method: Method includes weakly and strongly integrated multimodal models, with a comparison to monomodal approaches.

Result: Findings suggest multimodal approaches capture broader coordination patterns, though not all modalities provide unique insights.

Conclusion: A multimodal perspective improves detection of coordinated behavior, aiding efforts to protect digital platform integrity.

Abstract: Coordinated online behavior, which spans from beneficial collective actions
to harmful manipulation such as disinformation campaigns, has become a key
focus in digital ecosystem analysis. Traditional methods often rely on
monomodal approaches, focusing on single types of interactions like co-retweets
or co-hashtags, or consider multiple modalities independently of each other.
However, these approaches may overlook the complex dynamics inherent in
multimodal coordination. This study compares different ways of operationalizing
the detection of multimodal coordinated behavior. It examines the trade-off
between weakly and strongly integrated multimodal models, highlighting the
balance between capturing broader coordination patterns and identifying tightly
coordinated behavior. By comparing monomodal and multimodal approaches, we
assess the unique contributions of different data modalities and explore how
varying implementations of multimodality impact detection outcomes. Our
findings reveal that not all the modalities provide distinct insights, but that
with a multimodal approach we can get a more comprehensive understanding of
coordination dynamics. This work enhances the ability to detect and analyze
coordinated online behavior, offering new perspectives for safeguarding the
integrity of digital platforms.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [336] [Universal Fourier Neural Operators for Micromechanics](https://arxiv.org/abs/2507.12233)
*Binh Huy Nguyen,Matti Schneider*

Main category: cs.CE

TL;DR: This paper introduces Fourier Neural Operators (FNOs) enhanced by insights from FFT-based micromechanics methods to solve complex cell problems in homogenization efficiently and accurately.


<details>
  <summary>Details</summary>
Motivation: Traditional computational frameworks for solving cell problems in homogenization outperform current deep-learning approaches in speed and generality, but advancement is needed to improve machine-learning methods.

Method: The authors construct an FNO surrogate based on FFT schemes, guaranteeing accurate prediction for cell problems regardless of material symmetry, phases, or interface geometry, under a material-contrast constraint. An explicit FNO is designed without initial training, maintaining similar memory and runtime requirements to classical FFT solvers.

Result: The proposed method handles large-scale micromechanical problems with over 100 million voxels efficiently and demonstrates uniform fidelity in predicting solutions to cell problems.

Conclusion: The connection between FFT-based methods and FNOs highlights the potential of machine-learning approaches for micromechanical problems and offers opportunities for cross-disciplinary advancements.

Abstract: \noindent Solving cell problems in homogenization is hard, and available
deep-learning frameworks fail to match the speed and generality of traditional
computational frameworks. More to the point, it is generally unclear what to
expect of machine-learning approaches, let alone single out which approaches
are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs)
for micromechanics, empowering them by insights from computational
micromechanics methods based on the fast Fourier transform (FFT). We construct
an FNO surrogate mimicking the basic scheme foundational for FFT-based methods
and show that the resulting operator predicts solutions to cell problems with
\emph{arbitrary} stiffness distribution only subject to a material-contrast
constraint up to a desired accuracy. In particular, there are no restrictions
on the material symmetry like isotropy, on the number of phases and on the
geometry of the interfaces between materials. Also, the provided fidelity is
sharp and uniform, providing explicit guarantees leveraging our physical
empowerment of FNOs. To show the desired universal approximation property, we
construct an FNO explicitly that requires no training to begin with. Still, the
obtained neural operator complies with the same memory requirements as the
basic scheme and comes with runtimes proportional to classical FFT solvers. In
particular, large-scale problems with more than 100 million voxels are readily
handled. The goal of this work is to underline the potential of FNOs for
solving micromechanical problems, linking FFT-based methods to FNOs. This
connection is expected to provide a fruitful exchange between both worlds.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [337] [CRAFT: Latency and Cost-Aware Genetic-Based Framework for Node Placement in Edge-Fog Environments](https://arxiv.org/abs/2507.12445)
*Soheil Mahdizadeh,Amir Mahdi Rasouli,Mohammad Pourashory,Sadra Galavani,Mohsen Ansari*

Main category: cs.NI

TL;DR: The paper proposes a genetic algorithm-based method for optimizing edge and fog node placement, achieving reduced latency and system cost.


<details>
  <summary>Details</summary>
Motivation: To address the latency and cost challenges in IoT systems due to limitations in cloud computing's real-time capabilities.

Method: A genetic algorithm-based strategy is used for efficiently placing edge and fog nodes to minimize latency and system cost.

Result: Simulation results show the proposed method reduces latency by up to 2.77% and cost by 31.15%.

Conclusion: Strategically placing edge and fog nodes using the genetic algorithm significantly improves IoT system efficiency by lowering latency and costs.

Abstract: Reducing latency in the Internet of Things (IoT) is a critical concern. While
cloud computing facilitates communication, it falls short of meeting real-time
requirements reliably. Edge and fog computing have emerged as viable solutions
by positioning computing nodes closer to end users, offering lower latency and
increased processing power. An edge-fog framework comprises various components,
including edge and fog nodes, whose strategic placement is crucial as it
directly impacts latency and system cost. This paper presents an effective and
tunable node placement strategy based on a genetic algorithm to address the
optimization problem of deploying edge and fog nodes. The main objective is to
minimize latency and cost through optimal node placement. Simulation results
demonstrate that the proposed framework achieves up to 2.77% latency and 31.15%
cost reduction.

</details>


### [338] [LLM-Based Config Synthesis requires Disambiguation](https://arxiv.org/abs/2507.12443)
*Rajdeep Mondal,Nikolaj Bjorner,Todd Millstein,Alan Tang,George Varghese*

Main category: cs.NI

TL;DR: This paper addresses the ambiguity in user intent during program synthesis by LLMs, focusing on networking configurations such as route-maps and ACLs. It introduces Clarify, a prototype system with a Disambiguator module to handle intent clarifications.


<details>
  <summary>Details</summary>
Motivation: Ambiguity in user intent during program synthesis using LLMs could lead to unintended global behaviors, especially in applications like incremental configuration synthesis of route-maps and ACLs in networking.

Method: The paper introduces a prototype system called Clarify, powered by an LLM augmented by a module named Disambiguator. This module helps users clarify their intent before incremental synthesis and verification of routing policies.

Result: Measurements in a large cloud context demonstrated real ambiguity issues in complex ACLs with significant overlaps. On synthetic workloads, Clarify successfully synthesized routing policies after resolving ambiguities.

Conclusion: Clarify's framework offers a means to address intent ambiguities, ensuring that LLM-based program synthesis leads to correct integration and desired outcomes, particularly in networking applications.

Abstract: Beyond hallucinations, another problem in program synthesis using LLMs is
ambiguity in user intent. We illustrate the ambiguity problem in a networking
context for LLM-based incremental configuration synthesis of route-maps and
ACLs. These structures frequently overlap in header space, making the relative
priority of actions impossible for the LLM to infer without user interaction.
Measurements in a large cloud identify complex ACLs with 100's of overlaps,
showing ambiguity is a real problem. We propose a prototype system, Clarify,
which uses an LLM augmented with a new module called a Disambiguator that helps
elicit user intent. On a small synthetic workload, Clarify incrementally
synthesizes routing policies after disambiguation and then verifies them. Our
treatment of ambiguities is useful more generally when the intent of updates
can be correctly synthesized by LLMs, but their integration is ambiguous and
can lead to different global behaviors.

</details>


### [339] [Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview](https://arxiv.org/abs/2507.11935)
*Jikang Deng,Fizza Hassan,Hui Zhou,Saad Al-Ahmadi,Mohamed-Slim Alouini,Daniel B. Da Costa*

Main category: cs.NI

TL;DR: The paper proposes integrating ORAN principles into NTNs to address scalability and intelligence challenges, providing a detailed framework and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: 6G requirements for reliable, flexible wireless networks and the challenges posed by non-terrestrial network (NTN) operational complexities necessitate innovative solutions.

Method: Background analysis, identification of DevOps challenges in NTNs, exploration of ORAN features, and proposal of an ORAN-based NTN framework with architectural specifics.

Result: Detailed discussion of ORAN-based features like fronthaul split, distributed learning via RICs, scalable architecture, and multi-domain management.

Conclusion: ORAN principles offer scalable and intelligent solutions for NTNs, enabling efficient development and operation, with future research suggested to integrate enabling technologies and use cases.

Abstract: As the path toward 6G networks is being charted, the emerging applications
have motivated evolutions of network architectures to realize the efficient,
reliable, and flexible wireless networks. Among the potential architectures,
the non-terrestrial network (NTN) and open radio access network (ORAN) have
received increasing interest from both academia and industry. Although the
deployment of NTNs ensures coverage, enhances spectral efficiency, and improves
the resilience of wireless networks. The high altitude and mobility of NTN
present new challenges in the development and operations (DevOps) lifecycle,
hindering intelligent and scalable network management due to the lack of native
artificial intelligence (AI) capability. With the advantages of ORAN in
disaggregation, openness, virtualization, and intelligence, several works
propose integrating ORAN principles into the NTN, focusing mainly on ORAN
deployment options based on transparent and regenerative systems. However, a
holistic view of how to effectively combine ORAN and NTN throughout the DevOps
lifecycle is still missing, especially regarding how intelligent ORAN addresses
the scalability challenges in NTN. Motivated by this, in this paper, we first
provide the background knowledge about ORAN and NTN, outline the
state-of-the-art research on ORAN for NTNs, and present the DevOps challenges
that motivate the adoption of ORAN solutions. We then propose the ORAN-based
NTN framework, discussing its features and architectures in detail. These
include the discussion about flexible fronthaul split, RAN intelligent
controllers (RICs) enhancement for distributed learning, scalable deployment
architecture, and multi-domain service management. Finally, the future research
directions, including combinations of the ORAN-based NTN framework and other
enabling technologies and schemes, as well as the candidate use cases, are
highlighted.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [340] [Counting Answer Sets of Disjunctive Answer Set Programs](https://arxiv.org/abs/2507.11655)
*Mohimenul Kabir,Supratik Chakraborty,Kuldeep S Meel*

Main category: cs.LO

TL;DR: The paper introduces SharpASP-SR, a novel framework to efficiently count answer sets for disjunctive logic programs using subtractive reduction and leveraging advanced projection counting technologies.


<details>
  <summary>Details</summary>
Motivation: Counting answer sets for disjunctive logic programs remains challenging despite progress for normal logic programs. Applications in areas like probabilistic reasoning and network analysis demand more efficient methods.

Method: The method employs subtractive reduction to projected model counting with a focus on maintaining polynomial-sized representations, alongside a hybrid approach combining enumeration techniques for broader efficiency.

Result: Experimental results show significant improvement in counting efficiency compared to existing methods, especially for programs with large answer set counts.

Conclusion: SharpASP-SR advances answer set counting for disjunctive logic programs by bridging the gap between theoretical characterization and practical efficiency, achieving state-of-the-art performance.

Abstract: Answer Set Programming (ASP) provides a powerful declarative paradigm for
knowledge representation and reasoning. Recently, counting answer sets has
emerged as an important computational problem with applications in
probabilistic reasoning, network reliability analysis, and other domains. This
has motivated significant research into designing efficient ASP counters. While
substantial progress has been made for normal logic programs, the development
of practical counters for disjunctive logic programs remains challenging.
  We present SharpASP-SR, a novel framework for counting answer sets of
disjunctive logic programs based on subtractive reduction to projected
propositional model counting. Our approach introduces an alternative
characterization of answer sets that enables efficient reduction while ensuring
that intermediate representations remain of polynomial size. This allows
SharpASP-SR to leverage recent advances in projected model counting technology.
Through extensive experimental evaluation on diverse benchmarks, we demonstrate
that SharpASP-SR significantly outperforms existing counters on instances with
large answer set counts. Building on these results, we develop a hybrid
counting approach that combines enumeration techniques with SharpASP-SR to
achieve state-of-the-art performance across the full spectrum of disjunctive
programs.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [341] [Recent results on searches with boosted Higgs bosons at CMS](https://arxiv.org/abs/2507.11977)
*Farouk Mokhtar*

Main category: hep-ex

TL;DR: The paper discusses recent advancements by the CMS experiment in boosted Higgs boson searches at the LHC, utilizing innovative techniques for improved sensitivity.


<details>
  <summary>Details</summary>
Motivation: Investigating boosted Higgs bosons offers a way to explore Higgs boson couplings at high energy levels, shedding light on potential new physics beyond the Standard Model.

Method: The CMS experiment uses advanced reconstruction and tagging techniques to enhance the detection and analysis of boosted Higgs bosons.

Result: Recent developments in these innovative techniques have led to improved sensitivity in the detection of boosted Higgs bosons within the challenging high-energy regime.

Conclusion: The work underscores the importance of advanced methods to study boosted Higgs bosons, paving the way for deeper insights into high-energy physics and potential discoveries beyond the Standard Model.

Abstract: The study of boosted Higgs bosons at the LHC provides a unique window to
probe Higgs boson couplings at high energy scales and search for signs of
physics beyond the standard model. In these proceedings, we present recent
results on boosted Higgs boson searches at the CMS experiment, highlighting
innovative reconstruction and tagging techniques that enhance sensitivity in
this challenging regime.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [342] [RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection](https://arxiv.org/abs/2507.12175)
*Sungkyun Chang,Simon Dixon,Emmanouil Benetos*

Main category: cs.SD

TL;DR: This paper introduces RUMAA, a unified framework for analyzing musical performances using transformers, which integrates score-to-performance alignment, transcription, and mistake detection tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods handle score-to-performance alignment, transcription, and mistake detection tasks separately and face challenges in handling scores with repeats and interdependencies.

Method: The paper proposes a unified framework called RUMAA using pre-trained score and audio encoders and a tri-stream decoder that interconnects tasks via proxy tasks.

Result: RUMAA achieves state-of-the-art results in score alignment, particularly for scores with repeats, and provides strong results in transcription and mistake detection.

Conclusion: RUMAA proves effective in handling overlapping tasks, offering a robust solution for music performance analysis.

Abstract: This study introduces RUMAA, a transformer-based framework for music
performance analysis that unifies score-to-performance alignment,
score-informed transcription, and mistake detection in a near end-to-end
manner. Unlike prior methods addressing these tasks separately, RUMAA
integrates them using pre-trained score and audio encoders and a novel
tri-stream decoder capturing task interdependencies through proxy tasks. It
aligns human-readable MusicXML scores with repeat symbols to full-length
performance audio, overcoming traditional MIDI-based methods that rely on
manually unfolded score-MIDI data with pre-specified repeat structures. RUMAA
matches state-of-the-art alignment methods on non-repeated scores and
outperforms them on scores with repeats in a public piano music dataset, while
also delivering promising transcription and mistake detection results.

</details>


### [343] [Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations](https://arxiv.org/abs/2507.12197)
*Yichen Han,Xiaoyang Hao,Keming Chen,Weibo Xiong,Jun He,Ruonan Zhang,Junjie Cao,Yue Liu,Bowen Li,Dongrui Zhang,Hui Xia,Huilei Fu,Kai Jia,Kaixuan Guo,Mingli Jin,Qingyun Meng,Ruidong Ma,Ruiqian Fang,Shaotong Guo,Xuhui Li,Yang Xiang,Ying Zhang,Yulong Liu,Yunfeng Li,Yuyi Zhang,Yuze Zhou,Zhen Wang,Zhaowen Chen*

Main category: cs.SD

TL;DR: QTTS is a novel TTS framework using QDAC, an advanced audio codec, to improve synthesis quality, preserve expressive content, and enable scalable, near-lossless audio compression.


<details>
  <summary>Details</summary>
Motivation: Existing TTS approaches using single-codebook representations suffer from significant information loss, making it difficult to capture fine-grained details like prosodic nuances or speaker-specific timbres, especially in complex synthesis tasks.

Method: QTTS introduces QDAC, a new audio codec that leverages an ASR-based autoregressive network with GANs for feature disentanglement and scalable compression. It also employs a Hierarchical Parallel architecture for quality synthesis and a Delay Multihead approach for faster inference.

Result: QTTS showcases higher synthesis quality and better retention of expressive content compared to baseline methods, demonstrating its advantages in capturing fine audio details.

Conclusion: The paper highlights the promise of multi-codebook modeling and enhanced compression techniques for achieving high-fidelity and general-purpose speech and audio generation.

Abstract: Text-to-speech (TTS) synthesis has seen renewed progress under the discrete
modeling paradigm. Existing autoregressive approaches often rely on
single-codebook representations, which suffer from significant information
loss. Even with post-hoc refinement techniques such as flow matching, these
methods fail to recover fine-grained details (e.g., prosodic nuances,
speaker-specific timbres), especially in challenging scenarios like singing
voice or music synthesis. We propose QTTS, a novel TTS framework built upon our
new audio codec, QDAC. The core innovation of QDAC lies in its end-to-end
training of an ASR-based auto-regressive network with a GAN, which achieves
superior semantic feature disentanglement for scalable, near-lossless
compression. QTTS models these discrete codes using two innovative strategies:
the Hierarchical Parallel architecture, which uses a dual-AR structure to model
inter-codebook dependencies for higher-quality synthesis, and the Delay
Multihead approach, which employs parallelized prediction with a fixed delay to
accelerate inference speed. Our experiments demonstrate that the proposed
framework achieves higher synthesis quality and better preserves expressive
content compared to baseline. This suggests that scaling up compression via
multi-codebook modeling is a promising direction for high-fidelity,
general-purpose speech and audio generation.

</details>


### [344] [Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification](https://arxiv.org/abs/2507.12042)
*Kazuki Shimada,Archontis Politis,Iran R. Roman,Parthasaarathy Sudarsanam,David Diaz-Guerra,Ruchi Pandey,Kengo Uchida,Yuichiro Koyama,Naoya Takahashi,Takashi Shibuya,Shusuke Takahashi,Tuomas Virtanen,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: This paper introduces Task 3 of the DCASE2025 Challenge focusing on Stereo Sound Event Localization and Detection (SELD) with stereo audio data, instead of prior 360° audio formats. It evaluates models on aspects like DOA estimation, classic SELD tasks, and new audiovisual sub-tasks.


<details>
  <summary>Details</summary>
Motivation: To shift SELD research focus from specialized 360° audio and audiovisual scene analysis to more common audio/media scenarios with stereo audio and limited field-of-view.

Method: The challenge uses the DCASE2025 Task3 Stereo SELD Dataset, offering stereo audio and perspective video content. A baseline system was developed to process these inputs and tackle tasks like event classification, DOA estimation, and onscreen/offscreen classification in the audiovisual context.

Result: The experimental baseline system demonstrated reasonable performance handling stereo audio for SELD tasks and the new audiovisual sub-task.

Conclusion: The challenge broadens SELD research by addressing angular and onscreen/offscreen ambiguities in stereo setups, fostering innovations applicable to everyday audio and media scenarios.

Abstract: This paper presents the objective, dataset, baseline, and metrics of Task 3
of the DCASE2025 Challenge on sound event localization and detection (SELD). In
previous editions, the challenge used four-channel audio formats of first-order
Ambisonics (FOA) and microphone array. In contrast, this year's challenge
investigates SELD with stereo audio data (termed stereo SELD). This change
shifts the focus from more specialized 360{\deg} audio and audiovisual scene
analysis to more commonplace audio and media scenarios with limited
field-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,
the task focuses on direction-of-arrival (DOA) estimation in the azimuth plane
(left-right axis) along with distance estimation. The challenge remains divided
into two tracks: audio-only and audiovisual, with the audiovisual track
introducing a new sub-task of onscreen/offscreen event classification
necessitated by the limited FOV. This challenge introduces the DCASE2025 Task3
Stereo SELD Dataset, whose stereo audio and perspective video clips are sampled
and converted from the STARSS23 recordings. The baseline system is designed to
process stereo audio and corresponding video frames as inputs. In addition to
the typical SELD event classification and localization, it integrates
onscreen/offscreen classification for the audiovisual track. The evaluation
metrics have been modified to introduce an onscreen/offscreen accuracy metric,
which assesses the models' ability to identify which sound sources are
onscreen. In the experimental evaluation, the baseline system performs
reasonably well with the stereo audio data.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [345] [Galaxy image simplification using Generative AI](https://arxiv.org/abs/2507.11692)
*Sai Teja Erukude,Lior Shamir*

Main category: astro-ph.GA

TL;DR: The paper introduces a generative AI method for analyzing galaxy images by converting them into simplified 'skeletonized' forms for accurate and automated shape analysis.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate and automated methods to analyze billions of galaxy images in modern digital sky surveys.

Method: The authors propose a generative AI approach that simplifies galaxy images and automatically converts them into skeletonized forms, allowing for unrestricted shape measurements.

Result: The method was applied to 125,000 galaxy images from the DESI Legacy Survey, and a catalog of these simplified images is made publicly available.

Conclusion: This approach provides an effective tool for the automated analysis of galaxy shapes without reliance on pre-defined classification sets, offering publicly accessible data and code.

Abstract: Modern digital sky surveys have been acquiring images of billions of
galaxies. While these images often provide sufficient details to analyze the
shape of the galaxies, accurate analysis of such high volumes of images
requires effective automation. Current solutions often rely on machine learning
annotation of the galaxy images based on a set of pre-defined classes. Here we
introduce a new approach to galaxy image analysis that is based on generative
AI. The method simplifies the galaxy images and automatically converts them
into a ``skeletonized" form. The simplified images allow accurate measurements
of the galaxy shapes and analysis that is not limited to a certain pre-defined
set of classes. We demonstrate the method by applying it to galaxy images
acquired by the DESI Legacy Survey. The code and data are publicly available.
The method was applied to 125,000 DESI Legacy Survey images, and the catalog of
the simplified images is publicly available.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [346] [MOSPA: Human Motion Generation Driven by Spatial Audio](https://arxiv.org/abs/2507.11949)
*Shuyang Xu,Zhiyang Dou,Mingyi Shi,Liang Pan,Leo Ho,Jingbo Wang,Yuan Liu,Cheng Lin,Yuexin Ma,Wenping Wang,Taku Komura*

Main category: cs.GR

TL;DR: This study introduces a new dataset and model focusing on human motion generation driven by spatial audio inputs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of research connecting spatial audio signals to human motion synthesis.

Method: A diffusion-based generative framework called MOSPA is developed, leveraging spatial audio signals and effective fusion mechanisms.

Result: MOSPA achieves state-of-the-art performance in generating human motions corresponding to spatial audio inputs.

Conclusion: The proposed dataset and model are valuable resources for advancing spatial audio-driven human motion modeling, and both will be open-sourced.

Abstract: Enabling virtual humans to dynamically and realistically respond to diverse
auditory stimuli remains a key challenge in character animation, demanding the
integration of perceptual modeling and motion synthesis. Despite its
significance, this task remains largely unexplored. Most previous works have
primarily focused on mapping modalities like speech, audio, and music to
generate human motion. As of yet, these models typically overlook the impact of
spatial features encoded in spatial audio signals on human motion. To bridge
this gap and enable high-quality modeling of human movements in response to
spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human
Motion (SAM) dataset, which contains diverse and high-quality spatial audio and
motion data. For benchmarking, we develop a simple yet effective
diffusion-based generative framework for human MOtion generation driven by
SPatial Audio, termed MOSPA, which faithfully captures the relationship between
body motion and spatial audio through an effective fusion mechanism. Once
trained, MOSPA could generate diverse realistic human motions conditioned on
varying spatial audio inputs. We perform a thorough investigation of the
proposed dataset and conduct extensive experiments for benchmarking, where our
method achieves state-of-the-art performance on this task. Our model and
dataset will be open-sourced upon acceptance. Please refer to our supplementary
video for more details.

</details>


### [347] [HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing](https://arxiv.org/abs/2507.11971)
*Tielong Wang,Yuxuan Xiong,Jinfan Liu,Zhifan Zhang,Ye Chen,Yue Shi,Bingbing Ni*

Main category: cs.GR

TL;DR: This paper introduces a novel 3D Hierarchical Proxy Node representation to address limitations in existing 3D representations, offering efficient 3D modeling with high fidelity and editability.


<details>
  <summary>Details</summary>
Motivation: Existing 3D representations like meshes and NeRFs face challenges such as task-specificity, editing complexity, structural ambiguities, and difficulty balancing fidelity and data complexity.

Method: The method utilizes a sparse set of hierarchically organized proxy nodes storing local shape and texture information encoded by small neural networks for 3D representation, querying, and manipulation.

Result: Experiments show expressive efficiency, high-fidelity rendering, and improved editability in 3D reconstruction and editing tasks compared to conventional methods.

Conclusion: The proposed framework significantly enhances universal applicability and quality-control scalability, addressing issues of existing representations and enabling efficient, direct manipulation.

Abstract: Current 3D representations like meshes, voxels, point clouds, and NeRF-based
neural implicit fields exhibit significant limitations: they are often
task-specific, lacking universal applicability across reconstruction,
generation, editing, and driving. While meshes offer high precision, their
dense vertex data complicates editing; NeRFs deliver excellent rendering but
suffer from structural ambiguity, hindering animation and manipulation; all
representations inherently struggle with the trade-off between data complexity
and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical
Proxy Node representation. Its core innovation lies in representing an object's
shape and texture via a sparse set of hierarchically organized
(tree-structured) proxy nodes distributed on its surface and interior. Each
node stores local shape and texture information (implicitly encoded by a small
MLP) within its neighborhood. Querying any 3D coordinate's properties involves
efficient neural interpolation and lightweight decoding from relevant nearby
and parent nodes. This framework yields a highly compact representation where
nodes align with local semantics, enabling direct drag-and-edit manipulation,
and offers scalable quality-complexity control. Extensive experiments across 3D
reconstruction and editing demonstrate our method's expressive efficiency,
high-fidelity rendering quality, and superior editability.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [348] [Interactive Hybrid Rice Breeding with Parametric Dual Projection](https://arxiv.org/abs/2507.11848)
*Changjian Chen,Pengcheng Wang,Fei Lyu,Zhuo Tang,Li Yang,Long Wang,Yong Cai,Feng Yu,Kenli Li*

Main category: cs.HC

TL;DR: The paper introduces a visual analysis method to streamline hybrid rice breeding by combining genomic selection and breeder expertise. A novel dual projection technique enables easier selection of regulatory genes and hybrids.


<details>
  <summary>Details</summary>
Motivation: Hybrid rice breeding is arduous, requiring field cultivation to identify hybrids with desirable traits. Genomic selection models are promising but cumbersome due to limited accuracy and reliance on breeder experience.

Method: The authors developed a parametric dual projection method with theoretical foundations and visual analysis tools for identifying regulatory genes and selecting desired hybrids.

Result: Quantitative evaluations, case study findings, and breeder feedback confirmed the effectiveness of the dual projection method, identified genes, and hybrids.

Conclusion: The proposed visual analysis method simplifies hybrid rice breeding by integrating genomic predictions and interactive tools, aiding breeders in identifying optimal traits and hybrids efficiently.

Abstract: Hybrid rice breeding crossbreeds different rice lines and cultivates the
resulting hybrids in fields to select those with desirable agronomic traits,
such as higher yields. Recently, genomic selection has emerged as an efficient
way for hybrid rice breeding. It predicts the traits of hybrids based on their
genes, which helps exclude many undesired hybrids, largely reducing the
workload of field cultivation. However, due to the limited accuracy of genomic
prediction models, breeders still need to combine their experience with the
models to identify regulatory genes that control traits and select hybrids,
which remains a time-consuming process. To ease this process, in this paper, we
proposed a visual analysis method to facilitate interactive hybrid rice
breeding. Regulatory gene identification and hybrid selection naturally
ensemble a dual-analysis task. Therefore, we developed a parametric dual
projection method with theoretical guarantees to facilitate interactive dual
analysis. Based on this dual projection method, we further developed a gene
visualization and a hybrid visualization to verify the identified regulatory
genes and hybrids. The effectiveness of our method is demonstrated through the
quantitative evaluation of the parametric dual projection method, identified
regulatory genes and desired hybrids in the case study, and positive feedback
from breeders.

</details>


### [349] [Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness](https://arxiv.org/abs/2507.12212)
*Garyoung Kim,Huisung Kwon,Seoju Yun,Yu-Won Youn*

Main category: cs.HC

TL;DR: This paper examines how generative AI perpetuates cultural biases, especially in representations of 'ugliness.'


<details>
  <summary>Details</summary>
Motivation: To critically assess how generative AI embeds and reproduces cultural concepts like ugliness and social biases.

Method: Investigated 4 generative AI models with 624 images created based on 3 prompts and analyzed demographic/socioeconomic attributes coded in the images.

Result: Generative AI tends to associate ugliness with old white males and physical markers, revealing entrenched biases and paradoxical representation challenges.

Conclusion: Current generative AI systems perpetuate cultural biases despite efforts toward inclusive representations, highlighting the need for ethical AI development methodologies.

Abstract: Generative AI does not only replicate human creativity but also reproduces
deep-seated cultural biases, making it crucial to critically examine how
concepts like ugliness are understood and expressed by these tools. This study
investigates how four different generative AI models understand and express
ugliness through text and image and explores the biases embedded within these
representations. We extracted 13 adjectives associated with ugliness through
iterative prompting of a large language model and generated 624 images across
four AI models and three prompts. Demographic and socioeconomic attributes
within the images were independently coded and thematically analyzed. Our
findings show that AI models disproportionately associate ugliness with old
white male figures, reflecting entrenched social biases as well as paradoxical
biases, where efforts to avoid stereotypical depictions of marginalized groups
inadvertently result in the disproportionate projection of negative attributes
onto majority groups. Qualitative analysis further reveals that, despite
supposed attempts to frame ugliness within social contexts, conventional
physical markers such as asymmetry and aging persist as central visual motifs.
These findings demonstrate that despite attempts to create more equal
representations, generative AI continues to perpetuate inherited and
paradoxical biases, underscoring the critical work being done to create ethical
AI training paradigms and advance methodologies for more inclusive AI
development.

</details>


### [350] [AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding](https://arxiv.org/abs/2507.11911)
*Xiaoqing Chen,Siyang Li,Dongrui Wu*

Main category: cs.HC

TL;DR: The study introduces a novel Alignment-Based Frame-Patch Modeling (AFPM) framework to address channel inconsistencies and signal distribution challenges in EEG-based brain-computer interfaces (BCIs).


<details>
  <summary>Details</summary>
Motivation: Cross-dataset EEG decoding for BCIs faces difficulties due to inconsistencies in channel layout, signal variability, and limited integration of neurophysiological priors.

Method: AFPM employs Spatial Alignment for selecting task-relevant channels and aligning EEG distributions, and Frame-Patch Encoding to model unified spatiotemporal patches for decoding across datasets.

Result: The calibration-free AFPM outperforms 17 state-of-the-art methods, achieving up to 4.40% improvement in motor imagery tasks and 3.58% in event-related potential tasks.

Conclusion: AFPM significantly improves cross-dataset EEG decoding without calibration, enhancing the practical usability of BCIs in real-world scenarios.

Abstract: Electroencephalogram (EEG) decoding models for brain-computer interfaces
(BCIs) struggle with cross-dataset learning and generalization due to channel
layout inconsistencies, non-stationary signal distributions, and limited
neurophysiological prior integration. To address these issues, we propose a
plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has
two main components: 1) Spatial Alignment, which selects task-relevant channels
based on brain-region priors, aligns EEG distributions across domains, and
remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,
which models multi-dataset signals into unified spatiotemporal patches for EEG
decoding. Compared to 17 state-of-the-art approaches that need dataset-specific
tuning, the proposed calibration-free AFPM achieves performance gains of up to
4.40% on motor imagery and 3.58% on event-related potential tasks. To our
knowledge, this is the first calibration-free cross-dataset EEG decoding
framework, substantially enhancing the practicalness of BCIs in real-world
applications.

</details>


### [351] [d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement](https://arxiv.org/abs/2507.11960)
*Hyein Hong,Sangbong Yoo,SeokHwan Choi,Jisue Kim,Seongbum Seo,Haneol Cho,Chansoo Kim,Yun Jang*

Main category: cs.HC

TL;DR: The paper introduces d-DQIVAR, a visual analytics system focused on improving data quality (DQ) to enhance machine learning (ML) model performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional batch data preprocessing approaches for DQ, which often fail to improve ML model performance and distort data characteristics.

Method: The authors developed a visual analytics system combining data-driven strategies (e.g., imputation, outlier detection) and process-driven evaluations (considering DQ dimensions and applying Kolmogorov-Smirnov tests) to optimize practical workflows.

Result: The system successfully enables users to improve machine learning outcomes while effectively incorporating expert and domain knowledge.

Conclusion: The d-DQIVAR system bridges the gap between data preprocessing and genuine DQ improvement, demonstrating practical utility and enhancing ML model performance through a visual analytics framework.

Abstract: Approaches to enhancing data quality (DQ) are classified into two main
categories: data- and process-driven. However, prior research has predominantly
utilized batch data preprocessing within the data-driven framework, which often
proves insufficient for optimizing machine learning (ML) model performance and
frequently leads to distortions in data characteristics. Existing studies have
primarily focused on data preprocessing rather than genuine data quality
improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual
analytics system designed to facilitate DQI strategies aimed at improving ML
model performance. Our system integrates visual analytics techniques that
leverage both data-driven and process-driven approaches. Data-driven techniques
tackle DQ issues such as imputation, outlier detection, deletion, format
standardization, removal of duplicate records, and feature selection.
Process-driven strategies encompass evaluating DQ and DQI procedures by
considering DQ dimensions and ML model performance and applying the
Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness
expert and domain knowledge effectively within a practical workflow through
case studies, evaluations, and user studies.

</details>


### [352] [Dataset-Adaptive Dimensionality Reduction](https://arxiv.org/abs/2507.11984)
*Hyeon Jeon,Jeongin Park,Soohyun Lee,Dae Hyun Kim,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: This paper proposes structural complexity metrics to adaptively optimize dimensionality reduction (DR) techniques, minimizing trial-and-error and computational overhead.


<details>
  <summary>Details</summary>
Motivation: Trial-and-error processes in DR optimization typically cause unnecessary computational overhead and inefficiencies, motivating the need for a dataset-driven adaptive approach.

Method: The authors introduce structural complexity metrics to quantify dataset complexity and predict the optimal dimensional space for representation.

Result: The metrics approximate ground-truth dataset complexity and enable efficient, dataset-adaptive DR optimization, maintaining projection accuracy.

Conclusion: The proposed structural complexity metrics streamline DR optimization by reducing redundant trials and computational effort, ensuring accuracy and efficiency.

Abstract: Selecting the appropriate dimensionality reduction (DR) technique and
determining its optimal hyperparameter settings that maximize the accuracy of
the output projections typically involves extensive trial and error, often
resulting in unnecessary computational overhead. To address this challenge, we
propose a dataset-adaptive approach to DR optimization guided by structural
complexity metrics. These metrics quantify the intrinsic complexity of a
dataset, predicting whether higher-dimensional spaces are necessary to
represent it accurately. Since complex datasets are often inaccurately
represented in two-dimensional projections, leveraging these metrics enables us
to predict the maximum achievable accuracy of DR techniques for a given
dataset, eliminating redundant trials in optimizing DR. We introduce the design
and theoretical foundations of these structural complexity metrics. We
quantitatively verify that our metrics effectively approximate the ground truth
complexity of datasets and confirm their suitability for guiding
dataset-adaptive DR workflow. Finally, we empirically show that our
dataset-adaptive workflow significantly enhances the efficiency of DR
optimization without compromising accuracy.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [353] [JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs](https://arxiv.org/abs/2507.11636)
*Junyi Fan,Donald Williamson*

Main category: eess.AS

TL;DR: The paper presents JSQA, a two-stage method enhancing speech quality assessment (SQA) by leveraging perceptual features during pretraining via just noticeable difference (JND) audio pairs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in learning mappings for mean opinion score (MOS) due to high variance and insufficient consideration of perceptual factors in existing methods.

Method: JSQA utilizes a two-stage framework: (1) Contrastive pretraining of an audio encoder using perceptually-guided JND pairs created with various signal-to-noise ratios (SNRs), and (2) Fine-tuning on MOS prediction using the NISQA dataset.

Result: Experimental results show that using perceptually-inspired pretrained encoders significantly improves SQA performance across various metrics compared to training from scratch.

Conclusion: Incorporating perceptual factors like JND-based pretraining is effective in improving MOS prediction tasks, validating its importance for SQA.

Abstract: Speech quality assessment (SQA) is often used to learn a mapping from a
high-dimensional input space to a scalar that represents the mean opinion score
(MOS) of the perceptual speech quality. Learning such a mapping is challenging
for many reasons, but largely because MOS exhibits high levels of inherent
variance due to perceptual and experimental-design differences. Many solutions
have been proposed, but many approaches do not properly incorporate perceptual
factors into their learning algorithms (beyond the MOS label), which could lead
to unsatisfactory results. To this end, we propose JSQA, a two-stage framework
that pretrains an audio encoder using perceptually-guided contrastive learning
on just noticeable difference (JND) pairs, followed by fine-tuning for MOS
prediction. We first generate pairs of audio data within JND levels, which are
then used to pretrain an encoder to leverage perceptual quality similarity
information and map it into an embedding space. The JND pairs come from clean
LibriSpeech utterances that are mixed with background noise from CHiME-3, at
different signal-to-noise ratios (SNRs). The encoder is later fine-tuned with
audio samples from the NISQA dataset for MOS prediction. Experimental results
suggest that perceptually-inspired contrastive pretraining significantly
improves the model performance evaluated by various metrics when compared
against the same network trained from scratch without pretraining. These
findings suggest that incorporating perceptual factors into pretraining greatly
contributes to the improvement in performance for SQA.

</details>
