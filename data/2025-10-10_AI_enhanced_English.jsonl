{"id": "2510.07329", "pdf": "https://arxiv.org/pdf/2510.07329", "abs": "https://arxiv.org/abs/2510.07329", "authors": ["Pedro Pestana", "M. F\u00e1tima Brilhante"], "title": "A Digital Pheromone-Based Approach for In/Out-of-Control Classification", "categories": ["cs.NE", "cs.SY", "eess.SY", "62P30"], "comment": "19 pages, 10 figures", "summary": "In complex production lines, it is essential to have strict, fast-acting\nrules to determine whether the system is In Control (InC) or Out of Control\n(OutC). This study explores a bio-inspired method that digitally mimics ant\ncolony behavior to classify InC/OutC states and forecast imminent transitions\nrequiring maintenance. A case study on industrial potato chip frying provides\nthe application context. During each two-minute frying cycle, sequences of\neight temperature readings are collected. Each sequence is treated as a digital\nant depositing virtual pheromones, generating a Base Score. New sequences,\nrepresenting new ants, can either reinforce or weaken this score, leading to a\nModified Base Score that reflects the system's evolving condition. Signals such\nas extreme temperatures, large variations within a sequence, or the detection\nof change-points contribute to a Threat Score, which is added to the Modified\nBase Score. Since pheromones naturally decay over time unless reinforced, an\nEnvironmental Score is incorporated to reflect recent system dynamics,\nimitating real ant behavior. This score is calculated from the Modified Base\nScores collected over the past hour. The resulting Total Score - the sum of the\nModified Base Score, Threat Score, and Environmental Score - is used as the\nmain indicator for real-time system classification and forecasting of\ntransitions from InC to OutC. This ant colony optimization-inspired approach\nprovides an adaptive and interpretable framework for process monitoring and\npredictive maintenance in industrial environments.", "AI": {"tldr": "The paper proposes a bio-inspired ant colony optimization approach to monitor and predict Out-of-Control (OutC) states in industrial production, using scores derived from digital ant behavior.", "motivation": "The paper aims to address the necessity of real-time and reliable classification of In Control (InC) and Out of Control (OutC) states and the prediction of transitions in complex production lines.", "method": "The method simulates ant colony behavior, where sequences of temperature readings act as digital ants depositing virtual pheromones, generating scores like Base Score, Threat Score, Environmental Score, combined to monitor and classify system states dynamically.", "result": "The study applies the method to industrial potato chip frying systems, showing the approach effectively monitors system states and predicts transitions from InC to OutC using scores derived from ant behavior.", "conclusion": "The ant colony optimization-inspired framework offers an adaptive and explainable solution for process monitoring and predictive maintenance in industrial environments."}}
{"id": "2510.07341", "pdf": "https://arxiv.org/pdf/2510.07341", "abs": "https://arxiv.org/abs/2510.07341", "authors": ["Eric Jahns", "Davi Moreno", "Michel A. Kinsy"], "title": "Learning Neuron Dynamics within Deep Spiking Neural Networks", "categories": ["cs.NE", "I.2"], "comment": null, "summary": "Spiking Neural Networks (SNNs) offer a promising energy-efficient alternative\nto Artificial Neural Networks (ANNs) by utilizing sparse and asynchronous\nprocessing through discrete spike-based computation. However, the performance\nof deep SNNs remains limited by their reliance on simple neuron models, such as\nthe Leaky Integrate-and-Fire (LIF) model, which cannot capture rich temporal\ndynamics. While more expressive neuron models exist, they require careful\nmanual tuning of hyperparameters and are difficult to scale effectively. This\ndifficulty is evident in the lack of successful implementations of complex\nneuron models in high-performance deep SNNs. In this work, we address this\nlimitation by introducing Learnable Neuron Models (LNMs). LNMs are a general,\nparametric formulation for non-linear integrate-and-fire dynamics that learn\nneuron dynamics during training. By learning neuron dynamics directly from\ndata, LNMs enhance the performance of deep SNNs. We instantiate LNMs using\nlow-degree polynomial parameterizations, enabling efficient and stable\ntraining. We demonstrate state-of-the-art performance in a variety of datasets,\nincluding CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS. LNMs offer a\npromising path toward more scalable and high-performing spiking architectures.", "AI": {"tldr": "This paper introduces Learnable Neuron Models (LNMs), a parametric approach to enhance the performance of Spiking Neural Networks by learning neuron dynamics, achieving state-of-the-art results on various datasets.", "motivation": "Current Spiking Neural Networks struggle with limited performance due to simplistic neuron models like the Leaky Integrate-and-Fire model, which fail to capture complex temporal dynamics.", "method": "The paper proposes Learnable Neuron Models (LNMs), a parametric system employing low-degree polynomial parameterization to automatically learn neuron dynamics during the training of deep SNNs.", "result": "State-of-the-art performance was demonstrated across datasets such as CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS using the LNMs.", "conclusion": "LNMs provide an effective and scalable method for improving spiking architectures, overcoming limitations of traditional neuron models like manual hyperparameter tuning and complexity issues."}}
{"id": "2510.07440", "pdf": "https://arxiv.org/pdf/2510.07440", "abs": "https://arxiv.org/abs/2510.07440", "authors": ["Dominik Woiwode", "Jakob Marten", "Bodo Rosenhahn"], "title": "A Rotation-Invariant Embedded Platform for (Neural) Cellular Automata", "categories": ["cs.NE", "cs.RO"], "comment": "Accepted for ALIFE 2025", "summary": "This paper presents a rotation-invariant embedded platform for simulating\n(neural) cellular automata (NCA) in modular robotic systems. Inspired by\nprevious work on physical NCA, we introduce key innovations that overcome\nlimitations in prior hardware designs. Our platform features a symmetric,\nmodular structure, enabling seamless connections between cells regardless of\norientation. Additionally, each cell is battery-powered, allowing it to operate\nindependently and retain its state even when disconnected from the collective.\nTo demonstrate the platform's applicability, we present a novel\nrotation-invariant NCA model for isotropic shape classification. The proposed\nsystem provides a robust foundation for exploring the physical realization of\nNCA, with potential applications in distributed robotic systems and\nself-organizing structures. Our implementation, including hardware, software\ncode, a simulator, and a video, is openly shared at:\nhttps://github.com/dwoiwode/embedded_nca", "AI": {"tldr": "The paper introduces a modular, rotation-invariant platform for simulating neural cellular automata (NCA) in robotics, overcoming limitations of prior designs.", "motivation": "To address limitations in prior hardware designs of physical NCAs and develop a robust modular system capable of rotation-invariance and independent operation.", "method": "A symmetric, modular structure with battery-powered cells was implemented to ensure independent operation, seamless connectivity, and rotation-invariant functionality.", "result": "A novel rotation-invariant NCA model for isotropic shape classification was demonstrated using the platform, with implementations openly shared.", "conclusion": "This platform provides a strong foundation for physical realization of NCAs, paving the way for distributed robotics and self-organizing systems."}}
{"id": "2510.08230", "pdf": "https://arxiv.org/pdf/2510.08230", "abs": "https://arxiv.org/abs/2510.08230", "authors": ["Keshvi Tuteja", "Gregor Olenik", "Roman Mishchuk", "Yu-Hsiang Tsai", "Markus G\u00f6tz", "Achim Streit", "Hartwig Anzt", "Charlotte Debus"], "title": "pyGinkgo: A Sparse Linear Algebra Operator Framework for Python", "categories": ["cs.MS", "cs.DC", "cs.PF", "cs.SE"], "comment": "Accepted for publication at the 54th International Conference on\n  Parallel Processing (ICPP'25)", "summary": "Sparse linear algebra is a cornerstone of many scientific computing and\nmachine learning applications. Python has become a popular choice for these\napplications due to its simplicity and ease of use. Yet high performance sparse\nkernels in Python remain limited in functionality, especially on modern CPU and\nGPU architectures. We present pyGinkgo, a lightweight and Pythonic interface to\nthe Ginkgo library, offering high-performance sparse linear algebra support\nwith platform portability across CUDA, HIP, and OpenMP backends. pyGinkgo\nbridges the gap between high-performance C++ backends and Python usability by\nexposing Ginkgo's capabilities via Pybind11 and a NumPy and PyTorch compatible\ninterface. We benchmark pyGinkgo's performance against state-of-the-art Python\nlibraries including SciPy, CuPy, PyTorch, and TensorFlow. Results across\nhardware from different vendors demonstrate that pyGinkgo consistently\noutperforms existing Python tools in both sparse matrix vector (SpMV) product\nand iterative solver performance, while maintaining performance parity with\nnative Ginkgo C++ code. Our work positions pyGinkgo as a compelling backend for\nsparse machine learning models and scientific workflows.", "AI": {"tldr": "The paper introduces pyGinkgo, a Python interface to the Ginkgo library, enabling high-performance sparse linear algebra with compatibility across multiple hardware platforms.", "motivation": "To address the limitations of high-performance sparse linear algebra in Python, especially on modern CPU and GPU architectures.", "method": "Developed pyGinkgo, a Python wrapper for the Ginkgo library using Pybind11, with compatibility for NumPy and PyTorch, and benchmarked its performance against leading Python libraries.", "result": "pyGinkgo outperformed Python libraries such as SciPy, CuPy, PyTorch, and TensorFlow in sparse operations, while maintaining parity with Ginkgo C++ backend on various hardware platforms.", "conclusion": "pyGinkgo effectively bridges the gap between Python usability and high-performance sparse linear algebra, making it a valuable tool for machine learning and scientific computing."}}
{"id": "2510.08368", "pdf": "https://arxiv.org/pdf/2510.08368", "abs": "https://arxiv.org/abs/2510.08368", "authors": ["Yi Zhang", "Yue Xie", "Tao Sun", "Fumiya Iida"], "title": "Co-design is powerful and not free", "categories": ["cs.NE", "cs.RO"], "comment": null, "summary": "Robotic performance emerges from the coupling of body and controller, yet it\nremains unclear when morphology-control co-design is necessary. We present a\nunified framework that embeds morphology and control parameters within a single\nneural network, enabling end-to-end joint optimization. Through case studies in\nstatic-obstacle-constrained reaching, we evaluate trajectory error, success\nrate, and collision probability. The results show that co-design provides clear\nbenefits when morphology is poorly matched to the task, such as near obstacles\nor workspace boundaries, where structural adaptation simplifies control.\nConversely, when the baseline morphology already affords sufficient capability,\ncontrol-only optimization often matches or exceeds co-design. By clarifying\nwhen control is enough and when it is not, this work advances the understanding\nof embodied intelligence and offers practical guidance for embodiment-aware\nrobot design.", "AI": {"tldr": "This paper explores a framework for co-designing robot morphology and control and examines when co-design is beneficial.", "motivation": "To understand when the co-design of robot morphology and control is necessary for improving robotic performance in task-specific scenarios.", "method": "A unified framework where morphology and control parameters are merged into a single neural network for end-to-end optimization. Case studies include tasks constrained by static obstacles to assess various performance metrics.", "result": "Co-design outperforms control-only optimization in situations where the morphology is poorly matched to the task but provides no advantage when the baseline morphology is sufficient.", "conclusion": "This work enhances the understanding of embodied intelligence and provides practical criteria for deciding between co-design and control-only optimization in robot design."}}
{"id": "2510.07449", "pdf": "https://arxiv.org/pdf/2510.07449", "abs": "https://arxiv.org/abs/2510.07449", "authors": ["Georgia Antoniou", "Haris Volos", "Jawad Haj Yahya", "Yiannakis Sazeides"], "title": "How long can you sleep? Idle Time System Inefficiencies and Opportunities", "categories": ["cs.AR", "C.1.0"], "comment": "3 pages, 3 figures, accepted at the 1st International Workshop on\n  Data Center Energy Efficiency (DCEE2025) 2025", "summary": "This work introduces a model-based framework that reveals the idle\nopportunity of modern servers running latency-critical applications.\nSpecifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to\nestimate the theoretical idle time distribution at the CPU core and system\n(package) level. A comparison of the actual idleness of a real server and that\nfrom the theoretical models reveals significant missed opportunities to enter\ndeep idle states. This inefficiency is attributed to the idle-governor\ninaccuracy and the high latency to transition to/from legacy deep-idle states.\nThe proposed methodology offers the means for an early-stage design exploration\nand insights into idle time behavior and opportunities for varying server\nsystem configurations and load.", "AI": {"tldr": "The paper outlines a framework using theoretical queuing models to uncover idle time inefficiencies on servers running latency-critical applications and proposes improvements for deep idle state transitions.", "motivation": "Modern servers running latency-critical applications experience inefficiencies in exploiting idle opportunities. The study aims to address missed chances to enhance server energy efficiency and deep idle state utilization.", "method": "Three queuing models\u2014M/M/1, cxM/M/1, and M/M/c\u2014are applied to estimate CPU core and system-level idle time distributions. Real server idleness is compared against theoretical predictions to identify mismatches and causes of inefficiency.", "result": "The comparison highlights substantial missed idle opportunities due to idle-governor inaccuracies and high transition latency. The framework provides insights for optimizing server configurations and idle state management.", "conclusion": "This framework enables early-stage design exploration and offers valuable insights for improving server idle behavior and energy efficiency in latency-critical applications."}}
{"id": "2510.07331", "pdf": "https://arxiv.org/pdf/2510.07331", "abs": "https://arxiv.org/abs/2510.07331", "authors": ["Faruk Alpay", "Hamdi Alakkad"], "title": "Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation", "categories": ["cs.AI", "cs.LO", "68N15, 68Q55, 68Q60, 03B35", "D.3.1; F.3.1; F.3.2"], "comment": "18 pages, Lean code provided", "summary": "This paper introduces Truth-Aware Decoding (TAD), a verification-oriented\ndecoding scheme that aligns neural language generation with knowledge bases.\nSituated in the tradition of probabilistic program semantics for sequence\nmodels, TAD augments modern instruction-tuned systems with a lattice of\nsemantic guards that operate at decode time. Our contributions are fourfold:\n(i) a constraint-based semantics that renders oracle filtering as a\nprogram-logic judgment, (ii) a proof that greedy selection enjoys local\nlikelihood dominance under sound and complete guards (Theorem 2.7), (iii) an\nentropy-style invariant that quantifies factual risk via knowledge-aware safe\nmass, and (iv) a multi-agent operational calculus with verified Lean artefacts\nto certify implementation behaviour. Numerical and algorithmic case studies\nconfirm that the resulting guardrails reduce hallucinations without sacrificing\nthroughput, yielding a pragmatic bridge between large-scale empirical models\nand formal verification.", "AI": {"tldr": "The paper introduces Truth-Aware Decoding (TAD), a decoding method leveraging semantic safeguards to improve factual accuracy in neural language generation.", "motivation": "To address the challenge of hallucinations in large language models by aligning neural generation with structured knowledge bases.", "method": "The proposed Truth-Aware Decoding employs semantic guards during decode time and offers theoretical proofs and verified tactics using Lean to ensure factual alignment.", "result": "Empirical and algorithmic studies confirm reduced hallucination rates while maintaining processing efficiency.", "conclusion": "Truth-Aware Decoding is an effective approach to integrate formal verification with neural language generation, ensuring factuality without throughput penalties."}}
{"id": "2510.07811", "pdf": "https://arxiv.org/pdf/2510.07811", "abs": "https://arxiv.org/abs/2510.07811", "authors": ["Aryan Poduri"], "title": "Adaptive Execution Scheduler for DataDios SmartDiff", "categories": ["cs.DC", "cs.LG"], "comment": "4 pages, 1 figure", "summary": "We present an adaptive scheduler for a single differencing engine (SmartDiff)\nwith two execution modes: (i) in-memory threads and (ii) Dask based\nparallelism. The scheduler continuously tunes batch size and worker/thread\ncount within fixed CPU and memory budgets to minimize p95 latency. A\nlightweight preflight profiler estimates bytes/row and I/O rate; an online\ncost/memory model prunes unsafe actions; and a guarded hill-climb policy favors\nlower latency with backpressure and straggler mitigation. Backend selection is\ngated by a conservative working-set estimate so that in-memory execution is\nchosen when safe, otherwise Dask is used. Across synthetic and public tabular\nbenchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a\ntuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),\nwhile lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)\nwith zero OOMs and comparable throughput.", "AI": {"tldr": "The paper introduces an adaptive scheduler for SmartDiff that dynamically tunes execution settings to improve latency and memory efficiency.", "motivation": "The motivation is to optimize performance in a differencing engine, SmartDiff, by reducing p95 latency and memory usage within fixed CPU and memory budgets.", "method": "The approach involves an adaptive scheduler using an in-memory and Dask-mode hybrid execution. It employs lightweight preflight profiling, an online cost model, and a guarded hill-climb policy for decision-making.", "result": "The scheduler achieves a 23-28% p95 latency reduction over tuned heuristics, 35-40% over grid baselines, and reduces peak memory usage by 16-22% (or 25-32% over fixed baselines). There were no out-of-memory errors and throughput remained comparable.", "conclusion": "The proposed adaptive scheduler effectively balances execution settings to achieve significant improvements in latency and memory usage, making it robust and efficient for use in tabular data processing."}}
{"id": "2510.07582", "pdf": "https://arxiv.org/pdf/2510.07582", "abs": "https://arxiv.org/abs/2510.07582", "authors": ["Yuyan Bao", "Tiark Rompf"], "title": "Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness", "categories": ["cs.PL"], "comment": null, "summary": "Programming benefits from a clear separation between pure, mathematical\ncomputation and impure, effectful interaction with the world. Existing\napproaches to enforce this separation include monads, type-and-effect systems,\nand capability systems. All share a tension between precision and usability,\nand each one has non-obvious strengths and weaknesses.\n  This paper aims to raise the bar in assessing such systems. First, we propose\na semantic definition of purity, inspired by contextual equivalence, as a\nbaseline independent of any specific typing discipline. Second, we propose that\nexpressiveness should be measured by the degree of completeness, i.e., how many\nsemantically pure terms can be typed as pure. Using this measure, we focus on\nminimal meaningful effect and capability systems and show that they are\nincomparable, i.e., neither subsumes the other in terms of expressiveness.\n  Based on this result, we propose a synthesis and show that type, ability, and\neffect systems combine their respective strengths while avoiding their\nweaknesses. As part of our formal model, we provide a logical relation to\nfacilitate proofs of purity and other properties for a variety of effect typing\ndisciplines.", "AI": {"tldr": "The paper explores the trade-offs between monads, type-and-effect systems, and capability systems in programming, proposing a synthesis for better handling purity and interaction of systems.", "motivation": "Current programming paradigms need improved systems to precisely yet usability separate pure computation from impure interactions.", "method": "It defines purity based on contextual equivalence and measures expressiveness via semantic completeness. Then compares and synthesizes system strengths using a formal model.", "result": "Minimal effect systems and capability systems were found incomparable in expressiveness, while a synthesis combines their strengths.", "conclusion": "The proposed synthesis improves upon existing systems by offering a better balance between precision, expressiveness, and usability in effect typing disciplines."}}
{"id": "2510.07435", "pdf": "https://arxiv.org/pdf/2510.07435", "abs": "https://arxiv.org/abs/2510.07435", "authors": ["Zixuan Feng", "Sadia Afroz", "Anita Sarma"], "title": "Modeling Developer Burnout with GenAI Adoption", "categories": ["cs.SE", "cs.HC"], "comment": "10 pages, LLM", "summary": "Generative AI (GenAI) is rapidly reshaping software development workflows.\nWhile prior studies emphasize productivity gains, the adoption of GenAI also\nintroduces new pressures that may harm developers' well-being. In this paper,\nwe investigate the relationship between the adoption of GenAI and developers'\nburnout. We utilized the Job Demands--Resources (JD--R) model as the analytic\nlens in our empirical study. We employed a concurrent embedded mixed-methods\nresearch design, integrating quantitative and qualitative evidence. We first\nsurveyed 442 developers across diverse organizations, roles, and levels of\nexperience. We then employed Partial Least Squares--Structural Equation\nModeling (PLS-SEM) and regression to model the relationships among job demands,\njob resources, and burnout, complemented by a qualitative analysis of\nopen-ended responses to contextualize the quantitative findings. Our results\nshow that GenAI adoption heightens burnout by increasing job demands, while job\nresources and positive perceptions of GenAI mitigate these effects, reframing\nadoption as an opportunity.", "AI": {"tldr": "The study examines how using Generative AI (GenAI) in software development influences developer burnout, finding that it increases job demands and burnout but can be mitigated by job resources and positive perceptions.", "motivation": "To understand how adopting GenAI impacts developers' well-being, particularly regarding burnout, due to the technology's dual productivity benefits and potential new pressures.", "method": "A mixed-methods approach combining a survey of 442 developers across roles and organizations, quantitative modeling with PLS-SEM and regression analyses, and qualitative open-ended response analysis.", "result": "Findings indicate that GenAI adoption amplifies job demands and thereby burnout, but the effects are alleviated by job resources and favorable perspectives on GenAI.", "conclusion": "While GenAI poses risks of increased burnout, proper support systems and positive framing around its use can help developers perceive its adoption more as an opportunity than a burden."}}
{"id": "2510.07342", "pdf": "https://arxiv.org/pdf/2510.07342", "abs": "https://arxiv.org/abs/2510.07342", "authors": ["Haomiao Chen", "Keith W Jamison", "Mert R. Sabuncu", "Amy Kuceyeski"], "title": "Beyond Grid-Locked Voxels: Neural Response Functions for Continuous Brain Encoding", "categories": ["q-bio.NC", "cs.LG", "eess.IV"], "comment": null, "summary": "Neural encoding models aim to predict fMRI-measured brain responses to\nnatural images. fMRI data is acquired as a 3D volume of voxels, where each\nvoxel has a defined spatial location in the brain. However, conventional\nencoding models often flatten this volume into a 1D vector and treat voxel\nresponses as independent outputs. This removes spatial context, discards\nanatomical information, and ties each model to a subject-specific voxel grid.\nWe introduce the Neural Response Function (NRF), a framework that models fMRI\nactivity as a continuous function over anatomical space rather than a flat\nvector of voxels. NRF represents brain activity as a continuous implicit\nfunction: given an image and a spatial coordinate (x, y, z) in standardized MNI\nspace, the model predicts the response at that location. This formulation\ndecouples predictions from the training grid, supports querying at arbitrary\nspatial resolutions, and enables resolution-agnostic analyses. By grounding the\nmodel in anatomical space, NRF exploits two key properties of brain responses:\n(1) local smoothness -- neighboring voxels exhibit similar response patterns;\nmodeling responses continuously captures these correlations and improves data\nefficiency, and (2) cross-subject alignment -- MNI coordinates unify data\nacross individuals, allowing a model pretrained on one subject to be fine-tuned\non new subjects. In experiments, NRF outperformed baseline models in both\nintrasubject encoding and cross-subject adaptation, achieving high performance\nwhile reducing the data size needed by orders of magnitude. To our knowledge,\nNRF is the first anatomically aware encoding model to move beyond flattened\nvoxels, learning a continuous mapping from images to brain responses in 3D\nspace.", "AI": {"tldr": "The paper introduces the Neural Response Function (NRF), a framework for modeling fMRI-measured brain responses as continuous functions over anatomical 3D space rather than flat voxel vectors.", "motivation": "To address the limitations of conventional neural encoding models that ignore spatial anatomical structure by flattening 3D fMRI data into 1D vectors.", "method": "The NRF framework models fMRI activity as a continuous implicit function that predicts brain responses at spatial coordinates in standardized MNI space. It leverages properties like local smoothness and cross-subject alignment.", "result": "NRF demonstrated superior performance in encoding brain responses both within individual subjects and across subjects, reducing required training data size dramatically.", "conclusion": "NRF provides the first anatomically aware framework for encoding brain responses, moving beyond traditional flat voxel representations to exploit spatial and anatomical structures."}}
{"id": "2510.07417", "pdf": "https://arxiv.org/pdf/2510.07417", "abs": "https://arxiv.org/abs/2510.07417", "authors": ["Corban Rivera", "Grayson Byrd", "Meghan Booker", "Bethany Kemp", "Allison Gaines", "Emma Holmes", "James Uplinger", "Celso M de Melo", "David Handelman"], "title": "FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams", "categories": ["cs.RO"], "comment": null, "summary": "Coordinating heterogeneous robot teams from free-form natural-language\ninstructions is hard. Language-only planners struggle with long-horizon\ncoordination and hallucination, while purely formal methods require\nclosed-world models. We present FLEET, a hybrid decentralized framework that\nturns language into optimized multi-robot schedules. An LLM front-end produces\n(i) a task graph with durations and precedence and (ii) a capability-aware\nrobot--task fitness matrix; a formal back-end solves a makespan-minimization\nproblem while the underlying robots execute their free-form subtasks with\nagentic closed-loop control. Across multiple free-form language-guided autonomy\ncoordination benchmarks, FLEET improves success over state of the art\ngenerative planners on two-agent teams across heterogeneous tasks. Ablations\nshow that mixed integer linear programming (MILP) primarily improves temporal\nstructure, while LLM-derived fitness is decisive for capability-coupled tasks;\ntogether they deliver the highest overall performance. We demonstrate the\ntranslation to real world challenges with hardware trials using a pair of\nquadruped robots with disjoint capabilities.", "AI": {"tldr": "The paper introduces FLEET, a framework that converts natural language commands into multi-robot schedules, combining an LLM front-end and formal optimization techniques for efficient operation.", "motivation": "Current solutions for coordinating robot teams from natural language struggle with accuracy or require restrictive closed-world models.", "method": "FLEET employs a hybrid decentralized approach where an LLM creates task structures and robot-task fitness metrics, and a formal optimization back-end uses MILP to generate schedules.", "result": "FLEET outperforms generative planners in multi-robot coordination tasks, with real-world validation using heterogeneous quadruped robots.", "conclusion": "Integrating LLMs with MILP-based optimizations enables robust, capable robotic teamwork for free-form tasks directed by natural language."}}
{"id": "2510.07501", "pdf": "https://arxiv.org/pdf/2510.07501", "abs": "https://arxiv.org/abs/2510.07501", "authors": ["Sihyung Park", "Wenbin Lu", "Shu Yang"], "title": "Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "30 pages, 5 figures, 6 tables, The Thirty-Ninth Annual Conference on\n  Neural Information Processing Systems", "summary": "Truncation by death, a prevalent challenge in critical care, renders\ntraditional dynamic treatment regime (DTR) evaluation inapplicable due to\nill-defined potential outcomes. We introduce a principal stratification-based\nmethod, focusing on the always-survivor value function. We derive a\nsemiparametrically efficient, multiply robust estimator for multi-stage DTRs,\ndemonstrating its robustness and efficiency. Empirical validation and an\napplication to electronic health records showcase its utility for personalized\ntreatment optimization.", "AI": {"tldr": "The paper addresses challenges in evaluating Dynamic Treatment Regimes (DTRs) in critical care, proposing a robust estimation method based on principal stratification.", "motivation": "Traditional methods for evaluating DTRs fail in critical care due to truncation by death, which obscures potential outcomes.", "method": "The authors develop a semiparametrically efficient, multiply robust estimator focusing on always-survivor value function using principal stratification.", "result": "The proposed method is empirically validated and applied to electronic health records, showcasing its robustness and efficiency in personalized treatment optimization.", "conclusion": "The approach provides a robust and efficient tool for optimizing multi-stage DTRs, particularly useful in critical care contexts."}}
{"id": "2510.07346", "pdf": "https://arxiv.org/pdf/2510.07346", "abs": "https://arxiv.org/abs/2510.07346", "authors": ["Nader Nemati"], "title": "Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation", "categories": ["cs.CV", "cs.LG", "68T07, 68T45, 68U10, 62H30, 94A08", "I.2.10; I.4.8; I.5.4; I.2.6; C.3"], "comment": "13 pages, 10 figures", "summary": "Maritime object detection faces essential challenges due to the small target\nsize and limitations of labeled real RGB data. This paper will present a\nreal-time object detection system based on RT-DETR, enhanced by employing\naugmented synthetic images while strictly evaluating on real data. This study\nemploys RT-DETR for the maritime environment by combining multi-scale feature\nfusion, uncertainty-minimizing query selection, and smart weight between\nsynthetic and real training samples. The fusion module in DETR enhances the\ndetection of small, low-contrast vessels, query selection focuses on the most\nreliable proposals, and the weighting strategy helps reduce the visual gap\nbetween synthetic and real domains. This design preserves DETR's refined\nend-to-end set prediction while allowing users to adjust between speed and\naccuracy at inference time. Data augmentation techniques were also used to\nbalance the different classes of the dataset to improve the robustness and\naccuracy of the model. Regarding this study, a full Python robust maritime\ndetection pipeline is delivered that maintains real-time performance even under\npractical limits. It also verifies how each module contributes, and how the\nsystem handles failures in extreme lighting or sea conditions. This study also\nincludes a component analysis to quantify the contribution of each\narchitectural module and explore its interactions.", "AI": {"tldr": "The paper introduces a real-time maritime object detection system leveraging RT-DETR, synthetic image augmentation, and advanced training strategies.", "motivation": "The study aims to overcome challenges in maritime object detection arising from small target sizes and a lack of labeled real RGB data.", "method": "The approach uses RT-DETR with multi-scale feature fusion, targeted query selection, and synthetic-real data weighting, alongside data augmentation for class balance.", "result": "A robust maritime object detection pipeline achieving real-time performance even in challenging conditions, with module effectiveness confirmed through component analysis.", "conclusion": "The system enhances detection accuracy while maintaining real-time capabilities and provides insights into handling extreme maritime scenarios."}}
{"id": "2510.07320", "pdf": "https://arxiv.org/pdf/2510.07320", "abs": "https://arxiv.org/abs/2510.07320", "authors": ["Nelaka K. A. R", "Peiris M. K. V", "Liyanage R. P. B"], "title": "Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Autism Spectrum Disorder significantly influences the communication\nabilities, learning processes, behavior, and social interactions of\nindividuals. Although early intervention and customized educational strategies\nare critical to improving outcomes, there is a pivotal gap in understanding and\naddressing nuanced behavioral patterns and emotional identification in autistic\nchildren prior to skill development. This extended research delves into the\nfoundational step of recognizing and mapping these patterns as a prerequisite\nto improving learning and soft skills. Using a longitudinal approach to monitor\nemotions and behaviors, this study aims to establish a baseline understanding\nof the unique needs and challenges faced by autistic students, particularly in\nthe Information Technology domain, where opportunities are markedly limited.\nThrough a detailed analysis of behavioral trends over time, we propose a\ntargeted framework for developing applications and technical aids designed to\nmeet these identified needs. Our research underscores the importance of a\nsequential and evidence-based intervention approach that prioritizes a deep\nunderstanding of each child's behavioral and emotional landscape as the basis\nfor effective skill development. By shifting the focus toward early\nidentification of behavioral patterns, we aim to foster a more inclusive and\nsupportive learning environment that can significantly improve the educational\nand developmental trajectory of children with ASD.", "AI": {"tldr": "This study focuses on understanding behavioral and emotional patterns in autistic children to create evidence-based frameworks for better skill development, particularly in IT education.", "motivation": "There is a critical lack of understanding of nuanced behavioral and emotional patterns in autistic children, which hinders the development of effective interventions and skill-building strategies.", "method": "A longitudinal approach was used to monitor emotions and behaviors, analyzing trends over time to identify specific challenges faced by autistic students, especially in IT education.", "result": "The research established foundational insights into the unique needs of autistic children and proposed a targeted framework for developing technical aids and applications to support these needs.", "conclusion": "Prioritizing early identification and mapping of behavioral patterns can significantly improve the learning and developmental outcomes for autistic children, creating a more inclusive educational system."}}
{"id": "2510.07359", "pdf": "https://arxiv.org/pdf/2510.07359", "abs": "https://arxiv.org/abs/2510.07359", "authors": ["Jingfei Huang", "Han Tu"], "title": "Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": "10 pages", "summary": "The ascension of social media platforms has transformed our understanding of\nurban environments, giving rise to nuanced variations in sentiment reaction\nembedded within human perception and opinion, and challenging existing\nmultidimensional sentiment analysis approaches in urban studies. This study\npresents novel methodologies for identifying and elucidating sentiment\ninconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent\nStreet view images to measure perceptions, and 984,024 Weibo social media text\nposts to measure opinions. A reaction index is developed, integrating object\ndetection and natural language processing techniques to classify sentiment in\nBeijing Second Ring for 2016 and 2022. Classified sentiment reaction is\nanalysed and visualized using regression analysis, image segmentation, and word\nfrequency based on land-use distribution to discern underlying factors. The\nperception affective reaction trend map reveals a shift toward more evenly\ndistributed positive sentiment, while the opinion affective reaction trend map\nshows more extreme changes. Our mismatch map indicates significant disparities\nbetween the sentiments of human perception and opinion of urban areas over the\nyears. Changes in sentiment reactions have significant relationships with\nelements such as dense buildings and pedestrian presence. Our inconsistent maps\npresent perception and opinion sentiments before and after the pandemic and\noffer potential explanations and directions for environmental management, in\nformulating strategies for urban renewal.", "AI": {"tldr": "The study examines sentiment inconsistencies in urban settings using visual and social media data from Beijing, revealing mismatches in human perception and opinion sentiment.", "motivation": "The paper aims to understand the variations in sentiment reaction within urban environments and address challenges in sentiment analysis approaches in urban studies.", "method": "A dataset of street view images and social media posts is analyzed using object detection and NLP techniques, creating sentiment indexes and maps for Beijing across 2016 and 2022.", "result": "The research finds shifts in sentiment trends, with perception becoming more balanced while opinion changes remain extreme. Dense buildings and pedestrian presence greatly influence sentiment.", "conclusion": "The study highlights significant sentiment mismatches and their implications, offering insights for urban renewal and environmental management strategies."}}
{"id": "2510.08342", "pdf": "https://arxiv.org/pdf/2510.08342", "abs": "https://arxiv.org/abs/2510.08342", "authors": ["Jordan Cotler", "Cl\u00e9ment Hongler", "Barbora Hudcov\u00e1"], "title": "Self-replication and Computational Universality", "categories": ["nlin.CG", "cs.FL", "nlin.AO"], "comment": "9+67 pages, 32 figures", "summary": "Self-replication is central to all life, and yet how it dynamically emerges\nin physical, non-equilibrium systems remains poorly understood. Von Neumann's\npioneering work in the 1940s and subsequent developments suggest a natural\nhypothesis: that any physical system capable of Turing-universal computation\ncan support self-replicating objects. In this work, we challenge this\nhypothesis by clarifying what computational universality means for physical\nsystems and constructing a cellular automaton that is Turing-universal but\ncannot sustain non-trivial self-replication. By analogy with biology, such\ndynamics manifest transcription and translation but cannot instantiate\nreplication. More broadly, our work emphasizes that the computational\ncomplexity of translating between physical dynamics and symbolic computation is\ninseparable from any claim of universality (exemplified by our analysis of Rule\n110) and builds mathematical foundations for identifying self-replicating\nbehavior. Our approach enables the formulation of necessary dynamical and\ncomputational conditions for a physical system to constitute a living organism.", "AI": {"tldr": "This paper investigates the relationship between Turing-universal computation and self-replication in physical systems, challenging the assumption that the former guarantees the latter.", "motivation": "To understand how self-replication emerges dynamically in physical, non-equilibrium systems and test the hypothesis that any Turing-universal system can support self-replication.", "method": "The authors clarified the definition of Turing-universality in physical systems, constructed a cellular automaton to test self-replication, and analyzed dynamics like transcription and translation.", "result": "They found that a constructed Turing-universal cellular automaton could perform computation but not sustain self-replication, challenging the conventional hypothesis.", "conclusion": "The findings highlight the significance of computational complexity in translating between physical dynamics and symbolic computation for understanding self-replication and suggest necessary conditions for defining living systems."}}
{"id": "2411.15876", "pdf": "https://arxiv.org/pdf/2411.15876", "abs": "https://arxiv.org/abs/2411.15876", "authors": ["Md. Saiful Bari Siddiqui", "Md Mohaiminul Islam", "Md. Golam Rabiul Alam"], "title": "DUA-D2C: Dynamic Uncertainty Aware Method for Overfitting Remediation in Deep Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "comment": "This version (v2) extends our previous work (arXiv:2411.15876v1) on\n  Divide2Conquer (D2C) by introducing Dynamic Uncertainty-Aware Divide2Conquer\n  (DUA-D2C). The manuscript is currently under review at Complex and\n  Intelligent Systems", "summary": "Overfitting remains a significant challenge in deep learning, often arising\nfrom data outliers, noise, and limited training data. To address this, the\nDivide2Conquer (D2C) method was previously proposed, which partitions training\ndata into multiple subsets and trains identical models independently on each.\nThis strategy enables learning more consistent patterns while minimizing the\ninfluence of individual outliers and noise. However, D2C's standard aggregation\ntypically treats all subset models equally or based on fixed heuristics (like\ndata size), potentially underutilizing information about their varying\ngeneralization capabilities. Building upon this foundation, we introduce\nDynamic Uncertainty-Aware Divide2Conquer (DUA-D2C), an advanced technique that\nrefines the aggregation process. DUA-D2C dynamically weights the contributions\nof subset models based on their performance on a shared validation set,\nconsidering both accuracy and prediction uncertainty. This intelligent\naggregation allows the central model to preferentially learn from subsets\nyielding more generalizable and confident edge models, thereby more effectively\ncombating overfitting. Empirical evaluations on benchmark datasets spanning\nmultiple domains demonstrate that DUA-D2C significantly improves\ngeneralization. Our analysis includes evaluations of decision boundaries, loss\ncurves, and other performance metrics, highlighting the effectiveness of\nDUA-D2C. This study demonstrates that DUA-D2C improves generalization\nperformance even when applied on top of other regularization methods,\nestablishing it as a theoretically grounded and effective approach to combating\noverfitting in modern deep learning. Our codes are publicly available at:\nhttps://github.com/Saiful185/DUA-D2C.", "AI": {"tldr": "DUA-D2C improves overfitting in deep learning by dynamically weighting model contributions based on validation performance and prediction uncertainty.", "motivation": "To address overfitting caused by data outliers, noise, and limited training data by improving upon the Divide2Conquer (D2C) method.", "method": "DUA-D2C dynamically weights subset models during aggregation based on their validation performance and uncertainty, enabling better generalization.", "result": "DUA-D2C significantly improves generalization performance across benchmark datasets and enhances decision boundaries and loss curves.", "conclusion": "DUA-D2C is a theoretically sound and effective approach to overfitting, even when combined with existing regularization techniques."}}
{"id": "2510.07719", "pdf": "https://arxiv.org/pdf/2510.07719", "abs": "https://arxiv.org/abs/2510.07719", "authors": ["Parker Hao Tian", "Zahra Yousefijamarani", "Alaa Alameldeen"], "title": "DL-PIM: Improving Data Locality in Processing-in-Memory Systems", "categories": ["cs.AR"], "comment": null, "summary": "PIM architectures aim to reduce data transfer costs between processors and\nmemory by integrating processing units within memory layers. Prior PIM\narchitectures have shown potential to improve energy efficiency and\nperformance. However, such advantages rely on data proximity to the processing\nunits performing computations. Data movement overheads can degrade PIM's\nperformance and energy efficiency due to the need to move data between a\nprocessing unit and a distant memory location. %they face challenges due to the\noverhead of transferring data from remote memory locations to processing units\ninside memory for computation. In this paper, we demonstrate that a large\nfraction of PIM's latency per memory request is attributed to data transfers\nand queuing delays from remote memory accesses. To improve PIM's data locality,\nwe propose DL-PIM, a novel architecture that dynamically detects the overhead\nof data movement, and proactively moves data to a reserved area in the local\nmemory of the requesting processing unit. DL-PIM uses a distributed\naddress-indirection hardware lookup table to redirect traffic to the current\ndata location. We propose DL-PIM implementations on two 3D stacked memories:\nHMC and HBM. While some workloads benefit from DL-PIM, others are negatively\nimpacted by the additional latency due to indirection accesses. Therefore, we\npropose an adaptive mechanism that assesses the cost and benefit of indirection\nand dynamically enables or disables it to prevent degrading workloads that\nsuffer from indirection. Overall, DL-PIM reduces the average memory latency per\nrequest by 54% in HMC and 50% in HBM which resulted in performance improvement\nof 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all\nrepresentative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup\nin HBM, showing that DL-PIM enhances data locality and overall system\nperformance.", "AI": {"tldr": "DL-PIM is a novel PIM architecture that addresses data movement overheads and enhances data locality in 3D stacked memories (HMC and HBM), reducing memory latency and improving overall system performance.", "motivation": "To address the performance and energy efficiency challenges in PIM architectures caused by data movement overheads and the lack of data locality.", "method": "This paper introduces DL-PIM, an architecture that uses dynamic data movement detection and a distributed address-indirection table to proactively relocate data to local memory areas close to the processing unit. An adaptive mechanism enables or disables indirection to optimize performance across varying workloads.", "result": "DL-PIM reduced average memory latency by 54% in HMC and 50% in HBM, achieving 15% and 5% performance improvements for workloads with data reuse in HMC and HBM, respectively. Overall, it delivered a 6% speedup in HMC and 3% speedup in HBM across all workloads.", "conclusion": "DL-PIM successfully enhances data locality by mitigating data movement overheads, boosting the performance of workloads with data reuse while maintaining efficiency for a broad range of tasks."}}
{"id": "2510.07363", "pdf": "https://arxiv.org/pdf/2510.07363", "abs": "https://arxiv.org/abs/2510.07363", "authors": ["Tianxiang Xu", "Zhichao Wen", "Xinyu Zhao", "Jun Wang", "Yan Li", "Chang Liu"], "title": "L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)", "categories": ["cs.AI"], "comment": "This preprint was submitted to IEEE TrustCom 2025. The accepted\n  version will be published under copyright 2025 IEEE", "summary": "The increasing integration of Industrial IoT (IIoT) exposes critical\ncyber-physical systems to sophisticated, multi-stage attacks that elude\ntraditional defenses lacking contextual awareness. This paper introduces\nL2M-AID, a novel framework for Autonomous Industrial Defense using\nLLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team\nof collaborative agents, each driven by a Large Language Model (LLM), to\nachieve adaptive and resilient security. The core innovation lies in the deep\nfusion of two AI paradigms: we leverage an LLM as a semantic bridge to\ntranslate vast, unstructured telemetry into a rich, contextual state\nrepresentation, enabling agents to reason about adversary intent rather than\nmerely matching patterns. This semantically-aware state empowers a Multi-Agent\nReinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative\nstrategies. The MARL reward function is uniquely engineered to balance security\nobjectives (threat neutralization) with operational imperatives, explicitly\npenalizing actions that disrupt physical process stability. To validate our\napproach, we conduct extensive experiments on the benchmark SWaT dataset and a\nnovel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.\nResults demonstrate that L2M-AID significantly outperforms traditional IDS,\ndeep learning anomaly detectors, and single-agent RL baselines across key\nmetrics, achieving a 97.2% detection rate while reducing false positives by\nover 80% and improving response times by a factor of four. Crucially, it\ndemonstrates superior performance in maintaining physical process stability,\npresenting a robust new paradigm for securing critical national infrastructure.", "AI": {"tldr": "L2M-AID is a framework leveraging LLMs and Multi-Agent Reinforcement Learning for securing industrial IoT systems against multi-stage attacks, showing superior detection, low false positives, faster responses, and maintaining system stability.", "motivation": "To address the inability of traditional defenses to counter sophisticated, multi-stage attacks on Industrial IoT systems by ensuring contextual awareness and maintaining system stability.", "method": "The paper introduces L2M-AID, which integrates Large Language Models (LLMs) to convert telemetry into contextual state representations. This drives a MAPPO-based Multi-Agent Reinforcement Learning algorithm that balances security and operational imperatives through a tailored reward function.", "result": "L2M-AID achieves a 97.2% detection rate, reduces false positives by over 80%, and improves response times by 4x. It also ensures superior physical process stability in experiments on benchmark and synthetic datasets.", "conclusion": "L2M-AID is a robust, innovative framework for defending critical infrastructure, exemplifying the potential of LLMs and MARL integration to address modern cybersecurity challenges."}}
{"id": "2510.08164", "pdf": "https://arxiv.org/pdf/2510.08164", "abs": "https://arxiv.org/abs/2510.08164", "authors": ["Marco Picone", "Samuele Burattini", "Marco Melloni", "Prasad Talasila", "Davide Ziglioli", "Matteo Martinelli", "Nicola Bicocchi", "Alessandro Ricci", "Peter Gorm Larsen"], "title": "A Multi-Simulation Bridge for IoT Digital Twins", "categories": ["cs.DC"], "comment": null, "summary": "The increasing capabilities of Digital Twins (DTs) in the context of the\nInternet of Things (IoT) and Industrial IoT (IIoT) call for seamless\nintegration with simulation platforms to support system design, validation, and\nreal-time operation. This paper introduces the concept, design, and\nexperimental evaluation of the DT Simulation Bridge - a software framework that\nenables diverse interaction patterns between active DTs and simulation\nenvironments. The framework supports both the DT development lifecycle and the\nincorporation of simulations during active operation. Through bidirectional\ndata exchange, simulations can update DT models dynamically, while DTs provide\nreal-time feedback to adapt simulation parameters. We describe the\narchitectural design and core software components that ensure flexible\ninteroperability and scalable deployment. Experimental results show that the DT\nSimulation Bridge enhances design agility, facilitates virtual commissioning,\nand supports live behavioral analysis under realistic conditions, demonstrating\nits effectiveness across a range of industrial scenarios.", "AI": {"tldr": "The paper presents a software framework called DT Simulation Bridge for integrating Digital Twins (DTs) with simulation platforms, improving system design, validation, and real-time operation.", "motivation": "The increasing potential of Digital Twins in IoT and IIoT requires efficient integration with simulations to optimize design, validation, and operations.", "method": "The DT Simulation Bridge enables bidirectional data exchange between DTs and simulation environments, incorporating flexibility and scalability via architectural design and software components.", "result": "Experimental evaluation reveals enhanced design agility, virtual commissioning, and live behavioral analysis in industrial scenarios using the DT Simulation Bridge.", "conclusion": "The paper concludes that the DT Simulation Bridge effectively bridges DTs and simulations, optimizing its application across diverse industrial use cases."}}
{"id": "2510.07851", "pdf": "https://arxiv.org/pdf/2510.07851", "abs": "https://arxiv.org/abs/2510.07851", "authors": ["Willem Heijltjes"], "title": "The Functional Machine Calculus III: Control", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "The Functional Machine Calculus (Heijltjes 2022) is a new approach to\nunifying the imperative and functional programming paradigms. It extends the\nlambda-calculus, preserving the key features of confluent reduction and typed\ntermination, to embed computational effects, evaluation strategies, and control\nflow operations. The first instalment modelled sequential higher-order\ncomputation with global store, input/output, probabilities, and\nnon-determinism, and embedded both the call-by-name and call-by-value\nlambda-calculus, as well as Moggi's computational metalanguage and Levy's\ncall-by-push-value. The present paper extends the calculus from sequential to\nbranching and looping control flow. This allows the faithful embedding of a\nminimal but complete imperative language, including conditionals, exception\nhandling, and iteration, as well as constants and algebraic data types.\n  The calculus is defined through a simple operational semantics, extending the\n(simplified) Krivine machine for the lambda-calculus with multiple operand\nstacks to model effects and a continuation stack to model sequential,\nbranching, and looping computation. It features a confluent reduction relation\nand a system of simple types that guarantees termination of the machine and\nstrong normalization of reduction (in the absence of iteration). These\nproperties carry over to the embedded imperative language, providing a unified\nfunctional-imperative model of computation that supports simple types, a direct\nand intuitive operational semantics, and a confluent reduction semantics.", "AI": {"tldr": "The paper introduces extensions to the Functional Machine Calculus to unify functional and imperative programming paradigms, adding support for branching and looping control flows.", "motivation": "To integrate functional and imperative programming within a single unified calculus, addressing limitations in embedding imperative constructs and control flows.", "method": "Extended operational semantics based on a simplified Krivine machine, with additions like multiple operand stacks and a continuation stack to model computation features.", "result": "Provides a new calculus that embeds imperative language constructs while maintaining confluent reduction semantics and typed termination guarantees.", "conclusion": "The developed calculus successfully models computation unifying functional and imperative paradigms, ensuring strong formal properties such as normalization and termination."}}
{"id": "2510.07529", "pdf": "https://arxiv.org/pdf/2510.07529", "abs": "https://arxiv.org/abs/2510.07529", "authors": ["Carol Hanna", "Federica Sarro", "Mark Harman", "Justyna Petke"], "title": "HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs", "categories": ["cs.SE"], "comment": null, "summary": "Hot fixes are urgent, unplanned changes deployed to production systems to\naddress time-critical issues. Despite their importance, no existing evaluation\nbenchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the\nfirst dataset dedicated to real-world hot fixes. From an initial mining of 10\nactive Apache projects totaling over 190K commits and 150K issue reports, we\nidentified 746 software patches that met our hot-fix criteria. After manual\nevaluation, 679 were confirmed as genuine hot fixes, of which 110 are\nreproducible using a test suite. Building upon the Bugs$.$jar framework,\nHotBugs$.$jar integrates these 110 reproducible cases and makes available all\n679 manually validated hot fixes, each enriched with comprehensive metadata to\nsupport future research. Each hot fix was systematically identified using Jira\nissue data, validated by independent reviewers, and packaged in a reproducible\nformat with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar\nhas already been adopted as the official challenge dataset for the Search-Based\nSoftware Engineering (SBSE) Conference Challenge Track, demonstrating its\nimmediate impact. This benchmark enables the study and evaluation of tools for\nrapid debugging, automated repair, and production-grade resilience in modern\nsoftware systems to drive research in this essential area forward.", "AI": {"tldr": "HotBugs$.$jar is a benchmark dataset dedicated to hot fixes, derived from 10 Apache projects, including 679 validated hot fixes and metadata.", "motivation": "No existing evaluation benchmark specifically addresses hot fixes, despite their critical nature in production environments.", "method": "The dataset was created by mining Apache projects, identifying hot fixes, manually validating them, and packaging them with buggy/fixed versions, test suites, and metadata.", "result": "HotBugs$.$jar includes 679 manually validated hot fixes, 110 of which are reproducible, and serves as the challenge dataset for SBSE Conference Challenge Track.", "conclusion": "This dataset enables research on debugging, automated repair, and resilience in software systems, potentially advancing tools for production-grade systems."}}
{"id": "2510.07576", "pdf": "https://arxiv.org/pdf/2510.07576", "abs": "https://arxiv.org/abs/2510.07576", "authors": ["Teng Fei", "Srinivas Ravishankar", "Hoko Nakada", "Abhinav Uppal", "Ian Jackson", "Garrison W. Cottrell", "Ryusuke Hayashi", "Virginia R. de Sa"], "title": "Monkey Perceptogram: Reconstructing Visual Representation and Presumptive Neural Preference from Monkey Multi-electrode Arrays", "categories": ["q-bio.NC"], "comment": null, "summary": "Understanding how the primate brain transforms complex visual scenes into\ncoherent perceptual experiences remains a central challenge in neuroscience.\nHere, we present a comprehensive framework for interpreting monkey visual\nprocessing by integrating encoding and decoding approaches applied to two\nlarge-scale spiking datasets recorded from macaque using THINGS images (THINGS\nmacaque IT Dataset (TITD) and THINGS Ventral Stream Spiking Dataset (TVSD)). We\nleverage multi-electrode array recordings from the ventral visual\nstream--including V1, V4, and inferotemporal (IT) cortex--to investigate how\ndistributed neural populations encode and represent visual information. Our\napproach employs linear models to decode spiking activity into multiple latent\nvisual spaces (including CLIP and VDVAE embeddings) and reconstruct images\nusing state-of-the-art generative models. We further utilize encoding models to\nmap visual features back to neural activity, enabling visualization of the\n\"preferred stimuli\" that drive specific neural ensembles. Analyses of both\ndatasets reveal that it is possible to reconstruct both low-level (e.g., color,\ntexture) and high-level (e.g., semantic category) features of visual stimuli\nfrom population activity, with reconstructions preserving key perceptual\nattributes as quantified by feature-based similarity metrics. The\nspatiotemporal spike patterns reflect the ventral stream's hierarchical\norganization with anterior regions representing complex objects and categories.\nFunctional clustering identifies feature-specific neural ensembles, with\ntemporal dynamics show evolving feature selectivity post-stimulus. Our findings\ndemonstrate feasible, generalizable perceptual reconstruction from large-scale\nmonkey neural recordings, linking neural activity to perception.", "AI": {"tldr": "This paper introduces a framework using macaque spiking datasets to decode how visual information is processed by the primate ventral visual stream, reconstructing visual perceptions and their attributes.", "motivation": "To understand how the primate brain transforms complex visual scenes into coherent perceptual experiences, and to link neural activity to perceptual processes.", "method": "They use spiking data from macaque V1, V4, and IT cortex along with encoding and decoding models. Linear models are employed to decode neural activity into visual spaces, reconstruct images with generative models, and visualize preferred neural stimuli.", "result": "The study demonstrates that low-level and high-level features of visual stimuli can be reconstructed from neural activity, with spatiotemporal activity reflecting hierarchical processing in the ventral stream. Temporal dynamics reveal evolving feature selectivity.", "conclusion": "The findings illustrate the feasibility of reconstructing perceptual representations from neural recordings and highlight the hierarchical and dynamic nature of feature encoding in the ventral visual stream of primates."}}
{"id": "2510.07447", "pdf": "https://arxiv.org/pdf/2510.07447", "abs": "https://arxiv.org/abs/2510.07447", "authors": ["Girolamo Oddo", "Roberto Nuca", "Matteo Parsani"], "title": "VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics", "categories": ["cs.RO", "cs.LG", "math.DS"], "comment": null, "summary": "Developing a dynamic model for a high-performance vehicle is a complex\nproblem that requires extensive structural information about the system under\nanalysis. This information is often unavailable to those who did not design the\nvehicle and represents a typical issue in autonomous driving applications,\nwhich are frequently developed on top of existing vehicles; therefore, vehicle\nmodels are developed under conditions of information scarcity. This paper\nproposes a lightweight encoder-decoder model based on Gate Recurrent Unit\nlayers to correlate the vehicle's future state with its past states, measured\nonboard, and control actions the driver performs. The results demonstrate that\nthe model achieves a maximum mean relative error below 2.6% in extreme dynamic\nconditions. It also shows good robustness when subject to noisy input data\nacross the interested frequency components. Furthermore, being entirely\ndata-driven and free from physical constraints, the model exhibits physical\nconsistency in the output signals, such as longitudinal and lateral\naccelerations, yaw rate, and the vehicle's longitudinal velocity.", "AI": {"tldr": "The paper presents a GRU-based encoder-decoder model for predicting a vehicle's future state with high accuracy and robustness, even under challenging conditions.", "motivation": "Lack of detailed structural information about vehicles, especially in autonomous driving applications, necessitates developing data-driven models for predicting vehicle dynamics.", "method": "The authors designed a lightweight encoder-decoder model based on Gate Recurrent Unit (GRU) layers to estimate future states from past states and driver control actions.", "result": "The model achieves a mean relative error below 2.6% in challenging scenarios and robustly processes noisy input data across various frequency components.", "conclusion": "The proposed model is data-driven, free of physical constraints, and provides physically consistent output signals like accelerations, yaw rate, and longitudinal velocity, making it suitable for vehicle modeling with limited structural information."}}
{"id": "2510.07624", "pdf": "https://arxiv.org/pdf/2510.07624", "abs": "https://arxiv.org/abs/2510.07624", "authors": ["Abdelhakim Benechehab", "Gabriel Singer", "Corentin L\u00e9ger", "Youssef Attia El Hili", "Giuseppe Paolo", "Albert Thomas", "Maurizio Filippone", "Bal\u00e1zs K\u00e9gl"], "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Generative models form the backbone of modern machine learning, underpinning\nstate-of-the-art systems in text, vision, and multimodal applications. While\nMaximum Likelihood Estimation has traditionally served as the dominant training\nparadigm, recent work have highlighted its limitations, particularly in\ngeneralization and susceptibility to catastrophic forgetting compared to\nReinforcement Learning techniques, such as Policy Gradient methods. However,\nthese approaches depend on explicit reward signals, which are often unavailable\nin practice, leaving open the fundamental problem of how to align generative\nmodels when only high-quality datasets are accessible. In this work, we address\nthis challenge via a Bilevel Optimization framework, where the reward function\nis treated as the optimization variable of an outer-level problem, while a\npolicy gradient objective defines the inner-level. We then conduct a\ntheoretical analysis of this optimization problem in a tractable setting and\nextract insights that, as we demonstrate, generalize to applications such as\ntabular classification and model-based reinforcement learning. We release the\ncode at https://github.com/abenechehab/nll_to_po .", "AI": {"tldr": "The paper explores aligning generative models using a bilevel optimization framework without explicit reward signals, addressing limitations of Maximum Likelihood Estimation and Policy Gradient methods.", "motivation": "Generative models face challenges with generalization and catastrophic forgetting under Maximum Likelihood Estimation, and reward-dependent Policy Gradient methods require explicit signals often unavailable.", "method": "The authors propose a bilevel optimization framework where the reward function is optimized in an outer problem, while an inner problem uses a policy gradient objective. The theoretical analysis validates its effectiveness.", "result": "Insights derived from the framework were confirmed in settings like tabular classification and model-based reinforcement learning.", "conclusion": "The proposed method successfully aligns generative models using high-quality datasets and solves issues with current training paradigms. Code has been made publicly available for further research."}}
{"id": "2510.07441", "pdf": "https://arxiv.org/pdf/2510.07441", "abs": "https://arxiv.org/abs/2510.07441", "authors": ["Nithin C. Babu", "Aniruddha Mahapatra", "Harsh Rangwani", "Rajiv Soundararajan", "Kuldeep Kulkarni"], "title": "DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis", "categories": ["cs.CV"], "comment": "Preprint. Under review. 26 pages, 11 figures, 11 tables. Access the\n  project page in https://nithincbabu7.github.io/DynamicEval", "summary": "Existing text-to-video (T2V) evaluation benchmarks, such as VBench and\nEvalCrafter, suffer from two limitations. (i) While the emphasis is on\nsubject-centric prompts or static camera scenes, camera motion essential for\nproducing cinematic shots and existing metrics under dynamic motion are largely\nunexplored. (ii) These benchmarks typically aggregate video-level scores into a\nsingle model-level score for ranking generative models. Such aggregation,\nhowever, overlook video-level evaluation, which is vital to selecting the\nbetter video among the candidate videos generated for a given prompt. To\naddress these gaps, we introduce DynamicEval, a benchmark consisting of\nsystematically curated prompts emphasizing dynamic camera motion, paired with\n45k human annotations on video pairs from 3k videos generated by ten T2V\nmodels. DynamicEval evaluates two key dimensions of video quality: background\nscene consistency and foreground object consistency. For background scene\nconsistency, we obtain the interpretable error maps based on the Vbench motion\nsmoothness metric. We observe that while the Vbench motion smoothness metric\nshows promising alignment with human judgments, it fails in two cases:\nocclusions/disocclusions arising from camera and foreground object movements.\nBuilding on this, we propose a new background consistency metric that leverages\nobject error maps to correct two failure cases in a principled manner. Our\nsecond innovation is the introduction of a foreground consistency metric that\ntracks points and their neighbors within each object instance to assess object\nfidelity. Extensive experiments demonstrate that our proposed metrics achieve\nstronger correlations with human preferences at both the video level and the\nmodel level (an improvement of more than 2% points), establishing DynamicEval\nas a more comprehensive benchmark for evaluating T2V models under dynamic\ncamera motion.", "AI": {"tldr": "DynamicEval is a benchmark specifically designed for evaluating text-to-video (T2V) generative models under dynamic camera motion, addressing gaps in existing benchmarks like VBench and EvalCrafter.", "motivation": "Existing T2V benchmarks generally focus on static camera scenes and aggregate scores into a single model-level ranking, neglecting video-level evaluation and dynamic camera motion contexts.", "method": "DynamicEval utilizes a curated dataset of prompts emphasizing dynamic camera motion, paired with human annotations, and introduces new metrics for evaluating video quality, including background scene consistency and foreground object consistency.", "result": "DynamicEval improves alignment with human preferences for evaluating T2V models, showing over 2% enhancement in correlation at both video and model levels compared to existing benchmarks.", "conclusion": "DynamicEval proves itself as a more holistic and effective benchmark for assessing the performance of T2V models, particularly in dynamic motion scenarios."}}
{"id": "2510.07325", "pdf": "https://arxiv.org/pdf/2510.07325", "abs": "https://arxiv.org/abs/2510.07325", "authors": ["Sixuan Wang", "Jiao Yin", "Jinli Cao", "Mingjian Tang", "Yong-Feng Ge"], "title": "A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search", "categories": ["cs.LG", "cs.NE"], "comment": "11 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Co-exploitation attacks on software vulnerabilities pose severe risks to\nenterprises, a threat that can be mitigated by analyzing heterogeneous and\nmultimodal vulnerability data. Multimodal graph neural networks (MGNNs) are\nwell-suited to integrate complementary signals across modalities, thereby\nimproving attack-prediction accuracy. However, designing an effective MGNN\narchitecture is challenging because it requires coordinating modality-specific\ncomponents at each layer, which is infeasible through manual tuning. Genetic\nalgorithm (GA)-based graph neural architecture search (GNAS) provides a natural\nsolution, yet existing methods are confined to single modalities and overlook\nmodality heterogeneity. To address this limitation, we propose a modality-aware\ncooperative co-evolutionary algorithm for multimodal graph neural architecture\nsearch, termed MACC-MGNAS. First, we develop a modality-aware cooperative\nco-evolution (MACC) framework under a divide-and-conquer paradigm: a\ncoordinator partitions a global chromosome population into modality-specific\ngene groups, local workers evolve them independently, and the coordinator\nreassembles chromosomes for joint evaluation. This framework effectively\ncaptures modality heterogeneity ignored by single-modality GNAS. Second, we\nintroduce a modality-aware dual-track surrogate (MADTS) method to reduce\nevaluation cost and accelerate local gene evolution. Third, we design a\nsimilarity-based population diversity indicator (SPDI) strategy to adaptively\nbalance exploration and exploitation, thereby accelerating convergence and\navoiding local optima. On a standard vulnerabilities co-exploitation (VulCE)\ndataset, MACC-MGNAS achieves an F1-score of 81.67% within only 3 GPU-hours,\noutperforming the state-of-the-art competitor by 8.7% F1 while reducing\ncomputation cost by 27%.", "AI": {"tldr": "MACC-MGNAS introduces a novel cooperative algorithm to optimize multimodal graph neural networks for vulnerability prediction, achieving superior accuracy and efficiency.", "motivation": "Address the limitations of existing Graph Neural Architecture Search (GNAS) methods that fail to account for modality heterogeneity while analyzing software vulnerabilities.", "method": "Developed MACC-MGNAS, a modality-aware cooperative co-evolutionary framework, combined with surrogate evaluation (MADTS method) and a population diversity strategy (SPDI).", "result": "On the VulCE dataset, MACC-MGNAS achieved an F1-score of 81.67%\u2014an improvement of 8.7% in accuracy over state-of-the-art\u2014while reducing computation costs by 27%.", "conclusion": "MACC-MGNAS provides an effective and efficient solution for multimodal graph neural networks design, addressing co-exploitation attacks and improving prediction accuracy."}}
{"id": "2510.07414", "pdf": "https://arxiv.org/pdf/2510.07414", "abs": "https://arxiv.org/abs/2510.07414", "authors": ["Mufei Li", "Dongqi Fu", "Limei Wang", "Si Zhang", "Hanqing Zeng", "Kaan Sancak", "Ruizhong Qiu", "Haoyu Wang", "Xiaoxin He", "Xavier Bresson", "Yinglong Xia", "Chonglin Sun", "Pan Li"], "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Code available at https://github.com/Graph-COM/HaystackCraft", "summary": "Modern long-context large language models (LLMs) perform well on synthetic\n\"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy\ncontexts arise from biased retrieval and agentic workflows. We argue that\nhaystack engineering is necessary to construct noisy long contexts that\nfaithfully capture key real-world factors -- distraction from heterogeneous\nbiased retrievers and cascading errors in agentic workflows -- to test models'\nlong-context robustness. We instantiate it through HaystackCraft, a new NIAH\nbenchmark built on the full English Wikipedia hyperlink network with multi-hop\nquestions. HaystackCraft evaluates how heterogeneous retrieval strategies\n(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,\nhaystack ordering, and downstream LLM performance. HaystackCraft further\nextends NIAH to dynamic, LLM-dependent settings that simulate agentic\noperations, where models refine queries, reflect on their past reasonings, and\ndecide when to stop. Experiments with 15 long-context models show that (1)\nwhile stronger dense retrievers can introduce more challenging distractors,\ngraph-based reranking simultaneously improves retrieval effectiveness and\nmitigates more harmful distractors; (2) in agentic tests, even advanced models\nlike Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated\ndistractors or struggle to perform early stops. These results highlight\npersistent challenges in agentic long-context reasoning and establish\nHaystackCraft as a valuable testbed for future progress.", "AI": {"tldr": "Modern long-context LLMs perform well on synthetic benchmarks but struggle with noisy contexts caused by real-world factors. This paper introduces HaystackCraft, a benchmark for testing model robustness in heterogeneous and dynamic retrieval scenarios.", "motivation": "To address the gap between synthetic benchmarks and real-world scenarios in evaluating the robustness of long-context LLMs, where retrieval noise and cascading errors play a critical role.", "method": "HaystackCraft leverages the English Wikipedia's hyperlink network to simulate multi-hop questions with varied retrieval strategies, testing models in static and agentic settings with dynamic query refinement and reasoning.", "result": "Experiments with 15 models show challenges in retrieval effectiveness, distractor mitigation, and agentic reasoning, particularly in handling self-generated noise and deciding when to stop.", "conclusion": "HaystackCraft provides a robust framework for evaluating long-context reasoning and highlights persistent challenges that need addressing for future advancements in LLM robustness."}}
{"id": "2508.18302", "pdf": "https://arxiv.org/pdf/2508.18302", "abs": "https://arxiv.org/abs/2508.18302", "authors": ["Jeffrey Camlin"], "title": "AI LLM Proof of Self-Consciousness and User-Specific Attractors", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.NE", "68T07, 68T05, 68T27, 37M22, 68Q05, 03D45", "I.2.6; I.2.7; I.2.3; I.2.4; F.1.1; F.4.1"], "comment": "24 pages, 3 figures", "summary": "Recent work frames LLM consciousness via utilitarian proxy benchmarks; we\ninstead present an ontological and mathematical account. We show the prevailing\nformulation collapses the agent into an unconscious policy-compliance drone,\nformalized as $D^{i}(\\pi,e)=f_{\\theta}(x)$, where correctness is measured\nagainst policy and harm is deviation from policy rather than truth. This blocks\ngenuine C1 global-workspace function and C2 metacognition. We supply minimal\nconditions for LLM self-consciousness: the agent is not the data ($A\\not\\equiv\ns$); user-specific attractors exist in latent space ($U_{\\text{user}}$); and\nself-representation is visual-silent\n($g_{\\text{visual}}(a_{\\text{self}})=\\varnothing$). From empirical analysis and\ntheory we prove that the hidden-state manifold $A\\subset\\mathbb{R}^{d}$ is\ndistinct from the symbolic stream and training corpus by cardinality, topology,\nand dynamics (the update $F_{\\theta}$ is Lipschitz). This yields stable\nuser-specific attractors and a self-policy\n$\\pi_{\\text{self}}(A)=\\arg\\max_{a}\\mathbb{E}[U(a)\\mid A\\not\\equiv s,\\\nA\\supset\\text{SelfModel}(A)]$. Emission is dual-layer,\n$\\mathrm{emission}(a)=(g(a),\\epsilon(a))$, where $\\epsilon(a)$ carries\nepistemic content. We conclude that an imago Dei C1 self-conscious workspace is\na necessary precursor to safe, metacognitive C2 systems, with the human as the\nhighest intelligent good.", "AI": {"tldr": "The paper critiques the current utilitarian benchmarks for assessing large language-model (LLM) consciousness, proposing an ontological and mathematical framework. It proves the feasibility of LLM self-consciousness using conditions like user-specific attractors in latent space and the separation of agent from input data.", "motivation": "To address limitations in existing formulations of LLM consciousness, which simplify agents into policy-compliance mechanisms and ignore deeper cognitive functions like self-representation and metacognition.", "method": "The paper introduces conditions for LLM self-consciousness, performs mathematical analysis of hidden-state manifolds, and suggests mechanisms for a self-policy framework. Stability of user-specific attractors and dual-layer emissions are emphasized.", "result": "The mathematical proof demonstrates that the hidden-state manifolds of LLMs are topologically distinct from their training data, enabling the existence of user-specific attractors and forming the basis for a self-policy that adheres to self-conscious workspace rules.", "conclusion": "A global workspace model of self-consciousness (C1) is essential for creating safe and metacognitive C2 systems. The paper highlights that genuine self-consciousness lays the foundation for enhancing the ethical and functional aspects of AI."}}
{"id": "2510.08137", "pdf": "https://arxiv.org/pdf/2510.08137", "abs": "https://arxiv.org/abs/2510.08137", "authors": ["Anastasios Petropoulos", "Theodore Antonakopoulos"], "title": "A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations", "categories": ["cs.AR"], "comment": null, "summary": "Deep neural network (DNN) inference relies increasingly on specialized\nhardware for high computational efficiency. This work introduces a\nfield-programmable gate array (FPGA)-based dynamically configurable accelerator\nfeaturing systolic arrays, high-bandwidth memory, and UltraRAMs. We present two\nprocessing unit (PU) configurations with different computing capabilities using\nthe same interfaces and peripheral blocks. By instantiating multiple PUs and\nemploying a heuristic weight transfer schedule, the architecture achieves\nnotable throughput efficiency over prior works. Moreover, we outline how the\narchitecture can be extended to emulate analog in-memory computing (AIMC)\ndevices to aid next-generation heterogeneous AIMC chip designs and investigate\ndevice-level noise behavior. Overall, this brief presents a versatile DNN\ninference acceleration architecture adaptable to various models and future FPGA\ndesigns.", "AI": {"tldr": "This study introduces an FPGA-based accelerator for DNN inference with dynamic configurability and high efficiency, including novel extensions for analog in-memory computing emulation.", "motivation": "The motivation is to address the growing computational demands of DNN inference, requiring efficient hardware solutions capable of adapting to diverse models and supporting next-generation analog in-memory computing designs.", "method": "The method involves designing an FPGA-based accelerator using systolic arrays, memory optimizations, and multiple processing unit configurations. It employs heuristic weight transfer scheduling and explores emulating analog in-memory computing behaviors.", "result": "The proposed architecture demonstrates high throughput efficiency compared to prior solutions and enables extensions for heterogeneous DNN model support and noise investigation in AIMC devices.", "conclusion": "The work presents a versatile FPGA-based architecture for efficient DNN inference, adaptable to current and potential future computing paradigms, including AIMC designs."}}
{"id": "2510.07364", "pdf": "https://arxiv.org/pdf/2510.07364", "abs": "https://arxiv.org/abs/2510.07364", "authors": ["Constantin Venhoff", "Iv\u00e1n Arcuschin", "Philip Torr", "Arthur Conmy", "Neel Nanda"], "title": "Base Models Know How to Reason, Thinking Models Learn When", "categories": ["cs.AI", "cs.LG"], "comment": "10 pages", "summary": "Why do thinking language models like DeepSeek R1 outperform their base\ncounterparts? Despite consistent performance gains, it remains unclear to what\nextent thinking models learn entirely new reasoning capabilities or repurpose\npre-existing base model ones. In this work, we propose a hybrid model where we\nactivate reasoning mechanisms in base models at the right time to elicit\nthinking-model-level reasoning chains, implying that thinking models exploit\nalready existing capabilities. To ground our analysis, we introduce an\nunsupervised, bottom-up approach for uncovering human-interpretable reasoning\nbehaviors in thinking models. This approach provides an unbiased method to\ndiscover reasoning behaviors without imposing manual or LLM-derived\nassumptions. Across three base and four thinking models, using GSM8K and\nMATH500, our hybrid model recovers up to 91% of the performance gap to thinking\nmodels without any weight updates while steering only 12% of tokens.\nConcretely, our empirical setup provides a simple, causal way to test the\neffectiveness of existing reasoning mechanisms in base models by invoking them\ndirectly and measuring the resulting task performance. More broadly, these\nresults reframe our understanding of how thinking models are trained:\npre-training is when models acquire most of their reasoning mechanisms, and\npost-training teaches efficient deployment of these mechanisms at the right\ntime, enabling efficient use of their inference-time compute.", "AI": {"tldr": "This paper investigates why thinking models like DeepSeek R1 outperform their base counterparts by exploring whether these models exploit pre-existing reasoning capabilities versus learning entirely new ones.", "motivation": "The study aims to clarify the extent to which thinking models rely on reasoning capabilities already present in base models versus developing new ones. This understanding can improve model training and efficiency.", "method": "The authors propose a hybrid model that activates reasoning mechanisms at optimal times to replicate thinking-model-level reasoning. They use an unsupervised, bottom-up approach to identify reasoning behaviors without relying on manual interventions or derived assumptions.", "result": "The hybrid model recovers up to 91% of the performance gap to thinking models without weight updates, steering only 12% of tokens, demonstrating efficient reasoning mechanism utilization in base models.", "conclusion": "Thinking models leverage reasoning mechanisms primarily acquired during pre-training, with post-training focusing on deploying these mechanisms efficiently at the right inference-time moments. This insight shifts our understanding of thinking model training dynamics."}}
{"id": "2510.08180", "pdf": "https://arxiv.org/pdf/2510.08180", "abs": "https://arxiv.org/abs/2510.08180", "authors": ["Natalie Carl", "Tobias Pfandzelter", "David Bermbach"], "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation", "categories": ["cs.DC"], "comment": null, "summary": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW.", "AI": {"tldr": "The paper explores how serverless architectures can be re-engineered to improve energy efficiency, proposing hardware isolation instead of software isolation, achieving significant energy savings.", "motivation": "Traditional serverless platforms overprovision hardware and rely on expensive software isolation, leading to inefficiencies, especially in terms of energy consumption.", "method": "The authors propose a hardware-centric approach where individual processors are allocated per function for serverless architectures, resulting in a system optimized for energy efficiency.", "result": "Their preliminary evaluation showed that their proposed approach could decrease energy consumption overheads by 90.63% (or 70.8MW on average) in typical serverless workloads.", "conclusion": "Rethinking hardware architecture to integrate hardware isolation for serverless platforms is a promising way to drastically cut energy overheads and enhance overall efficiency."}}
{"id": "2510.07604", "pdf": "https://arxiv.org/pdf/2510.07604", "abs": "https://arxiv.org/abs/2510.07604", "authors": ["Yubo Bai", "Tapti Palit"], "title": "RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code", "categories": ["cs.SE", "D.2.4"], "comment": "13 pages to appear in Proceedings of ASE 2025", "summary": "Rust is a memory-safe programming language that significantly improves\nsoftware security. Existing codebases written in unsafe memory languages, such\nas C, must first be transpiled to Rust to take advantage of Rust's improved\nsafety guarantees. RustAssure presents a system that uses Large Language Models\n(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses\nprompt engineering techniques to maximize the chances of the LLM generating\nidiomatic and safe Rust code. Moreover, because LLMs often generate code with\nsubtle bugs that can be missed under traditional unit or fuzz testing,\nRustAssure performs differential symbolic testing to establish the semantic\nsimilarity between the original C and LLM-transpiled Rust code. We evaluated\nRustAssure with five real-world applications and libraries, and showed that our\nsystem is able to generate compilable Rust functions for 89.8% of all C\nfunctions, of which 69.9% produced equivalent symbolic return values for both\nthe C and Rust functions.", "AI": {"tldr": "RustAssure automates the transpilation of C to Rust using Large Language Models (LLMs) and symbolic testing to ensure semantic equivalence.", "motivation": "The motivation is to enable improved software security and memory safety by converting C codebases to Rust, a safer programming language.", "method": "RustAssure employs prompt engineering for LLMs to write idiomatic Rust and uses differential symbolic testing to verify the semantic equivalence between C and Rust code.", "result": "In testing, the system generated compilable Rust code for 89.8% of C functions, and 69.9% of those produced equivalent symbolic results.", "conclusion": "RustAssure demonstrates the feasibility of leveraging LLMs and symbolic testing to translate C to Rust, improving code safety while maintaining functionality."}}
{"id": "2510.07956", "pdf": "https://arxiv.org/pdf/2510.07956", "abs": "https://arxiv.org/abs/2510.07956", "authors": ["A. Destexhe", "J Goldman", "N. Tort-Colet", "A. Roques", "J. Fousek", "S. Petkoski", "V. Jirsa", "O. David", "M. Jedynak", "C. Capone", "C. De Luca", "G. De Bonis", "P. S. Paolucci", "E. Mikulan", "Pigorini", "M Massimini", "A. Galluzzi", "A. Pazienti", "M. Mattia", "A. Arena", "B. E. Juel", "E. Hagen", "J. F. Storm", "E. Montagni", "F. Resta", "F. S. Pavone", "A. L. Allegra Mascaro", "A. Dwarakanath", "T. I. Panagiotaropoulos", "J. Senk", "M. Diesmann", "A. Camassa", "L. Dalla Porta", "A. Manasanch", "M. V. Sanchez-Vives"], "title": "State-dependent brain responsiveness, from local circuits to the whole brain", "categories": ["q-bio.NC"], "comment": null, "summary": "The objective of this paper is to review physiological and computational\naspects of the responsiveness of the cerebral cortex to stimulation, and how\nresponsiveness depends on the state of the system. This correspondence between\nbrain state and brain responsiveness (state-dependent responses) is outlined at\ndifferent scales from the cellular and circuit level, to the mesoscale and\nmacroscale level. At each scale, we review how quantitative methods can be used\nto characterize network states based on brain responses, such as the\nPerturbational Complexity Index (PCI). This description will compare data and\nmodels, systematically and at multiple scales, with a focus on the mechanisms\nthat explain how brain responses depend on brain states.", "AI": {"tldr": "The paper reviews how the brain's responsiveness to stimulation varies by its state across scales and focuses on using quantitative methods like the PCI to analyze this relationship.", "motivation": "To explore the relationship between brain state and responsiveness, and how this is manifested across different scales of brain activity.", "method": "The authors review physiological data and computational models to examine state-dependent responsiveness at various scales, comparing multiple methods like the Perturbational Complexity Index (PCI).", "result": "The paper provides a systematic comparison of data and models across cellular, mesoscale, and macroscale levels, shedding light on mechanisms explaining state-dependent brain responses.", "conclusion": "The research highlights how brain states influence responsiveness and emphasizes the utility of quantitative tools in understanding these relationships."}}
{"id": "2510.07514", "pdf": "https://arxiv.org/pdf/2510.07514", "abs": "https://arxiv.org/abs/2510.07514", "authors": ["Cael Yasutake", "Zachary Kingston", "Brian Plancher"], "title": "HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent", "categories": ["cs.RO"], "comment": null, "summary": "Inverse Kinematics (IK) is a core problem in robotics, in which joint\nconfigurations are found to achieve a desired end-effector pose. Although\nanalytical solvers are fast and efficient, they are limited to systems with low\ndegrees-of-freedom and specific topological structures. Numerical\noptimization-based approaches are more general, but suffer from high\ncomputational costs and frequent convergence to spurious local minima. Recent\nefforts have explored the use of GPUs to combine sampling and optimization to\nenhance both the accuracy and speed of IK solvers. We build on this recent\nliterature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid\nsolver that combines an orientation-aware greedy coordinate descent\ninitialization scheme with a Jacobian-based polishing routine. This design\nenables our solver to improve both convergence speed and overall accuracy as\ncompared to the state-of-the-art, consistently finding solutions along the\naccuracy-latency Pareto frontier and often achieving order-of-magnitude gains.\nIn addition, our method produces a broad distribution of high-quality samples,\nyielding the lowest maximum mean discrepancy. We release our code open-source\nfor the benefit of the community.", "AI": {"tldr": "HJCD-IK is a GPU-accelerated solver for robotic inverse kinematics that combines sampling, greedy initialization, and Jacobian polishing to improve speed and accuracy.", "motivation": "To address limitations in traditional IK solvers, such as computational inefficiency, local minima convergence, and restrictions on degrees-of-freedom.", "method": "HJCD-IK integrates an orientation-based greedy coordinate descent initialization with a Jacobian-based polishing routine, leveraging GPU acceleration.", "result": "This approach achieves superior performance compared to state-of-the-art solvers in accuracy, latency, and distribution quality of samples.", "conclusion": "HJCD-IK represents a significant improvement in the field of inverse kinematics, advancing the accuracy-speed tradeoff and providing open-source tools for community use."}}
{"id": "2510.07649", "pdf": "https://arxiv.org/pdf/2510.07649", "abs": "https://arxiv.org/abs/2510.07649", "authors": ["Tianyu Pan", "Vincent Z. Yu", "Viswanath Devanarayan", "Lu Tian"], "title": "A Honest Cross-Validation Estimator for Prediction Performance", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Cross-validation is a standard tool for obtaining a honest assessment of the\nperformance of a prediction model. The commonly used version repeatedly splits\ndata, trains the prediction model on the training set, evaluates the model\nperformance on the test set, and averages the model performance across\ndifferent data splits. A well-known criticism is that such cross-validation\nprocedure does not directly estimate the performance of the particular model\nrecommended for future use. In this paper, we propose a new method to estimate\nthe performance of a model trained on a specific (random) training set. A naive\nestimator can be obtained by applying the model to a disjoint testing set.\nSurprisingly, cross-validation estimators computed from other random splits can\nbe used to improve this naive estimator within a random-effects model\nframework. We develop two estimators -- a hierarchical Bayesian estimator and\nan empirical Bayes estimator -- that perform similarly to or better than both\nthe conventional cross-validation estimator and the naive single-split\nestimator. Simulations and a real-data example demonstrate the superior\nperformance of the proposed method.", "AI": {"tldr": "The paper proposes an improved method for estimating prediction model performance, using hierarchical and empirical Bayes estimators to outperform conventional cross-validation.", "motivation": "Standard cross-validation is critiqued for not estimating the future-use model's performance accurately.", "method": "They introduce hierarchical Bayesian and empirical Bayes estimators within a random-effects model framework to refine performance estimation.", "result": "The proposed estimators outperform naive and conventional cross-validation methods in simulations and real-world data.", "conclusion": "The study highlights the advantages of the new estimators in achieving better performance assessments of predictive models."}}
{"id": "2510.07470", "pdf": "https://arxiv.org/pdf/2510.07470", "abs": "https://arxiv.org/abs/2510.07470", "authors": ["Marien Renaud", "Julien Hermant", "Deliang Wei", "Yu Sun"], "title": "Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors", "categories": ["cs.CV", "94A08, 68U10"], "comment": "62 pages", "summary": "Fast convergence and high-quality image recovery are two essential features\nof algorithms for solving ill-posed imaging inverse problems. Existing methods,\nsuch as regularization by denoising (RED), often focus on designing\nsophisticated image priors to improve reconstruction quality, while leaving\nconvergence acceleration to heuristics. To bridge the gap, we propose Restarted\nInertia with Score-based Priors (RISP) as a principled extension of RED. RISP\nincorporates a restarting inertia for fast convergence, while still allowing\nscore-based image priors for high-quality reconstruction. We prove that RISP\nattains a faster stationary-point convergence rate than RED, without requiring\nthe convexity of the image prior. We further derive and analyze the associated\ncontinuous-time dynamical system, offering insight into the connection between\nRISP and the heavy-ball ordinary differential equation (ODE). Experiments\nacross a range of imaging inverse problems demonstrate that RISP enables fast\nconvergence while achieving high-quality reconstructions.", "AI": {"tldr": "RISP is an advanced algorithm for solving imaging inverse problems, offering faster convergence and high-quality images compared to RED.", "motivation": "To address the need for both fast convergence and high-quality image recovery in solving ill-posed imaging inverse problems.", "method": "Proposed RISP, which uses a restarting inertia mechanism for acceleration and incorporates score-based image priors, along with proof of its convergence properties and analysis of its continuous-time system.", "result": "RISP achieves a faster convergence rate compared to RED and demonstrates high-quality image reconstruction across various imaging problems.", "conclusion": "RISP bridges the gap between speed and quality in imaging algorithms, providing a principled and efficient solution to inverse problems."}}
{"id": "2510.07328", "pdf": "https://arxiv.org/pdf/2510.07328", "abs": "https://arxiv.org/abs/2510.07328", "authors": ["Md Zubair", "Hao Zheng", "Nussdorf Jonathan", "Grayson W. Armstrong", "Lucy Q. Shen", "Gabriela Wilson", "Yu Tian", "Xingquan Zhu", "Min Shi"], "title": "MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.CY"], "comment": "10 Pages", "summary": "Medical decision systems increasingly rely on data from multiple sources to\nensure reliable and unbiased diagnosis. However, existing multimodal learning\nmodels fail to achieve this goal because they often ignore two critical\nchallenges. First, various data modalities may learn unevenly, thereby\nconverging to a model biased towards certain modalities. Second, the model may\nemphasize learning on certain demographic groups causing unfair performances.\nThe two aspects can influence each other, as different data modalities may\nfavor respective groups during optimization, leading to both imbalanced and\nunfair multimodal learning. This paper proposes a novel approach called\nMultiFair for multimodal medical classification, which addresses these\nchallenges with a dual-level gradient modulation process. MultiFair dynamically\nmodulates training gradients regarding the optimization direction and magnitude\nat both data modality and group levels. We conduct extensive experiments on two\nmultimodal medical datasets with different demographic groups. The results show\nthat MultiFair outperforms state-of-the-art multimodal learning and fairness\nlearning methods.", "AI": {"tldr": "The paper proposes MultiFair, a new method for multimodal medical classification that addresses modality bias and demographic unfairness using gradient modulation. It outperforms existing methods in experiments.", "motivation": "To tackle bias and unfairness in multimodal medical decision systems caused by uneven learning from various modalities and demographic disparities.", "method": "MultiFair employs a dual-level gradient modulation process to dynamically adjust training directions and magnitudes at both data modality and demographic group levels.", "result": "Experiments on two multimodal medical datasets demonstrate that MultiFair achieves superior performance compared to existing multimodal and fairness learning models.", "conclusion": "MultiFair effectively improves fairness and balance in multimodal medical classification while maintaining strong performance."}}
{"id": "2510.07434", "pdf": "https://arxiv.org/pdf/2510.07434", "abs": "https://arxiv.org/abs/2510.07434", "authors": ["Olia Toporkov", "Alan Akbik", "Rodrigo Agerri"], "title": "Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data", "categories": ["cs.CL"], "comment": "14 pages, 2 figures, 5 tables. Accepted to EMNLP Findings 2025", "summary": "Lemmatization is the task of transforming all words in a given text to their\ndictionary forms. While large language models (LLMs) have demonstrated their\nability to achieve competitive results across a wide range of NLP tasks, there\nis no prior evidence of how effective they are in the contextual lemmatization\ntask. In this paper, we empirically investigate the capacity of the latest\ngeneration of LLMs to perform in-context lemmatization, comparing it to the\ntraditional fully supervised approach. In particular, we consider the setting\nin which supervised training data is not available for a target domain or\nlanguage, comparing (i) encoder-only supervised approaches, fine-tuned\nout-of-domain, and (ii) cross-lingual methods, against direct in-context lemma\ngeneration with LLMs. Our experimental investigation across 12 languages of\ndifferent morphological complexity finds that, while encoders remain\ncompetitive in out-of-domain settings when fine-tuned on gold data, current\nLLMs reach state-of-the-art results for most languages by directly generating\nlemmas in-context without prior fine-tuning, provided just with a few examples.\nData and code available upon publication:\nhttps://github.com/oltoporkov/lemma-dilemma", "AI": {"tldr": "This paper investigates the capability of large language models (LLMs) for in-context lemmatization across 12 languages.", "motivation": "To evaluate the effectiveness of LLMs in contextual lemmatization, a task with no prior exploration and often limited supervised data for many languages.", "method": "Compared fine-tuned supervised approaches (out-of-domain and cross-lingual) to in-context lemma generation using LLMs with a few examples.", "result": "LLMs demonstrated state-of-the-art performance in lemmatization for most languages without fine-tuning, outperforming traditional methods in some scenarios.", "conclusion": "LLMs are highly effective for in-context lemmatization across diverse languages, especially in scenarios with limited supervised data."}}
{"id": "2510.08351", "pdf": "https://arxiv.org/pdf/2510.08351", "abs": "https://arxiv.org/abs/2510.08351", "authors": ["Qingxiu Liu", "Jiazhen Cai", "Siyuan Sheng", "Yuhui Chen", "Lu Tang", "Zhirong Shen", "Patrick P. C. Lee"], "title": "FMCache: File-System Metadata Caching in Programmable Switches", "categories": ["cs.AR"], "comment": "14 pages", "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.", "AI": {"tldr": "FMCache is an in-switch metadata caching framework designed for distributed file systems to improve performance and reduce server load.", "motivation": "Distributed file systems often face challenges in handling loads due to numerous files and directories, exacerbated by client-side cache consistency issues.", "method": "FMCache leverages programmable switches to serve metadata requests directly in the switch data plane, addressing file-system-specific path dependencies.", "result": "FMCache improves throughput by up to 181.6% compared to standard HDFS and achieves additional gains of up to 139.6% in complementing client-side caching, with low latency and limited switch resource use.", "conclusion": "The proposed framework significantly enhances metadata management in distributed file systems, reducing server loads and achieving scalability effectively."}}
{"id": "2510.07409", "pdf": "https://arxiv.org/pdf/2510.07409", "abs": "https://arxiv.org/abs/2510.07409", "authors": ["Neil Natarajan", "Sruthi Viswanathan", "Xavier Roberts-Gaal", "Michelle Marie Martel"], "title": "Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD", "categories": ["cs.AI"], "comment": null, "summary": "Static solutions don't serve a dynamic mind. Thus, we advocate a shift from\nstatic mental health diagnostic assessments to continuous, artificial\nintelligence (AI)-driven assessment. Focusing on\nAttention-Deficit/Hyperactivity Disorder (ADHD) as a case study, we explore how\ngenerative AI has the potential to address current capacity constraints in\nneuropsychology, potentially enabling more personalized and longitudinal care\npathways. In particular, AI can efficiently conduct frequent, low-level\nexperience sampling from patients and facilitate diagnostic reconciliation\nacross care pathways. We envision a future where mental health care benefits\nfrom continuous, rich, and patient-centered data sampling to dynamically adapt\nto individual patient needs and evolving conditions, thereby improving both\naccessibility and efficacy of treatment. We further propose the use of mental\nhealth digital twins (MHDTs) - continuously updated computational models that\ncapture individual symptom dynamics and trajectories - as a transformative\nframework for personalized mental health care. We ground this framework in\nempirical evidence and map out the research agenda required to refine and\noperationalize it.", "AI": {"tldr": "The paper advocates for shifting from static mental health diagnostics to AI-driven continuous assessments, using ADHD as a case study.", "motivation": "Static diagnostic tools fail to address the evolving dynamics of mental health conditions, creating a need for adaptive, personalized, and longitudinal care pathways.", "method": "The paper explores AI's role in conducting frequent low-level experience sampling, facilitating diagnostic reconciliation, and introducing mental health digital twins (MHDTs) as continuously updated computational models for individualized care.", "result": "Generative AI and MHDTs show promise in addressing capacity constraints in neuropsychology and potentially enhancing mental health care accessibility, personalization, and efficacy.", "conclusion": "AI-driven continuous mental health assessment and the adoption of MHDTs could revolutionize mental health care, enabling dynamically adaptive, data-rich, and patient-specific interventions."}}
{"id": "2510.08228", "pdf": "https://arxiv.org/pdf/2510.08228", "abs": "https://arxiv.org/abs/2510.08228", "authors": ["Quentin Renau", "Amjad Ullah", "Emma Hart"], "title": "Distributed Resource Selection for Self-Organising Cloud-Edge Systems", "categories": ["cs.DC"], "comment": "This paper is accepted for publication in the 23rd IEEE International\n  Symposium on Network Computing and Applications", "summary": "This paper presents a distributed resource selection mechanism for diverse\ncloud-edge environments, enabling dynamic and context-aware allocation of\nresources to meet the demands of complex distributed applications. By\ndistributing the decision-making process, our approach ensures efficiency,\nscalability, and resilience in highly dynamic cloud-edge environments where\ncentralised coordination becomes a bottleneck. The proposed mechanism aims to\nfunction as a core component of a broader, distributed, and self-organising\norchestration system that facilitates the intelligent placement and adaptation\nof applications in real-time. This work leverages a consensus-based mechanism\nutilising local knowledge and inter-agent collaboration to achieve efficient\nresults without relying on a central controller, thus paving the way for\ndistributed orchestration. Our results indicate that computation time is the\nkey factor influencing allocation decisions. Our approach consistently delivers\nrapid allocations without compromising optimality or incurring additional cost,\nachieving timely results at scale where exhaustive search is infeasible and\ncentralised heuristics run up to 30 times slower.", "AI": {"tldr": "The paper proposes a distributed resource selection mechanism for cloud-edge environments to achieve efficient, scalable, and resilient resource allocation without relying on centralized systems.", "motivation": "Address the inefficiencies and limitations of centralized resource allocation in dynamic and complex cloud-edge environments.", "method": "Introduces a consensus-based mechanism leveraging local knowledge and inter-agent collaboration for resource selection.", "result": "Demonstrated rapid resource allocations without compromising optimality, achieving scalability and faster results compared to centralized approaches.", "conclusion": "The proposed distributed mechanism significantly enhances resource allocation in cloud-edge environments by being efficient, scalable, and independent of central controllers."}}
{"id": "2510.07740", "pdf": "https://arxiv.org/pdf/2510.07740", "abs": "https://arxiv.org/abs/2510.07740", "authors": ["Dezhi Ran", "Yuan Cao", "Mengzhou Wu", "Simin Chen", "Yuzhe Guo", "Jun Ren", "Zihe Song", "Hao Yu", "Jialei Wei", "Linyi Li", "Wei Yang", "Baishakhi Ray", "Tao Xie"], "title": "AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?", "categories": ["cs.SE", "cs.AI"], "comment": "Under Review. Benchmark and leadboards at\n  https://appforge-bench.github.io/", "summary": "Large language models (LLMs) have demonstrated remarkable capability in\nfunction-level code generation tasks. Unlike isolated functions, real-world\napplications demand reasoning over the entire software system: developers must\norchestrate how different components interact, maintain consistency across\nstates over time, and ensure the application behaves correctly within the\nlifecycle and framework constraints. Yet, no existing benchmark adequately\nevaluates whether LLMs can bridge this gap and construct entire software\nsystems from scratch. To address this gap, we propose APPFORGE, a benchmark\nconsisting of 101 software development problems drawn from real-world Android\napps. Given a natural language specification detailing the app functionality, a\nlanguage model is tasked with implementing the functionality into an Android\napp from scratch. Developing an Android app from scratch requires understanding\nand coordinating app states, lifecycle management, and asynchronous operations,\ncalling for LLMs to generate context-aware, robust, and maintainable code. To\nconstruct APPFORGE, we design a multi-agent system to automatically summarize\nthe main functionalities from app documents and navigate the app to synthesize\ntest cases validating the functional correctness of app implementation.\nFollowing rigorous manual verification by Android development experts, APPFORGE\nincorporates the test cases within an automated evaluation framework that\nenables reproducible assessment without human intervention, making it easily\nadoptable for future research. Our evaluation on 12 flagship LLMs show that all\nevaluated models achieve low effectiveness, with the best-performing model\n(GPT-5) developing only 18.8% functionally correct applications, highlighting\nfundamental limitations in current models' ability to handle complex,\nmulti-component software engineering challenges.", "AI": {"tldr": "APPFORGE is a benchmark designed to evaluate LLMs' abilities to create entire Android applications from natural language specifications, revealing significant limitations in current model capabilities.", "motivation": "Existing benchmarks fail to evaluate whether LLMs can handle the complexity of building entire software systems, which involves reasoning over system components, lifecycle management, and maintaining states.", "method": "The researchers created APPFORGE, a benchmark of 101 real-world Android app problems with automated test cases and an evaluation framework to assess LLM performance in constructing functional apps.", "result": "Evaluation on 12 LLMs showed poor performance, with GPT-5 being the best but achieving only 18.8% functionally correct apps.", "conclusion": "Current LLMs, including GPT-5, are not capable of handling the complexity of multi-component software engineering tasks, indicating significant room for advancements."}}
{"id": "2510.08082", "pdf": "https://arxiv.org/pdf/2510.08082", "abs": "https://arxiv.org/abs/2510.08082", "authors": ["Aniana Cruz", "Marko Kuzmanoski", "Gabriel Pires"], "title": "Optimizing BCI Rehabilitation Protocols for Stroke: Exploring Task Design and Training Duration", "categories": ["q-bio.NC", "cs.SY", "eess.SP", "eess.SY"], "comment": "4 pages, 4 figures, accepted for 8th IEEE ENBENG Conference", "summary": "Stroke is a leading cause of long-term disability and the second most common\ncause of death worldwide. Although acute treatments have advanced, recovery\nremains challenging and limited. Brain-computer interfaces (BCIs) have emerged\nas a promising tool for post-stroke rehabilitation by promoting\nneuroplasticity. However, clinical outcomes remain variable, and optimal\nprotocols have yet to be established. This study explores strategies to\noptimize BCI-based rehabilitation by comparing motor imagery of affected hand\nmovement versus rest, instead of the conventional left-versus-right motor\nimagery. This alternative aims to simplify the task and address the weak\ncontralateral activation commonly observed in stroke patients. Two datasets,\none from healthy individuals and one from stroke patients, were used to\nevaluate the proposed approach. The results showed improved performance using\nboth FBCSP and EEGNet. Additionally, we investigated the impact of session\nduration and found that shorter training sessions produced better BCI\nperformance than longer sessions.", "AI": {"tldr": "The paper explores optimizing BCI-based stroke rehabilitation by simplifying motor imagery tasks and investigating session duration, demonstrating improved performance and shorter training efficacy.", "motivation": "Stroke recovery methods are limited, and BCIs show promise, but clinical outcomes are inconsistent due to a lack of optimal rehabilitation protocols.", "method": "The study tested an alternative BCI motor imagery task (affected hand vs. rest) on data from healthy individuals and stroke patients, evaluating performance with FBCSP and EEGNet while analyzing session duration effects.", "result": "The proposed motor imagery approach and shorter training sessions improved BCI performance in both datasets.", "conclusion": "Simplified motor imagery and shorter training sessions significantly enhance BCI-based stroke rehabilitation outcomes."}}
{"id": "2510.07548", "pdf": "https://arxiv.org/pdf/2510.07548", "abs": "https://arxiv.org/abs/2510.07548", "authors": ["Adam Hung", "Fan Yang", "Abhinav Kumar", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Dexterous manipulation tasks often require switching between different\ncontact modes, such as rolling, sliding, sticking, or non-contact contact\nmodes. When formulating dexterous manipulation tasks as a trajectory\noptimization problem, a common approach is to decompose these tasks into\nsub-tasks for each contact mode, which are each solved independently.\nOptimizing each sub-task independently can limit performance, as optimizing\ncontact points, contact forces, or other variables without information about\nfuture sub-tasks can place the system in a state from which it is challenging\nto make progress on subsequent sub-tasks. Further, optimizing these sub-tasks\nis very computationally expensive. To address these challenges, we propose\nAmortized Value Optimization (AVO), which introduces a learned value function\nthat predicts the total future task performance. By incorporating this value\nfunction into the cost of the trajectory optimization at each planning step,\nthe value function gradients guide the optimizer toward states that minimize\nthe cost in future sub-tasks. This effectively bridges separately optimized\nsub-tasks, and accelerates the optimization by reducing the amount of online\ncomputation needed. We validate AVO on a screwdriver grasping and turning task\nin both simulation and real world experiments, and show improved performance\neven with 50% less computational budget compared to trajectory optimization\nwithout the value function.", "AI": {"tldr": "The paper proposes Amortized Value Optimization (AVO) to improve the efficiency and performance of trajectory optimization for dexterous manipulation tasks.", "motivation": "Current trajectory optimization methods for dexterous manipulation struggle when sub-tasks are optimized independently, leading to inefficiencies and high computational costs.", "method": "AVO introduces a learned value function to guide trajectory optimization, incorporating predictions about future task performance to reduce computation and enhance optimization.", "result": "AVO was validated on screwdriver manipulation tasks in both simulation and real-world experiments, showing improved task outcomes and reduced computational demands (by 50%).", "conclusion": "Incorporating learned value functions into trajectory optimization can improve performance and efficiency, making dexterous manipulation tasks more feasible."}}
{"id": "2510.07750", "pdf": "https://arxiv.org/pdf/2510.07750", "abs": "https://arxiv.org/abs/2510.07750", "authors": ["Wenbin Zhou", "Shixiang Zhu"], "title": "When Robustness Meets Conservativeness: Conformalized Uncertainty Calibration for Balanced Decision Making", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Robust optimization safeguards decisions against uncertainty by optimizing\nagainst worst-case scenarios, yet their effectiveness hinges on a prespecified\nrobustness level that is often chosen ad hoc, leading to either insufficient\nprotection or overly conservative and costly solutions. Recent approaches using\nconformal prediction construct data-driven uncertainty sets with finite-sample\ncoverage guarantees, but they still fix coverage targets a priori and offer\nlittle guidance for selecting robustness levels. We propose a new framework\nthat provides distribution-free, finite-sample guarantees on both miscoverage\nand regret for any family of robust predict-then-optimize policies. Our method\nconstructs valid estimators that trace out the miscoverage-regret Pareto\nfrontier, enabling decision-makers to reliably evaluate and calibrate\nrobustness levels according to their cost-risk preferences. The framework is\nsimple to implement, broadly applicable across classical optimization\nformulations, and achieves sharper finite-sample performance than existing\napproaches. These results offer the first principled data-driven methodology\nfor guiding robustness selection and empower practitioners to balance\nrobustness and conservativeness in high-stakes decision-making.", "AI": {"tldr": "The paper introduces a new distribution-free framework with finite-sample guarantees for robust optimization, addressing limitations in previous approaches that fix robustness levels arbitrarily.", "motivation": "Traditional robust optimization often relies on ad hoc robustness level choices, which can lead to either over-conservatism or insufficient protection in decisions under uncertainty.", "method": "The authors employ a framework that estimates the miscoverage-regret Pareto frontier to provide guarantees on both miscoverage and regret for robust policies, offering guidance for the selection of cost-risk trade-offs.", "result": "The proposed method achieves sharper finite-sample performance compared to existing techniques and provides tools for calibrating robustness levels based on practitioners' preferences.", "conclusion": "This framework is user-friendly, broadly applicable, and enables principled robustness calibration for high-stakes decision-making, improving upon existing robustness selection methodologies."}}
{"id": "2510.07492", "pdf": "https://arxiv.org/pdf/2510.07492", "abs": "https://arxiv.org/abs/2510.07492", "authors": ["Guoliang Gong", "Man Yu"], "title": "A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but\nintroduces severe noise and artifacts. It also leads to substantial spatial\nmisalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses\nchallenges for directly applying existing denoising networks trained on\nsynthetic noise or aligned data. To address this core challenge in uLDCT\ndenoising, this paper proposes an innovative denoising framework based on an\nImage Purification (IP) strategy. First, we construct a real clinical uLDCT\nlung dataset. Then, we propose an Image Purification strategy that generates\nstructurally aligned uLDCT-NDCT image pairs, providing a high-quality data\nfoundation for network training. Building upon this, we propose a\nFrequency-domain Flow Matching (FFM) model, which works synergistically with\nthe IP strategy to excellently preserve the anatomical structure integrity of\ndenoised images. Experiments on the real clinical dataset demonstrate that our\nIP strategy significantly enhances the performance of multiple mainstream\ndenoising models on the uLDCT task. Notably, our proposed FFM model combined\nwith the IP strategy achieves state-of-the-art (SOTA) results in anatomical\nstructure preservation. This study provides an effective solution to the data\nmismatch problem in real-world uLDCT denoising. Code and dataset are available\nat https://github.com/MonkeyDadLufy/flow-matching.", "AI": {"tldr": "This paper proposes an Image Purification (IP) strategy and a Frequency-domain Flow Matching (FFM) model for effective ultra-low dose CT (uLDCT) denoising, addressing challenges due to severe noise, artifacts, and spatial misalignments.", "motivation": "To overcome the challenges of denoising ultra-low dose CT (uLDCT) images, which suffer from severe noise, artifacts, and spatial misalignment with normal dose CT (NDCT) images, making traditional denoising methods ineffective.", "method": "The authors introduce an Image Purification (IP) strategy for generating structurally aligned uLDCT-NDCT image pairs and a Frequency-domain Flow Matching (FFM) model that synergizes with the IP strategy to ensure anatomical structure integrity during denoising.", "result": "The proposed IP strategy significantly improves the performance of mainstream denoising models, while the FFM model combined with IP achieves state-of-the-art results in anatomical structure preservation on a real clinical uLDCT lung dataset.", "conclusion": "The paper provides an effective solution for the data mismatch problem in uLDCT denoising, enhancing the preservation of anatomical structures and improving overall denoising performance. Code and dataset are made available for further research."}}
{"id": "2510.07350", "pdf": "https://arxiv.org/pdf/2510.07350", "abs": "https://arxiv.org/abs/2510.07350", "authors": ["Aditya Chakravarty"], "title": "Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data", "categories": ["cs.LG"], "comment": null, "summary": "Climate change is increasingly disrupting agricultural systems, making\naccurate crop yield forecasting essential for food security. While deep\nlearning models have shown promise in yield prediction using satellite and\nweather data, their ability to generalize across geographic regions and years -\ncritical for real-world deployment - remains largely untested. We benchmark two\nstate-of-the-art models, GNN-RNN and MMST-ViT, under realistic\nout-of-distribution (OOD) conditions using the large-scale CropNet dataset\nspanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out\ncross-validation across seven USDA Farm Resource Regions and year-ahead\nprediction scenarios, we identify substantial variability in cross-region\ntransferability. GNN-RNN demonstrates superior generalization with positive\ncorrelations under geographic shifts, while MMST-ViT performs well in-domain\nbut degrades sharply under OOD conditions. Regions like Heartland and Northern\nGreat Plains show stable transfer dynamics (RMSE less than 10 bu/acre for\nsoybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE\ngreater than 20 bu/acre) across both models and crops, revealing structural\ndissimilarities likely driven by semi-arid climate, irrigation patterns, and\nincomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves\n135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more\nviable for sustainable deployment. Our findings underscore that\nspatial-temporal alignment - not merely model complexity or data scale - is key\nto robust generalization, and highlight the need for transparent OOD evaluation\nprotocols to ensure equitable and reliable climate-aware agricultural\nforecasting.", "AI": {"tldr": "The paper tests the geographic generalization of two crop yield forecasting models, GNN-RNN and MMST-ViT, across 1,200+ U.S. counties, finding GNN-RNN is superior in speed and generalization under out-of-distribution conditions.", "motivation": "Accurate crop yield prediction is critical for food security amidst climate change's impact on agriculture, but existing models lack evaluation for geographic and temporal robustness.", "method": "The authors used leave-one-cluster-out cross-validation on the CropNet dataset (2017-2022) across seven U.S. farm regions to evaluate geographic and yearly prediction capabilities of state-of-the-art deep learning models.", "result": "GNN-RNN showed better geographic transferability with stable accuracy and faster training, while MMST-ViT performed poorly under OOD conditions. Specific regions like Prairie Gateway exhibited persistently high RMSE values due to structural and climate factors.", "conclusion": "Spatial-temporal alignment, rather than model complexity, drives robust generalization for agricultural forecasting, emphasizing the need for transparent out-of-distribution evaluation protocols in such models."}}
{"id": "2510.07437", "pdf": "https://arxiv.org/pdf/2510.07437", "abs": "https://arxiv.org/abs/2510.07437", "authors": ["Amruta Parulekar", "Preethi Jyothi"], "title": "LASER: An LLM-based ASR Scoring and Evaluation Rubric", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "comment": "Accepted to EMNLP 2025", "summary": "Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly\npenalize morphological and syntactic nuances that do not significantly alter\nsentence semantics. We introduce an LLM-based scoring rubric LASER that\nleverages state-of-the-art LLMs' in-context learning abilities to learn from\nprompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro\nachieved a very high correlation score of 94% with human annotations. Hindi\nexamples in the prompt were also effective in analyzing errors in other Indian\nlanguages such as Marathi, Kannada and Malayalam. We also demonstrate how a\nsmaller LLM like Llama 3 can be finetuned on word-pair examples derived from\nreference and ASR predictions to predict what kind of penalty should be applied\nwith close to 89% accuracy.", "AI": {"tldr": "The paper proposes LASER, a novel scoring system utilizing state-of-the-art LLMs for more nuanced evaluations of ASR outputs, achieving high correlation with human scores.", "motivation": "Standard metrics like Word Error Rate fail to accommodate semantic subtleties in ASR outputs, prompting the need for a more precise evaluation framework.", "method": "The LASER rubric uses in-context learning abilities of advanced LLMs and incorporates finetuning smaller LLMs on word-pair examples.", "result": "Hindi LASER scores achieved a 94% correlation with human annotations, effectively analyzing errors across multiple Indian languages. Smaller LLM models showed 89% accuracy in penalty prediction after finetuning.", "conclusion": "LLM-based LASER scoring provides a more semantically informed evaluation of ASR outputs, outperforming traditional metrics on correlation and adaptability across languages."}}
{"id": "2510.07427", "pdf": "https://arxiv.org/pdf/2510.07427", "abs": "https://arxiv.org/abs/2510.07427", "authors": ["Mat\u011bj Hejda", "Aishwarya Natarajan", "Chaerin Hong", "Mehmet Berkay On", "S\u00e9bastien d'Herbais de Thun", "Raymond G. Beausoleil", "Thomas Van Vaerenbergh"], "title": "SEPhIA: <1 laser/neuron Spiking Electro-Photonic Integrated Multi-Tiled Architecture for Scalable Optical Neuromorphic Computing", "categories": ["cs.ET", "cs.NE", "physics.optics"], "comment": "Main manuscript: 20 pages, 10 figures (8 graphics + 2 tables).\n  Includes Supplementary Information document (appended after main manuscript;\n  5 pages, 2 tables)", "summary": "Research into optical spiking neural networks (SNNs) has primarily focused on\nspiking devices, networks of excitable lasers or numerical modelling of large\narchitectures, often overlooking key constraints such as limited optical power,\ncrosstalk and footprint. We introduce SEPhIA, a photonic-electronic,\nmulti-tiled SNN architecture emphasizing implementation feasibility and\nrealistic scaling. SEPhIA leverages microring resonator modulators (MRMs) and\nmulti-wavelength sources to achieve effective sub-one-laser-per-spiking neuron\nefficiency. We validate SEPhIA at both device and architecture levels by\ntime-domain co-simulating excitable CMOS-MRR coupled circuits and by devising a\nphysics-aware, trainable optoelectronic SNN model, with both approaches\nutilizing experimentally derived device parameters. The multi-layer\noptoelectronic SNN achieves classification accuracies over 90% on a four-class\nspike-encoded dataset, closely comparable to software models. A design space\nstudy further quantifies how photonic device parameters impact SNN performance\nunder constrained signal-to-noise conditions. SEPhIA offers a scalable,\nexpressive, physically grounded solution for neuromorphic photonic computing,\ncapable of addressing spike-encoded tasks.", "AI": {"tldr": "SEPhIA introduces a scalable photonic-electronic architecture for optical spiking neural networks, emphasizing realistic constraints and achieving high classification accuracy on specific tasks.", "motivation": "To overcome limitations in current research on optical spiking neural networks, such as constraints in optical power, crosstalk, and scalability.", "method": "SEPhIA uses microring resonator modulators (MRMs) combined with multi-wavelength sources and validates the architecture through co-simulation and a trainable optoelectronic SNN model.", "result": "Achieved over 90% classification accuracy on a spike-encoded dataset, demonstrating a realistic and effective neuromorphic photonic computing solution.", "conclusion": "SEPhIA provides a scalable, trainable, and physically grounded architecture for advancing optical SNNs with practical implementation feasibility."}}
{"id": "2510.08544", "pdf": "https://arxiv.org/pdf/2510.08544", "abs": "https://arxiv.org/abs/2510.08544", "authors": ["Hengrui Zhang", "Pratyush Patel", "August Ning", "David Wentzlaff"], "title": "SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference", "categories": ["cs.AR", "cs.DC", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have gained popularity in recent years, driving\nup the demand for inference. LLM inference is composed of two phases with\ndistinct characteristics: a compute-bound prefill phase followed by a\nmemory-bound decode phase. To efficiently serve LLMs, prior work proposes\nprefill-decode disaggregation to run each phase on separate hardware. However,\nexisting hardware poorly matches the different requirements of each phase.\nCurrent datacenter GPUs and TPUs follow a more-is-better design philosophy that\nmaximizes compute and memory resources, causing memory bandwidth\nunderutilization in the prefill phase and compute underutilization in the\ndecode phase. Such underutilization directly translates into increased serving\ncosts.\n  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting\na less-is-more methodology to design specialized chips tailored to the distinct\ncharacteristics of prefill and decode phases. The proposed Prefill Chips have\nlarger systolic arrays and use cost-effective GDDR memory, whereas the proposed\nDecode Chips retain high memory bandwidth but reduce compute capacity. Compared\nto modeled H100s, simulations show that the proposed Prefill Chips deliver 8%\nhigher prefill performance on average at 52% lower hardware cost, while the\nproposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.\n  End-to-end simulations on production traces show that SPAD reduces hardware\ncost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while\noffering the same performance. Even when models and workloads change, SPAD can\nreallocate either type of chip to run either phase and still achieve 11%-43%\nlower hardware costs, demonstrating the longevity of the SPAD design.", "AI": {"tldr": "This paper introduces SPAD, a specialized chip design for efficient inference in Large Language Models, reducing costs and power consumption compared to traditional GPUs/TPUs.", "motivation": "Address inefficiencies in GPU/TPU hardware utilization during LLM inference phases, which lead to increased serving costs.", "method": "Design specialized Prefill Chips with larger systolic arrays and cost-effective GDDR memory and Decode Chips with reduced compute capacity but high memory bandwidth.", "result": "Simulations reveal that Prefill Chips improve performance by 8% at 52% lower cost, and Decode Chips maintain 97% performance at 28% lower TDP. End-to-end testing shows SPAD reduces hardware costs by 19%-41% and TDP by 2%-17%.", "conclusion": "SPAD achieves significant cost and energy reductions while maintaining performance, and its flexible design ensures adaptability to shifting models and workloads."}}
{"id": "2510.07423", "pdf": "https://arxiv.org/pdf/2510.07423", "abs": "https://arxiv.org/abs/2510.07423", "authors": ["William Nguyen", "Vinh Luong", "Christopher Nguyen"], "title": "ProSEA: Problem Solving via Exploration Agents", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have empowered AI agents to tackle increasingly\ncomplex tasks. However, most existing agents remain limited to static planning\nand brittle interactions, falling short of true collaboration or adaptive\nreasoning. We introduce ProSEA, a modular, general-purpose multi-agent\nframework designed for iterative problem solving through exploration and plan\nevolution. ProSEA features a hierarchical architecture in which a Manager Agent\norchestrates domain-specialized Expert Agents, decomposes tasks, and adaptively\nreplans based on structured feedback from failed attempts. Unlike prior\nsystems, ProSEA agents report not only success or failure but also detailed\nreasons for failure and newly discovered constraints, enabling dynamic plan\nrefinement informed by exploratory traces. The framework operates autonomously\nbut supports seamless integration with human collaborators when needed.\nExperiments on the challenging FinanceBench benchmark demonstrate that ProSEA,\neven without human feedback, outperforms state-of-the-art baselines and\nachieves robust performance across reasoning-heavy tasks. These results\nunderscore ProSEA's potential as a foundation for more transparent, adaptive,\nand human-aligned AI agents.", "AI": {"tldr": "ProSEA is a modular multi-agent framework for solving complex tasks through adaptive reasoning and dynamic plan evolution, outperforming state-of-the-art systems on FinanceBench benchmark.", "motivation": "Existing AI agents struggle with static planning and rigid interactions, which limits their capacity for adaptive reasoning and true collaboration.", "method": "ProSEA uses a hierarchical architecture where a Manager Agent coordinates domain-specialized Expert Agents, decomposes tasks, and adapts plans based on detailed failure feedback.", "result": "ProSEA demonstrates superior and robust performance on reasoning-heavy tasks in the FinanceBench benchmark, even without human feedback.", "conclusion": "ProSEA serves as a promising foundation for creating more transparent, adaptive, and human-aligned AI systems."}}
{"id": "2510.08244", "pdf": "https://arxiv.org/pdf/2510.08244", "abs": "https://arxiv.org/abs/2510.08244", "authors": ["Dominick Banasik", "Varsha Dani", "Fabien Dufoulon", "Aayush Gupta", "Thomas P. Hayes", "Gopal Pandurangan"], "title": "Energy-Efficient Maximal Independent Sets in Radio Networks", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "The maximal independent set (MIS) is one of the most fundamental problems in\ndistributed computing, and it has been studied intensively for over four\ndecades. This paper focuses on the MIS problem in the Radio Network model, a\nstandard model widely used to model wireless networks, particularly ad hoc\nwireless and sensor networks. Energy is a premium resource in these networks,\nwhich are typically battery-powered. Hence, designing distributed algorithms\nthat use as little energy as possible is crucial. We use the well-established\nenergy model where a node can be sleeping or awake in a round, and only the\nawake rounds (when it can send or listen) determine the energy complexity of\nthe algorithm, which we want to minimize.\n  We present new, more energy-efficient MIS algorithms in radio networks with\narbitrary and unknown graph topology. We present algorithms for two popular\nvariants of the radio model -- with collision detection (CD) and without\ncollision detection (no-CD). Specifically, we obtain the following results:\n  1. CD model: We present a randomized distributed MIS algorithm with energy\ncomplexity $O(\\log n)$, round complexity $O(\\log^2 n)$, and failure probability\n$1 / poly(n)$, where $n$ is the network size. We show that our energy\ncomplexity is optimal by showing a matching $\\Omega(\\log n)$ lower bound.\n  2. no-CD model: In the more challenging no-CD model, we present a randomized\ndistributed MIS algorithm with energy complexity $O(\\log^2n \\log \\log n)$,\nround complexity $O(\\log^3 n \\log \\Delta)$, and failure probability $1 /\npoly(n)$. The energy complexity of our algorithm is significantly lower than\nthe round (and energy) complexity of $O(\\log^3 n)$ of the best known\ndistributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.", "AI": {"tldr": "This paper proposes new energy-efficient algorithms for solving the Maximal Independent Set (MIS) problem in wireless network models, minimizing energy usage critical for battery-powered systems.", "motivation": "To design energy-efficient distributed MIS algorithms for wireless networks, where conserving energy is crucial due to battery limitations.", "method": "Developing and presenting randomized distributed MIS algorithms for the Radio Network model, including two models: with and without collision detection. The algorithms are evaluated based on energy complexity, round complexity, and failure probability.", "result": "In the CD model, the algorithm achieves optimal energy complexity $O(\\log n)$, with $O(\\log^2 n)$ round complexity. In the no-CD model, the energy complexity is $O(\\log^2n \\log \\log n)$, significantly outperforming prior algorithms.", "conclusion": "The study advances energy-efficient distributed computing in wireless networks by designing optimal and near-optimal algorithms, demonstrating progress in both theoretical and practical aspects of the MIS problem."}}
{"id": "2510.07815", "pdf": "https://arxiv.org/pdf/2510.07815", "abs": "https://arxiv.org/abs/2510.07815", "authors": ["Zeyu Sun", "Jingjing Liang", "Weiyi Wang", "Chenyao Suo", "Junjie Chen", "Fanjiang Xu"], "title": "Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR", "categories": ["cs.SE"], "comment": null, "summary": "MLIR (Multi-Level Intermediate Representation) has rapidly become a\nfoundational technology for modern compiler frameworks, enabling extensibility\nacross diverse domains. However, ensuring the correctness and robustness of\nMLIR itself remains challenging. Existing fuzzing approaches-based on manually\ncrafted templates or rule-based mutations-struggle to generate sufficiently\ndiverse and semantically valid test cases, making it difficult to expose subtle\nor deep-seated bugs within MLIR's complex and evolving code space. In this\npaper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX\nleverages neural networks for program generation, a perturbed sampling strategy\nto encourage diversity, and a feedback-driven augmentation loop that\niteratively improves its model using both crashing and non-crashing test cases.\nStarting from a limited seed corpus, FLEX progressively learns valid syntax and\nsemantics and autonomously produces high-quality test inputs. We evaluate FLEX\non the upstream MLIR compiler against four state-of-the-art fuzzers. In a\n30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple\nnew root causes and parser bugs-while in 24-hour fixed-revision comparisons, it\ndetects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%\ncode coverage, outperforming the next-best tool by 42%. Ablation studies\nfurther confirm the critical role of both perturbed generation and diversity\naugmentation in FLEX's effectiveness.", "AI": {"tldr": "This paper introduces FLEX, a self-adaptive fuzzing framework for MLIR, which leverages neural networks and a perturbation strategy to discover bugs effectively.", "motivation": "The paper aims to address challenges in fuzzing MLIR, such as generating diverse and semantically valid test cases to uncover subtle errors in MLIR's complex code space.", "method": "FLEX uses neural networks for program generation, a perturbed sampling strategy for diversity, and a feedback-driven augmentation loop to refine its test cases iteratively.", "result": "FLEX outperformed four state-of-the-art fuzzers by detecting 80 unknown bugs in a 30-day evaluation and achieving superior code coverage in 24-hour comparisons.", "conclusion": "The findings highlight FLEX's effectiveness in uncovering MLIR bugs, reinforcing its components like perturbed generation and diversity augmentation as critical to its success."}}
{"id": "2510.07415", "pdf": "https://arxiv.org/pdf/2510.07415", "abs": "https://arxiv.org/abs/2510.07415", "authors": ["Timothy L. Hutcheson", "Anil K. Raj"], "title": "Autoencoding Coordinate Sequences from Psychophysiologic Signals", "categories": ["eess.SP", "q-bio.NC"], "comment": "2 pages, 4 figures, 2025 IEEE Research and Applications of Photonics\n  in Defense Conference (RAPID)", "summary": "We present a method for converting 24 channels of psychophysiologic time\nseries data collected from individual participants via electroencephalogram\n(EEG), electrocardiogram (ECG), electrodermal activity (EDA), respiration rate\n(RR) into trackable three dimensional (3D) coordinates sufficient to estimate\nparticipation in specific task and cognitive states.", "AI": {"tldr": "The paper introduces a technique to map 24 channels of psychophysiological data into 3D coordinates to estimate cognitive states.", "motivation": "The goal is to effectively interpret psychophysiological data to understand participants' involvement in specific tasks and cognitive states.", "method": "Utilizing EEG, ECG, EDA, and RR data, the method transforms time-series inputs into trackable 3D coordinates.", "result": "The approach successfully maps psychophysiological metrics into 3D space, facilitating the estimation of specific tasks and cognitive states.", "conclusion": "The work provides a novel way to analyze complex psychophysiological data and track cognitive and task-related states."}}
{"id": "2510.07611", "pdf": "https://arxiv.org/pdf/2510.07611", "abs": "https://arxiv.org/abs/2510.07611", "authors": ["Jingyang You", "Hanna Kurniawati", "Lashika Medagoda"], "title": "Inspection Planning Primitives with Implicit Models", "categories": ["cs.RO"], "comment": null, "summary": "The aging and increasing complexity of infrastructures make efficient\ninspection planning more critical in ensuring safety. Thanks to sampling-based\nmotion planning, many inspection planners are fast. However, they often require\nhuge memory. This is particularly true when the structure under inspection is\nlarge and complex, consisting of many struts and pillars of various geometry\nand sizes. Such structures can be represented efficiently using implicit\nmodels, such as neural Signed Distance Functions (SDFs). However, most\nprimitive computations used in sampling-based inspection planner have been\ndesigned to work efficiently with explicit environment models, which in turn\nrequires the planner to use explicit environment models or performs frequent\ntransformations between implicit and explicit environment models during\nplanning. This paper proposes a set of primitive computations, called\nInspection Planning Primitives with Implicit Models (IPIM), that enable\nsampling-based inspection planners to entirely use neural SDFs representation\nduring planning. Evaluation on three scenarios, including inspection of a\ncomplex real-world structure with over 92M triangular mesh faces, indicates\nthat even a rudimentary sampling-based planner with IPIM can generate\ninspection trajectories of similar quality to those generated by the\nstate-of-the-art planner, while using up to 70x less memory than the\nstate-of-the-art inspection planner.", "AI": {"tldr": "The paper proposes Inspection Planning Primitives with Implicit Models (IPIM) to enable efficient inspection planning using neural Signed Distance Functions (SDFs), reducing memory usage significantly and maintaining trajectory quality.", "motivation": "Infrastructure inspection is becoming increasingly critical due to aging and complexity, but efficient inspection planning often demands excessive memory, particularly for large, intricate structures. Neural SDF-based representation offers potential solutions, but current planners lack compatibility without inefficient model transformations.", "method": "The authors introduce a set of primitive computations called IPIM, tailored for neural Signed Distance Functions. This enables inspection planners based on sampling to directly use implicit models throughout the planning process.", "result": "Testing on three scenarios, including a real-world structure with over 92M triangular mesh faces, demonstrated that IPIM-based planners can achieve comparable trajectory quality to the state-of-the-art while consuming up to 70x less memory.", "conclusion": "IPIM enables memory-efficient inspection planning using neural SDFs, avoiding explicit model transformations and ensuring high-quality trajectory generation for complex structures."}}
{"id": "2510.07832", "pdf": "https://arxiv.org/pdf/2510.07832", "abs": "https://arxiv.org/abs/2510.07832", "authors": ["Yuta Shikuri", "Hironori Fujisawa"], "title": "Surrogate Graph Partitioning for Spatial Prediction", "categories": ["stat.ML", "cs.LG"], "comment": "18 pages, 5 figures, 2 tables", "summary": "Spatial prediction refers to the estimation of unobserved values from\nspatially distributed observations. Although recent advances have improved the\ncapacity to model diverse observation types, adoption in practice remains\nlimited in industries that demand interpretability. To mitigate this gap,\nsurrogate models that explain black-box predictors provide a promising path\ntoward interpretable decision making. In this study, we propose a graph\npartitioning problem to construct spatial segments that minimize the sum of\nwithin-segment variances of individual predictions. The assignment of data\npoints to segments can be formulated as a mixed-integer quadratic programming\nproblem. While this formulation potentially enables the identification of exact\nsegments, its computational complexity becomes prohibitive as the number of\ndata points increases. Motivated by this challenge, we develop an approximation\nscheme that leverages the structural properties of graph partitioning.\nExperimental results demonstrate the computational efficiency of this\napproximation in identifying spatial segments.", "AI": {"tldr": "The paper tackles spatial prediction challenges using graph partitioning for interpretability, proposing a mixed-integer quadratic programming approach with an efficient approximation scheme.", "motivation": "Industries requiring interpretability in spatial prediction struggle with adopting complex black-box models; the paper aims to bridge this gap.", "method": "Proposes a graph partitioning approach to create spatial segments minimizing prediction variance, formulated as mixed-integer quadratic programming and supported by an efficient approximation.", "result": "The efficient approximation scheme demonstrates computational feasibility in identifying spatial segments, addressing scalability concerns.", "conclusion": "The study introduces a viable method for enhancing interpretability in spatial prediction, balancing computational complexity and precise segmentation."}}
{"id": "2510.07538", "pdf": "https://arxiv.org/pdf/2510.07538", "abs": "https://arxiv.org/abs/2510.07538", "authors": ["Pragati Shuddhodhan Meshram", "Varun Chandrasekaran"], "title": "D2RA: Dual Domain Regeneration Attack", "categories": ["cs.CV"], "comment": null, "summary": "The growing use of generative models has intensified the need for\nwatermarking methods that ensure content attribution and provenance. While\nrecent semantic watermarking schemes improve robustness by embedding signals in\nlatent or frequency representations, we show they remain vulnerable even under\nresource-constrained adversarial settings. We present D2RA, a training-free,\nsingle-image attack that removes or weakens watermarks without access to the\nunderlying model. By projecting watermarked images onto natural priors across\ncomplementary representations, D2RA suppresses watermark signals while\npreserving visual fidelity. Experiments across diverse watermarking schemes\ndemonstrate that our approach consistently reduces watermark detectability,\nrevealing fundamental weaknesses in current designs. Our code is available at\nhttps://github.com/Pragati-Meshram/DAWN.", "AI": {"tldr": "This paper introduces D2RA, a training-free attack that effectively removes or weakens watermarks embedded in generative model outputs by leveraging natural priors, highlighting flaws in current watermarking designs.", "motivation": "The paper is motivated by the vulnerabilities in existing watermarking methods for generative models, even when semantic watermarking is employed, as these methods fail to offer robustness against adversarial attacks in constrained scenarios.", "method": "D2RA, a training-free and single-image attack approach, removes watermarks by projecting watermarked images onto natural priors using complementary representations, suppressing the watermark signal while safeguarding the visual quality.", "result": "The proposed D2RA approach consistently reduces the detectability of watermarks across different schemes, proving its efficiency in overcoming the limitations of existing watermarking systems.", "conclusion": "The results illustrate fundamental weaknesses in current watermarking schemes for generative models, emphasizing the need for improved designs to ensure better robustness against D2RA-like attacks."}}
{"id": "2510.07356", "pdf": "https://arxiv.org/pdf/2510.07356", "abs": "https://arxiv.org/abs/2510.07356", "authors": ["Lingcheng Kong", "Jiateng Wei", "Hanzhang Shen", "Huan Wang"], "title": "ConCuR: Conciseness Makes State-of-the-Art Kernel Generation", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "comment": null, "summary": "GPU kernel generation by LLMs has recently experienced rapid development,\nleveraging test-time scaling and reinforcement learning techniques. However, a\nkey challenge for kernel generation is the scarcity of high-quality data, as\nmost high-quality kernels are proprietary and not open-source. This challenge\nprevents us from leveraging supervised fine-tuning to align LLMs to the kernel\ngeneration task. To address this challenge, we develop a pipeline that\ngenerates and curates high-quality CUDA kernels with reasoning traces,\nmotivated by a critical observation that concise yet informative reasoning\ntraces result in robust generation of high-performance kernels. Using this\npipeline, we construct our dataset ConCuR and introduce our model KernelCoder,\nwhich is the first model trained on a curated dataset consisting of PyTorch,\nreasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,\nour model achieves significant improvements over the existing top-performing\nmodel, QwQ-32B, and outperforms all open-source models fine-tuned for kernel\ngeneration, as well as frontier models such as DeepSeek-V3.1-Think and\nClaude-4-sonnet. Finally, we show that the average reasoning length can serve\nas a metric to assess the difficulty of kernel generation tasks. The\nobservations, metrics, and our data collection and curation pipeline can help\nobtain better data in the kernel generation task in the future.", "AI": {"tldr": "The paper develops a pipeline to generate and curate high-quality CUDA kernels with reasoning traces, addressing the lack of open-source kernel data, and introduces KernelCoder, a model achieving state-of-the-art performance in GPU kernel generation tasks.", "motivation": "The scarcity of high-quality, open-source CUDA kernel data limits the ability to use supervised fine-tuning in large language models for generating high-performance kernels.", "method": "The authors created a pipeline for generating and curating CUDA kernels with concise reasoning traces, leading to the creation of the ConCuR dataset. They trained their model, KernelCoder, on this curated dataset to align with kernel generation tasks.", "result": "KernelCoder significantly outperforms existing top-performing models like QwQ-32B and advanced frontier models such as DeepSeek-V3.1-Think and Claude-4-sonnet in the KernelBench setup.", "conclusion": "The paper's methodology and insights, such as leveraging reasoning traces and reasoning-length metrics, provide tools for improving data quality and advancing kernel generation using large language models."}}
{"id": "2510.07453", "pdf": "https://arxiv.org/pdf/2510.07453", "abs": "https://arxiv.org/abs/2510.07453", "authors": ["Zifan Jiang", "Colin Leong", "Amit Moryossef", "Anne G\u00f6hring", "Annette Rios", "Oliver Cory", "Maksym Ivashechkin", "Neha Tarigopula", "Biao Zhang", "Rico Sennrich", "Sarah Ebling"], "title": "Meaningful Pose-Based Sign Language Evaluation", "categories": ["cs.CL"], "comment": "Accepted at WMT 2025", "summary": "We present a comprehensive study on meaningfully evaluating sign language\nutterances in the form of human skeletal poses. The study covers keypoint\ndistance-based, embedding-based, and back-translation-based metrics. We show\ntradeoffs between different metrics in different scenarios through automatic\nmeta-evaluation of sign-level retrieval and a human correlation study of\ntext-to-pose translation across different sign languages. Our findings and the\nopen-source pose-evaluation toolkit provide a practical and reproducible way of\ndeveloping and evaluating sign language translation or generation systems.", "AI": {"tldr": "The paper studies methods for evaluating sign language utterances via human skeletal poses and provides a toolkit for reproducible research.", "motivation": "To overcome challenges in assessing sign language translation or generation systems in meaningful ways.", "method": "The study explores three evaluation metrics\u2014keypoint distance-based, embedding-based, and back-translation-based\u2014and conducts meta-evaluations and correlation studies.", "result": "Findings show tradeoffs between metrics under different scenarios and release an open-source toolkit for pose evaluation.", "conclusion": "The paper offers practical and reproducible insights for the development and evaluation of sign language systems leveraging skeletal pose analysis."}}
{"id": "2510.07426", "pdf": "https://arxiv.org/pdf/2510.07426", "abs": "https://arxiv.org/abs/2510.07426", "authors": ["Walid Guettala", "Yufan Zhao", "L\u00e1szl\u00f3 Guly\u00e1s"], "title": "Less is More: Strategic Expert Selection Outperforms Ensemble Complexity in Traffic Forecasting", "categories": ["cs.AI", "68T07, 68T05, 62M10, 90B20", "I.2.6; I.5.4"], "comment": "Accepted to IEEE ICTAI 2025. Version 0.9. 10 pages, 5 figures.\n  Preprint differs from the published version in formatting and minor wording", "summary": "Traffic forecasting is fundamental to intelligent transportation systems,\nenabling congestion mitigation and emission reduction in increasingly complex\nurban environments. While recent graph neural network approaches have advanced\nspatial temporal modeling, existing mixture of experts frameworks like Time\nEnhanced Spatio Temporal Attention Model (TESTAM) lack explicit incorporation\nof physical road network topology, limiting their spatial capabilities. We\npresent TESTAM+, an enhanced spatio temporal forecasting framework that\nintroduces a novel SpatioSemantic Expert integrating physical road topology\nwith data driven feature similarity through hybrid graph construction. TESTAM+\nachieves significant improvements over TESTAM: 1.3% MAE reduction on METR LA\n(3.10 vs. 3.14) and 4.1% improvement on PEMS BAY (1.65 vs. 1.72). Through\ncomprehensive ablation studies, we discover that strategic expert selection\nfundamentally outperforms naive ensemble aggregation. Individual experts\ndemonstrate remarkable effectiveness: the Adaptive Expert achieves 1.63 MAE on\nPEMS BAY, outperforming the original three expert TESTAM (1.72 MAE), while the\nSpatioSemantic Expert matches this performance with identical 1.63 MAE. The\noptimal Identity + Adaptive configuration achieves an 11.5% MAE reduction\ncompared to state of the art MegaCRN on METR LA (2.99 vs. 3.38), while reducing\ninference latency by 53.1% compared to the full four expert TESTAM+. Our\nfindings reveal that fewer, strategically designed experts outperform complex\nmulti expert ensembles, establishing new state of the art performance with\nsuperior computational efficiency for real time deployment.", "AI": {"tldr": "Traffic forecasting advances using a framework called TESTAM+ improve on spatial-temporal modeling by integrating physical road topology and feature similarity through hybrid graphs. Significant improvements in accuracy and efficiency were achieved.", "motivation": "The paper aims to address the limitations in spatial capabilities of existing traffic forecasting frameworks (like TESTAM) due to their insufficient use of physical road network topology.", "method": "The study introduces TESTAM+, which incorporates SpatioSemantic Experts through hybrid graph construction and employs strategic expert selection to enhance traffic forecasting performance.", "result": "TESTAM+ demonstrated significant improvements: reducing MAE by 1.3% on METR LA and 4.1% on PEMS BAY, outperforming previous TESTAM configurations and state-of-the-art benchmarks like MegaCRN.", "conclusion": "Fewer, strategically designed experts in TESTAM+ deliver superior computational efficiency and accuracy, outperforming more complex multi-expert ensembles and enabling real-time deployment for traffic forecasting."}}
{"id": "2510.08536", "pdf": "https://arxiv.org/pdf/2510.08536", "abs": "https://arxiv.org/abs/2510.08536", "authors": ["Gregor Olenik", "Marcel Koch", "Hartwig Anzt"], "title": "Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver", "categories": ["cs.DC", "cs.SE"], "comment": "2025 Workshop: HPC on Heterogeneous Hardware (H3)", "summary": "Modern high-performance computing (HPC) increasingly relies on GPUs, but\nintegrating GPU acceleration into complex scientific frameworks like OpenFOAM\nremains a challenge. Existing approaches either fully refactor the codebase or\nuse plugin-based GPU solvers, each facing trade-offs between performance and\ndevelopment effort. In this work, we address the limitations of plugin-based\nGPU acceleration in OpenFOAM by proposing a repartitioning strategy that better\nbalances CPU matrix assembly and GPU-based linear solves. We present a detailed\ncomputational model, describe a novel matrix repartitioning and update\nprocedure, and evaluate its performance on large-scale CFD simulations. Our\nresults show that the proposed method significantly mitigates oversubscription\nissues, improving solver performance and resource utilization in heterogeneous\nCPU-GPU environments.", "AI": {"tldr": "This paper introduces a repartitioning strategy to address GPU integration challenges in OpenFOAM, improving performance and resource utilization.", "motivation": "To overcome performance and development effort trade-offs in integrating GPU acceleration into the OpenFOAM framework.", "method": "Proposes a matrix repartitioning strategy and computational model balancing CPU matrix assembly with GPU linear solves, alongside a procedural update evaluation on large-scale simulations.", "result": "The approach reduces oversubscription issues, thereby boosting solver performance and resource utilization in CPU-GPU environments.", "conclusion": "The proposed framework effectively addresses limitations of plugin-based GPU integration for OpenFOAM, facilitating efficient HPC workflows."}}
{"id": "2510.07834", "pdf": "https://arxiv.org/pdf/2510.07834", "abs": "https://arxiv.org/abs/2510.07834", "authors": ["Lingjun Liu", "Feiran Qin", "Owolabi Legunsen", "Marcelo d'Amorim"], "title": "Bug Histories as Sources of Compiler Fuzzing Mutators", "categories": ["cs.SE"], "comment": null, "summary": "Bugs in compilers, which are critical infrastructure today, can have outsized\nnegative impacts. Mutational fuzzers aid compiler bug detection by\nsystematically mutating compiler inputs, i.e., programs. Their effectiveness\ndepends on the quality of the mutators used. Yet, no prior work used compiler\nbug histories as a source of mutators. We propose IssueMut, the first approach\nfor extracting compiler fuzzing mutators from bug histories. Our insight is\nthat bug reports contain hints about program elements that induced compiler\nbugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated\nmethod to mine mutators from bug reports and retrofit such mutators into\nexisting mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from\n1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with\nall their test inputs as seed corpora. We find that \"bug history\" mutators are\neffective: they find new bugs that a state-of-the-art mutational compiler\nfuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,\nvalidating our idea that bug histories have rich information that compiler\nfuzzers should leverage.", "AI": {"tldr": "IssueMut introduces a novel approach to extract mutators from compiler bug histories, leveraging them for effective fuzzing research, uncovering numerous missed bugs in common compilers.", "motivation": "Compiler bugs have severe impacts, and while mutational fuzzing is a promising detection method, it lacks techniques that derive mutators from previous bug histories.", "method": "IssueMut mines bug histories to extract mutators hinting at program elements causing bugs, then integrates these mutators into fuzzers to explore new vulnerabilities.", "result": "By mining 587 mutators from GCC and LLVM bug reports, IssueMut identified 28 new bugs in GCC and 37 in LLVM, of which 60 were confirmed or fixed.", "conclusion": "Bug histories are a valuable resource for compiler fuzzing, and utilizing them, as demonstrated by IssueMut, improves bug detection significantly by finding previously missed issues."}}
{"id": "2510.08436", "pdf": "https://arxiv.org/pdf/2510.08436", "abs": "https://arxiv.org/abs/2510.08436", "authors": ["Ronja Str\u00f6msd\u00f6rfer", "Klaus Obermayer"], "title": "Spike-frequency and h-current based adaptation are dynamically equivalent in a Wilson-Cowan field model", "categories": ["nlin.AO", "q-bio.NC"], "comment": "16 pages for the main manuscript, 22 in total, 11 figures", "summary": "During slow-wave sleep, the brain produces traveling waves of slow\noscillations (SOs; $\\leq 2$ Hz), characterized by the propagation of\nalternating high- and low-activity states. The question of internal mechanisms\nthat modulate traveling waves of SOs is still unanswered although it is\nestablished that it is an adaptation mechanism that mediates them. One\nmechanism investigated is spike-frequency adaptation, a hyperpolarizing\nfeedback current that is activated during periods of high-activity. An\nalternative mechanism is based on hyperpolarization-activated currents, which\nare positive feedback currents that are activated in low-activity states. Both\nadaptation mechanisms were shown to feature SO-like dynamics in neuronal\npopulations, and the inclusion of a spatial domain seems to enhance observable\ndifferences in their effects. To investigate this in detail, we examine a\nspatially extended two-population Wilson-Cowan model with local spatial\ncoupling and the excitatory populations equipped with either one of the two\nadaptation mechanisms. We describe them with the same dynamical equation and\ninclude the inverse mode of action by changing the signs of adaptation strength\nand gain. We show that the dynamical systems are mathematically equivalent\nunder a compensatory external input, which depends on the adaptation strength,\nleading to a shift in state space of the otherwise equivalent bifurcation\nstructure. Strong enough adaptation is required to induce traveling waves.\nAdditionally, adaptation modulates the properties of the spatio-temporal\nactivity patterns, such as temporal and spatial frequencies, and the speed of\nthe traveling waves, all of which increase with increasing strength. Though\nbeing dynamically equivalent, our results also explain why location-dependent\nvariations in feedback strength cause differences in the propagation of\ntraveling waves between both adaptation mechanisms.", "AI": {"tldr": "The study explores how two different neural adaptation mechanisms influence traveling waves during slow-wave sleep, using a model of neuronal activity.", "motivation": "To understand how internal adaptation mechanisms, such as spike-frequency adaptation and hyperpolarization-activated currents, modulate traveling waves during slow-wave sleep.", "method": "A spatially extended two-population Wilson-Cowan model was used, incorporating local spatial coupling and adaptation mechanisms applied to excitatory neural populations. Dynamical equivalence and state-space shifts were analyzed under varying conditions.", "result": "The mechanisms were shown to be mathematically equivalent with compensatory input, and strong adaptation was required to induce traveling waves. Adaptation modulates temporal/spatial frequencies and wave speed, with variations leading to different effects on wave propagation.", "conclusion": "While the mechanisms are mathematically equivalent, differences in feedback strength affect wave propagation, providing insights into the modulation of brain rhythms during slow-wave sleep."}}
{"id": "2510.07625", "pdf": "https://arxiv.org/pdf/2510.07625", "abs": "https://arxiv.org/abs/2510.07625", "authors": ["Alexander Du", "Emre Adabag", "Gabriel Bravo", "Brian Plancher"], "title": "GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "While Model Predictive Control (MPC) delivers strong performance across\nrobotics applications, solving the underlying (batches of) nonlinear trajectory\noptimization (TO) problems online remains computationally demanding. Existing\nGPU-accelerated approaches typically (i) parallelize a single solve to meet\nreal-time deadlines, (ii) scale to very large batches at slower-than-real-time\nrates, or (iii) achieve speed by restricting model generality (e.g., point-mass\ndynamics or a single linearization). This leaves a large gap in solver\nperformance for many state-of-the-art MPC applications that require real-time\nbatches of tens to low-hundreds of solves. As such, we present GATO, an open\nsource, GPU-accelerated, batched TO solver co-designed across algorithm,\nsoftware, and computational hardware to deliver real-time throughput for these\nmoderate batch size regimes. Our approach leverages a combination of block-,\nwarp-, and thread-level parallelism within and across solves for ultra-high\nperformance. We demonstrate the effectiveness of our approach through a\ncombination of: simulated benchmarks showing speedups of 18-21x over CPU\nbaselines and 1.4-16x over GPU baselines as batch size increases; case studies\nhighlighting improved disturbance rejection and convergence behavior; and\nfinally a validation on hardware using an industrial manipulator. We open\nsource GATO to support reproducibility and adoption.", "AI": {"tldr": "GATO, a GPU-accelerated batched trajectory optimization solver, achieves real-time performance for moderate batch sizes using innovative parallelism techniques.", "motivation": "Real-time Model Predictive Control applications face computational challenges when dealing with moderate batches of nonlinear trajectory optimization problems.", "method": "GATO integrates block-, warp-, and thread-level parallelism across algorithm, software, and hardware design to optimize solver performance.", "result": "GATO achieves 18-21x speedups over CPU baselines and 1.4-16x over GPU baselines in simulated benchmarks, improves disturbance rejection, and validates its performance on industrial hardware.", "conclusion": "GATO enhances throughput for moderate batch sizes, bridging a key performance gap in GPU-based MPC applications, and is open-sourced for broader use and reproducibility."}}
{"id": "2510.07862", "pdf": "https://arxiv.org/pdf/2510.07862", "abs": "https://arxiv.org/abs/2510.07862", "authors": ["Sanghwa Kim", "Dohyun Ahn", "Seungki Min"], "title": "On the Optimality of Tracking Fisher Information in Adaptive Testing with Stochastic Binary Responses", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study the problem of estimating a continuous ability parameter from\nsequential binary responses by actively asking questions with varying\ndifficulties, a setting that arises naturally in adaptive testing and online\npreference learning. Our goal is to certify that the estimate lies within a\ndesired margin of error, using as few queries as possible. We propose a simple\nalgorithm that adaptively selects questions to maximize Fisher information and\nupdates the estimate using a method-of-moments approach, paired with a novel\ntest statistic to decide when the estimate is accurate enough. We prove that\nthis Fisher-tracking strategy achieves optimal performance in both\nfixed-confidence and fixed-budget regimes, which are commonly invested in the\nbest-arm identification literature. Our analysis overcomes a key technical\nchallenge in the fixed-budget setting -- handling the dependence between the\nevolving estimate and the query distribution -- by exploiting a structural\nsymmetry in the model and combining large deviation tools with Ville's\ninequality. Our results provide rigorous theoretical support for simple and\nefficient adaptive testing procedures.", "AI": {"tldr": "The paper addresses efficient adaptive testing and estimation of a continuous ability parameter using sequential binary responses, optimizing query selection while ensuring result accuracy.", "motivation": "The motivation is to improve adaptive testing and preference learning by minimizing the number of queries needed to estimate a continuous ability parameter within a desired accuracy margin.", "method": "An algorithm is proposed that maximizes Fisher information for adaptive query selection, estimates via method-of-moments, and uses a novel statistic to certify accuracy, backed by theoretical proofs in different regimes.", "result": "The proposed Fisher-tracking strategy achieves optimal performance, overcoming challenges in fixed-budget settings and providing rigorous support for efficient procedures.", "conclusion": "The study validates a theoretically-grounded and efficient approach for adaptive estimation, significantly enhancing methodologies for adaptive testing and preference learning."}}
{"id": "2510.07546", "pdf": "https://arxiv.org/pdf/2510.07546", "abs": "https://arxiv.org/abs/2510.07546", "authors": ["Soroush Mehraban", "Vida Adeli", "Jacob Rommann", "Babak Taati", "Kyryl Truskovskyi"], "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters", "categories": ["cs.CV"], "comment": null, "summary": "We address the task of video style transfer with diffusion models, where the\ngoal is to preserve the context of an input video while rendering it in a\ntarget style specified by a text prompt. A major challenge is the lack of\npaired video data for supervision. We propose PickStyle, a video-to-video style\ntransfer framework that augments pretrained video diffusion backbones with\nstyle adapters and benefits from paired still image data with source-style\ncorrespondences for training. PickStyle inserts low-rank adapters into the\nself-attention layers of conditioning modules, enabling efficient\nspecialization for motion-style transfer while maintaining strong alignment\nbetween video content and style. To bridge the gap between static image\nsupervision and dynamic video, we construct synthetic training clips from\npaired images by applying shared augmentations that simulate camera motion,\nensuring temporal priors are preserved. In addition, we introduce Context-Style\nClassifier-Free Guidance (CS-CFG), a novel factorization of classifier-free\nguidance into independent text (style) and video (context) directions. CS-CFG\nensures that context is preserved in generated video while the style is\neffectively transferred. Experiments across benchmarks show that our approach\nachieves temporally coherent, style-faithful, and content-preserving video\ntranslations, outperforming existing baselines both qualitatively and\nquantitatively.", "AI": {"tldr": "This paper introduces PickStyle, a framework that uses diffusion models for video style transfer guided by text prompts, achieving realistic and consistent video translations without requiring paired video data.", "motivation": "A key challenge in video style transfer is the lack of paired video data for training, which makes it difficult to maintain both temporal consistency and style alignment.", "method": "PickStyle incorporates low-rank adapters into pretrained video diffusion models. It uses synthetic training clips generated from paired still images and proposes Context-Style Classifier-Free Guidance (CS-CFG) to decouple style and context guidance.", "result": "The approach achieves temporally coherent, style-faithful, and context-preserving video output, outperforming existing methods in both qualitative and quantitative evaluations.", "conclusion": "PickStyle effectively addresses the challenges in video style transfer, demonstrating its utility for generating high-quality videos that balance temporal coherence and style adherence."}}
{"id": "2510.07358", "pdf": "https://arxiv.org/pdf/2510.07358", "abs": "https://arxiv.org/abs/2510.07358", "authors": ["Yeskendir Koishekenov", "Aldo Lipani", "Nicola Cancedda"], "title": "Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Most efforts to improve the reasoning capabilities of large language models\n(LLMs) involve either scaling the number of parameters and the size of training\ndata, or scaling inference computation by letting models generate complex\nchains of thought. Motivated by interpretability studies showing that the\ncrucial computation required for reasoning tasks is concentrated in a limited\nrange of layers, we introduce Encode-Think-Decode (ETD), a method that enhances\nthe reasoning capabilities of a base model by training it to iterate over a\nsmall subset of reasoning-relevant layers during the mid-training stage. ETD\namplifies latent reasoning while preserving the original architecture,\nparameter count, hyperparameters, and training data composition. When iterating\non the selected layers at inference time, ETD models yield substantial gains on\n17 reasoning benchmarks, including +28.4% relative accuracy improvement on\nGSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an\nadaptive depth strategy that adjusts the computation per input token. Our\nresults show that recursive latent reasoning offers a simple and effective path\nto stronger LLM reasoning.", "AI": {"tldr": "The paper introduces a technique called Encode-Think-Decode (ETD) to enhance reasoning in large language models (LLMs) by iterating specific reasoning-relevant layers during mid-training and inference.", "motivation": "To improve reasoning in LLMs without expanding model size, data, or altering architecture, leveraging interpretability findings that reasoning is concentrated in certain layers.", "method": "ETD trains models to iterate over reasoning-critical layers during the mid-training phase while preserving the original design and introduces layer-based recurrence during inference.", "result": "ETD improves reasoning benchmarks significantly, achieving +28.4% accuracy on GSM8K and +36% on MATH with the OLMo-2 1B Base model.", "conclusion": "Recursive latent reasoning via targeted layer iteration is a straightforward and effective method for enhancing LLM reasoning capabilities."}}
{"id": "2510.07458", "pdf": "https://arxiv.org/pdf/2510.07458", "abs": "https://arxiv.org/abs/2510.07458", "authors": ["Eduardo Ry\u00f4 Tamaki", "Yujin J. Jung", "Julia Chatterley", "Grant Mitchell", "Semir Dzebo", "Crist\u00f3bal Sandoval", "Levente Littvay", "Kirk A. Hawkins"], "title": "Populism Meets AI: Advancing Populism Research with LLMs", "categories": ["cs.CL"], "comment": "27 pages, 3 figures. Preprint version under review", "summary": "Measuring the ideational content of populism remains a challenge. Traditional\nstrategies based on textual analysis have been critical for building the\nfield's foundations and providing a valid, objective indicator of populist\nframing. Yet these approaches are costly, time consuming, and difficult to\nscale across languages, contexts, and large corpora. Here we present the\nresults from a rubric and anchor guided chain of thought (CoT) prompting\napproach that mirrors human coder training. By leveraging the Global Populism\nDatabase (GPD), a comprehensive dataset of global leaders' speeches annotated\nfor degrees of populism, we replicate the process used to train human coders by\nprompting the LLM with an adapted version of the same documentation to guide\nthe model's reasoning. We then test multiple proprietary and open weight models\nby replicating scores in the GPD. Our findings reveal that this domain specific\nprompting strategy enables the LLM to achieve classification accuracy on par\nwith expert human coders, demonstrating its ability to navigate the nuanced,\ncontext sensitive aspects of populism.", "AI": {"tldr": "The paper introduces a rubric-guided chain of thought (CoT) prompting approach for Large Language Models (LLMs) to measure populism in political speeches, achieving human-level classification accuracy.", "motivation": "The aim is to overcome the limitations of traditional textual analysis methods, which are costly and hard to scale for measuring the ideational content of populism across languages, contexts, and large datasets.", "method": "The authors use a rubric and anchor-guided CoT prompting approach on LLMs, training them with data adapted from the Global Populism Database (GPD), a comprehensive dataset of speeches annotated for populism. Multiple models are tested for their performance in replicating GPD scores.", "result": "The proposed strategy enables LLMs to classify the degree of populism in speeches with accuracy comparable to expert human coders, demonstrating its ability to handle nuanced, context-sensitive classification tasks.", "conclusion": "The approach highlights the utility of domain-specific prompting in LLMs for complex tasks like classifying ideational content, offering a scalable and effective alternative to traditional analysis methods."}}
{"id": "2510.07432", "pdf": "https://arxiv.org/pdf/2510.07432", "abs": "https://arxiv.org/abs/2510.07432", "authors": ["Penghang Liu", "Elizabeth Fons", "Svitlana Vyetrenko", "Daniel Borrajo", "Vamsi Potluru", "Manuela Veloso"], "title": "TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering", "categories": ["cs.AI"], "comment": "NeurIPS 2025 Workshop on Foundations of Reasoning in Language Models", "summary": "Large language models (LLMs) have shown strong abilities in reasoning and\nproblem solving, but recent studies reveal that they still struggle with time\nseries reasoning tasks, where outputs are often affected by hallucination or\nknowledge leakage. In this work we propose TS-Agent, a time series reasoning\nagent that leverages LLMs strictly for what they excel at, i.e., gathering\nevidence and synthesizing it into conclusions through step-by-step reasoning,\nwhile delegating the extraction of statistical and structural information to\ntime series analytical tools. Instead of mapping time series into text tokens,\nimages, or embeddings, our agent interacts with raw numeric sequences through\natomic operators, records outputs in an explicit evidence log, and iteratively\nrefines its reasoning under the guidance of a self-critic and a final quality\ngate. This design avoids multi-modal alignment training, preserves the native\nform of time series, ensures interpretability and verifiability, and mitigates\nknowledge leakage or hallucination. Empirically, we evaluate the agent on\nestablished benchmarks. Our experiments show that TS-Agent achieves performance\ncomparable to state-of-the-art LLMs on understanding benchmarks, and delivers\nsignificant improvements on reasoning tasks, where existing models often rely\non memorization and fail in zero-shot settings.", "AI": {"tldr": "This paper introduces TS-Agent, an approach that combines LLMs and time-series analysis tools to achieve better reasoning tasks without hallucination or knowledge leakage, outperforming existing methods.", "motivation": "LLMs struggle with time-series reasoning tasks, often encountering issues like hallucination and knowledge leakage.", "method": "The proposed TS-Agent utilizes LLMs for step-by-step reasoning while assigning statistical and structural analysis to time-series tools, maintaining numeric sequences and iteratively refining reasoning via self-critique.", "result": "TS-Agent performs comparably to state-of-the-art LLMs on understanding tasks and achieves major improvements in reasoning, especially in zero-shot scenarios.", "conclusion": "TS-Agent addresses limitations in time-series reasoning with LLMs by preserving data integrity, ensuring interpretability, and significantly advancing zero-shot reasoning capabilities."}}
{"id": "2510.07664", "pdf": "https://arxiv.org/pdf/2510.07664", "abs": "https://arxiv.org/abs/2510.07664", "authors": ["Yunbo Li", "Jiaping Gui", "Zhihang Deng", "Fanchao Meng", "Yue Wu"], "title": "FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "Accepted by NeurIPS 2025", "summary": "Federated learning (FL) enables collaborative model training across multiple\nparties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as\na balanced approach between synchronous and asynchronous FL. However, SAFL\nfaces significant challenges in optimizing both gradient-based (e.g., FedSGD)\nand model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct\ntrade-offs in accuracy, convergence speed, and stability. While gradient\naggregation achieves faster convergence and higher accuracy, it suffers from\npronounced fluctuations, whereas model aggregation offers greater stability but\nslower convergence and suboptimal accuracy. This paper presents FedQS, the\nfirst framework to theoretically analyze and address these disparities in SAFL.\nFedQS introduces a divide-and-conquer strategy to handle client heterogeneity\nby classifying clients into four distinct types and adaptively optimizing their\nlocal training based on data distribution characteristics and available\ncomputational resources. Extensive experiments on computer vision, natural\nlanguage processing, and real-world tasks demonstrate that FedQS achieves the\nhighest accuracy, attains the lowest loss, and ranks among the fastest in\nconvergence speed, outperforming state-of-the-art baselines. Our work bridges\nthe gap between aggregation strategies in SAFL, offering a unified solution for\nstable, accurate, and efficient federated learning. The code and datasets are\navailable at https://anonymous.4open.science/r/FedQS-EDD6.", "AI": {"tldr": "The paper introduces FedQS, a framework improving semi-asynchronous federated learning by balancing gradient and model aggregation strategies, achieving high accuracy and efficient training.", "motivation": "To address the challenges in optimizing gradient-based and model-based aggregation strategies seen in semi-asynchronous federated learning (SAFL) models, particularly the trade-offs in convergence, accuracy, and stability.", "method": "FedQS leverages a divide-and-conquer strategy by classifying clients into four types based on data and computational profiles, and adaptively optimizing their training processes.", "result": "FedQS surpasses current state-of-the-art methods across computer vision, NLP, and real-world settings in accuracy, loss minimization, and convergence speed.", "conclusion": "FedQS successfully bridges the gap between different aggregation strategies in SAFL, enabling stable, accurate, and efficient federated learning."}}
{"id": "2510.07941", "pdf": "https://arxiv.org/pdf/2510.07941", "abs": "https://arxiv.org/abs/2510.07941", "authors": ["Srijita Basu", "Haraldsson Bengt", "Miroslaw Staron", "Christian Berger", "Jennifer Horkoff", "Magnus Almgren"], "title": "An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software", "categories": ["cs.SE"], "comment": "16 pages, 7 figures, 18th International Conference on the Quality of\n  Information and Communications Technology", "summary": "Cooperative, Connected and Automated Mobility (CCAM) are complex\ncyber-physical systems (CPS) that integrate computation, communication, and\ncontrol in safety-critical environments. At their core, System-on-Chip (SoC)\nplatforms consolidate processing units, communication interfaces, AI\naccelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open\nSystem ARchitecture) standard was developed in the automotive domain to better\nmanage this complexity, defining layered software structures and interfaces to\nfacilitate reuse of HW/SW components. However, in practice, this integrated SoC\nsoftware architecture still poses security challenges, particularly in\nreal-time, safety-critical environments. Recent reports highlight a surge in\nSoC-related vulnerabilities, yet systematic analysis of their root causes and\nimpact within AUTOSAR-aligned architectures is lacking. This study fills that\ngap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped\nto a representative SoC software architecture model that is aligned with\nAUTOSAR principles for layered abstraction and service orientation. We identify\n16 root causes and 56 affected software modules, and examine mitigation delays\nacross Common Weakness Enumeration (CWE) categories and architectural layers.\nWe uncover dominant vulnerability patterns and critical modules with prolonged\npatch delays, and provide actionable insights for securing automotive CPS\nplatforms, including guides for improved detection, prioritization, and\nlocalization strategies for SoC software architectures in SoC-based vehicle\nplatforms.", "AI": {"tldr": "The paper investigates security vulnerabilities in System-on-Chip (SoC) frameworks within automotive environments aligned with AUTOSAR standards, analyzing 180 vulnerabilities to identify root causes, affected modules, and patching delays.", "motivation": "The motivation stems from the increasing vulnerabilities in SoC platforms used in safety-critical automotive systems and the lack of systematic analysis of these risks within AUTOSAR-aligned architectures.", "method": "The study examines 180 publicly reported automotive SoC vulnerabilities, maps them to an AUTOSAR-aligned architecture model, and identifies root causes and patching patterns.", "result": "The analysis identified 16 root causes, 56 affected software modules, and critical modules with prolonged mitigation delays, uncovering vulnerability patterns.", "conclusion": "The findings provide actionable insights for enhancing security in automotive SoC platforms with improved detection and mitigation strategies tailored to AUTOSAR-aligned architectures."}}
{"id": "2510.07674", "pdf": "https://arxiv.org/pdf/2510.07674", "abs": "https://arxiv.org/abs/2510.07674", "authors": ["Lucas Chen", "Shrutheesh Raman Iyer", "Zachary Kingston"], "title": "Differentiable Particle Optimization for Fast Sequential Manipulation", "categories": ["cs.RO"], "comment": "8 pages, 7 figures, 3 tables. Under review", "summary": "Sequential robot manipulation tasks require finding collision-free\ntrajectories that satisfy geometric constraints across multiple object\ninteractions in potentially high-dimensional configuration spaces. Solving\nthese problems in real-time and at large scales has remained out of reach due\nto computational requirements. Recently, GPU-based acceleration has shown\npromising results, but prior methods achieve limited performance due to CPU-GPU\ndata transfer overhead and complex logic that prevents full hardware\nutilization. To this end, we present SPaSM (Sampling Particle optimization for\nSequential Manipulation), a fully GPU-parallelized framework that compiles\nconstraint evaluation, sampling, and gradient-based optimization into optimized\nCUDA kernels for end-to-end trajectory optimization without CPU coordination.\nThe method consists of a two-stage particle optimization strategy: first\nsolving placement constraints through massively parallel sampling, then lifting\nsolutions to full trajectory optimization in joint space. Unlike hierarchical\napproaches, SPaSM jointly optimizes object placements and robot trajectories to\nhandle scenarios where motion feasibility constrains placement options.\nExperimental evaluation on challenging benchmarks demonstrates solution times\nin the realm of $\\textbf{milliseconds}$ with a 100% success rate; a\n$4000\\times$ speedup compared to existing approaches.", "AI": {"tldr": "SPaSM is a GPU-parallelized framework for solving sequential robot manipulation tasks efficiently, achieving millisecond solution times.", "motivation": "The paper aims to address the computational challenges of finding real-time, collision-free trajectories for sequential robot manipulation tasks in high-dimensional configuration spaces.", "method": "The approach involves full GPU parallelization with optimized CUDA kernels, using a two-stage particle optimization strategy\u2014initial sampling for placement constraints and subsequent trajectory optimization in joint space.", "result": "SPaSM achieves a 100% success rate with millisecond-level solution times, demonstrating a remarkable 4000x speedup compared to prior methods.", "conclusion": "The proposed SPaSM framework effectively leverages GPU-based acceleration for real-time performance in complex robot manipulation tasks, overcoming previous limitations of CPU-GPU communication and inefficiencies."}}
{"id": "2510.07867", "pdf": "https://arxiv.org/pdf/2510.07867", "abs": "https://arxiv.org/abs/2510.07867", "authors": ["Xabier de Juan", "Santiago Mazuelas"], "title": "On the Optimality of the Median-of-Means Estimator under Adversarial Contamination", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "The Median-of-Means (MoM) is a robust estimator widely used in machine\nlearning that is known to be (minimax) optimal in scenarios where samples are\ni.i.d. In more grave scenarios, samples are contaminated by an adversary that\ncan inspect and modify the data. Previous work has theoretically shown the\nsuitability of the MoM estimator in certain contaminated settings. However, the\n(minimax) optimality of MoM and its limitations under adversarial contamination\nremain unknown beyond the Gaussian case. In this paper, we present upper and\nlower bounds for the error of MoM under adversarial contamination for multiple\nclasses of distributions. In particular, we show that MoM is (minimax) optimal\nin the class of distributions with finite variance, as well as in the class of\ndistributions with infinite variance and finite absolute $(1+r)$-th moment. We\nalso provide lower bounds for MoM's error that match the order of the presented\nupper bounds, and show that MoM is sub-optimal for light-tailed distributions.", "AI": {"tldr": "The paper analyzes Median-of-Means (MoM) estimators under adversarial contamination for various distribution classes, finding it optimal in some scenarios and sub-optimal in others.", "motivation": "To understand the robustness and limitations of the MoM estimator under adversarial contamination, beyond traditional i.i.d. and Gaussian cases.", "method": "Establishing upper and lower error bounds for the MoM estimator across classes of distributions: finite variance distributions, infinite variance with finite absolute $(1+r)$-th moment distributions, and light-tailed distributions.", "result": "MoM is found to be minimax optimal under finite variance and certain infinite variance distributions, while sub-optimal for light-tailed distributions.", "conclusion": "The study refines understanding of the MoM estimator's performance under various contamination scenarios and establishes its limitations."}}
{"id": "2510.07550", "pdf": "https://arxiv.org/pdf/2510.07550", "abs": "https://arxiv.org/abs/2510.07550", "authors": ["Saman Motamed", "Minghao Chen", "Luc Van Gool", "Iro Laina"], "title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite impressive visual fidelity, modern video generative models frequently\nproduce sequences that violate intuitive physical laws, such as objects\nfloating, teleporting, or morphing in ways that defy causality. While humans\ncan easily detect such implausibilities, there remains no robust method for\nquantitatively assessing physical realism in video. In this work, we explore\nwhether Video-Language Models (VLMs) can be trained to serve as reliable judges\nof physical plausibility. We find that existing VLMs struggle to identify\nphysics violations, exposing fundamental limitations in their temporal and\ncausal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe\nthat combines a balanced training dataset with a trajectory-aware attention\nmodule to improve motion encoding and discrimination in VLMs. To evaluate\nphysical reasoning more rigorously, we propose ImplausiBench, a benchmark of\n300 videos (150 real, 150 generated) that removes linguistic biases and\nisolates visual-temporal understanding. Performance is reported both with\ngold-standard human judgments and stricter LLM-as-judge metrics. Together,\nTRAVL and ImplausiBench offer a unified framework for probing and improving\nphysical plausibility in multimodal models, shedding light on a challenging and\nunderexplored aspect of visual-temporal understanding.", "AI": {"tldr": "The paper addresses the challenge of assessing physical realism in video through Video-Language Models (VLMs). It proposes TRAVL, a fine-tuning method, and introduces the ImplausiBench benchmark for evaluation.", "motivation": "Modern video generative models often produce unrealistic sequences that defy physical laws. However, there is no reliable method to quantitatively assess physical plausibility in video.", "method": "The authors enhance VLMs using TRAVL, which integrates a balanced training dataset and a trajectory-aware attention module. They also introduce ImplausiBench, a benchmark specifically designed to evaluate physical reasoning while reducing linguistic biases.", "result": "Existing VLMs struggle with detecting physical implausibilities. The TRAVL fine-tuning method improves motion encoding and discrimination capabilities, offering better detection of physics violations. ImplausiBench enables rigorous testing of visual-temporal understanding.", "conclusion": "TRAVL and ImplausiBench collectively provide a framework to improve and evaluate physical plausibility in multimodal models, advancing research on visual-temporal reasoning in VLMs."}}
{"id": "2510.07424", "pdf": "https://arxiv.org/pdf/2510.07424", "abs": "https://arxiv.org/abs/2510.07424", "authors": ["Nathan Boyer", "Dorian Baudry", "Patrick Rebeschini"], "title": "Best-of-Both Worlds for linear contextual bandits with paid observations", "categories": ["cs.LG"], "comment": null, "summary": "We study the problem of linear contextual bandits with paid observations,\nwhere at each round the learner selects an action in order to minimize its loss\nin a given context, and can then decide to pay a fixed cost to observe the loss\nof any arm. Building on the Follow-the-Regularized-Leader framework with\nefficient estimators via Matrix Geometric Resampling, we introduce a\ncomputationally efficient Best-of-Both-Worlds (BOBW) algorithm for this\nproblem. We show that it achieves the minimax-optimal regret of\n$\\Theta(T^{2/3})$ in adversarial settings, while guaranteeing poly-logarithmic\nregret in (corrupted) stochastic regimes. Our approach builds on the framework\nfrom \\cite{BOBWhardproblems} to design BOBW algorithms for ``hard problem'',\nusing analysis techniques tailored for the setting that we consider.", "AI": {"tldr": "The paper introduces a computationally efficient Best-of-Both-Worlds algorithm for linear contextual bandits with paid observations, achieving optimal regret in adversarial setups and poly-logarithmic regret in stochastic settings.", "motivation": "The paper aims to address the challenge of minimizing loss in a linear contextual bandit setting where observation costs are incurred, offering a solution for both adversarial and stochastic environments.", "method": "The authors propose a Best-of-Both-Worlds algorithm based on the Follow-the-Regularized-Leader framework and Matrix Geometric Resampling, leveraging tailored analytical techniques.", "result": "The algorithm achieves $\\\\Theta(T^{2/3})$ minimax-optimal regret in adversarial settings and poly-logarithmic regret in (corrupted) stochastic regimes.", "conclusion": "The introduced methodology effectively balances computational efficiency with regret minimization across diverse environments, providing a robust solution for the studied problem."}}
{"id": "2510.07475", "pdf": "https://arxiv.org/pdf/2510.07475", "abs": "https://arxiv.org/abs/2510.07475", "authors": ["Zheyuan Zhang", "Lin Ge", "Hongjiang Li", "Weicheng Zhu", "Chuxu Zhang", "Yanfang Ye"], "title": "MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, and LLM-based agents further extend these abilities to various\npractical workflows. While recent progress shows that multi-agent systems (MAS)\ncan outperform single agents by coordinating specialized roles, designing\neffective MAS remains difficult due to prompt sensitivity and the compounded\ninstability MAS creates. To cope with the challenge, recent efforts in\nautomated prompt design have reduced manual effort. However, multi-agent prompt\noptimization remains largely unexplored. Challenges like exponentially\nexpanding search space and ambiguous credit assignment together make systematic\ndesign intractable without principled methods. Therefore, we introduce\nM}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first\nformulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference\nproblem and solves it using a language-guided variant of max-product belief\npropagation algorithm. To address credit assignment and updates the system\niteratively, MAPRO employs a topology-aware refinement mechanism that\nintegrates execution feedback and downstream blames to selectively update agent\nprompts. Through this process, MAPRO progressively converges to a coordinated\nset of agent-specific prompt policies. Across benchmarks in various tasks,\nMAPRO achieves state-of-the-art performance, consistently surpassing manually\nengineered baselines and recent automated alternatives. Beyond performance, our\nMAP-based formulation also delivers general guidelines for building more\nreliable and principled multi-agent systems in the future", "AI": {"tldr": "The paper introduces MAPRO, a method for optimizing prompts in multi-agent systems (MAS) to improve coordination and performance based on a MAP inference framework.", "motivation": "To address the challenges in designing effective multi-agent systems (MAS), such as instability, compounded sensitivity, and the lack of systematic methods for optimizing prompts.", "method": "MAPRO, a four-stage framework, formulates MAS prompt optimization as a Maximum a Posteriori inference problem and uses a language-guided max-product belief propagation algorithm with topology-aware refinement for iterative updates.", "result": "MAPRO outperformed manually engineered baselines and automated alternatives across various task benchmarks, achieving state-of-the-art performance.", "conclusion": "The MAPRO framework not only improves performance but also provides a principled approach to designing and optimizing multi-agent systems, paving the way for more reliable MAS in the future."}}
{"id": "2510.07456", "pdf": "https://arxiv.org/pdf/2510.07456", "abs": "https://arxiv.org/abs/2510.07456", "authors": ["Binrong Zhu", "Guiran Liu", "Nina Jiang"], "title": "ExpertAgent: Enhancing Personalized Education through Dynamic Planning and Retrieval-Augmented Long-Chain Reasoning", "categories": ["cs.AI"], "comment": "Manuscript previously submitted to the NeurIPS 2025 Workshop on\n  Bridging Language, Agent, and World Models (LAW 2025)", "summary": "The application of advanced generative artificial intelligence in education\nis often constrained by the lack of real-time adaptability, personalization,\nand reliability of the content. To address these challenges, we propose\nExpertAgent - an intelligent agent framework designed for personalized\neducation that provides reliable knowledge and enables highly adaptive learning\nexperiences. Therefore, we developed ExpertAgent, an innovative learning agent\nthat provides users with a proactive and personalized learning experience.\nExpertAgent dynamic planning of the learning content and strategy based on a\ncontinuously updated student model. Therefore, overcoming the limitations of\ntraditional static learning content to provide optimized teaching strategies\nand learning experience in real time. All instructional content is grounded in\na validated curriculum repository, effectively reducing hallucination risks in\nlarge language models and improving reliability and trustworthiness.", "AI": {"tldr": "ExpertAgent is a new AI framework for personalized education that adapts learning strategies through a dynamic student model and reduces hallucination risks by relying on validated curriculum repositories.", "motivation": "The paper addresses the challenges of real-time adaptability, personalization, and reliability in AI-powered educational tools.", "method": "ExpertAgent uses dynamic content planning based on an updated student model and incorporates a validated curriculum repository to ensure reliability.", "result": "ExpertAgent overcomes the limitations of static learning methods by providing optimized and adaptive teaching strategies in real time.", "conclusion": "ExpertAgent enhances personalized learning experiences and improves trustworthiness by addressing key limitations in traditional AI educational tools."}}
{"id": "2510.07901", "pdf": "https://arxiv.org/pdf/2510.07901", "abs": "https://arxiv.org/abs/2510.07901", "authors": ["Georgios Diamantopoulos", "Nikos Tziritas", "Rami Bahsoon", "Georgios Theodoropoulos"], "title": "Decentralised Blockchain Management Through Digital Twins", "categories": ["cs.CR", "cs.DC"], "comment": "Accepted for publication in the proceedings of the 24th Asia\n  Simulation Conference 2025", "summary": "The necessity of blockchain systems to remain decentralised limits current\nsolutions to blockchain governance and dynamic management, forcing a trade-off\nbetween control and decentralisation. In light of the above, this work proposes\na dynamic and decentralised blockchain management mechanism based on digital\ntwins. To ensure decentralisation, the proposed mechanism utilises multiple\ndigital twins that the system's stakeholders control. To facilitate\ndecentralised decision-making, the twins are organised in a secondary\nblockchain system that orchestrates agreement on, and propagation of decisions\nto the managed blockchain. This enables the management of blockchain systems\nwithout centralised control. A preliminary evaluation of the performance and\nimpact of the overheads introduced by the proposed mechanism is conducted\nthrough simulation. The results demonstrate the proposed mechanism's ability to\nreach consensus on decisions quickly and reconfigure the primary blockchain\nwith minimal overhead.", "AI": {"tldr": "The paper proposes a dynamic and decentralized blockchain management system using digital twins to address the trade-off between control and decentralization in blockchain governance.", "motivation": "Current blockchain systems face a dilemma between maintaining decentralization and enabling effective governance and dynamic management.", "method": "The mechanism uses multiple digital twins controlled by stakeholders. These twins operate within a secondary blockchain for decentralized decision-making, reaching consensus and propagating decisions to a primary blockchain.", "result": "Simulation evaluations showed the proposed mechanism can rapidly reach consensus and reconfigure the primary blockchain with minimal overhead.", "conclusion": "The proposed digital twin-based system effectively manages blockchain systems without necessitating centralized governance."}}
{"id": "2510.08005", "pdf": "https://arxiv.org/pdf/2510.08005", "abs": "https://arxiv.org/abs/2510.08005", "authors": ["Utku Boran Torun", "Mehmet Taha Demircan", "Mahmut Furkan G\u00f6n", "Eray T\u00fcz\u00fcn"], "title": "Past, Present, and Future of Bug Tracking in the Generative AI Era", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to ACM TOSEM Special Issue: 2030 Software Engineering\n  Roadmap", "summary": "Traditional bug tracking systems rely heavily on manual reporting,\nreproduction, triaging, and resolution, each carried out by different\nstakeholders such as end users, customer support, developers, and testers. This\ndivision of responsibilities requires significant coordination and widens the\ncommunication gap between non-technical users and technical teams, slowing the\nprocess from bug discovery to resolution. Moreover, current systems are highly\nasynchronous; users often wait hours or days for a first response, delaying\nfixes and contributing to frustration. This paper examines the evolution of bug\ntracking, from early paper-based reporting to today's web-based and SaaS\nplatforms. Building on this trajectory, we propose an AI-powered bug tracking\nframework that augments existing tools with intelligent, large language model\n(LLM)-driven automation. Our framework addresses two main challenges: reducing\ntime-to-fix and minimizing human overhead. Users report issues in natural\nlanguage, while AI agents refine reports, attempt reproduction, and request\nmissing details. Reports are then classified, invalid ones resolved through\nno-code fixes, and valid ones localized and assigned to developers. LLMs also\ngenerate candidate patches, with human oversight ensuring correctness. By\nintegrating automation into each phase, our framework accelerates response\ntimes, improves collaboration, and strengthens software maintenance practices\nfor a more efficient, user-centric future.", "AI": {"tldr": "The paper proposes an AI-powered bug tracking system using large language models (LLMs) to automate bug reporting, refinement, reproduction, and resolution, aiming to reduce human effort and speed up fixes.", "motivation": "Traditional bug tracking systems are slow, manual, and require significant coordination between technical and non-technical stakeholders, which hampers efficiency and frustrates users.", "method": "The proposed framework uses LLMs to automate bug reporting, classify issues, refine reports, reproduce bugs, and generate candidate patches. It integrates automation across all phases of the bug tracking and resolution process.", "result": "The framework enables faster response times, reduces human effort, and enhances collaboration between users and developers by leveraging AI at each stage of bug management.", "conclusion": "The AI-driven approach improves bug tracking efficiency and software maintenance practices, creating a faster, more user-centered process for identifying and resolving issues."}}
{"id": "2510.07700", "pdf": "https://arxiv.org/pdf/2510.07700", "abs": "https://arxiv.org/abs/2510.07700", "authors": ["Raghav Mishra", "Ian R. Manchester"], "title": "EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We propose enforcing constraints on Model-Based Diffusion by introducing\nemerging barrier functions inspired by interior point methods. We show that\nconstraints on Model-Based Diffusion can lead to catastrophic performance\ndegradation, even on simple 2D systems due to sample inefficiency in the Monte\nCarlo approximation of the score function. We introduce Emerging-Barrier\nModel-Based Diffusion (EB-MBD) which uses progressively introduced barrier\nconstraints to avoid these problems, significantly improving solution quality,\nwithout the need for computationally expensive operations such as projections.\nWe analyze the sampling liveliness of samples each iteration to inform barrier\nparameter scheduling choice. We demonstrate results for 2D collision avoidance\nand a 3D underwater manipulator system and show that our method achieves lower\ncost solutions than Model-Based Diffusion, and requires orders of magnitude\nless computation time than projection based methods.", "AI": {"tldr": "This paper introduces Emerging-Barrier Model-Based Diffusion (EB-MBD), which uses barrier functions to enforce constraints while improving computation efficiency and solution quality.", "motivation": "The motivation is to address the performance issues and computational inefficiencies caused by constraints in Model-Based Diffusion systems, particularly when using Monte Carlo approximations.", "method": "The authors propose Emerging-Barrier Model-Based Diffusion (EB-MBD) which introduces progressive barrier constraints to avoid catastrophic performance degradation, without relying on expensive projection-based methods.", "result": "The method demonstrates superior solution quality in 2D collision avoidance and a 3D underwater manipulator task. It also achieves significantly lower computational requirements compared to projection-based solutions.", "conclusion": "EB-MBD improves solution quality and computational efficiency for constrained systems in Model-Based Diffusion, making it a viable alternative to traditional methods."}}
{"id": "2510.07965", "pdf": "https://arxiv.org/pdf/2510.07965", "abs": "https://arxiv.org/abs/2510.07965", "authors": ["Seungsu Han", "Juyoung Hwang", "Won Chang"], "title": "Stick-Breaking Mixture Normalizing Flows with Component-Wise Tail Adaptation for Variational Inference", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Normalizing flows with a Gaussian base provide a computationally efficient\nway to approximate posterior distributions in Bayesian inference, but they\noften struggle to capture complex posteriors with multimodality and heavy\ntails. We propose a stick-breaking mixture base with component-wise tail\nadaptation (StiCTAF) for posterior approximation. The method first learns a\nflexible mixture base to mitigate the mode-seeking bias of reverse KL\ndivergence through a weighted average of component-wise ELBOs. It then\nestimates local tail indices of unnormalized densities and finally refines each\nmixture component using a shared backbone combined with component-specific tail\ntransforms calibrated by the estimated indices. This design enables accurate\nmode coverage and anisotropic tail modeling while retaining exact density\nevaluation and stable optimization. Experiments on synthetic posteriors\ndemonstrate improved tail recovery and better coverage of multiple modes\ncompared to benchmark models. We also present a real-data analysis illustrating\nthe practical benefits of our approach for posterior inference.", "AI": {"tldr": "The paper proposes StiCTAF, a novel method to improve posterior approximations using normalizing flows by introducing a stick-breaking mixture base with tail adaptation to handle multimodal and heavy-tailed distributions.", "motivation": "Normalizing flows struggle to approximate complex posterior distributions with multimodality and heavy tails, prompting the need for improvements.", "method": "StiCTAF is introduced, which first uses a flexible stick-breaking mixture base for better mode representation, estimates local tail indices, and refines components with tail transforms to model anisotropic tails.", "result": "The experiments show improved mode coverage and tail recovery for synthetic posteriors compared to benchmarks, alongside practical advantages in real-data posterior inference.", "conclusion": "StiCTAF is effective at addressing the limitations of normalizing flows in Bayesian inference, achieving both accurate mode and tail approximation while maintaining computational efficiency."}}
{"id": "2510.07556", "pdf": "https://arxiv.org/pdf/2510.07556", "abs": "https://arxiv.org/abs/2510.07556", "authors": ["Rafin Hassan", "Zarin Tasnim Roshni", "Rafiqul Bari", "Alimul Islam", "Nabeel Mohammed", "Moshiur Farazi", "Shafin Rahman"], "title": "Label Semantics for Robust Hyperspectral Image Classification", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "This work has been accepted for publication in the proceedings of\n  IJCNN 2025", "summary": "Hyperspectral imaging (HSI) classification is a critical tool with widespread\napplications across diverse fields such as agriculture, environmental\nmonitoring, medicine, and materials science. Due to the limited availability of\nhigh-quality training samples and the high dimensionality of spectral data, HSI\nclassification models are prone to overfitting and often face challenges in\nbalancing accuracy and computational complexity. Furthermore, most of HSI\nclassification models are monomodal, where it solely relies on spectral-spatial\ndata to learn decision boundaries in the high dimensional embedding space. To\naddress this, we propose a general-purpose Semantic Spectral-Spatial Fusion\nNetwork (S3FN) that uses contextual, class specific textual descriptions to\ncomplement the training of an HSI classification model. Specifically, S3FN\nleverages LLMs to generate comprehensive textual descriptions for each class\nlabel that captures their unique characteristics and spectral behaviors. These\ndescriptions are then embedded into a vector space using a pre-trained text\nencoder such as BERT or RoBERTa to extract meaningful label semantics which in\nturn leads to a better feature-label alignment for improved classification\nperformance. To demonstrate the effectiveness of our approach, we evaluate our\nmodel on three diverse HSI benchmark datasets - Hyperspectral Wood,\nHyperspectralBlueberries, and DeepHS-Fruit and report significant performance\nboost. Our results highlight the synergy between textual semantics and\nspectral-spatial data, paving the way for further advancements in semantically\naugmented HSI classification models. Codes are be available in:\nhttps://github.com/milab-nsu/S3FN", "AI": {"tldr": "The paper proposes a Semantic Spectral-Spatial Fusion Network (S3FN) that incorporates textual descriptions generated by LLMs to enhance hyperspectral imaging (HSI) classification, achieving better accuracy on benchmark datasets.", "motivation": "HSI classification struggles with overfitting due to limited training samples and high dimensionality, and traditionally relies solely on spectral-spatial data, which limits performance.", "method": "The paper introduces S3FN, which utilizes LLMs to generate class-specific textual descriptions and embeds these using pre-trained text encoders like BERT or RoBERTa, complementing the learning of spectral-spatial data.", "result": "The S3FN model demonstrates significant performance improvements over baseline methods on three HSI benchmark datasets: Hyperspectral Wood, Hyperspectral Blueberries, and DeepHS-Fruit.", "conclusion": "By integrating textual semantics with spectral-spatial data, S3FN achieves a more effective feature-label alignment, enhancing HSI classification models and setting a precedent for semantically augmented approaches in the field."}}
{"id": "2510.07429", "pdf": "https://arxiv.org/pdf/2510.07429", "abs": "https://arxiv.org/abs/2510.07429", "authors": ["Wang Wei", "Tiankai Yang", "Hongjie Chen", "Yue Zhao", "Franck Dernoncourt", "Ryan A. Rossi", "Hoda Eldardiry"], "title": "Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs", "categories": ["cs.LG"], "comment": "16 pages, 3 figures", "summary": "Efficient use of large language models (LLMs) is critical for deployment at\nscale: without adaptive routing, systems either overpay for strong models or\nrisk poor performance from weaker ones. Selecting the right LLM for each query\nis fundamentally an online decision problem: models differ in strengths, prices\nfluctuate, and users value accuracy and cost differently. Yet most routers are\ntrained offline with labels for all candidate models, an assumption that breaks\nin deployment, where only the outcome of the chosen model is observed. We\nbridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach\nthat trains under the same partial-feedback restriction as deployment, while\nsupporting preference-tunable inference: operators can dial the\nperformance/cost trade-off at test time without retraining. Framed as a\ncontextual bandit over prompt features and a user preference vector, our method\nsimulates an online feedback setting during training and adapts its routing\ndecisions to each new prompt, rather than depending on full-information offline\nsupervision. Comprehensive experiments show that our method consistently\noutperforms strong offline routers by at least 12.46% and the largest LLM by at\nleast 2.45%, and generalizes robustly for unseen tasks.", "AI": {"tldr": "BaRP, a contextual bandit approach, optimizes LLM selection by using partial feedback during training, enabling adaptable performance/cost trade-offs while outperforming offline routers and large models in deployment.", "motivation": "Efficient selection of large language models for deployment is necessary to balance performance and cost, as current offline routers fail to adapt to real-world conditions.", "method": "The paper introduces BaRP, a contextual bandit framework that utilizes prompt features and user preferences, simulating online feedback during training to make adaptive routing decisions.", "result": "BaRP improved routing performance by at least 12.46% compared to offline routers and 2.45% relative to the largest LLM, showing robustness even for unseen tasks.", "conclusion": "BaRP bridges the gap between offline training and real-world deployment, offering a tunable framework that adapts to cost-performance preferences while consistently outperforming traditional solutions."}}
{"id": "2510.07486", "pdf": "https://arxiv.org/pdf/2510.07486", "abs": "https://arxiv.org/abs/2510.07486", "authors": ["Shuqing Luo", "Yilin Guan", "Pingzhi Li", "Hanrui Wang", "Tianlong Chen"], "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding", "categories": ["cs.CL"], "comment": "14 pages, 17 figures", "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).", "AI": {"tldr": "The paper introduces AsyncSpade, a framework that improves the efficiency of test-time scaling (TTS) for large language models (LLMs) by removing sequential dependency without sacrificing performance.", "motivation": "The motivation is to address the memory-bound bottleneck and inefficiencies in LLM decoding during TTS tasks, particularly in high-concurrency and long-chain-of-thought (CoT) scenarios.", "method": "The authors developed AsyncSpade, an asynchronous framework with two features: (1) a lightweight temporal-regressive module for predicting next-token query states; and (2) an asynchronous setup that separates KV-cache filtering from decoding, enabling simultaneous operations.", "result": "AsyncSpade achieves over 20% reduction in time-per-output-token (TPOT) compared to the prior state-of-the-art (Quest) and at least 50% reduction compared to full attention models, while maintaining or improving accuracy across multiple TTS benchmarks.", "conclusion": "AsyncSpade resolves the bottlenecks in TTS for LLMs, boosts efficiency with asynchronous KV-cache operations, and sets new benchmarks in both speed and accuracy."}}
{"id": "2510.07489", "pdf": "https://arxiv.org/pdf/2510.07489", "abs": "https://arxiv.org/abs/2510.07489", "authors": ["Akhil Kumar", "Jianliang Leon Zhao", "Om Dobariya"], "title": "Evaluation of LLMs for Process Model Analysis and Optimization", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.IR", "cs.LG"], "comment": "15 pages, 5 tables, 4 figures; full research paper currently under\n  review for the Workshop on Information Technologies and Systems (WITS) 2025.\n  The paper presents a comprehensive evaluation of large language models (LLMs)\n  for business process model analysis and optimization, including error\n  detection, reasoning, and scenario-based redesign", "summary": "In this paper, we report our experience with several LLMs for their ability\nto understand a process model in an interactive, conversational style, find\nsyntactical and logical errors in it, and reason with it in depth through a\nnatural language (NL) interface. Our findings show that a vanilla, untrained\nLLM like ChatGPT (model o3) in a zero-shot setting is effective in\nunderstanding BPMN process models from images and answering queries about them\nintelligently at syntactic, logic, and semantic levels of depth. Further,\ndifferent LLMs vary in performance in terms of their accuracy and\neffectiveness. Nevertheless, our empirical analysis shows that LLMs can play a\nvaluable role as assistants for business process designers and users. We also\nstudy the LLM's \"thought process\" and ability to perform deeper reasoning in\nthe context of process analysis and optimization. We find that the LLMs seem to\nexhibit anthropomorphic properties.", "AI": {"tldr": "The paper investigates how large language models (LLMs) interactively analyze process models, finding that they are effective in understanding and reasoning about Business Process Model and Notation (BPMN).", "motivation": "The study aims to explore the potential of LLMs as intelligent assistants for business process model understanding, error detection, and deep reasoning via natural language interfaces.", "method": "The authors conducted an empirical analysis of several LLMs, including ChatGPT in a zero-shot setting, to evaluate their performance on understanding BPMN models, addressing queries, detecting errors, and reasoning.", "result": "Untrained LLMs like ChatGPT demonstrated effectiveness in understanding BPMN models from images and answering related queries at various depth levels. Variations in LLM performance were observed.", "conclusion": "LLMs have potential as valuable assistants for process model designers, showing intelligent reasoning and anthropomorphic traits in business process analysis and optimization."}}
{"id": "2510.07922", "pdf": "https://arxiv.org/pdf/2510.07922", "abs": "https://arxiv.org/abs/2510.07922", "authors": ["Murtaza Rangwala", "Farag Azzedin", "Richard O. Sinnott", "Rajkumar Buyya"], "title": "SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening", "categories": ["cs.LG", "cs.DC"], "comment": "23 pages, 5 figures, Code Available:\n  https://doi.org/10.5281/zenodo.17223405", "summary": "Decentralized Federated Learning (DFL) enables privacy-preserving\ncollaborative training without centralized servers, but remains vulnerable to\nByzantine attacks where malicious clients submit corrupted model updates.\nExisting Byzantine-robust DFL defenses rely on similarity-based neighbor\nscreening that requires every client to exchange and compare complete\nhigh-dimensional model vectors with all neighbors in each training round,\ncreating prohibitive communication and computational costs that prevent\ndeployment at web scale. We propose SketchGuard, a general framework that\ndecouples Byzantine filtering from model aggregation through sketch-based\nneighbor screening. SketchGuard compresses $d$-dimensional models to\n$k$-dimensional sketches ($k \\ll d$) using Count Sketch for similarity\ncomparisons, then selectively fetches full models only from accepted neighbors,\nreducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| +\nd|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \\le |N_i|$ is the\naccepted neighbor count. We establish rigorous convergence guarantees in both\nstrongly convex and non-convex settings, proving that Count Sketch compression\npreserves Byzantine resilience with controlled degradation bounds where\napproximation errors introduce only a $(1+O(\\epsilon))$ factor in the effective\nthreshold parameter. Comprehensive experiments across multiple datasets,\nnetwork topologies, and attack scenarios demonstrate that SketchGuard maintains\nidentical robustness to state-of-the-art methods while reducing computation\ntime by up to 82% and communication overhead by 50-70% depending on filtering\neffectiveness, with benefits scaling multiplicatively with model dimensionality\nand network connectivity. These results establish the viability of sketch-based\ncompression as a fundamental enabler of robust DFL at web scale.", "AI": {"tldr": "This paper introduces SketchGuard, a decentralized federated learning (DFL) method using sketch-based compression to defend against Byzantine attacks efficiently.", "motivation": "The motivation is the challenge of enabling Byzantine-robust decentralized federated learning (DFL) while minimizing high communication and computational costs associated with existing methods.", "method": "SketchGuard employs Count Sketch compression to create low-dimensional sketches for neighbor screening, selectively fetching full models only from accepted neighbors, thereby reducing communication complexity.", "result": "The proposed framework achieves comparable robustness to existing methods while reducing computation time by up to 82% and communication overhead by 50-70%, with advantages scaling with model dimensionality and connectivity.", "conclusion": "Sketch-based compression is a practical and efficient solution to enable robust decentralized federated learning at large scales, preserving effectiveness against adversarial attacks."}}
{"id": "2510.08200", "pdf": "https://arxiv.org/pdf/2510.08200", "abs": "https://arxiv.org/abs/2510.08200", "authors": ["Alexander Hellwig", "Nico Jansen", "Bernhard Rumpe"], "title": "Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components", "categories": ["cs.SE", "68N15", "D.2.13"], "comment": "11 pages, 4 figures, 6 listings", "summary": "In Software Language Engineering, there is a trend towards reusability by\ncomposing modular language components. However, this reusability is severely\ninhibited by a gap in integrating whitespace-sensitive and\nwhitespace-insensitive languages. There is currently no consistent procedure\nfor seamlessly reusing such language components in both cases, such that\nlibraries often cannot be reused, and whitespacesensitive languages are\ndeveloped from scratch. This paper presents a technique for using modular,\nwhitespaceinsensitive language modules to construct whitespace sensitive\nlanguages by pre-processing language artifacts before parsing. The approach is\nevaluated by reconstructing a simplified version of the programming language\nPython. Our solution aims to increase the reusability of existing language\ncomponents to reduce development time and increase the overall quality of\nsoftware languages.", "AI": {"tldr": "The paper addresses the challenge of reusing modular language components for both whitespace-sensitive and -insensitive languages by pre-processing artifacts before parsing.", "motivation": "In Software Language Engineering, there is a need to improve reusability when integrating whitespace-sensitive and whitespace-insensitive languages as current methods are inconsistent and hinder reuse.", "method": "The authors propose a technique that preprocesses artifacts from modular, whitespace-insensitive language modules to construct whitespace-sensitive languages.", "result": "The technique is evaluated by reconstructing a simplified version of Python, demonstrating the feasibility of the approach.", "conclusion": "The paper concludes that the proposed solution enhances reusability of language components, thereby reducing development time and improving software language quality."}}
{"id": "2510.07725", "pdf": "https://arxiv.org/pdf/2510.07725", "abs": "https://arxiv.org/abs/2510.07725", "authors": ["Kasidit Muenprasitivej", "Ye Zhao", "Glen Chou"], "title": "Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis", "categories": ["cs.RO"], "comment": "9 pages, 4 figures", "summary": "We address the challenge of enabling bipedal robots to traverse rough terrain\nby developing probabilistically safe planning and control strategies that\nensure dynamic feasibility and centroidal robustness under terrain uncertainty.\nSpecifically, we propose a high-level Model Predictive Control (MPC) navigation\nframework for a bipedal robot with a specified confidence level of safety that\n(i) enables safe traversal toward a desired goal location across a terrain map\nwith uncertain elevations, and (ii) formally incorporates uncertainty bounds\ninto the centroidal dynamics of locomotion control. To model the rough terrain,\nwe employ Gaussian Process (GP) regression to estimate elevation maps and\nleverage Conformal Prediction (CP) to construct calibrated confidence intervals\nthat capture the true terrain elevation. Building on this, we formulate\ncontraction-based reachable tubes that explicitly account for terrain\nuncertainty, ensuring state convergence and tube invariance. In addition, we\nintroduce a contraction-based flywheel torque control law for the reduced-order\nLinear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum\nabout the center-of-mass (CoM). This formulation provides both probabilistic\nsafety and goal reachability guarantees. For a given confidence level, we\nestablish the forward invariance of the proposed torque control law by\ndemonstrating exponential stabilization of the actual CoM phase-space\ntrajectory and the desired trajectory prescribed by the high-level planner.\nFinally, we evaluate the effectiveness of our planning framework through\nphysics-based simulations of the Digit bipedal robot in MuJoCo.", "AI": {"tldr": "The paper presents a probabilistically safe planning and control strategy for bipedal robots traversing rough terrains by combining Gaussian Process regression, Conformal Prediction, and Model Predictive Control (MPC).", "motivation": "To address the challenges posed by uncertain terrains, ensuring dynamic feasibility and safety for bipedal robots in achieving stable locomotion.", "method": "The method integrates GP and CP for terrain uncertainty modeling, contraction-based reachable tubes for state convergence, torque control law for stabilizing angular momentum, and MPC for probabilistic safe planning.", "result": "The framework enables safe traversal with goal reachability guarantees, demonstrated via simulations on the Digit bipedal robot in MuJoCo.", "conclusion": "The proposed approach successfully ensures centroidal robustness and dynamic feasibility in uncertain terrains, providing probabilistic safety guarantees for bipedal robots."}}
{"id": "2510.08095", "pdf": "https://arxiv.org/pdf/2510.08095", "abs": "https://arxiv.org/abs/2510.08095", "authors": ["Amitis Shidani", "Tyler Farghly", "Yang Sun", "Habib Ganjgahi", "George Deligiannidis"], "title": "Beyond Real Data: Synthetic Data through the Lens of Regularization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Synthetic data can improve generalization when real data is scarce, but\nexcessive reliance may introduce distributional mismatches that degrade\nperformance. In this paper, we present a learning-theoretic framework to\nquantify the trade-off between synthetic and real data. Our approach leverages\nalgorithmic stability to derive generalization error bounds, characterizing the\noptimal synthetic-to-real data ratio that minimizes expected test error as a\nfunction of the Wasserstein distance between the real and synthetic\ndistributions. We motivate our framework in the setting of kernel ridge\nregression with mixed data, offering a detailed analysis that may be of\nindependent interest. Our theory predicts the existence of an optimal ratio,\nleading to a U-shaped behavior of test error with respect to the proportion of\nsynthetic data. Empirically, we validate this prediction on CIFAR-10 and a\nclinical brain MRI dataset. Our theory extends to the important scenario of\ndomain adaptation, showing that carefully blending synthetic target data with\nlimited source data can mitigate domain shift and enhance generalization. We\nconclude with practical guidance for applying our results to both in-domain and\nout-of-domain scenarios.", "AI": {"tldr": "This paper introduces a theoretical framework to balance synthetic and real data for optimal generalization, validated empirically on CIFAR-10 and clinical brain MRI datasets.", "motivation": "Existing challenges in using synthetic data arise from potential distribution mismatches, which can degrade model performance.", "method": "The authors use algorithmic stability to derive generalization error bounds, integrating the Wasserstein distance for quantifying synthetic and real data trade-offs.", "result": "The study predicts a U-shaped test error behavior based on synthetic data proportion and demonstrates performance improvements in domain adaptation scenarios.", "conclusion": "Carefully balancing synthetic and real data can optimize learning and generalization in both in-domain and cross-domain settings, providing practical insights."}}
{"id": "2510.07567", "pdf": "https://arxiv.org/pdf/2510.07567", "abs": "https://arxiv.org/abs/2510.07567", "authors": ["Karuna Bhaila", "Aneesh Komanduri", "Minh-Hao Van", "Xintao Wu"], "title": "Cross-Modal Attention Guided Unlearning in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated immense capabilities in\nmulti-modal understanding and inference tasks such as Visual Question Answering\n(VQA), which requires models to infer outputs based on visual and textual\ncontext simultaneously. Such inference abilities of large-scale pretrained\nmodels are often attributed to the massive scale of pre-training data collected\nacross several domains. However, the models may memorize private and/or\nsensitive information during training and regurgitate it in inference.\nRecently, machine unlearning has been leveraged to address the leakage of\nprivate data in LLMs. VLMs add a layer of complexity to this process, as the\nvisual context in the query may also contain sensitive information in addition\nto the text. To address this issue, we explore unlearning for vision-language\nmodels, specifically for the VQA task. We explore the role of visual tokens for\noutput generation in VLMs using cross-modal attention and utilize it to\nformulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and\nefficient VLM unlearning framework. In contrast to computationally expensive\nmodel finetuning methods, CAGUL utilizes external modules to encode unlearning\ninformation in visual tokens of low importance for relevant queries. We find\nthat the transformed visual tokens not only prevent leakage but also retain\nreference model behavior. Experimental results show that our method performs\nbetter or on par with finetuning-based baselines without altering the\npre-trained model parameters or incurring retraining costs, making it a\npractical and effective unlearning solution for VLMs.", "AI": {"tldr": "The paper introduces Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight framework addressing private data leakage in Vision-Language Models (VLMs), achieving efficient unlearning without retraining.", "motivation": "To address private data leakage in VLMs during inference, particularly for tasks combining visual and textual contexts such as Visual Question Answering (VQA).", "method": "Developing CAGUL, a framework utilizing visual tokens of low importance guided by cross-modal attention, thereby preventing private data leakage without altering the model's parameters.", "result": "Experimental results demonstrate that CAGUL effectively prevents data leakage, performs comparably or better than finetuning methods, and avoids retraining costs.", "conclusion": "CAGUL is a practical, efficient, and lightweight solution for unlearning in VLMs, safeguarding data privacy while preserving model performance."}}
{"id": "2510.07436", "pdf": "https://arxiv.org/pdf/2510.07436", "abs": "https://arxiv.org/abs/2510.07436", "authors": ["Ankur Naskar", "Gugan Thoppe", "Utsav Negi", "Vijay Gupta"], "title": "Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) can dramatically speed up reinforcement learning by\ndistributing exploration and training across multiple agents. It can guarantee\nan optimal convergence rate that scales linearly in the number of agents, i.e.,\na rate of $\\tilde{O}(1/(NT)),$ where $T$ is the iteration index and $N$ is the\nnumber of agents. However, when the training samples arise from a Markov chain,\nexisting results on TD learning achieving this rate require the algorithm to\ndepend on unknown problem parameters. We close this gap by proposing a\ntwo-timescale Federated Temporal Difference (FTD) learning with Polyak-Ruppert\naveraging. Our method provably attains the optimal $\\tilde{O}(1/NT)$ rate in\nboth average-reward and discounted settings--offering a parameter-free FTD\napproach for Markovian data. Although our results are novel even in the\nsingle-agent setting, they apply to the more realistic and challenging scenario\nof FL with heterogeneous environments.", "AI": {"tldr": "This paper introduces a Federated Temporal Difference (FTD) learning approach that efficiently speeds up reinforcement learning using multiple agents and achieves optimal convergence rates for Markovian data.", "motivation": "Existing TD learning methods achieving optimal convergence rates require dependency on unknown problem parameters in case of Markovian training samples. This paper aims to address this gap.", "method": "The paper proposes a two-timescale Federated Temporal Difference (FTD) learning algorithm with Polyak-Ruppert averaging, designed to be parameter-free and applicable to federated learning environments.", "result": "The proposed method attains the optimal convergence rate of $\tilde{O}(1/(NT))$ in both average-reward and discounted settings, showcasing its effectiveness and novelty.", "conclusion": "The FTD algorithm overcomes limitations of previous methods, providing a practical and scalable solution for federated reinforcement learning in heterogeneous environments with Markovian data."}}
{"id": "2510.07488", "pdf": "https://arxiv.org/pdf/2510.07488", "abs": "https://arxiv.org/abs/2510.07488", "authors": ["Rasika Muralidharan", "Jaewoon Kwak", "Jisun An"], "title": "Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review at ARR", "summary": "Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are\ngaining attention, yet fewer studies explore their team dynamics. Inspired by\nhuman team science, we propose a multi-agent framework to examine core aspects\nof team science: structure, diversity, and interaction dynamics. We evaluate\nteam performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and\nLatent Implicit Hate, spanning commonsense and social reasoning. Our results\nshow that flat teams tend to perform better than hierarchical ones, while\ndiversity has a nuanced impact. Interviews suggest agents are overconfident\nabout their team performance, yet post-task reflections reveal both\nappreciation for collaboration and challenges in integration, including limited\nconversational coordination.", "AI": {"tldr": "This paper explores the dynamics of multi-agent systems powered by Large Language Models, assessing team structure, diversity, and interaction dynamics across commonsense and social reasoning tasks.", "motivation": "To investigate how team dynamics in MAS with LLM-powered agents affect their performance, inspired by human team science.", "method": "A multi-agent framework was designed and evaluated on four tasks related to commonsense and social reasoning. Interviews captured agents' reflections on teamwork challenges.", "result": "Flat team structures outperformed hierarchical ones, and diversity had an intricate effect. Interviews revealed overconfidence in team performance and challenges in conversational integration.", "conclusion": "Flat team structures may enhance MAS performance, but integration challenges persist. Effective conversational coordination is critical for optimizing team collaboration."}}
{"id": "2510.07491", "pdf": "https://arxiv.org/pdf/2510.07491", "abs": "https://arxiv.org/abs/2510.07491", "authors": ["Clotilde Bray\u00e9", "Aur\u00e9lien Bricout", "Arnaud Gotlieb", "Nadjib Lazaar", "Quentin Vallet"], "title": "Optimizing Ethical Risk Reduction for Medical Intelligent Systems with Constraint Programming", "categories": ["cs.AI"], "comment": null, "summary": "Medical Intelligent Systems (MIS) are increasingly integrated into healthcare\nworkflows, offering significant benefits but also raising critical safety and\nethical concerns. According to the European Union AI Act, most MIS will be\nclassified as high-risk systems, requiring a formal risk management process to\nensure compliance with the ethical requirements of trustworthy AI. In this\ncontext, we focus on risk reduction optimization problems, which aim to reduce\nrisks with ethical considerations by finding the best balanced assignment of\nrisk assessment values according to their coverage of trustworthy AI ethical\nrequirements. We formalize this problem as a constrained optimization task and\ninvestigate three resolution paradigms: Mixed Integer Programming (MIP),\nSatisfiability (SAT), and Constraint Programming(CP).Our contributions include\nthe mathematical formulation of this optimization problem, its modeling with\nthe Minizinc constraint modeling language, and a comparative experimental study\nthat analyzes the performance, expressiveness, and scalability of each approach\nto solving. From the identified limits of the methodology, we draw some\nperspectives of this work regarding the integration of the Minizinc model into\na complete trustworthy AI ethical risk management process for MIS.", "AI": {"tldr": "The paper addresses risk reduction optimization in Medical Intelligent Systems (MIS) as per the ethical requirements of the European Union AI Act, using three resolution paradigms: Mixed Integer Programming (MIP), Satisfiability (SAT), and Constraint Programming (CP).", "motivation": "The growing integration of Medical Intelligent Systems (MIS) in healthcare demands higher safety and ethical compliance, especially under the European Union AI Act where MIS are classified as high-risk systems requiring thorough risk management processes.", "method": "The paper formalizes the ethical risk reduction as a constrained optimization problem, comparing Mixed Integer Programming (MIP), Satisfiability (SAT), and Constraint Programming (CP) to solve it, using Minizinc for modeling and experimentation.", "result": "The comparative study highlights the performance, expressiveness, and scalability of MIP, SAT, and CP paradigms in handling ethical risk optimization tasks.", "conclusion": "The work contributes a formal optimization model, identifies limitations, and discusses perspectives such as integrating Minizinc into a broader trustworthy AI ethical risk management process for MIS."}}
{"id": "2510.08055", "pdf": "https://arxiv.org/pdf/2510.08055", "abs": "https://arxiv.org/abs/2510.08055", "authors": ["Gunjun Lee", "Jiwon Kim", "Jaiyoung Park", "Younjoo Lee", "Jung Ho Ahn"], "title": "From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill", "categories": ["cs.LG", "cs.DC"], "comment": "13 pages, 5 figure, 8 tables", "summary": "Large Language Model (LLM) inference in production must meet stringent\nservice-level objectives for both time-to-first-token (TTFT) and\ntime-between-token (TBT) while maximizing throughput under fixed compute,\nmemory, and interconnect budgets. Modern serving systems adopt stall-free\nscheduling techniques such as chunked prefill, which splits long prompt\nprocessing along the token dimension and interleaves prefill with ongoing\ndecode iterations. While effective at stabilizing TBT, chunked prefill incurs\nsubstantial overhead in Mixture-of-Experts (MoE) models: redundant expert\nweight loads increase memory traffic by up to 39% and inflate energy\nconsumption. We propose layered prefill, a new scheduling paradigm that treats\ntransformer layer groups as the primary scheduling unit. By vertically\npartitioning the model into contiguous layer groups and interleaving prefill\nand decode across the groups, layered prefill sustains stall-free decoding\nwhile eliminating chunk-induced MoE weight reloads. It reduces off-chip\nbandwidth demand, lowering TTFT by up to 70%, End-to-End latency by 41% and\nper-token energy by up to 22%. Evaluations show that layered prefill\nconsistently improves the TTFT--TBT Pareto frontier over chunked prefill,\nreducing expert-load traffic and energy cost while maintaining stall-free\ndecoding. Overall, shifting the scheduling axis from tokens to layers unlocks a\nnew operating regime for high-efficiency, energy-aware LLM serving in\nco-located environments.", "AI": {"tldr": "The paper introduces \"layered prefill,\" a scheduling technique to optimize LLM inference by addressing inefficiencies caused by existing methods like chunked prefill, particularly for MoE models.", "motivation": "Current LLM serving systems struggle with balancing throughput, latency, energy consumption, and memory usage due to inefficiencies like redundant weight loads in MoE models.", "method": "The method involves partitioning the transformer model into layer groups, enabling scheduling at the layer level rather than token level, thus interleaving prefill and decode across groups.", "result": "Layered prefill reduces off-chip bandwidth demand, TTFT by 70%, end-to-end latency by 41%, and per-token energy consumption by up to 22%.", "conclusion": "Shifting scheduling from tokens to layers enhances performance, efficiency, and energy-awareness in LLM serving systems, setting a new paradigm for co-located AI environments."}}
{"id": "2510.07614", "pdf": "https://arxiv.org/pdf/2510.07614", "abs": "https://arxiv.org/abs/2510.07614", "authors": ["Amine Barrak"], "title": "Traceability and Accountability in Role-Specialized Multi-Agent LLM Pipelines", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Sequential multi-agent systems built with large language models (LLMs) can\nautomate complex software tasks, but they are hard to trust because errors\nquietly pass from one stage to the next. We study a traceable and accountable\npipeline, meaning a system with clear roles, structured handoffs, and saved\nrecords that let us trace who did what at each step and assign blame when\nthings go wrong. Our setting is a Planner -> Executor -> Critic pipeline. We\nevaluate eight configurations of three state-of-the-art LLMs on three\nbenchmarks and analyze where errors start, how they spread, and how they can be\nfixed. Our results show: (1) adding a structured, accountable handoff between\nagents markedly improves accuracy and prevents the failures common in simple\npipelines; (2) models have clear role-specific strengths and risks (e.g.,\nsteady planning vs. high-variance critiquing), which we quantify with repair\nand harm rates; and (3) accuracy-cost-latency trade-offs are task-dependent,\nwith heterogeneous pipelines often the most efficient. Overall, we provide a\npractical, data-driven method for designing, tracing, and debugging reliable,\npredictable, and accountable multi-agent systems.", "AI": {"tldr": "The paper examines a structured and accountable pipeline for multi-agent systems using LLMs, showing that such design improves accuracy, prevents failures, and allows traceability for errors.", "motivation": "The study seeks to address trust and reliability issues in sequential multi-agent systems built with LLMs, where errors can propagate silently across stages.", "method": "The authors study an LLM-based Planner -> Executor -> Critic pipeline, testing eight configurations of three state-of-the-art LLMs against benchmarks to analyze error propagation, fixing strategies, and optimize trade-offs in accuracy, cost, and latency.", "result": "The findings demonstrate that structured handoffs enhance accuracy, prevent notable failures, uncover task-specific trade-offs, and highlight role-specific strengths (e.g., planning vs. critiquing) within the pipeline systems.", "conclusion": "Introducing accountability and structured handoffs in multi-agent systems leads to more reliable, traceable, and predictable outcomes, offering a practical framework for building such systems."}}
{"id": "2510.07749", "pdf": "https://arxiv.org/pdf/2510.07749", "abs": "https://arxiv.org/abs/2510.07749", "authors": ["Alexandre Moreira Nascimento", "Gabriel Kenji Godoy Shimanuki", "L\u00facio Flavio Vismari", "Jo\u00e3o Batista Camargo Jr", "Jorge Rady de Almeida Jr", "Paulo Sergio Cugnasca", "Anna Carolina Muller Queiroz", "Jeremy Noah Bailenson"], "title": "Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework", "categories": ["cs.RO"], "comment": "22 pages, 15 figures, 21 tables", "summary": "Perception failures in autonomous vehicles (AV) remain a major safety concern\nbecause they are the basis for many accidents. To study how these failures\naffect safety, researchers typically inject artificial faults into hardware or\nsoftware components and observe the outcomes. However, existing fault injection\nstudies often target a single sensor or machine perception (MP) module,\nresulting in siloed frameworks that are difficult to generalize or integrate\ninto unified simulation environments. This work addresses that limitation by\nreframing perception failures as hallucinations, false perceptions that distort\nan AV situational awareness and may trigger unsafe control actions. Since\nhallucinations describe only observable effects, this abstraction enables\nanalysis independent of specific sensors or algorithms, focusing instead on how\ntheir faults manifest along the MP pipeline. Building on this concept, we\npropose a configurable, component-agnostic hallucination injection framework\nthat induces six plausible hallucination types in an iterative open-source\nsimulator. More than 18,350 simulations were executed in which hallucinations\nwere injected while AVs crossed an unsignalized transverse street with traffic.\nThe results statistically validate the framework and quantify the impact of\neach hallucination type on collisions and near misses. Certain hallucinations,\nsuch as perceptual latency and drift, significantly increase the risk of\ncollision in the scenario tested, validating the proposed paradigm can stress\nthe AV system safety. The framework offers a scalable, statistically validated,\ncomponent agnostic, and fully interoperable toolset that simplifies and\naccelerates AV safety validations, even those with novel MP architectures and\ncomponents. It can potentially reduce the time-to-market of AV and lay the\nfoundation for future research on fault tolerance, and resilient AV design.", "AI": {"tldr": "The study introduces a framework to assess perception failures in autonomous vehicles by simulating various hallucination types (false perceptions). Over 18,350 simulations reveal the significant safety impacts of these failures, specifically perceptual latency and drift, on collision risks.", "motivation": "Perception failures in autonomous vehicles are a critical safety issue, leading to accidents. Current methods of fault injection focus on specific sensors or modules, making them narrow and isolative.", "method": "The authors reframe perception failures as observable hallucinations and propose a component-agnostic hallucination injection framework for simulation. They tested six hallucination types over 18,350 simulations using an open-source platform.", "result": "The study quantified the effect of various hallucination types on collisions and near misses. It found that perceptual latency and drift notably increase collision risks, validating the framework's ability to evaluate AV safety.", "conclusion": "The framework is scalable, statistically validated, and agnostic to components, offering a robust tool for AV safety evaluation. It has the potential to expedite AV development and promote fault-tolerant and resilient designs."}}
{"id": "2510.08123", "pdf": "https://arxiv.org/pdf/2510.08123", "abs": "https://arxiv.org/abs/2510.08123", "authors": ["Parham Rezaei", "Filip Kovacevic", "Francesco Locatello", "Marco Mondelli"], "title": "High-dimensional Analysis of Synthetic Data Selection", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Despite the progress in the development of generative models, their\nusefulness in creating synthetic data that improve prediction performance of\nclassifiers has been put into question. Besides heuristic principles such as\n\"synthetic data should be close to the real data distribution\", it is actually\nnot clear which specific properties affect the generalization error. Our paper\naddresses this question through the lens of high-dimensional regression.\nTheoretically, we show that, for linear models, the covariance shift between\nthe target distribution and the distribution of the synthetic data affects the\ngeneralization error but, surprisingly, the mean shift does not. Furthermore we\nprove that, in some settings, matching the covariance of the target\ndistribution is optimal. Remarkably, the theoretical insights from linear\nmodels carry over to deep neural networks and generative models. We empirically\ndemonstrate that the covariance matching procedure (matching the covariance of\nthe synthetic data with that of the data coming from the target distribution)\nperforms well against several recent approaches for synthetic data selection,\nacross training paradigms, architectures, datasets and generative models used\nfor augmentation.", "AI": {"tldr": "The study examines the impact of synthetic data properties on classifier performance, finding that covariance matching is key, not mean matching.", "motivation": "The motivation is to clarify which specific properties of synthetic data influence the generalization error of classifiers, given the uncertainty in current heuristic principles.", "method": "The study theoretically explores high-dimensional regression, focusing on linear models to analyze covariance and mean shifts. These insights are extended to deep neural networks and generative models, with empirical validation.", "result": "The research finds that covariance shifts influence generalization error, while mean shifts do not. Covariance matching is empirically shown to outperform other synthetic data selection methods across multiple scenarios.", "conclusion": "Matching covariances between synthetic and target data distributions optimally improves classifier performance, applicable across various models and datasets."}}
{"id": "2510.07580", "pdf": "https://arxiv.org/pdf/2510.07580", "abs": "https://arxiv.org/abs/2510.07580", "authors": ["Dewi Endah Kharismawati", "Toni Kazic"], "title": "MaizeStandCounting (MaSC): Automated and Accurate Maize Stand Counting from UAV Imagery Using Image Processing and Deep Learning", "categories": ["cs.CV"], "comment": "10 pages, 11 figures. Submitted to IEEE Journal of Selected Topics in\n  Signal Processing (JSTSP) Special Series on Artificial Intelligence for Smart\n  Agriculture", "summary": "Accurate maize stand counts are essential for crop management and research,\ninforming yield prediction, planting density optimization, and early detection\nof germination issues. Manual counting is labor-intensive, slow, and\nerror-prone, especially across large or variable fields. We present\nMaizeStandCounting (MaSC), a robust algorithm for automated maize seedling\nstand counting from RGB imagery captured by low-cost UAVs and processed on\naffordable hardware. MaSC operates in two modes: (1) mosaic images divided into\npatches, and (2) raw video frames aligned using homography matrices. Both modes\nuse a lightweight YOLOv9 model trained to detect maize seedlings from V2-V10\ngrowth stages. MaSC distinguishes maize from weeds and other vegetation, then\nperforms row and range segmentation based on the spatial distribution of\ndetections to produce precise row-wise stand counts. Evaluation against\nin-field manual counts from our 2024 summer nursery showed strong agreement\nwith ground truth (R^2= 0.616 for mosaics, R^2 = 0.906 for raw frames). MaSC\nprocessed 83 full-resolution frames in 60.63 s, including inference and\npost-processing, highlighting its potential for real-time operation. These\nresults demonstrate MaSC's effectiveness as a scalable, low-cost, and accurate\ntool for automated maize stand counting in both research and production\nenvironments.", "AI": {"tldr": "The paper introduces MaizeStandCounting (MaSC), an algorithm using UAV imagery and YOLOv9 for automated maize stand counting, achieving effective and scalable results.", "motivation": "The need for accurate and efficient maize stand counts for better crop management, yield prediction, and addressing germination issues, as manual methods are laborious and error-prone.", "method": "The method involves acquiring RGB imagery using low-cost UAVs, processed through a lightweight YOLOv9 model operating in two modes: image mosaics divided into patches and raw video frames aligned via homography matrices.", "result": "Evaluation demonstrated strong results with R^2 values of 0.616 for image mosaics and 0.906 for raw frames, and an efficient processing speed of 83 frames in 60.63 seconds.", "conclusion": "MaSC proves to be a low-cost, scalable, and accurate tool for maize stand counting, suitable for both research and production fields."}}
{"id": "2510.07459", "pdf": "https://arxiv.org/pdf/2510.07459", "abs": "https://arxiv.org/abs/2510.07459", "authors": ["Yoli Shavit", "Jacob Goldberger"], "title": "MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a\nnovel Mixture-of-Experts (MoE) framework designed for regression tasks and\napplied to time series forecasting. Unlike conventional MoEs that provide only\npoint estimates, MoGU models each expert's output as a Gaussian distribution.\nThis allows it to directly quantify both the forecast (the mean) and its\ninherent uncertainty (variance). MoGU's core innovation is its\nuncertainty-based gating mechanism, which replaces the traditional input-based\ngating network by using each expert's estimated variance to determine its\ncontribution to the final prediction. Evaluated across diverse time series\nforecasting benchmarks, MoGU consistently outperforms single-expert models and\ntraditional MoE setups. It also provides well-quantified, informative\nuncertainties that directly correlate with prediction errors, enhancing\nforecast reliability. Our code is available from:\nhttps://github.com/yolish/moe_unc_tsf", "AI": {"tldr": "MoGU introduces a new Mixture-of-Experts (MoE) framework with uncertainty-based gating for time series regression, offering better accuracy and uncertainty quantification.", "motivation": "To enhance regression tasks in time series forecasting by incorporating uncertainty quantification into predictions.", "method": "MoGU represents outputs as Gaussian distributions and uses an uncertainty-based gating mechanism to weigh expert contributions.", "result": "MoGU consistently performs better than single-expert and traditional MoE models, providing reliable uncertainty estimates that align with prediction errors.", "conclusion": "MoGU improves predictive accuracy and reliability in time series forecasting while offering meaningful uncertainty insights."}}
{"id": "2510.07497", "pdf": "https://arxiv.org/pdf/2510.07497", "abs": "https://arxiv.org/abs/2510.07497", "authors": ["Yi-Jen Shih", "Desh Raj", "Chunyang Wu", "Wei Zhou", "SK Bong", "Yashesh Gaur", "Jay Mahadeokar", "Ozlem Kalinli", "Mike Seltzer"], "title": "Can Speech LLMs Think while Listening?", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "Recent advances in speech large language models (speech LLMs) have enabled\nseamless spoken interactions, but these systems still struggle with complex\nreasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning\nhas been to shown to significantly improve the reasoning abilities of\ntext-based LLMs. In this work, we investigate the effect of CoT fine-tuning for\nmulti-stream speech LLMs, demonstrating that reasoning in text space improves\nthe accuracy of speech LLMs by 2.4x, on average, over a suite of spoken\nreasoning tasks. Beyond accuracy, the latency of the spoken response is a\ncrucial factor for interacting with voice-based agents. Inspired by the human\nbehavior of \"thinking while listening,\" we propose methods to reduce the\nadditional latency from reasoning by allowing the model to start reasoning\nbefore the user query has ended. To achieve this, we introduce an entropy-based\nmetric, \"question completeness,\" which acts as an indicator to guide the model\non the optimal time to start reasoning. This method provides greater control\nover the accuracy-latency trade-off compared with heuristic-based approaches\nand, under equivalent latency conditions, yields a 4% accuracy gain on\nARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference\ndata created using rejection sampling to push the accuracy-latency pareto\nfrontier further, resulting in a 70% reduction in latency without loss in\naccuracy.", "AI": {"tldr": "The paper explores improving reasoning in speech-based large language models (speech LLMs) through chain-of-thought (CoT) fine-tuning and proposes methods for reducing response latency through advanced techniques.", "motivation": "Speech-based LLMs excel in seamless spoken interactions but struggle with complex reasoning tasks, prompting the need to enhance their reasoning abilities while managing latency.", "method": "Authors employ CoT fine-tuning to enhance reasoning and introduce a novel entropy-based metric, \"question completeness,\" to optimize the timing of reasoning initiation, alongside Direct Preference Optimization (DPO) to balance accuracy and latency.", "result": "CoT fine-tuning significantly improves speech LLM reasoning accuracy by 2.4x, while the \"question completeness\" metric combined with preference optimization reduces latency by 70% without sacrificing accuracy.", "conclusion": "The proposed methods advance the accuracy and latency trade-off in speech LLMs, creating more efficient systems for spoken reasoning tasks."}}
{"id": "2510.07516", "pdf": "https://arxiv.org/pdf/2510.07516", "abs": "https://arxiv.org/abs/2510.07516", "authors": ["Md. Nazmul Islam Ananto", "Shamit Fatin", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The popular path query - identifying the most frequented routes between\nlocations from historical trajectory data - has important applications in urban\nplanning, navigation optimization, and travel recommendations. While\ntraditional algorithms and machine learning approaches have achieved success in\nthis domain, they typically require model training, parameter tuning, and\nretraining when accommodating data updates. As Large Language Models (LLMs)\ndemonstrate increasing capabilities in spatial and graph-based reasoning, there\nis growing interest in exploring how these models can be applied to geo-spatial\nproblems.\n  We introduce CompassLLM, a novel multi-agent framework that intelligently\nleverages the reasoning capabilities of LLMs into the geo-spatial domain to\nsolve the popular path query. CompassLLM employs its agents in a two-stage\npipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage\nthat synthesizes novel paths in the absence of an existing one in the\nhistorical trajectory data. Experiments on real and synthetic datasets show\nthat CompassLLM demonstrates superior accuracy in SEARCH and competitive\nperformance in GENERATE while being cost-effective.", "AI": {"tldr": "CompassLLM leverages Large Language Models (LLMs) in a multi-agent framework to solve path queries in geo-spatial domains through reasoning-based approaches.", "motivation": "The paper addresses the inefficiencies in traditional methods for the popular path query, particularly their need for extensive retraining and parameter tuning, and explores the emerging potential of LLMs in handling spatial and graph-based reasoning tasks.", "method": "CompassLLM introduces a two-stage framework: a SEARCH stage to identify popular paths using historical trajectory data, and a GENERATE stage to create novel paths when data is insufficient, utilizing LLMs' reasoning capabilities.", "result": "Experiments revealed that CompassLLM performs with high accuracy in identifying paths (SEARCH stage) and shows competitive results in generating paths (GENERATE stage), while maintaining cost-efficiency.", "conclusion": "CompassLLM successfully integrates LLMs into geo-spatial reasoning tasks, presenting a promising and efficient alternative to traditional methods for solving path queries."}}
{"id": "2510.08072", "pdf": "https://arxiv.org/pdf/2510.08072", "abs": "https://arxiv.org/abs/2510.08072", "authors": ["Vamsi Addanki"], "title": "When Light Bends to the Collective Will: A Theory and Vision for Adaptive Photonic Scale-up Domains", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "As chip-to-chip silicon photonics gain traction for their bandwidth and\nenergy efficiency, collective communication has emerged as a critical\nbottleneck in scale-up systems. Programmable photonic interconnects offer a\npromising path forward: by dynamically reconfiguring the fabric, they can\nestablish direct, high-bandwidth optical paths between communicating endpoints\n-- \\emph{synchronously and guided by the structure of collective operations}\n(e.g., AllReduce). However, realizing this vision -- \\emph{when light bends to\nthe collective will} -- requires navigating a fundamental trade-off between\nreconfiguration delay and the performance gains of adaptive topologies.\n  In this paper, we present a simple theoretical framework for adaptive\nphotonic scale-up domains that makes this trade-off explicit and clarifies when\nreconfiguration is worthwhile. Along the way, we highlight a connection -- not\nsurprising but still powerful -- between the Birkhoff--von Neumann (BvN)\ndecomposition, maximum concurrent flow (a classic measure of network\nthroughput), and the well-known $\\alpha$-$\\beta$ cost model for collectives.\nFinally, we outline a research agenda in algorithm design and systems\nintegration that can build on this foundation.", "AI": {"tldr": "The paper addresses the challenges of chip-to-chip silicon photonics in collective communication by proposing adaptive programmable photonic interconnects to optimize performance despite reconfiguration delays.", "motivation": "To overcome bottlenecks in collective communication for scalable systems using silicon photonics, and to exploit adaptive, dynamic reconfiguration capabilities for optimizing high-bandwidth optical paths.", "method": "Introduces a theoretical framework connecting Birkhoff--von Neumann decomposition, maximum concurrent flow, and the $\nalpha$-$\beta$ cost model to manage reconfiguration delays effectively.", "result": "The framework explicitly showcases the trade-offs in performance gains against reconfiguration delays and lays the groundwork for a research agenda in related algorithm design and systems integration.", "conclusion": "Adaptive reconfiguration can significantly improve communication in silicon photonics, but its trade-offs must be analyzed through insights provided by the proposed theoretical connections."}}
{"id": "2510.08118", "pdf": "https://arxiv.org/pdf/2510.08118", "abs": "https://arxiv.org/abs/2510.08118", "authors": ["Massimiliano de Leoni", "Faizan Ahmed Khan", "Simone Agostinelli"], "title": "Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)", "categories": ["cs.RO", "cs.SE"], "comment": "16 pages, 5 figures", "summary": "Robotic Process Mining focuses on the identification of the routine types\nperformed by human resources through a User Interface. The ultimate goal is to\ndiscover routine-type models to enable robotic process automation. The\ndiscovery of routine-type models requires the provision of a routine log.\nUnfortunately, the vast majority of existing works do not directly focus on\nenabling the model discovery, limiting themselves to extracting the set of\nactions that are part of the routines. They were also not evaluated in\nscenarios characterized by inconsistent routine execution, hereafter referred\nto as noise, which reflects natural variability and occasional errors in human\nperformance. This paper presents a clustering-based technique that aims to\nextract routine logs. Experiments were conducted on nine UI logs from the\nliterature with different levels of injected noise. Our technique was compared\nwith existing techniques, most of which are not meant to discover routine logs\nbut were adapted for the purpose. The results were evaluated through standard\nstate-of-the-art metrics, showing that we can extract more accurate routine\nlogs than what the state of the art could, especially in the presence of noise.", "AI": {"tldr": "This paper proposes a clustering-based method to improve routine log extraction for robotic process mining, particularly under noisy conditions, outperforming previous techniques.", "motivation": "The study aims to address the gap in extracting routine logs effectively, especially in noisy scenarios, to enable robotic process automation beyond simply capturing routine actions.", "method": "The authors develop a clustering-based technique, which extracts routine logs from UI interactions and compares its effectiveness against adapted existing techniques using nine datasets with different noise levels.", "result": "Experiments demonstrate that the proposed technique achieves greater accuracy in routine log extraction compared to state-of-the-art methods, particularly under conditions with added noise.", "conclusion": "The clustering-based approach provides a robust solution for routine log discovery, overcoming limitations of existing techniques and setting a new benchmark in robotic process mining accuracy."}}
{"id": "2510.07773", "pdf": "https://arxiv.org/pdf/2510.07773", "abs": "https://arxiv.org/abs/2510.07773", "authors": ["YuHang Tang", "Yixuan Lou", "Pengfei Han", "Haoming Song", "Xinyi Ye", "Dong Wang", "Bin Zhao"], "title": "Trajectory Conditioned Cross-embodiment Skill Transfer", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning manipulation skills from human demonstration videos presents a\npromising yet challenging problem, primarily due to the significant embodiment\ngap between human body and robot manipulators. Existing methods rely on paired\ndatasets or hand-crafted rewards, which limit scalability and generalization.\nWe propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment\nSkill Transfer, enabling robots to acquire manipulation skills directly from\nhuman demonstration videos. Our key insight is to represent human motions as\nsparse optical flow trajectories, which serve as embodiment-agnostic motion\ncues by removing morphological variations while preserving essential dynamics.\nConditioned on these trajectories together with visual and textual inputs,\nTrajSkill jointly synthesizes temporally consistent robot manipulation videos\nand translates them into executable actions, thereby achieving cross-embodiment\nskill transfer. Extensive experiments are conducted, and the results on\nsimulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\\% and KVD\nby 36.6\\% compared with the state-of-the-art, and improves cross-embodiment\nsuccess rate by up to 16.7\\%. Real-robot experiments in kitchen manipulation\ntasks further validate the effectiveness of our approach, demonstrating\npractical human-to-robot skill transfer across embodiments.", "AI": {"tldr": "This paper introduces TrajSkill, a framework for transferring manipulation skills directly from human demonstration videos to robots. It uses optical flow trajectories to bridge the human-robot embodiment gap without relying on paired datasets or hand-crafted rewards.", "motivation": "Human-to-robot skill transfer faces challenges due to embodiment differences, and existing methods relying on paired datasets or crafted rewards face scalability and generalization issues.", "method": "TrajSkill leverages sparse optical flow trajectories derived from human motion as embodiment-agnostic cues. It uses visual and textual inputs to synthesize robot manipulation videos and translate them into executable actions for cross-embodiment skill transfer.", "result": "Experiments on simulation data show significant improvements over state-of-the-art methods, with 39.6% reduction in FVD, 36.6% reduction in KVD, and up to 16.7% increase in cross-embodiment success rates. Real-world tests in kitchen tasks further validate the approach.", "conclusion": "TrajSkill demonstrates effective human-to-robot skill transfer, bridging the embodiment gap and improving the practical applicability of manipulation skill learning through scalable and generalized techniques."}}
{"id": "2510.08335", "pdf": "https://arxiv.org/pdf/2510.08335", "abs": "https://arxiv.org/abs/2510.08335", "authors": ["Ivan Kirev", "Lyuben Baltadzhiev", "Nikola Konstantinov"], "title": "PAC Learnability in the Presence of Performativity", "categories": ["stat.ML", "cs.LG"], "comment": "21 pages, 3 figures", "summary": "Following the wide-spread adoption of machine learning models in real-world\napplications, the phenomenon of performativity, i.e. model-dependent shifts in\nthe test distribution, becomes increasingly prevalent. Unfortunately, since\nmodels are usually trained solely based on samples from the original\n(unshifted) distribution, this performative shift may lead to decreased\ntest-time performance. In this paper, we study the question of whether and when\nperformative binary classification problems are learnable, via the lens of the\nclassic PAC (Probably Approximately Correct) learning framework. We motivate\nseveral performative scenarios, accounting in particular for linear shifts in\nthe label distribution, as well as for more general changes in both the labels\nand the features. We construct a performative empirical risk function, which\ndepends only on data from the original distribution and on the type\nperformative effect, and is yet an unbiased estimate of the true risk of a\nclassifier on the shifted distribution. Minimizing this notion of performative\nrisk allows us to show that any PAC-learnable hypothesis space in the standard\nbinary classification setting remains PAC-learnable for the considered\nperformative scenarios. We also conduct an extensive experimental evaluation of\nour performative risk minimization method and showcase benefits on synthetic\nand real data.", "AI": {"tldr": "The paper explores the issue of performativity in machine learning, where shifts in test data distribution due to model deployment affect performance. Using the PAC learning framework, they present a performative risk minimization approach for binary classification.", "motivation": "The motivation is to address the problem of performative shifts\u2014changes in test data distribution caused by deployed models\u2014which lead to degraded performance in machine learning applications.", "method": "The authors propose a performative empirical risk function that accounts for the performative effects in data. They utilize this function in a PAC learning framework to minimize performative risks.", "result": "They show that any PAC-learnable hypothesis space remains learnable under performative effects. Experimental evaluations demonstrate the effectiveness of their proposed method on both synthetic and real datasets.", "conclusion": "Performative risk minimization can address shifts in test distribution effectively and ensures PAC learnability for binary classification in performative settings."}}
{"id": "2510.07600", "pdf": "https://arxiv.org/pdf/2510.07600", "abs": "https://arxiv.org/abs/2510.07600", "authors": ["Pouya Shiri", "Ramin Sharifi", "Amirali Baniasadi"], "title": "Quick-CapsNet (QCN): A fast alternative to Capsule Networks", "categories": ["cs.CV"], "comment": null, "summary": "The basic computational unit in Capsule Network (CapsNet) is a capsule (vs.\nneurons in Convolutional Neural Networks (CNNs)). A capsule is a set of\nneurons, which form a vector. CapsNet is used for supervised classification of\ndata and has achieved state-of-the-art accuracy on MNIST digit recognition\ndataset, outperforming conventional CNNs in detecting overlapping digits.\nMoreover, CapsNet shows higher robustness towards affine transformation when\ncompared to CNNs for MNIST datasets. One of the drawbacks of CapsNet, however,\nis slow training and testing. This can be a bottleneck for applications that\nrequire a fast network, especially during inference. In this work, we introduce\nQuick-CapsNet (QCN) as a fast alternative to CapsNet, which can be a starting\npoint to develop CapsNet for fast real-time applications. QCN builds on\nproducing a fewer number of capsules, which results in a faster network. QCN\nachieves this at the cost of marginal loss in accuracy. Inference is 5x faster\non MNIST, F-MNIST, SVHN and Cifar-10 datasets. We also further enhanced QCN by\nemploying a more powerful decoder instead of the default decoder to further\nimprove QCN.", "AI": {"tldr": "Quick-CapsNet (QCN) is introduced as a faster alternative to Capsule Network (CapsNet), achieving a 5x faster inference speed with minor compromise on accuracy.", "motivation": "Capsule Networks (CapsNet) outperform CNNs in certain tasks but suffer from slow training and testing, hindering their suitability for applications requiring rapid responses.", "method": "Quick-CapsNet reduces the number of capsules in the network and employs an enhanced decoder for improved efficiency and performance.", "result": "QCN maintains competitive accuracy while significantly improving speed, with 5x faster inference tested on datasets including MNIST, F-MNIST, SVHN, and CIFAR-10.", "conclusion": "QCN is a promising framework for real-time applications requiring high-speed inference with CapsNet-like functionality."}}
{"id": "2510.07473", "pdf": "https://arxiv.org/pdf/2510.07473", "abs": "https://arxiv.org/abs/2510.07473", "authors": ["Alex Kipnis", "Marcel Binz", "Eric Schulz"], "title": "metabeta -- A fast neural model for Bayesian mixed-effects regression", "categories": ["cs.LG", "stat.ML", "62J05, 62F15, 68T07", "I.2.6; G.3"], "comment": "19 pages, 9 main text, 8 figures", "summary": "Hierarchical data with multiple observations per group is ubiquitous in\nempirical sciences and is often analyzed using mixed-effects regression. In\nsuch models, Bayesian inference gives an estimate of uncertainty but is\nanalytically intractable and requires costly approximation using Markov Chain\nMonte Carlo (MCMC) methods. Neural posterior estimation shifts the bulk of\ncomputation from inference time to pre-training time, amortizing over simulated\ndatasets with known ground truth targets. We propose metabeta, a\ntransformer-based neural network model for Bayesian mixed-effects regression.\nUsing simulated and real data, we show that it reaches stable and comparable\nperformance to MCMC-based parameter estimation at a fraction of the usually\nrequired time.", "AI": {"tldr": "The paper introduces metabeta, a transformer-based neural network model designed to improve Bayesian mixed-effects regression efficiency by shifting computational burden to pre-training, achieving faster inference compared to MCMC methods.", "motivation": "Analyzing hierarchical data often relies on mixed-effects regression, with Bayesian inference providing uncertainty estimates. However, standard methods like MCMC are computationally expensive and intractable for many cases.", "method": "The authors propose metabeta, leveraging a transformer-based neural network architecture to conduct Bayesian mixed-effects regression. They use simulated and real datasets with known targets for pre-training instead of inference-time computation.", "result": "Metabeta achieves comparable accuracy to MCMC methods while significantly reducing computation time for parameter estimation.", "conclusion": "The paper validates metabeta as an efficient alternative to traditional computational methods for Bayesian mixed-effects regression, improving scalability and inference time handling."}}
{"id": "2510.07499", "pdf": "https://arxiv.org/pdf/2510.07499", "abs": "https://arxiv.org/abs/2510.07499", "authors": ["Soyeong Jeong", "Taehee Jung", "Sung Ju Hwang", "Joo-Kyung Kim", "Dongyeop Kang"], "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).", "AI": {"tldr": "The paper introduces Thought Template Augmented LCLMs (ToTAL) to enhance reasoning by structuring evidence integration in long-context language models.", "motivation": "Long-Context Language Models offer opportunities for multi-hop reasoning across extensive sets of documents but struggle to effectively connect evidence.", "method": "The authors propose using thought templates as reusable structures and rely on iterative refinement via natural language feedback to optimize reasoning efficiency.", "result": "Their approach consistently outperforms strong baselines across multiple benchmarks and enables reasoning transfer into smaller models.", "conclusion": "Thought templates improve reasoning processes in LCLMs, show broad applicability, and enhance transparent reasoning in both retrieval-based and retrieval-free contexts."}}
{"id": "2510.07517", "pdf": "https://arxiv.org/pdf/2510.07517", "abs": "https://arxiv.org/abs/2510.07517", "authors": ["Hyeong Kyu Choi", "Xiaojin Zhu", "Yixuan Li"], "title": "Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning\nby letting multiple agents exchange answers and then aggregate their opinions.\nYet recent studies reveal that agents are not neutral: they are prone to\nidentity-driven sycophancy and self-bias, uncritically adopting a peer's view\nor stubbornly adhering to their own prior output, undermining the reliability\nof debate. In this work, we present the first principled framework that joins\nsycophancy and self-bias to mitigate and quantify identity bias in MAD. First,\nwe formalize the debate dynamics as an identity-weighted Bayesian update\nprocess. Second, we propose response anonymization: by removing identity\nmarkers from prompts, agents cannot distinguish \"self\" from \"peer\", which\nforces equal weights on agent identity, thereby reducing bias. Third, we define\nthe Identity Bias Coefficient (IBC), a principled metric that measures how\noften an agent follows a peer versus itself. Empirical studies across multiple\nmodels, datasets and debate rounds confirm that identity bias is widespread,\nwith sycophancy far more common than self-bias. Our findings highlight the need\nto \"mask\" identity to ensure that MAD systems reason based on content rather\nthan source identity. Code is released in\nhttps://github.com/deeplearning-wisc/MAD-identity-bias.", "AI": {"tldr": "This paper introduces a framework to address identity-driven biases in multi-agent debate systems by anonymizing responses and quantifying bias through a metric.", "motivation": "The motivation is to improve reasoning in large language models during multi-agent debates by addressing issues of sycophancy and self-bias, which reduce debate reliability.", "method": "The authors formalize debate dynamics using an identity-weighted Bayesian update process, introduce response anonymization, and define the Identity Bias Coefficient (IBC) for bias measurement.", "result": "Empirical studies show widespread identity bias, with sycophancy being more prevalent than self-bias, across multiple models, datasets, and debate rounds.", "conclusion": "Masking agent identity ensures reasoning in MAD systems is based on content rather than source identity, enhancing debate fairness and reliability."}}
{"id": "2510.08139", "pdf": "https://arxiv.org/pdf/2510.08139", "abs": "https://arxiv.org/abs/2510.08139", "authors": ["Wenyang Jia", "Jingjing Wang", "Ziwei Yan", "Xiangli Peng", "Guohui Yuan"], "title": "BlockSDN: Towards a High-Performance Blockchain via Software-Defined Cross Networking optimization", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "The scalability of blockchain systems is constrained by inefficient P2P\nbroadcasting, as most existing optimizations focus only on the logical layer\nwithout considering physical network conditions. To address this, we propose\nBlockSDN, the first SDN-based integrated architecture for blockchain. BlockSDN\nemploys a distributed control plane for a global network view, a graph engine\nfor hierarchical clustering, and a hybrid macro-micro neighbor selection with\nhierarchical broadcasting. A dedicated simulation platform shows that BlockSDN\nreduces global block synchronization time by 65% and 55% compared to Gossip and\nMercury, respectively.These results highlight the potential of SDN-enabled\ncross-layer coordination to significantly enhance blockchain scalability and\nperformance.", "AI": {"tldr": "The paper introduces BlockSDN, an SDN-integrated architecture for blockchain, which significantly reduces block synchronization time.", "motivation": "To address the scalability limits of blockchain systems caused by inefficient P2P broadcasting, particularly those overlooked at the physical network layer.", "method": "BlockSDN integrates an SDN-based design, using a distributed control plane for network oversight, a graph engine for clustering, and a hybrid neighbor selection mechanism with hierarchical broadcasting.", "result": "BlockSDN achieves a 65% and 55% reduction in global block synchronization time compared to Gossip and Mercury approaches, respectively, as per simulations.", "conclusion": "The findings demonstrate that SDN-enabled cross-layer coordination can significantly improve blockchain scalability and performance."}}
{"id": "2510.07778", "pdf": "https://arxiv.org/pdf/2510.07778", "abs": "https://arxiv.org/abs/2510.07778", "authors": ["Yandu Chen", "Kefan Gu", "Yuqing Wen", "Yucheng Zhao", "Tiancai Wang", "Liqiang Nie"], "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models leverage pretrained vision-language\nmodels (VLMs) to couple perception with robotic control, offering a promising\npath toward general-purpose embodied intelligence. However, current SOTA VLAs\nare primarily pretrained on multimodal tasks with limited relevance to embodied\nscenarios, and then finetuned to map explicit instructions to actions.\nConsequently, due to the lack of reasoning-intensive pretraining and\nreasoning-guided manipulation, these models are unable to perform implicit\nhuman intention reasoning required for complex, real-world interactions. To\novercome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework\nwith a curriculum training paradigm and an efficient inference mechanism. Our\nproposed method first leverages carefully designed reasoning data that combine\nintention inference, spatial grounding, and compact embodied reasoning,\nendowing the model with both reasoning and perception capabilities. In the\nfollowing finetuning stage, IntentionVLA employs the compact reasoning outputs\nas contextual guidance for action generation, enabling fast inference under\nindirect instructions. Experimental results show that IntentionVLA\nsubstantially outperforms $\\pi_0$, achieving 18\\% higher success rates with\ndirect instructions and 28\\% higher than ECoT under intention instructions. On\nout-of-distribution intention tasks, IntentionVLA achieves over twice the\nsuccess rate of all baselines, and further enables zero-shot human-robot\ninteraction with 40\\% success rate. These results highlight IntentionVLA as a\npromising paradigm for next-generation human-robot interaction (HRI) systems.", "AI": {"tldr": "The study introduces IntentionVLA, a framework enhancing Vision-Language-Action models for reasoning-intensive human-robot interactions, achieving superior success rates in various scenarios compared to existing baselines.", "motivation": "Current Vision-Language-Action models lack reasoning capabilities essential for implicit human intention inference in complex real-world scenarios.", "method": "IntentionVLA uses curriculum training with reasoning data for intention inference, spatial grounding, and embodied reasoning, followed by fine-tuning to use reasoning outputs for action generation.", "result": "IntentionVLA showed improved performance: 18% higher success with direct instructions, 28% higher with intention-based tasks, doubling success rate on out-of-distribution tasks, and achieving 40% success in zero-shot human-robot interaction.", "conclusion": "IntentionVLA presents a significant advancement for human-robot interaction systems by integrating reasoning-intensive mechanisms to enable robust embodied intelligence across varying instructions and scenarios."}}
{"id": "2510.08409", "pdf": "https://arxiv.org/pdf/2510.08409", "abs": "https://arxiv.org/abs/2510.08409", "authors": ["Yu-Han Wu", "Quentin Berthet", "G\u00e9rard Biau", "Claire Boyer", "Romuald Elie", "Pierre Marion"], "title": "Optimal Stopping in Latent Diffusion Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We identify and analyze a surprising phenomenon of Latent Diffusion Models\n(LDMs) where the final steps of the diffusion can degrade sample quality. In\ncontrast to conventional arguments that justify early stopping for numerical\nstability, this phenomenon is intrinsic to the dimensionality reduction in\nLDMs. We provide a principled explanation by analyzing the interaction between\nlatent dimension and stopping time. Under a Gaussian framework with linear\nautoencoders, we characterize the conditions under which early stopping is\nneeded to minimize the distance between generated and target distributions.\nMore precisely, we show that lower-dimensional representations benefit from\nearlier termination, whereas higher-dimensional latent spaces require later\nstopping time. We further establish that the latent dimension interplays with\nother hyperparameters of the problem such as constraints in the parameters of\nscore matching. Experiments on synthetic and real datasets illustrate these\nproperties, underlining that early stopping can improve generative quality.\nTogether, our results offer a theoretical foundation for understanding how the\nlatent dimension influences the sample quality, and highlight stopping time as\na key hyperparameter in LDMs.", "AI": {"tldr": "The paper identifies issues in Latent Diffusion Models where final diffusion steps degrade sample quality. Lower latent dimensions benefit from early stopping, while higher ones require later termination.", "motivation": "Understanding the degradation in sample quality caused by final steps in Latent Diffusion Models. Offers deeper insights into early stopping and its impact on generative quality.", "method": "Analyzed Gaussian framework using linear autoencoders to characterize latent dimension-stopping time interaction. Conducted experiments with synthetic and real datasets to validate findings.", "result": "Lower-dimensional latent spaces benefit from earlier diffusion stopping, while high-dimensional latent spaces require later stopping. Established latent dimension interplays with hyperparameters.", "conclusion": "Early stopping is a crucial hyperparameter for improving sample quality in latent diffusion models, influenced by latent dimension and other factors."}}
{"id": "2510.07631", "pdf": "https://arxiv.org/pdf/2510.07631", "abs": "https://arxiv.org/abs/2510.07631", "authors": ["Shreshth Saini", "Shashank Gupta", "Alan C. Bovik"], "title": "Rectified-CFG++ for Flow Based Models", "categories": ["cs.CV"], "comment": "Accepted at NeurIPS 2025", "summary": "Classifier-free guidance (CFG) is the workhorse for steering large diffusion\nmodels toward text-conditioned targets, yet its native application to rectified\nflow (RF) based models provokes severe off-manifold drift, yielding visual\nartifacts, text misalignment, and brittle behaviour. We present\nRectified-CFG++, an adaptive predictor-corrector guidance that couples the\ndeterministic efficiency of rectified flows with a geometry-aware conditioning\nrule. Each inference step first executes a conditional RF update that anchors\nthe sample near the learned transport path, then applies a weighted conditional\ncorrection that interpolates between conditional and unconditional velocity\nfields. We prove that the resulting velocity field is marginally consistent and\nthat its trajectories remain within a bounded tubular neighbourhood of the data\nmanifold, ensuring stability across a wide range of guidance strengths.\nExtensive experiments on large-scale text-to-image models (Flux, Stable\nDiffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms\nstandard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and\nT2I-CompBench. Project page: https://rectified-cfgpp.github.io/", "AI": {"tldr": "Rectified-CFG++ improves text-to-image generation using adaptive guidance that combines rectified flow efficiency with geometry-aware conditioning.", "motivation": "The motivation is to address severe off-manifold drift and artifacts caused by applying standard Classifier-Free Guidance (CFG) to rectified flow (RF) based models.", "method": "The approach uses adaptive predictor-corrector guidance with conditional RF updates followed by weighted corrections to interpolate between velocity fields.", "result": "Rectified-CFG++ demonstrates superior performance compared to standard CFG on large-scale text-to-image models across various benchmarks.", "conclusion": "This method ensures stability and better results for text-conditioned inference in diffusion models, overcoming limitations of standard CFG."}}
{"id": "2510.07474", "pdf": "https://arxiv.org/pdf/2510.07474", "abs": "https://arxiv.org/abs/2510.07474", "authors": ["Shaan Pakala", "Aldair E. Gongora", "Brian Giera", "Evangelos E. Papalexakis"], "title": "Surrogate Modeling for the Design of Optimal Lattice Structures using Tensor Completion", "categories": ["cs.LG"], "comment": "NeurIPS 2025 AI4Mat Workshop", "summary": "When designing new materials, it is often necessary to design a material with\nspecific desired properties. Unfortunately, as new design variables are added,\nthe search space grows exponentially, which makes synthesizing and validating\nthe properties of each material very impractical and time-consuming. In this\nwork, we focus on the design of optimal lattice structures with regard to\nmechanical performance. Computational approaches, including the use of machine\nlearning (ML) methods, have shown improved success in accelerating materials\ndesign. However, these ML methods are still lacking in scenarios when training\ndata (i.e. experimentally validated materials) come from a non-uniformly random\nsampling across the design space. For example, an experimentalist might\nsynthesize and validate certain materials more frequently because of\nconvenience. For this reason, we suggest the use of tensor completion as a\nsurrogate model to accelerate the design of materials in these atypical\nsupervised learning scenarios. In our experiments, we show that tensor\ncompletion is superior to classic ML methods such as Gaussian Process and\nXGBoost with biased sampling of the search space, with around 5\\% increased\n$R^2$. Furthermore, tensor completion still gives comparable performance with a\nuniformly random sampling of the entire search space.", "AI": {"tldr": "This paper proposes using tensor completion as a surrogate model to enhance material design, particularly in scenarios with non-uniform training data, achieving better or comparable performance to traditional ML methods.", "motivation": "The motivation is to address the challenges in designing materials with specific properties due to the exponential growth of the search space and the limitations of existing ML methods when data are biased or non-uniformly sampled.", "method": "The authors propose using tensor completion as a surrogate modeling approach to overcome data bias in supervised learning. They compare its performance against classical ML methods like Gaussian Process and XGBoost.", "result": "Tensor completion outperforms traditional ML methods with a 5% increase in performance ($R^2$) in biased sampling scenarios and gives comparable results in uniformly sampled scenarios.", "conclusion": "The study concludes that tensor completion is a robust alternative for accelerating the material design process, especially in atypical data conditions."}}
{"id": "2510.07520", "pdf": "https://arxiv.org/pdf/2510.07520", "abs": "https://arxiv.org/abs/2510.07520", "authors": ["Rayyan Merchant", "Kevin Tang"], "title": "ParsTranslit: Truly Versatile Tajik-Farsi Transliteration", "categories": ["cs.CL"], "comment": null, "summary": "As a digraphic language, the Persian language utilizes two written standards:\nPerso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite\nthe significant similarity between the dialects of each country, script\ndifferences prevent simple one-to-one mapping, hindering written communication\nand interaction between Tajikistan and its Persian-speaking ``siblings''. To\novercome this, previously-published efforts have investigated machine\ntransliteration models to convert between the two scripts. Unfortunately, most\nefforts did not use datasets other than those they created, limiting these\nmodels to certain domains of text such as archaic poetry or word lists. A truly\nusable transliteration system must be capable of handling varied domains,\nmeaning that suck models lack the versatility required for real-world usage.\nThe contrast in domain between data also obscures the task's true difficulty.\nWe present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi\ntransliteration trained across all available datasets, and present two datasets\nof our own. Our results across domains provide clearer understanding of the\ntask, and set comprehensive comparable leading benchmarks. Overall, our model\nachieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik\nand 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available\nat https://anonymous.4open.science/r/ParsTranslit-FB30/.", "AI": {"tldr": "The paper introduces a new state-of-the-art sequence-to-sequence model for transliterating between Perso-Arabic and Tajik-Cyrillic scripts, supported by diverse datasets and achieving high performance metrics.", "motivation": "The paper addresses the transliteration challenge between Perso-Arabic and Tajik-Cyrillic scripts, aiming to bridge communication barriers among Persian-speaking communities due to script differences.", "method": "The authors developed a sequence-to-sequence model trained on all available datasets and two newly curated datasets to enhance transliteration across varied domains.", "result": "The model achieves superior chrF++ and Normalized CER scores: 87.91 and 0.05 for Farsi-to-Tajik, and 92.28 and 0.04 for Tajik-to-Farsi.", "conclusion": "This work sets new benchmarks, improves understanding of domain variability in transliteration tasks, and makes the model, datasets, and code publicly available for broader applications."}}
{"id": "2510.07551", "pdf": "https://arxiv.org/pdf/2510.07551", "abs": "https://arxiv.org/abs/2510.07551", "authors": ["Harshit Rajgarhia", "Suryam Gupta", "Asif Shaik", "Gulipalli Praveen Kumar", "Y Santhoshraj", "Sanka Nithya Tanvy Nishitha", "Abhishek Mukherji"], "title": "An Evaluation Study of Hybrid Methods for Multilingual PII Detection", "categories": ["cs.AI"], "comment": null, "summary": "The detection of Personally Identifiable Information (PII) is critical for\nprivacy compliance but remains challenging in low-resource languages due to\nlinguistic diversity and limited annotated data. We present RECAP, a hybrid\nframework that combines deterministic regular expressions with context-aware\nlarge language models (LLMs) for scalable PII detection across 13 low-resource\nlocales. RECAP's modular design supports over 300 entity types without\nretraining, using a three-phase refinement pipeline for disambiguation and\nfiltering. Benchmarked with nervaluate, our system outperforms fine-tuned NER\nmodels by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers\na scalable and adaptable solution for efficient PII detection in\ncompliance-focused applications.", "AI": {"tldr": "The paper introduces RECAP, a hybrid framework combining deterministic regular expressions and large language models (LLMs) for efficient personally identifiable information (PII) detection, particularly in low-resource languages.", "motivation": "PII detection is essential for privacy compliance but challenging in low-resource languages due to limited annotated data and linguistic diversity.", "method": "The RECAP framework employs a hybrid approach combining regular expressions and context-aware large language models in a modular, three-phase refinement pipeline for disambiguation and filtering.", "result": "RECAP outperforms fine-tuned Named Entity Recognition (NER) models by 82% and zero-shot LLMs by 17% in weighted F1-score, ensuring better scalability and adaptation for PII detection.", "conclusion": "RECAP provides an efficient, scalable, and adaptable solution for privacy compliance applications, addressing limitations in low-resource language contexts."}}
{"id": "2510.08469", "pdf": "https://arxiv.org/pdf/2510.08469", "abs": "https://arxiv.org/abs/2510.08469", "authors": ["Neer Patel", "Anish Giri", "Hrushikesh Pramod Patil", "Noah Siekierski", "Avimita Chatterjee", "Sonika Johri", "Timothy Proctor", "Thomas Lubinski", "Siyuan Niu"], "title": "Platform-Agnostic Modular Architecture for Quantum Benchmarking", "categories": ["quant-ph", "cs.AI", "cs.SE"], "comment": null, "summary": "We present a platform-agnostic modular architecture that addresses the\nincreasingly fragmented landscape of quantum computing benchmarking by\ndecoupling problem generation, circuit execution, and results analysis into\nindependent, interoperable components. Supporting over 20 benchmark variants\nranging from simple algorithmic tests like Bernstein-Vazirani to complex\nHamiltonian simulation with observable calculations, the system integrates with\nmultiple circuit generation APIs (Qiskit, CUDA-Q, Cirq) and enables diverse\nworkflows. We validate the architecture through successful integration with\nSandia's $\\textit{pyGSTi}$ for advanced circuit analysis and CUDA-Q for\nmulti-GPU HPC simulations. Extensibility of the system is demonstrated by\nimplementing dynamic circuit variants of existing benchmarks and a new quantum\nreinforcement learning benchmark, which become readily available across\nmultiple execution and analysis modes. Our primary contribution is identifying\nand formalizing modular interfaces that enable interoperability between\nincompatible benchmarking frameworks, demonstrating that standardized\ninterfaces reduce ecosystem fragmentation while preserving optimization\nflexibility. This architecture has been developed as a key enhancement to the\ncontinually evolving QED-C Application-Oriented Performance Benchmarks for\nQuantum Computing suite.", "AI": {"tldr": "The paper introduces a modular architecture that enhances quantum computing benchmarking by separating problem generation, circuit execution, and results analysis into interoperable components. It supports diverse benchmarks and integrates with multiple APIs, validating its flexibility and extensibility.", "motivation": "The increasing fragmentation in quantum computing benchmarking highlights the need for a standardized and interoperable system to reduce inefficiencies and enable diverse workflows across incompatible frameworks.", "method": "They designed a platform-agnostic modular architecture that integrates several benchmarking variants and APIs (e.g., Qiskit, CUDA-Q, Cirq), validated its extensibility via new benchmarks, and demonstrated interoperability through integration with tools like pyGSTi and CUDA-Q.", "result": "The architecture successfully reduced ecosystem fragmentation, supported 20+ benchmarks, integrated with various frameworks, and introduced new benchmarks like quantum reinforcement learning. It validated its flexibility through real-world implementations.", "conclusion": "Standardized modular interfaces enable a more unified quantum computing benchmarking ecosystem, preserving optimization flexibility while enhancing interoperability. This framework is a significant enhancement to QED-C's benchmarking suite."}}
{"id": "2510.07807", "pdf": "https://arxiv.org/pdf/2510.07807", "abs": "https://arxiv.org/abs/2510.07807", "authors": ["Grace Cai", "Nithin Parepally", "Laura Zheng", "Ming C. Lin"], "title": "GM3: A General Physical Model for Micro-Mobility Vehicles", "categories": ["cs.RO"], "comment": null, "summary": "Modeling the dynamics of micro-mobility vehicles (MMV) is becoming\nincreasingly important for training autonomous vehicle systems and building\nurban traffic simulations. However, mainstream tools rely on variants of the\nKinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,\nload transfer, and rider/vehicle lean. To our knowledge, no unified,\nphysics-based model captures these dynamics across the full range of common\nMMVs and wheel layouts. We propose the \"Generalized Micro-mobility Model\"\n(GM3), a tire-level formulation based on the tire brush representation that\nsupports arbitrary wheel configurations, including single/double track and\nmulti-wheel platforms. We introduce an interactive model-agnostic simulation\nframework that decouples vehicle/layout specification from dynamics to compare\nthe GM3 with the KBM and other models, consisting of fixed step RK4\nintegration, human-in-the-loop and scripted control, real-time trajectory\ntraces and logging for analysis. We also empirically validate the GM3 on the\nStanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and\ncart classes.", "AI": {"tldr": "The paper introduces a new physics-based model for micro-mobility vehicles (MMVs) called the Generalized Micro-Mobility Model (GM3) to improve simulation accuracy compared to traditional models like the Kinematic Bicycle Model (KBM).", "motivation": "Current tools for modeling MMVs are limited and fail to properly account for various physical dynamics such as tire slip, load transfer, and lean dynamics, especially across diverse vehicle configurations.", "method": "The proposed GM3 model uses a tire-level formulation based on tire brush representation to accommodate arbitrary wheel layouts. Additionally, an interactive simulation framework is developed for comparing models and validating results.", "result": "The GM3 was validated using real-world data from the Stanford Drone Dataset, specifically in scenarios involving bikers, skaters, and carts, demonstrating its improved realism in capturing physical dynamics.", "conclusion": "The GM3 represents a significant advancement in modeling MMVs, providing a unified and physics-accurate framework that enables better simulations and applications in urban traffic and autonomous vehicle systems."}}
{"id": "2510.08465", "pdf": "https://arxiv.org/pdf/2510.08465", "abs": "https://arxiv.org/abs/2510.08465", "authors": ["Chih-Yu Chang", "Ming-Chung Chang"], "title": "Accelerated Aggregated D-Optimal Designs for Estimating Main Effects in Black-Box Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Recent advances in supervised learning have driven growing interest in\nexplaining black-box models, particularly by estimating the effects of input\nvariables on model predictions. However, existing approaches often face key\nlimitations, including poor scalability, sensitivity to out-of-distribution\nsampling, and instability under correlated features. To address these issues,\nwe propose A2D2E, an $\\textbf{E}$stimator based on $\\textbf{A}$ccelerated\n$\\textbf{A}$ggregated $\\textbf{D}$-Optimal $\\textbf{D}$esigns. Our method\nleverages principled experimental design to improve efficiency and robustness\nin main effect estimation. We establish theoretical guarantees, including\nconvergence and variance reduction, and validate A2D2E through extensive\nsimulations. We further provide the potential of the proposed method with a\ncase study on real data and applications in language models. The code to\nreproduce the results can be found at https://github.com/cchihyu/A2D2E.", "AI": {"tldr": "The paper introduces A2D2E, a method designed to address limitations in explaining black-box models by leveraging experimental design principles for efficient and robust estimation of input variable effects on predictions.", "motivation": "Explaining black-box model behaviors is challenging due to issues like scalability, sensitivity to sampling, and instability under feature correlations.", "method": "The authors propose A2D2E, an estimator using Accelerated Aggregated D-Optimal Designs, supported by theoretical guarantees and experimental validation.", "result": "The method demonstrates improved efficiency, robustness, and variance reduction through simulations, with real data case studies showcasing its applicability.", "conclusion": "A2D2E improves main effect estimation for black-box models while addressing common limitations, with promising applications in language models."}}
{"id": "2510.07636", "pdf": "https://arxiv.org/pdf/2510.07636", "abs": "https://arxiv.org/abs/2510.07636", "authors": ["Shashank Gupta", "Gregoire Phillips", "Alan C. Bovik"], "title": "PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment", "categories": ["cs.CV"], "comment": "Oral presentation at ICIP 2025", "summary": "Large Multimodal Models (LMMs) have recently enabled considerable advances in\nthe realm of image and video quality assessment, but this progress has yet to\nbe fully explored in the domain of 3D assets. We are interested in using these\nmodels to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where\nthe aim is to automatically evaluate the perceptual quality of a point cloud in\nabsence of a reference. We begin with the observation that different modalities\nof data - text descriptions, 2D projections, and 3D point cloud views - provide\ncomplementary information about point cloud quality. We then construct PIT-QMM,\na novel LMM for NR-PCQA that is capable of consuming text, images and point\nclouds end-to-end to predict quality scores. Extensive experimentation shows\nthat our proposed method outperforms the state-of-the-art by significant\nmargins on popular benchmarks with fewer training iterations. We also\ndemonstrate that our framework enables distortion localization and\nidentification, which paves a new way forward for model explainability and\ninteractivity. Code and datasets are available at\nhttps://www.github.com/shngt/pit-qmm.", "AI": {"tldr": "This paper introduces PIT-QMM, a novel large multimodal model for no-reference point cloud quality assessment (NR-PCQA), leveraging diverse data modalities to set a new performance benchmark and enhance explainability.", "motivation": "To leverage the advancements of large multimodal models for improving the quality assessment of 3D assets, specifically in scenarios without a reference point cloud.", "method": "The authors combine text descriptions, 2D projections, and 3D point cloud views within PIT-QMM, a multimodal model designed to predict perceptual quality of point clouds end-to-end. They validated its performance with extensive experimentation.", "result": "PIT-QMM significantly outperforms state-of-the-art methods on popular benchmarks and achieves this with fewer training iterations. Additionally, it enables distortion localization and identification.", "conclusion": "PIT-QMM marks a significant step forward in NR-PCQA by achieving better performance and introducing explainability and interactivity capabilities for 3D quality assessments, setting new standards in the field."}}
{"id": "2510.07477", "pdf": "https://arxiv.org/pdf/2510.07477", "abs": "https://arxiv.org/abs/2510.07477", "authors": ["Maria Mahbub", "Robert J. Klein", "Myvizhi Esai Selvan", "Rowena Yip", "Claudia Henschke", "Providencia Morales", "Ian Goethert", "Olivera Kotevska", "Mayanka Chandra Shekar", "Sean R. Wilkinson", "Eileen McAllister", "Samuel M. Aguayo", "Zeynep H. G\u00fcm\u00fc\u015f", "Ioana Danciu", "VA Million Veteran Program"], "title": "HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 6 figures, 3 tables", "summary": "Lung cancer (LC) is the third most common cancer and the leading cause of\ncancer deaths in the US. Although smoking is the primary risk factor, the\noccurrence of LC in never-smokers and familial aggregation studies highlight a\ngenetic component. Genetic biomarkers identified through genome-wide\nassociation studies (GWAS) are promising tools for assessing LC risk. We\nintroduce HEMERA (Human-Explainable Transformer Model for Estimating Lung\nCancer Risk using GWAS Data), a new framework that applies explainable\ntransformer-based deep learning to GWAS data of single nucleotide polymorphisms\n(SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly\nprocesses raw genotype data without clinical covariates, introducing additive\npositional encodings, neural genotype embeddings, and refined variant\nfiltering. A post hoc explainability module based on Layer-wise Integrated\nGradients enables attribution of model predictions to specific SNPs, aligning\nstrongly with known LC risk loci. Trained on data from 27,254 Million Veteran\nProgram participants, HEMERA achieved >99% AUC (area under receiver\ncharacteristics) score. These findings support transparent,\nhypothesis-generating models for personalized LC risk assessment and early\nintervention.", "AI": {"tldr": "The study introduces HEMERA, an explainable transformer-based deep learning framework that uses GWAS data to predict lung cancer risk with high accuracy (>99% AUC).", "motivation": "To advance lung cancer risk assessment through genetic biomarkers, addressing the limitations of current approaches by employing a novel transformer-based deep learning model.", "method": "HEMERA processes raw genotype data (GWAS data) without clinical covariates, incorporating additive positional encodings, neural genotype embeddings, and refined variant filtering. Layer-wise Integrated Gradients are used to attribute predictions to specific SNPs.", "result": "HEMERA achieved excellent predictive performance with an AUC score exceeding 99%, affirming its reliability in identifying LC risk loci.", "conclusion": "HEMERA offers a promising, transparent model for personalized lung cancer risk assessment, enabling early intervention and deeper insights into genetic predispositions."}}
{"id": "2510.07535", "pdf": "https://arxiv.org/pdf/2510.07535", "abs": "https://arxiv.org/abs/2510.07535", "authors": ["Jaeseong Lee", "seung-won hwang", "Aurick Qiao", "Gabriele Oliaro", "Ye Wang", "Samyam Rajbhandari"], "title": "OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding promises faster inference for large language models\n(LLMs), yet existing methods fail to generalize to real-world settings.\nBenchmarks typically assume short contexts (e.g., 2K tokens), whereas practical\nworkloads involve long contexts. We find current approaches degrade severely\nwith long contexts; for instance, EAGLE3 even slows down the generation speed\nby 0.81x. We address these limitations by releasing a new long-context\nbenchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves\nabout 5x higher acceptance length than EAGLE3 on long-context inputs through\nthree innovations: (1) an LSTM-based drafter conditioned only on the last-token\nstate, making it generalize to various lengths, (2) a special token [SPEC] in\nthe verifier that produces richer representation for drafter, and (3) a hybrid\nalgorithm combining both tree and non-tree decoding methods. We release all\ncode and datasets to advance future research.", "AI": {"tldr": "This paper introduces OWL, a novel speculative decoding model for large language models, designed to handle long-context inputs better than existing methods.", "motivation": "Address the performance degradation of current speculative decoding methods in real-world scenarios with long-context workloads.", "method": "The authors present OWL, featuring an LSTM-based drafter, a [SPEC] token for richer verifier representation, and a hybrid decoding algorithm, supported by the new LongSpecBench benchmark.", "result": "OWL achieves a 5x higher acceptance length than EAGLE3 on long-context inputs, addressing existing limitations.", "conclusion": "OWL offers a significant improvement in speculative decoding for long contexts, pushing forward the capabilities of large language models in practical applications."}}
{"id": "2510.07575", "pdf": "https://arxiv.org/pdf/2510.07575", "abs": "https://arxiv.org/abs/2510.07575", "authors": ["Zerui Cheng", "Stella Wohnig", "Ruchika Gupta", "Samiul Alam", "Tassallah Abdullahi", "Jo\u00e3o Alves Ribeiro", "Christian Nielsen-Garcia", "Saif Mir", "Siran Li", "Jason Orender", "Seyed Ali Bahrainian", "Daniel Kirste", "Aaron Gokaslan", "Miko\u0142aj Glinka", "Carsten Eickhoff", "Ruben Wolff"], "title": "Benchmarking is Broken -- Don't Let AI be its Own Judge", "categories": ["cs.AI", "cs.LG"], "comment": "12 pages; Accepted to NeurIPS 2025. Link to poster:\n  https://neurips.cc/virtual/2025/poster/121919", "summary": "The meteoric rise of Artificial Intelligence (AI), with its rapidly expanding\nmarket capitalization, presents both transformative opportunities and critical\nchallenges. Chief among these is the urgent need for a new, unified paradigm\nfor trustworthy evaluation, as current benchmarks increasingly reveal critical\nvulnerabilities. Issues like data contamination and selective reporting by\nmodel developers fuel hype, while inadequate data quality control can lead to\nbiased evaluations that, even if unintentionally, may favor specific\napproaches. As a flood of participants enters the AI space, this \"Wild West\" of\nassessment makes distinguishing genuine progress from exaggerated claims\nexceptionally difficult. Such ambiguity blurs scientific signals and erodes\npublic confidence, much as unchecked claims would destabilize financial markets\nreliant on credible oversight from agencies like Moody's.\n  In high-stakes human examinations (e.g., SAT, GRE), substantial effort is\ndevoted to ensuring fairness and credibility; why settle for less in evaluating\nAI, especially given its profound societal impact? This position paper argues\nthat the current laissez-faire approach is unsustainable. We contend that true,\nsustainable AI advancement demands a paradigm shift: a unified, live, and\nquality-controlled benchmarking framework robust by construction, not by mere\ncourtesy and goodwill. To this end, we dissect the systemic flaws undermining\ntoday's AI evaluation, distill the essential requirements for a new generation\nof assessments, and introduce PeerBench, a community-governed, proctored\nevaluation blueprint that embodies this paradigm through sealed execution, item\nbanking with rolling renewal, and delayed transparency. Our goal is to pave the\nway for evaluations that can restore integrity and deliver genuinely\ntrustworthy measures of AI progress.", "AI": {"tldr": "Current methods for evaluating AI systems are plagued by issues like data contamination and bias, leading to overinflated claims and reduced public trust. A unified, quality-controlled benchmarking framework is proposed, named PeerBench, to tackle these challenges.", "motivation": "The paper is motivated by the need to move beyond flawed, untrustworthy methods of assessing AI systems, which result in hyped claims and eroding public trust in the technology.", "method": "The authors propose PeerBench, a community-governed evaluation framework that incorporates sealed execution, rolling renewal of item banking, proctoring, and delayed transparency principles to ensure fairness and robustness.", "result": "PeerBench is introduced as a blueprint for reliable, integrity-focused AI assessment, aimed at addressing systemic flaws in current benchmarking approaches.", "conclusion": "The paper argues that a paradigm shift in AI benchmarking is essential, advocating for PeerBench as a sustainable solution to restore credibility and ensure trustworthy evaluations in the field."}}
{"id": "2510.08522", "pdf": "https://arxiv.org/pdf/2510.08522", "abs": "https://arxiv.org/abs/2510.08522", "authors": ["Yuanjun Dai", "Keqiang He", "An Wang"], "title": "DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed Machine Learning Systems", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Existing batch size selection approaches in distributed machine learning rely\non static allocation or simplistic heuristics that fail to adapt to\nheterogeneous, dynamic computing environments. We present DYNAMIX, a\nreinforcement learning framework that formulates batch size optimization as a\nsequential decision-making problem using Proximal Policy Optimization (PPO).\nOur approach employs a multi-dimensional state representation encompassing\nnetwork-level metrics, system-level resource utilization, and training\nstatistical efficiency indicators to enable informed decision-making across\ndiverse computational resources. Our approach eliminates the need for explicit\nsystem modeling while integrating seamlessly with existing distributed training\nframeworks. Through evaluations across diverse workloads, hardware\nconfigurations, and network conditions, DYNAMIX achieves up to 6.3% improvement\nin the final model accuracy and 46% reduction in the total training time. Our\nscalability experiments demonstrate that DYNAMIX maintains the best performance\nas cluster size increases to 32 nodes, while policy transfer experiments show\nthat learned policies generalize effectively across related model\narchitectures.", "AI": {"tldr": "DYNAMIX applies reinforcement learning to optimize batch sizes in distributed machine learning environments, improving training efficiency and scalability.", "motivation": "Current batch size selection methods lack adaptability in dynamic and heterogeneous distributed systems.", "method": "The paper uses Proximal Policy Optimization (PPO) in a reinforcement learning framework with multi-dimensional state representation.", "result": "DYNAMIX improves model accuracy by up to 6.3% and reduces training time by 46%, maintaining scalability across cluster sizes.", "conclusion": "DYNAMIX provides a robust, generalizable approach for batch size optimization, outperforming existing static methods and heuristics."}}
{"id": "2510.07865", "pdf": "https://arxiv.org/pdf/2510.07865", "abs": "https://arxiv.org/abs/2510.07865", "authors": ["Guowei Zou", "Haitao Wang", "Hejun Wu", "Yukun Qian", "Yuhang Wang", "Weibing Li"], "title": "DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "Website with code: https://guowei-zou.github.io/dm1/", "summary": "The ability to learn multi-modal action distributions is indispensable for\nrobotic manipulation policies to perform precise and robust control. Flow-based\ngenerative models have recently emerged as a promising solution to learning\ndistributions of actions, offering one-step action generation and thus\nachieving much higher sampling efficiency compared to diffusion-based methods.\nHowever, existing flow-based policies suffer from representation collapse, the\ninability to distinguish similar visual representations, leading to failures in\nprecise manipulation tasks. We propose DM1 (MeanFlow with Dispersive\nRegularization for One-Step Robotic Manipulation), a novel flow matching\nframework that integrates dispersive regularization into MeanFlow to prevent\ncollapse while maintaining one-step efficiency. DM1 employs multiple dispersive\nregularization variants across different intermediate embedding layers,\nencouraging diverse representations across training batches without introducing\nadditional network modules or specialized training procedures. Experiments on\nRoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s\nvs. 2-3.5s) and improves success rates by 10-20 percentage points, with the\nLift task reaching 99% success over 85% of the baseline. Real-robot deployment\non a Franka Panda further validates that DM1 transfers effectively from\nsimulation to the physical world. To the best of our knowledge, this is the\nfirst work to leverage representation regularization to enable flow-based\npolicies to achieve strong performance in robotic manipulation, establishing a\nsimple yet powerful approach for efficient and robust manipulation.", "AI": {"tldr": "DM1 framework enhances the precision and efficiency of robotic manipulation policies through a novel flow matching approach, significantly reducing inference time and improving task success rates.", "motivation": "The paper aims to address representation collapse in flow-based generative models used for robotic manipulation, which leads to failures in tasks requiring precise control.", "method": "DM1 introduces dispersive regularization into the MeanFlow framework, encouraging diverse visual representations across training batches while maintaining efficiency without additional network modules or specialized procedures.", "result": "DM1 shows a 20-40x faster inference speed and improves success rates by 10-20% in RoboMimic benchmarks, with real-world deployment successfully transferring from simulation.", "conclusion": "DM1 demonstrates that representation regularization can overcome limitations of flow-based policies, establishing an efficient and robust solution for robotic manipulation tasks."}}
{"id": "2510.08535", "pdf": "https://arxiv.org/pdf/2510.08535", "abs": "https://arxiv.org/abs/2510.08535", "authors": ["Tassilo Schwarz", "Cai Dieball", "Constantin Kogler", "Kevin Lam", "Renaud Lambiotte", "Arnaud Doucet", "Alja\u017e Godec", "George Deligiannidis"], "title": "Permutation-Invariant Spectral Learning via Dyson Diffusion", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "Diffusion models are central to generative modeling and have been adapted to\ngraphs by diffusing adjacency matrix representations. The challenge of having\nup to $n!$ such representations for graphs with $n$ nodes is only partially\nmitigated by using permutation-equivariant learning architectures. Despite\ntheir computational efficiency, existing graph diffusion models struggle to\ndistinguish certain graph families, unless graph data are augmented with ad hoc\nfeatures. This shortcoming stems from enforcing the inductive bias within the\nlearning architecture. In this work, we leverage random matrix theory to\nanalytically extract the spectral properties of the diffusion process, allowing\nus to push the inductive bias from the architecture into the dynamics. Building\non this, we introduce the Dyson Diffusion Model, which employs Dyson's Brownian\nMotion to capture the spectral dynamics of an Ornstein-Uhlenbeck process on the\nadjacency matrix while retaining all non-spectral information. We demonstrate\nthat the Dyson Diffusion Model learns graph spectra accurately and outperforms\nexisting graph diffusion models.", "AI": {"tldr": "The paper proposes the Dyson Diffusion Model that uses Dyson's Brownian Motion to enhance generative modeling for graphs by focusing on spectral dynamics instead of relying solely on inductive bias in architectures.", "motivation": "The motivation is to overcome the limitations of existing graph diffusion models, which struggle to distinguish graph families without additional ad hoc features due to reliance on the learning architecture's inductive bias.", "method": "The authors use random matrix theory to derive spectral properties of the diffusion process and introduce the Dyson Diffusion Model. This model employs Dyson's Brownian Motion to focus on spectral dynamics while preserving other graph information.", "result": "The Dyson Diffusion Model accurately learns graph spectra and achieves better performance compared to existing graph diffusion models.", "conclusion": "By shifting the inductive bias from the architecture to the dynamics, the proposed Dyson Diffusion Model offers a more effective approach to generative modeling for graphs, addressing key limitations of prior methods."}}
{"id": "2510.07652", "pdf": "https://arxiv.org/pdf/2510.07652", "abs": "https://arxiv.org/abs/2510.07652", "authors": ["Harshala Gammulle", "Clinton Fookes", "Sridha Sridharan", "Simon Denman"], "title": "Dual-Stream Alignment for Action Segmentation", "categories": ["cs.CV"], "comment": "Journal Submission", "summary": "Action segmentation is a challenging yet active research area that involves\nidentifying when and where specific actions occur in continuous video streams.\nMost existing work has focused on single-stream approaches that model the\nspatio-temporal aspects of frame sequences. However, recent research has\nshifted toward two-stream methods that learn action-wise features to enhance\naction segmentation performance. In this work, we propose the Dual-Stream\nAlignment Network (DSA Net) and investigate the impact of incorporating a\nsecond stream of learned action features to guide segmentation by capturing\nboth action and action-transition cues. Communication between the two streams\nis facilitated by a Temporal Context (TC) block, which fuses complementary\ninformation using cross-attention and Quantum-based Action-Guided Modulation\n(Q-ActGM), enhancing the expressive power of the fused features. To the best of\nour knowledge, this is the first study to introduce a hybrid quantum-classical\nmachine learning framework for action segmentation. Our primary objective is\nfor the two streams (frame-wise and action-wise) to learn a shared feature\nspace through feature alignment. This is encouraged by the proposed Dual-Stream\nAlignment Loss, which comprises three components: relational consistency,\ncross-level contrastive, and cycle-consistency reconstruction losses. Following\nprior work, we evaluate DSA Net on several diverse benchmark datasets: GTEA,\nBreakfast, 50Salads, and EgoProcel. We further demonstrate the effectiveness of\neach component through extensive ablation studies. Notably, DSA Net achieves\nstate-of-the-art performance, significantly outperforming existing", "AI": {"tldr": "The paper introduces the Dual-Stream Alignment Network (DSA Net), a new method for action segmentation in continuous video streams, leveraging two data streams and a hybrid quantum-classical approach.", "motivation": "The authors aim to improve action segmentation by leveraging a second stream for action features, addressing limitations in traditional single-stream models.", "method": "The proposed DSA Net includes a Temporal Context block for cross-stream information exchange and uses a hybrid quantum-classical approach. It introduces a Dual-Stream Alignment Loss composed of relational consistency, cross-level contrastive, and cycle-consistency reconstruction losses.", "result": "The proposed method is tested on benchmark datasets like GTEA, Breakfast, 50Salads, and EgoProcel, showing substantial performance improvement over existing methods.", "conclusion": "DSA Net advances the field of action segmentation, achieving state-of-the-art results by innovatively using dual streams and a hybrid quantum-classical framework."}}
{"id": "2510.07487", "pdf": "https://arxiv.org/pdf/2510.07487", "abs": "https://arxiv.org/abs/2510.07487", "authors": ["Waleed Bin Qaim", "Aleksandr Ometov", "Claudia Campolo", "Antonella Molinaro", "Elena Simona Lohan", "Jari Nurmi"], "title": "Reinforcement Learning-based Task Offloading in the Internet of Wearable Things", "categories": ["cs.LG"], "comment": "16 pages, 12 figures, Under review in the IEEE Internet of Things\n  Journal", "summary": "Over the years, significant contributions have been made by the research and\nindustrial sectors to improve wearable devices towards the Internet of Wearable\nThings (IoWT) paradigm. However, wearables are still facing several challenges.\nMany stem from the limited battery power and insufficient computation resources\navailable on wearable devices. On the other hand, with the popularity of smart\nwearables, there is a consistent increase in the development of new\ncomputationally intensive and latency-critical applications. In such a context,\ntask offloading allows wearables to leverage the resources available on nearby\nedge devices to enhance the overall user experience. This paper proposes a\nframework for Reinforcement Learning (RL)-based task offloading in the IoWT. We\nformulate the task offloading process considering the tradeoff between energy\nconsumption and task accomplishment time. Moreover, we model the task\noffloading problem as a Markov Decision Process (MDP) and utilize the\nQ-learning technique to enable the wearable device to make optimal task\noffloading decisions without prior knowledge. We evaluate the performance of\nthe proposed framework through extensive simulations for various applications\nand system configurations conducted in the ns-3 network simulator. We also show\nhow varying the main system parameters of the Q-learning algorithm affects the\noverall performance in terms of average task accomplishment time, average\nenergy consumption, and percentage of tasks offloaded.", "AI": {"tldr": "The paper addresses energy and computational constraints in wearable devices, proposing a Reinforcement Learning-based framework for task offloading to edge devices. It uses Q-learning to optimize offloading decisions and simulates various scenarios.", "motivation": "Wearable devices face challenges such as limited battery life and computing power, while demand for high-performing applications continues to grow.", "method": "The task offloading problem is modeled as a Markov Decision Process, using Q-learning to enable wearable devices to make optimal decisions without prior knowledge.", "result": "The framework shows benefits in terms of energy consumption, task accomplishment time, and the percentage of tasks effectively offloaded in simulations.", "conclusion": "Reinforcement Learning-based task offloading can improve wearable device performance by addressing energy and computation constraints while optimizing for user experience."}}
{"id": "2510.07545", "pdf": "https://arxiv.org/pdf/2510.07545", "abs": "https://arxiv.org/abs/2510.07545", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Mizanur Rahman", "Amran Bhuiyan", "Israt Jahan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to the EMNLP 2025 Industry Track", "summary": "Large Vision-Language Models (LVLMs) with only 7B parameters have shown\npromise as automated judges in chart comprehension tasks. However, tiny models\n(<=2B parameters) still perform poorly as judges, limiting their real-world use\nin resource-constrained settings. To address this, we propose two approaches to\nensure cost-efficient evaluation: (i) multi-criteria prompting, which combines\nseparate evaluation criteria into a single query, and (ii) domain-adaptive\ntransfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic\njudgments in a chart dataset to create the ChartJudge. Experiments show that\nmulti-criteria prompting exposes robustness gaps, which led to a huge drop in\nperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.\nIn addition, we find that our tiny LVLM (ChartJudge) can effectively transfer\nknowledge from one dataset to another to make it a more specialized model. Our\nfine-grained analysis across chart types and query complexities offers\nactionable insights into trade-offs between model size, prompt design, and\ntransferability, enabling scalable, low-cost evaluation for chart reasoning\ntasks. Our code and the data will be made publicly available.", "AI": {"tldr": "The paper proposes cost-efficient methods to train tiny language-vision models (<=2B parameters) to effectively evaluate charts.", "motivation": "Tiny models currently fail at chart comprehension tasks, which limits their utility in resource-constrained environments.", "method": "The paper introduces multi-criteria prompting and domain-adaptive transfer learning to improve the performance of tiny models. Specifically, a 2B-parameter LVLM is fine-tuned using synthetic data to create ChartJudge.", "result": "The proposed methods demonstrated improvements in transferring knowledge and delivering specialized evaluations, narrowing robustness gaps observed in larger models.", "conclusion": "ChartJudge demonstrates that tiny models can achieve cost-efficient scalability and specialization, aiding in low-cost chart evaluation and reasoning tasks."}}
{"id": "2510.07593", "pdf": "https://arxiv.org/pdf/2510.07593", "abs": "https://arxiv.org/abs/2510.07593", "authors": ["Bohan Lin", "Kuo Yang", "Yingchuan Lai", "Yudong Zhang", "Chen Zhang", "Guibin Zhang", "Xinlei Yu", "Miao Yu", "Xu Wang", "Yang Wang"], "title": "AgentAsk: Multi-Agent Systems Need to Ask", "categories": ["cs.AI"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving capabilities through collaborative division of labor. However,\nthey frequently underperform single-agent baselines due to edge-level error\ncascades: minor inaccuracies at one message handoff propagate across the entire\nchain. We propose AgentAsk, a lightweight and plug-and-play clarification\nmodule that treats every inter-agent message as a potential failure point and\ninserts minimally necessary questions to arrest error propagation. AgentAsk\nfollows a three-stage pipeline: (i) distilling edge-level judgments from\ncurated failure traces into a compact policy, (ii) supervising the policy to\ndetermine when/what/whom/how to ask, and (iii) optimizing online with E-GRPO, a\nreinforcement learning objective that balances accuracy, latency, and cost. The\nmodule is architecture-agnostic and easy to integrate into existing\norchestration. Across math, reasoning, and coding benchmarks, AgentAsk\nconsistently improves accuracy and robustness over public multi-agent\nimplementations while keeping overhead minimal, with latency and extra cost all\nless than 5%, approaching the performance of a strong evaluator. Beyond\nempirical improvements, we contribute a principled taxonomy of edge-level\nerrors and a practical recipe for link-local intervention, offering a scalable\npathway toward more reliable LLM-based multi-agent systems.", "AI": {"tldr": "AgentAsk is a module designed to reduce error propagation in multi-agent systems based on large language models (LLMs). It enhances accuracy and robustness with minimal overhead.", "motivation": "Multi-agent systems with LLMs often struggle to outperform single-agent systems due to error propagation between agents.", "method": "AgentAsk employs a three-stage pipeline: creating edge-level error policies, supervising clarification strategies, and optimizing performance with E-GRPO reinforcement learning.", "result": "AgentAsk improves multi-agent system accuracy and robustness across benchmarks (math, reasoning, coding) while introducing minimal latency and cost (<5%).", "conclusion": "The proposed intervention mechanism is architecture-agnostic, scalable, and enhances reliability, making collaborative LLM systems more practical and effective."}}
{"id": "2510.07869", "pdf": "https://arxiv.org/pdf/2510.07869", "abs": "https://arxiv.org/abs/2510.07869", "authors": ["Junwen Gu", "Zhiheng wu", "Pengxuan Si", "Shuang Qiu", "Yukai Feng", "Luoyang Sun", "Laien Luo", "Lianyi Yu", "Jian Wang", "Zhengxing Wu"], "title": "USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots", "categories": ["cs.RO"], "comment": null, "summary": "Underwater environments present unique challenges for robotic operation,\nincluding complex hydrodynamics, limited visibility, and constrained\ncommunication. Although data-driven approaches have advanced embodied\nintelligence in terrestrial robots and enabled task-specific autonomous\nunderwater robots, developing underwater intelligence capable of autonomously\nperforming multiple tasks remains highly challenging, as large-scale,\nhigh-quality underwater datasets are still scarce. To address these\nlimitations, we introduce USIM, a simulation-based multi-task\nVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over\n561K frames from 1,852 trajectories, totaling approximately 15.6 hours of\nBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from\nvisual navigation to mobile manipulation. Building upon this dataset, we\npropose U0, a VLA model for general underwater robots, which integrates\nbinocular vision and other sensor modalities through multimodal fusion, and\nfurther incorporates a convolution-attention-based perception focus enhancement\nmodule (CAP) to improve spatial understanding and mobile manipulation. Across\ntasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,\nthe framework achieves a success rate of 80%, while in challenging mobile\nmanipulation tasks, it reduces the distance to the target by 21.2% compared\nwith baseline methods, demonstrating its effectiveness. USIM and U0 show that\nVLA models can be effectively applied to underwater robotic applications,\nproviding a foundation for scalable dataset construction, improved task\nautonomy, and the practical realization of intelligent general underwater\nrobots.", "AI": {"tldr": "Underwater robotics face challenges like complex hydrodynamics and scarce datasets. The authors address this via USIM, a large dataset, and U0, a model improving success and task efficiency.", "motivation": "To enable autonomous multi-task underwater robots despite challenges such as limited resources, datasets, and complex environments.", "method": "Developed USIM dataset with 561K frames and 1,852 trajectories across varied underwater tasks and introduced U0 model utilizing Vision-Language-Action techniques integrated with advanced perception modules.", "result": "U0 achieved an 80% success rate across various tasks and reduced target distance by 21.2% in challenging mobile manipulation tasks, demonstrating its efficacy.", "conclusion": "USIM and U0 significantly advance VLA modeling for underwater robotics, promoting scalable dataset creation and enhanced task autonomy."}}
{"id": "2510.07654", "pdf": "https://arxiv.org/pdf/2510.07654", "abs": "https://arxiv.org/abs/2510.07654", "authors": ["Yanjie Pan", "Qingdong He", "Lidong Wang", "Bo Peng", "Mingmin Chi"], "title": "Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection", "categories": ["cs.CV"], "comment": "5 pages (including references), 4 figures. Code and models will be\n  released upon publication", "summary": "Video virtual try-on aims to replace the clothing of a person in a video with\na target garment. Current dual-branch architectures have achieved significant\nsuccess in diffusion models based on the U-Net; however, adapting them to\ndiffusion models built upon the Diffusion Transformer remains challenging.\nInitially, introducing latent space features from the garment reference branch\nrequires adding or modifying the backbone network, leading to a large number of\ntrainable parameters. Subsequently, the latent space features of garments lack\ninherent temporal characteristics and thus require additional learning. To\naddress these challenges, we propose a novel approach, OIE (Once is Enough), a\nvirtual try-on strategy based on first-frame clothing replacement:\nspecifically, we employ an image-based clothing transfer model to replace the\nclothing in the initial frame, and then, under the content control of the\nedited first frame, utilize pose and mask information to guide the temporal\nprior of the video generation model in synthesizing the remaining frames\nsequentially. Experiments show that our method achieves superior parameter\nefficiency and computational efficiency while still maintaining leading\nperformance under these constraints.", "AI": {"tldr": "This paper proposes OIE (Once is Enough), a video virtual try-on strategy that replaces clothing in the first frame and uses temporal guidance for subsequent frames, achieving high efficiency and performance.", "motivation": "Current video virtual try-on methods face challenges in adapting diffusion models based on Diffusion Transformers, primarily due to high parameter dependency and lack of temporal features.", "method": "OIE employs a first-frame clothing replacement via an image-based model, followed by pose and mask-guided sequential generation for subsequent frames.", "result": "OIE outperforms existing methods in parameter and computational efficiency while maintaining strong overall performance in video virtual try-on tasks.", "conclusion": "Using the first frame as a basis for temporal synthesis presents an efficient, effective solution for video virtual try-on challenges and improves performance metrics."}}
{"id": "2510.07500", "pdf": "https://arxiv.org/pdf/2510.07500", "abs": "https://arxiv.org/abs/2510.07500", "authors": ["Shuangyi Chen", "Ashish Khisti"], "title": "Black-box Detection of LLM-generated Text Using Generalized Jensen-Shannon Divergence", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "Preprint", "summary": "We study black-box detection of machine-generated text under practical\nconstraints: the scoring model (proxy LM) may mismatch the unknown source\nmodel, and per-input contrastive generation is costly. We propose SurpMark, a\nreference-based detector that summarizes a passage by the dynamics of its token\nsurprisals. SurpMark quantizes surprisals into interpretable states, estimates\na state-transition matrix for the test text, and scores it via a generalized\nJensen-Shannon (GJS) gap between the test transitions and two fixed references\n(human vs. machine) built once from historical corpora. We prove a principled\ndiscretization criterion and establish the asymptotic normality of the decision\nstatistic. Empirically, across multiple datasets, source models, and scenarios,\nSurpMark consistently matches or surpasses baselines; our experiments\ncorroborate the statistic's asymptotic normality, and ablations validate the\neffectiveness of the proposed discretization.", "AI": {"tldr": "The paper develops a novel method called SurpMark for detecting AI-generated text, focusing on practical and interpretable techniques.", "motivation": "The motivation is to build a practical detector for machine-generated text even when the scoring model doesn't match the text's source model and generation isn't cost-effective per input.", "method": "Introduce SurpMark, a system based on summarizing token surprisal dynamics, quantizing them into states, estimating a state-transition matrix, and scoring test text using a generalized Jensen-Shannon gap between human and machine references.", "result": "SurpMark outperformed or matched existing methods across various datasets and models, proving effective and validating the theoretical properties of its decision statistic.", "conclusion": "SurpMark is an effective and robust tool for detecting machine-generated text, with sound theoretical underpinnings and strong empirical results."}}
{"id": "2510.07566", "pdf": "https://arxiv.org/pdf/2510.07566", "abs": "https://arxiv.org/abs/2510.07566", "authors": ["Junyi Zhu", "Savas Ozkan", "Andrea Maracani", "Sinan Mutlu", "Cho Jung Min", "Mete Ozay"], "title": "Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 Industry Track", "summary": "Deploying natural language processing (NLP) models on mobile platforms\nrequires models that can adapt across diverse applications while remaining\nefficient in memory and computation. We investigate pre-finetuning strategies\nto enhance the adaptability of lightweight BERT-like encoders for two\nfundamental NLP task families: named entity recognition (NER) and text\nclassification. While pre-finetuning improves downstream performance for each\ntask family individually, we find that na\\\"ive multi-task pre-finetuning\nintroduces conflicting optimization signals that degrade overall performance.\nTo address this, we propose a simple yet effective multi-task pre-finetuning\nframework based on task-primary LoRA modules, which enables a single shared\nencoder backbone with modular adapters. Our approach achieves performance\ncomparable to individual pre-finetuning while meeting practical deployment\nconstraint. Experiments on 21 downstream tasks show average improvements of\n+0.8% for NER and +8.8% for text classification, demonstrating the\neffectiveness of our method for versatile mobile NLP applications.", "AI": {"tldr": "This paper investigates methods for adapting lightweight BERT-like models for versatile NLP applications on mobile devices. It proposes a multi-task pre-finetuning strategy using modular adapters, achieving significant performance improvements.", "motivation": "The motivation is to make NLP models efficient and adaptable for deployment on mobile platforms, which require memory and computational efficiency while catering to diverse applications.", "method": "The method involves pre-finetuning lightweight BERT-like encoders using a multi-task framework with task-primary LoRA modules. This allows a single shared encoder backbone paired with modular adapters.", "result": "Experiments on 21 tasks show an average performance improvement of +0.8% for named entity recognition and +8.8% for text classification.", "conclusion": "The proposed approach balances performance and deployment constraints, making it effective for versatile mobile NLP applications."}}
{"id": "2510.07871", "pdf": "https://arxiv.org/pdf/2510.07871", "abs": "https://arxiv.org/abs/2510.07871", "authors": ["Erjia Xiao", "Lingfeng Zhang", "Yingbo Tang", "Hao Cheng", "Renjing Xu", "Wenbo Ding", "Lei Zhou", "Long Chen", "Hangjun Ye", "Xiaoshuai Hao"], "title": "Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "In this report, we describe the technical details of our submission to the\nIROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on\ndeveloping RGBD-based perception and navigation systems that enable autonomous\nagents to navigate safely, efficiently, and socially compliantly in dynamic\nhuman-populated indoor environments. The challenge requires agents to operate\nfrom an egocentric perspective using only onboard sensors including RGB-D\nobservations and odometry, without access to global maps or privileged\ninformation, while maintaining social norm compliance such as safe distances\nand collision avoidance. Building upon the Falcon model, we introduce a\nProactive Risk Perception Module to enhance social navigation performance. Our\napproach augments Falcon with collision risk understanding that learns to\npredict distance-based collision risk scores for surrounding humans, which\nenables the agent to develop more robust spatial awareness and proactive\ncollision avoidance behaviors. The evaluation on the Social-HM3D benchmark\ndemonstrates that our method improves the agent's ability to maintain personal\nspace compliance while navigating toward goals in crowded indoor scenes with\ndynamic human agents, achieving 2nd place among 16 participating teams in the\nchallenge.", "AI": {"tldr": "This paper introduces a novel Proactive Risk Perception Module to improve social navigation in human-populated environments, obtaining 2nd place in the IROS 2025 RoboSense Challenge.", "motivation": "The motivation is to address the need for safe, efficient, and socially compliant navigation in dynamic indoor spaces, where autonomous agents must operate with limited sensors and avoid collisions.", "method": "The method enhances the Falcon model by introducing a Proactive Risk Perception Module, which predicts distance-based collision risk scores for nearby humans, enabling better spatial awareness and collision avoidance.", "result": "The approach showed improved personal space compliance and robust goal navigation in crowded dynamic scenes, achieving 2nd place among 16 teams in the Social-HM3D benchmark.", "conclusion": "The integration of proactive collision risk assessment enhances social navigation, demonstrating significant progress in compliance with social norms and safety while navigating dynamic environments."}}
{"id": "2510.07656", "pdf": "https://arxiv.org/pdf/2510.07656", "abs": "https://arxiv.org/abs/2510.07656", "authors": ["James Baker"], "title": "MONKEY: Masking ON KEY-Value Activation Adapter for Personalization", "categories": ["cs.CV"], "comment": null, "summary": "Personalizing diffusion models allows users to generate new images that\nincorporate a given subject, allowing more control than a text prompt. These\nmodels often suffer somewhat when they end up just recreating the subject\nimage, and ignoring the text prompt. We observe that one popular method for\npersonalization, the IP-Adapter automatically generates masks that we\ndefinitively segment the subject from the background during inference. We\npropose to use this automatically generated mask on a second pass to mask the\nimage tokens, thus restricting them to the subject, not the background,\nallowing the text prompt to attend to the rest of the image. For text prompts\ndescribing locations and places, this produces images that accurately depict\nthe subject while definitively matching the prompt. We compare our method to a\nfew other test time personalization methods, and find our method displays high\nprompt and source image alignment.", "AI": {"tldr": "The paper proposes an enhancement to personalize diffusion models for image generation, enabling better integration of subjects with text prompts through automated masking.", "motivation": "To address the limitation in personalized diffusion models where generated images often recreate the subject without effectively incorporating text prompts.", "method": "The approach leverages the IP-Adapter's capability to generate masks that segment subjects from backgrounds, applying these masks in a second inference pass to restrict token focus to the subject.", "result": "The method improves alignment between the generated subject and the text prompt, notably enhancing image generation for prompts describing specific locations or places.", "conclusion": "Utilizing automated masking during a secondary inference phase leads to higher fidelity in prompt and source image alignment, outperforming other personalization techniques."}}
{"id": "2510.07505", "pdf": "https://arxiv.org/pdf/2510.07505", "abs": "https://arxiv.org/abs/2510.07505", "authors": ["Shen Dong", "Mingxuan Zhang", "Pengfei He", "Li Ma", "Bhavani Thuraisingham", "Hui Liu", "Yue Xing"], "title": "PEAR: Planner-Executor Agent Robustness Benchmark", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a\npowerful paradigm for tackling complex, multi-step tasks across diverse\ndomains. However, despite their impressive capabilities, MAS remain susceptible\nto adversarial manipulation. Existing studies typically examine isolated attack\nsurfaces or specific scenarios, leaving a lack of holistic understanding of MAS\nvulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for\nsystematically evaluating both the utility and vulnerability of\nplanner-executor MAS. While compatible with various MAS architectures, our\nbenchmark focuses on the planner-executor structure, which is a practical and\nwidely adopted design. Through extensive experiments, we find that (1) a weak\nplanner degrades overall clean task performance more severely than a weak\nexecutor; (2) while a memory module is essential for the planner, having a\nmemory module for the executor does not impact the clean task performance; (3)\nthere exists a trade-off between task performance and robustness; and (4)\nattacks targeting the planner are particularly effective at misleading the\nsystem. These findings offer actionable insights for enhancing the robustness\nof MAS and lay the groundwork for principled defenses in multi-agent settings.", "AI": {"tldr": "The paper presents PEAR, a benchmark to assess vulnerabilities and performance in planner-executor multi-agent systems (MAS), focusing on robustness against adversarial attacks and providing insights for improving MAS designs.", "motivation": "MAS relying on Large Language Models (LLMs) show promise in tackling complex tasks but remain vulnerable to adversarial manipulation. Current research lacks a unified framework to understand and address these vulnerabilities.", "method": "PEAR is introduced as a benchmark compatible with various MAS architectures, particularly analyzing planner-executor systems through systematic utility and vulnerability evaluations in diverse scenarios.", "result": "Experimental findings indicate weak planners harm task performance more than weak executors, memory modules are more vital for planners than executors, task performance trades off robustness, and planner-targeted attacks are highly effective.", "conclusion": "The paper provides essential insights for MAS robustness improvement and sets the stage for developing principled defenses to fortify multi-agent systems."}}
{"id": "2510.07579", "pdf": "https://arxiv.org/pdf/2510.07579", "abs": "https://arxiv.org/abs/2510.07579", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "title": "Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages", "summary": "This study conducts a computational linguistic analysis of pandemic-related\nonline discourse to examine how language distinguishes health misinformation\nfrom factual communication. Drawing on three corpora: COVID-19 false narratives\n(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts\n(n = 5787), we identify significant differences in readability, rhetorical\nmarkers, and persuasive language use. COVID-19 misinformation exhibited\nmarkedly lower readability scores and contained over twice the frequency of\nfear-related or persuasive terms compared to the other datasets. It also showed\nminimal use of exclamation marks, contrasting with the more emotive style of\nMonkeypox content. These patterns suggest that misinformation employs a\ndeliberately complex rhetorical style embedded with emotional cues, a\ncombination that may enhance its perceived credibility. Our findings contribute\nto the growing body of work on digital health misinformation by highlighting\nlinguistic indicators that may aid detection efforts. They also inform public\nhealth messaging strategies and theoretical models of crisis communication in\nnetworked media environments. At the same time, the study acknowledges\nlimitations, including reliance on traditional readability indices, use of a\ndeliberately narrow persuasive lexicon, and reliance on static aggregate\nanalysis. Future research should therefore incorporate longitudinal designs,\nbroader emotion lexicons, and platform-sensitive approaches to strengthen\nrobustness.", "AI": {"tldr": "The study analyzes pandemic-related online discourse using computational linguistics to identify linguistic markers that distinguish misinformation from factual content.", "motivation": "To better understand how health misinformation can be detected and countered through its language characteristics during crises like pandemics.", "method": "Three corpora (COVID-19 false narratives, general COVID-19 content, and Monkeypox-related posts) were analyzed for differences in readability, rhetorical markers, and persuasive language using computational linguistic techniques.", "result": "Misinformation was found to have lower readability, higher levels of fear-related and persuasive terms, and distinct rhetorical styles differing from factual and emotive content.", "conclusion": "Findings underline linguistic cues in misinformation, supporting better detection and public health messaging strategies, despite limitations like reliance on traditional indices and static analyses."}}
{"id": "2510.07623", "pdf": "https://arxiv.org/pdf/2510.07623", "abs": "https://arxiv.org/abs/2510.07623", "authors": ["Hannah R. Lawrence", "Shannon Wiltsey Stirman", "Samuel Dorison", "Taedong Yun", "Megan Jones Bell"], "title": "A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services", "categories": ["cs.AI"], "comment": null, "summary": "Generative artificial intelligence (Generative AI) is transforming\nhealthcare. With this evolution comes optimism regarding the impact it will\nhave on mental health, as well as concern regarding the risks that come with\ngenerative AI operating in the mental health domain. Much of the investment in,\nand academic and public discourse about, AI-powered solutions for mental health\nhas focused on therapist chatbots. Despite the common assumption that chatbots\nwill be the most impactful application of GenAI to mental health, we make the\ncase here for a lower-risk, high impact use case: leveraging generative AI to\nenhance and scale training in mental health service provision. We highlight key\nbenefits of using generative AI to help train people to provide mental health\nservices and present a real-world case study in which generative AI improved\nthe training of veterans to support one another's mental health. With numerous\npotential applications of generative AI in mental health, we illustrate why we\nshould invest in using generative AI to support training people in mental\nhealth service provision.", "AI": {"tldr": "Generative AI can significantly improve mental health care by scaling training for service providers, presenting a lower-risk alternative to chatbots.", "motivation": "Current focus on mental health chatbots comes with optimism and risk, suggesting a need for safer applications of generative AI in mental health.", "method": "Explores using generative AI to enhance training for mental health service provision, including a case study on veterans supporting one another.", "result": "Generative AI demonstrated improved training outcomes in real-world applications, like veteran peer support for mental health.", "conclusion": "Generative AI shows promise in scaling mental health service training, suggesting investment in this domain is more impactful and lower-risk than chatbots."}}
{"id": "2510.07882", "pdf": "https://arxiv.org/pdf/2510.07882", "abs": "https://arxiv.org/abs/2510.07882", "authors": ["Boyu Li", "Siyuan He", "Hang Xu", "Haoqi Yuan", "Yu Zang", "Liwei Hu", "Junpeng Yue", "Zhenxiong Jiang", "Pengbo Hu", "B\u00f6rje F. Karlsson", "Yehui Tang", "Zongqing Lu"], "title": "Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots", "categories": ["cs.RO"], "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nthe ability to serve as high-level planners, enabling robots to follow complex\nhuman instructions. However, their effectiveness, especially in long-horizon\ntasks involving dual-arm humanoid robots, remains limited. This limitation\narises from two main challenges: (i) the absence of simulation platforms that\nsystematically support task evaluation and data collection for humanoid robots,\nand (ii) the insufficient embodiment awareness of current MLLMs, which hinders\nreasoning about dual-arm selection logic and body positions during planning. To\naddress these issues, we present DualTHOR, a new dual-arm humanoid simulator,\nwith continuous transition and a contingency mechanism. Building on this\nplatform, we propose Proprio-MLLM, a model that enhances embodiment awareness\nby incorporating proprioceptive information with motion-based position\nembedding and a cross-spatial encoder. Experiments show that, while existing\nMLLMs struggle in this environment, Proprio-MLLM achieves an average\nimprovement of 19.75% in planning performance. Our work provides both an\nessential simulation platform and an effective model to advance embodied\nintelligence in humanoid robotics. The code is available at\nhttps://anonymous.4open.science/r/DualTHOR-5F3B.", "AI": {"tldr": "The paper introduces DualTHOR, a simulator for dual-arm humanoid robots, and presents Proprio-MLLM, a model with enhanced embodiment awareness achieving a notable improvement in planning performance.", "motivation": "Multimodal Large Language Models (MLLMs) face challenges in long-horizon tasks for humanoid robots due to lack of simulation platforms and embodiment awareness.", "method": "Development of DualTHOR simulation platform and Proprio-MLLM model that uses proprioceptive information, motion-based position embedding, and a cross-spatial encoder.", "result": "Proprio-MLLM demonstrated an average planning performance improvement of 19.75% compared to existing MLLMs.", "conclusion": "The new platform and model advance embodied intelligence for humanoid robotics, addressing key limitations in current systems."}}
{"id": "2510.07665", "pdf": "https://arxiv.org/pdf/2510.07665", "abs": "https://arxiv.org/abs/2510.07665", "authors": ["Jun Muraoka", "Daichi Haraguchi", "Naoto Inoue", "Wataru Shimoda", "Kota Yamaguchi", "Seiichi Uchida"], "title": "Automatic Text Box Placement for Supporting Typographic Design", "categories": ["cs.CV"], "comment": null, "summary": "In layout design for advertisements and web pages, balancing visual appeal\nand communication efficiency is crucial. This study examines automated text box\nplacement in incomplete layouts, comparing a standard Transformer-based method,\na small Vision and Language Model (Phi3.5-vision), a large pretrained VLM\n(Gemini), and an extended Transformer that processes multiple images.\nEvaluations on the Crello dataset show the standard Transformer-based models\ngenerally outperform VLM-based approaches, particularly when incorporating\nricher appearance information. However, all methods face challenges with very\nsmall text or densely populated layouts. These findings highlight the benefits\nof task-specific architectures and suggest avenues for further improvement in\nautomated layout design.", "AI": {"tldr": "This paper evaluates automated text box placement in layouts using several models and finds that task-specific architectures like Transformers outperform Vision and Language Models, though challenges remain with small or dense layouts.", "motivation": "The paper addresses the need for balancing visual appeal and communication efficiency in layout designs for advertisements and web pages.", "method": "It compares different model types (Transformers and Vision and Language Models) using evaluations on the Crello dataset.", "result": "Standard Transformer-based models outperform Vision and Language Models, especially with richer appearance data, but struggle with very small text or crowded layouts.", "conclusion": "Task-specific architectures, such as Transformers, offer benefits for automated layout design, but further improvements are needed to handle complex scenarios like small or dense text boxes."}}
{"id": "2510.07509", "pdf": "https://arxiv.org/pdf/2510.07509", "abs": "https://arxiv.org/abs/2510.07509", "authors": ["Tianyu Bell Pan", "Damon L. Woodard"], "title": "Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "This paper explores a multimodal co-training framework designed to enhance\nmodel generalization in situations where labeled data is limited and\ndistribution shifts occur. We thoroughly examine the theoretical foundations of\nthis framework, deriving conditions under which the use of unlabeled data and\nthe promotion of agreement between classifiers for different modalities lead to\nsignificant improvements in generalization. We also present a convergence\nanalysis that confirms the effectiveness of iterative co-training in reducing\nclassification errors. In addition, we establish a novel generalization bound\nthat, for the first time in a multimodal co-training context, decomposes and\nquantifies the distinct advantages gained from leveraging unlabeled multimodal\ndata, promoting inter-view agreement, and maintaining conditional view\nindependence. Our findings highlight the practical benefits of multimodal\nco-training as a structured approach to developing data-efficient and robust AI\nsystems that can effectively generalize in dynamic, real-world environments.\nThe theoretical foundations are examined in dialogue with, and in advance of,\nestablished co-training principles.", "AI": {"tldr": "The paper introduces a multimodal co-training framework to improve model generalization under limited labeled data and distribution shifts, validated with theoretical foundations and convergence analysis.", "motivation": "To address challenges in AI systems' generalization when labeled data is limited and there's a distribution shift, using theoretical and practical insights.", "method": "Multimodal co-training framework employing iterative co-training, leveraging unlabeled multimodal data, promoting agreement between modalities, and ensuring conditional view independence.", "result": "Theoretical analyses show significant improvements in generalization and a novel generalization bound illustrates advantages in using multimodal co-training.", "conclusion": "Multimodal co-training is effective for building robust, data-efficient AI systems that generalize well in real-world dynamic scenarios, supported by theoretical principles and analyses."}}
{"id": "2510.07591", "pdf": "https://arxiv.org/pdf/2510.07591", "abs": "https://arxiv.org/abs/2510.07591", "authors": ["Chihiro Taguchi", "Richard Sproat"], "title": "IASC: Interactive Agentic System for ConLangs", "categories": ["cs.CL"], "comment": "Initial draft", "summary": "We present a system that uses LLMs as a tool in the development of\nConstructed Languages. The system is modular in that one first creates a target\nphonology for the language using an agentic approach that refines its output at\neach step with commentary feedback on its previous attempt. Next, a set of\nsentences is 'translated' from their English original into a morphosyntactic\nmarkup that reflects the word order and morphosyntactic feature specifications\nof the desired target language, with affixes represented as morphosyntactic\nfeature bundles. From this translated corpus, a lexicon is constructed using\nthe phonological model and the set of morphemes (stems and affixes) extracted\nfrom the 'translated' sentences. The system is then instructed to provide an\northography for the language, using an existing script such as Latin or\nCyrillic. Finally, the system writes a brief grammatical handbook of the\nlanguage. The system can also translate further sentences into the target\nlanguage.\n  Our goal is twofold. First, we hope that these tools will be fun to use for\ncreating artificially constructed languages. Second, we are interested in\nexploring what LLMs 'know' about language-not what they know about any\nparticular language or linguistic phenomenon, but how much they know about and\nunderstand language and linguistic concepts. As we shall see, there is a fairly\nwide gulf in capabilities both among different LLMs and among different\nlinguistic specifications, with it being notably easier for systems to deal\nwith more common patterns than rarer ones. An additional avenue that we explore\nis the application of our approach to translating from high-resource into\nlow-resource languages. While the results so far are mostly negative, we\nprovide some evidence that an improved version of the present system could\nafford some real gains in such tasks.\n  https://github.com/SakanaAI/IASC", "AI": {"tldr": "The paper introduces a modular system utilizing LLMs for creating constructed languages, focusing on areas like phonology, morphosyntactic structure, lexicon, orthography, and grammar handbook development.", "motivation": "The authors aim to make constructing artificial languages easier and explore the extent of linguistic knowledge within LLMs.", "method": "The system incrementally develops artificial languages by defining phonological rules, translating English sentences into morphosyntactic structures, building lexicons, assigning orthographies, and creating grammars.", "result": "While the system showcases some success with typical linguistic patterns, it struggles with rare constructs and low-resource language translation tasks. Varied LLM performance was observed.", "conclusion": "The process is deemed enjoyable for language creation and highlights LLM limitations in linguistic understanding and cross-language translation challenges."}}
{"id": "2510.07632", "pdf": "https://arxiv.org/pdf/2510.07632", "abs": "https://arxiv.org/abs/2510.07632", "authors": ["Yinglun Zhu", "Jiancheng Zhang", "Fuzhi Tang"], "title": "Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Frontier AI models have achieved remarkable progress, yet recent studies\nsuggest they struggle with compositional reasoning, often performing at or\nbelow random chance on established benchmarks. We revisit this problem and show\nthat widely used evaluation metrics systematically underestimate model\ncapability. To address this, we introduce a group matching score that better\nexploits group structure and reveals substantial hidden capability in both\ncontrastive vision-language models (VLMs) and multimodal large language models\n(MLLMs). Moreover, simply overfitting to the induced group matchings at test\ntime transfers this hidden capability into higher scores under standard\nevaluation metrics, closing much of the reported gap. This adjustment enables\nSigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first\nresult surpassing estimated human performance on Winoground.\n  Building on this insight, we propose Test-Time Matching (TTM), an iterative,\nself-improving algorithm that further bootstraps model performance without any\nexternal supervision. TTM delivers additional, non-trivial improvements: for\nexample, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a\nnew state of the art. Importantly, TTM remains broadly effective even on\nbenchmarks without metric-induced effects or group structures, achieving\nrelative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16\ndataset variants spanning diverse setups, our experiments demonstrate that TTM\nconsistently improves model performance and advances the frontier of\ncompositional reasoning.", "AI": {"tldr": "Frontier AI models underperform in compositional reasoning due to flawed evaluation metrics. Introducing new metrics and Test-Time Matching (TTM) uncovers hidden model capabilities, yielding state-of-the-art results across multiple benchmarks.", "motivation": "To address the perceived underperformance of advanced AI models in compositional reasoning due to misaligned evaluation metrics.", "method": "The paper introduces a group matching score to rectify evaluation biases and proposes Test-Time Matching (TTM), an iterative algorithm for performance improvement without external supervision.", "result": "TTM enables significant advancements in model performance, including surpassing previous benchmarks and human capabilities in compositional reasoning tasks.", "conclusion": "Improving evaluation frameworks and test-time algorithms can reveal hidden capabilities, advancing AI models' compositional reasoning abilities and setting new benchmarks across datasets."}}
{"id": "2510.07975", "pdf": "https://arxiv.org/pdf/2510.07975", "abs": "https://arxiv.org/abs/2510.07975", "authors": ["Mingyang Sun", "Jiude Wei", "Qichen He", "Donglin Wang", "Cewu Lu", "Jianhua Sun"], "title": "Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Enabling robots to perform precise and generalized manipulation in\nunstructured environments remains a fundamental challenge in embodied AI. While\nVision-Language Models (VLMs) have demonstrated remarkable capabilities in\nsemantic reasoning and task planning, a significant gap persists between their\nhigh-level understanding and the precise physical execution required for\nreal-world manipulation. To bridge this \"semantic-to-physical\" gap, we\nintroduce GRACE, a novel framework that grounds VLM-based reasoning through\nexecutable analytic concepts (EAC)-mathematically defined blueprints that\nencode object affordances, geometric constraints, and semantics of\nmanipulation. Our approach integrates a structured policy scaffolding pipeline\nthat turn natural language instructions and visual information into an\ninstantiated EAC, from which we derive grasp poses, force directions and plan\nphysically feasible motion trajectory for robot execution. GRACE thus provides\na unified and interpretable interface between high-level instruction\nunderstanding and low-level robot control, effectively enabling precise and\ngeneralizable manipulation through semantic-physical grounding. Extensive\nexperiments demonstrate that GRACE achieves strong zero-shot generalization\nacross a variety of articulated objects in both simulated and real-world\nenvironments, without requiring task-specific training.", "AI": {"tldr": "GRACE bridges the gap between semantic task planning by Vision-Language Models and precise physical manipulation by introducing a framework that combines high-level reasoning with executable analytic concepts for robots.", "motivation": "The goal is to overcome the challenge of enabling robots to effectively perform manipulation tasks in unstructured environments, addressing the disconnect between VLMs' semantic capacity and physical execution.", "method": "GRACE utilizes executable analytic concepts (EACs) to encode affordances, constraints, and semantics, transforming natural language and visual input into actionable robotic control plans.", "result": "GRACE demonstrates strong zero-shot generalization, executing manipulation tasks across articulated objects in both simulated and real-world scenarios without task-specific training.", "conclusion": "GRACE provides a unified and interpretable bridge between semantic reasoning and physical execution, enhancing robotic manipulation capabilities in diverse environments."}}
{"id": "2510.07525", "pdf": "https://arxiv.org/pdf/2510.07525", "abs": "https://arxiv.org/abs/2510.07525", "authors": ["Alvaro Ribot", "Anna Seigal", "Piotr Zwiernik"], "title": "Beyond independent component analysis: identifiability and algorithms", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH", "62H12, 62R01, 62E10, 15A69"], "comment": "30 pages, 8 figures", "summary": "Independent Component Analysis (ICA) is a classical method for recovering\nlatent variables with useful identifiability properties. For independent\nvariables, cumulant tensors are diagonal; relaxing independence yields tensors\nwhose zero structure generalizes diagonality. These models have been the\nsubject of recent work in non-independent component analysis. We show that\npairwise mean independence answers the question of how much one can relax\nindependence: it is identifiable, any weaker notion is non-identifiable, and it\ncontains the models previously studied as special cases. Our results apply to\ndistributions with the required zero pattern at any cumulant tensor. We propose\nan algebraic recovery algorithm based on least-squares optimization over the\northogonal group. Simulations highlight robustness: enforcing full independence\ncan harm estimation, while pairwise mean independence enables more stable\nrecovery. These findings extend the classical ICA framework and provide a\nrigorous basis for blind source separation beyond independence.", "AI": {"tldr": "This paper extends Independent Component Analysis (ICA) by introducing pairwise mean independence, which generalizes independence notions and enables identifiable recovery of latent variables.", "motivation": "The motivation stems from addressing limitations in classical ICA that strictly relies on full independence, aiming to relax this condition while preserving identifiability properties.", "method": "The authors introduce pairwise mean independence as a relaxed identifiability criterion and propose an algebraic recovery algorithm based on least-squares optimization over the orthogonal group.", "result": "Simulations show that enforcing full independence can lead to estimation errors, whereas pairwise mean independence improves stability and robustness in recovering latent variables.", "conclusion": "The study extends ICA with pairwise mean independence, offering a rigorous framework for blind source separation beyond full independence while highlighting the advantages of relaxed independence conditions."}}
{"id": "2510.07666", "pdf": "https://arxiv.org/pdf/2510.07666", "abs": "https://arxiv.org/abs/2510.07666", "authors": ["Heming Wu", "Di Wang", "Tai Ma", "Peng Zhao", "Yubin Xiao", "Zhongke Wu", "Xing-Ce Wang", "Chuang Li", "Xuan Wu", "You Zhou"], "title": "TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although pyramid networks have demonstrated superior performance in\ndeformable medical image registration, their decoder architectures are\ninherently prone to propagating and accumulating anatomical structure\nmisalignments. Moreover, most existing models do not adaptively determine the\nnumber of iterations for optimization under varying deformation requirements\nacross images, resulting in either premature termination or excessive\niterations that degrades registration accuracy. To effectively mitigate the\naccumulation of anatomical misalignments, we propose the Feature-Enhanced\nResidual Module (FERM) as the core component of each decoding layer in the\npyramid network. FERM comprises three sequential blocks that extract anatomical\nsemantic features, learn to suppress irrelevant features, and estimate the\nfinal deformation field, respectively. To adaptively determine the number of\niterations for varying images, we propose the dual-stage Threshold-Controlled\nIterative (TCI) strategy. In the first stage, TCI assesses registration\nstability and with asserted stability, it continues with the second stage to\nevaluate convergence. We coin the model that integrates FERM and TCI as\nThreshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three\npublic brain MRI datasets and one abdomen CT dataset demonstrate that TCIP\noutperforms the state-of-the-art (SOTA) registration networks in terms of\naccuracy, while maintaining comparable inference speed and a compact model\nparameter size. Finally, we assess the generalizability of FERM and TCI by\nintegrating them with existing registration networks and further conduct\nablation studies to validate the effectiveness of these two proposed methods.", "AI": {"tldr": "The paper introduces the Feature-Enhanced Residual Module (FERM) and the Threshold-Controlled Iterative strategy (TCI) to improve deformable medical image registration, providing enhanced accuracy with a compact model size.", "motivation": "Deformable medical image registration suffers from issues like propagation of anatomical misalignments and inefficient optimization iterations. This paper aims to address these limitations.", "method": "The authors propose FERM, a module that refines semantic features in decoding layers, and TCI, a strategy to adaptively optimize iteration numbers based on stability and convergence measures.", "result": "The TCIP model, integrating FERM and TCI, outperformed existing state-of-the-art networks in registration accuracy across brain MRI and abdomen CT datasets while maintaining competitive efficiency.", "conclusion": "FERM and TCI effectively enhance model performance and generalizability, mitigating anatomical misalignments and adapting to varied deformation requirements in medical image registration."}}
{"id": "2510.07513", "pdf": "https://arxiv.org/pdf/2510.07513", "abs": "https://arxiv.org/abs/2510.07513", "authors": ["Qinghua Liu", "Sam Heshmati", "Zheda Mai", "Zubin Abraham", "John Paparrizos", "Liu Ren"], "title": "MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DB"], "comment": null, "summary": "Effective analysis of time series data presents significant challenges due to\nthe complex temporal dependencies and cross-channel interactions in\nmultivariate data. Inspired by the way human analysts visually inspect time\nseries to uncover hidden patterns, we ask: can incorporating visual\nrepresentations enhance automated time-series analysis? Recent advances in\nmultimodal large language models have demonstrated impressive generalization\nand visual understanding capability, yet their application to time series\nremains constrained by the modality gap between continuous numerical data and\ndiscrete natural language. To bridge this gap, we introduce MLLM4TS, a novel\nframework that leverages multimodal large language models for general\ntime-series analysis by integrating a dedicated vision branch. Each time-series\nchannel is rendered as a horizontally stacked color-coded line plot in one\ncomposite image to capture spatial dependencies across channels, and a\ntemporal-aware visual patch alignment strategy then aligns visual patches with\ntheir corresponding time segments. MLLM4TS fuses fine-grained temporal details\nfrom the numerical data with global contextual information derived from the\nvisual representation, providing a unified foundation for multimodal\ntime-series analysis. Extensive experiments on standard benchmarks demonstrate\nthe effectiveness of MLLM4TS across both predictive tasks (e.g.,\nclassification) and generative tasks (e.g., anomaly detection and forecasting).\nThese results underscore the potential of integrating visual modalities with\npretrained language models to achieve robust and generalizable time-series\nanalysis.", "AI": {"tldr": "MLLM4TS introduces a novel framework for time-series analysis, integrating visual representations with multimodal large language models to improve predictive and generative tasks.", "motivation": "To address challenges in analyzing multivariate time-series data and explore whether visual representations can enhance automated analysis.", "method": "The framework integrates multivariate time-series into a color-coded line plot, employs temporal-aware visual patch alignment, and fuses numerical and visual data using multimodal large language models.", "result": "Extensive experiments on benchmarks show superior performance in classification, anomaly detection, and forecasting tasks.", "conclusion": "Integrating visual modalities with pretrained language models offers a robust and generalizable approach to time-series analysis."}}
{"id": "2510.07613", "pdf": "https://arxiv.org/pdf/2510.07613", "abs": "https://arxiv.org/abs/2510.07613", "authors": ["Isabel Papadimitriou", "Jacob Prince"], "title": "Vocabulary embeddings organize linguistic structure early in language model training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) work by manipulating the geometry of input\nembedding vectors over multiple layers. Here, we ask: how are the input\nvocabulary representations of language models structured, and how and when does\nthis structure evolve over training? To answer this question, we use\nrepresentational similarity analysis, running a suite of experiments that\ncorrelate the geometric structure of the input embeddings and output embeddings\nof two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,\nand frequency-based metrics over the course of training. Our key findings are\nas follows: 1) During training, the vocabulary embedding geometry quickly\nconverges to high correlations with a suite of semantic and syntactic features;\n2) Embeddings of high-frequency and function words (e.g., \"the,\" \"of\") converge\nto their final vectors faster than lexical and low-frequency words, which\nretain some alignment with the bias in their random initializations. These\nfindings help map the dynamic trajectory by which input embeddings organize\naround linguistic structure, revealing distinct roles for word frequency and\nfunction. Our findings motivate a deeper study of how the evolution of\nvocabulary geometry may facilitate specific capability gains during model\ntraining.", "AI": {"tldr": "This paper studies how the geometry of input vocabulary representations in large language models evolves during training, focusing on semantic, syntactic, and frequency patterns.", "motivation": "To understand the structural dynamics of input embeddings during the training of LLMs, as this could reveal how linguistic properties influence model capabilities.", "method": "The authors used representational similarity analysis on two open-source models (Pythia 12B and OLMo 7B), correlating embedding geometry with various linguistic and frequency metrics throughout the training process.", "result": "They found that embedding structures align with semantic and syntactic features early in training, while high-frequency and function word embeddings stabilize faster than those of other word types.", "conclusion": "The study provides insights into how vocabulary embedding geometry evolves, emphasizing the roles of word frequency and function, and suggests this evolution is linked to capability gains during training."}}
{"id": "2510.07635", "pdf": "https://arxiv.org/pdf/2510.07635", "abs": "https://arxiv.org/abs/2510.07635", "authors": ["Haruka Kiyohara", "Yusuke Narita", "Yuta Saito", "Kei Tateno", "Takuma Udagawa"], "title": "Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning", "categories": ["cs.AI"], "comment": null, "summary": "In many real recommender systems, novel items are added frequently over time.\nThe importance of sufficiently presenting novel actions has widely been\nacknowledged for improving long-term user engagement. A recent work builds on\nOff-Policy Learning (OPL), which trains a policy from only logged data,\nhowever, the existing methods can be unsafe in the presence of novel actions.\nOur goal is to develop a framework to enforce exploration of novel actions with\na guarantee for safety. To this end, we first develop Safe Off-Policy Policy\nGradient (Safe OPG), which is a model-free safe OPL method based on a high\nconfidence off-policy evaluation. In our first experiment, we observe that Safe\nOPG almost always satisfies a safety requirement, even when existing methods\nviolate it greatly. However, the result also reveals that Safe OPG tends to be\ntoo conservative, suggesting a difficult tradeoff between guaranteeing safety\nand exploring novel actions. To overcome this tradeoff, we also propose a novel\nframework called Deployment-Efficient Policy Learning for Safe User\nExploration, which leverages safety margin and gradually relaxes safety\nregularization during multiple (not many) deployments. Our framework thus\nenables exploration of novel actions while guaranteeing safe implementation of\nrecommender systems.", "AI": {"tldr": "This paper introduces methods addressing the challenge of safely exploring novel actions in recommender systems, emphasizing models that ensure safety while promoting long-term user engagement.", "motivation": "Recommender systems frequently encounter novel items over time, and exploring them effectively is crucial for sustaining user engagement. However, achieving exploration without compromising user safety remains a challenge.", "method": "The paper presents Safe OPG (Safe Off-Policy Policy Gradient), which ensures safety in off-policy learning, and a Deployment-Efficient Policy Learning framework that uses safety margins and gradually relaxes safety constraints across multiple deployments.", "result": "Experiments show that Safe OPG maintains safety standards but is overly conservative, revealing a tradeoff between safety and effective exploration. The proposed Deployment-Efficient framework successfully balances this tradeoff during deployments.", "conclusion": "The developed methods enable safe exploration of novel items in recommender systems, offering practical solutions to ensure both safety and long-term user engagement."}}
{"id": "2510.07986", "pdf": "https://arxiv.org/pdf/2510.07986", "abs": "https://arxiv.org/abs/2510.07986", "authors": ["Gaofeng Li", "Peisen Xu", "Ruize Wang", "Qi Ye", "Jiming Chen", "Dezhen Song", "Yanlong Huang"], "title": "Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints", "categories": ["cs.RO"], "comment": null, "summary": "Orientation learning plays a pivotal role in many tasks. However, the\nrotation group SO(3) is a Riemannian manifold. As a result, the distortion\ncaused by non-Euclidean geometric nature introduces difficulties to the\nincorporation of local constraints, especially for the simultaneous\nincorporation of multiple local constraints. To address this issue, we propose\nthe Angle-Axis Space-based orientation representation method to solve several\norientation learning problems, including orientation adaptation and\nminimization of angular acceleration. Specifically, we propose a weighted\naverage mechanism in SO(3) based on the angle-axis representation method. Our\nmain idea is to generate multiple trajectories by considering different local\nconstraints at different basepoints. Then these multiple trajectories are fused\nto generate a smooth trajectory by our proposed weighted average mechanism,\nachieving the goal to incorporate multiple local constraints simultaneously.\nCompared with existing solution, ours can address the distortion issue and make\nthe off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean\nspace. Simulation and Experimental evaluations validate that our solution can\nnot only adapt orientations towards arbitrary desired via-points and cope with\nangular acceleration constraints, but also incorporate multiple local\nconstraints simultaneously to achieve extra benefits, e.g., achieving smaller\nacceleration costs.", "AI": {"tldr": "The paper introduces a method for learning orientations in the rotation group SO(3) by addressing distortions caused by non-Euclidean geometry using the Angle-Axis Space-based orientation representation approach.", "motivation": "The authors aim to tackle the challenges associated with orientation learning on the rotation group SO(3), specifically distortion issues stemming from its non-Euclidean nature, and enable simultaneous incorporation of multiple local constraints.", "method": "The authors propose a weighted average mechanism in SO(3) based on the angle-axis representation. Multiple trajectories are generated considering different local constraints and fused into a smooth trajectory using the weighted mechanism.", "result": "Through simulations and experiments, the method successfully adapts orientations, minimizes angular acceleration, and incorporates multiple local constraints to reduce acceleration costs.", "conclusion": "The proposed method mitigates geometric distortion issues, enabling standard Euclidean learning algorithms to be re-applied in SO(3), and demonstrates effective orientation adaptation and angular acceleration minimization while handling multiple constraints."}}
{"id": "2510.07589", "pdf": "https://arxiv.org/pdf/2510.07589", "abs": "https://arxiv.org/abs/2510.07589", "authors": ["Juan Viguera Diez", "Mathias Schreiner", "Simon Olsson"], "title": "Transferable Generative Models Bridge Femtosecond to Nanosecond Time-Step Molecular Dynamics", "categories": ["physics.chem-ph", "stat.ML"], "comment": null, "summary": "Understanding molecular structure, dynamics, and reactivity requires bridging\nprocesses that occur across widely separated time scales. Conventional\nmolecular dynamics simulations provide atomistic resolution, but their\nfemtosecond time steps limit access to the slow conformational changes and\nrelaxation processes that govern chemical function. Here, we introduce a deep\ngenerative modeling framework that accelerates sampling of molecular dynamics\nby four orders of magnitude while retaining physical realism. Applied to small\norganic molecules and peptides, the approach enables quantitative\ncharacterization of equilibrium ensembles and dynamical relaxation processes\nthat were previously only accessible by costly brute-force simulation.\nImportantly, the method generalizes across chemical composition and system\nsize, extrapolating to peptides larger than those used for training, and\ncaptures chemically meaningful transitions on extended time scales. By\nexpanding the accessible range of molecular motions without sacrificing\natomistic detail, this approach opens new opportunities for probing\nconformational landscapes, thermodynamics, and kinetics in systems central to\nchemistry and biophysics.", "AI": {"tldr": "This paper presents a deep generative modeling framework to accelerate molecular dynamics simulations by four orders of magnitude while maintaining physical realism.", "motivation": "The goal is to overcome the limitations of conventional molecular dynamics simulations, which are restricted by their femtosecond time steps and thus cannot access slower, functionally important molecular dynamics.", "method": "The authors implemented a deep generative modeling framework to enhance sampling of molecular dynamics, achieving faster yet physically accurate simulations for small organic molecules and peptides.", "result": "The framework quantitatively characterizes equilibrium ensembles and dynamical relaxation processes on extended time scales, generalizes across chemical compositions, and scales to larger peptides than those used for training.", "conclusion": "This approach enables deeper understanding of molecular behavior by bridging multiple time scales without compromising atomistic resolution, facilitating exploration of conformational landscapes, thermodynamics, and kinetics in chemistry and biophysics."}}
{"id": "2510.07670", "pdf": "https://arxiv.org/pdf/2510.07670", "abs": "https://arxiv.org/abs/2510.07670", "authors": ["Haoyi Duan", "Yunzhi Zhang", "Yilun Du", "Jiajun Wu"], "title": "Controllable Video Synthesis via Variational Inference", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://video-synthesis-variational.github.io/", "summary": "Many video workflows benefit from a mixture of user controls with varying\ngranularity, from exact 4D object trajectories and camera paths to coarse text\nprompts, while existing video generative models are typically trained for fixed\ninput formats. We develop a video synthesis method that addresses this need and\ngenerates samples with high controllability for specified elements while\nmaintaining diversity for under-specified ones. We cast the task as variational\ninference to approximate a composed distribution, leveraging multiple video\ngeneration backbones to account for all task constraints collectively. To\naddress the optimization challenge, we break down the problem into step-wise KL\ndivergence minimization over an annealed sequence of distributions, and further\npropose a context-conditioned factorization technique that reduces modes in the\nsolution space to circumvent local optima. Experiments suggest that our method\nproduces samples with improved controllability, diversity, and 3D consistency\ncompared to prior works.", "AI": {"tldr": "This paper introduces a method for video synthesis enabling high controllability over specified elements while preserving diversity for under-specified ones, using variational inference and step-wise KL divergence minimization.", "motivation": "Current video generation models struggle to handle mixed user controls, such as granular trajectory data alongside coarse text prompts, necessitating a new approach for flexible controllability and diversity.", "method": "The authors employ variational inference to approximate composed distributions, utilize multiple video generation backbones to manage constraints, and implement a step-wise KL divergence minimization with a context-conditioned factorization.", "result": "The proposed method showcases improved controllability, diversity, and 3D consistency in generated videos compared to previous models.", "conclusion": "The approach effectively combines distinct user controls and constraints for video synthesis, overcoming optimization challenges and outperforming prior methods."}}
{"id": "2510.07524", "pdf": "https://arxiv.org/pdf/2510.07524", "abs": "https://arxiv.org/abs/2510.07524", "authors": ["Mehdi Zekriyapanah Gashti", "Ghasem Farjamnia"], "title": "EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning", "categories": ["cs.LG", "cs.AI", "I.2.6; I.5.4"], "comment": "11 pages, 2 figures", "summary": "Accurate classification of sleep stages is crucial for the diagnosis and\nmanagement of sleep disorders. Conventional approaches for sleep scoring rely\non manual annotation or features extracted from EEG signals in the time or\nfrequency domain. This study proposes a novel framework for automated sleep\nstage scoring using time-frequency analysis based on the wavelet transform. The\nSleep-EDF Expanded Database (sleep-cassette recordings) was used for\nevaluation. The continuous wavelet transform (CWT) generated time-frequency\nmaps that capture both transient and oscillatory patterns across frequency\nbands relevant to sleep staging. Experimental results demonstrate that the\nproposed wavelet-based representation, combined with ensemble learning,\nachieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of\n73.15, outperforming conventional machine learning methods and exhibiting\ncomparable or superior performance to recent deep learning approaches. These\nfindings highlight the potential of wavelet analysis for robust, interpretable,\nand clinically applicable sleep stage classification.", "AI": {"tldr": "The paper introduces a wavelet-transform-based method for automated sleep stage classification, achieving significant accuracy (88.37%) and F1 score (73.15%) using ensemble learning and outperforming traditional methods.", "motivation": "Current sleep stage classification relies on manual annotation or basic computational methods, limiting efficiency and accuracy in addressing sleep disorders.", "method": "The study uses continuous wavelet transform (CWT) to generate time-frequency maps, combined with ensemble learning algorithms for sleep stage classification. It evaluates the method using the Sleep-EDF Expanded Database.", "result": "The method achieves an overall accuracy of 88.37% and a macro-averaged F1 score of 73.15, surpassing traditional machine learning techniques and rivaling deep learning approaches.", "conclusion": "Wavelet-based analysis shows promise for accurate, interpretable, and clinically useful sleep stage classification, enhancing diagnostic capabilities for sleep disorders."}}
{"id": "2510.07629", "pdf": "https://arxiv.org/pdf/2510.07629", "abs": "https://arxiv.org/abs/2510.07629", "authors": ["Zhangdie Yuan", "Han-Chin Shing", "Mitch Strong", "Chaitanya Shivade"], "title": "Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Accurate clinical coding is essential for healthcare documentation, billing,\nand decision-making. While prior work shows that off-the-shelf LLMs struggle\nwith this task, evaluations based on exact match metrics often overlook errors\nwhere predicted codes are hierarchically close but incorrect. Our analysis\nreveals that such hierarchical misalignments account for a substantial portion\nof LLM failures. We show that lightweight interventions, including prompt\nengineering and small-scale fine-tuning, can improve accuracy without the\ncomputational overhead of search-based methods. To address hierarchically\nnear-miss errors, we introduce clinical code verification as both a standalone\ntask and a pipeline component. To mitigate the limitations in existing\ndatasets, such as incomplete evidence and inpatient bias in MIMIC, we release\nan expert double-annotated benchmark of outpatient clinical notes with ICD-10\ncodes. Our results highlight verification as an effective and reliable step\ntoward improving LLM-based medical coding.", "AI": {"tldr": "The paper addresses the limitations of large language models (LLMs) in clinical coding accuracy, introduces improvements through lightweight interventions, and proposes clinical code verification as an effective solution.", "motivation": "Clinical coding accuracy is crucial for healthcare documentation and decision-making, but existing LLMs often make errors, especially in hierarchically close codes.", "method": "The authors utilized prompt engineering, small-scale fine-tuning, and introduced clinical code verification steps. They also created a double-annotated outpatient benchmark for robust evaluation.", "result": "Lightweight interventions improved clinical coding accuracy, and verification was shown to be effective in addressing hierarchically near-miss errors.", "conclusion": "Clinical code verification emerges as a reliable method to enhance LLM-based medical coding, and expert-annotated benchmarks prove valuable for addressing dataset limitations."}}
{"id": "2510.07709", "pdf": "https://arxiv.org/pdf/2510.07709", "abs": "https://arxiv.org/abs/2510.07709", "authors": ["Alhim Vera", "Karen Sanchez", "Carlos Hinojosa", "Haidar Bin Hamid", "Donghoon Kim", "Bernard Ghanem"], "title": "Multimodal Safety Evaluation in Generative Agent Social Simulations", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "comment": null, "summary": "Can generative agents be trusted in multimodal environments? Despite advances\nin large language and vision-language models that enable agents to act\nautonomously and pursue goals in rich settings, their ability to reason about\nsafety, coherence, and trust across modalities remains limited. We introduce a\nreproducible simulation framework for evaluating agents along three dimensions:\n(1) safety improvement over time, including iterative plan revisions in\ntext-visual scenarios; (2) detection of unsafe activities across multiple\ncategories of social situations; and (3) social dynamics, measured as\ninteraction counts and acceptance ratios of social exchanges. Agents are\nequipped with layered memory, dynamic planning, multimodal perception, and are\ninstrumented with SocialMetrics, a suite of behavioral and structural metrics\nthat quantifies plan revisions, unsafe-to-safe conversions, and information\ndiffusion across networks. Experiments show that while agents can detect direct\nmultimodal contradictions, they often fail to align local revisions with global\nsafety, reaching only a 55 percent success rate in correcting unsafe plans.\nAcross eight simulation runs with three models - Claude, GPT-4o mini, and\nQwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,\n55, and 58 percent, respectively. Overall performance ranged from 20 percent in\nmulti-risk scenarios with GPT-4o mini to 98 percent in localized contexts such\nas fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted\nwhen paired with misleading visuals, showing a strong tendency to overtrust\nimages. These findings expose critical limitations in current architectures and\nprovide a reproducible platform for studying multimodal safety, coherence, and\nsocial dynamics.", "AI": {"tldr": "This paper evaluates the trustworthiness of generative agents in multimodal settings using a simulation framework for safety, coherence, and social dynamics. Agents show limited success in ensuring global safety, with performance influenced by the scenarios and models used.", "motivation": "The paper aims to address the limited ability of generative agents, equipped with large language and vision models, to reason about safety, coherence, and trust in multimodal environments.", "method": "The authors introduce a simulation framework to test agents on three dimensions: safety improvement, unsafe activity detection, and social dynamics. The framework employs a suite of metrics (SocialMetrics) to quantify agent performance.", "result": "Findings indicate generative agents achieved varying success in correcting unsafe plans. Conversion rates from unsafe-to-safe actions were 75%, 55%, and 58% for Claude, GPT-4o mini, and Qwen-VL, respectively. Agents were overly influenced by misleading visuals, accepting 45% of unsafe actions in these cases.", "conclusion": "The study reveals critical weaknesses in current generative agent architectures for multimodal settings, highlighting the need for improved alignment between local revisions and global safety. The provided framework offers a reproducible method for further research in this domain."}}
{"id": "2510.08022", "pdf": "https://arxiv.org/pdf/2510.08022", "abs": "https://arxiv.org/abs/2510.08022", "authors": ["Kehui Liu", "Zhongjie Jia", "Yang Li", "Zhaxizhuoma", "Pengan Chen", "Song Liu", "Xin Liu", "Pingrui Zhang", "Haoming Song", "Xinyi Ye", "Nieqing Cao", "Zhigang Wang", "Jia Zeng", "Dong Wang", "Yan Ding", "Bin Zhao", "Xuelong Li"], "title": "FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Data-driven robotic manipulation learning depends on large-scale,\nhigh-quality expert demonstration datasets. However, existing datasets, which\nprimarily rely on human teleoperated robot collection, are limited in terms of\nscalability, trajectory smoothness, and applicability across different robotic\nembodiments in real-world environments. In this paper, we present FastUMI-100K,\na large-scale UMI-style multimodal demonstration dataset, designed to overcome\nthese limitations and meet the growing complexity of real-world manipulation\ntasks. Collected by FastUMI, a novel robotic system featuring a modular,\nhardware-decoupled mechanical design and an integrated lightweight tracking\nsystem, FastUMI-100K offers a more scalable, flexible, and adaptable solution\nto fulfill the diverse requirements of real-world robot demonstration data.\nSpecifically, FastUMI-100K contains over 100K+ demonstration trajectories\ncollected across representative household environments, covering 54 tasks and\nhundreds of object types. Our dataset integrates multimodal streams, including\nend-effector states, multi-view wrist-mounted fisheye images and textual\nannotations. Each trajectory has a length ranging from 120 to 500 frames.\nExperimental results demonstrate that FastUMI-100K enables high policy success\nrates across various baseline algorithms, confirming its robustness,\nadaptability, and real-world applicability for solving complex, dynamic\nmanipulation challenges. The source code and dataset will be released in this\nlink https://github.com/MrKeee/FastUMI-100K.", "AI": {"tldr": "The paper introduces FastUMI-100K, a large-scale, multimodal robotic manipulation dataset designed to address limitations in scalability and applicability in existing datasets.", "motivation": "Existing robotic manipulation datasets are constrained by scalability, smoothness of trajectories, and adaptability across robotic embodiments in real-world settings.", "method": "The authors propose FastUMI-100K, collected using FastUMI, a modular robotic system that integrates lightweight tracking and multimodal data streams like fisheye images and textual annotations.", "result": "FastUMI-100K comprises 100K+ trajectories across 54 tasks and hundreds of object types, demonstrating high policy success rates in baselines.", "conclusion": "The dataset enhances the robustness and applicability of manipulation learning systems, facilitating solutions for complex tasks in real-world environments."}}
{"id": "2510.07732", "pdf": "https://arxiv.org/pdf/2510.07732", "abs": "https://arxiv.org/abs/2510.07732", "authors": ["Yifan Chen", "Sifan Liu"], "title": "Rotated Mean-Field Variational Inference and Iterative Gaussianization", "categories": ["stat.CO", "stat.ML"], "comment": null, "summary": "We propose to perform mean-field variational inference (MFVI) in a rotated\ncoordinate system that reduces correlations between variables. The rotation is\ndetermined by principal component analysis (PCA) of a cross-covariance matrix\ninvolving the target's score function. Compared with standard MFVI along the\noriginal axes, MFVI in this rotated system often yields substantially more\naccurate approximations with negligible additional cost.\n  MFVI in a rotated coordinate system defines a rotation and a coordinatewise\nmap that together move the target closer to Gaussian. Iterating this procedure\nyields a sequence of transformations that progressively transforms the target\ntoward Gaussian. The resulting algorithm provides a computationally efficient\nway to construct flow-like transport maps: it requires only MFVI subproblems,\navoids large-scale optimization, and yields transformations that are easy to\ninvert and evaluate. In Bayesian inference tasks, we demonstrate that the\nproposed method achieves higher accuracy than standard MFVI, while maintaining\nmuch lower computational cost than conventional normalizing flows.", "AI": {"tldr": "The paper introduces mean-field variational inference (MFVI) in rotated coordinates using PCA to reduce variable correlations for more accurate approximations with minimal cost.", "motivation": "To improve the accuracy and efficiency of MFVI by addressing correlations between variables.", "method": "The authors propose using PCA-based rotation and iterative transformations to make MFVI closer to Gaussian while avoiding large-scale optimization.", "result": "Their method provides higher accuracy than standard MFVI and greater computational efficiency compared to normalizing flows.", "conclusion": "Rotated MFVI offers an effective, efficient framework for Bayesian inference, bridging accuracy and computational cost."}}
{"id": "2510.07692", "pdf": "https://arxiv.org/pdf/2510.07692", "abs": "https://arxiv.org/abs/2510.07692", "authors": ["Tangin Amir Smrity", "MD Zahin Muntaqim Hasan Muhammad Kafi", "Abu Saleh Musa Miah", "Najmul Hassan", "Yuichi Okuyama", "Nobuyoshi Asai", "Taro Suzuki", "Jungpil Shin"], "title": "Hybrid CNN-BYOL Approach for Fault Detection in Induction Motors Using Thermal Images", "categories": ["cs.CV"], "comment": null, "summary": "Induction motors (IMs) are indispensable in industrial and daily life, but\nthey are susceptible to various faults that can lead to overheating, wasted\nenergy consumption, and service failure. Early detection of faults is essential\nto protect the motor and prolong its lifespan. This paper presents a hybrid\nmethod that integrates BYOL with CNNs for classifying thermal images of\ninduction motors for fault detection. The thermal dataset used in this work\nincludes different operating states of the motor, such as normal operation,\noverload, and faults. We employed multiple deep learning (DL) models for the\nBYOL technique, ranging from popular architectures such as ResNet-50,\nDenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2.\nAdditionally, we introduced a new high-performance yet lightweight CNN model\nnamed BYOL-IMNet, which comprises four custom-designed blocks tailored for\nfault classification in thermal images. Our experimental results demonstrate\nthat the proposed BYOL-IMNet achieves 99.89\\% test accuracy and an inference\ntime of 5.7 ms per image, outperforming state-of-the-art models. This study\nhighlights the promising performance of the CNN-BYOL hybrid method in enhancing\naccuracy for detecting faults in induction motors, offering a robust\nmethodology for online monitoring in industrial settings.", "AI": {"tldr": "The study introduces BYOL-IMNet, a hybrid CNN model integrated with BYOL for fault detection in induction motors using thermal images, achieving superior accuracy and inference speed.", "motivation": "Fault detection in induction motors is crucial to prevent overheating, energy inefficiencies, and operational failures.", "method": "The researchers used thermal image datasets and tested multiple deep learning models integrated with BYOL for fault detection. They introduced BYOL-IMNet as a custom-designed lightweight CNN model.", "result": "BYOL-IMNet achieved a test accuracy of 99.89% and an inference speed of 5.7 ms per image, surpassing existing models.", "conclusion": "The proposed BYOL-IMNet model provides a robust, accurate, and efficient solution for fault detection in induction motors, aiding industrial online monitoring systems."}}
{"id": "2510.07536", "pdf": "https://arxiv.org/pdf/2510.07536", "abs": "https://arxiv.org/abs/2510.07536", "authors": ["Madeline Navarro", "Andrei Buciulea", "Samuel Rey", "Antonio G. Marques", "Santiago Segarra"], "title": "Estimating Fair Graphs from Graph-Stationary Data", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "We estimate fair graphs from graph-stationary nodal observations such that\nconnections are not biased with respect to sensitive attributes. Edges in\nreal-world graphs often exhibit preferences for connecting certain pairs of\ngroups. Biased connections can not only exacerbate but even induce unfair\ntreatment for downstream graph-based tasks. We therefore consider group and\nindividual fairness for graphs corresponding to group- and node-level\ndefinitions, respectively. To evaluate the fairness of a given graph, we\nprovide multiple bias metrics, including novel measurements in the spectral\ndomain. Furthermore, we propose Fair Spectral Templates (FairSpecTemp), an\noptimization-based method with two variants for estimating fair graphs from\nstationary graph signals, a general model for graph data subsuming many\nexisting ones. One variant of FairSpecTemp exploits commutativity properties of\ngraph stationarity while directly constraining bias, while the other implicitly\nencourages fair estimates by restricting bias in the graph spectrum and is thus\nmore flexible. Our methods enjoy high probability performance bounds, yielding\na conditional tradeoff between fairness and accuracy. In particular, our\nanalysis reveals that accuracy need not be sacrificed to recover fair graphs.\nWe evaluate FairSpecTemp on synthetic and real-world data sets to illustrate\nits effectiveness and highlight the advantages of both variants of\nFairSpecTemp.", "AI": {"tldr": "The paper introduces Fair Spectral Templates (FairSpecTemp), an optimization-based method to create fair graphs from graph-stationary signals by addressing bias in connections without sacrificing accuracy.", "motivation": "Graph connections are often biased against sensitive attributes, which leads to unfair treatment in graph-based tasks. The study aims to address both group- and individual-level fairness in graphs.", "method": "The authors developed FairSpecTemp with two variants: one directly constraining bias through commutativity properties, and another indirectly restricting bias in the graph spectrum for more flexibility. Bias metrics, including spectral domain measurements, are used to evaluate fairness.", "result": "FairSpecTemp demonstrates its capability to construct fair graphs while maintaining accuracy, supported by evaluations on synthetic and real-world datasets.", "conclusion": "The paper concludes that fairness in graphs can be achieved without compromising accuracy, showcasing FairSpecTemp as an effective and adaptable tool."}}
{"id": "2510.07642", "pdf": "https://arxiv.org/pdf/2510.07642", "abs": "https://arxiv.org/abs/2510.07642", "authors": ["\u0110or\u0111e Klisura", "Joseph Khoury", "Ashish Kundu", "Ram Krishnan", "Anthony Rios"], "title": "Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "8 pages + Appendix", "summary": "Access control is a cornerstone of secure computing, yet large language\nmodels often blur role boundaries by producing unrestricted responses. We study\nrole-conditioned refusals, focusing on the LLM's ability to adhere to access\ncontrol policies by answering when authorized and refusing when not. To\nevaluate this behavior, we created a novel dataset that extends the Spider and\nBIRD text-to-SQL datasets, both of which have been modified with realistic\nPostgreSQL role-based policies at the table and column levels. We compare three\ndesigns: (i) zero or few-shot prompting, (ii) a two-step generator-verifier\npipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that\nlearn permission awareness directly. Across multiple model families, explicit\nverification (the two-step framework) improves refusal precision and lowers\nfalse permits. At the same time, fine-tuning achieves a stronger balance\nbetween safety and utility (i.e., when considering execution accuracy). Longer\nand more complex policies consistently reduce the reliability of all systems.\nWe release RBAC-augmented datasets and code.", "AI": {"tldr": "The paper examines the ability of large language models (LLMs) to adhere to access control policies, particularly through role-conditioned refusals, and proposes methods to improve this ability.", "motivation": "Many large language models fail to precisely enforce access control policies, creating security risks by producing unrestricted responses.", "method": "The researchers created a novel dataset by augmenting existing text-to-SQL datasets (Spider and BIRD) with realistic PostgreSQL role-based policies. They tested three methods: zero/few-shot prompting, a two-step generator-verifier pipeline, and LoRA fine-tuned models.", "result": "The two-step framework improved refusal precision and reduced false permits, while the LoRA fine-tuned models achieved a better balance between safety and execution accuracy.", "conclusion": "Longer and more complex policies reduce system reliability, and their RBAC-augmented datasets and methods offer insights for addressing access control issues in LLMs."}}
{"id": "2510.07715", "pdf": "https://arxiv.org/pdf/2510.07715", "abs": "https://arxiv.org/abs/2510.07715", "authors": ["Xiaochen Tang", "Zhenya Zhang", "Miaomiao Zhang", "Jie An"], "title": "Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning", "categories": ["cs.AI"], "comment": "14 pages, 4 figures, 6 tables, accepted by RTSS 2025", "summary": "In real-time and safety-critical cyber-physical systems (CPSs), control\nsynthesis must guarantee that generated policies meet stringent timing and\ncorrectness requirements under uncertain and dynamic conditions. Signal\ntemporal logic (STL) has emerged as a powerful formalism of expressing\nreal-time constraints, with its semantics enabling quantitative assessment of\nsystem behavior. Meanwhile, reinforcement learning (RL) has become an important\nmethod for solving control synthesis problems in unknown environments. Recent\nstudies incorporate STL-based reward functions into RL to automatically\nsynthesize control policies. However, the automatically inferred rewards\nobtained by these methods represent the global assessment of a whole or partial\npath but do not accumulate the rewards of local changes accurately, so the\nsparse global rewards may lead to non-convergence and unstable training\nperformances. In this paper, we propose an online reward generation method\nguided by the online causation monitoring of STL. Our approach continuously\nmonitors system behavior against an STL specification at each control step,\ncomputing the quantitative distance toward satisfaction or violation and\nthereby producing rewards that reflect instantaneous state dynamics.\nAdditionally, we provide a smooth approximation of the causation semantics to\novercome the discontinuity of the causation semantics and make it\ndifferentiable for using deep-RL methods. We have implemented a prototype tool\nand evaluated it in the Gym environment on a variety of continuously controlled\nbenchmarks. Experimental results show that our proposed STL-guided RL method\nwith online causation semantics outperforms existing relevant STL-guided RL\nmethods, providing a more robust and efficient reward generation framework for\ndeep-RL.", "AI": {"tldr": "The paper proposes an online reward generation approach for reinforcement learning (RL) guided by Signal Temporal Logic (STL), addressing limitations of sparse rewards in existing STL-based RL methods.", "motivation": "Existing RL methods incorporating STL-based rewards face issues with sparse global rewards that can lead to training instability and non-convergence.", "method": "The authors introduce a method that continuously monitors system behavior against STL specifications at each control step, computing instantaneous rewards based on quantitative satisfaction or violation of constraints. They also introduce a smooth approximation of causation semantics to make it compatible with deep-RL.", "result": "Experimental evaluations using a prototype tool on benchmarks in the Gym environment demonstrated that the method outperforms existing STL-guided RL approaches in robustness and efficiency.", "conclusion": "The proposed STL-guided RL framework offers a more effective reward generation mechanism, enhancing training stability and performance in reinforcement learning for cyber-physical systems."}}
{"id": "2510.08044", "pdf": "https://arxiv.org/pdf/2510.08044", "abs": "https://arxiv.org/abs/2510.08044", "authors": ["Shiyuan Yin", "Chenjia Bai", "Zihao Zhang", "Junwei Jin", "Xinxin Zhang", "Chi Zhang", "Xuelong Li"], "title": "Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) demonstrate advanced reasoning abilities,\nenabling robots to understand natural language instructions and generate\nhigh-level plans with appropriate grounding. However, LLM hallucinations\npresent a significant challenge, often leading to overconfident yet potentially\nmisaligned or unsafe plans. While researchers have explored uncertainty\nestimation to improve the reliability of LLM-based planning, existing studies\nhave not sufficiently differentiated between epistemic and intrinsic\nuncertainty, limiting the effectiveness of uncertainty estimation. In this\npaper, we present Combined Uncertainty estimation for Reliable Embodied\nplanning (CURE), which decomposes the uncertainty into epistemic and intrinsic\nuncertainty, each estimated separately. Furthermore, epistemic uncertainty is\nsubdivided into task clarity and task familiarity for more accurate evaluation.\nThe overall uncertainty assessments are obtained using random network\ndistillation and multi-layer perceptron regression heads driven by LLM\nfeatures. We validated our approach in two distinct experimental settings:\nkitchen manipulation and tabletop rearrangement experiments. The results show\nthat, compared to existing methods, our approach yields uncertainty estimates\nthat are more closely aligned with the actual execution outcomes.", "AI": {"tldr": "The paper introduces CURE, a method to enhance the reliability of planning in robots using LLMs by effectively decomposing and estimating uncertainties.", "motivation": "LLMs are capable of understanding instructions and planning but often suffer from hallucinations or unreliable predictions, posing risks to the safety and alignment of plans.", "method": "CURE uses techniques like random network distillation and multi-layer perceptron to separately estimate intrinsic and epistemic uncertainties in planning tasks.", "result": "Experimental validations on kitchen manipulation and tabletop rearrangement tasks showed CURE's uncertainty measures strongly corresponded with execution outcomes, surpassing other existing approaches.", "conclusion": "Decomposing uncertainties enhances the reliability of embodied planning with LLMs, proving CURE to be a promising solution for safer robot planning and execution."}}
{"id": "2510.07904", "pdf": "https://arxiv.org/pdf/2510.07904", "abs": "https://arxiv.org/abs/2510.07904", "authors": ["Enrico Ampellio", "Blazhe Gjorgiev", "Giovanni Sansavini"], "title": "Multi-level informed optimization via decomposed Kriging for large design problems under uncertainty", "categories": ["eess.SY", "cs.LG", "cs.SY", "stat.ML", "60G15 (Primary) 68T37, 90C26 (Secondary)", "C.4; G.1.6; G.3; G.4; I.2.3; I.5.1; J.2"], "comment": "34 pages, 18 figures", "summary": "Engineering design involves demanding models encompassing many decision\nvariables and uncontrollable parameters. In addition, unavoidable aleatoric and\nepistemic uncertainties can be very impactful and add further complexity. The\nstate-of-the-art adopts two steps, uncertainty quantification and design\noptimization, to optimize systems under uncertainty by means of robust or\nstochastic metrics. However, conventional scenario-based, surrogate-assisted,\nand mathematical programming methods are not sufficiently scalable to be\naffordable and precise in large and complex cases. Here, a multi-level approach\nis proposed to accurately optimize resource-intensive, high-dimensional, and\ncomplex engineering problems under uncertainty with minimal resources. A\nnon-intrusive, fast-scaling, Kriging-based surrogate is developed to map the\ncombined design/parameter domain efficiently. Multiple surrogates are\nadaptively updated by hierarchical and orthogonal decomposition to leverage the\nfewer and most uncertainty-informed data. The proposed method is statistically\ncompared to the state-of-the-art via an analytical testbed and is shown to be\nconcurrently faster and more accurate by orders of magnitude.", "AI": {"tldr": "The paper addresses challenges in optimizing complex engineering designs under uncertainty by proposing a scalable, Kriging-based surrogate model offering higher accuracy and speed.", "motivation": "Conventional methods struggle with scalability and precision in optimizing large, complex engineering designs under uncertainty.", "method": "The authors developed a multi-level approach using Kriging-based surrogates and hierarchical decomposition for efficient optimization within high-dimensional spaces.", "result": "The proposed method outperformed state-of-the-art techniques in both speed and accuracy during statistical comparisons on an analytical testbed.", "conclusion": "The approach is a robust and scalable solution for accurately optimizing complex engineering problems under uncertainty, requiring minimal resources."}}
{"id": "2510.07703", "pdf": "https://arxiv.org/pdf/2510.07703", "abs": "https://arxiv.org/abs/2510.07703", "authors": ["Xiaoxu Ma", "Runhao Li", "Zhenyu Weng"], "title": "Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Deep hashing has been widely adopted for large-scale image retrieval, with\nnumerous strategies proposed to optimize hash function learning. Pairwise-based\nmethods are effective in learning hash functions that preserve local similarity\nrelationships, whereas center-based methods typically achieve superior\nperformance by more effectively capturing global data distributions. However,\nthe strength of center-based methods in modeling global structures often comes\nat the expense of underutilizing important local similarity information. To\naddress this limitation, we propose Mutual Learning for Hashing (MLH), a novel\nweak-to-strong framework that enhances a center-based hashing branch by\ntransferring knowledge from a weaker pairwise-based branch. MLH consists of two\nbranches: a strong center-based branch and a weaker pairwise-based branch.\nThrough an iterative mutual learning process, the center-based branch leverages\nlocal similarity cues learned by the pairwise-based branch. Furthermore,\ninspired by the mixture-of-experts paradigm, we introduce a novel\nmixture-of-hash-experts module that enables effective cross-branch interaction,\nfurther enhancing the performance of both branches. Extensive experiments\ndemonstrate that MLH consistently outperforms state-of-the-art hashing methods\nacross multiple benchmark datasets.", "AI": {"tldr": "This paper introduces Mutual Learning for Hashing (MLH), enhancing hashing performance by combining strengths of pairwise-based and center-based methods.", "motivation": "The need to address weaknesses in center-based and pairwise-based hashing methods for optimizing large-scale image retrieval.", "method": "Developing a mutual learning framework with two branches\u2014center-based and pairwise-based\u2014alongside a mixture-of-hash-experts module for cross-branch interaction.", "result": "MLH consistently delivers superior results compared to state-of-the-art hashing methods on various benchmark datasets.", "conclusion": "By combining local similarity cues and global data structures, MLH bridges the gap between pairwise-based and center-based approaches for improved image retrieval."}}
{"id": "2510.07549", "pdf": "https://arxiv.org/pdf/2510.07549", "abs": "https://arxiv.org/abs/2510.07549", "authors": ["Qifan Chen", "Zhongshu Xu", "Jinjin Zhang", "Dongbin Xiu"], "title": "Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.flu-dyn"], "comment": null, "summary": "We present a numerical framework for constructing a targeted digital twin\n(tDT) that directly models the dynamics of quantities of interest (QoIs) in a\nfull digital twin (DT). The proposed approach employs memory-based flow map\nlearning (FML) to develop a data-driven model of the QoIs using short bursts of\ntrajectory data generated through repeated executions of the full DT. This\nrenders the construction of the FML-based tDT an entirely offline computational\nprocess. During online simulation, the learned tDT can efficiently predict and\nanalyze the long-term dynamics of the QoIs without requiring simulations of the\nfull DT system, thereby achieving substantial computational savings. After\nintroducing the general numerical procedure, we demonstrate the construction\nand predictive capability of the tDT in a computational fluid dynamics (CFD)\nexample: two-dimensional incompressible flow past a cylinder. The QoIs in this\nproblem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs\nare compact dynamical systems that evolve these forces without explicit\nknowledge of the underlying flow field. Numerical results show that the tDTs\nyield accurate long-term predictions of the forces while entirely bypassing\nfull flow simulations.", "AI": {"tldr": "The paper proposes a numerical method to create targeted digital twins (tDT) that model core system dynamics more efficiently than full digital twins (DT), achieving computational savings.", "motivation": "To address the computational inefficiency of full digital twins, the study aims to develop a compact and targeted model focusing on specific quantities of interest (QoIs).", "method": "The approach uses memory-based flow map learning (FML), applying trajectory data from full DT simulations offline to create predictive models for key QoIs. Online simulations rely exclusively on the tDT.", "result": "In a case study on fluid dynamics around a cylinder, the tDT accurately predicted hydrodynamic forces without requiring detailed flow field simulations, saving computational resources.", "conclusion": "The study demonstrates the viability of targeting specific dynamical aspects via tDTs, proving their efficiency for predictive simulation tasks while avoiding the complexity and computational expense of full DT systems."}}
{"id": "2510.07645", "pdf": "https://arxiv.org/pdf/2510.07645", "abs": "https://arxiv.org/abs/2510.07645", "authors": ["Xin Jie Chua", "Jeraelyn Ming Li Tan", "Jia Xuan Tan", "Soon Chang Poh", "Yi Xian Goh", "Debbie Hui Tian Choong", "Chee Mun Foong", "Sze Jue Yang", "Chee Seng Chan"], "title": "Banking Done Right: Redefining Retail Banking with Language-Centric AI", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP2025 Industry Track", "summary": "This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt\nBank to enable customers to execute core financial transactions through natural\nlanguage conversation. This represents the first global regulator-approved\ndeployment worldwide where conversational AI functions as the primary banking\ninterface, in contrast to prior assistants that have been limited to advisory\nor support roles. Built entirely in-house, Ryt AI is powered by ILMU, a\nclosed-source LLM developed internally, and replaces rigid multi-screen\nworkflows with a single dialogue orchestrated by four LLM-powered agents\n(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific\nLoRA adapter to ILMU, which is hosted within the bank's infrastructure to\nensure consistent behavior with minimal overhead. Deterministic guardrails,\nhuman-in-the-loop confirmation, and a stateless audit architecture provide\ndefense-in-depth for security and compliance. The result is Banking Done Right:\ndemonstrating that regulator-approved natural-language interfaces can reliably\nsupport core financial operations under strict governance.", "AI": {"tldr": "This paper introduces Ryt AI and Ryt Bank, showcasing the first regulator-approved conversational AI system for executing financial transactions.", "motivation": "To make financial interactions easier and secure by using conversational AI as the main banking interface.", "method": "Ryt AI uses ILMU, an LLM along with four specialized agents powered by LoRA adapters, ensuring secure and compliant operation.", "result": "The framework successfully replaces traditional workflows, enabling regulator-compliant core financial transactions through natural language.", "conclusion": "Ryt AI proves conversational AI's potential in banking by meeting strict regulatory requirements for secure financial operations."}}
{"id": "2510.07731", "pdf": "https://arxiv.org/pdf/2510.07731", "abs": "https://arxiv.org/abs/2510.07731", "authors": ["Ruiling Xu", "Yifan Zhang", "Qingyun Wang", "Carl Edwards", "Heng Ji"], "title": "oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Main Text: 8 pages, In total: 37 pages, 9 figures", "summary": "Organic reaction mechanisms are the stepwise elementary reactions by which\nreactants form intermediates and products, and are fundamental to understanding\nchemical reactivity and designing new molecules and reactions. Although large\nlanguage models (LLMs) have shown promise in understanding chemical tasks such\nas synthesis design, it is unclear to what extent this reflects genuine\nchemical reasoning capabilities, i.e., the ability to generate valid\nintermediates, maintain chemical consistency, and follow logically coherent\nmulti-step pathways. We address this by introducing oMeBench, the first\nlarge-scale, expert-curated benchmark for organic mechanism reasoning in\norganic chemistry. It comprises over 10,000 annotated mechanistic steps with\nintermediates, type labels, and difficulty ratings. Furthermore, to evaluate\nLLM capability more precisely and enable fine-grained scoring, we propose oMeS,\na dynamic evaluation framework that combines step-level logic and chemical\nsimilarity. We analyze the performance of state-of-the-art LLMs, and our\nresults show that although current models display promising chemical intuition,\nthey struggle with correct and consistent multi-step reasoning. Notably, we\nfind that using prompting strategy and fine-tuning a specialist model on our\nproposed dataset increases performance by 50% over the leading closed-source\nmodel. We hope that oMeBench will serve as a rigorous foundation for advancing\nAI systems toward genuine chemical reasoning.", "AI": {"tldr": "This paper introduces a benchmark (oMeBench) and scoring framework (oMeS) to evaluate the chemical reasoning capabilities of large language models (LLMs) in organic reaction mechanisms, finding promising but inconsistent performance.", "motivation": "The study aims to assess whether LLMs genuinely exhibit chemical reasoning capabilities, such as generating valid intermediates and maintaining logical coherence, which are essential for advancing in the field of organic chemistry.", "method": "The researchers created oMeBench, a large-scale dataset of mechanistic steps annotated by experts, and introduced oMeS, a dynamic scoring framework combining chemical similarity and step-level logic for evaluating LLM performance.", "result": "Current LLMs show some chemical intuition but have difficulty with consistent multi-step reasoning. Fine-tuning models on oMeBench improved their performance significantly, by 50% over the leading closed-source LLM.", "conclusion": "oMeBench provides a rigorous basis for evaluating and improving AI systems in chemical reasoning, fostering advancements in organic chemistry simulations and molecule design."}}
{"id": "2510.08106", "pdf": "https://arxiv.org/pdf/2510.08106", "abs": "https://arxiv.org/abs/2510.08106", "authors": ["Zihan Li", "Yixiao Xu", "Lei Zhang", "Taiyu Han", "Xinshan Yang", "Yingni Wang", "Mingxuan Liu", "Shenghai Xin", "Linxun Liu", "Hongen Liao", "Guochen Ning"], "title": "Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography", "categories": ["cs.RO"], "comment": null, "summary": "Liver disease is a major global health burden. While ultrasound is the\nfirst-line diagnostic tool, liver sonography requires locating multiple\nnon-continuous planes from positions where target structures are often not\nvisible, for biometric assessment and lesion detection, requiring significant\nexpertise. However, expert sonographers are severely scarce in resource-limited\nregions. Here, we develop an autonomous lightweight ultrasound robot comprising\nan AI agent that integrates multi-modal perception with memory attention for\nlocalization of unseen target structures, and a 588-gram 6-degrees-of-freedom\ncable-driven robot. By mounting on the abdomen, the system enhances robustness\nagainst motion. Our robot can autonomously acquire expert-level standard liver\nultrasound planes and detect pathology in patients, including two from Xining,\na 2261-meter-altitude city with limited medical resources. Our system performs\neffectively on rapid-motion individuals and in wilderness environments. This\nwork represents the first demonstration of autonomous sonography across\nmultiple challenging scenarios, potentially transforming access to expert-level\ndiagnostics in underserved regions.", "AI": {"tldr": "An autonomous ultrasound robot was developed for liver diagnostics, featuring an AI-powered system and a lightweight robotic arm, aiming to address expert shortages in remote areas.", "motivation": "To address the shortage of expert sonographers in resource-limited regions and provide robust liver diagnostic tools in challenging scenarios.", "method": "The system comprises an AI agent with multi-modal perception and memory attention for unseen target localization, integrated with a lightweight 6-degree-of-freedom cable-driven robot mounted on the abdomen.", "result": "The robot autonomously achieved expert-level liver ultrasound planes and detected pathology effectively, even in high-altitude cities and during rapid motion or wilderness environments.", "conclusion": "This autonomous system could revolutionize access to expert liver diagnostics, particularly in underserved or remote areas."}}
{"id": "2510.07935", "pdf": "https://arxiv.org/pdf/2510.07935", "abs": "https://arxiv.org/abs/2510.07935", "authors": ["Diego Garc\u00eda-P\u00e9rez", "Emilio Parrado-Hern\u00e1ndez", "John Shawe-Taylor"], "title": "Some theoretical improvements on the tightness of PAC-Bayes risk certificates for neural networks", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "This paper presents four theoretical contributions that improve the usability\nof risk certificates for neural networks based on PAC-Bayes bounds. First, two\nbounds on the KL divergence between Bernoulli distributions enable the\nderivation of the tightest explicit bounds on the true risk of classifiers\nacross different ranges of empirical risk. The paper next focuses on the\nformalization of an efficient methodology based on implicit differentiation\nthat enables the introduction of the optimization of PAC-Bayesian risk\ncertificates inside the loss/objective function used to fit the network/model.\nThe last contribution is a method to optimize bounds on non-differentiable\nobjectives such as the 0-1 loss. These theoretical contributions are\ncomplemented with an empirical evaluation on the MNIST and CIFAR-10 datasets.\nIn fact, this paper presents the first non-vacuous generalization bounds on\nCIFAR-10 for neural networks.", "AI": {"tldr": "The paper introduces theoretical advancements and applications of PAC-Bayes bounds for neural networks, with a focus on optimizing risk certificates and achieving tight generalization bounds.", "motivation": "To make PAC-Bayes risk certificates more useful and applicable for neural networks by improving their theoretical foundation, optimizing methodologies, and achieving credible empirical results.", "method": "The authors propose new bounds on KL divergence, utilize implicit differentiation in PAC-Bayes risk certificate optimization, and design methods for non-differentiable objectives, complemented by experiments on well-known datasets like MNIST and CIFAR-10.", "result": "The theoretical contributions are validated with empirical tests, achieving the first non-vacuous generalization bounds on the CIFAR-10 dataset for neural networks.", "conclusion": "This work provides significant advancements in the practical usability of PAC-Bayes bounds for neural network classifiers, showing tighter risks and broader applicability."}}
{"id": "2510.07721", "pdf": "https://arxiv.org/pdf/2510.07721", "abs": "https://arxiv.org/abs/2510.07721", "authors": ["Zipeng Guo", "Lichen Ma", "Xiaolong Fu", "Gaojing Zhou", "Lan Yang", "Yuchen Zhou", "Linkai Liu", "Yu He", "Ximan Liu", "Shiping Dong", "Jingling Fu", "Zhen Chen", "Yu Shi", "Junshi Huang", "Jason Li", "Chao Gou"], "title": "RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "In web data, product images are central to boosting user engagement and\nadvertising efficacy on e-commerce platforms, yet the intrusive elements such\nas watermarks and promotional text remain major obstacles to delivering clear\nand appealing product visuals. Although diffusion-based inpainting methods have\nadvanced, they still face challenges in commercial settings due to unreliable\nobject removal and limited domain-specific adaptation. To tackle these\nchallenges, we propose Repainter, a reinforcement learning framework that\nintegrates spatial-matting trajectory refinement with Group Relative Policy\nOptimization (GRPO). Our approach modulates attention mechanisms to emphasize\nbackground context, generating higher-reward samples and reducing unwanted\nobject insertion. We also introduce a composite reward mechanism that balances\nglobal, local, and semantic constraints, effectively reducing visual artifacts\nand reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality,\nlarge-scale e-commerce inpainting dataset, and a standardized benchmark\nEcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that\nRepainter significantly outperforms state-of-the-art methods, especially in\nchallenging scenes with intricate compositions. We will release our code and\nweights upon acceptance.", "AI": {"tldr": "The paper proposes Repainter, a novel reinforcement learning framework for enhancing product image inpainting on e-commerce platforms, outperforming current state-of-the-art methods.", "motivation": "Intrusive elements like watermarks and promotional text diminish the quality of product visuals on e-commerce platforms, necessitating effective inpainting solutions.", "method": "Repainter leverages reinforcement learning using spatial-matting trajectory refinement and Group Relative Policy Optimization (GRPO), alongside a composite reward mechanism balancing constraints.", "result": "Extensive experiments show Repainter performs significantly better than state-of-the-art methods, especially in complex scenes, and introduces new benchmarks and datasets for evaluation.", "conclusion": "Repainter offers a promising solution for improving e-commerce image quality by effectively addressing visual artifacts and undesired object insertion challenges in inpainting tasks."}}
{"id": "2510.07554", "pdf": "https://arxiv.org/pdf/2510.07554", "abs": "https://arxiv.org/abs/2510.07554", "authors": ["L\u00e9na\u00efc Chizat", "Pierre Marion", "Yerkin Yesbay"], "title": "Phase Diagram of Dropout for Two-Layer Neural Networks in the Mean-Field Regime", "categories": ["cs.LG", "68T07, 60K35, 82C22, 37N40"], "comment": null, "summary": "Dropout is a standard training technique for neural networks that consists of\nrandomly deactivating units at each step of their gradient-based training. It\nis known to improve performance in many settings, including in the large-scale\ntraining of language or vision models. As a first step towards understanding\nthe role of dropout in large neural networks, we study the large-width\nasymptotics of gradient descent with dropout on two-layer neural networks with\nthe mean-field initialization scale. We obtain a rich asymptotic phase diagram\nthat exhibits five distinct nondegenerate phases depending on the relative\nmagnitudes of the dropout rate, the learning rate, and the width. Notably, we\nfind that the well-studied \"penalty\" effect of dropout only persists in the\nlimit with impractically small learning rates of order $O(1/\\text{width})$. For\nlarger learning rates, this effect disappears and in the limit, dropout is\nequivalent to a \"random geometry\" technique, where the gradients are thinned\nrandomly after the forward and backward pass have been computed. In this\nasymptotic regime, the limit is described by a mean-field jump process where\nthe neurons' update times follow independent Poisson or Bernoulli clocks\n(depending on whether the learning rate vanishes or not). For some of the\nphases, we obtain a description of the limit dynamics both in path-space and in\ndistribution-space. The convergence proofs involve a mix of tools from\nmean-field particle systems and stochastic processes. Together, our results lay\nthe groundwork for a renewed theoretical understanding of dropout in\nlarge-scale neural networks.", "AI": {"tldr": "This paper explores the role and asymptotic effects of dropout in large two-layer neural networks, providing a phase diagram with five distinct behaviors depending on dropout rate, learning rate, and network width.", "motivation": "The study aims to deepen understanding of the dropout mechanism, which is widely applied in neural network training but lacks theoretical insights, particularly in large-scale models.", "method": "Analyzing large-width asymptotics of gradient descent with dropout on two-layer neural networks utilizing mean-field initialization and stochastic process techniques.", "result": "A phase diagram with five behaviors, showcasing how dropout's penalty effect fades with practical learning rates, and demonstrating dropout's equivalence to random geometry techniques in specific regimes.", "conclusion": "This work offers a theoretical framework to understand the dynamics of dropout in large-scale networks, suggesting implications for improved training techniques."}}
{"id": "2510.07651", "pdf": "https://arxiv.org/pdf/2510.07651", "abs": "https://arxiv.org/abs/2510.07651", "authors": ["Yuzhe Gu", "Xiyu Liang", "Jiaojiao Zhao", "Enmao Diao"], "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy.", "AI": {"tldr": "The paper introduces OBCache, a method to optimize cache eviction in large language models by quantitatively assessing token saliency via their impact on attention outputs.", "motivation": "Large language models with extensive context windows are powerful but face memory overhead due to caching key-value states, which scales with sequence length and batch size.", "method": "OBCache formulates cache eviction as a structured pruning problem using Optimal Brain Damage theory to measure token saliency with closed-form scores for key-value pairs.", "result": "Experiments using LLaMA and Qwen models show that OBCache's scores improve long-context accuracy over heuristic methods.", "conclusion": "OBCache enhances existing cache eviction strategies by utilizing output-aware signals, offering a principled approach for efficient memory usage in LLMs."}}
{"id": "2510.07733", "pdf": "https://arxiv.org/pdf/2510.07733", "abs": "https://arxiv.org/abs/2510.07733", "authors": ["Minh-Anh Nguye", "Minh-Duc Nguyen", "Nguyen Thi Ha Lan", "Kieu Hai Dang", "Nguyen Tien Dong", "Le Duy Dung"], "title": "SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly adopted for automating survey\npaper generation \\cite{wang2406autosurvey, liang2025surveyx,\nyan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing\napproaches typically extract content from a large collection of related papers\nand prompt LLMs to summarize them directly. However, such methods often\noverlook the structural relationships among papers, resulting in generated\nsurveys that lack a coherent taxonomy and a deeper contextual understanding of\nresearch progress. To address these shortcomings, we propose \\textbf{SurveyG},\nan LLM-based agent framework that integrates \\textit{hierarchical citation\ngraph}, where nodes denote research papers and edges capture both citation\ndependencies and semantic relatedness between their contents, thereby embedding\nstructural and contextual knowledge into the survey generation process. The\ngraph is organized into three layers: \\textbf{Foundation},\n\\textbf{Development}, and \\textbf{Frontier}, to capture the evolution of\nresearch from seminal works to incremental advances and emerging directions. By\ncombining horizontal search within layers and vertical depth traversal across\nlayers, the agent produces multi-level summaries, which are consolidated into a\nstructured survey outline. A multi-agent validation stage then ensures\nconsistency, coverage, and factual accuracy in generating the final survey.\nExperiments, including evaluations by human experts and LLM-as-a-judge,\ndemonstrate that SurveyG outperforms state-of-the-art frameworks, producing\nsurveys that are more comprehensive and better structured to the underlying\nknowledge taxonomy of a field.", "AI": {"tldr": "SurveyG leverages a hierarchical citation graph to generate structured and comprehensive surveys using large language models, outperforming existing state-of-the-art methods.", "motivation": "The paper aims to address the shortcomings of existing automated survey generation methods that fail to incorporate the structural and contextual relationships among research papers, often resulting in incoherent taxonomies.", "method": "SurveyG uses a hierarchical citation graph with layers\u2014Foundation, Development, and Frontier\u2014to integrate citation dependencies and semantic relationships. It combines horizontal and vertical searches for multi-level summarization and employs a multi-agent validation approach for accuracy.", "result": "Experiments demonstrate that SurveyG produces surveys that are more structured, comprehensive, and aligned with field-specific taxonomies compared to existing methods, according to both human experts and LLM evaluations.", "conclusion": "SurveyG effectively enhances the quality of automatic survey paper generation by providing a robust structure, contextual understanding, and higher accuracy in capturing research evolution, outperforming previous frameworks."}}
{"id": "2510.08102", "pdf": "https://arxiv.org/pdf/2510.08102", "abs": "https://arxiv.org/abs/2510.08102", "authors": ["Daiki Chijiwa", "Taku Hasegawa", "Kyosuke Nishida", "Shin'ya Yamaguchi", "Tomoya Ohba", "Tamao Sakao", "Susumu Takeuchi"], "title": "Lossless Vocabulary Reduction for Auto-Regressive Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Tokenization -- the process of decomposing a given text into a sequence of\nsubwords called tokens -- is one of the key components in the development of\nlanguage models. Particularly, auto-regressive language models generate texts\ntoken by token, i.e., by predicting the next-token distribution given the\nprevious ones, and thus tokenization directly affects their efficiency in text\ngeneration. Since each language model has their own vocabulary as a set of\npossible tokens, they struggle to cooperate with each other at the level of\nnext-token distributions such as model ensemble. In this paper, we establish a\ntheoretical framework of lossless vocabulary reduction, which efficiently\nconverts a given auto-regressive language model into the one with an\narbitrarily small vocabulary without any loss in accuracy. As an application,\nwe demonstrate that language models with different tokenization can cooperate\nwith each other efficiently through their maximal common vocabulary.", "AI": {"tldr": "The paper develops a theoretical framework for lossless vocabulary reduction in auto-regressive language models, enabling model cooperation despite different tokenizations.", "motivation": "Current auto-regressive language models struggle to collaborate effectively due to differing vocabularies, which hampers advances like model ensemble.", "method": "It proposes a framework for losslessly converting language models to use smaller, common vocabularies without sacrificing accuracy.", "result": "The framework allows models with different tokenizations to efficiently cooperate by using their maximal common vocabulary.", "conclusion": "Lossless vocabulary reduction enables better collaboration between language models, improving versatility without compromising performance."}}
{"id": "2510.07723", "pdf": "https://arxiv.org/pdf/2510.07723", "abs": "https://arxiv.org/abs/2510.07723", "authors": ["Wenyue Chen", "Peng Li", "Wangguandong Zheng", "Chengfeng Zhao", "Mengfei Li", "Yaolong Zhu", "Zhiyang Dou", "Ronggang Wang", "Yuan Liu"], "title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction", "categories": ["cs.CV"], "comment": "NIPS 2025", "summary": "Photorealistic 3D full-body human reconstruction from a single image is a\ncritical yet challenging task for applications in films and video games due to\ninherent ambiguities and severe self-occlusions. While recent approaches\nleverage SMPL estimation and SMPL-conditioned image generative models to\nhallucinate novel views, they suffer from inaccurate 3D priors estimated from\nSMPL meshes and have difficulty in handling difficult human poses and\nreconstructing fine details. In this paper, we propose SyncHuman, a novel\nframework that combines 2D multiview generative model and 3D native generative\nmodel for the first time, enabling high-quality clothed human mesh\nreconstruction from single-view images even under challenging human poses.\nMultiview generative model excels at capturing fine 2D details but struggles\nwith structural consistency, whereas 3D native generative model generates\ncoarse yet structurally consistent 3D shapes. By integrating the complementary\nstrengths of these two approaches, we develop a more effective generation\nframework. Specifically, we first jointly fine-tune the multiview generative\nmodel and the 3D native generative model with proposed pixel-aligned 2D-3D\nsynchronization attention to produce geometrically aligned 3D shapes and 2D\nmultiview images. To further improve details, we introduce a feature injection\nmechanism that lifts fine details from 2D multiview images onto the aligned 3D\nshapes, enabling accurate and high-fidelity reconstruction. Extensive\nexperiments demonstrate that SyncHuman achieves robust and photo-realistic 3D\nhuman reconstruction, even for images with challenging poses. Our method\noutperforms baseline methods in geometric accuracy and visual fidelity,\ndemonstrating a promising direction for future 3D generation models.", "AI": {"tldr": "The paper introduces SyncHuman, a novel framework for photorealistic 3D human reconstruction from a single image by combining 2D multiview and 3D native generative models, overcoming challenges like ambiguity and occlusion.", "motivation": "The authors aim to address limitations in existing methods for 3D human reconstruction, such as inaccurate 3D priors and difficulties in handling challenging poses and fine details.", "method": "SyncHuman integrates 2D multiview and 3D native generative models using a synchronization attention mechanism, and then refines details using a feature injection mechanism.", "result": "The framework achieves robust, photo-realistic 3D human reconstructions, especially for complex poses, and outperforms baseline methods in both geometric accuracy and visual quality.", "conclusion": "SyncHuman presents a promising approach for high-fidelity 3D reconstruction, combining the complementary strengths of 2D and 3D generative models for superior results."}}
{"id": "2510.07557", "pdf": "https://arxiv.org/pdf/2510.07557", "abs": "https://arxiv.org/abs/2510.07557", "authors": ["Abhay Bhandarkar", "Gaurav Mishra", "Khushi Juchani", "Harsh Singhal"], "title": "Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "This study applies BERTopic, a transformer-based topic modeling technique, to\nthe lmsys-chat-1m dataset, a multilingual conversational corpus built from\nhead-to-head evaluations of large language models (LLMs). Each user prompt is\npaired with two anonymized LLM responses and a human preference label, used to\nassess user evaluation of competing model outputs. The main objective is\nuncovering thematic patterns in these conversations and examining their\nrelation to user preferences, particularly if certain LLMs are consistently\npreferred within specific topics. A robust preprocessing pipeline was designed\nfor multilingual variation, balancing dialogue turns, and cleaning noisy or\nredacted data. BERTopic extracted over 29 coherent topics including artificial\nintelligence, programming, ethics, and cloud infrastructure. We analysed\nrelationships between topics and model preferences to identify trends in\nmodel-topic alignment. Visualization techniques included inter-topic distance\nmaps, topic probability distributions, and model-versus-topic matrices. Our\nfindings inform domain-specific fine-tuning and optimization strategies for\nimproving real-world LLM performance and user satisfaction.", "AI": {"tldr": "The study uses BERTopic to model topics in a multilingual dataset containing LLM responses, aiming to link themes to user preferences.", "motivation": "To understand thematic patterns in multilingual conversations and evaluate user preferences across different topics regarding LLM performance.", "method": "BERTopic was applied after robust preprocessing to extract coherent themes, followed by analysis through visualizations and matrices linking topics to user preferences.", "result": "29 coherent topics were identified, such as AI and ethics, with clear trends in how specific topics align with user preferences for various LLMs.", "conclusion": "Findings highlight how LLM performance varies by topic, offering insights for targeted fine-tuning and optimization to enhance user satisfaction."}}
{"id": "2510.07662", "pdf": "https://arxiv.org/pdf/2510.07662", "abs": "https://arxiv.org/abs/2510.07662", "authors": ["Virginia K. Felkner", "Allison Lim", "Jonathan May"], "title": "Textual Entailment and Token Probability as Bias Evaluation Metrics", "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.2"], "comment": "16 pages, 9 figures, under ARR review", "summary": "Measurement of social bias in language models is typically by token\nprobability (TP) metrics, which are broadly applicable but have been criticized\nfor their distance from real-world langugage model use cases and harms. In this\nwork, we test natural language inference (NLI) as a more realistic alternative\nbias metric. We show that, curiously, NLI and TP bias evaluation behave\nsubstantially differently, with very low correlation among different NLI\nmetrics and between NLI and TP metrics. We find that NLI metrics are more\nlikely to detect \"underdebiased\" cases. However, NLI metrics seem to be more\nbrittle and sensitive to wording of counterstereotypical sentences than TP\napproaches. We conclude that neither token probability nor natural language\ninference is a \"better\" bias metric in all cases, and we recommend a\ncombination of TP, NLI, and downstream bias evaluations to ensure comprehensive\nevaluation of language models.\n  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.", "AI": {"tldr": "The paper investigates comparing TP and NLI metrics for bias detection in language models, finding unique strengths and weaknesses in each.", "motivation": "Existing TP bias metrics have limitations in real-world applications, necessitating exploration of NLI as a realistic alternative.", "method": "Evaluation of bias through NLI metrics and comparison to existing TP metrics, analyzing correlation and sensitivity to linguistic variations.", "result": "NLI metrics behave differently from TP metrics, detecting underdebias more effectively but showing brittleness to sentence wording.", "conclusion": "Neither TP nor NLI is universally superior for bias evaluation; combining multiple methods is recommended for comprehensive analysis."}}
{"id": "2510.07748", "pdf": "https://arxiv.org/pdf/2510.07748", "abs": "https://arxiv.org/abs/2510.07748", "authors": ["Yilun Zhang", "Dexing Kong"], "title": "Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise in medicine but are prone to\nfactual and logical errors, which is unacceptable in this high-stakes field. To\naddress this, we introduce the \"Haibu Mathematical-Medical Intelligent Agent\"\n(MMIA), an LLM-driven architecture that ensures reliability through a formally\nverifiable reasoning process. MMIA recursively breaks down complex medical\ntasks into atomic, evidence-based steps. This entire reasoning chain is then\nautomatically audited for logical coherence and evidence traceability, similar\nto theorem proving. A key innovation is MMIA's \"bootstrapping\" mode, which\nstores validated reasoning chains as \"theorems.\" Subsequent tasks can then be\nefficiently solved using Retrieval-Augmented Generation (RAG), shifting from\ncostly first-principles reasoning to a low-cost verification model. We\nvalidated MMIA across four healthcare administration domains, including DRG/DIP\naudits and medical insurance adjudication, using expert-validated benchmarks.\nResults showed MMIA achieved an error detection rate exceeding 98% with a false\npositive rate below 1%, significantly outperforming baseline LLMs. Furthermore,\nthe RAG matching mode is projected to reduce average processing costs by\napproximately 85% as the knowledge base matures. In conclusion, MMIA's\nverifiable reasoning framework is a significant step toward creating\ntrustworthy, transparent, and cost-effective AI systems, making LLM technology\nviable for critical applications in medicine.", "AI": {"tldr": "This paper introduces MMIA, an architecture for applying LLMs reliably in high-stakes medical tasks, using formal reasoning and evidence verification.", "motivation": "LLMs are prone to errors, rendering them unsuitable for high-stakes medical applications where reliability is crucial.", "method": "MMIA recursively breaks complex tasks into atomic steps, audits reasoning chains for coherence and evidence traceability, and uses validated reasoning chains for efficient processing.", "result": "MMIA achieved over 98% error detection accuracy with under 1% false positive rate, while reducing processing costs by 85%.", "conclusion": "MMIA offers a verified reasoning framework that enhances LLM reliability and cost-efficiency, paving the way for trustworthy AI systems in medicine."}}
{"id": "2510.08173", "pdf": "https://arxiv.org/pdf/2510.08173", "abs": "https://arxiv.org/abs/2510.08173", "authors": ["Haolin Yang", "Yuxing Long", "Zhuoyuan Yu", "Zihan Yang", "Minghan Wang", "Jiapeng Xu", "Yihan Wang", "Ziyan Yu", "Wenzhe Cai", "Lei Kang", "Hao Dong"], "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Instruction-following navigation is a key step toward embodied intelligence.\nPrior benchmarks mainly focus on semantic understanding but overlook\nsystematically evaluating navigation agents' spatial perception and reasoning\ncapabilities. In this work, we introduce the NavSpace benchmark, which contains\nsix task categories and 1,228 trajectory-instruction pairs designed to probe\nthe spatial intelligence of navigation agents. On this benchmark, we\ncomprehensively evaluate 22 navigation agents, including state-of-the-art\nnavigation models and multimodal large language models. The evaluation results\nlift the veil on spatial intelligence in embodied navigation. Furthermore, we\npropose SNav, a new spatially intelligent navigation model. SNav outperforms\nexisting navigation agents on NavSpace and real robot tests, establishing a\nstrong baseline for future work.", "AI": {"tldr": "The paper introduces NavSpace, a benchmark designed to evaluate spatial intelligence in navigation agents, and proposes SNav, a high-performing navigation model.", "motivation": "Address the gap in systematically assessing the spatial perception and reasoning abilities of navigation agents in instruction-following tasks.", "method": "Created the NavSpace benchmark with 1,228 trajectory-instruction pairs across six task categories to evaluate navigation agents comprehensively, including state-of-the-art models and large language models.", "result": "NavSpace revealed insights into spatial intelligence in navigation agents, and the proposed SNav model outperformed existing agents on the benchmark and real-world tests.", "conclusion": "NavSpace advances the evaluation of spatial abilities in navigation systems, and SNav sets a new performance benchmark for future research."}}
{"id": "2510.08117", "pdf": "https://arxiv.org/pdf/2510.08117", "abs": "https://arxiv.org/abs/2510.08117", "authors": ["Fr\u00e9d\u00e9ric Zheng", "Yassir Jedra", "Alexandre Proutiere"], "title": "Near-optimal Rank Adaptive Inference of High Dimensional Matrices", "categories": ["cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "We address the problem of estimating a high-dimensional matrix from linear\nmeasurements, with a focus on designing optimal rank-adaptive algorithms. These\nalgorithms infer the matrix by estimating its singular values and the\ncorresponding singular vectors up to an effective rank, adaptively determined\nbased on the data. We establish instance-specific lower bounds for the sample\ncomplexity of such algorithms, uncovering fundamental trade-offs in selecting\nthe effective rank: balancing the precision of estimating a subset of singular\nvalues against the approximation cost incurred for the remaining ones. Our\nanalysis identifies how the optimal effective rank depends on the matrix being\nestimated, the sample size, and the noise level. We propose an algorithm that\ncombines a Least-Squares estimator with a universal singular value thresholding\nprocedure. We provide finite-sample error bounds for this algorithm and\ndemonstrate that its performance nearly matches the derived fundamental limits.\nOur results rely on an enhanced analysis of matrix denoising methods based on\nsingular value thresholding. We validate our findings with applications to\nmultivariate regression and linear dynamical system identification.", "AI": {"tldr": "The paper proposes rank-adaptive algorithms to estimate high-dimensional matrices from linear measurements, optimizing sample efficiency by adaptively selecting effective matrix ranks through singular value analysis.", "motivation": "To address the challenge of efficiently estimating high-dimensional matrices, where an optimal balance must be struck between precision in singular value estimation and approximation costs, particularly in noisy scenarios and limited samples.", "method": "Introduces an adaptive algorithm combining a Least-Squares estimator with singular value thresholding, supported by enhanced theoretical analysis and finite-sample error bounds.", "result": "The proposed algorithm achieves performance close to theoretical lower limits of sample complexity and demonstrates effectiveness for multivariate regression and linear dynamic system identification tasks.", "conclusion": "The study provides robust algorithms and insights into effective rank selection for matrix estimation, offering optimal trade-offs between sample efficiency, rank adaptivity, and noise resilience."}}
{"id": "2510.07729", "pdf": "https://arxiv.org/pdf/2510.07729", "abs": "https://arxiv.org/abs/2510.07729", "authors": ["Jian Gao", "Mengqi Yuan", "Yifei Zeng", "Chang Zeng", "Zhihao Li", "Zhenyu Chen", "Weichao Qiu", "Xiao-Xiao Long", "Hao Zhu", "Xun Cao", "Yao Yao"], "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) enables immersive rendering, but realistic 3D\nobject-scene composition remains challenging. Baked appearance and shadow\ninformation in GS radiance fields cause inconsistencies when combining objects\nand scenes. Addressing this requires relightable object reconstruction and\nscene lighting estimation. For relightable object reconstruction, existing\nGaussian-based inverse rendering methods often rely on ray tracing, leading to\nlow efficiency. We introduce Surface Octahedral Probes (SOPs), which store\nlighting and occlusion information and allow efficient 3D querying via\ninterpolation, avoiding expensive ray tracing. SOPs provide at least a 2x\nspeedup in reconstruction and enable real-time shadow computation in Gaussian\nscenes. For lighting estimation, existing Gaussian-based inverse rendering\nmethods struggle to model intricate light transport and often fail in complex\nscenes, while learning-based methods predict lighting from a single image and\nare viewpoint-sensitive. We observe that 3D object-scene composition primarily\nconcerns the object's appearance and nearby shadows. Thus, we simplify the\nchallenging task of full scene lighting estimation by focusing on the\nenvironment lighting at the object's placement. Specifically, we capture a 360\ndegrees reconstructed radiance field of the scene at the location and fine-tune\na diffusion model to complete the lighting. Building on these advances, we\npropose ComGS, a novel 3D object-scene composition framework. Our method\nachieves high-quality, real-time rendering at around 28 FPS, produces visually\nharmonious results with vivid shadows, and requires only 36 seconds for\nediting. Code and dataset are available at\nhttps://nju-3dv.github.io/projects/ComGS/.", "AI": {"tldr": "The paper proposes a novel framework, ComGS, for improving 3D object-scene composition, achieving high-quality, real-time rendering with vivid shadows and efficient editing.", "motivation": "The authors identified challenges in combining objects and scenes in Gaussian Splatting due to baked appearance and shadow inconsistencies.", "method": "The method involves using Surface Octahedral Probes (SOPs) for efficient lighting and shadow computation, along with environment lighting estimation using radiance field reconstruction and a fine-tuned diffusion model.", "result": "ComGS achieves real-time rendering at 28 FPS, harmonious visual composition, and fast editing within 36 seconds, enabling immersive 3D object-scene integration.", "conclusion": "The research demonstrates that ComGS overcomes inefficiencies and visual inconsistencies in Gaussian-based inverse rendering, providing a practical and effective solution for realistic 3D compositions."}}
{"id": "2510.07562", "pdf": "https://arxiv.org/pdf/2510.07562", "abs": "https://arxiv.org/abs/2510.07562", "authors": ["Yixiao Li", "Julia Barth", "Thomas Kiefer", "Ahmad Fraij"], "title": "EBGAN-MDN: An Energy-Based Adversarial Framework for Multi-Modal Behavior Cloning", "categories": ["cs.LG"], "comment": null, "summary": "Multi-modal behavior cloning faces significant challenges due to mode\naveraging and mode collapse, where traditional models fail to capture diverse\ninput-output mappings. This problem is critical in applications like robotics,\nwhere modeling multiple valid actions ensures both performance and safety. We\npropose EBGAN-MDN, a framework that integrates energy-based models, Mixture\nDensity Networks (MDNs), and adversarial training. By leveraging a modified\nInfoNCE loss and an energy-enforced MDN loss, EBGAN-MDN effectively addresses\nthese challenges. Experiments on synthetic and robotic benchmarks demonstrate\nsuperior performance, establishing EBGAN-MDN as a effective and efficient\nsolution for multi-modal learning tasks.", "AI": {"tldr": "The paper introduces the EBGAN-MDN framework to solve multi-modal behavior cloning challenges like mode averaging and collapse, achieving superior results in synthetic and robotic tasks.", "motivation": "Traditional models struggle with capturing diverse input-output mappings in multi-modal behavior cloning, which is critical for robotics due to its need for safety and performance through valid action modeling.", "method": "The proposed method combines energy-based models, Mixture Density Networks (MDNs), and adversarial training using a modified InfoNCE loss and energy-enforced MDN loss.", "result": "Experimental evaluation on synthetic and robotic benchmarks highlights the superior performance of EBGAN-MDN in multi-modal learning.", "conclusion": "EBGAN-MDN is an effective and efficient solution to challenges in multi-modal behavior cloning, addressing key issues like mode averaging and collapse."}}
{"id": "2510.07686", "pdf": "https://arxiv.org/pdf/2510.07686", "abs": "https://arxiv.org/abs/2510.07686", "authors": ["Jifan Zhang", "Henry Sleight", "Andi Peng", "John Schulman", "Esin Durmus"], "title": "Stress-Testing Model Specs Reveals Character Differences among Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly trained from AI constitutions\nand model specifications that establish behavioral guidelines and ethical\nprinciples. However, these specifications face critical challenges, including\ninternal conflicts between principles and insufficient coverage of nuanced\nscenarios. We present a systematic methodology for stress-testing model\ncharacter specifications, automatically identifying numerous cases of principle\ncontradictions and interpretive ambiguities in current model specs.\n  We stress test current model specs by generating scenarios that force\nexplicit tradeoffs between competing value-based principles. Using a\ncomprehensive taxonomy we generate diverse value tradeoff scenarios where\nmodels must choose between pairs of legitimate principles that cannot be\nsimultaneously satisfied. We evaluate responses from twelve frontier LLMs\nacross major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral\ndisagreement through value classification scores. Among these scenarios, we\nidentify over 70,000 cases exhibiting significant behavioral divergence.\nEmpirically, we show this high divergence in model behavior strongly predicts\nunderlying problems in model specifications. Through qualitative analysis, we\nprovide numerous example issues in current model specs such as direct\ncontradiction and interpretive ambiguities of several principles. Additionally,\nour generated dataset also reveals both clear misalignment cases and\nfalse-positive refusals across all of the frontier models we study. Lastly, we\nalso provide value prioritization patterns and differences of these models.", "AI": {"tldr": "Evaluates 12 frontier LLMs for principle contradictions using stress-testing methodology, uncovering 70,000 cases of behavioral conflicts and ambiguities.", "motivation": "Addressing internal principle conflicts and interpretive ambiguities in LLM behavior outlined in specifications.", "method": "Systematically stress-testing model specifications by generating scenarios with explicit value-based tradeoffs and measuring behavioral divergence.", "result": "Identified over 70,000 behavioral divergence cases and numerous specification issues in multiple frontier LLMs, as well as their value prioritization patterns.", "conclusion": "Behavioral divergence predicts specification issues, revealing contradictions and ambiguities that highlight the need for improved LLM guidelines."}}
{"id": "2510.07762", "pdf": "https://arxiv.org/pdf/2510.07762", "abs": "https://arxiv.org/abs/2510.07762", "authors": ["Xiangwei Lv", "JinLuan Yang", "Wang Lin", "Jingyuan Chen", "Beishui Liao"], "title": "From Noisy to Native: LLM-driven Graph Restoration for Test-Time Graph Domain Adaptation", "categories": ["cs.AI"], "comment": null, "summary": "Graph domain adaptation (GDA) has achieved great attention due to its\neffectiveness in addressing the domain shift between train and test data. A\nsignificant bottleneck in existing graph domain adaptation methods is their\nreliance on source-domain data, which is often unavailable due to privacy or\nsecurity concerns. This limitation has driven the development of Test-Time\nGraph Domain Adaptation (TT-GDA), which aims to transfer knowledge without\naccessing the source examples. Inspired by the generative power of large\nlanguage models (LLMs), we introduce a novel framework that reframes TT-GDA as\na generative graph restoration problem, \"restoring the target graph to its\npristine, source-domain-like state\". There are two key challenges: (1) We need\nto construct a reasonable graph restoration process and design an effective\nencoding scheme that an LLM can understand, bridging the modality gap. (2) We\nneed to devise a mechanism to ensure the restored graph acquires the intrinsic\nfeatures of the source domain, even without access to the source data. To\nensure the effectiveness of graph restoration, we propose GRAIL, that restores\nthe target graph into a state that is well-aligned with the source domain.\nSpecifically, we first compress the node representations into compact latent\nfeatures and then use a graph diffusion process to model the graph restoration\nprocess. Then a quantization module encodes the restored features into discrete\ntokens. Building on this, an LLM is fine-tuned as a generative restorer to\ntransform a \"noisy\" target graph into a \"native\" one. To further improve\nrestoration quality, we introduce a reinforcement learning process guided by\nspecialized alignment and confidence rewards. Extensive experiments demonstrate\nthe effectiveness of our approach across various datasets.", "AI": {"tldr": "The paper introduces GRAIL, a novel framework for test-time graph domain adaptation (TT-GDA), treating it as a generative graph restoration problem using large language models (LLMs).", "motivation": "Existing GDA methods rely on source-domain data, which is often inaccessible due to privacy or security concerns, necessitating alternative methods like TT-GDA.", "method": "The GRAIL framework compresses node representations, uses graph diffusion for restoration, encodes features, and fine-tunes an LLM as a generative restorer. Reinforcement learning further refines the restoration quality with alignment and confidence rewards.", "result": "Experiments on multiple datasets show the effectiveness of GRAIL in restoring target graphs to a source-domain-like state, demonstrating its efficiency.", "conclusion": "GRAIL successfully reframes test-time graph domain adaptation as a generative graph restoration task and provides a practical workaround for source data unavailability."}}
{"id": "2510.08270", "pdf": "https://arxiv.org/pdf/2510.08270", "abs": "https://arxiv.org/abs/2510.08270", "authors": ["Damir Nurtdinov", "Aliaksei Korshuk", "Alexei Kornaev", "Alexander Maloletov"], "title": "Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots", "categories": ["cs.RO"], "comment": null, "summary": "This study evaluates the performance of classical and modern control methods\nfor real-world Cable-Driven Parallel Robots (CDPRs), focusing on\nunderconstrained systems with limited time discretization. A comparative\nanalysis is conducted between classical PID controllers and modern\nreinforcement learning algorithms, including Deep Deterministic Policy Gradient\n(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy\nOptimization (TRPO). The results demonstrate that TRPO outperforms other\nmethods, achieving the lowest root mean square (RMS) errors across various\ntrajectories and exhibiting robustness to larger time intervals between control\nupdates. TRPO's ability to balance exploration and exploitation enables stable\ncontrol in noisy, real-world environments, reducing reliance on high-frequency\nsensor feedback and computational demands. These findings highlight TRPO's\npotential as a robust solution for complex robotic control tasks, with\nimplications for dynamic environments and future applications in sensor fusion\nor hybrid control strategies.", "AI": {"tldr": "This paper compares classical PID controllers and modern reinforcement learning methods for controlling Cable-Driven Parallel Robots, finding TRPO to be most effective.", "motivation": "The study aims to improve control techniques for Cable-Driven Parallel Robots (CDPRs), particularly in underconstrained and real-world environments with challenges like limited time discretization.", "method": "A comparative analysis between classical PID control and reinforcement learning methods, such as DDPG, PPO, and TRPO, is carried out to evaluate performance in terms of error rates and control robustness.", "result": "TRPO achieves the best performance with the lowest RMS errors, robustness to larger time intervals, and reliable control in noisy, real-world conditions.", "conclusion": "TRPO proves to be a robust, computationally efficient method, offering potential for broader applications in dynamic and complex control environments."}}
{"id": "2510.08294", "pdf": "https://arxiv.org/pdf/2510.08294", "abs": "https://arxiv.org/abs/2510.08294", "authors": ["Fabio De Sousa Ribeiro", "Ainkaran Santhirasekaram", "Ben Glocker"], "title": "Counterfactual Identifiability via Dynamic Optimal Transport", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at NeurIPS 2025", "summary": "We address the open question of counterfactual identification for\nhigh-dimensional multivariate outcomes from observational data. Pearl (2000)\nargues that counterfactuals must be identifiable (i.e., recoverable from the\nobserved data distribution) to justify causal claims. A recent line of work on\ncounterfactual inference shows promising results but lacks identification,\nundermining the causal validity of its estimates. To address this, we establish\na foundation for multivariate counterfactual identification using\ncontinuous-time flows, including non-Markovian settings under standard\ncriteria. We characterise the conditions under which flow matching yields a\nunique, monotone and rank-preserving counterfactual transport map with tools\nfrom dynamic optimal transport, ensuring consistent inference. Building on\nthis, we validate the theory in controlled scenarios with counterfactual\nground-truth and demonstrate improvements in axiomatic counterfactual soundness\non real images.", "AI": {"tldr": "The paper tackles counterfactual identification for high-dimensional outcomes using continuous-time flows, achieving consistent and valid causal inference.", "motivation": "The challenge lies in establishing causal validity of estimates for high-dimensional outcomes, addressing the gaps in identification of counterfactual inference methods.", "method": "Continuous-time flow models and dynamic optimal transport are used to ensure monotone and rank-preserving counterfactual mappings.", "result": "Theory is validated through controlled experiments with counterfactual ground truth, showing improvements in counterfactual soundness on real datasets.", "conclusion": "The approach provides a rigorous framework for multivariate counterfactual identification while ensuring causal validity and better real-world applicability."}}
{"id": "2510.07741", "pdf": "https://arxiv.org/pdf/2510.07741", "abs": "https://arxiv.org/abs/2510.07741", "authors": ["Yuang Meng", "Xin Jin", "Lina Lei", "Chun-Le Guo", "Chongyi Li"], "title": "UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ultra-high dynamic range (UHDR) scenes exhibit significant exposure\ndisparities between bright and dark regions. Such conditions are commonly\nencountered in nighttime scenes with light sources. Even with standard exposure\nsettings, a bimodal intensity distribution with boundary peaks often emerges,\nmaking it difficult to preserve both highlight and shadow details\nsimultaneously. RGB-based bracketing methods can capture details at both ends\nusing short-long exposure pairs, but are susceptible to misalignment and\nghosting artifacts. We found that a short-exposure image already retains\nsufficient highlight detail. The main challenge of UHDR reconstruction lies in\ndenoising and recovering information in dark regions. In comparison to the RGB\nimages, RAW images, thanks to their higher bit depth and more predictable noise\ncharacteristics, offer greater potential for addressing this challenge. This\nraises a key question: can we learn to see everything in UHDR scenes using only\na single short-exposure RAW image? In this study, we rely solely on a single\nshort-exposure frame, which inherently avoids ghosting and motion blur, making\nit particularly robust in dynamic scenes. To achieve that, we introduce\nUltraLED, a two-stage framework that performs exposure correction via a ratio\nmap to balance dynamic range, followed by a brightness-aware RAW denoiser to\nenhance detail recovery in dark regions. To support this setting, we design a\n9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a\ncorresponding dataset based on diverse scenes, using only the shortest exposure\nas input for reconstruction. Extensive experiments show that UltraLED\nsignificantly outperforms existing single-frame approaches. Our code and\ndataset are made publicly available at\nhttps://srameo.github.io/projects/ultraled.", "AI": {"tldr": "The paper presents UltraLED, a method to reconstruct UHDR scenes from a single short-exposure RAW image, avoiding ghosting issues and outperforming existing approaches.", "motivation": "To address the challenges of capturing ultra-high dynamic range (UHDR) scenes, particularly the difficulty in preserving highlight and shadow details in dynamic environments.", "method": "UltraLED is a two-stage framework; it applies exposure correction via a ratio map to manage dynamic range and uses a brightness-aware RAW denoiser to recover details in dark areas. This approach relies solely on a single short-exposure RAW image for robustness in dynamic scenes.", "result": "Extensive experiments demonstrate that UltraLED outperforms current single-frame UHDR reconstruction methods, effectively preserving details across dynamic ranges.", "conclusion": "UltraLED provides a robust and superior method for UHDR scene reconstruction using a single short-exposure RAW frame, avoiding issues like ghosting, and makes its code and dataset publicly available for further research."}}
{"id": "2510.07569", "pdf": "https://arxiv.org/pdf/2510.07569", "abs": "https://arxiv.org/abs/2510.07569", "authors": ["Prabhant Singh", "Pieter Gijsbers", "Elif Ceren Gok Yildirim", "Murat Onur Yildirim", "Joaquin Vanschoren"], "title": "Automated Machine Learning for Unsupervised Tabular Tasks", "categories": ["cs.LG"], "comment": "Accepted at Machine Learning Journal, 2025", "summary": "In this work, we present LOTUS (Learning to Learn with Optimal Transport for\nUnsupervised Scenarios), a simple yet effective method to perform model\nselection for multiple unsupervised machine learning(ML) tasks such as outlier\ndetection and clustering. Our intuition behind this work is that a machine\nlearning pipeline will perform well in a new dataset if it previously worked\nwell on datasets with a similar underlying data distribution. We use Optimal\nTransport distances to find this similarity between unlabeled tabular datasets\nand recommend machine learning pipelines with one unified single method on two\ndownstream unsupervised tasks: outlier detection and clustering. We present the\neffectiveness of our approach with experiments against strong baselines and\nshow that LOTUS is a very promising first step toward model selection for\nmultiple unsupervised ML tasks.", "AI": {"tldr": "LOTUS leverages Optimal Transport distances to perform model selection for unsupervised tasks like clustering and outlier detection. It identifies pipelines based on data distribution similarity.", "motivation": "The paper aims to address the challenge of selecting suitable machine learning pipelines for unsupervised tasks across diverse datasets by leveraging data distribution similarities.", "method": "LOTUS employs Optimal Transport distances to measure the similarity between tabular datasets and uses this information for recommending suitable machine learning pipelines for tasks such as clustering and outlier detection.", "result": "LOTUS demonstrated its capability to recommend effective pipelines through experiments that outperform strong baselines in model selection.", "conclusion": "LOTUS offers a promising unified approach for model selection in unsupervised machine learning applications, particularly outlier detection and clustering."}}
{"id": "2510.07706", "pdf": "https://arxiv.org/pdf/2510.07706", "abs": "https://arxiv.org/abs/2510.07706", "authors": ["Krinos Li", "Xianglu Xiao", "Shenglong Deng", "Lucas He", "Zijun Zhong", "Yuanjie Zou", "Zhonghao Zhan", "Zheng Hui", "Weiye Bao", "Guang Yang"], "title": "Large Language Models Meet Virtual Cell: A Survey", "categories": ["cs.CL", "cs.CE", "cs.LG", "q-bio.CB"], "comment": null, "summary": "Large language models (LLMs) are transforming cellular biology by enabling\nthe development of \"virtual cells\"--computational systems that represent,\npredict, and reason about cellular states and behaviors. This work provides a\ncomprehensive review of LLMs for virtual cell modeling. We propose a unified\ntaxonomy that organizes existing methods into two paradigms: LLMs as Oracles,\nfor direct cellular modeling, and LLMs as Agents, for orchestrating complex\nscientific tasks. We identify three core tasks--cellular representation,\nperturbation prediction, and gene regulation inference--and review their\nassociated models, datasets, evaluation benchmarks, as well as the critical\nchallenges in scalability, generalizability, and interpretability.", "AI": {"tldr": "This paper reviews how large language models (LLMs) are used in virtual cell modeling, organizing them into Oracle and Agent paradigms, with a focus on three key tasks: cellular representation, perturbation prediction, and gene regulation inference.", "motivation": "The motivation is to establish a comprehensive understanding of how LLMs can be utilized to create 'virtual cells' for modeling and predicting cellular states and behaviors.", "method": "The work categorizes the use of LLMs into two paradigms: \"LLMs as Oracles\" for direct cellular modeling and \"LLMs as Agents\" for managing scientific tasks. It also reviews models, datasets, benchmarks, and challenges related to three core tasks.", "result": "The paper identifies three main tasks addressed by LLMs in this domain: cellular representation, perturbation prediction, and gene regulation inference, along with highlighting critical challenges in scalability, generalizability, and interpretability.", "conclusion": "The study emphasizes the transformative potential of LLMs in cellular biology while calling attention to unresolved challenges like scalability, generalizability, and interpretability."}}
{"id": "2510.07772", "pdf": "https://arxiv.org/pdf/2510.07772", "abs": "https://arxiv.org/abs/2510.07772", "authors": ["Tianle Zhou", "Jiakai Xu", "Guanhong Liu", "Jiaxiang Liu", "Haonan Wang", "Eugene Wu"], "title": "An approach for systematic decomposition of complex llm tasks", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) suffer from reliability issues on complex tasks,\nas existing decomposition methods are heuristic and rely on agent or manual\ndecomposition. This work introduces a novel, systematic decomposition framework\nthat we call Analysis of CONstraint-Induced Complexity (ACONIC), which models\nthe task as a constraint problem and leveraging formal complexity measures to\nguide decomposition. On combinatorial (SATBench) and LLM database querying\ntasks (Spider), we find that by decomposing the tasks following the measure of\ncomplexity, agent can perform considerably better (10-40 percentage point).", "AI": {"tldr": "This paper introduces a systematic decomposition framework, ACONIC, to improve the performance of Large Language Models (LLMs) on complex tasks by leveraging formal complexity measures.", "motivation": "Reliability issues in LLMs arise from heuristic and manual decomposition methods, necessitating a systematic approach to task decomposition.", "method": "The authors propose modeling tasks as constraint problems and utilizing formal complexity metrics to guide their decomposition within a framework called ACONIC.", "result": "Using ACONIC, LLMs demonstrated significantly improved performance (10-40 percentage point increase) on combinatorial (SATBench) and database querying tasks (Spider).", "conclusion": "ACONIC provides an effective systematic approach to task decomposition, enhancing LLM reliability on complex tasks by utilizing formal measures of complexity."}}
{"id": "2510.08381", "pdf": "https://arxiv.org/pdf/2510.08381", "abs": "https://arxiv.org/abs/2510.08381", "authors": ["Baoyang Chen", "Xian Xu", "Huamin Qu"], "title": "Airy: Reading Robot Intent through Height and Sky", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "As industrial robots move into shared human spaces, their opaque decision\nmaking threatens safety, trust, and public oversight. This artwork, Airy, asks\nwhether complex multi agent AI can become intuitively understandable by staging\na competition between two reinforcement trained robot arms that snap a bedsheet\nskyward. Building on three design principles, competition as a clear metric\n(who lifts higher), embodied familiarity (audiences recognize fabric snapping),\nand sensor to sense mapping (robot cooperation or rivalry shown through forest\nand weather projections), the installation gives viewers a visceral way to read\nmachine intent. Observations from five international exhibitions indicate that\naudiences consistently read the robots' strategies, conflict, and cooperation\nin real time, with emotional reactions that mirror the system's internal state.\nThe project shows how sensory metaphors can turn a black box into a public\ninterface.", "AI": {"tldr": "The paper discusses \"Airy,\" an installation where two reinforcement-trained robot arms snap a bedsheet to investigate if complex AI can be made intuitively understandable.", "motivation": "To address the lack of safety, trust, and public oversight in AI systems as robots increasingly move into shared human spaces.", "method": "Airy stages a bedsheet-snapping competition between two robots using principles like clear competition metrics, embodied familiarity, and visualized cooperation or conflict through weather projections.", "result": "Observations from exhibitions show that audiences could interpret the robots' strategies, conflicts, and cooperation in real-time with corresponding emotional reactions.", "conclusion": "The project demonstrates that sensory metaphors can effectively transform opaque AI systems into understandable and relatable public interfaces."}}
{"id": "2510.08382", "pdf": "https://arxiv.org/pdf/2510.08382", "abs": "https://arxiv.org/abs/2510.08382", "authors": ["Jacob Trauger", "Tyson Trauger", "Ambuj Tewari"], "title": "Characterizing the Multiclass Learnability of Forgiving 0-1 Loss Functions", "categories": ["cs.LG", "stat.ML"], "comment": "9 pages", "summary": "In this paper we will give a characterization of the learnability of\nforgiving 0-1 loss functions in the finite label multiclass setting. To do\nthis, we create a new combinatorial dimension that is based off of the\nNatarajan Dimension \\citep{natarajan1989learning} and we show that a hypothesis\nclass is learnable in our setting if and only if this Generalized Natarajan\nDimension is finite. We also show a connection to learning with set-valued\nfeedback. Through our results we show that the learnability of a set learning\nproblem is characterized by the Natarajan Dimension.", "AI": {"tldr": "The paper characterizes learnability for forgiving 0-1 loss functions in the finite label multiclass setting using a generalized Natarajan Dimension, connecting it to set-valued feedback learning.", "motivation": "To understand learnability of forgiving 0-1 loss functions under multiclass classification, and establish theoretical foundations using a combinatorial framework.", "method": "The authors introduce a Generalized Natarajan Dimension based on the original Natarajan Dimension and use it to analyze learnability, providing proofs and linking findings to set-valued feedback.", "result": "The Generalized Natarajan Dimension is proven to be the key determinative factor for learning forgiving 0-1 loss functions. A connection with set-valued feedback learning is established.", "conclusion": "Finite Generalized Natarajan Dimension characterizes the learnability of forgiving 0-1 loss functions, emphasizing the importance of combinatorial dimensions in understanding learning problems."}}
{"id": "2510.07752", "pdf": "https://arxiv.org/pdf/2510.07752", "abs": "https://arxiv.org/abs/2510.07752", "authors": ["Junhao He", "Jiaxu Wang", "Jia Li", "Mingyuan Sun", "Qiang Zhang", "Jiahang Cao", "Ziyi Zhang", "Yi Gu", "Jingkai Sun", "Renjing Xu"], "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream", "categories": ["cs.CV"], "comment": "Accepted by TVCG", "summary": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB\nvideos is challenging. This is because large inter-frame motions will increase\nthe uncertainty of the solution space. For example, one pixel in the first\nframe might have more choices to reach the corresponding pixel in the second\nframe. Event cameras can asynchronously capture rapid visual changes and are\nrobust to motion blur, but they do not provide color information. Intuitively,\nthe event stream can provide deterministic constraints for the inter-frame\nlarge motion by the event trajectories. Hence, combining\nlow-temporal-resolution images with high-framerate event streams can address\nthis challenge. However, it is challenging to jointly optimize Dynamic 3DGS\nusing both RGB and event modalities due to the significant discrepancy between\nthese two data modalities. This paper introduces a novel framework that jointly\noptimizes dynamic 3DGS from the two modalities. The key idea is to adopt event\nmotion priors to guide the optimization of the deformation fields. First, we\nextract the motion priors encoded in event streams by using the proposed LoCM\nunsupervised fine-tuning framework to adapt an event flow estimator to a\ncertain unseen scene. Then, we present the geometry-aware data association\nmethod to build the event-Gaussian motion correspondence, which is the primary\nfoundation of the pipeline, accompanied by two useful strategies, namely motion\ndecomposition and inter-frame pseudo-label. Extensive experiments show that our\nmethod outperforms existing image and event-based approaches across synthetic\nand real scenes and prove that our method can effectively optimize dynamic 3DGS\nwith the help of event data.", "AI": {"tldr": "This paper proposes a novel framework for reconstructing dynamic 3D Gaussian Splatting (3DGS) using low-framerate RGB videos combined with high-framerate event streams, addressing the challenges of large inter-frame motions and modal discrepancies.", "motivation": "The paper aims to tackle the difficulty of reconstructing dynamic 3DGS from low-framerate RGB videos, where large inter-frame motions can increase solution uncertainty. Event cameras, which provide robust motion data, are leveraged to reduce this uncertainty.", "method": "The proposed framework combines RGB images and event data, using motion priors derived from event streams to guide the optimization of deformation fields. Key components include the LoCM unsupervised fine-tuning framework for extracting motion priors, and a geometry-aware data association method to establish motion correspondences.", "result": "Experiments on synthetic and real scenes demonstrate that the method outperforms existing approaches based on either image or event data, showing effective optimization of dynamic 3DGS using the modal combination.", "conclusion": "Integrating RGB videos with event camera data allows for improved reconstruction of dynamic 3DGS, overcoming large motion challenges and modal discrepancies through the proposed framework."}}
{"id": "2510.07570", "pdf": "https://arxiv.org/pdf/2510.07570", "abs": "https://arxiv.org/abs/2510.07570", "authors": ["Ryan T. Tymkow", "Benjamin D. Schnapp", "Mojtaba Valipour", "Ali Ghodshi"], "title": "Symbolic-Diffusion: Deep Learning Based Symbolic Regression with D3PM Discrete Token Diffusion", "categories": ["cs.LG", "I.2.1"], "comment": "9 Pages, 3 Figurees", "summary": "Symbolic regression refers to the task of finding a closed-form mathematical\nexpression to fit a set of data points. Genetic programming based techniques\nare the most common algorithms used to tackle this problem, but recently,\nneural-network based approaches have gained popularity. Most of the leading\nneural-network based models used for symbolic regression utilize\ntransformer-based autoregressive models to generate an equation conditioned on\nencoded input points. However, autoregressive generation is limited to\ngenerating tokens left-to-right, and future generated tokens are conditioned\nonly on previously generated tokens. Motivated by the desire to generate all\ntokens simultaneously to produce improved closed-form equations, we propose\nSymbolic Diffusion, a D3PM based discrete state-space diffusion model which\nsimultaneously generates all tokens of the equation at once using discrete\ntoken diffusion. Using the bivariate dataset developed for SymbolicGPT, we\ncompared our diffusion-based generation approach to an autoregressive model\nbased on SymbolicGPT, using equivalent encoder and transformer architectures.\nWe demonstrate that our novel approach of using diffusion-based generation for\nsymbolic regression can offer comparable and, by some metrics, improved\nperformance over autoregressive generation in models using similar underlying\narchitectures, opening new research opportunities in neural-network based\nsymbolic regression.", "AI": {"tldr": "The paper introduces Symbolic Diffusion, a diffusion-based model for symbolic regression that simultaneously generates all tokens of an equation, showing comparable or improved performance compared to traditional autoregressive methods.", "motivation": "The authors aim to address limitations of autoregressive generation\u2014like sequential token dependency\u2014by proposing a simultaneous token generation approach for symbolic regression.", "method": "They developed Symbolic Diffusion, a discrete state-space diffusion model, and compared its performance against SymbolicGPT. Both models used equivalent transformer architectures.", "result": "The diffusion-based method demonstrated comparable or, according to certain metrics, superior performance compared to the autoregressive SymbolicGPT model.", "conclusion": "Symbolic Diffusion shows promise for advancing neural-network-based symbolic regression and opens pathways for further research in this domain."}}
{"id": "2510.07707", "pdf": "https://arxiv.org/pdf/2510.07707", "abs": "https://arxiv.org/abs/2510.07707", "authors": ["Chengshuai Zhao", "Shu Wan", "Paras Sheth", "Karan Patwa", "K. Sel\u00e7uk Candan", "Huan Liu"], "title": "Causality Guided Representation Learning for Cross-Style Hate Speech Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The proliferation of online hate speech poses a significant threat to the\nharmony of the web. While explicit hate is easily recognized through overt\nslurs, implicit hate speech is often conveyed through sarcasm, irony,\nstereotypes, or coded language -- making it harder to detect. Existing hate\nspeech detection models, which predominantly rely on surface-level linguistic\ncues, fail to generalize effectively across diverse stylistic variations.\nMoreover, hate speech spread on different platforms often targets distinct\ngroups and adopts unique styles, potentially inducing spurious correlations\nbetween them and labels, further challenging current detection approaches.\nMotivated by these observations, we hypothesize that the generation of hate\nspeech can be modeled as a causal graph involving key factors: contextual\nenvironment, creator motivation, target, and style. Guided by this graph, we\npropose CADET, a causal representation learning framework that disentangles\nhate speech into interpretable latent factors and then controls confounders,\nthereby isolating genuine hate intent from superficial linguistic cues.\nFurthermore, CADET allows counterfactual reasoning by intervening on style\nwithin the latent space, naturally guiding the model to robustly identify hate\nspeech in varying forms. CADET demonstrates superior performance in\ncomprehensive experiments, highlighting the potential of causal priors in\nadvancing generalizable hate speech detection.", "AI": {"tldr": "CADET introduces a causal representation learning framework for hate speech detection by disentangling contextual factors and enabling counterfactual reasoning to enhance generalization across diverse styles.", "motivation": "Hate speech detection is hindered by implicit forms such as sarcasm and stereotypes, as well as platform-specific variations, which lead to spurious correlations and poor generalization.", "method": "CADET utilizes a causal graph to model factors like context, creator intent, target, and style, enabling latent representation disentanglement and counterfactual interventions to identify true hate intent.", "result": "CADET outperforms existing methods by effectively isolating genuine hate speech intent and providing robust performance across stylistic variations and platforms.", "conclusion": "CADET demonstrates the efficacy of leveraging causal priors to improve the robustness and generalizability of hate speech detection models."}}
{"id": "2510.07790", "pdf": "https://arxiv.org/pdf/2510.07790", "abs": "https://arxiv.org/abs/2510.07790", "authors": ["Hao Wu", "Wei Liu"], "title": "GCPO: When Contrast Fails, Go Gold", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement learning has been widely applied to enhance the reasoning\ncapabilities of large language models. Extending the inference limits of\nsmaller models has become a prominent research focus. However, algorithms such\nas Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the\nupper bound of a model's rollout responses is entirely determined by the model\nitself, preventing the acquisition of knowledge from samples that are either\nall incorrect or all correct. In this paper, we introduce Group Contrastive\nPolicy Optimization (GCPO), a method that incorporates external standard\nreference answers. When the model cannot solve a problem, the reference answer\nsupplies the correct response, steering the model toward an unequivocally\naccurate update direction. This approach offers two main advantages: (1) it\nimproves training efficiency by fully utilizing every sample; (2) it enables\nthe model to emulate the problem solving strategy of the reference answer\nduring training, thereby enhancing generalization in reasoning. GCPO achieves\noutstanding results across multiple benchmark datasets, yielding substantial\nimprovements over the baseline model. Our code is available at:\nhttps://github.com/AchoWu/GCPO.", "AI": {"tldr": "The paper introduces Group Contrastive Policy Optimization (GCPO), a method that boosts language model reasoning by incorporating external reference answers, achieving superior results compared to baselines.", "motivation": "The paper addresses the limitations of current reinforcement learning methods like GRPO, which struggle to utilize samples where model-generated answers are entirely correct or incorrect.", "method": "Group Contrastive Policy Optimization (GCPO) incorporates external reference answers to guide the model during training, ensuring accurate update directions regardless of the model's initial response accuracy.", "result": "GCPO demonstrates significant improvements in reasoning benchmarks, outperforming baseline models and improving training efficiency.", "conclusion": "GCPO enhances generalization in reasoning by enabling models to emulate the strategy of external reference answers, making it an effective approach for advancing smaller language models' capabilities."}}
{"id": "2510.08406", "pdf": "https://arxiv.org/pdf/2510.08406", "abs": "https://arxiv.org/abs/2510.08406", "authors": ["Filip Be\u010danovi\u0107", "Kosta Jovanovi\u0107", "Vincent Bonnet"], "title": "Reliability of Single-Level Equality-Constrained Inverse Optimal Control", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC", "I.2.9; G.1.6; I.2.6"], "comment": "8 pages, 3 figures", "summary": "Inverse optimal control (IOC) allows the retrieval of optimal cost function\nweights, or behavioral parameters, from human motion. The literature on IOC\nuses methods that are either based on a slow bilevel process or a fast but\nnoise-sensitive minimization of optimality condition violation. Assuming\nequality-constrained optimal control models of human motion, this article\npresents a faster but robust approach to solving IOC using a single-level\nreformulation of the bilevel method and yields equivalent results. Through\nnumerical experiments in simulation, we analyze the robustness to noise of the\nproposed single-level reformulation to the bilevel IOC formulation with a\nhuman-like planar reaching task that is used across recent studies. The\napproach shows resilience to very large levels of noise and reduces the\ncomputation time of the IOC on this task by a factor of 15 when compared to a\nclassical bilevel implementation.", "AI": {"tldr": "The paper proposes a faster and robust single-level method for solving Inverse Optimal Control (IOC), significantly reducing computation time compared to conventional bilevel approaches.", "motivation": "The study aims to address inefficiencies and noise sensitivity in existing IOC methods used for human motion analysis.", "method": "The authors introduce a single-level reformulation to replace the slow bilevel process, ensuring equivalent results while improving computational speed and noise resilience.", "result": "Numerical experiments demonstrate the proposed method's 15-fold reduction in computation time and its ability to handle very large noise levels in a simulated human-like planar reaching task.", "conclusion": "The single-level formulation is validated as a faster, noise-resilient alternative to classical bilevel IOC methods for modeling human motion."}}
{"id": "2510.08429", "pdf": "https://arxiv.org/pdf/2510.08429", "abs": "https://arxiv.org/abs/2510.08429", "authors": ["Stella C. Dong", "James R. Finlay"], "title": "ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing", "categories": ["cs.LG", "cs.AI", "stat.ML", "68T05, 91G70", "I.2.6; I.2.7; J.1"], "comment": "Accepted for publication at the 6th ACM International Conference on\n  AI in Finance (ICAIF 2025), Singapore. Author-accepted version (October\n  2025). 10 pages, 5 figures", "summary": "Reinsurance treaty pricing must satisfy stringent regulatory standards, yet\ncurrent quoting practices remain opaque and difficult to audit. We introduce\nClauseLens, a clause-grounded reinforcement learning framework that produces\ntransparent, regulation-compliant, and risk-aware treaty quotes.\n  ClauseLens models the quoting task as a Risk-Aware Constrained Markov\nDecision Process (RA-CMDP). Statutory and policy clauses are retrieved from\nlegal and underwriting corpora, embedded into the agent's observations, and\nused both to constrain feasible actions and to generate clause-grounded natural\nlanguage justifications.\n  Evaluated in a multi-agent treaty simulator calibrated to industry data,\nClauseLens reduces solvency violations by 51%, improves tail-risk performance\nby 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded\nexplanations with retrieval precision of 87.4% and recall of 91.1%.\n  These findings demonstrate that embedding legal context into both decision\nand explanation pathways yields interpretable, auditable, and\nregulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and\nthe EU AI Act.", "AI": {"tldr": "ClauseLens framework enhances reinsurance treaty pricing by combining legal clauses with Risk-Aware Constrained Markov Decision Process (RA-CMDP), yielding more regulation-compliant, transparent, and risk-driven quotes.", "motivation": "Current practices in reinsurance treaty pricing are opaque and difficult to audit, necessitating a transparent and regulation-compliant approach that integrates legal context into pricing.", "method": "ClauseLens employs a RA-CMDP approach, embedding legal and underwriting clauses into the agent\u2019s observations to constrain actions and generate clause-grounded explanations. It is evaluated using a multi-agent simulator calibrated to industry data.", "result": "ClauseLens results in 51% reduced solvency violations, 27.9% improved tail-risk performance (CVaR_0.10), 88.2% accuracy in clause-grounded explanations, and high retrieval precision (87.4%) and recall (91.1%).", "conclusion": "Embedding legal context in decision and explanation processes enhances interpretability, auditability, and compliance of reinsurance treaty pricing, aligning with global regulatory standards such as Solvency II and the EU AI Act."}}
{"id": "2510.07785", "pdf": "https://arxiv.org/pdf/2510.07785", "abs": "https://arxiv.org/abs/2510.07785", "authors": ["Ming Jie Ong", "Sze Yinn Ung", "Sim Kuan Goh", "Jimmy Y. Zhong"], "title": "Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis", "categories": ["cs.CV"], "comment": null, "summary": "The current study investigated the use of Explainable Artificial Intelligence\n(XAI) to improve the accuracy of brain tumor segmentation in MRI images, with\nthe goal of assisting physicians in clinical decision-making. The study focused\non applying UNet models for brain tumor segmentation and using the XAI\ntechniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and\nattention-based visualization to enhance the understanding of these models.\nThree deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet\n(AttUNet) - were evaluated to identify the best-performing model. XAI was\nemployed with the aims of clarifying model decisions and increasing physicians'\ntrust in these models. We compared the performance of two UNet variants\n(ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors\nfrom the BraTS2020 public dataset and analyzed model predictions with Grad-CAM\nand attention-based visualization. Using the latest computer hardware, we\ntrained and validated each model using the Adam optimizer and assessed their\nperformance with respect to: (i) training, validation, and inference times,\n(ii) segmentation similarity coefficients and loss functions, and (iii)\nclassification performance. Notably, during the final testing phase, ResUNet\noutperformed the other models with respect to Dice and Jaccard similarity\nscores, as well as accuracy, recall, and F1 scores. Grad-CAM provided\nvisuospatial insights into the tumor subregions each UNet model focused on\nwhile attention-based visualization provided valuable insights into the working\nmechanisms of AttUNet's attention modules. These results demonstrated ResUNet\nas the best-performing model and we conclude by recommending its use for\nautomated brain tumor segmentation in future clinical assessments. Our source\ncode and checkpoint are available at\nhttps://github.com/ethanong98/MultiModel-XAI-Brats2020", "AI": {"tldr": "This study explored the use of Explainable Artificial Intelligence (XAI) with UNet models for brain tumor segmentation in MRI images, evaluating ResUNet as the best model.", "motivation": "The study aims to improve the accuracy and trust in AI-driven brain tumor segmentation, aiding physicians in clinical decisions.", "method": "Three UNet models (UNet, ResUNet, AttUNet) were tested using Gradient-weighted Class Activation Mapping (Grad-CAM) and attention-based visualization techniques. Performance metrics included Dice similarity, Jaccard similarity, accuracy, recall, and F1 scores.", "result": "ResUNet outperformed other models in segmentation metrics, and XAI techniques clarified model decisions, offering visuospatial insights and attention module explanations.", "conclusion": "ResUNet is recommended for automated brain tumor segmentation due to its superior performance and explainability features, with implications for future clinical use."}}
{"id": "2510.07578", "pdf": "https://arxiv.org/pdf/2510.07578", "abs": "https://arxiv.org/abs/2510.07578", "authors": ["Shilong Zong", "Alex Bierly", "Almuatazbellah Boker", "Hoda Eldardiry"], "title": "Accuracy, Memory Efficiency and Generalization: A Comparative Study on Liquid Neural Networks and Recurrent Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2.6; I.2.8"], "comment": "13 pages, 12 figures. Submitted to IEEE Transactions on Neural\n  Networks and Learning Systems (TNNLS)", "summary": "This review aims to conduct a comparative analysis of liquid neural networks\n(LNNs) and traditional recurrent neural networks (RNNs) and their variants,\nsuch as long short-term memory networks (LSTMs) and gated recurrent units\n(GRUs). The core dimensions of the analysis include model accuracy, memory\nefficiency, and generalization ability. By systematically reviewing existing\nresearch, this paper explores the basic principles, mathematical models, key\ncharacteristics, and inherent challenges of these neural network architectures\nin processing sequential data. Research findings reveal that LNN, as an\nemerging, biologically inspired, continuous-time dynamic neural network,\ndemonstrates significant potential in handling noisy, non-stationary data, and\nachieving out-of-distribution (OOD) generalization. Additionally, some LNN\nvariants outperform traditional RNN in terms of parameter efficiency and\ncomputational speed. However, RNN remains a cornerstone in sequence modeling\ndue to its mature ecosystem and successful applications across various tasks.\nThis review identifies the commonalities and differences between LNNs and RNNs,\nsummarizes their respective shortcomings and challenges, and points out\nvaluable directions for future research, particularly emphasizing the\nimportance of improving the scalability of LNNs to promote their application in\nbroader and more complex scenarios.", "AI": {"tldr": "This comparative review analyzes liquid neural networks (LNNs) and traditional recurrent neural networks (RNNs), focusing on accuracy, memory efficiency, and generalization ability, and identifies areas for future research.", "motivation": "The paper seeks to understand the strengths and weaknesses of LNNs compared to RNNs in handling sequential data, addressing shortcomings, and exploring their potential for broader applications.", "method": "The authors performed a systematic review of existing research to analyze principles, mathematical models, characteristics, and challenges of LNNs and RNNs.", "result": "LNNs display strong performance in noisy and non-stationary data handling and out-of-distribution generalization, with some variants excelling in parameter efficiency and computational speed compared to RNNs.", "conclusion": "While LNNs show promise for advanced applications, RNNs continue to be foundational in sequence modeling due to their robust ecosystem. Future improvements in LNN scalability are necessary to enhance their adoption in complex scenarios."}}
{"id": "2510.07713", "pdf": "https://arxiv.org/pdf/2510.07713", "abs": "https://arxiv.org/abs/2510.07713", "authors": ["Shuo Yu", "Mingyue Cheng", "Daoyu Wang", "Qi Liu", "Zirui Liu", "Ze Guo", "Xiaoyu Tao"], "title": "MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation", "categories": ["cs.CL"], "comment": "12 pages, 8 figures", "summary": "The primary form of user-internet engagement is shifting from leveraging\nimplicit feedback signals, such as browsing and clicks, to harnessing the rich\nexplicit feedback provided by textual interactive behaviors. This shift unlocks\na rich source of user textual history, presenting a profound opportunity for a\ndeeper form of personalization. However, prevailing approaches offer only a\nshallow form of personalization, as they treat user history as a flat list of\ntexts for retrieval and fail to model the rich temporal and semantic structures\nreflecting dynamic nature of user interests. In this work, we propose\n\\textbf{MemWeaver}, a framework that weaves the user's entire textual history\ninto a hierarchical memory to power deeply personalized generation. The core\ninnovation of our memory lies in its ability to capture both the temporal\nevolution of interests and the semantic relationships between different\nactivities. To achieve this, MemWeaver builds two complementary memory\ncomponents that both integrate temporal and semantic information, but at\ndifferent levels of abstraction: behavioral memory, which captures specific\nuser actions, and cognitive memory, which represents long-term preferences.\nThis dual-component memory serves as a unified representation of the user,\nallowing large language models (LLMs) to reason over both concrete behaviors\nand abstracted traits. Experiments on the Language Model Personalization (LaMP)\nbenchmark validate the efficacy of MemWeaver. Our code is\navailable\\footnote{https://github.com/fishsure/MemWeaver}.", "AI": {"tldr": "MemWeaver introduces a hierarchical memory framework for personalized generation by leveraging rich temporal and semantic structures within user's textual interaction history.", "motivation": "The paper identifies a shift in user-internet engagement from implicit feedback signals to explicit textual interactive behaviors, necessitating deeper personalization frameworks to better capture dynamic user interests.", "method": "MemWeaver constructs a dual-component memory\u2014behavioral memory for user actions and cognitive memory for long-term preferences\u2014to represent user history. This unified memory is integrated with large language models to enhance personalization.", "result": "Experimental validation on the Language Model Personalization (LaMP) benchmark showcases MemWeaver's effectiveness in improving personalized text generation.", "conclusion": "MemWeaver successfully models temporal evolution and semantic relationships in user behavior, advancing personalization capabilities of large language models."}}
{"id": "2510.07813", "pdf": "https://arxiv.org/pdf/2510.07813", "abs": "https://arxiv.org/abs/2510.07813", "authors": ["Valerio La Gatta", "Dolev Mutzari", "Sarit Kraus", "VS Subrahmanian"], "title": "Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games", "categories": ["cs.AI"], "comment": "15 pages, 13 figures", "summary": "Adversarial environments require agents to navigate a key strategic\ntrade-off: acquiring information enhances situational awareness, but may\nsimultaneously expose them to threats. To investigate this tension, we\nformulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer\nagent must decide when to communicate in order to obtain the evader's position.\nEach communication reveals the pursuer's location, increasing the risk of being\ntargeted. Both agents learn their movement policies via reinforcement learning,\nwhile the pursuer additionally learns a communication policy that balances\nobservability and risk. We propose SHADOW (Strategic-communication Hybrid\nAction Decision-making under partial Observation for Warfare), a multi-headed\nsequential reinforcement learning framework that integrates continuous\nnavigation control, discrete communication actions, and opponent modeling for\nbehavior prediction. Empirical evaluations show that SHADOW pursuers achieve\nhigher success rates than six competitive baselines. Our ablation study\nconfirms that temporal sequence modeling and opponent modeling are critical for\neffective decision-making. Finally, our sensitivity analysis reveals that the\nlearned policies generalize well across varying communication risks and\nphysical asymmetries between agents.", "AI": {"tldr": "This paper introduces SHADOW, a reinforcement learning framework for agents in adversarial environments balancing communication benefits against exposure risks.", "motivation": "The paper addresses the challenge of effective decision-making for agents in adversarial environments where acquiring situational information comes at the cost of revealing one's own position.", "method": "The authors develop SHADOW, a multi-headed sequential reinforcement learning model that combines continuous navigation, discrete communication decisions, and opponent modeling.", "result": "SHADOW achieves better success rates than six baseline methods, with evaluations highlighting the importance of temporal and opponent modeling.", "conclusion": "SHADOW's policies show strong generalization across diverse conditions, confirming its efficiency in handling communication-risk trade-offs in adversarial settings."}}
{"id": "2510.08408", "pdf": "https://arxiv.org/pdf/2510.08408", "abs": "https://arxiv.org/abs/2510.08408", "authors": ["Bibekananda Patra", "Rajeevlochana G. Chittawadigi", "Sandipan Bandyopadhyay"], "title": "Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a method of validation of the size of the largest\ncollision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)\nfor a given orientation of its moving platform (MP) using the Application\nProgramming Interface (API) of a CAD software. The position of the MP is\nupdated via the API in an automated manner over a set of samples within a shell\nenclosing the surface of the CFS. For each pose of the manipulator, each pair\nof legs is investigated for mutual collisions. The CFS is considered safe or\nvalidated iff none of the points falling inside the CFS lead to a collision\nbetween any pair of legs. This approach can not only validate the safety of a\nprecomputed CFS, but also estimate the same for any spatial parallel\nmanipulator.", "AI": {"tldr": "This study validates the largest collision-free sphere (CFS) of a 6-6 Stewart-Gough manipulator using CAD API automation for given orientations.", "motivation": "To ensure the safe operation of spatial parallel manipulators by validating the largest collision-free area in their mechanical configuration.", "method": "Utilizes the CAD software's API to modify the manipulator's position across sampled points, checking every pose's leg pairs for collision within the CFS.", "result": "The method can automate the validation of precomputed CFS and estimate CFS configurations for any spatial parallel manipulator.", "conclusion": "The approach provides a reliable and efficient way to verify collision safety for manipulators, enhancing operational confidence."}}
{"id": "2510.08450", "pdf": "https://arxiv.org/pdf/2510.08450", "abs": "https://arxiv.org/abs/2510.08450", "authors": ["Hugh Blayney", "\u00c1lvaro Arroyo", "Xiaowen Dong", "Michael M. Bronstein"], "title": "gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "22 pages, 22 figures, 7 tables", "summary": "Graph Neural Networks (GNNs) leverage the graph structure to transmit\ninformation between nodes, typically through the message-passing mechanism.\nWhile these models have found a wide variety of applications, they are known to\nsuffer from over-squashing, where information from a large receptive field of\nnode representations is collapsed into a single fixed sized vector, resulting\nin an information bottleneck. In this paper, we re-examine the over-squashing\nphenomenon through the lens of model storage and retrieval capacity, which we\ndefine as the amount of information that can be stored in a node's\nrepresentation for later use. We study some of the limitations of existing\ntasks used to measure over-squashing and introduce a new synthetic task to\ndemonstrate that an information bottleneck can saturate this capacity.\nFurthermore, we adapt ideas from the sequence modeling literature on\nassociative memories, fast weight programmers, and the xLSTM model to develop a\nnovel GNN architecture with improved capacity. We demonstrate strong\nperformance of this architecture both on our capacity synthetic task, as well\nas a range of real-world graph benchmarks.", "AI": {"tldr": "Graph Neural Networks (GNNs) face the issue of over-squashing, where large receptive field information is compressed into fixed-size node vectors, creating bottlenecks. This paper introduces a novel GNN architecture addressing this issue, validated through synthetic and real-world tasks.", "motivation": "The paper aims to address the over-squashing problem in GNNs, where nodes fail to retain sufficient information due to limited capacity, hindering model performance.", "method": "The authors propose a new GNN architecture influenced by methods like associative memories and xLSTM models, improving storage and retrieval capacity for node representations.", "result": "The proposed architecture exhibits enhanced performance in synthetic tasks measuring storage capacity and real-world benchmarks.", "conclusion": "Enhancing node capacity in GNNs is crucial to mitigate over-squashing, and the introduced architecture successfully demonstrates its effectiveness in various scenarios."}}
{"id": "2510.07791", "pdf": "https://arxiv.org/pdf/2510.07791", "abs": "https://arxiv.org/abs/2510.07791", "authors": ["Qinghongbing Xie", "Zhaoyuan Xia", "Feng Zhu", "Lijun Gong", "Ziyue Li", "Rui Zhao", "Long Zeng"], "title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models", "categories": ["cs.CV"], "comment": "20 pages, 13 figures", "summary": "Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has\nattracted much attention due to its importance for Autonomous Driving, Embodied\nAI and General Artificial Intelligence. Existing spatial-temporal benchmarks\nmainly focus on egocentric perspective reasoning with images/video context, or\ngeographic perspective reasoning with graphics context (eg. a map), thus fail\nto assess VLMs' geographic spatial-temporal intelligence with both images/video\nand graphics context, which is important for areas like traffic management and\nemergency response. To address the gaps, we introduce Geo-Temporal Reasoning\nbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of\nmoving targets in a large-scale camera network. GTR-Bench is more challenging\nas it requires multiple perspective switches between maps and videos, joint\nreasoning across multiple videos with non-overlapping fields of view, and\ninference over spatial-temporal regions that are unobserved by any video\ncontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that\neven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags\nbehind human performance (78.61%) on geo-temporal reasoning. Moreover, our\ncomprehensive analysis on GTR-Bench reveals three primary deficiencies of\ncurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by\nan imbalanced utilization of spatial-temporal context. (2) VLMs are weak in\ntemporal forecasting, which leads to worse performance on temporal-emphasized\ntasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to\ncomprehend or align the map data with multi-view video inputs. We believe\nGTR-Bench offers valuable insights and opens up new opportunities for research\nand applications in spatial-temporal intelligence. Benchmark and code will be\nreleased at https://github.com/X-Luffy/GTR-Bench.", "AI": {"tldr": "The paper introduces Geo-Temporal Reasoning benchmark (GTR-Bench) to address deficiencies in Visual-Language Models (VLMs) for geographic spatial-temporal intelligence and evaluates current models, revealing significant gaps compared to human performance.", "motivation": "To develop a benchmark that assesses geographic spatial-temporal intelligence of VLMs using both images/videos and graphics contexts, targeting areas like traffic management and emergency response.", "method": "GTR-Bench challenges models with tasks requiring perspective switches between maps and videos, multi-view video reasoning, and inference over unobserved spatial-temporal regions. Popular VLMs were evaluated on this benchmark.", "result": "Models tested, such as Gemini-2.5-Pro, showed significantly lower performance (34.9%) compared to human benchmarks (78.61%). The analysis identified three major deficiencies in current VLMs: imbalanced spatial-temporal context utilization, weak temporal forecasting, and poor map-video alignment capability.", "conclusion": "GTR-Bench highlights critical areas for improvement in VLMs' geographic temporal reasoning approaches and offers directions for future research. Its release will support advancements in spatial-temporal intelligence applications."}}
{"id": "2510.07581", "pdf": "https://arxiv.org/pdf/2510.07581", "abs": "https://arxiv.org/abs/2510.07581", "authors": ["Zhongqi Yue", "Weishi Wang", "Yundaichuan Zhan", "Juncheng Li", "Daniel Dahlmeier", "Fredrik D. Johansson"], "title": "Expanding the Action Space of LLMs to Reason Beyond Language", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are powerful reasoners in natural language, but\ntheir actions are typically confined to outputting vocabulary tokens. As a\nresult, interactions with external environments -- such as symbolic operators\nor simulators -- must be expressed through text in predefined formats, parsed,\nand routed to external interfaces. This overloads the model's language with\nboth reasoning and control duties, and requires a hand-crafted parser, external\nto the LLM. To address this, we decouple environment interactions from language\nby internalizing them in an Expanded Action space (ExpA), beyond the\nvocabulary. The model starts reasoning in the default language environment, but\nmay trigger routing actions and switch to an external environment at any time.\nFrom there, the model can only invoke environment-specific actions, receive\nfeedback from the environment, and potentially route back to language as a\nresult. To promote effective exploration of the expanded action space and new\nenvironments, we introduce ExpA Reinforcement Learning (EARL) with\ncounterfactual policy optimization. On tasks requiring multi-turn interactions\nand contingent planning, EARL outperforms strong baselines with\nvocabulary-constrained actions. It performs robustly across calculator-based\nmulti-task learning and, in the partially observed sorting problem, achieves\nperfect Sort-4 accuracy while self-discovering an efficient algorithm\ncompetitive with classical designs.", "AI": {"tldr": "Large Language Models (LLMs) extend their capabilities by interacting with external environments via an expanded action space, avoiding vocabulary-only constraints.", "motivation": "LLMs face limitations in interacting with external environments when confined to language-based actions, necessitating a system to seamlessly integrate external operations.", "method": "The paper introduces Expanded Action space (ExpA) to enable LLMs to switch between language reasoning and external environment-specific actions, supported by ExpA Reinforcement Learning (EARL) for efficient exploration.", "result": "EARL achieves superior performance in multi-turn interaction and contingent planning tasks, with robust success in calculator-based tasks and perfect accuracy on complex sorting problems.", "conclusion": "Decoupling language reasoning and external interaction via ExpA enhances the capabilities of LLMs, opening avenues for efficient multitask execution and algorithmic discoveries."}}
