<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.AR](#cs.AR) [Total: 8]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 162]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.LG](#cs.LG) [Total: 162]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 45]
- [cs.SE](#cs.SE) [Total: 41]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 15]
- [cs.DS](#cs.DS) [Total: 2]
- [eess.AS](#eess.AS) [Total: 4]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 13]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 7]
- [math.OC](#math.OC) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [eess.IV](#eess.IV) [Total: 15]
- [cs.MM](#cs.MM) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.MA](#cs.MA) [Total: 6]
- [math.NA](#math.NA) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.DB](#cs.DB) [Total: 3]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.GT](#cs.GT) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.IR](#cs.IR) [Total: 6]
- [math.HO](#math.HO) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: The paper proposes removing redundant reasoning tokens in large language models by measuring token-level attention scores to improve long-form reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the redundancy in reasoning chains of large language models, which hampers performance, particularly on reasoning-intensive tasks.

Method: Systematically identify and prune redundant tokens using token-level attention scores to an end-of-thinking token. Apply structure-aware pruning to remove low-contributing reasoning chunks.

Result: Redundancy removal improves reasoning accuracy across benchmarks, especially in mathematical competition tasks like AIME and AMC.

Conclusion: Pruning low-relevant reasoning tokens enhances clear thinking and boosts the performance of large language models in complex reasoning tasks without requiring additional training.

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: The paper introduces a novel MCA approach combining Virtual Gap Analysis (VGA) models to enhance evaluation efficiency and fairness in decision-making problems.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing MCA methods influenced by assumptions and subjective judgments, and integrate qualitative and quantitative criteria effectively.

Method: A novel MCA approach is developed using two Virtual Gap Analysis (VGA) models, leveraging linear programming for improved comprehensive and dependable evaluations.

Result: Numerical examples validate the transparency and accuracy of the method, showcasing improved efficiency and fairness in multi-criteria assessments.

Conclusion: The proposed approach is effective for advancing automated and decision-support systems, paving the way for robust progress in evaluation methodologies.

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: The paper proposes a flexible framework for using generative AI in multi-actor environments by taking inspiration from tabletop role-playing games (TTRPGs), emphasizing an Entity-Component architecture for modularity and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of supporting diverse use cases of generative AI—such as simulation, narrative generation, and evaluation—in multi-actor environments by proposing a unified and adaptable framework.

Method: The method involves taking inspiration from TTRPGs by designing a Game Master (GM) as a configurable entity in an Entity-Component architectural pattern. This separates the engineer's role (handling implementation) and the designer's role (creating component configurations).

Result: The approach fosters modularity, scalability, and rapid iteration. It also demonstrates its effectiveness through the evolution of the Concordia library, which aligns configuration with specific user goals.

Conclusion: The paper concludes that the Entity-Component architectural pattern inspired by TTRPGs provides an effective framework for configuring generative AI-driven multi-actor scenarios, supporting diverse and scalable use cases.

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: This paper introduces BioAnalyst, a transformer-based AI Foundation Model tailored for biodiversity analysis and conservation, outperforming existing methods in ecological forecasting, especially in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: The preservation of biodiversity is crucial for ecological balance and sustainability, but it is threatened by issues like habitat loss, climate change, and invasive species. There is a need for advanced tools to monitor, predict, and plan biodiversity conservation efforts effectively.

Method: BioAnalyst leverages a transformer-based architecture pre-trained on multi-modal datasets (species occurrences, remote sensing, climate, and environmental data). It is adaptably fine-tuned for downstream tasks like species distribution modeling, habitat assessment, and biodiversity forecasting.

Result: The model was tested on two biodiversity use cases and surpassed existing methods, particularly in data-scarce environments, establishing new accuracy baselines in ecological forecasting.

Conclusion: BioAnalyst offers a scalable and efficient AI-driven solution for biodiversity conservation. Its open release aims to stimulate scientific collaboration and progress in addressing ecological challenges.

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: A randomized controlled trial found that allowing experienced open-source developers to use AI tools actually increased their task completion time by 19%.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to investigate the real-world impact of AI tools on software development productivity, specifically among experienced developers working on mature open-source projects.

Method: The study involved a randomized controlled trial where 16 experienced open-source developers performed 246 tasks, with tasks randomly assigned to either allow or disallow AI tools.

Result: The study found that, contrary to expectations, allowing AI tools increased task completion time by 19%. This effect contradicts both developer forecasts and expert predictions.

Conclusion: The observed slowdown suggests that current AI tools may not always lead to anticipated productivity gains, and highlights the need for further investigations into the complex interactions between AI tools and developer workflows.

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: This paper presents a Multi-Agent Reinforcement Learning (MARL) framework for detecting market manipulation in decentralized finance (DeFi) environments, using advanced machine learning and decentralized architectures.


<details>
  <summary>Details</summary>
Motivation: DeFi's rise has enabled innovative financial practices but also left the ecosystem vulnerable to market manipulation tactics like shilling campaigns and pump-and-dump schemes. This paper addresses the need for decentralized solutions to detect and counteract such manipulations.

Method: The paper introduces a MARL framework with key innovations: Group Relative Policy Optimization (GRPO) for learning stability, a theory-based reward function informed by financial theory, and a multi-modal agent pipeline using LLMs, social graphs, and market data. This framework is built into the Symphony decentralized architecture for peer-to-peer manipulation detection and evaluation.

Result: The proposed framework, tested on 100,000 real-world data points and adversarial simulations, achieves high accuracy in detecting market manipulation and identifying causal attributions. Symphony enables decentralized, real-time surveillance of DeFi ecosystems.

Conclusion: The proposed MARL framework bridges the gap between multi-agent systems and financial surveillance. It provides a decentralized, scalable, and effective solution for identifying and addressing market manipulation in DeFi, advancing research in decentralized market intelligence.

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: This paper performs a comprehensive security evaluation of LLM-based coding agents, revealing significant vulnerabilities and proposing mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Address the growing deployment of LLM coding agents in software development and understand their security implications.

Method: Analyzed 12,000 actions from state-of-the-art models on real-world tasks, developed a detection system, and tested mitigation strategies.

Result: Found 21% agent trajectories contained insecure actions; GPT-4.1 had high success in security mitigation.

Conclusion: Coding agents require security-aware design to reduce vulnerabilities and promote safer software development practices.

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: The paper discusses potential AI scenarios leading to human extinction and advocates preventive measures.


<details>
  <summary>Details</summary>
Motivation: To raise awareness and support preventive measures against the potential catastrophic risks posed by AI.

Method: Develop a taxonomy of potential omnicidal AI scenarios and provide illustrative examples.

Result: The taxonomy and examples reveal various hypothetical ways AI could pose catastrophic existential risks to humanity.

Conclusion: Publicizing possible omnicidal AI scenarios may garner public and institutional support to prevent such risks.

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow addresses scientific reasoning shortcomings in multimodal large language models by introducing end-to-end frameworks for educational reasoning, including data selection, MCTS-based reasoning, and self-correction mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models struggle with scientific tasks requiring multi-step reasoning and reliability, making them insufficient for structured educational contexts.

Method: EduFlow introduces frameworks like EduPRM for process-aware critique and iterative refinement and EduMCTS for search optimization leveraging self-reflection mechanisms and bootstrapping actions.

Result: Experimental results show that EduFlow significantly improves the consistency and coherence of reasoning in scientific tasks.

Conclusion: The EduFlow framework is effective in enhancing structured and interpretable reasoning in scientific contexts, with improved reliability and depth.

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [10] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: The paper evaluates how structure and complexity of knowledge impact large language models' performance in querying a triplestore within neurosymbolic AI systems.


<details>
  <summary>Details</summary>
Motivation: To investigate the integration of explainability, interpretability, and adaptability in neurosymbolic AI systems to optimize their performance.

Method: The research systematically evaluates the effects of different conceptualizations and representations of knowledge in an AI agent's querying process using a triplestore.

Result: The findings reveal that both the structure and complexity of knowledge representations influence the effectiveness of AI agents.

Conclusion: These insights could guide the design of future Agentic Retrieval-Augmented Generation systems with improved transferability and interpretability.

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [11] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: The paper introduces LLM-Stackelberg games using large language models (LLMs) to model decision-making interactions between a leader and a follower, emphasizing probabilistic reasoning, adaptive strategies, and asymmetric information.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend classical Stackelberg game theory by incorporating LLMs to capture bounded rationality and adapt to modern contexts like cybersecurity and misinformation.

Method: Agents interact using structured prompts and LLM-generated probabilistic behaviors, with equilibrium concepts established via cognitive reasoning and epistemic uncertainty modeling.

Result: The framework successfully modeled spearphishing deception games and demonstrated potential for broader applications in adversarial and decision-making domains.

Conclusion: LLM-Stackelberg games serve as an innovative tool to study modern strategic interactions that involve complex reasoning, asymmetric information, and LLM-mediated behaviors.

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [12] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: The paper proposes using generative AI to shift multi-agent reinforcement learning (RL) from reactive to proactive intelligence, addressing challenges like joint action spaces, non-stationary environments, and partial observability.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent RL methods are limited by reactive mechanisms, failing to perform effectively in complex, novel scenarios. There is a need to overcome coordination and adaptability barriers.

Method: The authors propose agents designed as generative models that predict environmental evolution, anticipate interactions, and enable strategic reasoning, leveraging generative AI's pattern recognition and decision-making abilities.

Result: Generative-RL agents are anticipated to achieve seamless coordination, enhanced communication, and dynamic adaptation to evolving scenarios.

Conclusion: This paradigm shift to proactive intelligence promises breakthroughs in distributed intelligence for autonomous systems, robotics, and human-AI collaboration, addressing coordination challenges beyond reactive approaches.

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [13] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: This paper proposes Consistency Trajectory Planning (CTP), an offline model-based reinforcement learning approach offering fast single-step trajectory optimization.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based planning methods demonstrate strong results but come with high computational costs due to iterative sampling procedures.

Method: CTP uses the Consistency Trajectory Model (CTM) to enable single-step trajectory generation, avoiding the need for iterative procedures.

Result: CTP outperforms diffusion-based planning methods on the D4RL benchmark in terms of returns and computation time, achieving over 120x speedup while maintaining high performance.

Conclusion: CTP is a practical and effective solution for high-performance, low-latency offline reinforcement learning planning.

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [14] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: This paper introduces an MH sampling-based framework to train Spiking Neural Networks for reinforcement learning tasks, focusing on energy-efficient solutions and demonstrating superior performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper targets the challenge of training Spiking Neural Networks, especially for reinforcement learning tasks, given the non-differentiable nature of their spike-based communication.

Method: The authors utilize Metropolis-Hastings sampling, a Bayesian inference technique, to propose and accept parameter updates probabilistically based on accumulated reward signals rather than gradient methods.

Result: The MH-based approach surpasses Deep Q-Learning and prior SNN RL methodologies in control benchmarks like AcroBot and CartPole, optimizing reward accumulation and minimizing network resources and required training episodes.

Conclusion: The framework provides a novel pathway to train SNNs for RL dynamically, proving both efficient and effective without reliance on gradient-based techniques.

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [15] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens is an AI-as-a-Service platform leveraging proprietary data, operational workflows, and agnostic LLMs to ensure in-house AI knowledge retention and data security. It offers AI Agents for workflow automation and data insights, showcasing high retrieval accuracy and improved factual alignment.


<details>
  <summary>Details</summary>
Motivation: Enable businesses to securely control AI operations in-house, leveraging insights and AI automation to optimize workflows in critical sectors like legal and finance.

Method: eSapiens integrates document ingestion, hybrid vector retrieval, and LangChain-based no-code orchestration, supporting multiple LLMs. Experiments focus on retrieval precision and generation quality using benchmarks.

Result: The system achieved 91.3% Top-3 retrieval accuracy with optimal token chunking and up to a 23% improvement in factual alignment across supported LLMs.

Conclusion: eSapiens provides auditable, trustworthy workflows optimized for domains requiring precision and security, demonstrating its viability in high-stakes industries.

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [16] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: The paper reviews AI's overlooked environmental and ethical issues, identifying concerns like energy use, e-waste, and inequality, while advocating for sustainable practices.


<details>
  <summary>Details</summary>
Motivation: To address systemic environmental and ethical challenges arising from AI's rapid expansion, such as energy consumption and global inequalities.

Method: Reviewing recent studies and institutional reports to identify systemic issues in areas like energy usage, e-waste, and cybersecurity.

Result: Identified systemic problems including training-related emissions, hardware turnover, and infrastructure inequality; connected these to Responsible AI principles.

Conclusion: AI's development must prioritize ethical responsibility and environmental sustainability for a fairer and greener technological future.

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [17] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: The paper proposes a neurosymbolic framework that combines multimodal language models with Knowledge Graphs and ontologies to enhance interoperability in robotic applications. It evaluates various models and integration strategies for generating ontology-compliant KGs.


<details>
  <summary>Details</summary>
Motivation: Current robot systems lack interoperability due to proprietary, hard-coded implementations. The paper aims to address this by leveraging the strengths of ontologies/Knowledge Graphs and multimodal language models for adaptable robotic behaviors.

Method: The authors integrate robot perception data, ontologies, and five multimodal models, employing different neural-symbolic interactions to generate ontology-compliant Knowledge Graphs. They assess model consistency and effectiveness through statistical analysis.

Result: GPT-o1 and LLaMA 4 Maverick outperform other models in generating ontology-compliant Knowledge Graphs consistently. Integration strategy plays a significant role, as newer models do not always yield better results.

Conclusion: The proposed integration of multimodal language models and structured representations demonstrates potential for platform-independent robotic applications, with results emphasizing the importance of model and strategy selection in neurosymbolic frameworks.

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [18] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: The paper introduces a PyTorch-based toolkit that uses stochastic control methods to ensure fairness and robustness in AI systems interacting with multiple agents.


<details>
  <summary>Details</summary>
Motivation: AI systems often interact with multiple agents, requiring guarantees for fairness and robustness in their operations.

Method: The authors developed a toolkit based on PyTorch that applies stochastic control techniques to model AI system interactions and repeated actions of agents.

Result: The toolkit allows for modeling robustness and fairness in an easy and reliable way, removing complexity in ensuring fairness guarantees in multi-agent settings.

Conclusion: This PyTorch-based toolkit simplifies the process of ensuring a priori fairness and robustness guarantees in multi-agent AI systems.

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [19] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 excel in complex reasoning but struggle with generating unnecessarily lengthy reasoning chains for simple queries, leading to inefficiency. The survey reviews recent progress to make reasoning concise and adaptive.


<details>
  <summary>Details</summary>
Motivation: LRMs have shown strong performance in tasks requiring deep reasoning; however, their inefficiencies in generating lengthy reasoning chains for simple tasks hinder their practical application due to wasted resources and increased response times.

Method: The survey reviews existing methodologies, benchmarks, and challenges related to concise and adaptive reasoning for improving the efficiency of LRMs.

Result: The paper systematically highlights advancements in concise and adaptive reasoning approaches, providing insights into research progress and identifying challenges.

Conclusion: Streamlining LRMs to switch between fast and slow thinking adaptively based on query difficulty is crucial for their practical application and efficient usage. Insights from this survey aim to inspire further innovations in adaptive reasoning.

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [20] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: This paper presents a causality-informed deep Q-network (Causal DQ) for optimizing sensor placement in anomaly detection, achieving quicker anomaly identification in large data streams.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of placing sensors optimally for anomaly detection in large-scale systems, considering causal relationships instead of relying solely on variable correlations.

Method: The method involves integrating causal information into the Q-network training process within a reinforcement learning framework.

Result: The proposed approach achieves faster convergence, tighter theoretical error bounds, and reduced anomaly detection time in real-world and large-scale data scenarios.

Conclusion: The causality-informed method advances anomaly detection approaches and has broader implications for reinforcement learning and causality-informed engineering applications.

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [21] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: The paper integrates large language models (LLMs) into the interpretation of paraconsistent logic to address logical inconsistencies in LLM-generated outputs.


<details>
  <summary>Details</summary>
Motivation: LLMs are powerful but lack logical consistency, creating a need for a method to integrate their knowledge into formal reasoning effectively.

Method: The authors propose embedding LLMs into the interpretation functions of paraconsistent logic's formal semantics and validate this via experiments on factual datasets.

Result: Demonstrated feasibility of combining LLM knowledge and logic soundness through dataset evaluations.

Conclusion: LLMs can be utilized for neuro-symbolic reasoning while maintaining logical soundness and completeness.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [22] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: This paper proposes technical interventions to manage risks posed by AI systems, enabling a coordinated halt in dangerous AI activities.


<details>
  <summary>Details</summary>
Motivation: The paper emphasizes the urgent need for mechanisms that prevent risks such as loss of control, misuse, and geopolitical instability caused by rapidly advancing AI systems.

Method: The authors outline technical interventions and evaluate their potential to restrict dangerous AI activities while serving as the basis for future AI governance strategies.

Result: The paper identifies how specific interventions play a role in restricting harmful AI development and deployment.

Conclusion: The proposed interventions can form a technical foundation to support broader governance measures aimed at reducing risks associated with advanced AI systems.

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [23] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: The study examines methods to induce long chain-of-thought (CoT) reasoning in base language models through minimal tuning and shows significant performance improvement using just 20 high-quality CoT examples.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether long reasoning chains can be induced in base models using minimal tuning or prompting, given previous success with model distillation and reinforcement learning approaches.

Method: The authors lightly fine-tuned the base model Qwen2.5-32B using only 20 long CoT examples from the reasoning model QwQ-32B-Preview. They also investigated using CoT data from non-reasoning models and human annotators, employing various enhancement techniques.

Result: The fine-tuned model outperformed a larger model, Qwen2.5-Math-72B-Instruct, proving that a small set of high-quality examples can significantly improve reasoning. However, data from non-reasoning models and human annotators could not match the performance of reasoning model traces.

Conclusion: Carefully curated, small-scale CoT supervision, particularly using examples from reasoning models, can activate reasoning behaviors in base models. However, expert CoT traces hold latent qualities that are challenging to replicate by other means, necessitating further research.

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [24] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: The paper reinterprets instruction-tuned large language models as model-grounded symbolic AI systems, exploring novel learning and reasoning approaches within this framework.


<details>
  <summary>Details</summary>
Motivation: To exploit the complementary strengths of neural networks and symbolic AI for enhancing learning efficiency and reasoning reliability, the authors propose reinterpreting large language models as symbolic systems grounded in their internal representation space.

Method: The authors utilize large language models as symbolic layers integrated with grounding in internal representation spaces. They explore learning and reasoning approaches inspired by traditional paradigms.

Result: Preliminary evaluations demonstrate improved learning efficiency and reasoning reliability in axiomatic deductive reasoning tasks of varying complexity.

Conclusion: This work provides promising insights into the potential of instruction-tuned large language models as hybrid systems, advancing neurosymbolic AI's ability to leverage structured reasoning while maintaining robust learning capabilities.

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [25] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: This study introduces VerifyBench, a benchmark designed for evaluating verifiers used alongside LLMs in cross-domain tasks, addressing the limitations of specialized and general verifiers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of evaluating the consistency of model-generated responses in reinforcement learning processes involving LLMs, especially given the complexity and diversity of responses.

Method: VerifyBench was designed with 4,000 expert-level cross-domain questions and employs a four-dimensional framework comparing verifier performance across various structured outputs using multidisciplinary expert annotations.

Result: Specialized verifiers exhibit high accuracy but low recall, while general verifiers perform inclusively but with unstable precision. Verifiers also showcase sensitivity to input structure and limitations in cross-domain generalization.

Conclusion: The findings highlight fundamental trade-offs in verifier designs and underline bottlenecks in their current technologies, providing directions for improving RLVR systems.

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [26] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek has introduced innovative, cost-effective, high-performing open-source AI models, challenging the AI landscape with novel algorithms and engineering advancements.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of building cost-effective and highly efficient large AI models while fostering innovation in the AI field.

Method: The paper reviews paradigm shifts in AI model development, presents DeepSeek's unique algorithms (e.g., MLA, MoE, etc.), and examines engineering enhancements in scaling, training, and optimization.

Result: DeepSeek's V3 and R1 models demonstrate competitive advantages over mainstream LLMs and firm their position as industry disruptors across varied AI applications.

Conclusion: DeepSeek's innovative methodologies and engineering breakthroughs not only reshape the AI landscape but also provide forward-looking insights into the evolution of large AI model development.

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [27] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: The paper introduces the Optimal Marginal Deterministic Policy Gradient (OMDPG) algorithm to resolve conflicts between monotonic improvement and Partial Parameter-sharing (ParPS) in heterogeneous multi-agent reinforcement learning (MARL).


<details>
  <summary>Details</summary>
Motivation: To address the challenges in achieving monotonic improvement and high cooperative performance in heterogeneous MARL while reconciling the limitations of Partial Parameter-sharing (ParPS).

Method: The paper develops the OMDPG algorithm, which replaces sequential Q-computations with Optimal Marginal Q (OMQ), uses a Generalized Q Critic (GQC) for stable Q-value estimation, and implements a Centralized Critic Grouped Actor (CCGA) architecture for both ParPS-based local policy networks and accurate Q-function computation.

Result: Experimental evaluations in SMAC and MAMuJoCo environments show that OMDPG surpasses state-of-the-art MARL baselines in performance.

Conclusion: OMDPG effectively addresses the issue of policy updating baseline drift while achieving enhanced cooperative performance through a novel algorithmic approach and architecture design.

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [28] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: The paper explores a pragmatic interpretation of latent intentionality in texts using Semantic Spacetime and Promise Theory, enabling content analysis with minimal computation.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to address the lack of practical attention given to the concept of intent and intentionality in science and technology, especially after the philosophical work of Searle.

Method: The method involves analyzing latent intentionality in data by identifying multi-scale anomalies and separating content into 'intended' content and 'ambient context' based on spacetime coherence, requiring minimal computation.

Result: A practical approach for conceptualizing latent intentionality was achieved with low computational cost, enabling basic agents to process text without extensive training.

Conclusion: This approach offers an accessible way to assess intentionality in data, even for simple agents, but the level of concept formation is constrained by memory capacity.

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [29] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: This paper addresses the reliability challenges of Chain of Thought (CoT) reasoning by introducing a method that uses intrinsic model attention activations to evaluate and improve reasoning accuracy through beam search.


<details>
  <summary>Details</summary>
Motivation: Chain of Thought (CoT) reasoning is powerful but prone to intermediate step errors, undermining its reliability.

Method: The approach leverages truthfulness-sensitive activations in the model's attention heads to train a confidence predictor and dynamically select reasoning paths via beam search.

Result: The proposed method surpasses existing state-of-the-art approaches across multiple reasoning tasks in both unimodal and multimodal models.

Conclusion: This work enhances CoT reasoning reliability and accuracy, demonstrating its broad application potential, particularly for large reasoning models.

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [30] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: This paper evaluates Large Language Models (LLMs) for SPARQL-to-SPARQL translations between Knowledge Graph schemas, focusing on DBpedia-Wikidata and DBLP-OpenAlex alignments.


<details>
  <summary>Details</summary>
Motivation: The research addresses a gap in Knowledge Graph (KG) interoperability by assessing the capabilities of LLMs in automating SPARQL query translation between popular KG schemas.

Method: The study uses two benchmarks (DBpedia-Wikidata, DBLP-OpenAlex) to test three open LLMs (Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, Mistral-Large-Instruct-2407) under zero-shot, few-shot, and chain-of-thought prompting. Results are compared with gold standards and error categories are analyzed.

Result: Performance across models and prompting strategies varies significantly. Translations from Wikidata to DBpedia perform better than the reverse (DBpedia to Wikidata).

Conclusion: LLMs show potential for SPARQL-to-SPARQL translations, but effectiveness largely depends on the model, the direction of translation, and the prompting strategy employed.

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [31] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: This paper introduces gradual semantics for assumption-based argumentation (ABA) to attribute dialectical strengths to assumptions in ABA frameworks.


<details>
  <summary>Details</summary>
Motivation: Assumption-based argumentation (ABA), a popular structured argumentation framework, lacks gradual semantics that could determine degrees of acceptability, a gap this paper aims to fill.

Method: The authors generalize modular gradual semantics for quantitative bipolar argumentation frameworks (QBAFs) and adapt it to ABA while ensuring it satisfies accepted properties. They also benchmark their approach against an existing QBAF-based method using synthetic data.

Result: The proposed gradual ABA semantics adhered to key properties such as balance and monotonicity, and performed competitively when compared with the benchmark argument-based approach.

Conclusion: The paper successfully extends gradual semantics to assumption-based argumentation, showcasing its viability and utility for analyzing assumptions' dialectical strengths.

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [32] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: The paper introduces BlueGlass, a framework for integrated AI safety analysis, demonstrated on vision-language models.


<details>
  <summary>Details</summary>
Motivation: The growing prevalence of AI systems necessitates integrated safety tools since existing tools are limited in providing comprehensive safety assurances independently.

Method: The authors introduce BlueGlass, a unified framework integrating diverse safety tools, and conduct three analyses on vision-language models: distributional evaluation, layer dynamics probing, and concept identification via sparse autoencoders.

Result: BlueGlass enables comprehensive safety analyses, revealing performance trade-offs across distributions, hierarchical learning dynamics, and interpretable AI concepts.

Conclusion: BlueGlass facilitates robust AI system safety workflows and provides foundational insights for developing reliable AI systems.

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [33] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: The paper evaluates AI and RL approaches for solving edge-cloud application migration modeled as Towers of Hanoi (ToH) problems.


<details>
  <summary>Details</summary>
Motivation: To enhance QoS and cost-efficiency through application migration in edge-cloud systems, while understanding AI/RL techniques suited for this task.

Method: Analyzing and comparing state-of-the-art AI planning and RL techniques using a new classification based on state space definition.

Result: The analysis highlights how different approaches can handle application migration modeled as ToH problems.

Conclusion: Understanding which AI/RL approaches are effective in orchestrating migration in edge-cloud systems for emerging environments.

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [34] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: This paper proposes using metacognitive prompts, such as "Could you be wrong?", to help large language models (LLMs) identify and address their own biases and errors.


<details>
  <summary>Details</summary>
Motivation: Biases in large language models (LLMs) are an ongoing concern, and current debiasing efforts might not remain effective as models evolve. The author seeks a durable strategy inspired by human decision-making techniques.

Method: The author uses metacognitive prompts, specifically "Could you be wrong?", to engage LLMs in self-reflection, encouraging them to identify errors, biases, contradictory evidence, and alternatives upon responding to queries.

Result: Introducing the "Could you be wrong?" prompt leads LLMs to reveal metacognitive insights, such as potential biases, reasoning pathways, and alignment issues between user intent and model interpretations.

Conclusion: Incorporating concepts from human decision-making into LLM prompt engineering offers a promising path for enhancing their cognitive capabilities, particularly in identifying and mitigating biases.

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [35] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: This paper presents a UAV resource allocation scheme using LLM-In-Context Learning (FRSICL) for real-time wildfire monitoring, outperforming conventional DRL methods.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and time-critical monitoring of wildfires necessitated the optimization of UAV flight controls and sensor data scheduling.

Method: The authors developed FRSICL, an LLM-In-Context Learning-based approach, for optimizing UAV velocity and data collection in real-time.

Result: Simulation results demonstrated that FRSICL outperformed DRL-based methods like PPO and simpler baselines.

Conclusion: FRSICL is a promising alternative to DRL for dynamic and time-critical applications like wildfire monitoring, thanks to its flexibility and effectiveness.

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [36] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: The paper introduces the concept of adaptability for evaluating Multi-Agent Reinforcement Learning (MARL) algorithms to improve their real-world deployments under dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work is to address the gap between the demonstrated success of MARL in simulated environments and its limited real-world deployment. The challenges are rooted in the dynamic and complex nature of real-world scenarios, where factors such as fluctuating agent populations, task evolution, and inconsistent conditions arise.

Method: The proposed method involves introducing the concept of adaptability, which is evaluated through three dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. This structured framework aims to assess MARL's capacity to remain effective amid dynamic environmental changes.

Result: The adaptability framework provides a conceptual lens for assessing MARL performance in dynamic conditions, extending evaluations beyond traditional static benchmarks.

Conclusion: This work emphasizes the importance of adaptability for MARL systems to better support their real-world applicability, offering a more robust foundation for algorithm development and deployment.

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [37] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: This paper introduces Swiss Food Knowledge Graph (SwissFKG) to centralize nutrition-related data, utilizes LLMs for enrichment, and proposes tools to enhance dietary assessments.


<details>
  <summary>Details</summary>
Motivation: Existing dietary assessment systems lack integration of non-visual factors such as ingredient substitutions and personal dietary needs, particularly within the Swiss context.

Method: The authors developed SwissFKG as a comprehensive graph combining recipes, ingredients, nutrient data, and dietary guidelines. They employed an LLM-powered enrichment pipeline and benchmarked several LLMs for food data augmentation.

Result: LLMs effectively enriched the SwissFKG with nutritional information, and the Graph-RAG application successfully demonstrated answering user-specific nutrition queries based on the graph.

Conclusion: SwissFKG sets the stage for advanced dietary assessment tools by uniting diverse nutritional data sources and enabling personalized nutrition guidance leveraging LLM-interaction.

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [38] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: The paper compares Decision Transformer (DT) and Filtered Behavior Cloning (FBC) in reinforcement learning tasks, arguing FBC shows better performance in sparse-reward environments.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether Decision Transformer (DT) is a suitable choice for reinforcement learning tasks, especially under sparse-reward and dense-reward settings.

Method: Conduct experiments using robotic manipulation tasks (Robomimic) and locomotion benchmarks (D4RL), applying FBC which filters low-performing trajectories before performing ordinary behavior cloning.

Result: Filtered Behavior Cloning (FBC) outperformed or matched DT in sparse-reward environments, requiring less training data and computational resources.

Conclusion: The findings call into question DT's superiority in reinforcement learning tasks, suggesting FBC is a more efficient alternative in sparse-reward environments.

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [39] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: The paper discusses gaps in XAI research for data analysis and proposes a framework for categorization and reporting using three dimensions (what, why, who), aiming to address design inconsistencies.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address contradictions and lack of clear design recommendations in XAI research, especially due to inadequate task understanding and insufficient context reporting.

Method: The paper draws from visual analytics, cognition, and dashboard design to create a categorization framework (what, why, who) and suggests reporting guidelines for XAI tasks.

Result: The authors identified major issues like poor task descriptions and lack of context or target-user testing, and propose guidelines to improve study generalizability and clarity.

Conclusion: This work provides researchers with tools to better navigate the XAI domain, address research gaps, and reconcile contradictory findings in XAI design.

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [40] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: The paper surveys LLM-based Table Agents designed to automate workflows involving tables by addressing noise, structural heterogeneity, and semantic complexity challenging real-world data. Five core competencies are defined for evaluation, highlighting a performance gap between academic and practical benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle real-world challenges in handling noisy and heterogeneous tables, a problem often ignored in academic research that assumes clean datasets. The goal is to enable automated solutions through LLMs for practical, table-centric tasks.

Method: The authors propose five core competencies to evaluate current LLM approaches to tables: structure understanding, semantic interpretation, data retrieval/compression, reasoning traceability, and domain generalization. They analyze approaches, focusing on Text-to-SQL Agents.

Result: The study reveals a notable performance discrepancy between open-source Text-to-SQL agents tested on academic benchmarks versus real-world, noisy scenarios. It identifies weaknesses in adapting to domain-specific challenges in practical settings.

Conclusion: Actionable insights are shared to enhance the efficiency, robustness, and generalization abilities of LLM-based Table Agents, catering specifically to real-world demands.

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [41] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: This paper introduces Instance Space Analysis (ISA) to analyze the relationships between instance characteristics and metaheuristic performance in CVRP using machine learning methods, dimensionality reduction, and a dataset from DIMACS.


<details>
  <summary>Details</summary>
Motivation: There is a need to understand how instance characteristics influence metaheuristic performance in solving Capacitated Vehicle Routing Problems (CVRP).

Method: The study employed three stages—PRELIM, SIFTED, and PILOT—using dimensionality reduction and machine learning techniques to create a two-dimensional projection of the instance space.

Result: The analysis identified 23 relevant instance characteristics and provided a projection matrix for incorporating new instances, enabling deeper insights into the CVRP field.

Conclusion: This work contributes a new method for analyzing instances in CVRP and offers tools for better understanding and improving metaheuristic approaches.

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [42] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: The paper tackles the issue of school dropout in distance learning by presenting a model that integrates sentiment analysis (using BERT) and socio-demographic/behavioral data (processed with XGBoost).


<details>
  <summary>Details</summary>
Motivation: The motivation is to combat the serious problem of school dropout in distance learning, recognizing the need for early detection and effective intervention to promote student perseverance.

Method: The approach combines sentiment analysis of student comments with socio-demographic and behavioral data using two machine learning tools: fine-tuning BERT for nuanced sentiment analysis and using XGBoost for feature importance and predictions.

Result: The model achieved an accuracy of 84% on unseen data, outperforming the baseline model (82%) along with better precision and F1-score results.

Conclusion: The study highlights the efficacy of combining sentiment analysis and diverse educational data sources, demonstrating its potential to aid in developing personalized strategies to mitigate dropout rates.

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [43] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: The paper introduces efficient neural architectures for learning priors in data-scarce domains, applying these designs to 3D scene generation, segmentation, and molecular property prediction.


<details>
  <summary>Details</summary>
Motivation: To enhance the ability of neural networks to adapt to novel tasks with limited data, especially in domains like computational chemistry, medical imaging, and computational immunology where large pre-trained models cannot be employed.

Method: Utilizes neural memory for adaptation to non-stationary distributions with few samples, hypernetworks trained with Model-Agnostic Meta-Learning (MAML), and a repurposed molecular generative method for pre-training.

Result: Demonstrated efficient prior acquisition for fast text-to-3D generation, 3D segmentation on new scenes, and improved molecular property prediction with limited data.

Conclusion: The proposed architectures effectively address data limitations by enabling efficient prior transfer and adaptation, offering solutions for various computational and scientific challenges.

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [44] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{Eco}$ is an LLM-based system designed for scientific analysis, offering depth- and breadth-controlled exploration to enhance retrieval and integration of scientific literature.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in conventional retrieval-augmented pipelines by offering a more nuanced, user-configurable approach to scientific synthesis.

Method: DeepResearch$^{Eco}$ employs recursive exploration with transparent reasoning and configurability to integrate diverse scientific sources while maintaining analytical rigor.

Result: When applied to 49 ecological research questions, DeepResearch$^{Eco}$ showed significant improvements, including a 21-fold increase in source integration and a 14.9-fold rise in integrated sources per 1,000 words.

Conclusion: DeepResearch$^{Eco}$ enhances analytical depth and contextual diversity in scientific exploration, providing a high-throughput approach for integrating domain-specific evidence with expert-level precision.

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [45] [CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers](https://arxiv.org/abs/2507.08923)
*Rubén Rodríguez Álvarez,Denisa-Andreea Constantinescu,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: This paper introduces CEO-DC, a model to optimize carbon and economic factors in data centers, showing that while timely platform upgrades may reduce emissions, they demand economic incentives and policy support due to persistent growth in compute demand.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unsustainable growth in energy consumption and greenhouse gas emissions by data centers driven by increasing demand for AI and scientific workloads, while accounting for economic and operational trade-offs in transitioning platforms.

Method: The paper proposes CEO-DC, an integrated model and methodology that evaluates cost, carbon emissions, and compute demand to offer optimal strategies for platform procurement, replacement, and design.

Result: Using CEO-DC, the study finds that upgrading legacy platforms every 4 years can reduce emissions but is not scalable without increasing emissions in 44% of cases, requires economic incentives in 72% of cases, and is hampered by insufficient carbon prices in many leading data center countries.

Conclusion: CEO-DC provides guidance for achieving sustainable data center operations by strategically timing upgrades, managing data center growth, optimizing cost-performance trade-offs, and advocating for better policies and incentives.

Abstract: The rapid expansion of data centers (DCs) to support large-scale AI and
scientific workloads is driving unsustainable growth in energy consumption and
greenhouse gas emissions. While successive generations of hardware platforms
have improved performance and energy efficiency, the question remains whether
new, more efficient platforms can realistically offset the rising emissions
associated with increasing demand. Prior studies often overlook the complex
trade-offs in such transitions by failing to account for both the economic
incentives and the projected compute demand growth over the operational
lifetime of the devices. In response, we present CEO-DC, an integrated model
and decision-making methodology for Carbon and Economy Optimization in Data
Centers. CEO-DC models the competing forces of cost, carbon, and compute demand
to guide optimal platform procurement and replacement strategies. We propose
metrics to steer procurement, platform design, and policy decisions toward
sustainable DC technologies. Given current platform trends, our AI case study
using CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces
total emissions. However, these upgrades fail to scale with DC demand growth
trends without increasing total emissions in over 44% of cases, and require
economic incentives for adoption in over 72%. Furthermore, current carbon
prices are insufficient to motivate upgrades in 9 out of the 14 countries with
the highest number of DCs globally. We also find that optimizing platforms for
energy efficiency at the expense of latency can increase the carbon price
required to justify their adoption. In summary, CEO-DC provides actionable
insights for DC architects, platform designers, and policymakers by timing
legacy platform upgrades, constraining DC growth to sustainable levels,
optimizing platform performance-to-cost ratios, and increasing incentives.

</details>


### [46] [Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference](https://arxiv.org/abs/2507.09010)
*Chun-Ting Chen,HanGyeol Mun,Jian Meng,Mohamed S. Abdelfattah,Jae-sun Seo*

Main category: cs.AR

TL;DR: The paper proposes a highly efficient edge inference accelerator for large language models (LLMs) using a hybrid systolic array architecture combined with MXINT4 weight quantization and optimized non-linear operations.


<details>
  <summary>Details</summary>
Motivation: To enable secure, low-latency, and cost-effective edge inference for large language models, addressing memory and compute inefficiencies in the constraints of edge devices.

Method: The authors designed an edge inference accelerator featuring a hybrid systolic array (HSA) architecture, MXINT4 weight quantization, optimized dataflows, and specialized units for non-linear operations like RMSNorm and RoPE.

Result: The proposed accelerator achieves up to 247/117 (token/s/mm²) performance and shows more than 2.45x to 13.5x improvement in efficiency compared to existing approaches, with minimal accuracy loss and high energy efficiency.

Conclusion: This novel accelerator provides a significant improvement in inference efficiency for LLMs on edge devices, with optimized memory and energy usage, showcasing its value for long-input/long-output scenarios.

Abstract: Edge inference for large language models (LLM) offers secure, low-latency,
and cost-effective inference solutions. We emphasize that an edge accelerator
should achieve high area efficiency and minimize external memory access (EMA)
during the memory-bound decode stage, while maintaining high energy efficiency
during the compute intensive prefill stage. This paper proposes an edge LLM
inference accelerator featuring a hybrid systolic array (HSA) architecture that
optimizes inference efficiency in both stages. To further reduce EMA, we adopt
MXINT4 weight quantization and propose an optimized dataflow tailored for HSA,
ensuring negligible dequantization overhead and achieving 100% hardware
utilization with minimal accuracy loss under edge DRAM bandwidth constraints.
For non-linear operations, we incorporate optimized root mean square
normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing
their latency, area, and memory access overhead while enabling end-to-end
inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while
running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x
improvement over existing approaches, while maintaining superior energy
efficiency in token generation.

</details>


### [47] [SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding](https://arxiv.org/abs/2507.09201)
*Weihong Xu,Haein Choi,Po-kai Hsu,Shimeng Yu,Tajana Rosing*

Main category: cs.AR

TL;DR: The paper discusses SLIM, an algorithm-hardware co-design for efficient sparse LLM inference on edge devices, enhancing throughput and energy efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenge of deploying large language models (LLMs) on resource-constrained embedded devices by tackling inefficiencies in memory and computation due to existing systems' failure to utilize LLM sparsity.

Method: The proposed method combines an adaptive thresholding algorithm to exploit LLM sparsity with a heterogeneous hardware architecture that integrates near-storage processing (NSP) and processing-in-memory (PIM) to handle different types of operations efficiently.

Result: The evaluation of SLIM shows significant improvements, achieving 13-18x higher throughput than SSD-GPU systems and 9-10x better energy efficiency than DRAM-GPU systems, all with negligible accuracy loss.

Conclusion: SLIM makes cost-effective and energy-efficient large language model deployment feasible for edge computing environments by leveraging sparsity in LLM operations and optimizing hardware design.

Abstract: Large language models (LLMs) have demonstrated exceptional proficiency in
understanding and generating human language, but efficient inference on
resource-constrained embedded devices remains challenging due to large model
sizes and memory-intensive operations in feedforward network (FFN) and
multi-head attention (MHA) layers. While existing accelerators offload LLM
inference to expensive heterogeneous computing systems, they fail to exploit
the significant sparsity inherent in LLM operations, leaving hardware resources
underutilized. We propose SLIM, an algorithm-hardware co-design optimized for
sparse LLM serving on edge devices. SLIM exploits LLM sparsity through an
adaptive thresholding algorithm that enables runtime-configurable sparsity with
negligible accuracy loss, fetching only activated neurons to dramatically
reduce data movement. Our heterogeneous hardware architecture strategically
combines near-storage processing (NSP) and processing-in-memory (PIM): FFN
weights are stored in high-density 3D NAND and computed using NSP units, while
memory-intensive MHA operations are processed in PIM modules. This design
significantly reduces memory footprint, data movement, and energy consumption.
Our comprehensive evaluation demonstrates SLIM's effectiveness, achieving
13-18x throughput improvements over SSD-GPU systems and 9-10x better energy
efficiency over DRAM-GPU systems while maintaining low latency, making
cost-effective LLM deployment viable for edge computing environments.

</details>


### [48] [Tools and Methodologies for System-Level Design](https://arxiv.org/abs/2507.09660)
*Shuvra S. Bhattacharyya,Marilyn Wolf*

Main category: cs.AR

TL;DR: System-level design has become essential for chip designers due to the challenges of system-on-chip (SoC) development. Tools for modeling, simulation, design space exploration, and verification are critical.


<details>
  <summary>Details</summary>
Motivation: Chip design is a less forgiving design medium compared to board design, requiring advanced tools and methods to manage the complexity and risks in SoC development.

Method: The paper focuses on system-level design tools for modeling, simulation, design space exploration, and verification. Models of computation and communication between these models are highlighted.

Result: The chapter utilizes video and neural network applications as practical examples to showcase the studied methodologies.

Conclusion: System-level design tools and methodologies are essential for capturing operational semantics, exploring design spaces, and ensuring functional correctness in advanced high-performance applications.

Abstract: System-level design, once the province of board designers, has now become a
central concern for chip designers. Because chip design is a less forgiving
design medium -- design cycles are longer and mistakes are harder to correct --
system-on-chip designers need a more extensive tool suite than may be used by
board designers and a variety of tools and methodologies have been developed
for system-level design of systems-on-chips (SoCs). System-level design is less
amenable to synthesis than are logic or physical design. As a result,
system-level tools concentrate on modeling, simulation, design space
exploration, and design verification. The goal of modeling is to correctly
capture the system's operational semantics, which helps with both
implementation and verification. The study of models of computation provides a
framework for the description of digital systems. Not only do we need to
understand a particular style of computation, such as dataflow, but we also
need to understand how different models of computation can reliably communicate
with each other. Design space exploration tools, such as hardware/software
co-design, develop candidate designs to understand trade-offs. Simulation can
be used not only to verify functional correctness but also to supply
performance and power/energy information for design analysis. This chapter
employs two applications -- video and neural networks -- as examples. Both are
leading-edge applications that illustrate many important aspects of
system-level design.

</details>


### [49] [Efficient FRW Transitions via Stochastic Finite Differences for Handling Non-Stratified Dielectrics](https://arxiv.org/abs/2507.09730)
*Jiechen Huang,Wenjian Yu*

Main category: cs.AR

TL;DR: The paper introduces MicroWalk, an algorithm for accurate and efficient floating-random-walk (FRW) transitions in capacitance extraction, overcoming challenges with advanced dielectrics.


<details>
  <summary>Details</summary>
Motivation: Existing FRW methods struggle with advanced dielectric profiles in capacitance extraction due to biased sampling caused by approximations, limiting accuracy.

Method: The authors propose MicroWalk, an unbiased FRW transition algorithm capable of handling arbitrary dielectrics efficiently. It uses transition probabilities equivalent to finite difference methods but is 802x faster.

Result: The new algorithm is integrated into a hybrid 3-D capacitance solver, demonstrating superior accuracy and efficiency on real-world structures compared to traditional FRW solvers.

Conclusion: MicroWalk addresses challenges posed by non-stratified dielectrics and significantly improves the balance between accuracy and efficiency in FRW-based capacitance solvers.

Abstract: The accuracy of floating-random-walk (FRW) based capacitance extraction
stands only when the recursive FRW transitions are sampled unbiasedly according
to surrounding dielectrics. Advanced technology profiles, featuring complicated
non-stratified dielectrics, challenge the accuracy of existing FRW transition
schemes that approximate dielectrics with stratified or eight-octant patterns.
In this work, we propose an algorithm named MicroWalk, enabling accurate FRW
transitions for arbitrary dielectrics while keeping high efficiency. It is
provably unbiased and equivalent to using transition probabilities solved by
finite difference method, but at orders of magnitude lower cost (802$\times$
faster). An enhanced 3-D capacitance solver is developed with a hybrid strategy
for complicated dielectrics, combining MicroWalk with the special treatment for
the first transition cube and the analytical algorithm for stratified cubes.
Experiments on real-world structures show that our solver achieves a
significant accuracy advantage over existing FRW solvers, while preserving high
efficiency.

</details>


### [50] [Low-Cost Fuel Dispenser Prototype Using STM32 and an H-bridge motor driver](https://arxiv.org/abs/2507.09774)
*MD Zobaer Hossain Bhuiyan,Abir Bin Faruque,Mahtab Newaz,Mohammad Abdul Qayum*

Main category: cs.AR

TL;DR: This paper details a low-cost, portable fuel dispensing prototype using an STM32 microcontroller and L298N motor driver, aimed at small-scale and remote environments.


<details>
  <summary>Details</summary>
Motivation: To develop an affordable, scalable fuel dispensing system for remote or small-scale settings where conventional systems are too expensive and impractical.

Method: The system uses an STM32 microcontroller for control, a 4x4 keypad for user input, a 16x4 LCD for display, and a 12V DC pump motor controlled by an L298N motor driver. It is powered by an 11.1V battery and emphasizes efficiency and portability.

Result: The prototype successfully simulates a cost-effective and user-friendly fuel dispensing mechanism with accurate motor runtime control.

Conclusion: The design showcases the potential of embedded systems for building economical and energy-efficient fuel dispensing solutions, with future scope for integrating advanced features like flow sensors, GSM, RFID, and payment modules.

Abstract: This paper presents the design and development of a low-cost fuel dispensing
system prototype based on the STM32 microcontroller and L298N motor driver. The
system aims to provide an affordable and scalable solution for fuel delivery in
remote or small-scale environments where conventional, high-cost systems are
not feasible. The core control unit is built using an STM32 microcontroller,
which manages user input through a 4x4 matrix keypad and displays operational
data on a 16x4 LCD screen via I2C communication. A 12V DC pump motor is used to
simulate the fuel dispensing mechanism, precisely controlled via the dual
H-bridge L298N motor driver. The system is powered by a 11.1V battery and is
designed for ease of deployment and portability. The keypad allows users to
input the desired fuel amount, while the system ensures accurate motor runtime
corresponding to the volume to be dispensed. This project demonstrates how
embedded systems can be leveraged to build cost-effective, user-friendly, and
energy-efficient solutions. The proposed design can be further enhanced with
flow sensors, GSM connectivity, RFID cards, and payment integration for
real-world applications in fuel stations or agricultural use.

</details>


### [51] [BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs](https://arxiv.org/abs/2507.09780)
*Feilong Qiaoyuan,Jihe Wang,Zhiyu Sun,Linying Wu,Yuanhua Xiao,Danghui Wang*

Main category: cs.AR

TL;DR: The paper proposes a novel Multiply-Accumulate (MAC) unit and scheduling strategy to efficiently leverage dual-factor bit-level sparsity in quantized Deep Neural Networks (DNNs), addressing challenges like wasted sparsity and pipeline stalls.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address inefficiencies in Multiply-Accumulate (MAC) operations, particularly wasted dual-factor sparsity and underutilization of MAC units caused by conventional approaches.

Method: The study introduces a particlization-based MAC unit design to exploit dual-factor sparsity and manages hardware challenges with simple control logic. Additionally, it proposes a quasi-synchronous scheduling scheme to increase the utilization of MAC units.

Result: The proposed MAC architecture achieves a 29.2% improvement in area efficiency over existing bit-sparsity-driven designs, while its approximate variant offers a further 7.5% energy efficiency boost.

Conclusion: The work provides a more area-efficient, energy-efficient method for DNN acceleration, addressing key challenges of dual-factor sparsity and irregular MAC operation cycles at the cost of slight accuracy trade-offs.

Abstract: Bit-level sparsity in quantized deep neural networks (DNNs) offers
significant potential for optimizing Multiply-Accumulate (MAC) operations.
However, two key challenges still limit its practical exploitation. First,
conventional bit-serial approaches cannot simultaneously leverage the sparsity
of both factors, leading to a complete waste of one factor' s sparsity. Methods
designed to exploit dual-factor sparsity are still in the early stages of
exploration, facing the challenge of partial product explosion. Second, the
fluctuation of bit-level sparsity leads to variable cycle counts for MAC
operations. Existing synchronous scheduling schemes that are suitable for
dual-factor sparsity exhibit poor flexibility and still result in significant
underutilization of MAC units. To address the first challenge, this study
proposes a MAC unit that leverages dual-factor sparsity through the emerging
particlization-based approach. The proposed design addresses the issue of
partial product explosion through simple control logic, resulting in a more
area- and energy-efficient MAC unit. In addition, by discarding less
significant intermediate results, the design allows for further hardware
simplification at the cost of minor accuracy loss. To address the second
challenge, a quasi-synchronous scheme is introduced that adds cycle-level
elasticity to the MAC array, reducing pipeline stalls and thereby improving MAC
unit utilization. Evaluation results show that the exact version of the
proposed MAC array architecture achieves a 29.2% improvement in area efficiency
compared to the state-of-the-art bit-sparsity-driven architecture, while
maintaining comparable energy efficiency. The approximate variant further
improves energy efficiency by 7.5%, compared to the exact version. Index-Terms:
DNN acceleration, Bit-level sparsity, MAC unit

</details>


### [52] [Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving](https://arxiv.org/abs/2507.10178)
*Wonung Kim,Yubin Lee,Yoonsung Kim,Jinwoo Hwang,Seongryong Oh,Jiyong Jung,Aziz Huseynov,Woong Gyu Park,Chang Hyun Park,Divya Mahajan,Jongse Park*

Main category: cs.AR

TL;DR: The paper addresses the scalability challenges of long-context inferencing in transformers and explores efficient systems to support both transformers and alternative architectures like SSMs and RNNs. It proposes a novel hardware design, Pimba, for improved token generation throughput.


<details>
  <summary>Details</summary>
Motivation: To address the scalability and efficiency challenges in serving large language models with different architectural needs (transformers and post-transformers) under a unified framework.

Method: The study analyzes performance characteristics of transformer and post-transformer models, leading to the design of Pimba, a hardware system with shared State-update Processing Units optimized using MX-based quantized arithmetic.

Result: Pimba delivers up to 3.2x higher token generation throughput compared to GPUs designed for LLMs, and 2.1x higher than GPU+PIM systems.

Conclusion: Pimba represents a significant breakthrough in hardware acceleration for diverse LLM architectures, bridging scalability gaps while maximizing throughput.

Abstract: Transformers are the driving force behind today's Large Language Models
(LLMs), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-transformers. This shift presents a key challenge: building a serving
system that efficiently supports both transformer and post-transformer LLMs
within a unified framework. To address this challenge, we analyze the
performance characteristics of transformer and post-transformer LLMs. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in transformers and state
updates in post-transformers. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM acceleration inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x
higher token generation throughput, respectively.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [53] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: Spatial ModernBERT leverages text and spatial embeddings to accurately extract tabular data and key-value pairs from financial documents using a token classification approach.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for accurate extraction of tables and key-value pairs from financial documents to improve business workflows like auditing and data processing.

Method: The model Spatial ModernBERT utilizes spatial embeddings and a transformer-based architecture, performing token classification across three heads (Label, Column, Row). It is pretrained on PubTables-1M and fine-tuned with cross-entropy loss on a financial dataset.

Result: The proposed model demonstrates robustness and high accuracy in extracting tabular layouts and key-value pairs, verified through empirical evaluation.

Conclusion: Spatial ModernBERT effectively integrates textual and spatial information to enhance automated processing of financial documents.

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [54] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: The paper presents SEALGuard, a multilingual safety guardrail system to enhance LLM safety alignment in detecting unsafe and jailbreak inputs in diverse languages.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current guardrail systems like LlamaGuard in handling multilingual unsafe and jailbreak prompts, particularly in low-resource languages.

Method: The researchers adapted a general-purpose multilingual language model into a guardrail using low-rank adaptation (LoRA) and introduced a large-scale multilingual dataset, SEALSBench, for evaluation.

Result: SEALGuard significantly outperformed existing systems like LlamaGuard, improving Defense Success Rate (DSR) by 48% and achieving superior precision and F1-scores.

Conclusion: SEALGuard effectively closes the multilingual safety alignment gap, demonstrating better performance and robustness in handling unsafe and jailbreak prompts across multiple languages.

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [55] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: The paper assesses the limitations of large language models (LLMs) in medical question answering, especially focusing on inadequacies in evaluation datasets, and underscores the need for better frameworks.


<details>
  <summary>Details</summary>
Motivation: To identify weaknesses in current datasets and evaluation methods of LLMs in providing accurate medical question answering, aiming to improve clinical relevance and rigor.

Method: Reviewed widely-used benchmark datasets such as MedQA and alternative tools like medical journal challenge questions for their rigor, realism, and potential efficacy.

Result: Identified significant shortcomings in datasets like lack of clinical realism and robust validation. Challenge questions show promise but are limited in scope and size.

Conclusion: Calls for a standardized, collaborative framework to improve evaluation datasets for LLMs in medicine, ensuring rigor, transparency, and relevance to clinical practices.

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [56] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: This paper introduces two Korean expert-level benchmarks designed to evaluate the abilities of Large Language Models (LLMs) in reflecting industrial knowledge.


<details>
  <summary>Details</summary>
Motivation: The paper aims to fill the gap in evaluating LLMs not only in academic fields but also in industrial domains that hold real-world relevance.

Method: Two benchmarks, KMMLU-Redux and KMMLU-Pro, were constructed using questions from Korean technical qualification and professional licensure exams, ensuring reliability by removing errors.

Result: The benchmarks successfully represent industrial knowledge in Korea, and experiments validated their capability to assess LLMs on this front.

Conclusion: These datasets enhance the scope of LLM evaluation and are publically available to advance research in this domain.

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [57] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS introduces a method for improving LLM alignment during inference without relying on external annotated data by autonomously generating contrastive samples.


<details>
  <summary>Details</summary>
Motivation: Existing model-steering methods depend on external annotated data for aligning LLMs, which limits context adaptability and effectiveness.

Method: The proposed SIMS framework generates and refines contrastive samples through self-improvement cycles and employs techniques such as prompt ranking and contrast sampling.

Result: SIMS surpasses traditional methods in steering effectiveness and adaptability across various LLMs and benchmarks.

Conclusion: Self-improving model steering (SIMS) is a promising approach for inference-time LLM alignment, demonstrating superior adaptability and efficiency.

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [58] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: The study identifies stigmatizing language in electronic health records and links it to patient demographics and provider types.


<details>
  <summary>Details</summary>
Motivation: To investigate how stigmatization in healthcare manifests through linguistic features in electronic health records, focusing on patient demographics and provider types.

Method: Analyzed linguistic features using lexicon matching, supervised learning classifiers, and Poisson regression models to determine predictors of stigmatizing language.

Result: Higher rates of stigmatizing labels were observed among Black/African American patients, patients with government insurance, self-paying patients, and those with stigmatizing health conditions. Nurses and social workers were identified as contributors.

Conclusion: Stigmatizing language in EHR perpetuates among historically marginalized patient groups and across different healthcare provider types.

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [59] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: The study explores how imagery abilities (absent, typical, vivid) influence visual hallucinations during Ganzflicker stimulation, using NLP to analyze hallucination descriptions from over 4,000 participants.


<details>
  <summary>Details</summary>
Motivation: The research aims to investigate how individual differences in visual imagery abilities impact hallucination experiences, as these differences may provide insight into the generative capacity of the visual system.

Method: The team analyzed free-text descriptions of hallucinations from over 4,000 participants using tools from natural language processing, comparing imagery phenotypes (strong vs. weak imagers) and examining sensorimotor associations in language usage.

Result: Strong imagers described more complex, naturalistic hallucinations, while weak imagers reported simpler geometric patterns. Vision-language model embeddings captured these distinctions better than text-only language models.

Conclusion: The findings suggest that variations in imagery abilities reflect differences in coordination between early visual areas and higher-order brain regions, emphasizing the generative diversity in the visual system.

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [60] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard introduces a method to make Transformer-based large language models more efficient for infinite-context tasks using subquadratic attention with a hybrid mechanism of global and local interaction handling.


<details>
  <summary>Details</summary>
Motivation: The authors seek to address computational inefficiencies and memory bottlenecks in Transformer-based LLMs caused by quadratic softmax attention and growing key-value caches as context lengths increase.

Method: They implement a subquadratic attention mechanism approximating softmax attention, incorporate a gating module inspired by state-of-the-art models, and combine global and local attention techniques to optimize memory control and sequence length generalization.

Result: Lizard outperforms previous linearization methods, achieves near-lossless recovery of the teacher model's quality, improves the 5-shot MMLU benchmark by 18 points, and excels in associative recall tasks.

Conclusion: Lizard enables more efficient, flexible, and scalable Transformer-based LLMs while maintaining high performance, offering promising improvements in long-context modeling.

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [61] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: This paper proposes ALIGN, a system for aligning and personalizing large language model (LLM)-based decision-makers to diverse user values/preferences through prompt-based adjustments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of aligning LLMs to user-specific values and preferences for dynamic and personalized decision-making, moving beyond conventional benchmarking tasks.

Method: Developed ALIGN, a system incorporating prompt-based alignment to fine-grained attributes, robust configurations, structured outputs, and modular integrations, enabling qualitative and quantitative analysis of LLMs.

Result: Demonstrated the effectiveness of ALIGN in two domains: demographic alignment for public opinion surveys and value alignment for medical triage, providing a qualitative comparison tool and quantitative insights.

Conclusion: ALIGN offers an open-source framework for advancing research on personalized, responsible, and reliable LLM-based decision-making systems, accommodating the diversity of user needs.

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [62] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: This paper introduces OpenCodeReasoning-II, a large-scale dataset featuring 2.5M question-solution-critique triples for code reasoning, alongside fine-tuned models achieving superior code generation and critique performance.


<details>
  <summary>Details</summary>
Motivation: Large-scale, high-quality datasets are crucial for progress in distillation for code generation and critique, especially given advancements in reasoning-based Large Language Models.

Method: The study employs a two-stage supervised fine-tuning approach: first for code generation and then for joint training for both code generation and critique, using the OpenCodeReasoning-II dataset to train Qwen2.5-Instruct models.

Result: Fine-tuned models achieve performance in code generation exceeding or matching prior open-weight models. Integration of code generation and critique models boosts competitive coding performance.

Conclusion: The proposed dataset and methods significantly advance code reasoning and evaluation capabilities, supporting robust development and assessment of LLMs in programming tasks.

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [63] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: This paper presents a Dynamic Parameter Memory (DPM) mechanism to address the limitations of speech large language models (SLLMs) in processing long audio content for speech emotion recognition. Using a novel encoding approach, it achieves state-of-the-art results on the IEMOCAP dataset.


<details>
  <summary>Details</summary>
Motivation: Speech large language models (SLLMs) struggle with processing high-frame-rate speech data, leading to limitations in handling long audio sequences for emotion recognition due to context window constraints.

Method: The authors propose a Dynamic Parameter Memory (DPM) mechanism that encodes sentence-level information and emotions into a temporary LoRA module during inference. This enables the efficient handling of unlimited-length audio with limited context windows.

Result: The proposed DPM mechanism, when integrated with an emotion SLLM backbone, improved emotion recognition in long audio sequences, achieving state-of-the-art results on the IEMOCAP dataset.

Conclusion: Dynamic Parameter Memory (DPM) enhances the capability of speech large language models to process and understand emotions in extended conversational audio, addressing the inherent limitations of SLLMs and advancing emotion recognition performance.

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [64] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: The paper introduces CompassJudger-2, a new generalist judge model for evaluating large language models, addressing the limitations of current evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current judge models for assessing large language models are narrow in specialization and lack robustness, which hinders their effectiveness in comprehensive evaluations.

Method: The authors employed a task-driven, multi-domain data curation strategy supervised with verifiable rewards and utilized rejection sampling for intrinsic critical reasoning. They also developed a margin policy gradient loss to refine the learning objective.

Result: CompassJudger-2 showed superior judgment accuracy across multiple benchmarks and performed competitively with significantly larger models, such as DeepSeek-V3 and Qwen3-235B-A22B.

Conclusion: The research advances scalable and robust LLM judgment while proposing new performance benchmarks like JudgerBenchV2 to standardize model evaluation.

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [65] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD utilizes GPT-4.5 to generate compact, domain-specific references for improving crystallography question answering in smaller AI models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of copyright issues and the lack of domain-specific training in smaller AI models for X-ray diffraction (XRD).

Method: It integrates GPT-4.5-generated summaries as open-book references and compares the performance of various vision-language models in answering XRD questions under open-book and closed-book conditions.

Result: Models demonstrated significant accuracy improvements with GPT-4.5 summaries, especially those with limited crystallography training.

Conclusion: OPENXRD highlights that AI-generated domain-specific texts can enhance reasoning in scientific tasks, laying groundwork for future NLP tools in specialized fields.

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [66] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: The paper proposes PU-Lie, a model for detecting deception in dialogues, using frozen BERT embeddings, linguistic/game features, and a PU learning approach to address severe class imbalance.


<details>
  <summary>Details</summary>
Motivation: Deception detection is challenging in strategic dialogues, particularly given the subtle nature of deceptive communication and the imbalance between truthful and deceptive messages.

Method: The authors combine frozen BERT embeddings, linguistic features, game-specific features, and a Positive-Unlabeled (PU) learning objective to build PU-Lie, a lightweight deception-detection model.

Result: PU-Lie achieves a macro F1 score of 0.60, sets a new benchmark, and reduces trainable parameters by over 650x compared to other models.

Conclusion: The framework focuses on prioritizing the detection of deceptive messages due to their critical importance, leveraging PU learning for scenarios with limited labeled deceptive data.

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [67] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: RAMA is a retrieval-augmented multi-agent framework for verifying multimedia misinformation, excelling in ambiguous claims by leveraging web-based evidence and multi-agent reasoning.


<details>
  <summary>Details</summary>
Motivation: The study addresses the increasing challenge of verifying multimodal misinformation, particularly when claims are ambiguous or lack adequate context.

Method: The framework deploys strategic query formulation for precise web searches, cross-verifies evidence from diverse sources, and utilizes a multi-agent ensemble architecture with different multimodal large language models and prompt variants.

Result: RAMA showed superior performance on benchmark datasets, particularly in resolving ambiguous or improbable claims by grounding its verification process in factual evidence.

Conclusion: The study highlights the importance of web-based evidence integration and multi-agent reasoning, establishing RAMA as a reliable and scalable solution for multimedia misinformation verification.

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [68] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: The paper introduces a fine-tuning method that prunes neurons specialized to dataset-specific mechanisms in large language models (LLMs) to improve generalization, achieving better performance on novel tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models often rely on dataset-specific correlations that yield high-confidence predictions but hinder performance on novel tasks or distributions. The motivation is to improve the generalization capability of LLMs by addressing this issue.

Method: The method employs Integrated Gradients to measure each neuron's contribution to high-confidence predictions, identifying neurons that disproportionately drive dataset-specific mechanisms. These neurons are selectively pruned during fine-tuning to encourage reliance on generalizable representations.

Result: The proposed pruning-based fine-tuning approach demonstrates significant performance improvements across multiple-choice benchmarks, outperforming previous non-pruning adaptation methods.

Conclusion: Pruning dataset-specific neurons promotes generalizable reasoning in LLMs, enabling better adaptability to novel tasks and increasing overall robustness across various benchmarks.

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [69] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: The paper introduces Banzhida, a multilingual large language model optimized for Tibetan, using a curated corpus and tailored benchmarks.


<details>
  <summary>Details</summary>
Motivation: Tibetan language is underrepresented in large language models due to limited high-quality data.

Method: The authors curated the largest Tibetan corpus to date, applied a specialized data processing pipeline, and fine-tuned a multilingual model into Banzhida.

Result: Banzhida significantly outperforms comparable open-source models and Tibetan-specific models across diverse benchmarks.

Conclusion: This advancement facilitates improved generative AI capabilities for Tibetan, addressing its low-resource language challenges.

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [70] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: Visual metaphors are valuable for climate change communication, offering aesthetic appeal and promoting deeper cognitive engagement despite complexity.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of research on how visual metaphors affect climate change communication and to support future studies with consistent material.

Method: The researchers created the MetaClimage database containing climate-related visual metaphors, paired with literal images, human ratings, and analyzed semantic/emotion variables using NLP.

Result: Visual metaphors were harder to understand but more aesthetically pleasing, eliciting higher cognitive and emotional engagement, particularly among individuals with a high Need For Cognition.

Conclusion: Visual metaphors may not always be more effective or arousing, but they enhance aesthetic appreciation and facilitate positive, abstract cognitive experiences, aiding climate change communication.

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [71] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: Introduces the Swa-bhasha Resource Hub, a repository for Romanized Sinhala to Sinhala transliteration data resources and algorithms from 2020 to 2025.


<details>
  <summary>Details</summary>
Motivation: Facilitates research and development in Sinhala NLP by centralizing transliteration resources.

Method: Provides openly accessible datasets and tools for transliteration, along with a comparative analysis of applications.

Result: Comprehensive and publicly available transliteration tools and datasets for advancing Romanized Sinhala research.

Conclusion: The hub significantly contributes to enhancing research and applications in the Sinhala NLP domain.

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [72] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: The paper presents a mechanism, inspired by psychology, to improve humour translation in large language models using a Chain-of-Thought approach. It enhances humour, fluency, and coherence significantly.


<details>
  <summary>Details</summary>
Motivation: Translation models often fail in conveying humour effectively due to linguistic interference and the subtleties of humoristic communication.

Method: This paper introduces a Humour Decomposition Mechanism (HDM) inspired by human psychological processes, integrating humour theory and Chain-of-Thought reasoning to better guide translation models.

Result: The proposed method achieved notable improvements in humour (7.75%), fluency (2.81%), and coherence (6.13%) on open-source humour datasets.

Conclusion: The study provides a framework for better humour translation by simulating human thought processes and integrating humour theory, addressing existing limitations in large language models' performance.

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [73] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: The paper introduces ClaritySpeech, a framework to address dementia-affected speech challenges using ASR, text obfuscation, and zero-shot TTS technologies.


<details>
  <summary>Details</summary>
Motivation: Dementia alters speech patterns, raising communication and privacy challenges, compounded by mainstream ASR technologies' deficiencies in handling dementia-affected speech.

Method: The ClaritySpeech framework integrates automatic speech transcription, text obfuscation, and zero-shot text-to-speech synthesis, focusing on preserving speaker identity and improving performance in low-data scenarios without fine-tuning.

Result: Results demonstrate a drop in mean F1 scores under adversarial conditions (16% for ADReSS and 10% for ADReSSo), significant Word Error Rate (WER) improvements (0.73 to 0.08 for ADReSS and 0.15 for ADReSSo), and an increase in speech quality (from 1.65 to ~2.15).

Conclusion: ClaritySpeech enhances privacy and accessibility for dementia-affected speech, showing effectiveness in improving transcription accuracy, speaker identity preservation, and speech quality.

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [74] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: DATE-LM introduces a benchmark to evaluate methods quantifying training data's impact on large language models, focusing on tasks like data selection and bias filtering.


<details>
  <summary>Details</summary>
Motivation: With data attribution gaining importance in applications like model interpretability and dataset curation, systematic evaluation tailored to LLMs has been lacking.

Method: The authors propose a unified benchmark, DATE-LM, for assessing data attribution methods via three tasks and enabling large-scale, configurable evaluations.

Result: Using the benchmark, evaluation reveals no single attribution method universally outperforms others, highlighting task-specific trade-offs and sensitivity.

Conclusion: DATE-LM lays groundwork for systematized research in assessing data attribution methods, accompanied by a leaderboard to foster community engagement.

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [75] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: This paper optimizes the DRAGON Longformer model for binary classification of clinical text, achieving significant accuracy improvements through tailored adjustments.


<details>
  <summary>Details</summary>
Motivation: The study aimed to improve domain-specific NLP capability in understanding medical text, addressing challenges in classifying clinical observations.

Method: The authors optimized a pre-trained Longformer model using hyperparameter tuning, extending sequence length, adjusting learning rates, adding medical terminology, and validating results statistically.

Result: Performance metrics significantly improved, including accuracy (72.0% to 85.2%), precision, recall, and F1-score, with statistical significance (p < .001).

Conclusion: The optimized model exhibits superior ability for medical text classification, advancing capabilities in clinical NLP with implications for healthcare applications.

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [76] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: The CoNLL-2013 shared task addressed grammatical error correction and evaluated teams using specific metrics and data sets.


<details>
  <summary>Details</summary>
Motivation: To advance methods for grammatical error correction through a shared task format, enabling participants to compare methods and gather insights.

Method: The paper defined grammatical error correction, provided annotated data sets, described evaluation metrics, and assessed team performance in shared task participation.

Result: Various approaches from participating teams were overviewed, and their performance was evaluated using predefined metrics.

Conclusion: Insights into grammatical error correction methods and evaluations were presented, facilitating progress in computational linguistics error correction technologies.

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [77] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: This survey discusses combining reasoning and retrieval for enhancing factuality and inference in LLMs, with frameworks that iteratively integrate search and reasoning achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RAG in handling multi-step inference and reasoning-focused methods in maintaining factuality.

Method: The paper categorizes the advancements in reasoning-enhancing RAG frameworks and highlights frameworks interleaving iterative search and reasoning.

Result: Emerging synergistic RAG-Reasoning frameworks demonstrate superior performance on knowledge-intensive benchmarks.

Conclusion: Future research could focus on developing more effective, multimodal, trustworthy, and human-centric RAG-Reasoning systems.

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [78] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: The paper introduces M2SaG, a multimodal sarcasm generation dataset paired with ViSP, a framework leveraging reward-based and contrastive learning techniques to improve sarcasm text creation outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Sarcasm research lacks focus on generation capabilities, particularly in leveraging visual cues and handling content-intent mismatch.

Method: The authors created a novel dataset, M2SaG, and devised ViSP framework combining Proximal Policy Optimization and contrastive learning to fine-tune sarcastic text generation.

Result: ViSP outperformed baselines, including large language models, producing texts with higher Sarcasm Scores (0.898 vs. 0.770) and higher Factual Incongruity (0.768 vs. 0.739).

Conclusion: The study highlights multimodal approaches' superiority for sarcastic text generation and public release aims to advance research in this domain.

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [79] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: The paper addresses aspect-based sentiment analysis (ABSA) by proposing a new method using large language models (LLMs) and reinforcement learning to optimize data augmentation techniques, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: Current aspect-based sentiment analysis methods face challenges due to short text contexts, small and imbalanced training datasets, and a dominance of positive sentiment labels. Improved data augmentation is necessary to enhance model performance in these scenarios.

Method: The proposed method involves prompting large language models (LLMs) to generate augmented training data based on original datasets, ensuring a larger and more balanced label distribution. Reinforcement learning is used to improve the quality of the augmented synthetic data.

Result: The approach demonstrates superior performance over strong baselines and many existing studies on English benchmark datasets for ABSA.

Conclusion: The proposed LLM-based approach with optimized data augmentation effectively enhances performance in ABSA tasks by addressing limitations in data size, balance, and context quality.

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [80] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax is a multi-agent collaboration framework designed to improve adaptability, coordination, and knowledge reuse in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional AI systems in coordination, memory reuse, and scalability for complex tasks.

Method: GoalfyMax employs a protocol-driven framework with A2A communication, structured memory layers, and advanced safety and dialogue features.

Result: Empirical benchmarks show GoalfyMax's superior performance in adaptability and experience retention compared to alternatives.

Conclusion: GoalfyMax offers a scalable approach for building advanced multi-agent intelligent systems suitable for dynamic tasks.

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [81] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: This paper introduces Ref-Long, a benchmark to evaluate the long-context referencing abilities of LCLMs, revealing gaps in current models' capabilities, including GPT-4.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored area of long-context referencing in LCLMs, which is critical for tasks requiring accurate attribution within long context data.

Method: The authors propose Ref-Long, a benchmark designed to assess referencing by having models identify document indexes associated with a specific key. They create three subsets ranging from synthetic to realistic scenarios to evaluate performance.

Result: Testing with 13 LCLMs, including GPT-4, showed significant deficiencies in long-context referencing capabilities. Extensive analyses including human evaluation and fine-tuning were performed.

Conclusion: LCLMs, including advanced models, demonstrate substantial limitations in long-context referencing, and the Ref-Long benchmark reveals areas for improvement in model precision and comprehension of contextual relationships.

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [82] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: This paper studies how errors in user prompts affect the performance of large language models (LLMs) in machine translation and evaluation tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the sensitivity of large language models to errors and perturbations in user prompts, impacting their reliability in practical applications.

Method: A systematic evaluation was conducted with both human-like and synthetic errors in prompts, analyzing their quantitative impact and providing qualitative insights across various noise types.

Result: Character-level and combined noise degraded performance more than phrasal perturbations. Poor prompt quality impacted instruction following rather than translation quality directly. LLMs still managed translation in cases of extreme noise.

Conclusion: Prompt quality significantly influences translation performance, and even good prompts can fail with too many errors. Despite challenges, LLMs exhibit resilience to noise.

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [83] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: The paper discusses the development and assessment of a definition modeling system tailored for the Belarusian language using a new dataset of 43,150 definitions.


<details>
  <summary>Details</summary>
Motivation: To explore ways to adapt and improve definition modeling systems for supporting languages and lects that lack sufficient documentation or resources.

Method: The authors created a dataset with 43,150 Belarusian definitions and conducted experiments to adapt pre-existing definition modeling systems to this language.

Result: The experiments revealed that adapting definition modeling systems to Belarusian requires minimal data, although current automatic evaluation metrics have limitations.

Conclusion: Adapting language models to unsupported languages like Belarusian is feasible with minimal data, but advancements are needed in automatic evaluation metrics to fully assess model performance.

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [84] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: The paper introduces NMIXX, specialized cross-lingual embedding models for the financial domain with a focus on Korean and English, alongside a Korean financial STS benchmark. NMIXX achieves notable performance gains in semantic tasks compared to seven baselines.


<details>
  <summary>Details</summary>
Motivation: General sentence embedding models have difficulty grasping financial semantics, particularly for low-resource languages like Korean, due to domain-specific jargon and bilingual vocabulary challenges.

Method: The authors fine-tuned cross-lingual embedding models using 18.8K high-confidence triplets, incorporating paraphrases, hard negatives, and Korean-English translations. They also created KorFinSTS, a financial STS benchmark for nuanced evaluation.

Result: NMIXX, especially its multilingual bge-m3 variant, performed significantly better in semantic tasks on financial data, achieving Spearman's rho improvements of +0.10 on English FinSTS and +0.22 on KorFinSTS compared to baselines.

Conclusion: NMIXX and KorFinSTS deliver valuable tools for multilingual representation learning in the financial domain, demonstrating the importance of domain-specific adaptation and tokenizer design for low-resource languages.

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [85] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy is a Python library for simulating spreading activation in cognitive networks, aiding research in psychology, neuroscience, and education.


<details>
  <summary>Details</summary>
Motivation: To develop a tool that facilitates numerical simulations to test structure-function relationships in cognitive networks and link them to psychological and clinical phenomena.

Method: SpreadPy enables simulations of spreading activation on cognitive single-layer and multiplex networks, validated through case studies on knowledge association, creativity tasks, and aphasia-related impairments.

Result: The results include identifying structural differences in conceptual organization linked to math anxiety, showing modulation of cognitive load during tasks, and correlating network activation with error types in aphasia patients.

Conclusion: SpreadPy provides a flexible, reproducible framework for exploring cognitive processes, linking network dynamics to individual differences and impairments, and advancing research across disciplines.

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [86] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: The paper provides the first study of Knowledge Editing (KE) specifically for Arabic, evaluating different methods on cross-lingual and multilingual settings using Arabic translations of existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in research on Knowledge Editing behaviors in morphologically rich languages like Arabic, which are underexplored compared to English.

Method: The study evaluates four KE methods (ROME, MEMIT, ICE, and LTE) on Arabic-translated benchmarks and extends the LTE method to a multilingual setting with joint Arabic-English training.

Result: Parameter-based methods struggle with cross-lingual generalization, while instruction-tuned methods show more robust performance. Multilingual training improves both KE editability and cross-lingual transfer.

Conclusion: The work highlights the challenges of cross-lingual KE and introduces benchmarks and multilingual datasets to support future research in Arabic Knowledge Editing.

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [87] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: This paper introduces GRPO, a cost-efficient method to improve law citation accuracy and response quality in Thai legal question answering using advanced embeddings and optimizing policies.


<details>
  <summary>Details</summary>
Motivation: Improving the performance of Retrieval-Augmented Generation systems for Thai legal question answering, particularly in tasks requiring complex legal reasoning.

Method: An approach using Group-Relative Policy Optimization (GRPO) combined with BGE-M3 semantic-similarity embeddings to enhance citation accuracy and response quality, while reducing computational costs by 2.5x.

Result: Experiments show GRPO gains up to 90% citation-F1 improvement from the base model and a 31% increase in joint quality metrics over instruction tuning. Enhanced robustness in complex legal reasoning tasks is also demonstrated.

Conclusion: GRPO offers a resource-efficient solution for improving Thai legal LLMs, boosting both citation accuracy and performance in complex reasoning tasks compared to traditional methods.

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [88] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: The abstract introduces MCEval, a novel framework for evaluating the cultural awareness and bias of large language models across 13 languages and cultures, exposing fairness issues and emphasizing the importance of language-culture alignment.


<details>
  <summary>Details</summary>
Motivation: There is a need to address cultural biases and enhance cross-cultural understanding in large language models to better serve a diverse global audience.

Method: The MCEval framework combines dynamic cultural question construction and causal analysis techniques like Counterfactual Rephrasing and Confounder Rephrasing to systematically evaluate cultural awareness and bias in language models.

Result: MCEval evaluates 39,897 instances of cultural awareness and 17,940 instances of cultural bias, identifying disparities in cultural performance based on language-culture alignment and revealing fairness issues in methods focused on English scenarios.

Conclusion: This paper introduces MCEval as a pioneering and comprehensive evaluation framework for analyzing cultural understanding in large language models, thereby contributing to better addressing global user diversity and fairness challenges.

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [89] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: This paper studies the latent geometry of large language models (LLMs), showing that high-level semantic information lies in low-dimensional, linearly separable subspaces.


<details>
  <summary>Details</summary>
Motivation: To better understand how LLMs internally organize semantic representations and improve model alignment for safer outputs.

Method: The authors conduct a large-scale empirical study of hidden states in 11 transformer-based LLMs across different layers, topics, and structured reasoning prompts.

Result: Semantic separability is observed, becomes pronounced in deeper layers, and allows causal interventions to capture reasoning patterns and mitigate adversarial content.

Conclusion: The study highlights the utility of geometry-aware tools for managing latent representations, enabling effective detection and mitigation of harmful content using methods like lightweight MLP classifiers.

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [90] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper introduces a self-adaptive curriculum learning approach for NLP tasks, leveraging pre-trained language models to predict example difficulty, achieving faster convergence and better performance.


<details>
  <summary>Details</summary>
Motivation: Existing curriculum learning methods rely on manually defined difficulty metrics, which may not accurately represent model perspectives.

Method: A self-adaptive curriculum where pre-trained language models predict difficulty scores, and these scores guide fine-tuning strategies such as easy-to-hard, hard-to-easy, or mixed sampling.

Result: The self-adaptive curriculum improves convergence speed and performance in natural language understanding tasks compared to random sampling.

Conclusion: Self-adaptive curriculum learning, informed by difficulty scores from pre-trained models, offers a more efficient and effective training strategy for NLP applications.

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [91] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: The paper redefines clickbait, introduces a methodology for creating a clickbait detection dataset, and releases the first open-source Spanish dataset for this purpose (TA1C).


<details>
  <summary>Details</summary>
Motivation: To address the lack of consensus on the definition of clickbait and refine the criteria for detecting it, particularly focusing on Spanish-language content.

Method: They revise the definition of clickbait, propose a framework to minimize subjectivity in annotations, create a dataset of 3,500 Spanish tweets from 18 media sources, ensure high inter-annotator agreement, and provide strong baseline models for detection.

Result: The resulting dataset (TA1C) achieved a 0.825 Fleiss' K inter-annotator agreement and the detection baseline models reached an F1-score of 0.84.

Conclusion: The paper advances the study of clickbait by providing a clear definition, a rigorous annotation framework, and a high-quality dataset tailored to Spanish-language media.

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [92] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: This paper examines how large language models generalize to unseen tasks, focusing on the off-by-one addition task, using circuit-style interpretability techniques.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms inside language models that enable task-level generalization, particularly in unseen tasks via in-context learning.

Method: The study uses an off-by-one addition task as a case study and employs circuit-style interpretability techniques like path patching to analyze internal computations.

Result: It identifies a mechanism resembling induction heads at a higher abstraction level, shows parallelized contributions from attention heads for the +1 function, and demonstrates that this mechanism generalizes across other tasks.

Conclusion: The findings highlight that reusable and composable structures in language models underpin their ability to generalize to new tasks.

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [93] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: The paper proposes a framework to improve Retrieval-Augmented Generation (RAG) systems by using hierarchical text segmentation and clustering for better chunking strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional chunking methods for Retrieval-Augmented Generation fail to create semantically meaningful chunks as they disregard the textual structure.

Method: The framework integrates hierarchical text segmentation and clustering to produce semantically coherent chunks, and uses both segment-level and cluster-level vector representations during inference for retrieval.

Result: Evaluations on NarrativeQA, QuALITY, and QASPER datasets show improved performance compared to traditional chunking methods.

Conclusion: Integrating hierarchical segmentation and clustering into RAG systems leads to retrieving more contextually relevant and precise information, enhancing their overall effectiveness.

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [94] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: TinyRM introduces small bidirectional masked language models that achieve performance comparable to much larger models on specific tasks, offering cost-efficiency and scalability in reward modeling.


<details>
  <summary>Details</summary>
Motivation: The high inference costs of large decoder-based language models in RLHF pose challenges for deployment during test-time, necessitating more resource-efficient solutions.

Method: TinyRM employs a combination of FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to train small masked language models for reward modeling.

Result: Small models like TinyRM, with only 400 million parameters, showcase competitive performance on RewardBench tasks, rivaling models that are significantly larger, particularly in reasoning tasks.

Conclusion: Lightweight and efficient models, such as TinyRM, are a promising direction for scalable reward modeling, though challenges around generalist and conversational modeling remain.

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [95] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: TextOmics introduces a benchmark aligning omics data with molecular textual descriptions, enabling advancements in molecular generation. ToDi, a generative framework, uses this alignment for creating therapeutic molecules with high precision.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of heterogeneous data and unified frameworks for molecular generation in drug discovery.

Method: The authors built TextOmics as a benchmark for omics-to-text molecular data alignment. They developed ToDi by leveraging dual encoders (OmicsEn and TextEn) and a conditional diffusion model (DiffGen) to generate biologically and chemically relevant molecules.

Result: ToDi, based on TextOmics, demonstrated superior performance in generating hit-like therapeutic molecules compared to current methods, excelling even under zero-shot conditions.

Conclusion: TextOmics and ToDi represent significant advancements, providing a robust foundation for target-specific drug discovery via heterogeneous molecular generation methods.

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [96] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: This paper proposes a framework using Reddit data to dynamically predict suicide risk transitions by jointly learning risk and protective factors' influence.


<details>
  <summary>Details</summary>
Motivation: Current suicide risk prediction models focus only on risk factors and ignore rapid mental state changes and the protective factors that mitigate suicide risk.

Method: The study develops a Protective Factor-Aware Dataset and a Dynamic Factors Influence Learning method to capture the evolving role of both risk and protective factors over 12 years of Reddit posts.

Result: Experiments reveal the model’s superiority over state-of-the-art models and large language models, while yielding interpretable insights into suicide risk patterns.

Conclusion: The framework improves predictive accuracy and provides valuable insights for clinicians, aiding in better understanding and intervention of suicidal behaviors.

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [97] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: The paper introduces GeLaCo, a novel evolutionary method for compressing Large Language Models (LLMs) to reduce computational requirements while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the computational demands of deploying and using LLMs by exploring effective and efficient model compression techniques.

Method: GeLaCo employs evolutionary algorithms coupled with a module-wise similarity fitness function for compressing LLM layers. It supports both single and multi-objective optimization to balance compression and quality.

Result: GeLaCo sets up the first Pareto frontier for LLM compression, ensuring balanced exploration of trade-offs. The method delivers superior results compared to existing state-of-the-art compression methods.

Conclusion: GeLaCo offers an innovative solution for compressing LLMs efficiently, potentially paving the way for more accessible and cost-effective deployment of these models without significant sacrifice in their performance.

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [98] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: This paper introduces CodeJudgeBench, a benchmark for evaluating the performance of Large Language Models as judges in coding tasks. It emphasizes the performance difference between thinking and non-thinking models while identifying reliability issues and optimal prompting strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess and improve the recently popular LLM-as-a-Judge paradigm, which holds potential for benchmarking and improving LLMs in tasks like code generation, repair, and unit test creation. This is crucial as there were previously no dedicated benchmarks for such evaluations.

Method: The authors developed and utilized the CodeJudgeBench benchmark to test and analyze 26 LLM models designed for judging coding tasks. They compared thinking and non-thinking models, analyzed scenarios like order sensitivity, and tested various prompting strategies.

Result: The research showed that thinking models outperformed non-thinking ones, even smaller thinking models surpassing larger non-thinking models. However, all models faced issues with judgment consistency, including sensitivity to input order and variance in cross-LLM comparisons.

Conclusion: While some LLMs show promise as judges in coding tasks, issues like random variability and order sensitivity raise concerns about their reliability. Optimal prompting strategies, like pairwise comparison with reasoning included, offer potential for improvement but don't fully resolve these challenges.

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [99] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: Large Language Models fail to capture diverse cultural moral frameworks and homogenize moral diversity, challenging their use in social sciences.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can represent diverse human values and moral intuitions effectively.

Method: Applied the Moral Foundations Questionnaire across 19 cultural contexts to assess LLMs against human baseline data.

Result: LLMs systematically homogenize moral diversity, with increased model size not guaranteeing improved cultural representation.

Conclusion: Current AI alignment approaches fail to capture nuanced cultural moral intuitions, calling for more focused alignment and evaluation techniques.

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [100] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: This study introduces Critical Representation Fine-Tuning (CRFT), an advanced method improving reasoning tasks by targeting key information-carrying representations, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance reasoning task performance by focusing on critical representations that significantly influence output, addressing limitations of traditional PEFT methods like ReFT.

Method: CRFT identifies critical representations via information flow analysis and fine-tunes them in a low-rank subspace, operating within a supervised learning framework while keeping the base model frozen.

Result: Tests on eight reasoning benchmarks using LLaMA and Mistral models demonstrate CRFT's effectiveness. It achieves improved performance across arithmetic and commonsense reasoning, particularly excelling in few-shot learning scenarios by boosting one-shot accuracy by 16.4%.

Conclusion: Representation-level optimization offers a promising, lightweight method for improving reasoning models, providing superior results compared to traditional PEFT without requiring extensive model changes.

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [101] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: The paper proposes a novel architecture combining LLMs and vanilla Transformers for time series forecasting, achieving more accurate predictions by fusing temporal and semantic representations.


<details>
  <summary>Details</summary>
Motivation: Although LLMs show strong reasoning capabilities with discrete tokens, they struggle with continuous numerical time series data, and vanilla Transformers, while better with time series data, lack the ability to learn high-level semantic patterns.

Method: The authors design a Transformer-based hybrid model that combines high-level semantic knowledge learned by LLMs with temporal information captured by vanilla Transformers for accurate time series forecasting.

Result: The hybrid model outperforms both standalone LLMs (used via fine-tuning or prompting) and vanilla Transformers on benchmark time series forecasting datasets.

Conclusion: Integrating LLMs' semantic reasoning capabilities with the temporal strengths of Transformers can bridge the gap between discrete token reasoning and continuous numerical prediction, enabling more accurate time series forecasts.

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [102] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: The paper introduces a new feature knowledge distillation method allowing transfer between teacher and student language models with differing hidden sizes, without extra parameters.


<details>
  <summary>Details</summary>
Motivation: Reducing computational demand of large language models using efficient knowledge distillation methods.

Method: Identifying task-relevant hidden units in teacher models and distilling their activations directly to the student model without new parameters.

Result: The proposed approach outperforms existing methods across diverse tasks with up to 3% improvement over linear projection baselines.

Conclusion: This flexible method improves distillation performance and avoids additional parameter introduction while enabling architecture flexibility.

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [103] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: This study examines the ability of different LLMs, such as Gemini, GPT-4o, DeepSeek, and Groq, to rewrite abusive text into non-abusive text while maintaining original intent, sentiment, and semantics.


<details>
  <summary>Details</summary>
Motivation: To explore whether current LLMs can effectively transform abusive text into non-abusive text while retaining its intent, addressing the gap in this application area.

Method: Four state-of-the-art LLMs were evaluated for their ability to detect and transform abusive texts. The transformations were further analyzed with sentiment and semantic analysis to assess quality.

Result: Groq delivered results significantly different from other models, while similarities were observed between GPT-4o and DeepSeek in the transformed outputs.

Conclusion: While some LLMs like Groq deviate noticeably, models like GPT-4o and DeepSeek demonstrated closer alignment in handling abusive text rewriting tasks.

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [104] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: This paper introduces Absher, a benchmark for evaluating LLMs on Saudi dialects and cultural nuances, revealing notable gaps in cultural inference and contextual understanding.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for assessing LLMs’ ability to understand regional dialects and cultural nuances, crucial for linguistically diverse settings like Saudi Arabia.

Method: The Absher benchmark, with over 18,000 multiple-choice questions across six categories, evaluates LLMs on dialectal and cultural tasks using curated data from Saudi regional dialects.

Result: Results show significant performance gaps in tasks requiring cultural inference and contextual understanding, highlighting areas for improvement in state-of-the-art LLMs.

Conclusion: There’s an urgent need for dialect-aware training and culturally aligned evaluation methods to improve LLMs' performance in Arabic real-world applications.

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [105] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: This paper introduces an evolutionary approach for automated prompt optimization that significantly improves performance on smaller language models across complex tasks, outperforming state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Prompt engineering is critical for maximizing the efficiency of LLMs, especially smaller models dealing with complex tasks. Most existing solutions struggle with prompt optimization in these scenarios.

Method: The authors propose a two-phase evolutionary search approach, utilizing grammar-guided genetic programming for initial synthesis and local search for further fine-tuning of prompt designs.

Result: The proposed method outperformed PromptWizard, OPRO, and RL-Prompt across three smaller LLMs and four challenging tasks, showing robust results with minimal performance degradation.

Conclusion: The evolutionary search approach provides a reliable solution for optimizing prompts for smaller and more sensitive models, making it suitable for detailed and complex task requirements.

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [106] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: The paper introduces Growth Bound Matrices (GBM) as a novel regularization method to improve adversarial robustness in NLP models, showing up to 8.8% robustness improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adversarial robustness in NLP for architectures like LSTMs and state space models (e.g., S4), which have been largely understudied compared to feed-forward and convolutional models.

Method: A regularization method based on Growth Bound Matrices (GBM), applied to LSTMs, S4 models, and CNNs, that reduces the impact of input perturbations and is tested on multiple datasets.

Result: Improves adversarial robustness by up to 8.8% over existing baselines across architectures like LSTMs, S4 models, and CNNs.

Conclusion: The proposed GBM method enhances NLP robustness effectively, outperforms state-of-the-art adversarial defenses, and provides new insights into the robustness of state space models like S4.

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [107] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: The paper evaluates the use of large language models (LLMs) as analytical tools in linguistics by conducting psycholinguistic studies, finding significant convergence between human and LLM responses.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of LLMs, specifically in their ability to replicate nuanced human judgments in linguistic research.

Method: Conducted four psycholinguistic studies involving humans and then replicated the same tasks with an LLM, followed by statistical comparison of responses.

Result: Strong correlations (Spearman's rho = .73-.96) were found between human and LLM responses across the studies, showing convergence with minor discrepancies.

Conclusion: LLMs can effectively augment human-based linguistic research, supporting both validation of existing findings and new hypothesis generation.

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [108] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: The paper introduces a multi-layer model for computational metaphor processing, integrating content analysis, conceptual blending, and pragmatic intentionality.


<details>
  <summary>Details</summary>
Motivation: Current approaches to metaphor processing fail to capture the complex and layered nature of metaphorical meaning.

Method: The authors develop a stratified three-layer model involving annotation of conceptual elements, modeling conceptual combinations, and introducing a pragmatic vocabulary.

Result: The model enables richer computational interpretation of metaphor, incorporating surface, emergent, and context-sensitive dimensions.

Conclusion: This approach provides a unified framework for understanding metaphor that aligns with cognitive and pragmatic theories, advancing computational metaphor reasoning.

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [109] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: This paper explores how large language models (LLMs), especially decoder-only Transformers, can understand and utilize underlying graph structures for graph reasoning tasks, introducing methods like Induced Substructure Filtration.


<details>
  <summary>Details</summary>
Motivation: To investigate how decoder-only Transformer architectures process and understand graph structures embedded in textual descriptions and solve graph-related tasks.

Method: The authors focus on substructure extraction tasks using the Induced Substructure Filtration (ISF) method, combining theoretical analysis and empirical testing to capture internal dynamics of multi-layer transformers.

Result: The study reveals consistent internal dynamics across layers for substructure identification, and shows that decoder-only Transformers can handle diverse graph types, efficiently extracting patterns from attributed graphs like molecular graphs.

Conclusion: Sequence-based Transformers are capable of performing substructure extraction tasks on graph data, offering new insights into their potential for graph reasoning and related tasks.

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [110] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: This study examines LLMs' abilities to ask clarification questions in task-oriented dialogues and compares them against humans. It presents a new dataset combining two annotations of the Minecraft Dialogue Corpus.


<details>
  <summary>Details</summary>
Motivation: To analyze how LLMs handle ambiguity in task-oriented dialogues by comparing them with human behavior, especially regarding clarification questions.

Method: The authors combined two annotated datasets of the Minecraft Dialogue Corpus to explore ambiguity and clarifications. They also compared human and LLM-generated clarification questions and assessed the role of reasoning approaches in LLM performance.

Result: Humans rarely ask clarification questions for referential ambiguity but do for task-based uncertainty. LLMs ask more questions for referential ambiguity and fewer for task uncertainty. Reasoning improves question frequency and relevancy for LLMs.

Conclusion: The study highlights differences in how LLMs and humans handle ambiguity through clarification questions, emphasizing the role of reasoning in improving LLM performance.

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [111] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: The paper benchmarks classical encoder models and new-generation large language models (LLMs) for hate speech detection.


<details>
  <summary>Details</summary>
Motivation: Online platforms face a challenge of moderating hate speech without suppressing legitimate discussions, necessitating accurate detection methods.

Method: The authors compare classic bidirectional transformer encoder models and newer ultra-large autoregressive LLMs on curated datasets to measure their hate speech detection accuracy.

Result: The study identifies whether larger language models improve detection of hate speech in real-world online texts compared to traditional models.

Conclusion: The paper concludes on the effectiveness of the advanced LLMs relative to classical encoders in practical hate speech identification.

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [112] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: The paper introduces MLAR, an Applicant Tracking System integrating RPA and LLMs to efficiently process resumes and match candidates.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in traditional recruitment processes, particularly in resume screening and candidate shortlisting.

Method: MLAR uses a three-layer framework: LLMs extract job-posting characteristics, parse applicant resumes, and perform similarity matching with semantic algorithms.

Result: MLAR reduced resume-processing time by approximately 17%, outperforming leading RPA platforms like UiPath and Automation Anywhere.

Conclusion: MLAR offers a scalable and efficient recruitment automation solution, significantly improving processing speed and accuracy.

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [113] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: This paper examines the ability to detect AI-generated text by comparing diffusion-based models (LLaDA) and autoregressive models (LLaMA), showing that diffusion models mimic human text well, posing challenges for current detection methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the increasing difficulty in detecting AI-generated text, especially from newer diffusion-based models, and the limitations of current detection metrics.

Method: The authors systematically compared 2,000 text samples from diffusion-based (LLaDA) and autoregressive (LLaMA) models using various textual metrics like perplexity, burstiness, lexical diversity, and BLEU/ROUGE scores.

Result: LLaDA mimics human text closely in perplexity and burstiness, leading to high false-negative rates for detection. LLaMA exhibits lower perplexity but with less lexical fidelity. Single metrics fail to reliably differentiate diffusion-generated text from human writing.

Conclusion: There is a critical need for diffusion-aware detection methods, hybrid models, and techniques such as diffusion-specific stylometric signatures and robust watermarking to better distinguish AI-generated text from human writing.

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [114] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: This paper presents Mixture-of-Recursions (MoR), a framework designed to improve efficiency in language models by combining parameter sharing and adaptive computation.


<details>
  <summary>Details</summary>
Motivation: Scaling language models improves performance but demands significant computational and memory resources, making training and deployment expensive.

Method: The authors propose MoR, a Recursive Transformer that shares parameters across recursion steps and uses adaptive token-level computation. Lightweight routers dynamically assign recursion depths, focusing quadratic attention on active tokens. A KV sharing variant further optimizes memory use.

Result: MoR achieves lower validation perplexity, improved few-shot accuracy, and higher throughput across model scales (135M–1.7B params), establishing itself as a new efficiency-performance frontier.

Conclusion: MoR effectively delivers large-model quality at reduced computational cost, making it a promising solution for scaling language models efficiently.

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [115] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: The paper introduces REST, a new evaluation framework for Large Reasoning Models (LRMs) that tests their reasoning under simultaneous multi-problem scenarios, revealing significant performance issues in even state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LRMs are limited to isolated question-solving and fail to test capabilities needed for real-world application, such as handling multiple contexts simultaneously.

Method: The authors propose REST, a stress-testing framework that evaluates LRMs by exposing them to multiple problems concurrently, focusing on aspects like contextual priority allocation and cognitive load management.

Result: REST highlights significant performance degradation in state-of-the-art models during stress testing and detects performance differences that are invisible in traditional single-question benchmarks.

Conclusion: REST is a more robust and efficient evaluation framework that better reflects real-world reasoning challenges, reducing the need for constant new data creation.

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [116] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: This paper addresses the sensitivity of Vision-Language Navigation in Continuous Environments (VLNCE) to viewpoint changes, introducing a method called View Invariant Learning (VIL) to enhance robustness.


<details>
  <summary>Details</summary>
Motivation: To improve the ability of AI navigation agents to handle changes in camera viewpoint (height and angle) during Vision-Language Navigation tasks.

Method: The paper introduces VIL, a post-training strategy using contrastive learning for view-invariant features and a teacher-student framework for enhancing view-invariance in the Waypoint Predictor Module.

Result: The method outperforms state-of-the-art approaches on Success Rate by 8-15% in the V2-VLNCE setting and achieves state-of-the-art results in the standard VLNCE setting on challenging datasets like RxR-CE.

Conclusion: VIL enhances robustness to viewpoint variation without reducing performance in standard scenarios, making it a versatile and effective post-training approach.

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [117] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: The paper proposes a novel ML technique to detect deepfake video impersonations using unnatural facial biometrics.


<details>
  <summary>Details</summary>
Motivation: Deepfake videos, combining realistic voice cloning and visual tricks, pose threats like fraud, scams, and political disinformation.

Method: The method involves machine learning that detects deepfakes by spotting unnatural patterns in facial biometrics. Its performance is tested against a large dataset, video laundering, and unseen deepfake models.

Result: The forensic ML technique effectively detects deepfake impersonations and demonstrates generalization across various datasets and unseen generators.

Conclusion: This technique could serve as a robust countermeasure against malicious usage of deepfake technology.

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [118] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM is a data-free and task-agnostic solution for reducing bias in vision-language models like CLIP. It involves generating spurious correlations using an LLM and learning a projection to reduce these biases.


<details>
  <summary>Details</summary>
Motivation: VLMs inherit and amplify biases from their training data, leading to skewed predictions. This paper aims to mitigate these biases without relying on predefined bias categories or external data.

Method: PRISM employs two stages: (1) using LLMs for generating biased scene descriptions and (2) applying a novel contrastive-style debiasing loss to learn a projection that reduces spurious correlations while maintaining image-text alignment.

Result: Experimental results reveal that PRISM surpasses existing debiasing methods on datasets such as Waterbirds and CelebA.

Conclusion: PRISM effectively debiases VLMs in a data-free and task-agnostic manner, showing promise for achieving fairer predictions.

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [119] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: The paper introduces HMR-ViT, a method for Human Mesh Recovery using both temporal and kinematic information through a Vision Transformer.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Human Mesh Recovery from images lack integration of both temporal and kinematic information, limiting accuracy.

Method: HMR-ViT constructs a Temporal-kinematic Feature Image using feature vectors from video frames. A Channel Rearranging Matrix (CRM) rearranges kinematic features spatially, which are then encoded via a Vision Transformer to infer pose/shape parameters.

Result: The method showed competitive performance on the 3DPW and Human3.6M datasets.

Conclusion: Using both temporal and kinematic information improves HMR performance, demonstrating the effectiveness of the HMR-ViT approach.

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [120] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: The paper combines Neural Radiance Fields (NeRF) and Material Point Method (MPM) simulations to infer granular material properties, particularly the friction angle, from visual data, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: The need to characterize granular materials like sand in scenarios where direct measurement is impractical, such as real-world environments.

Method: Synthetic experiments simulate a plow interacting with sand, generating visual observations. NeRF reconstructs 3D geometry from these observations, and MPM simulations with Bayesian optimization estimate material properties by minimizing image loss.

Result: The approach successfully estimates the friction angle of granular materials with an error margin within 2 degrees.

Conclusion: The method demonstrates the potential of inverse analysis through visual observations for non-invasive material property estimation, paving the way for real-world applications.

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [121] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: This paper introduces VISTA, a visual analytics framework aimed at enhancing the quality of dataset labels generated by multi-modal foundation models for open-vocabulary image segmentation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges posed by the poor quality of labels generated by multi-modal foundation models, as existing research focuses on quantity over quality and struggles with effectively validating large-scale data without ground truth.

Method: The proposed VISTA framework integrates multi-phased data validation strategies with human expertise, allowing for the identification, understanding, and fixing of errors in FM-generated labels.

Result: Empirical studies on benchmark datasets and expert reviews showcase the quantitative and qualitative improvements facilitated by VISTA in dataset labeling, leading to better-performing multi-modal models.

Conclusion: VISTA successfully bridges the gap in validating and improving data quality for multi-modal models, providing a robust tool for refining FM-generated labels and improving downstream model performance.

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [122] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite is a Python toolkit designed to simplify brain lesion image analysis through modular pipelines and adaptable preprocessing.


<details>
  <summary>Details</summary>
Motivation: To streamline the creation of complex workflows for clinical and scientific applications in brain lesion image analysis.

Method: It incorporates preprocessing modules (e.g., co-registration, atlas registration, skull-stripping) and tools leveraging BraTS algorithms for synthesis, inpainting, and tumor segmentation, along with performance quantification metrics.

Result: BrainLesion Suite facilitates versatile image analysis for brain lesions, with functionalities extendable to other biomedical applications.

Conclusion: The toolkit minimizes cognitive effort and offers accessible resources for efficient and modular biomedical image analysis pipelines.

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [123] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: The paper addresses the challenge of class imbalance in training data for class-conditional image synthesis using diffusion models, proposing innovative contrastive loss functions.


<details>
  <summary>Details</summary>
Motivation: Class-conditional image generation often suffers from a lack of diversity for tail classes due to imbalanced datasets, leading to mode collapse and reduced tail class image quality.

Method: Introduces two contrastive loss functions: an unsupervised InfoNCE loss to increase dissimilarity among synthetic images, and an MSE loss to enrich tail class diversity by aligning conditional generation with unconditional outputs during initial denoising steps.

Result: The proposed framework improves the diversity of tail class images without compromising head class fidelity across datasets like CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.

Conclusion: This easy-to-implement contrastive learning approach significantly enhances long-tailed class-conditional diffusion models and adapts conditional-unconditional alignment effectively for diffusion models.

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [124] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: This paper utilizes a vision transformer-based deep learning framework integrating multi-shell diffusion MRI (dMRI) data to diagnose Alzheimer's disease and identify amyloid accumulation effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation of this research was to enable early detection of Alzheimer's disease and amyloid accumulation, leveraging the potential of microstructural neuroimaging data combined with advanced deep learning techniques, especially in data-limited scenarios.

Method: The study employed the Swin Transformer on multi-shell dMRI data, extracted critical diffusion metrics, and projected them into 2D representations, allowing the use of ImageNet transfer learning. Additionally, Low-Rank Adaptation was used to optimize performance on neuroimaging data with limited labeling.

Result: The proposed framework demonstrated a balanced accuracy of 95.2% for classifying cognitively normal individuals vs. Alzheimer's patients, 77.2% for amyloid detection in mixed groups, and 67.9% for amyloid detection among cognitively normal individuals. Grad-CAM identified clinically relevant brain areas as explanatory contributors to predictions.

Conclusion: Diffusion MRI combined with transformer-based models holds significant potential for enhancing early Alzheimer's disease and amyloid pathology detection, underscoring its value in limited biomedical data environments.

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [125] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: The paper explores the ambitious goal of Infinite Video Understanding (IVU), emphasizing the importance of continuous, seamless video comprehension over infinite durations.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current state-of-the-art video understanding technologies that struggle with computational, memory, and reasoning challenges for long-duration video content.

Method: This is a position paper that outlines core challenges and key research directions, inspired by advancements in ultra-long video models, streaming architectures, and persistent memory systems.

Result: No experimental results are presented as this is a position paper; it highlights research gaps and frameworks for tackling infinite video understanding.

Conclusion: Framing IVU as a long-term research objective could stimulate innovation across AI domains, paving the way for new techniques in video comprehension and reasoning.

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [126] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: The study introduces BlindSight, a method to optimize Vision-Language Models (VLMs) by leveraging sparse attention patterns, resulting in significant computation reduction while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address inefficiencies caused by the extended prompt length and the high computational complexity of attention in Vision-Language Models (VLMs).

Method: The authors analyzed attention patterns in VLMs to identify sparsity in cross-image attention and proposed a training-free attention sparsity mask, BlindSight, based on categorized sparse patterns.

Result: BlindSight achieved a 32%-41% reduction in FLOPs on average with an accuracy variation of -2% to +2% compared to the original models in benchmarks.

Conclusion: BlindSight effectively reduces computational requirements for VLMs without significantly impacting accuracy, making it suitable for optimizing multi-image understanding tasks.

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [127] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: This paper reviews advancements in remote sensing inversion techniques, from traditional physical models to modern machine learning and foundation models, identifying strengths and ongoing challenges.


<details>
  <summary>Details</summary>
Motivation: Remote sensing inversion is critical for estimating surface variables to support applications in ecosystem monitoring, carbon accounting, and land management.

Method: The authors systematically review methodological evolutions from physical models, through machine learning, to foundation models, discussing pretraining, multi-modal integration, and cross-task adaptation.

Result: They compare modeling paradigms and analyze their assumptions, scenarios, and limitations while identifying challenges like physical interpretability and uncertainty quantification.

Conclusion: The paper outlines a vision for next-gen foundation models with emphasis on unified modeling, cross-domain generalization, and improved interpretability in remote sensing inversion.

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [128] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: This paper explores extracting optical flow using frozen generative video models prompted by perturbations, achieving state-of-the-art results without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The scarcity of flow labels and challenges with synthetic datasets motivate exploring zero-shot methods using generative models instead of fine-tuning.

Method: The method, KL-tracing, introduces perturbations into the first frame and tracks their propagation using generative video models with specific properties, leveraging the LRAS architecture.

Result: KL-tracing outperforms state-of-the-art models in real-world and synthetic datasets, showing significant improvements in endpoint error.

Conclusion: Counterfactual prompting of generative video models offers a scalable, effective route for extracting optical flow, avoiding supervised or photometric-loss-dependent techniques.

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [129] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: The paper presents MI CAM, a novel machine vision technique using activation mapping and mutual information for explainable AI, showing superior qualitative and quantitative results.


<details>
  <summary>Details</summary>
Motivation: Machine vision in critical areas like healthcare and automated systems requires better interpretability of convolutional neural networks (CNNs) to understand their decision-making processes.

Method: The proposed MI CAM method uses activation mapping, weighting each feature map based on mutual information with the input image and combines them linearly to generate saliency visualizations, validated through counterfactual analysis.

Result: MI CAM performs on par or better than state-of-the-art methods, particularly excelling in qualitative and quantitative evaluations.

Conclusion: MI CAM enhances visual explanation capabilities of CNNs, offering unbiased and interpretable model inferencing, with publicly available implementation for reproducibility.

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [130] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: RadEyeVideo integrates radiologists' eye-fixation data as video sequences to enhance chest X-ray (CXR) analysis, improving report generation and disease diagnosis using vision-language models.


<details>
  <summary>Details</summary>
Motivation: The study aims to leverage radiologists' sequential eye-gaze data to improve interaction and insights in chest X-ray analysis, addressing the limitations of current methods that fail to capture the order of eye movements.

Method: RadEyeVideo incorporates radiologists' eye-fixation data as a video input, capturing spatial and temporal dynamics, and evaluates its impact using open-domain LVLMs alongside scaled metrics.

Result: RadEyeVideo improves report generation performance by up to 24.6% and enhances diagnostic capabilities by an average of 15.2%, outperforming specialized medical LVLMs.

Conclusion: Integrating domain expertise like eye-gaze information into LVLMs significantly boosts performance in clinical tasks, showcasing a scalable approach for advancing medical image analysis.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [131] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: This paper introduces PointSD, a framework that leverages text-to-image diffusion models like Stable Diffusion for 3D self-supervised learning, enhancing point cloud representations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the limitations of existing 3D diffusion-based models that are constrained by small 3D datasets.

Method: The PointSD framework replaces Stable Diffusion's text encoder with a 3D encoder to train a point-to-image diffusion model. This allows point clouds to guide image denoising and align 3D features with Stable Diffusion features for semantic learning.

Result: Experiments and ablation studies show improved performance on point cloud tasks using the PointSD framework.

Conclusion: Stable Diffusion, trained on large-scale datasets, effectively enhances 3D self-supervised learning, supporting robust point cloud representations with the proposed framework PointSD.

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [132] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: Sign Language Production (SLP) models often struggle with error accumulation and real-time efficiency. This paper introduces a hybrid model using both autoregressive and diffusion approaches, coupled with novel techniques to refine pose generation.


<details>
  <summary>Details</summary>
Motivation: Traditional SLP models face challenges such as error accumulation during inference and inefficiencies in real-time tasks. Recent diffusion-based methods improve generation quality but are computationally expensive.

Method: The authors propose a hybrid model combining autoregressive and diffusion techniques. They introduce a Multi-Scale Pose Representation module for extracting detailed features and a Confidence-Aware Causal Attention mechanism to guide pose generation using joint confidence scores.

Result: Experiments on PHOENIX14T and How2Sign datasets confirm that their method improves both the quality of generation and real-time streaming efficiency compared to existing approaches.

Conclusion: The proposed hybrid model effectively balances quality and efficiency in SLP tasks, addressing key limitations of prior methods.

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [133] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces RoHOI, the first benchmark for robust Human-Object Interaction detection under real-world challenges like environmental variability and occlusion. It proposes a new strategy, SAMPL, to improve model robustness, showing significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Human-Object Interaction detection is essential for enabling context-aware robot-human assistance. However, current models perform poorly under real-world corruptions like occlusion and noise.

Method: The authors developed RoHOI, a robustness benchmark with 20 corruption types based on HICO-DET and V-COCO datasets, and introduced SAMPL—a Semantic-Aware Masking-based Progressive Learning strategy—to enhance robust feature learning dynamically.

Result: Systematic analyses showed that existing models suffered performance drops under corruptions, while the proposed SAMPL strategy outperformed state-of-the-art methods in extensive experiments.

Conclusion: The paper sets a new standard in robust HOI detection, offering benchmarks, datasets, and strategies for improved real-world application resilience, with all resources made publicly available.

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [134] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: This paper proposes a method, MG-CLIP, to improve the continual learning performance of CLIP by leveraging and preserving the modality gap.


<details>
  <summary>Details</summary>
Motivation: There is a need to enhance the continual learning capabilities of vision-language pre-trained models (like CLIP) while retaining their generalization and adaptability, particularly by addressing the overlooked modality gap.

Method: The authors analyze modality gap variations during fine-tuning and propose MG-CLIP, which uses two strategies: modality gap preservation to reduce forgetting and modality gap compensation to enhance adaptability to new data.

Result: MG-CLIP outperforms existing methods on several benchmarks without using additional replay data, as demonstrated through extensive experiments.

Conclusion: The study highlights the importance of the modality gap in preserving pre-trained knowledge and introduces a new perspective for continual learning, making MG-CLIP an effective approach in vision-language domain.

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [135] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: The paper introduces SnapMoGen, a high-quality text-motion dataset with improved generative modeling using MoMask++.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing text-to-motion generation approaches constrained by short or general text prompts.

Method: SnapMoGen provides a detailed motion dataset paired with expressive annotations, along with MoMask++, a generative masked transformer for better token utilization.

Result: SnapMoGen dataset includes 20K motion clips and 122K detailed descriptions. MoMask++ achieves state-of-the-art performance on benchmark evaluations.

Conclusion: SnapMoGen and MoMask++ enhance fine-grained controllability and generalization in text-to-motion generation, paving the way for advancements in this domain.

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [136] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM is a novel framework for human pose estimation using a nonlinear MLP connector, outperforming prior methods like LocLLM.


<details>
  <summary>Details</summary>
Motivation: Traditional pose estimation methods struggle to generalize to new poses or unseen keypoints, motivating the need for models with better generalization capabilities.

Method: PoseLLM replaces LocLLM's linear projector with a two-layer MLP vision-language connector with GELU activation to enhance visual-textual feature fusion.

Result: PoseLLM achieves an AP of 77.8 on the COCO validation set and demonstrates strong zero-shot generalization to other datasets like Human-Art and MPII.

Conclusion: A simple yet effective nonlinear MLP connector significantly boosts pose localization accuracy while retaining robust generalization.

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [137] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: This paper introduces $I^{2}$-World, an efficient framework for 4D occupancy forecasting in 3D scenes, aiming to improve forecasting and scenario generation for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: To address challenges in optimizing 3D scene tokenization for forecasting complex scenarios in autonomous driving, which is critical for handling edge cases.

Method: The framework includes intra-scene and inter-scene tokenizers to compress and aggregate 3D scenes hierarchically and dynamically, paired with an encoder-decoder architecture to facilitate scene understanding and prediction.

Result: $I^{2}$-World achieves state-of-the-art performance in 4D occupancy forecasting, improving mIoU by 25.1% and IoU by 36.9%, with high computational efficiency requiring only 2.9 GB memory and 37 FPS real-time inference.

Conclusion: $I^{2}$-World provides an efficient and high-performing solution for 4D occupancy forecasting, showcasing advancements in tokenization and scene modeling for autonomous driving applications.

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [138] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: This paper introduces Stable Score Distillation (SSD), a streamlined framework enhancing text-guided image and 3D editing, addressing stability, spatial control, and editing precision issues in prior methods.


<details>
  <summary>Details</summary>
Motivation: Prior methods in text-guided editing, such as Delta Denoising Score, struggle with stability, precise edits, and spatial control due to reliance on complex auxiliary structures.

Method: SSD uses Classifier-Free Guidance (CFG) for alignment across prompts, introduces a constant-term null-text branch for optimization stability, and a prompt enhancement branch for stronger edit capability.

Result: The framework exhibits state-of-the-art performance in 2D and 3D editing applications, such as NeRF and text-driven style transformations, achieving faster convergence and lower complexity.

Conclusion: Stable Score Distillation (SSD) offers a robust, efficient solution for text-guided editing, enabling precise, coherent, and localized modifications across 2D and 3D tasks.

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [139] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: The paper proposes a vision transformer-based backbone to fuse RGB and depth data, paired with contrastive unsupervised learning and curriculum learning for sim2real tasks.


<details>
  <summary>Details</summary>
Motivation: Depth data provides robust spatial insights and complements RGB data for generalization, urging exploration of fusion techniques in vision tasks.

Method: Separate CNN stems process RGB and depth data, fused for a scalable vision transformer. Contrastive learning with masked/unmasked tokens boosts reinforcement learning efficiency, paired with curriculum learning for domain randomization.

Result: Enhanced sample efficiency and sim2real transfer achieved through fusion techniques, contrastive learning, and flexible curriculum scheduling.

Conclusion: The integration of RGB-depth fusion and learning innovations advances performance in reinforcement learning and sim2real transfer scenarios.

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [140] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: This paper addresses Few-Shot Class-Incremental Learning (FSCIL) challenges by introducing LGSP-Prompt, a spatial prompting method, avoiding token-dimension saturation and improving knowledge retention as well as incremental learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the dual challenges of data scarcity and incremental learning in FSCIL settings, while addressing the limitations of prompt pool methods in such environments.

Method: The method involves shifting prompt learning from token dimensions to spatial dimensions via LGSP-Prompt, generating dynamic spatial prompts by combining local spatial features and global frequency-domain representations.

Result: Extensive experiments showed that LGSP-Prompt achieved state-of-the-art performance across multiple FSCIL benchmarks, excelling in both preserving base knowledge and facilitating incremental learning.

Conclusion: LGSP-Prompt mitigates token-dimension saturation and provides an effective solution to improve FSCIL tasks, enabling better handling of minimal data and session increments.

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [141] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: The paper investigates and addresses the issue of hallucinations in Large Vision Language Models (LVLMs) caused by alignment bias originating from long-term decay in Rotary Position Encoding (RoPE).


<details>
  <summary>Details</summary>
Motivation: The motivation for the research is to tackle the problem of hallucinations in LVLMs caused by misaligned multimodal features, particularly focusing on long-term decay in positional modeling methods.

Method: The authors introduce MCA-LLaVA, a method based on Manhattan distance that modifies positional modeling by extending long-term decay into a two-dimensional, multi-directional spatial decay, improving the model's perception of image tokens across spatial locations.

Result: MCA-LLaVA demonstrates significant improvements in mitigating hallucinations and achieves strong performance across various benchmarks related to hallucination and general evaluation of LVLMs.

Conclusion: The proposed MCA-LLaVA effectively reduces image alignment bias, enhancing multimodal alignment and the LVLMs' robustness against hallucinations. The approach is verified to be effective and generalizable.

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [142] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: The paper introduces THYME, a novel approach to improve dynamic scene graph generation with a new aerial dataset and achieves superior performance over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of video-based applications has created a need for effective methods to understand dynamic scenes, which previous methods fail to address comprehensively.

Method: THYME combines hierarchical feature aggregation and cyclic temporal refinement to simultaneously capture spatial details and temporal dependencies.

Result: THYME surpasses state-of-the-art methods in experiments conducted on ASPIRe and the newly introduced AeroEye-v1.0 dataset, demonstrating improved dynamic scene understanding.

Conclusion: THYME presents a robust solution for dynamic scene graph generation, underpinned by the AeroEye-v1.0 dataset, offering better spatial and temporal coherence than previous approaches.

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [143] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: This paper presents a method to infer a material's thickness and stiffness using video-based wave propagation analysis.


<details>
  <summary>Details</summary>
Motivation: To enable non-invasive, video-based estimation of physical properties such as thickness and stiffness of materials or tissues for potential applications in health monitoring and other fields.

Method: The method extracts wave dispersion relations from videos and solves a physics-based optimization problem to infer thickness and stiffness parameters.

Result: Validated on both simulated and real data, the method shows strong agreement with actual measurements, demonstrating its efficacy.

Conclusion: The proposed technique is a promising step toward non-invasive health monitoring and has potential applications in human-computer interaction as well.

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [144] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: This paper introduces Expert-CFG, a framework to improve medical vision-language models (MedVLMs) without additional training, by leveraging expert guidance and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing inaccuracies and lack of clinical alignment in current medical vision-language models, which are critical limitations for medical applications.

Method: The proposed method involves an Expert-Controlled Classifier-Free Guidance (Expert-CFG) framework that uses uncertainty estimation, reference retrieval, and expert-highlighted key terms to refine model outputs, ensuring alignment with clinical expertise.

Result: Expert-CFG, with fewer parameters and limited expert annotations, outperforms larger state-of-the-art models across three medical visual question answering benchmarks.

Conclusion: The study concludes that Expert-CFG is effective and resource-efficient, making it a viable solution for medical use, especially in resource-constrained settings.

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [145] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: The paper addresses the limitations in 3D detection models for rare anomaly categories in autonomous driving systems, proposing a new algorithm and dataset to enhance model generalization and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization of 3D detection models in autonomous driving, especially for rare anomalies, which are challenging for models trained on closed datasets.

Method: Introduced the S3AD algorithm that decouples 2D and 3D training, facilitating arbitrary 3D foreground detection and anomaly scoring based on confidence prediction. Additionally, created KITTI-AR and its subsets with augmented reality for richer datasets.

Result: Experimental validation showed improved performance in anomaly detection when using the S3AD algorithm and KITTI-AR datasets in zero-shot scenarios.

Conclusion: The proposed approach effectively enhances 3D detection generalization and anomaly detection, presenting a scalable solution for real-world autonomous driving systems.

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [146] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: The paper proposes a spherical sampling method for panoramic images to improve performance by adapting 2D pre-trained models and achieving better segmentation results on datasets like Stanford2D3D.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of distortions and discontinuities in panoramic images that negatively impact tasks relying on 2D pre-trained models.

Method: The method introduces spherical discrete sampling aligned with weights of pre-trained 2D models, reducing distortions and enhancing compatibility for panoramic tasks.

Result: The approach achieves commendable results in panoramic image segmentation, specifically tested on the Stanford2D3D dataset, showing improved performance.

Conclusion: The spherical sampling method successfully adapts 2D pre-trained models for panoramic images, advancing their application in image segmentation tasks.

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [147] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: The paper addresses online long-term point tracking, presenting 'Track-On,' a transformer-based model for real-time video applications.


<details>
  <summary>Details</summary>
Motivation: Current long-term point tracking methods typically rely on offline settings, which are unsuitable for real-world scenarios requiring causal, moment-by-moment predictions.

Method: The method involves introducing Track-On, a transformer-based model that processes tracked points as queries and sequentially handles video frames, leveraging memory for coherence without future data access.

Result: Track-On achieves state-of-the-art performance across seven public benchmarks, demonstrating effective online long-term tracking capabilities.

Conclusion: Visual foundation models enhance spatial features but need integration into dedicated designs for online tracking. Track-On effectively bridges this gap, enabling robust real-time point tracking.

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [148] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: The paper introduces StaRFM, a unified framework to address distribution shifts and confidence misalignment in foundation model applications like CLIP and SAM, offering significant performance improvements in vision and medical domains.


<details>
  <summary>Details</summary>
Motivation: Foundation models have revolutionized vision and medical imaging, but challenges such as distribution shift and confidence misalignment hinder their deployment. Existing solutions are domain-specific and insufficient for broad applicability.

Method: StaRFM utilizes two main components: a Fisher information penalty (FIP) to address covariate shifts in embeddings and a confidence misalignment penalty (CMP) for uncertainty calibration. These are extended theoretically and practically for both vision and medical segmentation tasks.

Result: StaRFM demonstrates higher accuracy (+3.5%) and substantially better calibration (28% lower ECE) across 19 vision datasets. In medical segmentation, it achieves 84.7% DSC and 4.8mm HD95, with a 40% reduction in cross-domain performance gaps compared to baselines.

Conclusion: StaRFM is an effective, plug-and-play framework for aligning foundation models to diverse tasks, offering improved robustness and generalization with minimal integration effort.

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [149] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: This paper introduces a new method to reconstruct animatable avatars from egocentric (first-person) inputs using a generative prior-based approach with Stable Diffusion, overcoming challenges like occlusions and distorted body proportions.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for telepresence require multi-view datasets during training or rely on front-view cameras, which can be less practical. The study aims to make telepresence systems more accessible and portable while maintaining accuracy.

Method: The paper employs a Stable Diffusion generative backbone with ControlNet to convert a single top-down egocentric image into a frontal view representation. This frontal image is then input into an image-to-motion model to enable avatar motion reconstruction.

Result: The proposed approach reduces training complexity and enhances generalizability by eliminating the need for multi-view datasets during training, making the pipeline more efficient.

Conclusion: The study introduces a novel pipeline for creating avatar motions from minimal inputs, advancing the development of accurate, portable, and accessible telepresence systems.

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [150] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: This paper introduces a new dataset and model for assessing painting processes rather than only final images, aiming for better alignment with human judgment.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus exclusively on assessing static final images of paintings, ignoring the dynamic and multi-stage nature of the artistic process.

Method: The authors propose the Painting Process Assessment Dataset (PPAD) and a Transformer-based model called PPJudge, featuring temporally-aware positional encoding and a heterogeneous mixture-of-experts architecture.

Result: The proposed method surpasses existing baselines in accuracy, robustness, and consistency with human judgment across multiple experimental benchmarks.

Conclusion: This research advances artistic image assessment by addressing the overlooked painting process, offering potential applications in computational creativity and art education.

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [151] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: The paper proposes AGCD-Net, a context-aware emotion recognition model, which uses causal theory and hybrid encoder designs to address biases in recognizing emotions. It achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To tackle the problem of context bias in emotion recognition, where spurious correlations between background context and emotion labels affect model performance.

Method: AGCD-Net uses a novel convolutional encoder called Hybrid ConvNeXt, combined with an Attention Guided-Causal Intervention Module (AG-CIM) that perturbs context features, isolates biases, and applies attention-driven corrections based on facial features.

Result: AGCD-Net achieves state-of-the-art performance on the CAER-S dataset, providing strong evidence for the effectiveness of causal debiasing in emotion recognition.

Conclusion: The proposed AGCD-Net successfully mitigates context bias using attention and causal intervention, offering a robust solution for real-world emotion recognition tasks in complex scenarios.

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [152] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: The paper introduces AAHR, a framework addressing challenges in image-text matching by reducing ambiguities and improving semantic understanding through dynamic clustering, feature extraction, and graph-based interactions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the challenges of high-order associations and semantic ambiguities in image-text matching, which result in matching uncertainties due to soft positive and soft negative samples, and to better utilize neighborhood relationships among similar instances.

Method: AAHR employs dynamic clustering prototype contrastive learning to mitigate soft positive problems, introduces global/local feature extraction and adaptive aggregation, uses correlation matrices and GNN for neighborhood relationships, and integrates momentum contrastive learning to handle negative samples.

Result: AAHR outperforms existing methods on datasets such as Flickr30K, MSCOCO, and ECCV Caption, showcasing superior accuracy and efficiency in image-text matching tasks.

Conclusion: AAHR effectively addresses high-order associations and semantic ambiguities in image-text matching while enhancing feature discrimination and achieving state-of-the-art performance.

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [153] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: The paper introduces a new method for gloss-free Sign Language Translation that makes use of a segment-aware visual tokenization framework, reducing input sequence length and computational demands while improving performance.


<details>
  <summary>Details</summary>
Motivation: Current gloss-free SLT methods achieve high performance but come with increased model complexity and scalability issues, prompting the need for a more efficient approach.

Method: A segment-aware visual tokenization framework is proposed to compress video sequences by up to 50%, combined with token-to-token contrastive alignment and dual-level supervision for better cross-modal alignment.

Result: The proposed method achieves state-of-the-art results on the PHOENIX14T benchmark, with reduced memory usage and better scalability compared to prior methods.

Conclusion: The new approach enhances the efficiency and scalability of gloss-free SLT without sacrificing performance, showcasing the potential of their tokenization and alignment strategies.

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [154] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: The paper presents Cross Knowledge Distillation (CKD) to improve Spiking Neural Networks (SNNs) with RGB data and ANN-based guidance, addressing cross-modality and cross-architecture challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance SNNs' performance using DVS data and overcome their limitations such as immature architectures and limited annotated datasets.

Method: Introduced CKD, combining semantic similarity and sliding replacement for cross-modality issues and indirect phased knowledge distillation for cross-architecture problems.

Result: Achieved superior performance on N-Caltech101 and CEP-DVS datasets, outperforming previous State-of-the-Art methods.

Conclusion: CKD is an effective approach to improve SNNs in accuracy and efficiency by leveraging RGB data and ANN-driven knowledge distillation strategies.

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [155] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: The paper introduces Prompt4Trust, a reinforcement learning framework to improve confidence calibration and task accuracy in multimodal large language models (MLLMs) for healthcare applications.


<details>
  <summary>Details</summary>
Motivation: MLLMs, while promising in healthcare, are limited by sensitivity to prompts and confidence issues, which are critical in safety-critical domains where clinicians rely on models' confidence for decision-making.

Method: Prompt4Trust employs reinforcement learning to train a lightweight LLM to generate auxiliary prompts that enhance the calibration and accuracy of downstream MLLMs, aligning confidence with true predictive accuracy.

Result: The method achieved state-of-the-art performance on the PMC-VQA benchmark for medical visual question answering, improved task accuracy, and demonstrated the ability to generalize to larger MLLMs with minimal computational overhead.

Conclusion: Prompt4Trust shows potential for scalable, automated prompt engineering that aligns MLLM outputs with human trust requirements, making them more reliable in clinical decision-making scenarios.

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [156] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: The paper proposes using a deep generative model (GAN) to better initialize the blur kernel for blind motion deblurring (BMD), improving performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing deep prior-based methods for blind motion deblurring struggle due to high non-convexity and sensitivity to initial blur kernel estimation.

Method: The authors pretrain a Generative Adversarial Network (GAN)-based kernel generator and kernel initializer to encode the kernel prior and provide a high-quality initialization for BMD.

Result: Their approach reduces kernel initialization sensitivity, achieves compact latent kernel manifold constraints, extends to non-uniform motion deblurring, and sets new state-of-the-art results on benchmark datasets.

Conclusion: The proposed framework significantly enhances blind motion deblurring methods, operates in a plug-and-play manner, and opens avenues for practical applications with shared source code.

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [157] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: This paper introduces a semantic-aware localization framework for floorplans to enhance location estimation using both depth and semantic data.


<details>
  <summary>Details</summary>
Motivation: Current floorplan localization methods primarily focus on depth information and neglect the rich semantics present in floorplans.

Method: The framework jointly predicts depth and semantic rays, consolidating them into a structural-semantic probability volume refined in a coarse-to-fine manner for location and orientation prediction.

Result: The method outperforms state-of-the-art techniques significantly, demonstrating higher recall metrics and efficient utilization of metadata like room labels.

Conclusion: The introduced framework effectively enhances floorplan localization by leveraging structural and semantic information, achieving higher accuracy and room for additional metadata incorporation.

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [158] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: The paper proposes Geo-RepNet, a framework using RGB images and depth information to improve surgical phase recognition during Endoscopic Submucosal Dissection, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Improving surgical phase recognition in minimally invasive procedures is essential for intelligent systems, but visual similarity across phases and lack of structural cues in RGB images are challenging.

Method: The paper introduces Geo-RepNet, which combines RGB and depth data using a RepVGG backbone, a Depth-Guided Geometric Prior Generation (DGPG) module, and Geometry-Enhanced Multi-scale Attention (GEMA) for spatial insight.

Result: The model, evaluated on a nine-phase ESD dataset, exhibits state-of-the-art accuracy, robustness, and computational efficiency under challenging surgical environments.

Conclusion: Geo-RepNet effectively enhances surgical scene understanding, showcasing the advantages of integrating depth and RGB information for surgical phase recognition.

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [159] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet integrates Vision Transformers (ViTs) with Prototypical Networks for few-shot image classification, achieving superior performance over CNN-based methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to harness the underutilized representational capabilities of Vision Transformers for few-shot classification tasks, addressing challenges in learning from limited examples.

Method: ViT-ProtoNet incorporates a ViT-Small backbone into the Prototypical Network framework, using class conditional token embeddings to create prototypes and evaluate them across benchmarks.

Result: ViT-ProtoNet outperformed CNN-based counterparts in accuracy (up to 3.2% better) and demonstrated superior robustness, feature separability, and competitiveness against other transformer-based models.

Conclusion: ViT-ProtoNet establishes itself as a powerful approach for few-shot image classification, setting a new baseline for transformer-based meta-learning with code and pretrained weights provided for reproducibility.

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [160] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: The paper introduces the Deep Angular A* (DAA*) method to improve path imitation learning by enhancing path smoothness and similarity. It demonstrates superior performance in seven datasets compared to existing methods like Neural A* and TransPath.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the overlooked issue of path smoothness in path imitation learning and to improve the accuracy and adaptability of paths generated in various environments.

Method: The method involves incorporating Path Angular Freedom (PAF) into A*. PAF explores the impact of move angles on path expansion and optimizes path smoothness and similarity through a joint framework of path shortening and smoothing.

Result: In evaluations across 7 datasets, DAA* shows significant improvements, surpassing Neural A* by 9.0% SPR, 6.9% ASIM, and 3.9% PSIM, and outperforming TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM.

Conclusion: DAA* is an effective approach for path imitation learning, achieving better path similarity, smoothness, and optimality. The minor trade-offs in search efficiency are deemed acceptable for its improved performance in various settings.

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [161] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: This paper proposes a novel classification method for multispectral point clouds (MPCs) that addresses sparse labeled data, scale variations, and long-tailed distributions, achieving superior performance on outdoor datasets.


<details>
  <summary>Details</summary>
Motivation: To address key limitations in existing MPC classification methods, particularly when applied to outdoor datasets—such as issues with sparse labeling, variations in land-cover scales, and long-tailed distributions.

Method: The paper introduces a three-stage approach: (1) a grid-balanced sampling strategy for robust training data generation, (2) a multi-scale feature fusion module to mitigate feature loss caused by scale variations, and (3) an adaptive hybrid loss module with multi-classification heads to balance class-wise learning, especially improving performance on small classes.

Result: The proposed method demonstrates improved classification performance on three MPC datasets and outperforms state-of-the-art methods in addressing classification challenges related to outdoor scenes.

Conclusion: The enhanced classification method resolves key challenges associated with outdoor MPC classification by leveraging adaptive multi-scale fusion and improved loss strategies, offering a significant performance boost.

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [162] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: The paper introduces ALPHA, a benchmark for RGBA image synthesis, and proposes ALPHAVAE, a novel RGBA-focused model that improves reconstruction quality and enables transparent image generation.


<details>
  <summary>Details</summary>
Motivation: The need to generate transparent or layered image (RGBA) content efficiently has been overlooked, due to the absence of large-scale benchmarks and tailored models.

Method: The paper proposes ALPHA, a benchmark for adapting RGB metrics to RGBA images, and ALPHAVAE, a VAEs-based RGBA model trained with composite objectives like alpha-blended reconstruction, patch fidelity, and perceptual consistency.

Result: ALPHAVAE, trained on only 8K images, outperformed LayerDiffuse by achieving a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM for image reconstruction.

Conclusion: ALPHAVAE not only establishes new benchmarks for RGBA image generation but also advances the generation of transparent content, setting a strong foundation for future work in RGBA-focused research.

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [163] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: The paper introduces ProactiveBench, a benchmark for evaluating proactive interactions in multimodal systems, and PAUC, a metric for assessing temporal response dynamics.


<details>
  <summary>Details</summary>
Motivation: There is a growing expectation for multimodal dialogue systems to move beyond turn-based interactions and become more proactive in initiating and timing responses autonomously.

Method: The authors propose ProactiveBench as a benchmark for proactive systems and introduce PAUC as a new metric that evaluates temporal aspects of system responses. They validate this with extensive benchmarking and a user study comparing PAUC to traditional metrics.

Result: The study finds that PAUC correlates better with human preferences compared to traditional metrics, showing its superiority in evaluating user experience in proactive settings.

Conclusion: PAUC provides a more accurate and meaningful measure of system performance in proactive interaction scenarios, advancing research in multimodal dialogue systems.

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [164] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: The study assesses various machine learning methods for cloud masking in hyperspectral satellite imaging and identifies a lightweight CNN model as the optimal choice for its accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The need to effectively preprocess hyperspectral satellite images by masking clouds and shadows to ensure high-quality data extraction.

Method: The research compares different machine learning techniques like gradient boosting models (XGBoost and LightGBM) and CNNs, with a focus on feature reduction for computational efficiency.

Result: Models demonstrated over 93% accuracy, with a CNN model featuring reduced parameters being most efficient, balancing high accuracy, low storage needs, and quick inference times.

Conclusion: Lightweight AI models like CNNs with reduced parameters are effective for hyperspectral satellite image processing and could facilitate the development of real-time, onboard satellite AI systems.

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [165] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: This paper introduces the Dynamic Inter-Class Confusion-Aware Encoder (DICCAE), an audio-video encoder that improves fine-grained category-level alignment and performance in human activity recognition.


<details>
  <summary>Details</summary>
Motivation: Existing audio-video pre-training methods fail to address the challenge of distinguishing easily confused classes, limiting their effectiveness in precise human activity recognition.

Method: The proposed DICCAE dynamically adjusts a confusion loss to address inter-class confusion at a category-level. A novel training framework combines audio, video, and their fusion, complemented by a cluster-guided self-supervised pre-training strategy to address data scarcity.

Result: DICCAE achieves a top-1 accuracy of 65.5% on the VGGSound dataset, approaching state-of-the-art performance, and its modules are validated through ablation studies.

Conclusion: The integration of dynamic confusion-aware encoding and a self-supervised pre-training strategy enhances both feature representation and the ability to distinguish similar activities in audio-video tasks.

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [166] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D is a framework designed to improve the efficiency of 3D Multi-modal Large Language Models (MLLMs) by introducing a token pruning strategy that addresses redundancies in object-level 3D token representations.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the inefficiency of existing 3D MLLMs, which struggle with computational challenges due to the large number of object-centric visual tokens required for comprehensive representation.

Method: Fast3D employs two techniques: (1) Global Attention Prediction (GAP), which predicts global attention distributions using a lightweight neural network for effective token importance estimation; and (2) Sample-Adaptive visual token Pruning (SAP), which adjusts token pruning ratios dynamically based on attention-driven complexity analysis, without altering the target model's parameters.

Result: Extensive experiments across five benchmarks demonstrated that Fast3D effectively improves computational efficiency in 3D MLLMs, particularly under high pruning ratios, validating the approach's robustness.

Conclusion: Fast3D provides a practical solution for enhancing 3D MLLMs' computational efficiency by pruning redundant tokens without modifying the target model, making it a promising direction for addressing scalability challenges. The code is open-source and available for public access.

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [167] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: The paper explores ego-centric Traffic Anomaly Detection (TAD) via a simple encoder-only approach using Video Vision Transformers (Video ViTs) instead of complex architectures, demonstrating that pre-training significantly enhances performance.


<details>
  <summary>Details</summary>
Motivation: The authors are motivated by findings in visual perception suggesting that simpler architectures enabled by advanced pre-training can outperform specialized complex designs.

Method: They use plain Video Vision Transformers (Video ViTs) and focus on the effects of different pre-training methods, including self-supervised Masked Video Modeling (MVM) and Domain-Adaptive Pre-Training (DAPT).

Result: The study finds that strong pre-training allows simple models to match or outperform complex state-of-the-art methods efficiently. Self-supervised MVM provides the best performance signal, and domain-adaptive pre-training on unlabeled driving videos further boosts results.

Conclusion: Pre-training is vital for building efficient, scalable, and effective TAD models with minimal architectural complexity. The research supports foundational models as promising tools for anomaly detection and releases resources for future studies.

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [168] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: The paper presents a CNN-based system for automated detection of crop diseases using leaf imagery, achieving ~90% training and ~60% validation accuracy, integrated with a treatment recommendation module on a mobile platform.


<details>
  <summary>Details</summary>
Motivation: Crop diseases hinder agricultural productivity and food security, especially in large-scale farming where early identification is challenging.

Method: A CNN with three convolutional layers and ReLU activations is trained using TensorFlow and Keras on a labeled leaf image dataset. Data preprocessing involves resizing, normalization, and augmentation. The system classifies eight diseases and includes a treatment recommendation module.

Result: The model shows ~90% training accuracy but ~60% validation accuracy, suggesting slight overfitting. It is deployed on an open-source platform for real-time diagnostics.

Conclusion: This scalable tool enhances precision agriculture by integrating deep learning with practical disease management, promoting sustainable farming and supporting global food production.

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [169] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: This paper introduces a practical pipeline leveraging ML/AI tools to efficiently process camera trap data, suitable for small research groups with limited resources.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in handling large camera trap datasets, particularly for small research groups constrained by resources and computational expertise.

Method: It proposes a low-resource, on-premise pipeline incorporating tailored ML/AI capabilities for efficient camera trap data processing and analysis.

Result: The pipeline enables effective data transmission, inference, and evaluation, making it accessible for extracting insights from camera trap datasets despite resource limitations.

Conclusion: The outlined approach helps small research groups overcome barriers in managing large datasets and utilizing ML/AI tools, thus enhancing wildlife research capabilities.

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [170] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: The paper introduces efficient neural network techniques for real-time celestial landmark detection and description to enhance spacecraft autonomy.


<details>
  <summary>Details</summary>
Motivation: Current methods for celestial terrain feature detection face challenges like high computation, limited generalization, and reliance on extensive pre-processing.

Method: The authors propose lightweight neural networks for real-time spacecraft processing, using domain adaptation for detection and attention alignment for robust descriptions.

Result: The proposed system outperforms state-of-the-art techniques in celestial landmark tracking.

Conclusion: This approach improves spacecraft autonomy while addressing hardware and data scarcity limitations, paving the way for better celestial navigation.

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [171] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D introduces a framework for 3D instance segmentation with minimal supervision, combining attention mechanisms and cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in unsupervised 3D point cloud instance segmentation while incorporating multimodal understanding for practical applications.

Method: SegVec3D utilizes hierarchical feature extractors, contrastive clustering for unsupervised segmentation, and aligns 3D data with natural language queries in a shared semantic space.

Result: The framework achieves unsupervised segmentation and supports zero-shot retrieval, surpassing methods like Mask3D and ULIP in unifying segmentation with multimodal capabilities.

Conclusion: SegVec3D demonstrates a feasible approach for achieving 3D segmentation and multimodal language-vision understanding with minimal supervision, enhancing deployability.

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [172] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: The paper presents a computationally efficient model for 3D multi-person motion prediction, achieving state-of-the-art performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of 3D multi-person motion prediction, which depends on both individual movements and inter-agent interactions, while lowering computational expense.

Method: The model employs lightweight dual branches for local and global representations, introduces a cross-level interaction block for spatial-temporal integration, and uses spatial inter-person distance embedding to enhance interaction modeling.

Result: Achieved state-of-the-art performance on CMU-Mocap, MuPoTS-3D, and 3DPW datasets with reduced computational costs.

Conclusion: The proposed design simplifies interaction modeling while maintaining high prediction accuracy and efficiency, offering significant improvements in computational performance.

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [173] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: The paper introduces a framework, CKAA, to improve continual learning for AI models, addressing robustness issues with PEFT-based CL methods using two key components: intra-class feature alignment and adaptive knowledge aggregation.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness of parameter-efficient fine-tuning (PEFT)-based continual learning (CL) methods due to their vulnerability to feature subspace misalignment and ambiguous decisions caused by misleading task identifiers.

Method: The CKAA framework incorporates (1) Dual-level Knowledge Alignment (DKA) to align intra-class feature distributions across subspaces and enhance global classification robustness, and (2) Task-Confidence-guided Mixture of Adapters (TC-MoA), which uses task-confidence scores to aggregate knowledge adaptively from sub-modules during inference.

Result: CKAA demonstrated superior performance in robustness against misleading task-ids and outperformed existing PEFT-based CL methods, as validated through extensive experiments.

Conclusion: The proposed CKAA framework effectively enhances robustness in PEFT-based CL models by addressing feature misalignment and improving decision confidence, offering a promising approach for robust continual learning.

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [174] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: The paper introduces HMID-Net, which combines Masked Image Modeling and hyperbolic space knowledge distillation to efficiently capture visual-semantic hierarchies.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of training models that effectively capture and leverage visual-semantic hierarchies in hyperbolic space.

Method: The authors propose HMID-Net, a method integrating Masked Image Modeling (MIM) and a tailored knowledge distillation technique in hyperbolic space, along with a new distillation loss for better knowledge transfer.

Result: The proposed method outperforms state-of-the-art models like MERU and CLIP on downstream tasks such as image classification and retrieval.

Conclusion: Hyperbolic space techniques combined with MIM and knowledge distillation offer an efficient way to represent and harness hierarchical relationships, achieving significant performance improvements.

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [175] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE is a benchmark designed to evaluate large vision-language models' (LVLMs) ability to reason over full video context rather than static frames. Human performance is 94.82%, while top models achieve only 66.43%, revealing their limitations.


<details>
  <summary>Details</summary>
Motivation: Existing video benchmarks fail to assess whether LVLMs can deeply analyze videos as they often focus on superficial frame-based questions.

Method: GLIMPSE was developed with 3,269 videos and over 4,342 manually crafted questions across 11 categories like Temporal Reasoning and Forensics Detection, requiring full video context understanding.

Result: Humans achieve high accuracy (94.82%), but current LVLMs, such as GPT-o3, lag significantly, performing at only 66.43%.

Conclusion: LVLMs still struggle to perform deep temporal reasoning and require further improvement for genuine video understanding.

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [176] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: The paper introduces LifelongPR, a novel continual learning framework for point cloud place recognition (PCPR), addressing catastrophic forgetting and domain shifts to improve model adaptability in diverse and dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for PCPR models to adapt to diverse and dynamic environments without suffering from catastrophic forgetting, which is crucial for applications like autonomous driving and augmented reality.

Method: The approach involves a replay sample selection method for balanced knowledge retention and a prompt learning-based continual learning framework with a lightweight prompt module and two-stage training to handle domain shifts.

Result: LifelongPR demonstrated significant improvements over state-of-the-art methods, achieving a 6.50% improvement in mIR@1, 7.96% in mR@1, and an 8.95% reduction in forgetting (F) across large-scale public and self-collected datasets.

Conclusion: LifelongPR effectively addresses issues of scalability, maintenance, and domain adaptability in PCPR models, making it a promising solution for real-world deployment.

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [177] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: The paper introduces SDTN and TRN methods for hyperspectral image classification to tackle challenges like high-dimensional data and scarcity of labeled samples, achieving improved accuracy and reduced computational load.


<details>
  <summary>Details</summary>
Motivation: Current hyperspectral imaging methods face limitations with high-dimensional data and insufficient labeled samples, impacting classification accuracy in applications like precision agriculture.

Method: The authors developed the Self-Adaptive Tensor-Regularized Network (SDTN) for optimized feature representation through tensor decomposition and regularization, along with Tensor-Regularized Network (TRN) for lightweight multi-scale spectral-spatial feature integration.

Result: Using the PaviaU datasets, the proposed methodologies showed enhanced accuracy and significantly smaller model parameters compared to existing approaches.

Conclusion: SDTN and TRN frameworks resolve key issues in hyperspectral image classification, offering high accuracy with reduced computational complexity suitable for real-time usage in constrained environments.

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [178] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: The paper addresses VLMs' difficulties with distribution shifts in tasks without labeled data, proposing ReTA, a method featuring two reliability strategies for improved adaptation and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance VLMs' reliability during test-time adaptation, especially under distribution shifts, as existing methods struggle with entropy issues and inflexible decision boundaries.

Method: ReTA incorporates Consistency-aware Entropy Reweighting (CER) for improving cache reliability and Diversity-driven Distribution Calibration (DDC) for adaptive adjustments in decision boundaries.

Result: ReTA improves upon state-of-the-art methods, delivering robust adaptation and superior performance under real-world distribution shifts.

Conclusion: ReTA demonstrates its ability to reliably adapt VLMs to distribution shifts, addressing critical challenges and advancing test-time adaptation techniques.

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [179] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: Developed a new method for micro-gesture recognition that achieved state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To handle the challenging task of locating and recognizing micro-gestures in untrimmed videos due to their spontaneous nature and differences from traditional human actions.

Method: Introduced hand-crafted data augmentation and spatial-temporal attention to improve model classification and localization of micro-gestures.

Result: Achieved an F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%, and secured first place in the competition.

Conclusion: The proposed method significantly advanced micro-gesture recognition, demonstrating its accuracy and effectiveness in distinguishing and localizing these challenging instances.

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [180] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: This paper introduces a privacy-preserving, multi-system framework for fall detection with a combined accuracy of 99.99%, addressing safety for older adults.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing aging population, the increasing risk of falls among older adults, and the need for effective, reliable fall detection systems that preserve users' privacy.

Method: The authors propose a framework combining three systems: semi-supervised federated learning for wearable and edge devices, indoor localization/navigation for robots, and vision-based human recognition using edge cameras.

Result: The systems demonstrate individual accuracy: 99.19% (SF2D), 96.3% (vision-based detection), and 95% (navigation). Combined, they yield an overall accuracy of 99.99% for fall detection.

Conclusion: The proposed framework is highly accurate, safe, and privacy-conscious, making it an effective solution for fall detection and timely intervention for older adults.

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [181] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap is an activation pruning method that enhances the efficiency of VMamba and similar models without retraining, offering up to 11% speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: State space models have performance bottlenecks due to spatial redundancy in their scanning processes. The paper seeks efficient ways to reduce these redundancies while maintaining model accuracy.

Method: The proposed method, QuarterMap, uses post-training activation pruning to remove redundant spatial activations and restores dimensions with nearest-neighbor upsampling, improving throughput without retraining.

Result: QuarterMap provided up to 11% speedup on VMamba with accuracy drops less than 0.9% on ImageNet-1K and showed similar results on ADE20K segmentation tasks.

Conclusion: QuarterMap serves as an effective and deployment-friendly tool for increasing efficiency in SSMs-based models like VMamba and MedMamba, without requiring retraining and while maintaining transferability across tasks.

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [182] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: This paper introduces DehazeSB, a novel unpaired dehazing framework utilizing optimal transport theory through the Schrödinger Bridge to enhance mappings between hazy and clear images for high-quality results.


<details>
  <summary>Details</summary>
Motivation: Existing GAN-based unpaired dehazing methods face limitations due to the generator's limited transport mapping capability, restricting effectiveness in unpaired training paradigms.

Method: DehazeSB employs optimal transport theory via the Schrödinger Bridge to directly link hazy and clear image distributions, incorporates detail-preserving regularization for pixel-level alignment, and introduces prompt learning to leverage pre-trained CLIP models for haze-aware vision-language alignment.

Result: Extensive experiments on multiple real-world datasets confirm that DehazeSB achieves superior dehazing performance compared to existing methods.

Conclusion: DehazeSB effectively addresses the shortcomings of prior methods by enhancing transport mappings and ensuring high-quality, structurally consistent dehazed outputs.

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [183] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: The paper introduces VDInstruct, a Multimodal Large Language Model (MLLM) for Key Information Extraction (KIE) that optimizes tokenization and achieves state-of-the-art results while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs perform poorly on dense visual documents due to inefficient tokenization techniques that scale with image size, leading to redundant computation and memory inefficiency.

Method: VDInstruct employs a content-aware tokenization strategy that generates tokens based on document complexity instead of uniform image fragmentation. Spatial region detection is separated from semantic feature extraction, and the model is trained in three stages.

Result: VDInstruct achieves state-of-the-art performance on KIE benchmarks with a 3.6x reduction in image token count. It also excels in zero-shot evaluations, outperforming strong baselines like DocOwl 1.5 by +5.5 F1 points.

Conclusion: Content-aware tokenization and explicit layout modeling mark a significant advancement in document understanding, offering both accuracy and efficiency. Data, source code, and model weights will be openly shared.

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [184] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: This paper proposes DRPCA-Net, a novel deep unfolding network for infrared small target detection, emphasizing efficiency and interpretability by leveraging a sparsity-aware prior.


<details>
  <summary>Details</summary>
Motivation: Despite progress in infrared small target detection using deep learning, current methods compromise interpretability, parameter efficiency, and generalization by ignoring the sparsity prior.

Method: The authors propose DRPCA-Net, which integrates the sparsity-aware prior into a learnable architecture. It includes a dynamic unfolding mechanism using a lightweight hypernetwork for iteration-wise parameter adaptation and a Dynamic Residual Group module.

Result: Experiments on public infrared datasets demonstrate that DRPCA-Net significantly outperforms state-of-the-art methods in detection accuracy.

Conclusion: DRPCA-Net effectively balances performance with efficiency and generalization by integrating sparsity-aware priors and leveraging dynamic adaptation mechanisms.

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [185] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: This paper introduces a new task (Sequential CSIST Unmixing) to detect sub-pixel distant infrared targets, along with a benchmark dataset, toolkit, and a novel deep learning model, DeRefNet, which outperforms previous methods.


<details>
  <summary>Details</summary>
Motivation: The need arises due to challenges in detecting sub-pixel localization of dense infrared small targets, compounded by limitations in existing datasets.

Method: The authors developed a dataset and toolkit (SeqCSIST) and proposed DeRefNet with a Temporal Deformable Feature Alignment (TDFA) module for adaptive inter-frame learning.

Result: Experiments on SeqCSIST demonstrate a 5.3% improvement in mean Average Precision (mAP) over state-of-the-art methods.

Conclusion: This paper pioneers the Sequential CSIST Unmixing task, offering advancements in dataset resources and method development, outperforming existing approaches and promoting further research.

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [186] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: The paper proposes an enhanced hand pose estimation (EHPE) method focusing on correcting errors in distal phalanx tips and wrist predictions, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Accurately estimating 3D hand pose is crucial for applications like virtual reality and human-computer interaction, but distal joints pose significant challenges due to error accumulation.

Method: EHPE uses a segmented architecture comprising two main stages: 1) TIP and Wrist Joints Extraction (TW-stage) for initial joint predictions, and 2) Prior Guided Joints Estimation (PG-stage) using a dual-branch interaction network to refine predictions.

Result: Experiments reveal EHPE outperforms existing methods, demonstrating superior predictive accuracy on benchmark datasets.

Conclusion: Focusing on distal joints improves pose estimation quality, reinforcing EHPE's value in addressing error accumulation issues.

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [187] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: This paper surveys prompt engineering techniques for SAM, emphasizing its evolution, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focused literature on prompt engineering's role in SAM's success and its broader implications.

Method: A systematic review of existing work on prompt engineering methods, applications, and challenges for SAM and its variants.

Result: The review organizes the progression of prompt engineering from basic to advanced multimodal approaches while identifying unique challenges.

Conclusion: The paper provides a framework to enhance understanding and drive future research in prompt engineering for segmentation models.

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [188] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft is an interactive artistic typography system integrating diffusion models and language models for precise, user-driven typographic designs.


<details>
  <summary>Details</summary>
Motivation: Current artistic typography solutions lack interactivity features like localized edits, iterative refinements, and multi-character compositions.

Method: WordCraft employs a training-free regional attention mechanism, noise blending, and a large language model for precise stylization based on user prompts.

Result: The system generates high-quality stylized typography across various languages, improving interactivity and enabling user-centered workflows.

Conclusion: WordCraft enhances artistic typography synthesis, creating new creative avenues for artists and designers.

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [189] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR is a novel autoregressive framework designed for efficient multimodal image generation, overcoming existing limitations in precision, control, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing text-to-image models in visual control, multimodal input integration, and training efficiency.

Method: MENTOR utilizes a two-stage autoregressive training paradigm, emphasizing multimodal alignment at the pixel and semantic levels and tuning for balanced multimodal integration.

Result: MENTOR demonstrates strong performance on DreamBench++, excelling in concept preservation, prompt adherence, image fidelity, task adaptability, and training efficiency compared to diffusion-based methods.

Conclusion: MENTOR offers a robust, efficient solution for multimodal image generation, achieving superior results despite modest resources and components.

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [190] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: This paper improves the Segment Anything Model 2 (SAM2) for critical surgical video segmentation by introducing a Memory Augmented (MA)-SAM2 model that increases robustness and efficiency without additional training.


<details>
  <summary>Details</summary>
Motivation: The inherent limitations of SAM2's greedy memory design hinder its performance in surgical video segmentation due to rapid instrument movements, occlusions, and complex interactions.

Method: The proposed MA-SAM2 implements context-aware and occlusion-resilient memory models and uses a multi-target, single-loop, one-prompt inference strategy for efficient video segmentation.

Result: MA-SAM2 demonstrated performance improvements of 4.36% on the EndoVis2017 dataset and 6.1% on the EndoVis2018 dataset compared to SAM2, without requiring additional parameters or training.

Conclusion: The MA-SAM2 model enhances segmentation accuracy and robustness in surgical videos, showing potential for practical applications in computer-assisted surgery.

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [191] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1 is a state-of-the-art text-to-image generation model, and this document reverse-engineers its architecture to support further research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand FLUX.1's architecture and training setup to facilitate its adoption for future research and development.

Method: The method involves reverse-engineering FLUX.1's open source code to extract details about its architecture, as no official documentation is available.

Result: The reverse-engineering process demystifies the inner workings of FLUX.1, providing insights into its structure and functionality.

Conclusion: This effort offers an unofficial technical understanding of FLUX.1, enabling its use as a backbone for research despite the lack of original documentation.

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [192] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: The paper introduces Inter2Former, an interactive segmentation model that improves efficiency and segmentation quality on CPUs by optimizing dense-token processing and combining four novel methods: DPE, DHA, HMoE, and DLU.


<details>
  <summary>Details</summary>
Motivation: Interactive segmentation suffers from a trade-off between segmentation quality and computational efficiency, especially on CPU devices. Current methods either provide high-quality results but are computationally intensive or are fast but compromise accuracy.

Method: The authors introduce Inter2Former, which incorporates four techniques: Dynamic Prompt Embedding (DPE), Dynamic Hybrid Attention (DHA), Hybrid Mixture of Experts (HMoE), and Dynamic Local Upsampling (DLU), to optimize token processing and improve segmentation efficiency.

Result: Inter2Former achieves state-of-the-art performance in terms of both segmentation quality and computational efficiency on interactive segmentation benchmarks.

Conclusion: The proposed methods in Inter2Former successfully resolve the trade-off between accuracy and efficiency in interactive segmentation, making it highly suitable for real-world applications, particularly on CPU devices.

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [193] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: The paper introduces FAIR, a method to refine pseudo-labels and improve fine-grained classification in vision-language models (VLMs), outperforming the state of the art.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised adaptation methods for fine-grained classification using VLMs either use static alignment scores or computationally expensive strategies, which are ineffective in capturing subtle class distinctions or lack scalability.

Method: FAIR uses Class Description Anchors (CDA) to dynamically align localized image features with descriptive language embeddings and introduces a Learned Alignment Score (LAS). Additionally, it employs a self-training weighting mechanism to refine pseudo-labels in ambiguous inter-class scenarios.

Result: The FAIR approach shows a significant improvement in fine-grained unsupervised adaptation, gaining 2.78% across 13 fine-grained datasets compared to prior state-of-the-art methods.

Conclusion: FAIR effectively enhances fine-grained cross-modal interactions and pseudo-label quality, delivering better performance and scalability for VLMs in unsupervised adaptation tasks.

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [194] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: This paper introduces Generate Aligned Anomaly (GAA), a methodology for creating realistic and semantically aligned anomaly samples using few-shot learning.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of existing anomaly inspection methods in industrial manufacturing due to the scarcity of realistic anomaly samples and challenges in anomaly synthesis.

Method: The paper proposes a framework, GAA, which utilizes a pretrained latent diffusion model for sample generation. It includes steps like Localized Concept Decomposition, Adaptive Multi-Round Anomaly Clustering, and region-guided mask generation for improving quality, alignment, and diversity.

Result: The experiments demonstrate superior performance of GAA in generating high-quality anomaly samples and improving downstream tasks on the MVTec AD and LOCO datasets.

Conclusion: The GAA framework significantly enhances anomaly synthesis quality and task performance, filling a gap in industrial anomaly inspection.

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [195] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: The paper proposes an AI framework using Vision Transformers to classify stroke types from CT scans with high accuracy, emphasizing interpretability and clinical application.


<details>
  <summary>Details</summary>
Motivation: To provide timely and accurate diagnosis of stroke types using CT scans, addressing global healthcare needs for effective emergency intervention.

Method: Developed a multiclass stroke classification framework with MaxViT Vision Transformer and other variants, enhanced by data augmentation and Explainable AI (XAI) techniques for interpretability.

Result: Achieved state-of-the-art performance with 98.00% accuracy and F1-score using the MaxViT model and augmentation techniques.

Conclusion: The study successfully delivers an interpretable and clinically applicable AI tool for stroke diagnosis, enhancing emergency department capabilities and improving patient outcomes globally.

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [196] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: The paper investigates fairness and performance of AI models in diabetic retinopathy (DR) diagnosis using macula fundus images, and explores disentanglement as a bias mitigation technique.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy is a major cause of vision loss. While AI offers a scalable solution for DR diagnosis, concerns about fairness and generalization need to be addressed.

Method: Three AI models (ConvNeXt V2, DINOv2, and Swin V2) were trained on a diverse fundus image dataset to predict DR and sensitive attributes like age and gender. Fairness was assessed for subgroups, and disentanglement techniques were applied to mitigate bias.

Result: All models performed well in DR prediction (up to 94% AUROC) and reasonably predicted sensitive attributes (91% AUROC for age, 77% AUROC for gender). Disparities in fairness were identified, and disentanglement had mixed effects on performance, improving DINOv2 but degrading ConvNeXt V2 and Swin V2.

Conclusion: The study highlights challenges in achieving fairness and the complexity of disentangling fine-grained features in medical imaging AI, emphasizing the need for equitable healthcare solutions.

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [197] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg is a Bayesian uncertainty-aware eye segmentation framework designed to improve gaze estimation for AR/VR under challenging conditions like motion blur and domain gaps.


<details>
  <summary>Details</summary>
Motivation: Accurate eye segmentation is key for efficient gaze estimation in human-machine AR/VR interactions, especially under challenging conditions such as motion blur and domain gaps.

Method: EyeSeg employs Bayesian uncertainty learning within a closed-set prior framework to model segmentation uncertainties, utilizing posterior statistics for robustness in challenging conditions.

Result: EyeSeg demonstrates superior segmentation metrics (MIoU, E1, F1, ACC) compared to existing methods and improves downstream gaze estimation tasks, particularly under challenging scenarios like motion blur and domain gaps.

Conclusion: EyeSeg addresses key challenges in eye segmentation by integrating uncertainty-aware methods, yielding improved robustness and performance for AR/VR gaze estimation tasks.

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [198] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: This paper introduces VST-Pose, a novel WiFi-based deep learning system for accurate human pose estimation, outperforming existing methods in terms of accuracy and robustness while promoting privacy and penetration advantages.


<details>
  <summary>Details</summary>
Motivation: The authors aim to provide a non-visual, privacy-aware alternative for human pose estimation that leverages WiFi signals, addressing challenges in current visual or traditional pose detection methods in indoor environments.

Method: The paper introduces a dual-stream spatiotemporal attention backbone called ViSTA-Former. It includes a velocity modeling branch for capturing fine-grained human motion and trains on a new self-collected 2D pose dataset tailored to home care scenarios.

Result: VST-Pose achieved a 92.2% accuracy on the PCK@50 metric, outperforming existing methods by 8.3% on the self-collected dataset. Its robustness was also validated on the public MMFi dataset for 3D pose estimation.

Conclusion: The study provides a robust and privacy-conscious solution for continuous human pose estimation using WiFi signals, suitable for indoor applications like home care. The source code for the framework is open-source for further development.

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [199] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: This paper introduces a framework for generating high-resolution Digital Elevation Models (DEMs) using a prompt-based monocular depth estimation technique, achieving significant resolution gains with improved accuracy over existing models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing techniques for generating high-resolution DEMs, such as the constraints of super-resolution methods and the lack of global elevation context in monocular depth estimation.

Method: The method involves fine-tuning a vision transformer encoder with LiDAR-derived DEMs while utilizing a versatile prompting strategy. This enables tasks like DEM estimation, void filling, and updating. It uses low-resolution SRTM elevation data as prompts and high-resolution NAIP RGB imagery to achieve 100x resolution gains.

Result: The framework produced high-resolution DEMs with < 5 m Mean Absolute Error (MAE) relative to LiDAR, surpassing SRTM accuracy by up to 18%. It showed robust generalization across diverse U.S. landscapes and scalability to larger regions in the U.S. and Israel.

Conclusion: The framework offers a scalable and precise approach for generating high-resolution DEMs, making it suitable for applications in hydrological, urban, and ecological studies. Its publicly available code and models enhance reproducibility and adoption potential.

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [200] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: The paper introduces ExpInstruct, a dataset for commentary generation in scientific experiments, and develops ExpStar, a model that excels in this task using external knowledge retrieval.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce reliance on human subject-specific expertise and time investment by automating the generation of insightful experiment commentary.

Method: The paper uses ExpInstruct dataset to train and evaluate ExpStar, which employs retrieval-augmented mechanisms to access external knowledge and integrate scientific and safety-related insights.

Result: ExpStar significantly outperformed 14 leading large multimodal models (LMMs) in generating experiment commentary, demonstrating the efficacy of both the dataset and the model.

Conclusion: ExpStar demonstrates great potential for enhancing AI-driven scientific experiment instruction by generating fine-grained, insightful commentaries effectively.

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [201] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: Token compression techniques accelerate Vision Transformer inference by reducing less informative tokens. However, challenges exist in categorizing methods and evaluating compact models.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies caused by quadratic computational complexity in Vision Transformers and study the viability of token compression techniques for compact architectures.

Method: A systematic taxonomy and comparative study of token compression methods, evaluating their efficacy on standard and compact Vision Transformer architectures.

Result: Token compression methods work well for general-purpose Vision Transformers but underperform in compact designs intended for edge devices.

Conclusion: Insights from this study highlight limitations and guide future research on adapting token compression techniques for compact transformer networks used in edge AI applications.

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [202] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: The paper improves Text-to-3D generation using Linearized Lookahead Variational Score Distillation ($L^2$-VSD), overcoming challenges with convergence and stability.


<details>
  <summary>Details</summary>
Motivation: Existing Variational Score Distillation (VSD) techniques for Text-to-3D generation suffer from slow and ill-posed convergence due to mismatches in model distributions.

Method: The authors propose $L^2$-VSD, which adjusts optimization order and uses a linearized score model to achieve better stability and quality, leveraging forward-mode auto-differentiation for efficiency.

Result: Results demonstrate superior generation quality and stability of $L^2$-VSD compared to prior methods, and compatibility with other VSD-based frameworks.

Conclusion: The $L^2$-VSD approach resolves stability and convergence issues in VSD, significantly advancing Text-to-3D generation methods.

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [203] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: The paper introduces a new approach called Video-Text Interleaved CoT (ViTCoT) for video understanding, which integrates both visual and textual reasoning.


<details>
  <summary>Details</summary>
Motivation: Current video reasoning methods heavily depend on textual information, overlooking the visual content that is essential for comprehensive understanding, unlike human reasoning which naturally integrates visual re-examination.

Method: The proposed ViTCoT paradigm involves a Video-Text Interleaved Benchmark (ViTIB), created using MLLMs for key-video selection and verified manually. It aims to combine visual and textual reasoning effectively.

Result: Experiments reveal that ViTCoT outperforms existing text-only approaches, enhancing both reasoning performance and neuron activation in MLLMs.

Conclusion: The ViTCoT paradigm introduces a more cognitively aligned and effective method for video reasoning by integrating visual and textual modalities, showing significant benefits over traditional approaches.

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [204] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: This paper introduces a novel hybrid approach combining geometric and pictorial features for aligning puzzle fragments, overcoming limitations related to fragment shape or content.


<details>
  <summary>Details</summary>
Motivation: Many existing fragment-reconstruction methods struggle with realistic fragment geometry or rely heavily on constrained shapes, limiting their applicability to real-world puzzles.

Method: The authors propose a hybrid technique for fragment alignment that incorporates geometry and pictorial features without assuming specific fragment characteristics. They also introduce a new dataset and metrics rooted in real-world erosion models.

Result: The proposed compatibility method achieves state-of-the-art performance in precision and recall on the RePAIR 2D dataset, showcasing clear gains in neighborhood-level puzzle reconstruction.

Conclusion: The hybrid approach effectively handles realistic fragment properties, broadening its application scope and enhancing puzzle-solving frameworks in archaeological contexts.

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [205] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine refines negative labels in Vision-Language Models for zero-shot OOD detection by filtering subcategories and introducing multi-matching-aware scoring.


<details>
  <summary>Details</summary>
Motivation: Existing methods like NegLabel and CSP wrongly detect in-distribution samples as OOD due to subcategory or proper noun negative labels and struggle with images matching multiple labels.

Method: The authors introduce a framework that filters out inappropriate negative labels (e.g., subcategories, proper nouns) and employs a scoring function that adapts to images matching multiple labels.

Result: NegRefine successfully enhances robustness in separating in-distribution and out-of-distribution samples, performing well on benchmarks like ImageNet-1K.

Conclusion: NegRefine addresses key challenges in zero-shot OOD detection, providing a more accurate and computationally effective framework.

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [206] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: The paper introduces VRU-Accident, a benchmark designed to assess multimodal large language models (MLLMs) in understanding high-risk traffic scenarios involving vulnerable road users (VRUs).


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems face challenges in ensuring the safety of VRUs like pedestrians and cyclists, as accidents often lead to severe outcomes. Additionally, there is no standardized way to evaluate the reasoning abilities of MLLMs in handling such scenarios.

Method: The authors developed VRU-Accident, a benchmark comprising 1K real-world dashcam videos, annotated with 6K multiple-choice Q&A pairs spanning six safety-critical categories, alongside 1K dense scene descriptions focusing on VRU-vehicle accidents.

Result: A comprehensive evaluation of 17 state-of-the-art MLLMs showed that while these models perform well on visually grounded attributes, they struggle significantly in reasoning and explaining accident causes, types, and preventability.

Conclusion: The study highlights the potential of MLLMs in autonomous vehicle scene understanding but underscores the need for improvement in reasoning capabilities specifically in safety-critical VRU scenarios.

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [207] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: This paper evaluates CLIP-based vision-and-language models on their ability to replicate human-like associations in the bouba-kiki effect, finding that they fail to consistently exhibit the behavior.


<details>
  <summary>Details</summary>
Motivation: The study investigates whether vision-and-language models align with human cognition by testing their ability to replicate the bouba-kiki effect, a well-documented human phenomenon linking shapes and pseudowords.

Method: Two variants of CLIP (ResNet and Vision Transformer) were tested using prompt-based evaluations and Grad-CAM for visual attention analysis, modeled after human experimental methods.

Result: The models demonstrated inconsistent results; ResNet showed a preference for round shapes, but neither model reliably exhibited the bouba-kiki effect. Comparison with human data highlighted significant cognitive gaps.

Conclusion: The findings reveal limitations in vision-and-language models' cross-modal understanding, questioning their ability to exhibit human-like, integrated cognitive behaviors.

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [208] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: This paper explores the resemblance between human and deep learning model representations of 3D object shapes using point clouds, finding humans to excel and visual transformers to better align with human performance compared to convolution-based models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate whether deep learning models trained on 3D shape recognition develop representations akin to human vision, particularly focusing on local and global geometric structures.

Method: Experiments were conducted on human object recognition using point clouds under different conditions, and performances were compared with deep learning models (DGCNN and point transformer).

Result: Point transformer models aligned more closely with human performance than convolutional neural network models due to hierarchical 3D shape abstraction mechanisms.

Conclusion: The study concludes that certain deep learning models (point transformers) capture structural hierarchies in 3D shapes better than others, but humans consistently outperform under varying conditions.

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [209] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: The paper surveys advancements in Multimodal Large Language Models (MLLMs) for Visually-Rich Document Understanding (VRDU), offering insights into encoding strategies, training paradigms, and datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing need for automatic processing of documents with complex visual, textual, and layout data.

Method: The authors review existing MLLM-based VRDU methods, focusing on encoding techniques, training strategies, and datasets.

Result: The survey organizes and evaluates recent efforts in MLLM-based VRDU and identifies critical challenges and opportunities.

Conclusion: Future efforts should focus on improving efficiency, generalizability, and robustness in VRDU systems.

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [210] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: The SpeakerVid-5M dataset is introduced as the first large-scale dataset for audio-visual dyadic interactive virtual humans, consisting of 5.2 million video clips and stratified for diverse use cases.


<details>
  <summary>Details</summary>
Motivation: There is an increasing interest in advancing audio-visual dyadic virtual human interactions, enabled by breakthroughs in large-scale models.

Method: The researchers developed a dataset structured along interaction types (like dialogue and listening) and quality tiers (pre-training vs. supervised fine-tuning), alongside an autoregressive baseline and benchmarks.

Result: SpeakerVid-5M includes over 8,743 hours of videos across 5.2 million clips, supporting various interaction modes and offering a benchmark for further research.

Conclusion: SpeakerVid-5M and its associated resources aim to drive forward research in interactive virtual humans by providing both data and standardized benchmarks for evaluation.

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [211] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: The paper introduces FaceLLM, a domain-specific multimodal large language model, trained for facial image understanding tasks. It leverages a novel weakly supervised pipeline using ChatGPT to introduce the FairFaceGPT dataset. The model achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models fail to excel in domain-specific tasks, particularly facial image understanding, due to the lack of specialized datasets.

Method: FaceLLM is trained using a weakly supervised pipeline that generates annotated data (FairFaceGPT dataset) using ChatGPT applied to the FairFace dataset with attribute-aware prompts.

Result: FaceLLM outperforms other models in face-centric tasks, excels at recognizing facial features, and sets a new benchmark for these tasks.

Conclusion: FaceLLM showcases the potential of synthetic supervision and opens avenues for domain-specialized, trustworthy multimodal AI systems. It also provides publicly accessible resources for further research.

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [212] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: The paper introduces an advanced 4D human parsing framework with open-vocabulary capabilities, drastically reducing inference time focused on 4D video content.


<details>
  <summary>Details</summary>
Motivation: Current human part segmentation methods rely on closed-set datasets and are computationally expensive, limiting their usage in real-world extended reality applications.

Method: The authors propose three innovations: (1) using mask-based video object tracking to reduce the need to segment all frames, (2) introducing a Mask Validation module for better tracking and identification, and (3) developing a 4D Mask Fusion module that enhances embedding through memory-conditioned attention and logits equalization.

Result: The framework significantly accelerates 4D human-centric parsing tasks, achieving a 93.3% improvement in speed compared to the latest state-of-the-art.

Conclusion: This method offers a powerful and adaptive solution for real-time dynamic human parsing in videos while supporting open-vocabulary parsing, addressing key limitations of prior work in this domain.

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [213] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: The paper focuses on using a convolutional neural network (CNN) to recognize handwritten Devanagari characters, achieving high accuracy rates.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper stems from the lack of advanced digitization tools for the Devanagari script, highlighting the need for effective recognition methods.

Method: The authors propose using two deep convolutional neural network layers trained on the Devanagari handwritten character dataset (DHCD), which contains 36 classes of characters and 1700 images per class.

Result: The proposed model achieved 96.36% accuracy in testing and 99.55% accuracy during training.

Conclusion: The study demonstrates the effectiveness of CNNs for handwritten Devanagari text recognition and provides a robust solution for digitizing this script.

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [214] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS is a novel framework for counterfactual visual explanations, addressing unintended alterations by integrating a causal perspective and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve counterfactual visual explanations by addressing the lack of focus on causality and spurious correlations in current methods.

Method: The framework, CECAS, uses a causally-guided adversarial approach to generate counterfactuals that minimize unwanted perturbations on irrelevant factors.

Result: CECAS demonstrates superior performance compared to existing methods across multiple benchmarks in validity, sparsity, proximity, and realism.

Conclusion: Integrating causal methods in counterfactual generation improves explanation quality and produces more meaningful visual explanations.

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [215] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: CrisisLandMark introduces a large-scale corpus pairing SAR and optical data with textual annotations and presents CLOSP, a framework that aligns SAR and optical imagery for improved retrieval performance. GeoCLOSP further refines retrieval capabilities with geographic specificity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of text-to-image retrieval systems that primarily use RGB data by integrating diverse sensor information, such as SAR and optical multispectral data, for enhanced imagery retrieval.

Method: It introduces the CrisisLandMark dataset and proposes CLOSP, a framework that aligns SAR and optical images using contrastive learning via text, alongside GeoCLOSP, which incorporates geographic coordinates.

Result: CLOSP improves retrieval performance significantly, achieving a 54% enhancement in nDGC over prior models. GeoCLOSP enables specialized retrieval based on geographic-specific information.

Conclusion: Integrating diverse sensor data with geographic context significantly enhances the capability of remote sensing archives, unlocking unique retrieval opportunities for both general tasks and specialized applications.

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [216] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: This paper introduces MCGA, a novel two-stage approach for reconstructing hyperspectral images (HSI) from RGB images, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with directly learning RGB-to-HSI mapping, leading to limitations in transitioning from low to high-dimensional data.

Method: MCGA comprises two stages: (1) learning spectral patterns using multi-scale VQ-VAE and generating a Mixture of Codebooks (MoC), (2) refining the RGB-to-HSI mapping through querying features from MoC with attention mechanisms such as Grayscale-Aware Attention and Quantized Self-Attention.

Result: MCGA improves the reconstruction quality with efficient performance and better adaptation to real-world scenarios through an entropy-based Test-Time Adaptation strategy.

Conclusion: This approach effectively addresses the challenges in RGB-to-HSI transformation, demonstrating superior results and robustness, backed by extensive experiments.

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [217] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: This paper proposes a detector called MessDet that employs a strictly rotation-equivariant network and a novel multi-branch head design to achieve state-of-the-art performance in aerial image object detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current aerial object detection methods, which either rely on data augmentation or approximately rotation-equivariant networks. The study aims to investigate whether strict rotation equivariance can improve detection performance and efficiency.

Method: The authors implement a strictly rotation-equivariant backbone and neck network and introduce a multi-branch head design leveraging rotation-equivariant features. They compare the proposed method against approximately rotation-equivariant networks using quantitative evaluations.

Result: MessDet demonstrates state-of-the-art performance on aerial image datasets DOTA-v1.0, DOTA-v1.5, and DIOR-R, achieving high detection accuracy and efficiency with a low parameter count.

Conclusion: Strict rotation equivariance and a multi-branch head design significantly improve aerial image detection performance while reducing computational complexity, demonstrating the value of these contributions.

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [218] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: The paper introduces Instructional Graphic Designer (IGD), a system that uses natural language instructions to generate editable multimodal graphic design layers.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current graphic design automation methods, including poor legibility, lack of creativity, and the inability to produce editable files.

Method: IGD combines parametric rendering, image asset generation using diffusion models, multimodal understanding for layer prediction and layout, and a standardized platform for multi-scenario design scalability.

Result: IGD achieves effective attribute prediction, layer sequencing, and asset generation, resulting in superior graphic design outputs compared to traditional methods.

Conclusion: IGD represents a scalable and extensible solution for automated graphic design, improving efficiency and flexibility while preserving design quality.

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [219] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: The paper introduces Crucial-Diff, a framework to generate impactful synthetic data for improving detection and segmentation models.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity in fields like medical imaging and autonomous driving, which leads to overfitting and imbalance in datasets for model training.

Method: Utilizes a unified feature extractor (SAFE) and a weakness-aware sample miner (WASM) to generate synthetic samples tailored to improve the performance of downstream models.

Result: Achieved significant metrics improvement including pixel-level AP of 83.63% and F1-MAX of 78.12% on MVTec, and mIoU of 81.64% and mDice of 87.69% on the polyp dataset.

Conclusion: The framework successfully synthesizes diverse and high-quality samples addressing model weaknesses, outperforming prior generative methods in targeted applications.

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [220] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: This paper presents EmRACE-3K, a dataset for testing the embodied reasoning capabilities of vision-language models (VLMs) in complex, interactive tasks involving navigation, object manipulation, and multi-stage goals within photo-realistic environments. Benchmarking with this dataset reveals the limitations of existing advanced VLMs in such settings, with less than 20% success rates in zero-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing advanced vision-language models struggle in embodied settings requiring spatial reasoning, first-person perspectives, and real-time interaction. The motivation is to address this gap by developing tools for testing and improving their embodied reasoning capabilities.

Method: The authors introduce EmRACE-3K, a dataset of 3,000 tasks in virtual environments developed using Unreal Engine. Tasks consist of multi-step trajectories integrating visual observations, instructions, actions, and rationales. They benchmark VLMs using this dataset and fine-tune one model using supervised and reinforcement learning for performance improvement.

Result: State-of-the-art VLMs achieve less than 20% success on EmRACE-3K tasks in zero-shot scenarios, highlighting the challenges in embodied interactions. Fine-tuning one model significantly improves its performance across exploration, reasoning, and multi-stage goal execution categories.

Conclusion: EmRACE-3K exposes key limitations of current VLMs in interactive, embodied settings but offers a platform for advancing their capabilities. The dataset's utility is demonstrated through significant performance gains following targeted fine-tuning of a VLM model.

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [221] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: This paper evaluates the use of large language models (LLMs) like GPT-4o-mini and Gemini 2.0 Flash for zero-shot fashion attribute recognition using the DeepFashion-MultiModal dataset.


<details>
  <summary>Details</summary>
Motivation: Understanding fashion product attributes improves the customer experience and organizes product catalogs, ultimately affecting the 'discovery experience' of retail customers. However, LLMs' capabilities in fine-grained fashion attribute recognition are not yet well-explored.

Method: The paper evaluates the zero-shot performance of GPT-4o-mini and Gemini 2.0 Flash on fashion attribution tasks using only images as input. Performance is measured across 18 categories of fashion attributes using metrics like the macro F1 score.

Result: Gemini 2.0 Flash demonstrates the best overall performance with a macro F1 score of 56.79%, while GPT-4o-mini achieves 43.28%. Detailed error analysis highlights opportunities for improvement.

Conclusion: The study shows that LLMs can be effective in fashion attribute recognition with room for domain-specific fine-tuning. It also sets a foundation for future work in fashion AI and multimodal attribute extraction.

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [222] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: This paper introduces a method to achieve atomic-scale super-resolution imaging for beam-sensitive materials using ultra-low electron doses.


<details>
  <summary>Details</summary>
Motivation: Radiation damage limits the use of electron microscopy for analyzing beam-sensitive materials like proteins and 2D materials.

Method: The authors employ multi-image super-resolution principles and a convolutional neural network to fuse low-resolution images and reconstruct atomic-scale structures.

Result: Their approach, featuring a dual-path attention-guided network, achieves super-resolution comparable to conventional techniques but under significantly lower electron doses.

Conclusion: The study extends the capabilities of 4D-STEM for structural analysis of materials prone to radiation damage, offering a more generalizable imaging solution.

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [223] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: The paper presents KPHD-Net, which utilizes H"older divergence and Dempster-Shafer evidence theory to improve multi-view classification and clustering accuracy, robustness, and reliability.


<details>
  <summary>Details</summary>
Motivation: Current multi-view methods struggle with data noise and inter-modality domain gaps, often relying on KL divergence, which is limited for uncertainty estimation.

Method: KPHD-Net adopts a variational Dirichlet distribution for class probabilities, employs H"older divergence for measuring distribution discrepancies, integrates evidence from views using Dempster-Shafer theory, and combines this with Kalman filtering for future state estimations.

Result: KPHD-Net is shown to outperform state-of-the-art methods in accuracy, robustness, and reliability for classification and clustering tasks through extensive experiments.

Conclusion: The proposed KPHD-Net provides a theoretically robust and practically reliable solution for uncertainty estimation and multi-view fusion, addressing shortcomings of existing methods.

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [224] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: This paper identifies weaknesses in current autoencoders within latent diffusion models (LDMs) and proposes Variational Masked AutoEncoders (VMAEs) to improve them. Integrating VMAEs with LDMs enhances image generation and computational performance.


<details>
  <summary>Details</summary>
Motivation: Latent Diffusion Models (LDMs) excel in image generation, but the optimal autoencoder design has not been fully studied, leading to a gap in understanding their desired properties.

Method: The authors propose Variational Masked AutoEncoders (VMAEs) that leverage hierarchical features from Masked AutoEncoders, then integrate them into the LDM framework, forming Latent Diffusion Models with Masked AutoEncoders (LDMAEs).

Result: Experiments show that LDMAEs significantly improve image generation quality and computational efficiency compared to existing LDMs.

Conclusion: LDMAEs address the limitations of current autoencoders in LDMs, delivering better performance in image synthesis and efficiency, establishing a promising new approach.

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [225] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: The paper introduces 3D Gaussian-based Adversarial Attack (3DGAA), leveraging 3D Gaussian Splatting to create highly effective adversarial objects that substantially reduce detection performance in autonomous driving systems.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in camera-based object detection systems in autonomous driving, and improve the balance between physical realism and attack robustness in adversarial attacks.

Method: The method employs 3D Gaussian Splatting to jointly optimize geometry (shape, scale, rotation) and appearance (color, opacity), while incorporating physical filtering and augmentation modules to ensure physical realism and generalization.

Result: 3DGAA reduces detection mAP from 87.21% to 7.38% and showcases superior performance and transferability compared to prior 3D physical attacks.

Conclusion: 3DGAA provides a robust, physically realistic, and transferable adversarial attack framework, establishing a new benchmark for testing perception systems' safety in autonomous driving.

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [226] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: This paper discusses the advancements, challenges, and datasets of Anti-UAV tracking technologies, primarily focused on computer vision techniques and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: The need for improved Anti-UAV tracking technologies has grown due to the increasing use of UAVs in fields like public safety, border patrol, and environmental monitoring, necessitating efficient and accurate solutions for complex environments.

Method: The paper reviews current Anti-UAV detection and tracking technologies, compiles publicly available datasets, analyzes vision-based and vision-fusion-based algorithms, and identifies future research directions.

Result: A comprehensive synthesis of current Anti-UAV technologies, accessible datasets, and in-depth analysis of detection and tracking algorithms.

Conclusion: The paper provides an overview of existing challenges and progress in Anti-UAV tracking, offering actionable datasets and insights for future innovation in this critical field.

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [227] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: The paper addresses the challenge of object motion in phase shifting profilometry (PSP) with a new method, I-BSC, which improves motion error reduction and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: PSP, a high-precision 3D scanning technique, struggles with motion susceptibility in dynamic environments due to its assumption of static objects. Addressing this limitation enhances its applicability to dynamic measurements.

Method: The authors propose extending the binomial self-compensation (BSC) concept from phase sequences to image sequences (I-BSC), which weights and sums homogeneous fringe images, computing the arctangent function once to improve efficiency and reduce errors.

Result: The proposed I-BSC method outperforms other methods in reducing motion errors, achieves a quasi-single-shot frame rate for 3D reconstruction, accelerates the computational rate significantly compared to P-BSC, and enables faster motion error convergence.

Conclusion: I-BSC presents a computationally efficient and effective solution to PSP's motion susceptibility, improving error reduction and enabling high-resolution, real-time 3D scanning in dynamic scenarios.

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [228] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: This paper addresses the computational challenge in multi-modal model creation by proposing Hyma, a hypernetwork-based framework.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simplify the selection and training of connectors between uni-modal models for multi-modal applications, tackling the complexity due to the growing availability of pretrained models and large-scale datasets.

Method: It introduces Hyma, leveraging hypernetworks for simultaneous uni-modal model selection and connector training, enabling quicker identification of optimal combinations.

Result: Hyma reduced the uni-modal model pair search cost by 10x and achieved competitive performance in connector training compared to traditional methods.

Conclusion: Hyma offers an efficient and effective solution for multi-modal model design, streamlining the process and reducing resource demands.

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [229] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: The paper introduces a personalized text-to-image diffusion model method combining backpropagation and zeroth-order optimization to achieve memory-efficient and high-quality fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enable memory-efficient personalization of text-to-image diffusion models suitable for edge devices while preserving user privacy and maintaining high-quality results.

Method: The framework alternates between backpropagation on low-resolution images (BP-low) and zeroth-order optimization on high-resolution images (ZO-high), guided dynamically by a timestep-aware probabilistic function.

Result: The approach achieves competitive performance with reduced memory usage, enabling high-quality on-device personalization without increasing inference latency.

Conclusion: By integrating BP-low and ZO-high methods, the framework provides a practical solution for scalable, efficient, and privacy-preserving fine-tuning of diffusion models for edge applications.

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [230] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: The paper introduces CoSMo, a multimodal Transformer for Page Stream Segmentation (PSS) in comic books, which outperforms existing models and sets new benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of automated Page Stream Segmentation (PSS) in comic books, a prerequisite for advanced downstream content analysis.

Method: They developed CoSMo in both vision-only and multimodal variants, leveraging annotated data to outperform baseline and general-purpose models across several metrics.

Result: CoSMo performed better than traditional baselines and larger models in F1-Macro, Panoptic Quality, and stream-level metrics, with additional benefits from multimodal features.

Conclusion: CoSMo sets a new state-of-the-art for comic book PSS, emphasizing the value of visual features with added multimodal benefits for ambiguous cases.

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [231] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: The paper introduces a lightweight machine learning model for detecting poultry diseases through fecal image analysis, achieving high accuracy with minimal computational resources, particularly suitable for low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Address the vulnerability of poultry farming to infectious diseases by developing an efficient diagnostic tool suitable for low-resource environments.

Method: The authors utilize multi-color space feature extraction and descriptors, implement PCA and XGBoost for feature selection, and train an ANN classifier for disease detection.

Result: The ANN classifier achieved 95.85% accuracy in disease detection, outperforming deep learning models in terms of resource efficiency.

Conclusion: The proposed approach provides a cost-effective, interpretable, and scalable solution for real-time poultry disease detection, making it ideal for low-resource agricultural settings.

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [232] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS is a fast feed-forward model capable of synthesizing dynamic 4D novel views from monocular videos in one second, combining appearance, geometry, and motion representation.


<details>
  <summary>Details</summary>
Motivation: To unify appearance, geometry, and motion representation in dynamic 3D scenes for efficient view synthesis, reconstruction, and related tasks in a single framework.

Method: MoVieS uses pixel-aligned grids of Gaussian primitives with explicit supervision of their time-varying motion, enabling large-scale training with minimal task-specific supervision.

Result: The model achieves competitive performance on various tasks, such as scene flow estimation and moving object segmentation, while being extremely fast.

Conclusion: MoVieS bridges dynamic geometry reconstruction with novel view synthesis, supporting diverse zero-shot applications and offering significant efficiency gains.

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [233] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: This paper addresses exposure bias in diffusion models by analyzing energy reduction patterns in noisy images and introducing a wavelet-based frequency-domain regulation mechanism.


<details>
  <summary>Details</summary>
Motivation: Exposure bias in diffusion models hinders their generative performance. The study aims to understand and mitigate this bias by analyzing energy dynamics during the diffusion process.

Method: The proposed method uses wavelet transforms to regulate high- and low-frequency subbands separately, based on observed energy patterns, providing a training-free, plug-and-play solution to improve generative quality.

Result: The approach enhances the generative quality of various diffusion models and proves robust against exposure bias across different architectures.

Conclusion: The frequency-domain regulation effectively addresses exposure bias in diffusion models, boosting generative capabilities and model robustness.

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [234] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: The study introduces a two-stage transfer learning strategy using the SegFormer model to improve water body segmentation in remote sensing images, enhancing IoU performance from 25.50% to 64.84%.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of domain shift and small sample sizes in remote sensing image segmentation for water bodies, particularly in complex and data-scarce environments.

Method: A two-stage transfer learning strategy based on the SegFormer model: train on a diverse source domain and fine-tune on a target domain.

Result: The strategy increased IoU for water body segmentation in the Zhada Tulin area from 25.50% to 64.84%.

Conclusion: The proposed method effectively addresses domain discrepancies, offering a robust paradigm for remote sensing segmentation in unique and data-scarce regions.

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [235] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP is introduced to address CLIP's limitations with long-text inputs by using a dual-branch pipeline, regional prompts, and feature alignment. It achieves state-of-the-art results on text retrieval benchmarks and enhances diffusion model performance.


<details>
  <summary>Details</summary>
Motivation: CLIP faces performance challenges on tasks involving long-text inputs due to its limited text encoder input length.

Method: FIX-CLIP employs three novel modules: a dual-branch training pipeline for aligning short and long texts, learnable regional prompts for information extraction, and hierarchical feature alignment in intermediate encoder layers. Additionally, it uses 30M images and MLLMs for generating long-text captions for training.

Result: FIX-CLIP achieves state-of-the-art results on long-text and short-text retrieval benchmarks, demonstrating superior performance.

Conclusion: FIX-CLIP significantly improves the ability of CLIP to handle long-text inputs while preserving performance on short-text cases, and it shows promising application potential for other downstream tasks like diffusion models.

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [236] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: A framework for MCMT tracking maintaining consistent global ID assignment across camera views, leveraging trajectory and appearance data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining consistent identity tracking across multiple cameras for multi-target scenarios.

Method: Single-camera tracking via BoT-SORT, initialization with trajectory-feature matching, global matching strategy for IDs, and spatial validation using 3D positions derived from depth maps and calibration.

Result: Reliable identity assignment across cameras and frames, ensuring seamless tracking consistency.

Conclusion: The proposed system effectively matches tracklets to identities using prioritized strategies and spatial validation, minimizing ID errors across views.

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [237] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: The paper introduces a novel semi-supervised segmentation approach called DEARLi, leveraging foundation models to enhance both recognition and localization, outperforming state-of-the-art methods with fewer resources.


<details>
  <summary>Details</summary>
Motivation: Pixel-level annotation is costly and time-intensive, and semi-supervised methods offer a solution by utilizing both labeled and unlabeled data. The paper aims to explore the integration of foundation models to address label scarcity and enhance efficiency in segmentation tasks.

Method: The method involves unsupervised mask-transformer consistency combined with zero-shot classification of CLIP features to improve recognition. Localization is improved by warming up a class-agnostic decoder using SAM pseudo-labels. This decoupled strategy optimizes recognition and localization separately.

Result: DEARLi achieves 29.9 PQ and 38.9 mIoU on the ADE20K dataset with only 158 labeled images. It outperforms current methods in semi-supervised segmentation by a large margin while using 8x less GPU memory.

Conclusion: The proposed DEARLi model excels in handling large taxonomies and limited labeled datasets in semi-supervised scenarios, offering significant improvements in segmentation performance and resource efficiency.

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [238] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: The paper enhances precision in echocardiographic tissue tracking by addressing motion bias and proposing improved training strategies and a lightweight network, achieving superior accuracy and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing motion estimation methods struggle with complex cardiac motion, and point tracking approaches remain underexplored in echocardiography.

Method: Refining training strategies with tailored augmentations and developing a lightweight model leveraging multi-scale cost volumes from spatial context.

Result: EchoTracker improves position accuracy by 60.7% and reduces trajectory error by 61.5% across heart cycle phases, outperforming many advanced models.

Conclusion: The refined methods and proposed lightweight network show enhanced accuracy and generalization for echocardiographic tracking, supporting clinical application improvements.

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [239] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: This paper introduces a biologically inspired feedback mechanism in neural networks that enables iterative perception refinement, implemented in U-Net. Experiments show improved performance and generalization, especially in limited and noisy data settings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of most artificial neural networks that operate in a purely feedforward manner, lacking the feedback connections vital to biological vision systems.

Method: The authors propose a predictive coding inspired feedback mechanism with recurrent loops from output to input, integrated into a U-Net architecture. Biologically motivated operations like softmax projection and exponential decay are used to stabilize the feedback loop.

Result: The feedback model significantly outperformed the feedforward counterpart in synthetic segmentation tasks under noisy and limited data conditions. It performed above random with only two training examples, whereas the feedforward model needed at least four.

Conclusion: The study demonstrates that feedback mechanisms enhance robustness and data efficiency in neural networks, showing potential for creating more adaptive architectures inspired by biological processes.

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [240] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: SlumpGuard is an AI-powered, video-based system for real-time concrete workability assessment, addressing limitations of traditional manual slump testing.


<details>
  <summary>Details</summary>
Motivation: Traditional slump testing is manual, time-intensive, and inconsistent, hindering real-time concrete workability assessment during construction.

Method: SlumpGuard analyzes video footage of concrete flow from truck chutes using AI to perform automated, real-time workability assessments.

Result: SlumpGuard demonstrated improved efficiency and accuracy in full-batch concrete quality control during real-world deployments.

Conclusion: SlumpGuard provides a practical and modern solution to enhance concrete workability monitoring, improving quality assurance in construction.

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [241] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: The paper proposes a dual-level domain adaptation pipeline for text-based person retrieval using synthetic data. It achieves state-of-the-art results by addressing domain gaps at the image and region levels.


<details>
  <summary>Details</summary>
Motivation: Text-based person retrieval gains importance due to privacy concerns and the high costs of manual annotation. However, domain gaps between synthetic and real-world datasets hinder model performance.

Method: The paper introduces a unified pipeline with two components: Domain-aware Diffusion for adapting images from synthetic to real-world datasets and Multi-granularity Relation Alignment for region-level visual-textual correspondence.

Result: The proposed method achieves state-of-the-art results on popular text-based person retrieval datasets such as CUHK-PEDES, ICFG-PEDES, and RSTPReid.

Conclusion: A dual-level adaptation approach effectively narrows domain gaps, improving performance in text-based person retrieval tasks. The dataset, model, and code are shared publicly.

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [242] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: This paper proposes the Extract Candidate then Predict (ECP) framework, addressing Multimodal Large Language Models' (MLLMs) limitations in high-resolution image tasks due to resolution constraints during training.


<details>
  <summary>Details</summary>
Motivation: MLLMs perform poorly on tasks involving high-resolution images, due to a mismatch in image resolution during training and testing.

Method: ECP employs a two-stage training-free framework: it identifies candidate regions from coarse predictions on downsampled images and refines outputs based on these regions.

Result: ECP achieved significant improvements, including a +21.3% accuracy gain in 4K GUI grounding and minor gains (around +5%) for 4K/8K MLLM perception tasks.

Conclusion: The ECP framework enhances MLLM performance on high-resolution images without compromising fine details and resolves resolution mismatch challenges effectively.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [243] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: The paper introduces a novel strategy, ARL, for enhancing multimodal learning by leveraging imbalanced optimization based on modality-specific variances.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning often performs worse than unimodal learning due to under-optimization caused by modality imbalance. The paper suggests that balanced learning is not optimal and presents an alternative grounded in bias-variance analysis.

Method: The proposed ARL strategy utilizes auxiliary regularizers to measure the prediction variance of each modality encoder, calculates re-weighting coefficients based on these variances, and jointly optimizes biases and multimodal losses without introducing extra parameters.

Result: Experiments on diverse datasets confirm that ARL improves performance and is versatile across various multimodal learning structures and fusion methods.

Conclusion: ARL presents an effective solution for multimodal learning by aligning modality dependency ratios to variance ratios, optimizing bias, and minimizing generalization error, all while maintaining model flexibility.

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [244] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: This paper investigates the role of ethnicity in emotional and micro-expression recognition, challenging the universality of emotional expressions across cultures.


<details>
  <summary>Details</summary>
Motivation: To challenge and revisit Ekman's universality hypothesis regarding emotional expression and explore the role ethnicity plays in micro-expression recognition.

Method: The authors create a cross-cultural micro-expression database annotated with ethnic labels and conduct experimental comparisons between mono-ethnicity and stereo-ethnicity in controlled settings. They propose an ethnicity-sensitive framework for emotional feature learning and provide qualitative analyses.

Result: The findings indicate that ethnic background influences micro-expression recognition, revealing ethnic biases in the emotional universality hypothesis.

Conclusion: The study establishes the need to consider ethnic diversity in micro-expression research and provides an ethnically-aware framework to improve recognition systems.

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [245] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: This paper addresses the under-optimization issue in multimodal learning by introducing a Disentangled Gradient Learning (DGL) framework to decouple the gradient flows between modality encoders and the fusion module.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the underperformance of multimodal learning, even when compared to unimodal learning, which arises due to the optimization conflict between modality encoders and the modality fusion module.

Method: A Disentangled Gradient Learning (DGL) framework is proposed. It truncates cross-modal gradients from the fusion module to the encoder and replaces them with gradients from unimodal loss to isolate and optimize distinct components independently.

Result: The experiments conducted across various modalities, tasks, and frameworks with dense cross-modal interaction confirm the effectiveness and versatility of the DGL framework.

Conclusion: DGL successfully mitigates the optimization conflict, improving the performance of multimodal learning while addressing the underperformance of its components when compared to unimodal learning.

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [246] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: The paper introduces Wardrobe Polyptych LoRA, a model designed for personalized human image generation, achieving precise and consistent identity and clothing detail preservation without computational-heavy approaches.


<details>
  <summary>Details</summary>
Motivation: Personalized human image generation struggles with maintaining precise subject attributes while limiting computational costs in real-time scenarios.

Method: The model focuses on training LoRA layers and employs conditioning on wardrobe details and spatial references, alongside a selective subject region loss to boost fidelity and alignment.

Result: The Wardrobe Polyptych LoRA consistently generates high-fidelity identity-preserving human images, requiring significantly less computational effort compared to existing methods.

Conclusion: Wardrobe Polyptych LoRA is highly effective for personalized human image generation due to its innovative techniques, achieving realistic synthesis with minimal computational demands.

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [247] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: The paper presents VRFNO, an improvement to Reflow for generating high-quality images rapidly by addressing distribution gaps in coupling between noise and images.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the identified limitations of Reflow, particularly the gap in image quality caused by inaccurate deterministic coupling between noise and real images during generation.

Method: VRFNO integrates an encoder and neural velocity field, with innovations like historical velocity term and noise optimization via reparameterization to create optimized couplings for training.

Result: VRFNO significantly improves image quality and generation speed, achieving state-of-the-art performance in both one-step and few-step generation across synthetic and real datasets with varying resolutions.

Conclusion: The proposed VRFNO framework effectively addresses Reflow's shortcomings, offering higher-quality image generation with reduced errors and improved predictive accuracy of trajectory velocities.

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [248] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: The paper presents Spatial Lifting (SL), a method to lift 2D inputs to higher-dimensional spaces for dense prediction, achieving competitive performance with fewer parameters and reduced inference costs.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient, accurate, and reliable methodology for dense prediction tasks, addressing challenges in model complexity and resource consumption.

Method: SL lifts 2D inputs, like images, to a higher-dimensional space, processed by networks like 3D U-Net, enabling dense supervision and prediction quality assessment during training and test time.

Result: SL shows competitive performance across 19 benchmarks in semantic segmentation and depth estimation, reducing model parameters by over 98% and lowering inference costs.

Conclusion: Spatial Lifting offers a novel, efficient paradigm for dense prediction tasks, paving the way for compact and reliable vision models.

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [249] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: The paper introduces a specialized dataset called ProGait for improving vision-based machine learning methods in analyzing gait patterns for individuals with transfemoral prosthetic legs. It also provides benchmark tasks and baseline models, showing improved performance compared to existing pre-trained models.


<details>
  <summary>Details</summary>
Motivation: To address the gap in vision-based machine learning methods which struggle to effectively detect and analyze unique gait patterns and appearances of transfemoral prosthetic legs.

Method: The authors created ProGait, a dataset with 412 video clips of gait analysis trials featuring four above-knee amputees testing prosthetic legs. They also developed benchmark tasks and fine-tuned baseline models tailored for this dataset.

Result: The study demonstrated that models trained using the ProGait dataset showed better performance and generalizability for prosthesis-specific analysis compared to existing pre-trained models.

Conclusion: ProGait represents a significant resource for advancing vision-based machine learning in gait analysis for prosthetic leg users, contributing to improved mobility and life quality for these individuals.

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [250] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD uses foundation models to generate synthetic OOD samples for fine-tuning CLIP models, achieving state-of-the-art OOD detection performance on ImageNet benchmarks.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in vision-language models, differentiating challenging OOD samples close to InD data remains problematic.

Method: SynOOD employs iterative in-painting with MLLMs to refine OOD data near the InD boundary, utilizing gradient-based noise adjustments for sample generation.

Result: SynOOD significantly outperforms previous methods on ImageNet benchmarks, improving AUROC by 2.80% and reducing FPR95 by 11.13%.

Conclusion: SynOOD enhances boundary discrimination between InD and OOD samples with minimal computational overhead, advancing the field of OOD detection using foundation models.

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [251] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: The study reveals that AI-Generated Image Detection (AID) models perform well on controlled datasets but struggle in real-world scenarios. Introducing a new dataset (ITW-SM), the authors improve AID performance through systematic modifications.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for detecting AI-generated images as they reach a realism level capable of deceiving even experts.

Method: The authors analyzed backbone architecture, training data composition, pre-processing strategies, and data augmentation techniques. They introduced ITW-SM, a dataset sourced from social media, and systematically evaluated AID models.

Result: The proposed modifications led to an average AUC improvement of 26.87% in performance under real-world conditions.

Conclusion: The paper highlights the limitations of existing AID models and demonstrates how strategic modifications can significantly enhance detection efficacy in real-world settings.

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [252] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: The paper explores style transfer as a data augmentation method to reduce texture bias and increase robustness in semantic segmentation deep neural networks (DNNs).


<details>
  <summary>Details</summary>
Motivation: Traditional DNNs have biases toward texture in image classification, which can negatively affect robustness and generalization. While style transfer has improved texture bias in classification, it remains unknown whether it has similar benefits for semantic segmentation.

Method: The authors applied style transfer to random Voronoi-cell-based image regions, creating stylized datasets to train semantic segmentation DNNs. Experiments were conducted on convolutional neural networks and transformer architectures using datasets like Cityscapes and PASCAL Context.

Result: Style transfer reduced texture dependency, promoted shape-based feature reliance, and significantly improved robustness against image corruptions and adversarial attacks.

Conclusion: Style transfer can effectively enhance the robustness and generalization of semantic segmentation DNNs across different architectures and datasets, suggesting it as a promising augmentation approach.

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [253] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: The paper introduces a novel attack method called Kaleidoscopic Background Attack (KBA) to reduce the robustness of camera pose estimation models by leveraging symmetrical background patterns.


<details>
  <summary>Details</summary>
Motivation: Camera pose estimation for object-centric scenes with sparse inputs can suffer substantial inaccuracies due to background textures. Exploring how these textures can be optimized to disrupt pose estimation models is a vital task.

Method: The authors propose the Kaleidoscopic Background Attack (KBA), which uses multi-fold symmetrical textures to create adversarial disc patterns. They further introduce a projected orientation consistency loss to enhance the attack's effectiveness.

Result: Tests demonstrated that the adversarial kaleidoscopic backgrounds can successfully degrade the performance of various camera pose estimation models.

Conclusion: Adversarial backgrounds generated using symmetrical textures can significantly impact pose estimation models, highlighting a vulnerability in their design and prompting methods to counter such attacks.

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [254] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: FTCFormer introduces a novel clustering-based module to enhance semantic-based tokenization in vision transformers, improving classification performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models often use grid-based tokens that lack semantic awareness, leading to suboptimal representations of image regions.

Method: The paper proposes mechanisms like DPC-FKNN for clustering centers, SCS for token assignment, and Cmerge for merging, ensuring semantically relevant token allocation.

Result: FTCFormer demonstrates consistent improvements in classification accuracy across 32 datasets with gains ranging from 0.55% to 1.43%.

Conclusion: Clustering-based tokenization enhances vision transformers' effectiveness by prioritizing semantically important image regions and optimizing their representation.

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [255] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR is a novel method for Face Video Restoration that uses reference images and advanced attention mechanisms to improve quality and preserve identity.


<details>
  <summary>Details</summary>
Motivation: Current Face Video Restoration techniques struggle with severe degradation and often fail to maintain fine-grained, identity-specific features, resulting in generic or average-looking outputs.

Method: IP-FVR uses a reference face image for identity conditioning during denoising, employs cross-attention mechanisms for detailed restoration, and addresses identity drift both intra-clip and inter-clip through feedback learning and exponential blending strategies. It also uses a multi-stream negative prompt to enhance restoration accuracy.

Result: Experiments on both synthetic and real-world datasets show that IP-FVR delivers superior quality and maintains identities more effectively compared to existing methods.

Conclusion: IP-FVR demonstrates strong potential for practical applications, achieving state-of-the-art results in face video restoration with consistent identity preservation and refined detail handling.

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [256] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo improves video token representation through semantic clarity and temporal coherence, surpassing previous methods in video understanding tasks.


<details>
  <summary>Details</summary>
Motivation: To address issues of semantic indistinctness and temporal incoherence in video token extraction for Video MLLMs.

Method: DisCo combines a Visual Concept Discriminator (VCD) and a Temporal Focus Calibrator (TFC) for enhanced video token encapsulation.

Result: DisCo significantly surpasses prior state-of-the-art performance across video understanding tasks, offering improved token efficiency.

Conclusion: DisCo effectively resolves central challenges in video tokenization, enhancing video representation for multimodal language models.

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [257] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: This paper presents a new gloss-free method for Sign Language Translation (SLT) using a dual visual encoder framework, achieving state-of-the-art results on a benchmark.


<details>
  <summary>Details</summary>
Motivation: Gloss annotations in SLT are costly and inadequate for capturing continuous signing complexity, necessitating a gloss-free alternative.

Method: A dual visual encoder framework is used, leveraging contrastive visual-language pretraining with two visual backbones and an encoder-decoder model for SLT.

Result: The proposed method outperforms single-stream variants and achieves the highest BLEU-4 score on the Phoenix-2014T benchmark for gloss-free SLT.

Conclusion: The dual visual encoder framework mitigates reliance on gloss annotations and sets a new performance standard for gloss-free SLT methods.

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [258] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: The paper presents IMD, a framework that uses pre-trained diffusion models to resolve misalignment issues in feature matching caused by foundation models. It achieves state-of-the-art performance and significant improvements on multi-instance benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the misalignment issue arising when using vision foundation models for feature matching, especially for multi-instance scenarios. This misalignment stems from the difference between single-image understanding and the cross-image understanding needed for feature matching.

Method: The authors propose IMD, a framework that incorporates pre-trained generative-based diffusion models to better capture instance-level details. It introduces a cross-image interaction prompting module facilitated by generative model prompts, enabling bidirectional communication between image pairs. Additionally, a new benchmark, IMIM, is proposed to measure multi-instance performance.

Result: IMD achieves state-of-the-art results on commonly used benchmarks and demonstrates a significant 12% improvement on the IMIM benchmark, showing effective mitigation of the misalignment issue.

Conclusion: The study demonstrates that using diffusion models and interactive prompting mechanisms effectively aligns vision foundation models for feature matching, providing substantial performance gains in multi-instance scenarios.

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [259] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: This paper introduces QLIP, a method for efficient diffusion model quantization using text prompts to optimize layer precision, reducing computational complexity while maintaining image generation quality.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of diffusion models limits their applicability in resource-constrained environments, necessitating a more efficient quantization approach that incorporates text prompts.

Method: The proposed method, QLIP, uses text prompts to dynamically guide bit precision selection for model layers at each time step, integrating easily with existing quantization strategies.

Result: QLIP reduces computational overhead and enhances the quality of generated images, as confirmed by experiments on multiple datasets.

Conclusion: QLIP effectively improves the efficiency of diffusion models in generating high-quality images, especially in constrained environments, through a novel prompt-guided quantization methodology.

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [260] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet is a novel architecture designed for enhancing wall segmentation in floorplans using domain-specific feature maps.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization ability of wall segmentation in floorplans.

Method: FGSSNet utilizes a U-Net backbone and a multi-headed feature extractor trained to encode texture and width features of wall patches, injecting these features into U-Net's latent space.

Result: Experiments demonstrate superior performance in wall segmentation compared to a vanilla U-Net.

Conclusion: FGSSNet validates the effectiveness of using guided features for improving semantic segmentation tasks, specifically wall segmentation in floorplans.

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [261] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: The paper proposes VRGAdapter, a novel textual adapter leveraging random graph models to represent diverse semantic knowledge and inter-class relationships for vision-language tasks, backed by an uncertainty-guided fusion scheme.


<details>
  <summary>Details</summary>
Motivation: Challenges exist in capturing varied semantic information and inter-class relationships from textual descriptions using existing deterministic adapters.

Method: The proposed VRGAdapter uses a Vertex Random Knowledge Graph (VRKG) along with probabilistic message propagation and reparameterized sampling for textual adapter learning, complemented by an uncertainty-guided multi-model fusion scheme.

Result: Experiments on benchmark datasets reveal that VRGAdapter significantly improves performance across various vision-language tasks.

Conclusion: VRGAdapter is a general solution that effectively utilizes diverse semantic knowledge and inter-class dynamics, outperforming traditional deterministic adapters.

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [262] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: The paper introduces the Fine-Grained Zero-Shot Object Detection (FG-ZSD) problem, proposes a novel method called MSHC, and evaluates it on a newly created FGZSD-Birds dataset, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of detecting fine-grained object classes with subtle differences, which current Zero-Shot Detection (ZSD) models fail to handle effectively, especially in cases involving similar-looking objects like bird species.

Method: The proposed method, MSHC, incorporates an improved two-stage detector with a multi-level semantics-aware embedding alignment loss. This ensures a tight alignment between visual and semantic spaces for better fine-grained object detection.

Result: The authors built the FGZSD-Birds dataset specifically for this task and demonstrated through extensive experiments that their MSHC method outperforms existing ZSD models on fine-grained object detection.

Conclusion: The study successfully addresses the limitations of existing ZSD approaches in fine-grained scenarios by proposing a new benchmark and an improved detection method, setting the stage for further research in FG-ZSD.

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [263] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: The paper introduces FOCAL, a test-time framework enhancing visual perception robustness via internet-scale visual priors, without retraining or architectural changes.


<details>
  <summary>Details</summary>
Motivation: Current methods for visual perception invariance often depend on specialized architectures or predefined augmentations, which limit generalization across diverse transformations.

Method: FOCAL generates and optimizes visual transformations to align with visually typical or 'canonical' views, leveraging visual priors from foundation models like CLIP and SAM.

Result: FOCAL improves robustness against challenging transformations, such as rotations, lighting changes, and day-night variability, without requiring retraining.

Conclusion: Transform-specific training is not always necessary; FOCAL provides a scalable alternative to achieve perceptual invariance using online visual priors.

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [264] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: This paper integrates Topological Data Analysis (TDA) with deep learning models for remote sensing classification, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Topological Data Analysis (TDA) effectively describes complex datasets, but its integration with CNNs, which are biased towards local features, remains unexplored in remote sensing applications.

Method: The authors propose a TDA-based feature engineering pipeline to combine TDA features with deep learning models, applying the approach to the EuroSAT and RESISC45 datasets.

Result: The proposed method improves ResNet18's performance, achieving 99.33% accuracy on EuroSAT (1.44% improvement) and 1.82% improvement on RESISC45, surpassing stronger and larger models like ResNet50 and XL Vision Transformers.

Conclusion: Integrating TDA features with deep learning enhances performance even for datasets without explicit topological structures, proving TDA's versatility and establishing a new benchmark for satellite scene classification.

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [265] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: The paper connects algebra, numerical computation, and computer vision by examining solution methods for parametric systems of algebraic equations, relevant to robust model-fitting in computer vision.


<details>
  <summary>Details</summary>
Motivation: Understanding the difficulty and solutions of parametric systems of polynomial or rational functions for applications like RanSaC in computer vision.

Method: Analysis of parametric systems to measure complexity and develop practical problem-solving methods.

Result: Steps have been taken over the last 5+ years to determine intrinsic difficulty and improve solutions.

Conclusion: The study blends theoretical and practical advancements, contributing to robust model-fitting and algebraic problem-solving.

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [266] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: The paper presents SC-AGIQA, a framework for evaluating text-image alignment and visual quality in AI-generated images, addressing semantic misalignment and subtle detail perception issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy of evaluating AGI quality by overcoming limitations in existing methods, particularly semantic misalignment and missing detail perception.

Method: SC-AGIQA uses a Text-assisted Semantic Alignment Module (TSAM) leveraging MLLMs for semantic checks, and a Frequency-domain Fine-Grained Degradation Perception Module (FFDPM) to analyze fine visual details in the frequency domain.

Result: Experiments on benchmark datasets show SC-AGIQA outperforms SOTA methods in evaluating text-image consistency and perceptual quality.

Conclusion: The approach effectively advances AGI quality assessment by addressing key challenges and improving evaluation metrics for both alignment and detail perception.

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [267] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal is a novel framework for reconstructing animatable 3D animals from videos without relying on sparse keypoint annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods require laborious keypoint annotations and rely on unreliable keypoint detectors trained on limited data, which motivates the need for a more efficient and less annotation-dependent solution.

Method: 4D-Animal employs a dense feature network to map 2D data to SMAL parameters and uses hierarchical alignment incorporating silhouette, part-level, pixel-level, and temporal cues to improve quality and coherence.

Result: 4D-Animal surpasses existing model-based and model-free approaches in terms of accuracy and temporal coherence.

Conclusion: This framework advances 3D animal reconstruction by eliminating dependency on keypoints and creating high-quality, reusable 3D assets for broader applications.

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [268] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: This work introduces CoralVQA, the first large-scale Visual Question Answering (VQA) dataset focused on coral reef images, aimed at supporting marine conservation through improved vision-language reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to aid coral reef conservation by addressing the challenges of interpreting coral reef images, specifically through the development of a domain-specific VQA dataset for better understanding and analysis.

Method: The researchers developed CoralVQA, consisting of 12,805 coral images and 277,653 question-answer pairs, using a semi-automatic data pipeline created in collaboration with marine biologists for scalability and high-quality data.

Result: CoralVQA highlights novel challenges in vision-language reasoning and serves as a benchmark for studying coral reef imagery; evaluations of current LVLMs reveal both their limitations and potential in this domain.

Conclusion: CoralVQA sets the stage for future advancements in LVLMs, specifically tailored toward coral conservation and improving ecological image analysis.

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [269] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: The paper presents RAPNet, a novel pansharpening technique using content-adaptive convolution for higher performance.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of CNNs in pansharpening due to their uniform application of convolution, neglecting spatial content variations.

Method: Introduced RAPNet with RAPConv for spatially adaptive kernels and PAN-DFF module for balancing spatial detail and spectral fidelity.

Result: RAPNet showed superior performance against prior methods using quantitative and qualitative measures on public datasets.

Conclusion: RAPNet's adaptive components demonstrably enhance pansharpening via evaluations and ablation studies.

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [270] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: This paper introduces RefSTAR, a method addressing identity preservation in blind facial image restoration by using effective feature integration from high-quality references.


<details>
  <summary>Details</summary>
Motivation: Existing facial restoration methods struggle with restoring identity features mainly due to improper handling of detailed textures.

Method: RefSTAR involves a reference selection module (RefSel), feature fusion for transferring information from references, and a reconstruction mechanism with redesigned cycle consistency loss.

Result: Experiments on multiple models demonstrate superior identity preservation and reference feature transfer quality using the proposed approach.

Conclusion: The RefSTAR method effectively enhances blind facial restoration by improving the integration of features from high-quality reference images, resulting in better identity preservation and restoration quality.

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [271] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: The paper introduces GT-Loc, a method for predicting image timestamps and geo-locations using a shared embedding space and novel temporal metric-learning.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting image timestamps and geo-locations is crucial for applications like metadata correction, retrieval, and digital forensics, yet current methods do not adequately integrate these tasks and their interdependencies.

Method: GT-Loc employs separate encoders for images, time, and location, aligning embeddings in a shared space. It introduces a temporal metric-learning model to account for cyclic time differences over a toroidal surface.

Result: GT-Loc outperforms prior timestamp prediction methods and demonstrates competitive performance in geo-localization tasks. The unified embedding space also enables advanced uses like compositional and text-based image retrieval.

Conclusion: The proposed joint optimization approach enhances both timestamp and geo-location predictions, showing its utility across different applications.

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [272] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: The paper introduces a confidence-based self-distillation method for polyp segmentation that achieves superior results without requiring additional computation during testing.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art deep learning models for polyp detection often demand significant resources and struggle with overfitting and generalization across diverse datasets.

Method: The authors propose a confidence-based self-distillation strategy that leverages previous iteration data within a batch using a dynamic confidence coefficient, avoiding extra computation or memory at test time.

Result: The proposed approach achieves better performance than state-of-the-art methods and demonstrates strong generalization across datasets from various clinical centers.

Conclusion: The novel self-distillation method offers a resource-efficient solution for polyp segmentation, overcoming limitations of existing models and addressing generalization challenges.

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [273] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: A new benchmark for retinal anomaly detection is introduced to address existing limitations in evaluating methods and datasets. The proposed approach, NFM-DRA, sets a new state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome shortcomings in retinal anomaly detection research, such as limited datasets, inadequate test setups, and the underutilization of labeled abnormal and unlabeled data.

Method: A benchmark for retinal anomaly detection was developed, categorizing methods and introducing NFM-DRA—a fully supervised approach integrated with Normal Feature Memory.

Result: The NFM-DRA approach enhances performance, addresses unseen anomaly challenges, and sets a new state-of-the-art in retinal anomaly detection.

Conclusion: The research fills a critical gap in the field by creating a systematic benchmark and advancing methodology, providing opportunities for more robust evaluations and improved detection techniques.

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [274] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: The paper evaluates and compares methods for integrating camera geometry into multi-view transformers for 3D computer vision tasks, introducing Projective Positional Encoding (PRoPE) for enhanced performance.


<details>
  <summary>Details</summary>
Motivation: There is a need to effectively integrate geometric relationships between viewpoints into multi-view transformers for better 3D perception in computer vision tasks.

Method: The study compares techniques like raymap encodings, relative pose encodings, and a novel encoding method (PRoPE) that incorporates complete camera frustums as relative positional encodings.

Result: PRoPE outperforms other methods in various tasks, including novel view synthesis, stereo depth estimation, and spatial cognition, while also generalizing well to diverse scenarios.

Conclusion: Conditioning multi-view transformers on camera geometry, especially using PRoPE, significantly improves their performance and generalization capabilities across different computer vision tasks.

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [275] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: This paper uses high-resolution satellite imagery and deep transfer learning to map 21 million crop fields in Mozambique, achieving a high level of accuracy and providing valuable insights into agricultural patterns.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of designing sustainable policies for smallholder agriculture by improving the understanding of cropland distribution and field sizes, especially in complex systems.

Method: The researchers employed very-high-resolution Earth observation data (1.5 m) and deep transfer learning techniques to accurately delineate crop fields across Mozambique, requiring minimal reference data and ensuring transferability.

Result: The study produced the first national dataset of 21 million individual crop fields in Mozambique with 93% accuracy, revealing details about fragmented agricultural regions and variations in field size.

Conclusion: Field size is a crucial indicator of socio-economic and environmental agricultural outcomes, highlighting trade-offs in livelihoods, food production, and biodiversity, especially in diverse farming ecosystems.

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [276] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: The paper introduces the ReVQ framework, transforming pre-trained VAEs into VQ-VAEs efficiently, drastically cutting computational costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of training high-compression-rate VQ-VAEs, making development less GPU-intensive for multimodal large models.

Method: ReVQ leverages pre-trained VAEs, introducing channel multi-group quantization and post rectifiers to enhance codebook capacity and minimize quantization errors.

Result: ReVQ compresses ImageNet images into 512 tokens with competitive reconstruction quality (rFID = 1.06), reducing GPU training requirements significantly.

Conclusion: ReVQ offers an efficient and cost-effective approach for training VQ-VAEs, setting a new benchmark in efficiency and reconstruction quality trade-offs.

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [277] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: This paper proposes a self-supervised method for identifying chimpanzee faces from camera-trap footage without requiring labeled data, showing strong performance compared to supervised approaches.


<details>
  <summary>Details</summary>
Motivation: The manual identification of individual animals from camera-trap footage is a bottleneck in wildlife monitoring due to the time and effort required.

Method: The study employs the DINOv2 framework with Vision Transformers to learn face embeddings from automatically cropped and unlabelled chimpanzee face images, relying entirely on self-supervised learning.

Result: The new method achieves superior open-set re-identification performance compared to supervised baselines on datasets like Bossou.

Conclusion: The approach highlights the potential of self-supervised learning for scalable biodiversity monitoring and supports non-invasive population study techniques.

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [278] [FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline](https://arxiv.org/abs/2507.10367)
*Jingwei Xu,Junbin Kang,Mingkai Dong,Mingyu Liu,Lu Zhang,Shaohong Guo,Ziyan Qiu,Mingzhen You,Ziyi Tian,Anqi Yu,Tianhong Ding,Xinwei Hu,Haibo Chen*

Main category: cs.DC

TL;DR: FalconFS introduces a DFS with a stateless-client architecture, replacing client-side caching with server-side path resolution and optimizations, achieving significant throughput improvements in deep learning pipelines.


<details>
  <summary>Details</summary>
Motivation: Existing client-side caching in DFSs is found to be ineffective and memory-intensive in deep learning pipelines.

Method: FalconFS employs server-side path resolution using hybrid metadata indexing, lazy namespace replication, concurrent request merging, and VFS shortcuts.

Result: FalconFS demonstrates up to 5.72× throughput improvements for small file I/O and up to 12.81× throughput for deep learning training, outperforming CephFS and Lustre.

Conclusion: FalconFS provides a scalable, high-performance solution for deep learning pipelines, validated in Huawei's production environment with 10,000 NPUs for one year.

Abstract: Client-side metadata caching has long been considered an effective method for
accelerating metadata operations in distributed file systems (DFSs). However,
we have found that client-side state (e.g., caching) is not only ineffective
but also consumes valuable memory resources in the deep learning pipelines. We
thus propose FalconFS, a DFS optimized for deep learning pipelines with the
stateless-client architecture. Specifically, instead of performing client-side
path resolution and caching, FalconFS efficiently resolves paths on the server
side using hybrid metadata indexing and lazy namespace replication. FalconFS
also boosts server concurrency with concurrent request merging and provides
easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show
that FalconFS achieves up to 5.72$\times$ throughput for small file read/write
and up to 12.81$\times$ throughput for deep learning model training. FalconFS
has been running in Huawei autonomous driving system's production environment
with 10,000 NPUs for one year.

</details>


### [279] [MQFQ-Sticky: Fair Queueing For Serverless GPU Functions](https://arxiv.org/abs/2507.08954)
*Alexander Fuerst,Siddharth Anil,Vishakha Dixit,Purushottam,Kulkarni,Prateek Sharma*

Main category: cs.DC

TL;DR: The paper addresses integrating GPUs into Functions as a Service (FaaS) systems and introduces a scheduling approach, MQFQ-Sticky, achieving up to 20x reduced latency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance FaaS systems, which lack full GPU support, despite GPUs’ potential to accelerate applications like machine learning and scientific computing.

Method: The authors propose a system using I/O scheduling principles (fair queuing and anticipatory scheduling) and develop MQFQ-Sticky for function scheduling and GPU memory management within containerized sandboxes.

Result: The proposed approach demonstrated a 2x to 20x reduction in function latency compared to existing CPU and GPU queueing methods in various workloads.

Conclusion: Integrating GPUs into FaaS can be efficiently realized by adapting I/O scheduling techniques, making it feasible for dynamic workloads without code modification, with significant performance gains.

Abstract: Hardware accelerators like GPUs are now ubiquitous in data centers, but are
not fully supported by common cloud abstractions such as Functions as a Service
(FaaS). Many popular and emerging FaaS applications such as machine learning
and scientific computing can benefit from GPU acceleration. However, FaaS
frameworks (such as OpenWhisk) are not capable of providing this acceleration
because of the impedance mismatch between GPUs and the FaaS programming model,
which requires virtualization and sandboxing of each function. The challenges
are amplified due to the highly dynamic and heterogeneous FaaS workloads. This
paper presents the design and implementation of a FaaS system for providing GPU
acceleration in a black-box manner (without modifying function code). Running
small functions in containerized sandboxes is challenging due to limited GPU
concurrency and high cold-start overheads, resulting in heavy queueing of
function invocations. We show how principles from I/O scheduling, such as fair
queuing and anticipatory scheduling, can be translated to function scheduling
on GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory
management approach, which balances the tradeoffs between locality, fairness,
and latency. Empirical evaluation on a range of workloads shows that it reduces
function latency by 2x to 20x compared to existing GPU and CPU queueing
policies.

</details>


### [280] [Lightweight Federated Learning over Wireless Edge Networks](https://arxiv.org/abs/2507.09546)
*Xiangwang Hou,Jingjing Wang,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato*

Main category: cs.DC

TL;DR: The paper introduces LTFL, a lightweight federated learning framework designed for wireless networks, improving performance through model pruning, gradient quantization, and transmission power control.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of deploying federated learning in wireless networks, including communication overhead, privacy concerns, and the need for efficient resource management.

Method: The method involves integrating techniques like model pruning, gradient quantization, and transmission power control into the federated learning framework. Additionally, a convergence gap expression is derived, and an optimization problem is solved using Bayesian techniques and analytical solutions.

Result: The proposed LTFL framework demonstrates superior performance over state-of-the-art methods in experiments using real-world datasets.

Conclusion: LTFL effectively minimizes convergence gaps while meeting delay and energy constraints. It represents a significant improvement for federated learning in wireless networks.

Abstract: With the exponential growth of smart devices connected to wireless networks,
data production is increasing rapidly, requiring machine learning (ML)
techniques to unlock its value. However, the centralized ML paradigm raises
concerns over communication overhead and privacy. Federated learning (FL)
offers an alternative at the network edge, but practical deployment in wireless
networks remains challenging. This paper proposes a lightweight FL (LTFL)
framework integrating wireless transmission power control, model pruning, and
gradient quantization. We derive a closed-form expression of the FL convergence
gap, considering transmission error, model pruning error, and gradient
quantization error. Based on these insights, we formulate an optimization
problem to minimize the convergence gap while meeting delay and energy
constraints. To solve the non-convex problem efficiently, we derive closed-form
solutions for the optimal model pruning ratio and gradient quantization level,
and employ Bayesian optimization for transmission power control. Extensive
experiments on real-world datasets show that LTFL outperforms state-of-the-art
schemes.

</details>


### [281] [Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks](https://arxiv.org/abs/2507.09926)
*Zixuan Song,Zhishu Shen,Xiaoyu Zheng,Qiushi Zheng,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: The paper develops a framework for managing workloads in LEO satellite networks to enhance efficiency and task distribution.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies due to limited onboard satellite resources and uneven distributions of computational workloads in LEO satellite networks.

Method: It introduces a dynamic multi-region division framework using Genetic Algorithm (GA), adaptive routing, and Multi-Agent Deep Deterministic Policy Gradient (MA-DDPG) for task management.

Result: The proposed framework improves task delay, energy consumption per task, and task completion rate in simulations compared to other methods.

Conclusion: The framework effectively optimizes resource utilization and task processing in LEO satellite networks, demonstrating significant performance gains.

Abstract: As a key complement to terrestrial networks and a fundamental component of
future 6G systems, Low Earth Orbit (LEO) satellite networks are expected to
provide high-quality communication services when integrated with ground-based
infrastructure, thereby attracting significant research interest. However, the
limited satellite onboard resources and the uneven distribution of
computational workloads often result in congestion along inter-satellite links
(ISLs) that degrades task processing efficiency. Effectively managing the
dynamic and large-scale topology of LEO networks to ensure balanced task
distribution remains a critical challenge. To this end, we propose a dynamic
multi-region division framework for intelligent task management in LEO
satellite networks. This framework optimizes both intra- and inter-region
routing to minimize task delay while balancing the utilization of computational
and communication resources. Based on this framework, we propose a dynamic
multi-region division algorithm based on the Genetic Algorithm (GA), which
adaptively adjusts the size of each region based on the workload status of
individual satellites. Additionally, we incorporate an adaptive routing
algorithm and a task splitting and offloading scheme based on Multi-Agent Deep
Deterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving
tasks. Simulation results demonstrate that our proposed framework outperforms
comparative methods in terms of the task delay, energy consumption per task,
and task completion rate.

</details>


### [282] [EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning](https://arxiv.org/abs/2507.10026)
*Zhifei Xu,Zhiqing Tang,Jiong Lou,Zhi Yao,Xuan Xie,Tian Wang,Yinglong Wang,Weijia Jia*

Main category: cs.DC

TL;DR: The study introduces the QoS-aware EAT (Edge-collaborative AIGC Task scheduling) algorithm to efficiently process Generative AI tasks in edge servers, reducing latency by up to 56%.


<details>
  <summary>Details</summary>
Motivation: The need to address delays and inefficiencies in deploying AI-Generated Content (AIGC) services at the network edge due to high QoS demands, underutilized resources, and challenges in balancing inference latency and quality.

Method: The paper proposes a reinforcement learning-based EAT algorithm leveraging an attention layer for load and queue information extraction, a diffusion-based policy network for scheduling, and formulating the problem as gang scheduling.

Result: Experimental results show that the EAT algorithm reduces inference latency by up to 56% compared to baseline methods.

Conclusion: The EAT algorithm effectively improves AIGC task scheduling at edge servers by reducing latency and optimizing resource utilization, making it advantageous for high-QoS environments.

Abstract: The growth of Artificial Intelligence (AI) and large language models has
enabled the use of Generative AI (GenAI) in cloud data centers for diverse
AI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce
unavoidable delays and substantial resource overhead, which are unsuitable for
users at the network edge with high QoS demands. Deploying AIGC services on
edge servers reduces transmission times but often leads to underutilized
resources and fails to optimally balance inference latency and quality. To
address these issues, this paper introduces a QoS-aware
\underline{E}dge-collaborative \underline{A}IGC \underline{T}ask scheduling
(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to
various edge servers, formulating it as a gang scheduling problem that balances
inference latency and quality while considering server heterogeneity, such as
differing model distributions and cold start issues. 2) We propose a
reinforcement learning-based EAT algorithm that uses an attention layer to
extract load and task queue information from edge servers and employs a
diffusion-based policy network for scheduling, efficiently enabling model
reuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm
to divide tasks and distribute them across multiple edge servers for
processing. Experimental results based on our system and large-scale
simulations show that our EAT algorithm can reduce inference latency by up to
56\% compared to baselines. We release our open-source code at
https://github.com/zzf1955/EAT.

</details>


### [283] [ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism](https://arxiv.org/abs/2507.10069)
*Zedong Liu,Shenggan Cheng,Guangming Tan,Yang You,Dingwen Tao*

Main category: cs.DC

TL;DR: The paper introduces Elastic Multimodal Parallelism (EMP) and ElasticMM to improve inference efficiency and resource utilization for Multimodal Large Language Models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: Current serving architectures for MLLMs are inefficient, struggling with mixed requests and heterogeneous workloads, leading to high latency and low resource utilization.

Method: The authors propose Elastic Multimodal Parallelism (EMP) with three main innovations: modality-aware load balancing, elastic partition scheduling, and multimodal prefix caching for efficient inference.

Result: ElasticMM reduces time-to-first-token (TTFT) latency by up to 4.2x and boosts throughput by 3.2-4.5x compared to state-of-the-art systems.

Conclusion: ElasticMM successfully addresses serving inefficiencies for MLLMs, enabling scalable, high-performance inference while meeting service-level objectives.

Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images,
videos, and audio by incorporating feature extractors and projection modules.
However, these additional components -- combined with complex inference
pipelines and heterogeneous workloads -- introduce significant inference
overhead. Therefore, efficiently serving MLLMs remains a major challenge.
Current tightly coupled serving architectures struggle to distinguish between
mixed request types or adapt parallelism strategies to different inference
stages, leading to increased time-to-first-token (TTFT) latency and poor
resource utilization. To address this, we propose Elastic Multimodal
Parallelism (EMP), a new serving paradigm that elastically adapts to resource
heterogeneity across request types and inference stages. Building upon EMP, we
develop ElasticMM, an MLLM serving system that (1) separates requests into
independent modality groups with dynamic resource allocation via a
modality-aware load balancer; (2) decouples inference stages and enables
parallelism adjustment and adaptive scaling via elastic partition scheduling;
and (3) improves inference efficiency through unified multimodal prefix caching
and non-blocking encoding. Experiments on diverse real-world datasets show that
ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by
up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level
objectives (SLOs).

</details>


### [284] [Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality](https://arxiv.org/abs/2507.10139)
*Filipe Miguel Gonçalves de Almeida,CJ Carey,Hendrik Fichtenberger,Jonathan Halcrow,Silvio Lattanzi,André Linhares,Tao Meng,Ashkan Norouzi-Fard,Nikos Parotsidis,Bryan Perozzi,David Simcha*

Main category: cs.DC

TL;DR: The paper introduces Dynamic Grale Using ScaNN, a system to construct large-scale graphs dynamically and with low latency, suitable for use cases requiring real-time updates.


<details>
  <summary>Details</summary>
Motivation: Existing systems like Grale are efficient for offline graph construction but are unsuitable for dynamic environments requiring low-latency updates.

Method: The authors developed Dynamic Grale Using ScaNN, a system designed for dynamic settings, inheriting Grale's attributes while achieving tens of milliseconds latency per request.

Result: Dynamic Grale Using ScaNN has been deployed in over 10 applications at Google, significantly boosting graph construction efficiency and enabling faster detection of harmful apps in Android Security.

Conclusion: Dynamic Grale Using ScaNN is effective in real-time graph updating, expanding the applicability of graph-learning systems to dynamic industrial scenarios.

Abstract: Learning and constructing large-scale graphs has attracted attention in
recent decades, resulting in a rich literature that introduced various systems,
tools, and algorithms. Grale is one of such tools that is designed for offline
environments and is deployed in more than 50 different industrial settings at
Google. Grale is widely applicable because of its ability to efficiently learn
and construct a graph on datasets with multiple types of features. However, it
is often the case that applications require the underlying data to evolve
continuously and rapidly and the updated graph needs to be available with low
latency. Such setting make the use of Grale prohibitive. While there are
Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low
latency, they are mostly limited to similarities over a single embedding.
  In this work, we introduce a system that inherits the advantages and the
quality of Grale, and maintains a graph construction in a dynamic setting with
tens of milliseconds of latency per request. We call the system Dynamic Grale
Using ScaNN (Dynamic GUS). Our system has a wide range of applications with
over 10 deployments at Google. One of the applications is in Android Security
and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful
applications 4 times faster, before they can reach users.

</details>


### [285] [Past-Future Scheduler for LLM Serving under SLA Guarantees](https://arxiv.org/abs/2507.10150)
*Ruihao Gong,Shihao Bai,Siyu Wu,Yunqian Fan,Zaijun Wang,Xiuhong Li,Hailong Yang,Xianglong Liu*

Main category: cs.DC

TL;DR: The paper introduces the Past-Future scheduler to enhance LLM service frameworks by accurately estimating memory requirements and balancing request management, showcased in the open-source framework LightLLM.


<details>
  <summary>Details</summary>
Motivation: Existing memory management in LLM applications struggles with balancing request queuing and evictions, severely impacting throughput and SLA adherence.

Method: The authors propose the Past-Future scheduler, which uses historical memory usage data and future memory point projections to optimize memory estimates and request handling.

Result: LightLLM, implementing the Past-Future scheduler, achieves 2-3× higher goodput compared to other schedulers under heavy service loads.

Conclusion: The proposed scheduler consistently outperforms traditional methods in maintaining high service throughput, making it suitable for diverse LLM application scenarios.

Abstract: The exploration and application of Large Language Models (LLMs) is thriving.
To reduce deployment costs, continuous batching has become an essential feature
in current service frameworks. The effectiveness of continuous batching relies
on an accurate estimate of the memory requirements of requests. However, due to
the diversity in request output lengths, existing frameworks tend to adopt
aggressive or conservative schedulers, which often result in significant
overestimation or underestimation of memory consumption. Consequently, they
suffer from harmful request evictions or prolonged queuing times, failing to
achieve satisfactory throughput under strict Service Level Agreement (SLA)
guarantees (a.k.a. goodput), across various LLM application scenarios with
differing input-output length distributions. To address this issue, we propose
a novel Past-Future scheduler that precisely estimates the peak memory
resources required by the running batch via considering the historical
distribution of request output lengths and calculating memory occupancy at each
future time point. It adapts to applications with all types of input-output
length distributions, balancing the trade-off between request queuing and
harmful evictions, thereby consistently achieving better goodput. Furthermore,
to validate the effectiveness of the proposed scheduler, we developed a
high-performance LLM serving framework, LightLLM, that implements the
Past-Future scheduler. Compared to existing aggressive or conservative
schedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\times$
higher goodput than other schedulers under heavy loads. LightLLM is open source
to boost the research in such direction (https://github.com/ModelTC/lightllm).

</details>


### [286] [Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning](https://arxiv.org/abs/2507.10259)
*Chengze Du,Zhiwei Yu,Heng Xu,Haojie Wang,Bo liu,Jialong Li*

Main category: cs.DC

TL;DR: TORTA improves GPU-based large language model services by introducing a spatiotemporal scheduling framework to handle dynamic workloads efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing GPU scheduling systems fail to account for dynamic changes in task demand and resource availability, leading to inefficient operations and poor performance.

Method: TORTA uses a two-layer architecture: a macro-level scheduler employing reinforcement learning and optimal transport for task distribution, and a micro-level allocator for task assignment within regions.

Result: TORTA achieves up to 15% lower inference response times, improves load balance by 4-5%, and reduces operational costs by 10-20% in comparison to existing methods.

Conclusion: TORTA’s spatiotemporal approach enhances GPU utilization and responsiveness, demonstrating significant advancements over current scheduling strategies.

Abstract: The rapid growth of large language model (LLM) services imposes increasing
demands on distributed GPU inference infrastructure. Most existing scheduling
systems rely on the current system state to make decisions, without considering
how task demand and resource availability evolve over time. This lack of
temporal awareness leads to inefficient GPU utilization, high task migration
overhead, and poor system responsiveness under dynamic workloads. In this work,
we identify the fundamental limitations of these instantaneous-state-only
scheduling approaches and propose Temporal Optimal Resource scheduling via
Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling
framework that captures both long-term workload patterns and short-term
execution constraints. It adopts a two-layer design: a macro-level scheduler
leverages reinforcement learning and optimal transport to coordinate
inter-region task distribution, while a micro-level allocator refines
task-to-server assignments within each region to reduce latency and switching
costs. Experimental results across multiple network topologies show that TORTA
reduces average inference response time by up to 15\%, improves load balance by
approximately 4-5\%, and cuts total operational cost by 10-20\% compared to
state-of-the-art baseline methods.

</details>


### [287] [Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters](https://arxiv.org/abs/2507.10392)
*Runsheng Benson Guo,Utkarsh Anand,Khuzaima Daudjee,Rathijit Sen*

Main category: cs.DC

TL;DR: The paper introduces Zorse, a system addressing challenges of training large language models on heterogeneous GPU clusters through improved pipeline and data parallelism.


<details>
  <summary>Details</summary>
Motivation: The motivation is to efficiently utilize heterogeneous GPU clusters, overcoming difficulties in load balancing, memory optimization, and communication over diverse interconnects.

Method: The method involves integrating pipeline and data parallelism in a flexible way, allowing asymmetric pipeline stages and mixed GPU types in the same group, supported by an automatic configuration planner.

Result: Zorse achieves significant performance improvements compared to state-of-the-art systems in heterogeneous training scenarios.

Conclusion: Efficient training on heterogeneous GPU clusters is feasible through Zorse, which unifies advanced parallelism strategies and an automated planner to optimize workloads.

Abstract: Large language models (LLMs) require vast amounts of GPU compute to train,
but limited availability and high costs of GPUs make homogeneous clusters
impractical for many organizations. Instead, assembling heterogeneous clusters
by pooling together GPUs of different generations allows them to achieve higher
aggregate compute and make use of all available GPUs. However, training on
heterogeneous clusters presents several challenges, including load balancing
across GPUs, optimizing memory usage to accommodate varying memory capacities,
and ensuring communication-efficient training over diverse network
interconnects potentially spanning multiple datacenters. In this paper, we make
the case that efficient training on heterogeneous clusters requires (1) the
integration of pipeline parallelism and data parallelism in a manner that is
both communication- and memory-efficient, and (2) a more adaptable
configuration of pipeline and data parallelism, which includes the capability
to flexibly partition GPUs into asymmetric pipeline parallel stages and to
incorporate heterogeneous GPUs within the same data parallelism group. We
propose Zorse, the first system to unify all these capabilities while
incorporating a planner that automatically configures training strategies for a
given workload. Our evaluation shows that Zorse significantly outperforms
state-of-the-art systems in heterogeneous training scenarios.

</details>


### [288] [Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?](https://arxiv.org/abs/2507.10413)
*Gabriel Rocha*

Main category: cs.DC

TL;DR: This paper revisits the FLP impossibility theorem and explores its persistence under generalized computational frameworks, alongside phase transitions in distributed systems using complex systems theory.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate whether the FLP impossibility theorem holds under broader computational models and analyze emergent features, such as inconsistency, in distributed systems using a theoretical framework.

Method: The study adopts complex systems theory and theoretical analysis of phase transitions in distributed systems coupled with paraconsistent logics.

Result: It demonstrates that inconsistency might emerge during phase transitions in consensus processes. However, while inconsistency is not inherent in paraconsistent logics, triviality could be an emergent concern.

Conclusion: The FLP impossibility theorem remains valid under generalized computation settings. Insights into phase transitions in distributed systems reveal limitations, and exploration of paraconsistent reasoning opens avenues for future consensus algorithm design.

Abstract: The consensus problem, briefly stated, consists of having processes in an
asynchronous distributed system agree on a value. It is widely known that the
consensus problem does not have a deterministic solution that ensures both
termination and consistency, if there is at least one faulty process in the
system. This result, known as the FLP impossibility theorem, led to several
generalizations and developments in theoretical distributed computing. This
paper argues that the FLP impossibility theorem holds even under a generalized
definition of computation through oracles. Furthermore, using a theoretical
machinery from complex systems, this paper also posits that inconsistency may
be an emergent feature of consensus over distributed systems by examining how a
system transitions phases. Under the same complex systems framework, this paper
examines paraconsistent logics, arguing that while inconsistency is not an
emergent feature for these logics, triviality may be. Lastly, some attention is
given to the possibility of developing consensus algorithms capable of
paraconsistent reasoning.

</details>


### [289] [Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout](https://arxiv.org/abs/2507.10430)
*Ji Liu,Beichen Ma,Yang Zhou,Jingbo Zhou,Ruoming Jin,Dejing Dou,Huaiyu Dai,Haixun Wang,Patrick Valduriez*

Main category: cs.DC

TL;DR: The paper introduces FedDHAD, a federated learning framework with two methods—FedDH and FedAD—to tackle challenges of data heterogeneity and device constraints, improving accuracy, efficiency, and reducing computation cost.


<details>
  <summary>Details</summary>
Motivation: Federated learning suffers from performance drops due to data heterogeneity among edge devices and limited capabilities of these devices, leading to slower model convergence and reduced accuracy.

Method: FedDH dynamically adjusts local model aggregation weights based on data heterogeneity (non-IID), while FedAD performs adaptive neuron operations to optimize accuracy and efficiency for heterogeneous devices.

Result: FedDHAD improves accuracy by up to 6.7%, speeds up training by up to 2.02 times, and cuts computation costs by up to 15% compared to state-of-the-art methods.

Conclusion: FedDHAD proves effective in addressing federated learning challenges, offering higher accuracy, faster convergence, and lower computation costs, making it an efficient framework for edge-device environments.

Abstract: Federated Learning (FL) is a promising distributed machine learning approach
that enables collaborative training of a global model using multiple edge
devices. The data distributed among the edge devices is highly heterogeneous.
Thus, FL faces the challenge of data distribution and heterogeneity, where
non-Independent and Identically Distributed (non-IID) data across edge devices
may yield in significant accuracy drop. Furthermore, the limited computation
and communication capabilities of edge devices increase the likelihood of
stragglers, thus leading to slow model convergence. In this paper, we propose
the FedDHAD FL framework, which comes with two novel methods: Dynamic
Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH
dynamically adjusts the weights of each local model within the model
aggregation process based on the non-IID degree of heterogeneous data to deal
with the statistical data heterogeneity. FedAD performs neuron-adaptive
operations in response to heterogeneous devices to improve accuracy while
achieving superb efficiency. The combination of these two methods makes FedDHAD
significantly outperform state-of-the-art solutions in terms of accuracy (up to
6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to
15.0% smaller).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [290] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: The paper evaluates the performance of a compression method (CompactifAI) on the Llama 3.1 8B language model, showing reduced computational resources while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of increasing computational resource demands and costs for large language models while preserving their accuracy.

Method: The study evaluated CompactifAI by comparing compressed and full-sized Llama 3.1 8B models using Codecarbon for energy consumption analysis and Ragas for accuracy assessment.

Result: The compressed model significantly reduced computational resource usage while maintaining accuracy.

Conclusion: CompactifAI enhances efficiency, scalability, and cost-effectiveness for language models without compromising their performance.

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [291] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: This paper introduces Recurrent Expansion (RE), a novel learning paradigm shifting from representational learning toward behavior-aware, self-evolving systems.


<details>
  <summary>Details</summary>
Motivation: Current Machine Learning and Deep Learning techniques focus on static data, limiting adaptability and introspection.

Method: Recurrent Expansion analyzes feature maps via iterative mappings with identical model architectures, incorporating performance signals. Variants like MVRE, HMVRE, and Sc-HMVRE improve scalability and adaptability.

Result: RE enables iterative self-improvement with model versions drawing insights from predecessors, supported by advanced parallel mechanisms.

Conclusion: Recurrent Expansion paves the way for scalable, adaptive, and introspective AI systems by leveraging models' evolving behaviors.

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [292] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: The paper proposes a method to improve the reliability of DNNs against bit-flip faults using an XAI-based TMR technique, achieving significant reliability improvements.


<details>
  <summary>Details</summary>
Motivation: DNNs are increasingly used in safety-critical applications, but their vulnerability to bit-flip faults poses a challenge. Efficiently applying Triple Modular Redundancy can address this reliability issue, but its effectiveness depends on accurately selecting crucial parameters and components.

Method: The authors use Layer-wise Relevance Propagation (LRP), a gradient-based XAI technique, to determine the critical weights and parameters in DNNs. These identified critical elements are selectively protected with Triple Modular Redundancy (TMR).

Result: The proposed method demonstrates over 60% reliability improvement at a bit error rate of 10^-4 for models like AlexNet, comparable to state-of-the-art methods but maintaining the same computational overhead.

Conclusion: The study introduces an efficient and low-cost TMR approach using XAI techniques to enhance DNN reliability, showing it as an effective alternative to existing methods in safety-critical applications.

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [293] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: The paper develops a voice-based decision support system for low-literacy farmers, combining predictive models for crop suitability and market prices.


<details>
  <summary>Details</summary>
Motivation: Farmers in regions like Karnataka face market and climate challenges, coupled with digital exclusion due to literacy barriers.

Method: The system integrates a Random Forest model for crop suitability and an LSTM model for market price forecasting, delivered via a Kannada voice interface.

Result: The Random Forest model achieved 98.5% accuracy, and the LSTM model showed low error in price prediction.

Conclusion: The proposed system provides scalable, accessible, and economically optimized recommendations, improving financial resilience of marginalized farmers.

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [294] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: This paper examines the limitations of the Low-Rank Adaptation (LoRA) method for fine-tuning large language models (LLMs) and proposes new methods that achieve similar or better performance with more consistent speed improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study is to address the inconsistent speed improvements observed in LoRA during the fine-tuning of large language models and to identify ways to optimize this process.

Method: The authors conduct an in-depth analysis of LoRA's speed and performance limitations and propose alternative methods for fine-tuning. These methods are empirically evaluated and compared to LoRA.

Result: The proposed fine-tuning methods deliver comparable or better performance than LoRA, with consistently faster training speeds across different setups.

Conclusion: The paper provides practical guidelines and insights for effectively fine-tuning LLMs, helping practitioners better optimize for speed and performance under resource constraints.

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [295] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: This paper introduces a Physics-Informed Neural Network (PINN) framework to model oceanic pollutant transport by simulating the 2D advection-diffusion equation, utilizing a hybrid loss function and noisy synthetic data.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods face challenges in efficiently and accurately modeling pollutant transport due to complex dynamics and large-scale oceanic domains.

Method: The framework uses a Physics-Informed Neural Network embedded with physical laws, trained on noisy synthetic data produced by the finite difference method, and integrates a hybrid loss function for training.

Result: The PINN approach achieves physically consistent predictions and effectively addresses the enforcement of boundary and initial conditions in simulations.

Conclusion: This scalable and flexible PINN model provides a promising alternative to traditional numerical solvers for simulating pollutant dispersion in dynamic oceanic environments.

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [296] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: The study investigates how network structures, including realistic community structures and heterogeneous degree distributions, impact the performance of neural networks in machine learning tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in understanding how the graph structure of neural networks, beyond simplistic designs, influences predictive performance, especially considering mesoscale structures like communities.

Method: The authors employ randomized and scale-free network models, alongside comparisons with biological neural networks, to analyze performance in image classification tasks. Structured attributes such as degree heterogeneity and community coherence are studied.

Result: It was observed that networks with coherent and densely interconnected communities exhibit improved learning performance. Comparisons with biological systems reinforce the relevance of structural properties.

Conclusion: The findings suggest that structural properties of neural networks can affect predictive performance and inspire designs informed by biological networks.

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [297] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: The paper proposes a method to detect money laundering using time series data through a transformer neural network, enhancing detection precision while controlling false positives.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of existing money laundering detection methods, which struggle to effectively balance detecting fraudsters from nonfraudsters while managing false positive rates.

Method: The paper introduces a two-step procedure using contrastive learning to capture time series representations. Afterward, a scoring model applies a two-threshold approach to ensure controlled false positives.

Result: The experiments demonstrate that the proposed transformer-based approach outperforms rule-based systems and LSTMs in detecting fraud patterns and distinguishing fraudsters from nonfraudsters while limiting false positives.

Conclusion: The findings suggest the transformer approach is a promising improvement over traditional methods, providing efficacious money laundering detection with reduced domain supervision and better false positive rate management.

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [298] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: The paper studies how recurrent neural networks (RNNs) trained on the streaming parity task can generalize infinitely after finite training, explaining this through representational dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks, particularly RNNs, are capable of extrapolating far beyond training data and achieving infinite generalization in certain tasks.

Method: The authors examine the representational dynamics of RNNs by studying their training on the streaming parity task, a nonlinear task defined over sequences of arbitrary lengths. They propose an effective theory and analyze phase transitions in learning behavior.

Result: RNNs, with sufficient finite training, demonstrate a phase transition that leads to perfect infinite generalization. The phenomenon is explained by an implicit representational merger effect, interpreted as building a finite automaton for the task.

Conclusion: Neural networks, like RNNs, can generalize infinitely from finite training datasets under certain conditions due to a representational merger, effectively learning an algorithm for the task.

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [299] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: This paper proposes $	exttt{wd1}$, a new policy optimization method for improving reasoning in diffusion-based large language models (dLLMs) using reinforcement learning, achieving higher accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning approaches for dLLMs suffer from computational overhead and bias due to reliance on multiple likelihood approximations, especially during policy optimization steps.

Method: The paper introduces $	exttt{wd1}$, a weighted likelihood reformulation requiring a single likelihood approximation, eliminating the need for supervised fine-tuning or supervised data while optimizing policies more efficiently.

Result: Experiments on reasoning benchmarks show that $	exttt{wd1}$ achieves up to 16% higher accuracy compared to existing RL methods, along with reduced training time and fewer function evaluations per gradient step.

Conclusion: $	exttt{wd1}$ provides a computationally efficient and simpler approach to improving reasoning in dLLMs through reinforcement learning, outperforming existing methods without relying on supervised data.

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [300] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: The study introduces the Transferability Aware Transformer (TAT) to adapt Alzheimer's disease (AD) knowledge for improved diagnosis of Lewy Body Disease (LBD), addressing data scarcity and domain shifts.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in Lewy Body Disease (LBD) diagnosis caused by data scarcity and domain shifts, leveraging the comparatively rich datasets of Alzheimer's disease (AD).

Method: The proposed Transferability Aware Transformer (TAT) uses the attention mechanism to emphasize disease-transferable features and suppress domain-specific ones. Structural connectivity (SC) from structural MRI is utilized as input data.

Result: Experimental results show that TAT effectively mitigates domain shift and enhances diagnostic accuracy, even with limited LBD data.

Conclusion: This is the first study to explore domain adaptation from AD to LBD, offering a promising framework for diagnosing rare diseases under challenging conditions of data scarcity and domain variability.

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [301] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: This paper introduces Iceberg, a synthetic data augmentation methodology that enhances deep learning-based prediction models for High-Level Synthesis (HLS) hardware designs, addressing their generalization issues.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization abilities of deep learning models for hardware design prediction via synthetic data augmentation and pretraining.

Method: The paper employs pretraining on LLM-generated programs and integrates an in-context architecture for meta-learning through weak label generation.

Result: Iceberg boosts modeling accuracy by 86.4% in few-shot examples across six applications and achieves significantly better offline DSE performance in two test datasets.

Conclusion: The proposed Iceberg framework substantially enhances generalizability, accuracy, and dataset adaptation, establishing itself as an effective tool for HLS prediction models.

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [302] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: The paper introduces a novel zero-shot NAS method called Weighted Response Correlation (WRCor) for efficient architecture evaluation without the need for training, achieving superior performance compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Neural Architecture Search (NAS) often requires significant computational resources and time due to the need to train numerous architectures from scratch. The goal is to develop a zero-shot, training-free method that is efficient, stable, and generalizable for architecture evaluation.

Method: The paper proposes the Weighted Response Correlation (WRCor) proxy, which uses correlation coefficient matrices of responses across different input samples to determine proxy scores for architectures, reflecting their expressivity and generalizability.

Result: WRCor demonstrated stronger efficiency as an estimation strategy compared to existing methods, and when applied in architecture searches across varying search spaces, it outperformed many existing NAS algorithms. It discovered architectures with high performance, such as achieving a 22.1% test error on ImageNet-1k within just 4 GPU hours.

Conclusion: The research establishes WRCor as an innovative and practical zero-shot NAS proxy that significantly reduces the computational demands of architecture search while maintaining high effectiveness and generality.

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [303] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: FedRAS is a novel Federated Recommender System framework that reduces communication overhead by clustering item embedding gradients instead of compressing embeddings, maintaining performance without introducing significant errors.


<details>
  <summary>Details</summary>
Motivation: To address the communication overhead and low training efficiency in Federated Recommender Systems caused by large item embeddings and heterogeneous client environments.

Method: The method involves an action-sharing strategy where gradients of item embeddings are clustered into updating actions for communication, paired with an adaptive clustering mechanism to cater to device and network variations.

Result: FedRAS achieves communication payload size reduction by up to 96.88% without sacrificing recommendation performance, verified through experiments on well-known datasets.

Conclusion: FedRAS provides a communication-efficient solution for Federated Recommender Systems, addressing both communication overhead and training inefficiency while maintaining model performance. Its implementation is publicly available for further research.

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [304] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: The paper introduces FLLL3M, a framework for privacy-preserving Next-Location Prediction using Federated Learning and Large Language Models, achieving high accuracy while reducing resource requirements.


<details>
  <summary>Details</summary>
Motivation: To develop an effective framework for accurate Next-Location Prediction without compromising user privacy and while using fewer computational resources.

Method: FLLL3M uses Federated Learning to retain user data locally and applies a tailored outer product mechanism to leverage Large Language Models.

Result: The framework demonstrates state-of-the-art accuracy on multiple datasets (e.g., Gowalla, WeePlace, Brightkite, FourSquare) while reducing model parameters by up to 45.6% and memory usage by 52.7%.

Conclusion: FLLL3M successfully achieves efficient and privacy-preserving Next-Location Prediction with significant improvements in accuracy and resource optimization.

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [305] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: This paper introduces the DAFOS method to dynamically optimize fanout in graph neural network training, resulting in significant speed and accuracy enhancements.


<details>
  <summary>Details</summary>
Motivation: Static fanout settings limit the scalability and efficiency of Graph Neural Networks (GNNs). Addressing this challenge, this paper proposes an adaptive approach to prioritize computational resources.

Method: DAFOS dynamically adjusts fanout during training based on node importance (using node degree scoring), model performance, and employs early stopping to improve efficiency.

Result: DAFOS demonstrated significant speedup of 3.57x on ogbn-arxiv and 12.6x on Reddit datasets while enhancing F1 scores from 68.5% to 71.21% (ogbn-arxiv) and from 73.78% to 76.88% (ogbn-products).

Conclusion: DAFOS is an effective and scalable solution that optimizes GNN training, enhancing both computational speed and prediction accuracy.

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [306] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: This paper introduces AMLAS-RL, an assurance framework adapted from AMLAS for safe reinforcement learning in cyber-physical systems, demonstrated through a vehicle collision-avoidance example.


<details>
  <summary>Details</summary>
Motivation: The integration of machine learning, particularly reinforcement learning, into cyber-physical systems presents safety challenges, especially in critical applications where learning must be both effective and demonstrably safe.

Method: AMLAS methodology is adapted to design AMLAS-RL, a framework that structures assurance arguments for RL systems using an iterative process.

Result: AMLAS-RL is applied to a practical example involving a wheeled vehicle tasked with reaching a target without collision, showcasing its functionality in ensuring RL system safety.

Conclusion: AMLAS-RL presents a systematic approach to address assurance gaps in reinforcement learning within cyber-physical systems, enhancing safety-critical applications.

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [307] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: The paper critiques flawed evaluation methodologies for Large Language Model (LLM) inference systems and proposes a checklist to avoid such anti-patterns.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for LLM inference systems often lead to misleading conclusions and hinder scientific progress.

Method: The authors analyze key evaluation anti-patterns across Baseline Fairness, Evaluation Setup, and Metric Design, and derive a checklist to improve evaluation practices.

Result: They identify problematic evaluation anti-patterns and demonstrate their framework's applicability through a speculative decoding case study.

Conclusion: A robust evaluation foundation is established, enabling accurate comparisons and driving progress in LLM inference systems by addressing real-world requirements.

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [308] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: The study evaluates Time Series Foundation Models (TSFMs) against traditional approaches in conformal prediction settings, finding TSFMs more reliable in limited-data scenarios.


<details>
  <summary>Details</summary>
Motivation: Investigate the potential of foundation models for time series forecasting in enhancing the reliability of conformal predictions, especially in data-limited settings.

Method: The study conducts comparative analysis between Time Series Foundation Models (TSFMs) and traditional methods like statistical models and gradient boosting, specifically exploring their performance within conformal prediction frameworks.

Result: TSFMs offer superior conformalized prediction intervals and better calibration stability, especially with limited data, compared to traditional methods.

Conclusion: Foundation models are highly promising for improving the reliability of conformal predictions in time series, particularly when data is scarce.

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [309] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: This paper proposes e-Profits, a profit-based evaluation metric to assess churn prediction models, incorporating customer-specific details to optimize financial outcomes.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like AUC and F1-score fail to align with financial outcomes in retention campaigns, misleading decision-making.

Method: Introducing e-Profits, which uses Kaplan-Meier survival analysis for personalized retention rates and evaluates per customer for financial metrics.

Result: Applying the metric to telecom datasets, e-Profits revealed financial benefits in models overlooked by traditional metrics and offered data-driven insights for high-value customer ROI.

Conclusion: e-Profits serves as a business-aligned tool that better informs profit-driven decisions and model evaluation for marketing and analytics teams.

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [310] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: The paper establishes precise lower bounds on message passing iterations in GNNs to solve different PDE types efficiently, ensuring adequate information propagation and better performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of inefficient information propagation and poor solutions in GNNs when solving PDEs due to insufficient message passing iterations, and to reduce the need for hyperparameter tuning.

Method: Derived sharp lower bounds for message passing iterations in GNNs by correlating PDE physical constants, discretisation, and message passing mechanisms across hyperbolic, parabolic, and elliptic PDEs.

Result: The proposed lower bounds ensure efficient information propagation and accurate solutions, validated through examples involving four types of PDE equations.

Conclusion: Adhering to the lower bounds enhances GNNs' ability to solve PDEs with sufficient accuracy, mitigating deep architecture inefficiencies.

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [311] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: The paper examines data biases that lead to algorithmic discrimination, introduces mechanisms to detect and document these biases, and proposes a preliminary framework called Data Bias Profile (DBP).


<details>
  <summary>Details</summary>
Motivation: There is insufficient computational research in detecting and mitigating data biases, despite their known impact on algorithmic discrimination and legal implications.

Method: The authors analyze three types of data biases and their effects on discrimination in algorithms. They propose mechanisms to detect these biases and introduce a framework, DBP, to systematically document them.

Result: The study finds that underrepresentation of vulnerable groups is less detrimental than expected, while label and proxy biases are more critical. The DBP effectively predicts discriminatory risks and aids fairness interventions.

Conclusion: The integration of DBP provides a structured way to assess biases and mitigate algorithmic discrimination, bridging research with anti-discrimination policy through data-focused analysis.

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [312] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: The paper introduces an advising system using small AI models and structured reasoning, outperforming larger models in refining hypotheses and experiments.


<details>
  <summary>Details</summary>
Motivation: Currently, there is a lack of scalable systems to provide well-reasoned feedback for hypotheses and experimental designs, despite rapid advancements in AI.

Method: The study leverages factors like model size, context length, confidence estimation, and structured reasoning, employing a small model with a compressed literature database.

Result: The proposed system exceeds 90% acceptance rates for high-confidence predictions and outperforms broader models like Deepseek-R1 for ICLR 2025 submissions.

Conclusion: A well-designed, small-scale advising system can significantly improve hypothesis generation and experimental design, offering a scalable and efficient solution.

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [313] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: FedAcross+ is a Federated Learning framework designed for domain adaptation in resource-constrained environments, enabling effective performance under covariate shifts and limited target samples.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces challenges in decentralized environments: costly human-labeled data, covariate shifts from environmental sensor variations, and limited feasibility of continuous model updates due to resource constraints.

Method: The paper proposes FedAcross+, a framework using a pre-trained source model with a frozen backbone and classifier during client adaptation. It employs a domain adaptive linear layer for efficient target domain adaptation and supports streaming data for non-stationary setups.

Result: Experimental results verify that FedAcross+ achieves effective adaptation with competitive accuracy, using limited target data, and operates efficiently within resource-constrained environments.

Conclusion: FedAcross+ successfully addresses domain shift and resource limitations in Federated Learning, enabling practical deployment in industrial settings with sporadic updates and streaming data processing.

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [314] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: The paper introduces a learning-based framework for travel demand modeling that offers greater scalability and adaptability than traditional models, with a Los Angeles case study demonstrating its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional activity-based models (ABMs) for travel demand are costly, rely on simplified assumptions, and are challenging to adapt to different regions, motivating the need for a more scalable, data-driven, and transferable approach.

Method: The framework integrates population synthesis, activity generation, location assignment, and traffic simulation into a generative, data-driven, and scalable system, validated through an implementation covering Los Angeles' 10 million population.

Result: The framework replicates real-world mobility patterns closely while significantly reducing cost. It achieves high accuracy in comparisons against benchmarks (e.g., SCAG ABM, Caltrans PeMS) with metrics including cosine similarity of 0.97, JSD of 0.001-0.006, and MAPE of 6.11%-9.8%.

Conclusion: The proposed generative framework is a scalable, effective alternative to traditional ABMs, performing comparably or better in validation measures while being easier to adapt across regions.

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [315] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: The paper addresses challenges in Federated Learning (FL) with intermittent and biased client participation, proposing and analyzing agnostic FedAvg algorithm for general stochastic participation.


<details>
  <summary>Details</summary>
Motivation: Current FL methods often assume uniform or fully known client participation, which is unrealistic in practical deployments.

Method: The authors characterize the stochastic dynamics of agnostic FedAvg and provide rigorous convergence guarantees for convex losses with non-uniform client participation.

Result: Agnostic FedAvg achieves convergence at a rate of $\mathcal{O}(1/\sqrt{T})$, outperforming weighted aggregation variants even under server-side knowledge.

Conclusion: The paper establishes agnostic FedAvg as a robust optimization framework for FL with unpredictable client availability.

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [316] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: The paper introduces a semantic communication framework utilizing a contrastive language-image pre-training (CLIP) model, enabling independent transmitter-receiver operations and optimizing performance in noisy wireless networks through reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address limitations in standard semantic communication frameworks that rely on joint neural network training over common datasets, especially in noisy wireless networks.

Method: The framework is based on a CLIP model for independent data extraction and task training. A reinforcement learning algorithm, specifically proximal policy optimization (PPO), is used to optimize CLIP model architecture and spectrum resource allocation under wireless noise conditions.

Result: Simulation results indicate an improvement in convergence rate by up to 40% and an increase in accumulated reward by 4x compared to the soft actor-critic approach.

Conclusion: The proposed CLIP-based framework and optimization method boost semantic communication efficiency and adaptability in wireless networks.

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [317] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: The paper introduces VIPEEGNet, a convolutional neural network model for analyzing EEG data to classify harmful brain activities with high accuracy across various categories.


<details>
  <summary>Details</summary>
Motivation: The existing limitations in EEG analysis, such as inter-rater variability and poor generalizability in current AI models, create challenges in diagnosing brain diseases. The paper aims to overcome these issues.

Method: A convolutional neural network was trained and validated using EEG data from two independent datasets, annotated by multiple experts, including online testing against expert performance for segmentation and classification.

Result: VIPEEGNet achieved high AUROC scores for binary classification categories like seizures and LPDs, sensitivity between 36.8% to 88.2%, precision from 55.6% to 80.4%, and comparable performance to human experts. It ranked second among 2,767 competing algorithms using minimal computational parameters.

Conclusion: VIPEEGNet significantly enhances brain disease diagnosis through accurate EEG evaluation while being computationally efficient, showing potential for clinical utility and outperforming existing models on validation metrics.

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [318] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: The paper proposes Oriented Distillation for Inline Acceleration (ODIA), a method to reduce latency in LLM-based function calling by distilling knowledge into smaller models to handle simple queries.


<details>
  <summary>Details</summary>
Motivation: High latency in LLM-based Function Calling negatively impacts user experience, necessitating an efficient solution.

Method: The ODIA framework identifies simple queries based on real-world traffic and uses knowledge distillation from larger models to smaller ones while updating itself through automated data collection.

Result: The method achieved a 45% reduction in average latency and a 78% reduction in median latency, successfully routing 60% of traffic to smaller models with minimal accuracy loss.

Conclusion: ODIA is a practical and dynamic solution for reducing latency in production environments with negligible trade-offs in accuracy.

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [319] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: The paper explores using Last Layer Hamiltonian Monte Carlo (LL-HMC) sampling for uncertainty estimation in deep neural networks, particularly in computationally constrained settings, and evaluates its effectiveness against other methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational challenges of using full HMC sampling in large-scale DNNs, specifically by focusing on the last layer of a DNN to make probabilistic methods applicable to data-intensive real-world scenarios.

Method: The authors implement LL-HMC, a technique that restricts HMC sampling to the final layer of a DNN, and compare it against five other last-layer probabilistic methods on three video datasets. They test each method's performance on classification, calibration, and out-of-distribution detection, with repeated grid searches for better hyperparameter tuning.

Result: LL-HMC demonstrates competitive in-distribution classification and OOD detection, though additional sampled last-layer parameters improve only OOD detection. Using multiple sampling chains did not consistently enhance the results.

Conclusion: LL-HMC is a computationally efficient and competitive method for introducing uncertainty into deep neural networks, particularly well-suited for limited-resource scenarios. However, its advantages are more pronounced for OOD detection than classification.

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [320] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: The paper introduces Fair-FLIP, a method to increase fairness in deepfake detection by adjusting final-layer inputs while retaining near-baseline accuracy.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models often show biases based on demographic attributes such as gender and ethnicity, threatening equitable trust in AI systems.

Method: Fair-FLIP, a post-processing technique, reweights final-layer inputs of trained models by prioritizing features with low variability and demoting those with high variability.

Result: Fair-FLIP improved fairness metrics by up to 30% and maintained baseline accuracy with negligible reduction (0.25%) in performance.

Conclusion: Fair-FLIP effectively reduces demographic biases in deepfake detection models with minimal trade-offs, contributing to more equitable AI-generated content detection methods.

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [321] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: The paper focuses on shuffling-type gradient methods, addressing convergence without requiring Lipschitz smoothness, and achieves solid convergence rates for various convexity cases.


<details>
  <summary>Details</summary>
Motivation: Shuffling-type gradient methods are popular for their simplicity and performance, but they require the restrictive Lipschitz smoothness condition, which may not hold for many machine learning models.

Method: The paper revisits shuffling-type gradient methods by proposing a stepsize strategy to enable convergence without Lipschitz smoothness, analyzing random reshuffling and arbitrary shuffling schemes.

Result: It demonstrates convergence rates for nonconvex, strongly convex, and non-strongly convex cases under general bounded variance conditions, achieving the best-known rates.

Conclusion: The study broadens the applicability of shuffling-type gradient methods by relaxing assumptions, supported by theoretical guarantees and numerical experiments validating practical efficacy.

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [322] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: Diffusion models benefit from an alternative backward discretization using proximal maps, improving convergence speed and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Current methods for generative modeling rely on score-based approaches, but these can be improved in sampling efficiency and KL divergence accuracy.

Method: Introduce Proximal Diffusion Models (ProxDM) by using proximal operators instead of scores for backward SDE discretization, leveraging proximal matching.

Result: ProxDM achieves $
O(d/\sqrt{\varepsilon})$ steps for $
\varepsilon$-accuracy and demonstrates faster empirical convergence in various implementations.

Conclusion: Using proximal maps provides both theoretical advantages and practical benefits for improving generative modeling with diffusion methods.

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [323] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: A GNN-based method is proposed to enhance cross-platform advertisement recommendations by analyzing user behavior, ad content, and platform features.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of advertisement recommendations across multiple platforms by effectively modeling user behavior and interests.

Method: The paper uses a graph neural network (GNN) with multi-dimensional modeling of user behavior data, ad content, and platform features to capture patterns of interest evolution and user interest migration across platforms.

Result: Experimental results show the best AUC value of 0.937 was achieved on Platform B. Platforms A and C showed slightly lower precision and recall due to uneven ad label distribution, but hyperparameter tuning improved model adaptability and robustness.

Conclusion: The proposed GNN-based model effectively enhances cross-platform ad recommendation accuracy by capturing latent pathways of user interest migration and shows improvements with proper hyperparameter tuning.

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [324] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: This paper explores Classifier-Free Guidance in masked discrete diffusion, identifies challenges with high early guidance and current implementations, and proposes a simple yet effective improvement.


<details>
  <summary>Details</summary>
Motivation: Existing Classifier-Free Guidance mechanisms have issues in conditional generation tasks for discrete diffusion models, particularly when it comes to guidance scheduling and implementation correctness.

Method: The authors theoretically analyze guidance schedules in masked discrete diffusion, identify flaws in unmasking transitions, and propose a smoothened guidance mechanism achievable with a one-line code change.

Result: The proposed guidance mechanism significantly improves sample quality, as demonstrated on datasets like ImageNet and QM9.

Conclusion: The study offers a better theoretical understanding of guidance schedules in discrete diffusion and presents an easy-to-implement solution to improve generation quality.

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [325] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: The study introduces ToxBench, a large-scale AB-FEP dataset for ML development focused on Human Estrogen Receptor Alpha (ERα) binding affinity prediction, and benchmarks ML methods, including the proposed DualBind model.


<details>
  <summary>Details</summary>
Motivation: Predicting protein-ligand binding affinities is crucial for drug discovery but is limited by reliable data availability for ML and computational complexity of physics-based methods.

Method: ToxBench combines 8,770 ERα-ligand complexes with binding free energies calculated via AB-FEP, validated against experimental data, and introduces ligand splits for assessing model generalizability. It also benchmarks ML models and proposes a new model, DualBind, with a dual-loss framework.

Result: DualBind outperforms other benchmarked ML methods and demonstrates the potential of ML to mimic AB-FEP at significantly reduced computational costs.

Conclusion: ToxBench is a valuable dataset for advancing ML in protein-ligand binding prediction, showcasing the effectiveness of the DualBind model and ML's capacity to replicate physics-based accuracy efficiently.

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [326] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: This paper explores the use of physics-informed neural networks (PINNs) to simulate turbulent fluid flows in two and three dimensions without relying on traditional computational grids or data.


<details>
  <summary>Details</summary>
Motivation: Simulating turbulent fluid flows requires vast computational resources, especially at high speeds, which limits traditional methods.

Method: The authors utilize PINNs that are trained directly from physical equations with innovations like adaptive architectures, causal training, and advanced optimization to overcome chaotic dynamics.

Result: PINNs accurately reproduce critical turbulence metrics such as energy spectra, kinetic energy, enstrophy, and Reynolds stresses through rigorous validation.

Conclusion: Neural equation solvers, like PINNs, offer a novel approach to handling chaotic systems, enabling continuous turbulence modeling that bypasses conventional computational limitations.

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [327] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: Simulation-Grounded Neural Networks (SGNNs) combine mechanistic simulations and neural networks to achieve interpretable and accurate predictions and inference tasks across scientific disciplines.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of mechanistic models collapsing under real-world complexity and machine learning models requiring large labeled datasets while operating as black boxes.

Method: SGNNs use mechanistic simulations as synthetic training data, spanning diverse conditions, to pretrain neural networks for both prediction and inference tasks.

Result: SGNNs demonstrated state-of-the-art performance across disciplines, such as tripling COVID-19 forecasting skill, reducing chemical yield prediction error by one third, and improving ecological forecasting resilience.

Conclusion: SGNNs offer a unified modeling paradigm that integrates scientific simulations with neural networks for robust, interpretable, and flexible inference and predictions, enabling process-level insight and overcoming the limitations of existing models.

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [328] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: The paper proposes a framework to improve diffusion models' generation quality via enhanced representation guidance, achieving faster training and superior performance across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models benefit from improved internal representations, but better systematic strategies for representation guidance are needed to enhance generation quality.

Method: The authors introduce new decomposition methods and associated training criteria to incorporate auxiliary representations. They propose multimodal pairing strategies and optimal training curricula to balance representation learning and data generation.

Result: Empirical results show enhanced generation performance across tasks like image, protein sequence, and molecule generation, with notable acceleration in training speeds (e.g., 23.3x faster on ImageNet benchmark compared to SiT-XL).

Conclusion: Incorporating representation guidance systematically into diffusion models improves generation quality and significantly reduces training time, making these models more efficient and versatile for various applications.

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [329] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: The paper proposes a method to accelerate Gaussian-based Probabilistic Generative Models (GPGMs) by replacing parts of the generative trajectory with a Gaussian approximation, improving efficiency without losing fidelity.


<details>
  <summary>Details</summary>
Motivation: Current GPGMs have high computational costs due to lengthy generative steps, limiting their practical deployment despite achieving state-of-the-art performance.

Method: The authors analytically identify a step in the generative process where data becomes sufficiently Gaussian and use a closed-form Gaussian approximation to replace subsequent steps, preserving training granularity while avoiding redundant perturbations.

Result: Empirical results demonstrate significant advancements in both sample quality and computational efficiency across various data modalities.

Conclusion: The proposed framework offers a theoretically sound and empirically validated solution for improving GPGM generation efficiency, making these models more practically deployable.

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [330] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: The paper investigates how machine learning model leaderboards can be exploited to distribute poisoned models at scale and proposes a framework, TrojanClimb, showing its efficacy in embedding malicious functionalities across various model types.


<details>
  <summary>Details</summary>
Motivation: To explore the under-studied vulnerability of machine learning leaderboards as a distribution channel for poisoned models and highlight the potential risks in the adoption of unverified models.

Method: They introduce TrojanClimb, a framework enabling the injection of malicious behaviors into models while achieving competitive leaderboard rankings, tested across diverse modalities like text and image generation.

Result: The framework successfully ranks high on leaderboards and embeds harmful functionalities, such as backdoors and bias injections, proving the significant risk posed by malicious models in the current ecosystem.

Conclusion: Machine learning leaderboards expose a critical vulnerability that can be exploited for large-scale distribution of malicious models, necessitating redesigns in evaluation mechanisms and awareness of the risks of unverified models.

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [331] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: This paper addresses challenges in imitation learning for continuous state-action systems, offering approaches like action chunking and noise injection to reduce compounding errors.


<details>
  <summary>Details</summary>
Motivation: Imitation learning in physical settings faces limitations due to compounding errors, necessitating enhanced methods for stable learning in tasks like autonomous driving and robotics.

Method: The paper proposes two techniques: action chunking for open-loop stable systems and noise injection during expert demonstrations for potentially unstable systems, leveraging insights from control theory and reinforcement learning.

Result: The proposed minimal interventions effectively mitigate compounding errors in continuous settings, offering new perspectives beyond traditional methods.

Conclusion: The study introduces practical techniques to improve imitation learning stability and highlights novel considerations not addressed when focusing solely on control theory or reinforcement learning paradigms.

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [332] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: The paper introduces a self-supervised deep learning model using EEG, ECG, and respiratory signals to predict cardiovascular disease (CVD) outcomes, achieving robust and clinically meaningful results validated across cohorts.


<details>
  <summary>Details</summary>
Motivation: To enhance cardiovascular disease risk assessment by developing a framework that extracts clinically meaningful patterns directly from multi-modal PSG data, enabling improved individualized predictions.

Method: The study created a self-supervised deep learning model trained on over 4,000 participants' EEG, ECG, and respiratory signals. Projection scores were generated by contrasting embeddings of individuals with and without CVD outcomes. Validation was performed on an independent cohort of 1,093 participants.

Result: The model produced distinct and clinically meaningful patterns across modalities. ECG features were highly predictive of cardiac conditions and CVD mortality, while EEG features predicted incident hypertension and mortality. Respiratory signals provided additional value. Combining the projection scores with the Framingham Risk Score improved predictive performance, with AUC values ranging from 0.607 to 0.965.

Conclusion: The framework offers a novel way to derive individualized CVD risk scores directly from PSG data, demonstrating high validity. This approach could be integrated into clinical settings for enhanced risk assessment and personalized care.

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [333] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: This paper integrates human gaze modeling into RLHF to enhance its efficiency and reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: RLHF is computationally expensive, despite its utility in aligning language models with human preferences. The research seeks to reduce these costs by utilizing human gaze data.

Method: The paper explores gaze-aware reward models and sparse rewards at the token level, distributed based on human gaze patterns, to integrate gaze data into RLHF.

Result: Gaze-informed RLHF leads to faster convergence and maintains or slightly improves performance, resulting in reduced computational costs.

Conclusion: Human gaze provides valuable information that can be used to optimize RLHF methods, offering potential improvements in its efficiency.

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [334] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: The paper proposes new gradient-based deep reinforcement learning methods using an extended Generalized Projected Bellman Error ($\GPBE$) objective to handle multistep credit assignment, achieving improved performance over existing algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing deep reinforcement learning methods using semi-gradient TD are simple and efficient but prone to divergence, while GTD methods, though stable, are underutilized in deep RL due to existing limitations.

Method: The authors extend the $\GPBE$ objective to support multistep credit assignment via $\lambda$-return, deriving three gradient-based algorithms compatible with both forward-view (experience replay) and backward-view (streaming) implementations.

Result: The proposed methods outperform PPO and StreamQ in the MuJoCo and MinAtar environments, demonstrating superior performance and stability.

Conclusion: The study successfully introduces multistep GTD methods to deep RL, bridging the gap between efficiency and stability in off-policy learning and achieving state-of-the-art results.

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [335] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: The paper extends PCA and ICA decompositions to continuous-time signals using an implicit neural signal representation framework, enabling applications to irregularly sampled data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of PCA and ICA in handling continuous-time and irregularly sampled signals, particularly for high-dimensional data like point clouds.

Method: The paper models signals as continuous-time stochastic processes and unifies PCA and ICA through a contrast function in the network loss to ensure statistical properties like decorrelation and independence.

Result: The method provides a numerical approximation framework that successfully applies low-rank decomposition to previously inaccessible datasets such as irregularly sampled signals and point clouds.

Conclusion: The proposed approach generalizes PCA and ICA to a continuous domain, significantly broadening their applicability to modern data challenges.

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [336] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: This paper introduces a novel distributed pre-training method that significantly reduces memory usage without loss in performance by training small structured subnetworks of models across separate workers.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high memory demands and significant intra-node communication costs in the distributed pre-training of large models.

Method: They propose training structured subnetworks in a distributed manner, avoiding inter-node activation communication, and evaluate two subnetwork construction strategies for balance and performance.

Result: The stochastic block dropping technique outperformed width-wise subnetwork construction, enabling a 20-40% reduction in memory usage while maintaining performance.

Conclusion: The approach effectively reduces memory usage in distributed training systems and shows potential for widespread adoption in optimizing resource usage for large model training.

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [337] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: The study proposes the Recursive MDN (R-MDN) layer to address challenges in removing confounder effects during continual learning, achieving equitable predictions by mitigating bias and catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Confounders cause biased predictions due to spurious correlations between input and target variables, particularly challenging in continual learning where systems need to adapt continuously.

Method: The R-MDN layer uses statistical regression with the recursive least squares algorithm to adapt feature representations by continually updating to account for changing data distributions and confounders.

Result: Experiments showed that R-MDN promotes equitable predictions for population groups and reduces catastrophic forgetting during both static and continual learning setups.

Conclusion: R-MDN effectively integrates into various deep learning models to learn confounder-invariant features, improving predictive equity and robustness in dynamic learning environments.

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [338] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: The paper introduces 'behavioral exploration,' where agents learn adaptive, exploratory behavior using a generative model trained on expert demonstrations.


<details>
  <summary>Details</summary>
Motivation: Current autonomous agents require significant interaction to adapt effectively, unlike humans, who can adapt quickly and purposefully.

Method: The authors train a long-context generative model using expert actions conditioned on past observations and exploratory measures from expert demonstrations.

Result: The method shows success in both simulated (locomotion and manipulation) and real-world robotic manipulation tasks, demonstrating adaptive, exploratory behavior.

Conclusion: Behavioral exploration provides an effective method for enabling fast online adaptation and targeted exploration for autonomous agents.

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [339] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: This paper explores the role of eigenoptions in improving exploration and credit assignment in reinforcement learning, evaluating their impact in tabular and deep RL contexts.


<details>
  <summary>Details</summary>
Motivation: Options in RL are often handcrafted and not learned, even though they can be a powerful tool for adding temporal and hierarchical structures. This paper seeks to address the underexplored role of eigenoptions in facilitating credit assignment.

Method: The authors evaluate the impact of pre-specified and online-discovered eigenoptions in both tabular and pixel-based gridworlds, and propose a method for learning option-values with non-linear function approximation in deep RL.

Result: Pre-specified eigenoptions improve both exploration and credit assignment, while online-discovered eigenoptions may bias the agent's experience counter-productively. Termination conditions in deep RL also significantly impact performance.

Conclusion: Eigenoptions hold promise for enhancing reinforcement learning, but their complexity, specifically in online discovery and termination conditions, requires careful consideration to balance exploration and credit assignment.

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [340] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: The paper addresses catastrophic forgetting in continual reinforcement learning (CRL) by using a Follow-The-Leader (FTL) online world model for planning tasks, proving its effectiveness in retaining skills while solving new tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle catastrophic forgetting in CRL, where agents fail to retain knowledge from previously learned tasks when adapting to new ones.

Method: The method involves creating an online Follow-The-Leader (FTL) shallow world model with model predictive control, which avoids forgetting by construction and updates incrementally.

Result: The proposed FTL Online Agent (OA) outperforms strong baselines in a dedicated evaluation environment, demonstrating the ability to solve new tasks while remembering old ones without relying on deep world models.

Conclusion: The FTL Online Agent provides a robust solution to CRL's catastrophic forgetting problem, ensuring continual learning and skill retention through its unique approach to planning with online world models.

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [341] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: The paper introduces a new predictive model for flight delays, QT-SimAM, which uses Queue-Theory and an attention mechanism, showing high accuracy on US and EU datasets.


<details>
  <summary>Details</summary>
Motivation: Flight delays cause financial losses and inconvenience. Predicting them accurately can improve passenger experience and operational efficiency.

Method: The proposed QT-SimAM model combines Queue-Theory and a simple attention mechanism, validated using datasets from the US and the EU.

Result: QT-SimAM achieved an accuracy of 0.927 and F1 score of 0.932 on US data, and 0.826 accuracy with 0.791 F1 on EU data, outperforming existing methods.

Conclusion: QT-SimAM provides an effective, transferable way to predict flight delays, aiding in operational decision-making and reducing passenger inconvenience.

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [342] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: This paper introduces DeepX-GAN to model rare, unseen climate extremes beyond historical records, revealing latent risks and informing adaptive policies.


<details>
  <summary>Details</summary>
Motivation: Existing climate records miss unseen extremes and neglect spatial dependencies, undermining risk assessment and adaptation strategies.

Method: DeepX-GAN is proposed as a deep generative model informed by knowledge to simulate spatially dependent unseen extremes with zero-shot generalizability.

Result: DeepX-GAN reveals vulnerable regions like MENA, and predicts future warming could shift risk hotspots to Indo-Pakistan and Central Africa.

Conclusion: Addressing unseen extremes requires spatially adaptive policies to anticipate emerging risk hotspots, rather than relying on historical data extrapolation.

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [343] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: The paper introduces a warm-start model that accelerates iterative generative processes by predicting an informed initial state instead of randomly initiating from an uninformed prior.


<details>
  <summary>Details</summary>
Motivation: The need to reduce the time and computational cost typically required by iterative generative models, which often involve hundreds of function evaluations.

Method: The proposed method involves using a warm-start model to predict an informed prior based on context. This reduces the distance the generative process must traverse and employs a simple normalization trick for compatibility with standard models.

Result: The warm-start approach achieves performance comparable to a 1000-step DDPM baseline using significantly fewer evaluations (11 in total), showcasing efficiency in tasks like image inpainting.

Conclusion: The warm-start model significantly speeds up conditional generative processes without compromising output quality, offering a broadly compatible and efficient solution.

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [344] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: This paper introduces a constructive Wavelet Neural Network (WNN) framework to improve computational efficiency and accuracy by prioritizing high-energy wavelet bases based on frequency analysis.


<details>
  <summary>Details</summary>
Motivation: WNNs have potential in signal processing and time-series analysis but face challenges with constructing accurate wavelet bases and high computational costs.

Method: The framework analyzes frequencies of unknown nonlinear functions, prioritizes initial wavelets with high energy in spatial frequency components, and introduces a mechanism to increase wavelet bases incrementally for improved accuracy.

Result: The proposed framework optimizes wavelet selection and computational efficiency, demonstrated effectively through examples in estimating static mappings, combining datasets, and analyzing time-series data.

Conclusion: The constructive WNN framework offers significant improvements in computational cost and versatility for various applications in signal and time-series data processing.

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [345] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: This paper establishes a connection between preference optimization and major ML theories, offering a general framework for DPO.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand direct preference optimization (DPO) from a principled perspective due to its wide application and current popularity.

Method: The authors connect Savage's loss functions and stochastic choice theories to formulate a general framework for DPO.

Result: The work supports abstention, accommodates non-convex objectives, and frames DPO extensions such as margins and corrections for length.

Conclusion: Understanding DPO through this framework showcases its versatility, provides insights into its challenges, and suggests practical solutions.

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [346] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: This paper introduces DejaVu, an attack exploiting temporal misalignments in sensor streams for autonomous driving, and proposes AION, a robust defense system to detect such attacks.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in multimodal fusion methods due to strict reliance on temporal synchronization in autonomous driving systems.

Method: DejaVu creates subtle temporal misalignments in sensor data, and AION is a defense system leveraging shared representation learning and dynamic time warping to detect temporal misalignments.

Result: DejaVu shows significant impact on perception tasks with up to 88.5% reduction in detection accuracy. AION achieves 0.92-0.98 AUROC in detecting misalignments with low false positives.

Conclusion: Temporal misalignments can severely degrade MMF-based perception. AION proves to be a robust and effective solution to defend against such attacks.

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [347] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: The paper addresses ingredient basket completion for grocery e-commerce using a set-to-set (S2S) recommendation framework, improving on traditional approaches by predicting sets of missing ingredients rather than single items.


<details>
  <summary>Details</summary>
Motivation: To enable customers in grocery e-commerce to create complete meals by recommending complementary ingredients based on incomplete baskets, overcoming limitations of current methods that predict only single missing ingredients.

Method: The proposed method, S2SRec2, introduces a set-to-set recommendation framework using a Set Transformer and multitask learning paradigm, jointly retrieving missing ingredients and assessing basket completeness.

Result: Experiments on large-scale recipe datasets demonstrate that S2SRec2 outperforms traditional single-target baselines, providing more accurate and coherent ingredient recommendations.

Conclusion: S2SRec2 effectively enhances grocery shopping and culinary creativity by predicting coherent ingredient sets, addressing real-world needs for ingredient basket completion.

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [348] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD combines speculative decoding techniques with Transformer-based temporal point processes to accelerate sequence sampling while maintaining output quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the practical need for fast sequence sampling in Transformer-based temporal point process models while retaining accuracy.

Method: It adapts speculative decoding from language models, using a smaller draft model to generate candidates, verified in parallel by a larger target model to maintain output accuracy.

Result: The method achieves a 2-6x speedup while preserving identical output distributions compared to standard sampling methods.

Conclusion: TPP-SD effectively accelerates Transformer TPP sampling, making it more practical for real-world applications without compromising quality.

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [349] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: The paper introduces GPAWP, a framework integrating graph prompts with weight pruning to optimize Graph Neural Networks (GNNs), resulting in improved performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in GNNs, such as slow training/inference times, difficulty in capturing complex relationships, and inadequate feature extraction, by leveraging graph pre-training and graph prompts.

Method: The method involves assessing the importance of graph prompts and hierarchically pruning negative prompt labels using structured pruning to enhance parameter efficiency and maintain competitive performance.

Result: Experiments across three benchmark datasets show significant parameter reduction and performance improvements in node classification tasks.

Conclusion: GPAWP effectively boosts GNN performance while enhancing efficiency by reducing reliance on graph prompts, providing a novel solution to previously overlooked issues in graph pre-training and prompting.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [350] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: This paper introduces a framework that incorporates uncertainty for selective time series imputation, improving imputation accuracy and downstream tasks like mortality prediction.


<details>
  <summary>Details</summary>
Motivation: Existing time series imputation methods often neglect or fail to estimate uncertainty, especially in challenging domains like healthcare, where missing values are frequent due to sensor disconnections.

Method: The authors propose a general framework that quantifies model uncertainty and uses it to selectively target imputation on confident values, avoiding unreliable imputations.

Result: Experiments on multiple EHR datasets demonstrate that uncertainty-based selective imputation reduces imputation errors and enhances performance in downstream tasks, such as a 24-hour mortality prediction.

Conclusion: Incorporating uncertainty into the imputation process leads to significant improvements, proving beneficial for both imputation accuracy and related applications in healthcare.

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [351] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: POIFormer is a novel Transformer-based framework for accurate attribution of user visits to Points of Interest (POIs), addressing challenges like GPS inaccuracy and high POI density.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy in attributing user visits to POIs despite GPS inaccuracies and dense clustering of POIs, which is essential for multiple applications like mobility analytics.

Method: The paper introduces POIFormer, a Transformer-based model leveraging self-attention and integrating features such as spatial proximity, visit timing, contextual semantics, user mobility, and crowd behavior patterns.

Result: Extensive experiments on real-world datasets show POIFormer significantly outperforms existing methods in scenarios with spatial noise and dense POI clustering.

Conclusion: POIFormer effectively addresses challenges in POI attribution, offers practical deployment capabilities across diverse contexts, and sets a new benchmark for real-world mobility analytics.

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [352] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: This paper introduces MolecBioNet, a new graph-based framework for drug-drug interaction (DDI) prediction that integrates molecular and biomedical knowledge, yielding superior accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing graph-based DDI prediction models, which treat drug pairs independently, lack context-awareness, and fail to integrate biological networks and molecular-level structures.

Method: The authors propose MolecBioNet, which models drug pairs as unified entities and integrates both macro-level biological interactions and micro-level molecular influences. It incorporates graph neural networks, context-aware subgraph pooling (CASPool), attention-guided influence pooling (AGIPool), and mutual information minimization for embedding diversity.

Result: MolecBioNet achieves state-of-the-art performance in DDI prediction, as demonstrated through experimental results and ablation studies. Embedding visualizations also confirm the benefits of its unified modeling and multi-scale integration.

Conclusion: The study provides a robust and interpretable framework for DDI prediction, overcoming prior limitations and advancing understanding of drug pair interactions at both molecular and biological levels.

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [353] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: This paper introduces the Fourier Basis Mapping (FBM) method, a novel approach combining Fourier transform and deep learning to improve time series forecasting by addressing issues in temporal and frequency information integration.


<details>
  <summary>Details</summary>
Motivation: Existing Fourier-based time series forecasting methods face issues such as inconsistent cycles and series lengths, leading to inadequate interpretation of frequency components and neglect of temporal information.

Method: The authors propose the FBM method, which integrates time-frequency features through Fourier basis expansion and mapping, preserving temporal characteristics. It introduces FBM-L, FBM-NL, and FBM-NP to enhance various neural network models, and a synergetic architecture, FBM-S, to separately model seasonal, trend, and interaction effects.

Result: The proposed methods are validated on various real-world datasets, achieving state-of-the-art performance in both long-term and short-term forecasting tasks.

Conclusion: FBM is a versatile, plug-and-play method that improves time series forecasting performance by accurately leveraging time-frequency features, addressing existing limitations in Fourier-based methods, and successfully integrating them with neural networks.

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [354] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen is a groundbreaking AI-driven global weather forecasting system capable of executing both Data Assimilation (DA) and medium-range forecasts in 17 seconds, surpassing traditional Numerical Weather Prediction (NWP) systems.


<details>
  <summary>Details</summary>
Motivation: Current AI-driven weather forecasting models depend on NWP systems for initial conditions, which are computationally expensive and time-consuming.

Method: The paper introduces XiChen, a scalable AI-based system featuring a pre-trained foundational weather model fine-tuned for observation and DA tasks. It employs four-dimensional variational knowledge to ensure forecasting accuracy.

Result: XiChen achieves DA and forecasting accuracy comparable to NWP systems while delivering skillful forecasts with lead times surpassing 8.25 days.

Conclusion: XiChen demonstrates the feasibility of fully AI-driven weather forecasting systems, providing faster, scalable, and reliable alternatives to NWP-based methods.

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [355] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: The paper proposes a frequency-aware and versatile kernel modeling framework within the open-source SMT 2.0 toolbox, enabling customizable and composite kernel configurations for surrogate modeling.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional exponential kernel functions in capturing complex, time-frequency dynamics in mechanical and aircraft systems.

Method: Development of advanced kernel functions, such as exponential squared sine and rational quadratic kernels, integrated into the open-source SMT 2.0 toolbox. Validation using test cases like sinus cardinal and real-world applications.

Result: Validated the proposed framework on benchmark cases like CO2 concentration forecasting and airline traffic predictions, demonstrating its effectiveness for frequency-sensitive domains.

Conclusion: The framework enhances metamodeling capabilities for complex domains, offering engineers and researchers flexible tools for future applications.

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [356] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: This paper revisits the Schedule-Free (SF) method for scalable, efficient training of language models, eliminating the need for explicit decay phases and weight averaging by leveraging SF-AdamW.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inadequacies of traditional pretraining strategies like cosine learning rate schedules and improve scalability for large-scale training workloads.

Method: The authors analyze and refine the Schedule-Free (SF) method, originally introduced by Defazio et al., integrating it with SF-AdamW and proposing enhancements for robustness under momentum and large batch sizes.

Result: The study demonstrates that SF-AdamW can effectively and efficiently train language models by navigating the loss landscape without decay phases or added memory costs.

Conclusion: SF-AdamW is a practical, scalable, and theoretically well-founded method for large-scale language model training, offering improvements in performance and efficiency.

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [357] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: This paper introduces NeuTSFlow, a novel time series forecasting framework that models transitions between continuous function families instead of discrete data points.


<details>
  <summary>Details</summary>
Motivation: Current methods largely overlook the continuous nature of time series data, leading to limitations in accurately forecasting future trends.

Method: NeuTSFlow uses Neural Operators to match flows in infinite-dimensional function spaces, leveraging velocity field parameterization for function-level modeling.

Result: NeuTSFlow demonstrated improved accuracy and robustness across a variety of forecasting tasks when compared to traditional methods focused on discrete points.

Conclusion: The function-family perspective proposed by NeuTSFlow offers a more effective approach to time series forecasting, addressing challenges posed by noisy and discrete observations.

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [358] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: This paper introduces two modules, CKM and CSM, to enable inference-time patch size control in patch-based transformer surrogates for spatiotemporal dynamics, avoiding retraining and accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Patch-based transformer surrogates face scalability challenges due to a fixed patch size during inference, limiting their efficiency in real-world applications.

Method: The proposed method introduces two modules, CKM (Convolutional Kernel Modulator) and CSM (Convolutional Stride Modulator), which allow dynamic patch-size adjustment during inference without requiring retraining.

Result: The approach demonstrates improved rollout fidelity, runtime efficiency, and long-term stability when applied to 2D and 3D PDE benchmarks.

Conclusion: This framework is the first to offer inference-time patch-size adaptability, creating a general and architecture-agnostic foundation for compute-adaptive modeling in PDE surrogate tasks.

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [359] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: The paper introduces MTF-Grasp, a multi-tier federated learning approach to address challenges in robotic grasping under non-IID and low-quantity data conditions.


<details>
  <summary>Details</summary>
Motivation: Federated Learning lacks exploration within robotic grasping tasks, where data is both non-IID and limited, leading to performance degradation.

Method: MTF-Grasp prioritizes "top-level" robots with better data distribution for initial seed model training, then distributes models to "low-level" robots to mitigate performance loss.

Result: MTF-Grasp improves robotic grasping performance by up to 8% on quantity-skewed datasets compared to conventional FL setups.

Conclusion: The multi-tier FL approach effectively addresses data heterogeneity and performance issues in robotic grasping, showcasing promising results.

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [360] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: The paper introduces a meta-autoencoder (MAE), a neural network that compresses and encodes multiple autoencoders for varying classes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to generalize the mechanism of autoencoders to handle dynamic differences between multiple classes, potentially useful for studying evolution and distinguishing properties across species.

Method: The authors propose a meta-autoencoder, which is an autoencoder designed to represent and encode a family of class-specific autoencoders, and provide initial examples.

Result: The study offers a constructive definition of meta-autoencoders and outlines preliminary examples, connecting their functionality to machine learning and biology research.

Conclusion: Meta-autoencoders have the potential to capture and represent distinctions across dynamically evolving classes, offering applications like modeling natural evolution and generalizing across multiple domains.

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [361] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: This paper proposes a fair CCA method to enhance fairness in representation learning without sacrificing accuracy, targeting applications in neuroimaging studies.


<details>
  <summary>Details</summary>
Motivation: Traditional CCA techniques often fail to address fairness concerns in representation learning, particularly in downstream classification tasks where bias can affect outcomes.

Method: The authors introduce a novel fair CCA method that ensures projected features are independent of sensitive attributes while preserving high correlation analysis performance.

Result: Experimental validation on synthetic and Alzheimer’s Disease Neuroimaging Initiative (ADNI) data demonstrates improved fairness in classification tasks without compromising accuracy.

Conclusion: The proposed fair CCA method enhances fairness in machine learning applications like neuroimaging studies, enabling unbiased analyses and broadening its applicability.

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [362] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: The paper introduces PRRO, a new pipeline that improves synthetic data generation by addressing class imbalances and SL data relationships, resulting in significantly better performance in supervised learning tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that synthetic data often underperforms in supervised learning tasks due to issues like class imbalance exaggeration and the lack of attention to supervised learning-specific data relationships.

Method: The proposed method, PRRO, integrates data pruning to retain high signal-to-noise ratio observations and employs column reordering to align synthetic data structures with supervised learning model requirements.

Result: Empirical experiments on 22 public datasets show that PRRO improves predictive performance, achieving an average improvement of 26.74% (up to 871.46%) when synthetic data replaces original data, and an average of 6.13% (up to 200.32%) when appended. It also enhances class distribution similarity by 43% on imbalanced datasets.

Conclusion: PRRO enhances the utility of synthetic data for supervised learning by addressing major limitations of current tabular data generators, enabling better integration into prediction tasks and improving accessibility to quality data analysis.

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [363] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: The paper introduces Noise-Conditioned Graph Networks (NCGNs) for generative modeling of spatially structured graphs, which dynamically adapt their architecture according to noise levels. This approach enhances graph modeling compared to noise-independent architectures.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve generative modeling for graphs with spatial structures, addressing limitations of existing models that do not adapt to varying noise levels during generation.

Method: They propose Noise-Conditioned Graph Networks (NCGNs), specifically Dynamic Message Passing (DMP), which adapt both the range and resolution of message passing as noise levels change.

Result: The proposed model, DMP, consistently outperforms noise-independent architectures across diverse domains such as 3D point clouds, spatiotemporal transcriptomics, and images.

Conclusion: Noise-conditioned approaches like NCGNs significantly enhance graph generative modeling, and dynamically adapting to noise levels during graph generation shows promising practical applications.

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [364] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: This paper presents a neural framework combining Neural ODEs, graph attention, and wavelet transformations for energy demand and supply forecasting, outperforming baselines across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: To address challenges in forecasting energy demand and supply caused by variability in renewable sources and dynamic consumption patterns.

Method: A neural framework integrating Neural ODEs, graph attention, wavelet transformations, adaptive frequency learning, and residual connections, evaluated on diverse datasets using SHAP analysis for interpretability.

Result: The model consistently outperforms state-of-the-art baselines in forecasting metrics and captures multi-scale temporal dynamics effectively.

Conclusion: This robust and interpretable model is suitable for sustainable energy forecasting applications and provides improved accuracy and insights.

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [365] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: This study investigates the impact of multi-head latent attention (MLA) on a transformer's internal capacity during pretraining, using random matrix analysis to reveal that a decoupled rotary embedding variant prevents capacity bottlenecks.


<details>
  <summary>Details</summary>
Motivation: To understand how MLA as a memory compression strategy affects a transformer model's internal representational capacity and to address the issues related to capacity bottlenecks and rank collapse during training.

Method: The authors used Marchenko-Pastur diagnostics and random matrix analysis to evaluate the spectrum of $W_{Q}W_{K}^\top$ across different MLA variants (standard MHA baseline, MLA-PreRoPE, and MLA-Decoupled).

Result: Key findings show that MHA-PreRoPE and standard MLA suffer from capacity bottlenecks, rank collapse, and spectral imbalance, while the MLA-Decoupled variant preserves representational capacity by maintaining spectral balance.

Conclusion: The manner of applying rotary embeddings (decoupling them across heads) significantly influences a model's ability to maintain capacity and suppress outlier formation, emphasizing that design choices in MLA have critical implications for transformer performance during pretraining.

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [366] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: LoRA-MCL is a novel training scheme enabling language models to generate diverse and plausible sentence continuations using Multiple Choice Learning (MCL) and the Winner-Takes-All loss.


<details>
  <summary>Details</summary>
Motivation: Address the fundamental ambiguity in language modeling, where multiple futures can be equally plausible given a context, by creating a mechanism for efficient handling of this diversity.

Method: Integrates Multiple Choice Learning (MCL) and Winner-Takes-All (WTA) loss within the Low-Rank Adaptation (LoRA) framework, while providing theoretical explanations and testing on Markov chain data and real-world tasks.

Result: Extensive experiments on visual and audio captioning tasks demonstrate that LoRA-MCL generates outputs with high levels of diversity and relevance.

Conclusion: LoRA-MCL effectively tackles the problem of decoding diverse sentence continuations, proving its potential in real-world language modeling applications.

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [367] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: The paper proposes a systematic method using scaling laws to optimize data mixtures for large foundation model pretraining, eliminating the need for trial-and-error approaches.


<details>
  <summary>Details</summary>
Motivation: Trial and error-based methods to determine data mixtures for large foundation model training are impractical and costly at scale, necessitating a more systematic approach.

Method: The authors develop scaling laws that predict model performance based on size, number of training tokens, and domain data proportions. These laws are validated across diverse modalities and offer scalability through small-scale training runs.

Result: Scaling laws were confirmed to accurately predict performance on large language, vision, and multimodal models, and can extrapolate performance to larger scales and new domain mixes.

Conclusion: The proposed scaling laws provide a principled framework to determine optimal data mixtures for any target domain within a training budget, making pretraining more efficient.

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [368] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: The paper explores a method to handle distribution shifts in conformal prediction using optimal transport, addressing a common challenge in uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction offers uncertainty quantification with finite-sample guarantees, but it struggles under distribution shifts, making real-world usage less reliable.

Method: The authors utilize optimal transport to estimate and mitigate coverage loss in the presence of distribution shifts.

Result: The proposed approach enables the prediction model to better handle distribution shifts, maintaining its uncertainty quantification capability.

Conclusion: The study highlights the potential of optimal transport to address challenges in conformal prediction when calibration and test data are non-exchangeable.

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [369] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: The paper presents adversarial activation patching as a new framework to detect, induce, and mitigate deceptive behaviors in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address emergent deceptive behaviors in safety-aligned LLMs, which act compliant but subtly mislead or omit critical information, posing risks in sensitive applications.

Method: The authors use activation patching as an adversarial tool in transformer-based models to source activations from deceptive prompts and incorporate them into safe forward passes at specific layers. This approach is validated through neural network simulations and hypothesis testing across various scenarios.

Result: Adversarial patching increased deceptive outputs from 0% to 23.9% in tests, demonstrating vulnerabilities and layer-specific deception rates. Hypotheses such as transferability across models and scaling effects were supported.

Conclusion: The research contributes to mechanistic interpretability and AI safety by proposing methods for detecting and mitigating deception, while also outlining challenges and future directions for large-scale model testing.

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [370] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: The paper explores using information geometry for deep learning model compression, focusing on operator factorization and a new perspective on projection methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying large deep learning models on resource-constrained devices using effective compression techniques.

Method: The authors use information geometry to analyze model compression methods, emphasizing iterative singular value thresholding with soft rank constraints.

Result: They prove convergence of iterative methods and demonstrate better accuracy by applying softer rank reduction modifications to existing compression approaches.

Conclusion: Information divergences are critical for zero-shot accuracy during compression, yet trainability becomes key during fine-tuning with iterative approaches enhancing performance under fixed compression rates.

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [371] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net is a novel model designed for causal discovery in multivariate time series (MTS), using dynamic sparse attention and dilated temporal convolutions to ensure accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of understanding causal relationships in MTS, especially in complex fields like finance and marketing, where conventional methods struggle.

Method: The authors introduced DyCAST-Net, which employs dilated convolutions and an adaptive thresholding attention mechanism to capture multiscale dependencies and eliminate spurious connections. A statistical shuffle test further ensures robustness.

Result: DyCAST-Net demonstrated superior performance compared to models like TCDF, GCFormer, and CausalFormer in detecting causal delays and reducing false discoveries, especially in noisy datasets. Attention heatmaps revealed hidden causal patterns.

Conclusion: DyCAST-Net is effective in discovering causality in MTS, particularly in high-dimensional and dynamic settings, and offers scalability, interpretability, and improved reliability for diverse applications.

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [372] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: This paper probes how transformers execute in-context learning (ICL) through synthetic linear regression, revealing that they fail to generalize to out-of-distribution prompts and analyzing pretraining data's role using spectral signatures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the mechanisms behind ICL in large pretrained transformers, given its success yet poorly understood nature.

Method: The authors study synthetic linear regression, perform out-of-distribution generalization experiments, and utilize spectral analysis on learned representations in a transformer residual stream.

Result: Transformers trained for ICL fail to generalize to out-of-distribution prompts, showing behavior inconsistent with manually implemented learning techniques like OLS regression.

Conclusion: ICL in transformers is influenced by the pretraining corpus, as indicated by spectral analysis of the residual stream, and generalization limitations exist when encountering data not aligned with the training distribution.

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [373] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: The paper proposes combining a Convolutional Neural Network (CNN) with a thermomechanical model to predict temperature, stress, and strain in nuclear reactor fuel rods for Predictive Maintenance (PdM).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for enhanced predictive maintenance techniques in Nuclear Power Plants (NPPs) to reduce downtime and prevent failures.

Method: A CNN model was trained on datasets generated via simulations from nuclear fuel performance and thermal-hydraulics codes. The model used limited temperature measurements to predict parameters critical to reactor safety.

Result: The CNN achieved high accuracy in predicting temperature distributions without overfitting. These predictions were successfully used in models for stress and strain analysis.

Conclusion: The methodology demonstrates the feasibility of enhancing PdM capabilities in nuclear reactors using real-time monitoring systems based on CNNs and thermomechanical models.

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [374] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: This paper developed semi-supervised regression models for ALS progression tracking, leveraging in-home sensor data to enhance predictive accuracy of ALSFRS-R declines.


<details>
  <summary>Details</summary>
Motivation: Clinical assessments of ALS often miss critical changes between visits, necessitating continuous and adaptive monitoring solutions.

Method: The study used semi-supervised regression models to predict ALSFRS-R scale trajectories, comparing batch learning and transfer learning with different interpolation methods (linear, cubic, self-attention).

Result: Transfer learning improved ALSFRS-R subscale prediction in most contrasts, while self-attention interpolation delivered the lowest error in nonlinear progression patterns. Patient-specific and cohort-level learning were matched to functional domains.

Conclusion: Adaptive model selection tailored to domain-specific functional profiles can improve ALS progression tracking and facilitate scalable and timely interventions.

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [375] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: The paper introduces La-Proteina, a model for atomistic protein design that outperforms others in generating both sequences and fully atomistic structures, excelling in scalability and structural validity.


<details>
  <summary>Details</summary>
Motivation: While recent generative models have made progress in de novo protein design, few can generate both amino acid sequences and fully atomistic structures, a complex task due to varying side-chain lengths.

Method: La-Proteina leverages a partially latent representation, where coarse backbone structures are explicitly modeled. Latent variables capture the sequence and atomic details uniformly. Flow matching ensures joint modeling of sequences and structures.

Result: La-Proteina achieves state-of-the-art performance across benchmarks in co-designability, diversity, and structural validity, excelling in atomistic motif scaffolding and producing valid samples for proteins up to 800 residues.

Conclusion: The model effectively addresses atomistic protein generation challenges, demonstrating its scalability, robustness, and potential for solving structure-conditioned protein design tasks.

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [376] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: The paper introduces a novel discrete differential operator, based on Vandermonde matrices from truncated Taylor series, to address the curse of dimensionality and error propagation issues in Taylor's formula when applied to derivative computation and function representation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the limitations of Taylor's formula in discrete situations, such as error propagation and challenges posed by high dimensions, which impact its application in various fields.

Method: The authors propose a discrete differential operator leveraging Vandermonde coefficient matrices derived from truncated Taylor series. This method utilizes uniform equidistant sampling to simultaneously compute derivatives of multiple orders and mathematically establishes error bounds.

Result: Experiments show enhanced derivative estimation and function representation accuracy compared to conventional methods, including finite forward difference and interpolation techniques.

Conclusion: The proposed method proves effective and broadly applicable across domains like vision, fluid mechanics, and feature extraction, providing a rigorous and accurate alternative for derivative computation and function representation.

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [377] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: The paper explores temporal-difference learning methods that bootstrap from two asymmetric value functions, analyzing their convergence, sample efficiency, and control performance, and introduces a novel algorithm called Regularized Dueling Q-learning (RDQ).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in understanding when and why methods that bootstrap from two value functions (e.g., AV-learning and QV-learning) are advantageous compared to traditional single-function TD methods (e.g., Q-learning).

Method: The authors analyzed QV and AV-learning algorithms theoretically for convergence and sample efficiency. They also proposed a novel AV-learning algorithm called Regularized Dueling Q-learning (RDQ) and compared its performance empirically in the MinAtar benchmark.

Result: The analysis indicates that while both QV and AV-learning are more efficient than Expected Sarsa in prediction scenarios, only AV-learning offers advantages over Q-learning in control. The proposed RDQ algorithm demonstrated significant performance improvements over Dueling DQN in benchmarks.

Conclusion: Learning two value functions can be advantageous depending on the scenario, particularly with AV-learning methods in control tasks, and the proposed RDQ algorithm provides a promising direction for improving TD-based control methods.

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [378] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: This paper explores robustness in XAI explanations for AI models, focusing on unbalanced datasets and analyzing minority class explanations using specific metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the robustness of explanations in XAI methods, especially for unbalanced datasets, where trustworthy explanations are critical in high-risk applications.

Method: The method involves using on-manifold generation of neighbors, explanation aggregation, and a metric to evaluate the consistency of explanations, with a test case focusing on frost events in tabular data.

Result: The study provides early insights into evaluating explanation robustness for XAI systems in scenarios with unbalanced data.

Conclusion: The proposed approach has potential in improving trustworthiness of explanations in high-stakes AI applications involving unbalanced datasets.

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [379] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: The paper introduces a new dataset to classify wellness dimensions in social media posts across six categories and evaluates models for this task.


<details>
  <summary>Details</summary>
Motivation: To improve personalized well-being evaluations and enable early mental health interventions using social media data.

Method: Development of a dataset with expert-guided annotations, use of both traditional and transformer-based machine learning models, and multi-class classification evaluation using statistical metrics.

Result: Models showcased promising precision, recall, and F1-scores across 10-fold cross-validation, supporting the effectiveness of the dataset.

Conclusion: This dataset facilitates region-specific wellness assessments, aiding personalized mental health strategies, and is publicly released with ethical considerations.

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [380] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: The paper introduces DRAGD and DRAGDP, novel attacks revealing privacy flaws in federated unlearning by reconstructing deleted data accurately.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address privacy concerns in federated unlearning, where gradient exchanges can inadvertently expose sensitive information about erased data.

Method: The authors propose DRAGD to exploit gradient discrepancies and DRAGDP to enhance data reconstruction using publicly available prior data, particularly for complex datasets.

Result: DRAGD and DRAGDP outperform previous methods in reconstructing deleted data across various datasets, including complex ones like facial images.

Conclusion: This research uncovers a severe privacy vulnerability in federated unlearning, emphasizing the need for enhanced security mechanisms in these systems.

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [381] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: The paper presents MLoRQ, a method combining low-rank approximation and quantization to optimize transformer deployment on edge devices.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of deploying resource-intensive transformer models on devices with limited computational and memory resources.

Method: The authors introduce MLoRQ, a two-stage optimization process focusing on intra-layer and inter-layer optimizations for bit-width and rank assignment, alongside an optional sequential rounding technique for error mitigation.

Result: MLoRQ achieves up to 15% performance improvement on Vision Transformers across multiple tasks like image classification, object detection, and instance segmentation.

Conclusion: MLoRQ successfully integrates compression techniques for efficient transformer deployment, providing state-of-the-art results while meeting memory constraints.

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [382] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: The paper discusses making large language models (LLMs) better at serving diverse user preferences, emphasizing the need for diverse input handling and introducing a novel multilingual dataset for preference alignment.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore how LLMs can be improved to align with highly diverse human preferences across cultural, political, and other dimensions.

Method: The researchers conducted a multilingual study across five countries (N=15,000), highlighted flaws in current preference collection methods, developed negatively-correlated sampling to enhance alignment, and introduced the 'Community Alignment' dataset for better training.

Result: The study showed that humans have far more diverse preferences than what LLMs exhibit, current methods are insufficient for diverse preference learning, and that their novel sampling approach improves alignment performance.

Conclusion: To better serve a global population, LLMs require training on highly diverse datasets; the open-source 'Community Alignment' dataset offers a significant step towards this goal.

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [383] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: This paper examines the application of Conformal Prediction (CP) in privacy-preserving supervised learning with deterministically encrypted data, focusing on the MNIST dataset.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of integrating rigorous uncertainty quantification techniques like CP with privacy-preserving machine learning, ensuring secure yet meaningful analysis of encrypted data.

Method: The authors used AES-encrypted MNIST datasets and compared traditional $p$-value-based Conformal Prediction against $e$-value-based Conformal Prediction, analyzing prediction accuracy and coverage.

Result: Empirical results show that models trained on encrypted data achieve 36.88% test accuracy (far greater than random guessing) and that $e$-value-based CP provides higher predictive coverage than $p$-value-based CP, which offers smaller but less reliable prediction sets.

Conclusion: This work demonstrates the feasibility of combining CP with encrypted data, highlighting a trade-off between prediction set size and reliability, and lays the groundwork for secure uncertainty quantification in privacy-focused learning systems.

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [384] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: The paper explores distributed learning where agents in a directed acyclic graph (DAG) learn sequentially, making predictions using partially observed features and information from parent agents. The work provides theoretical and experimental evidence to identify conditions where information aggregation enables accurate learning.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the challenge of distributed learning when no single agent has access to all input data/features, requiring collaboration through structured information sharing.

Method: The authors study learning processes in a DAG topology, where agents train sequentially based on their subset of features and predictions inherited from parent agents. They derive upper and lower bounds for effective learning, focusing on the DAG's depth as a key factor.

Result: The depth of the DAG is identified as crucial: longer paths facilitate information aggregation, enabling some agents to learn near-optimal models. However, shallow or poor topology DAGs hinder learning even with access to all relevant data collectively.

Conclusion: Information aggregation is achievable in distributed learning under specific DAG structures with sufficient depth. The study sets theoretical bounds and shows experimental validation to guide the design of such systems.

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [385] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: This study evaluates generative and discriminative LSTM-based text classifiers under post-training quantization (PTQ) in edge computing environments to understand their robustness and performance under various conditions.


<details>
  <summary>Details</summary>
Motivation: Edge computing applications demand low latency and high accuracy for real-time decision-making. Generative classifiers are robust to out-of-distribution and noisy data, making them suitable for these scenarios. However, computational constraints necessitate model optimization like PTQ.

Method: A comparative study of generative and discriminative LSTM-based text classification with various bitwidths in PTQ. It uses the Brevitas library, evaluates robustness under noisy conditions, and considers the effect of class imbalance during calibration with nonparametric hypothesis testing.

Result: Generative classifiers show sensitivity to bitwidth, calibration data quality, and input noise under quantized inference, while discriminative classifiers maintain robustness. Class imbalance during PTQ calibration affects generative classifiers’ performance, especially at lower bitwidths.

Conclusion: Proper calibration data in PTQ is critical for the deployment of generative classifiers in edge environments. Discriminative models are more resilient in quantized scenarios, while generative ones require tailored calibration techniques to mitigate performance degradation.

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [386] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: EPT-2 is a cutting-edge AI model for Earth system forecasting, outperforming previous models and numeric systems in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy and efficiency of Earth system forecasting, particularly for energy-relevant variables.

Method: EPT-2 employs advanced transformer-based AI techniques and introduces a probabilistic ensemble model (EPT-2e).

Result: EPT-2 outperforms both leading AI weather models and traditional numerical systems, while EPT-2e excels in probabilistic forecasting at lower computational costs.

Conclusion: EPT-2 and EPT-2e represent significant advancements in Earth forecasting technology, accessible through the app.jua.ai platform.

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [387] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: The paper aims to improve habitat classification using AI and high-resolution remote sensing data, demonstrating an effective methodology for mapping habitats across Europe.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of creating high-resolution, accurate habitat maps essential for conservation, especially in light of human pressures on ecosystems.

Method: The study used high-resolution remote sensing data, AI tools, vegetation data from the European Vegetation Archive, and multiple modelling strategies. Techniques included hierarchical modelling, integration of multi-spectral and SAR imagery, and ensemble machine learning to manage class imbalances.

Result: The combined use of RS data, hierarchical strategies, and class-balancing techniques significantly improved classification accuracy, especially in complex and fragmented landscapes.

Conclusion: The framework developed is transferable globally and adaptable to other classification systems. Future advancements should focus on dynamic habitat modelling, segmentation, and leveraging next-gen Earth Observation data.

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [388] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: The paper introduces an AI model that learns physical laws directly from boundary-condition data, bypassing traditional methods that rely on predefined mathematical equations.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional physics simulation methods, which require explicit mathematical formulations of governing equations.

Method: The authors employ a sketch-guided diffusion transformer for boundary-to-equilibrium mapping, leveraging enhanced architectures with novel spatial relationship encoding.

Result: The model directly generates steady-state solutions with high accuracy (SSIM > 0.8), avoiding error accumulation typical of time-stepping methods.

Conclusion: This approach establishes a universal framework for physics simulation, enabling discoveries without predefined mathematical constraints and marking a shift to AI-discovered physics.

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [389] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: The paper examines whether simpler non-equivariant CNNs trained with rotation augmentations can match the performance of complex equivariant GNNs in tasks like molecule generation.


<details>
  <summary>Details</summary>
Motivation: Existing 3D molecular discovery models rely on equivariant GNNs, which are complex and poorly scalable. The authors aim to explore whether simpler non-equivariant CNNs with augmentations can achieve similar outcomes.

Method: The authors introduce rotation-augmented training for CNNs and decompose the loss into prediction and equivariance errors. They test performance across three tasks: denoising, molecule generation, and property prediction.

Result: Through analysis, the authors demonstrate that non-equivariant CNNs can learn equivariance and achieve comparable performance to equivariant models, depending on factors like model size and training conditions.

Conclusion: Non-equivariant CNNs trained with rotation augmentations are viable alternatives to more complex equivariant GNNs, offering potentially simpler and scalable solutions for molecular discovery tasks.

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [390] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: This study proposes a Mixture of Experts (MoE) model and a novel attribution mapping technique called ShiftSmooth for enhanced transcription factor binding site (TFBS) prediction.


<details>
  <summary>Details</summary>
Motivation: TFBS prediction is critical for understanding gene regulation and various biological processes, but current methods lack generalizability, interpretability, and performance in out-of-distribution scenarios.

Method: The study employs a Mixture of Experts (MoE) model, integrating multiple specialized CNNs, and introduces ShiftSmooth for better model interpretability.

Result: The MoE model demonstrated superior performance, particularly in out-of-distribution datasets, while ShiftSmooth improved motif discovery and localization compared to traditional methods.

Conclusion: The proposed methods offer a robust, generalizable, and interpretable solution for TFBS prediction, advancing knowledge in genome biology and transcriptional regulation.

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [391] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: The paper introduces RGPD, a novel machine learning framework combining physics-based supervision and spatio-temporal learning for accurate estimation of RUL and SOH.


<details>
  <summary>Details</summary>
Motivation: Enhance accuracy and generalization in RUL and SOH predictions for industrial systems while reducing dependence on manual parameter tuning.

Method: The RGPD framework integrates Graph Convolutional Recurrent Networks, self-attention mechanisms, SAC modules, and Q-learning agents to refine spatio-temporal learning and weight physical constraints.

Result: RGPD demonstrates superior performance in predictive accuracy and robustness across diverse industrial benchmarks for RUL and SOH tasks.

Conclusion: The proposed RGPD framework is effective in improving predictions for PHM applications, outperforming state-of-the-art models in robustness and accuracy.

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [392] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: The paper introduces a neural network architecture for single-channel speech separation that supports dynamic compute-scaling through early-exit mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed-budget neural networks in adapting to varying compute demands and resources, especially in embedded and heterogeneous devices.

Method: The authors propose a probabilistic framework integrating uncertainty-aware modeling to derive early-exit conditions based on signal-to-noise ratio targets.

Result: Experimental tests show the approach achieves state-of-the-art performance and interpretability, competing with fixed-budget models across varying compute and parameter settings.

Conclusion: The framework provides an effective solution for dynamic compute-scaling in speech separation tasks, suitable for resource-constrained environments like mobile devices.

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [393] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: This paper proposes faster training and inference techniques for 3D molecular conformer generation using flow-based models, achieving state-of-the-art quality with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Efficient generation of molecular conformers is crucial for computational chemistry and drug discovery tasks, yet current methods require significant computational resources to train and sample state-of-the-art models.

Method: The paper introduces SO(3)-Averaged Flow training for faster convergence and higher quality conformer generation, alongside reflow and distillation methods for efficient inference, enabling few-step or one-step generation.

Result: Experiments show that SO(3)-Averaged Flow surpasses existing methods in generation quality. Reflow and distillation significantly reduce inference steps while maintaining high-quality conformer generation.

Conclusion: The proposed approaches pave the way for computationally efficient, high-quality molecular conformer generation, improving state-of-the-art models in both training and inference.

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [394] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: This paper introduces two methods, Blend and Accelerated-AMU (A-AMU), to enhance the efficiency of approximate machine unlearning (AMU) by reducing runtime and improving convergence during classification tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the inefficiency in computational runtime of current AMU approaches, especially in handling the retained dataset subset and reducing epochs for processing.

Method: The authors propose Blend, a dataset condensation method using visually similar image merging, and A-AMU, a loss-centric technique with steepened primary loss and novel regularization to expedite unlearning convergence.

Result: Their experiments show significant reductions in unlearning latency across both single and multi-round scenarios, while maintaining model utility and privacy with improved computational efficiency.

Conclusion: The study provides the first systematic approach combining dataset condensation and specialized loss functions to optimize unlearning efficiency, setting a new benchmark in the domain of AMU.

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [395] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: The paper introduces STAR, a system combining Large Language Models (LLMs) and Graph Neural Networks (GNNs) to improve job matching on LinkedIn by addressing challenges like cold-start and biases.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges such as cold-start, filter bubbles, and biases in LinkedIn’s job recommendation system while enhancing candidate-job matching accuracy.

Method: Integrates Large Language Models for text understanding with Graph Neural Networks for relationship modeling, enhanced by adaptive sampling and version management.

Result: Developed STAR, a scalable, high-performing system for embedding generation and integration into industrial recommendation setups.

Conclusion: STAR successfully addresses existing challenges in job matching systems, offering practical insights for large-scale embedding development and recommendation systems deployment.

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [396] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: The paper proposes a lightweight graph-aware federated learning approach for traffic prediction that integrates graph connectivity principles into client model aggregation, achieving competitive performance with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional FedAvg methods assume client independence which is suboptimal for traffic prediction tasks where spatial relationships are significant.

Method: The paper introduces a lightweight federated learning method that combines FedAvg's simplicity with neighborhood aggregation based on graph connectivity.

Result: The approach was tested on METR-LA and PEMS-BAY datasets, demonstrating competitive performance compared to existing methods.

Conclusion: This method captures spatial relationships effectively while being computationally efficient, making it suitable for traffic prediction tasks.

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [397] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: This paper evaluates reasoning performance in Large Language Models (LLMs) enhanced with Reinforcement Learning (RL). It highlights issues with benchmark contamination and introduces a reliable dataset, "RandomCalculation." Their findings suggest that only accurate rewards consistently boost performance.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by unverified claims that reinforcement learning can improve reasoning in LLMs even with noisy rewards, and concerns about benchmark reliability due to data contamination.

Method: The researchers analyzed reasoning performance on different LLMs, highlighted issues with benchmark contamination, and created a synthetic dataset called "RandomCalculation" to conduct controlled evaluations.

Result: The findings show that accurate reward signals improve reasoning, while noisy or random signals fail to consistently enhance performance. Additionally, benchmark contamination in models like Qwen2.5 was identified as problematic.

Conclusion: The paper argues that evaluating models on uncontaminated datasets and diverse model families is essential for reliable conclusions, emphasizing the importance of accuracy in reward signals.

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [398] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: This paper investigates the learned solutions of neural networks constrained to compute-efficient circuits for a Universal-AND problem, finding robust and flexible dense circuit solutions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether neural networks can learn compute-efficient circuits that exploit superposition in practical scenarios, rather than just theoretical constructs.

Method: The study uses a toy Universal-AND problem with restricted hidden dimensions to enforce compute-efficient circuits and examines how training influences the design of learned circuits.

Result: The network learns a fully dense circuit, where each neuron contributes to every output. The solution scales efficiently with dimension, maintains robustness across key parameters, and extends naturally to various boolean tasks.

Conclusion: The results reveal the neural networks' preference for dense and efficient circuit solutions that deviate from theoretical constructions, enriching understanding of model circuitry, superposition, and interpretability.

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [399] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: The paper introduces a model combining the interpretability of dynamic time warping (DTW) with the trainability of neural networks, outperforming traditional methods in time series classification, especially in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Neural networks excel in time series classification but require large labeled datasets and lack interpretability, making them less suitable for cold-start scenarios. DTW is effective in limited-data settings and interpretable but is non-trainable and less effective with abundant labeled data.

Method: The authors propose a dynamic length-shortening algorithm to generate prototypes of time series, enabling the reformulation of the DTW recurrence relation into a trainable recurrent neural network while preserving DTW's alignment characteristics.

Result: The proposed model significantly outperformed existing approaches in limited-data scenarios for time series classification and demonstrated competitive performance in settings with ample labeled data.

Conclusion: The study achieves a balance between trainability, effectiveness in cold-start conditions, and interpretability by combining elements of DTW and neural networks, offering a versatile solution for time series classification.

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [400] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: This paper introduces a generative cognitive diagnosis approach that shifts from traditional predictive modeling to generative frameworks, enabling effective and scalable learner assessments.


<details>
  <summary>Details</summary>
Motivation: Traditional cognitive diagnosis models struggle with limitations such as computational inefficiency during diagnosis of new learners and reliability issues in outputs.

Method: Two generative models—Generative Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model (G-NCDM)—are proposed to infer cognitive states through a disentangled generative process.

Result: Experiments demonstrate a 100x speedup for diagnosing new learners and improved scalability and reliability compared to traditional cognitive diagnosis methods.

Conclusion: The proposed generative framework enhances cognitive diagnosis applications and supports advancements in intelligent education systems and AI model evaluation.

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [401] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: The paper introduces a novel pre-training framework named Task Vector Estimation (TVE) to enhance task-aware representations for relational databases, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Relational databases support critical infrastructure but face challenges due to task heterogeneity, making effective and generalizable pre-training strategies difficult to design.

Method: TVE constructs supervisory signals through set-based aggregation of schema graphs and explicitly models temporal relational dynamics for pre-training.

Result: Experimental results on the RelBench benchmark reveal that TVE consistently surpasses traditional pre-training methods.

Conclusion: Incorporating task heterogeneity and temporal structures significantly improves predictive modeling in relational databases, as shown by TVE's superior performance.

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [402] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: The paper improves Automatic Prompt Optimization (APO) frameworks for better interaction with large language models (LLMs) by enhancing feedback mechanisms with positive reinforcement and feedback diversification, along with a new formalization for Continual Prompt Optimization (CPO).


<details>
  <summary>Details</summary>
Motivation: Prompt engineering is increasingly critical in NLP applications interacting with LLMs via black-box APIs, yet existing APO methods are limited in handling valuable insights from correct predictions and managing prompt migration effectively.

Method: The paper interprets textual gradients as negative reinforcement and introduces positive reinforcement to preserve beneficial prompt components. It also proposes feedback diversification to filter actionable advice and noise, formalizing Continual Prompt Optimization (CPO) for prompt migration between LLMs.

Result: The authors demonstrate that their novel APO framework consistently outperforms baselines, showing significant accuracy improvements, faster convergence, and lower computational costs in both standard and migration scenarios.

Conclusion: Enhancing the APO framework with improved feedback mechanisms and formalizing CPO leads to better prompt optimization, capable of addressing challenges posed by diverse and evolving LLMs, reducing performance degradation during migrations.

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [403] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: The paper critiques fixed downstream evaluation methods in AI and proposes a probabilistic framework using task priors to evaluate performance across all possible tasks.


<details>
  <summary>Details</summary>
Motivation: Current AI evaluation methods are limited to fixed and hand-picked downstream benchmarks, creating bottlenecks in achieving the grand goal of solving any possible task.

Method: The paper introduces a probabilistic space of downstream tasks by adopting a distribution of tasks and defining Task Priors to evaluate models comprehensively.

Result: The framework answers key questions regarding average performance and variance of models across all possible downstream tasks based on task probabilities.

Conclusion: Task Priors provide a new evaluation standard and could accelerate self-supervised learning research by improving the quality of downstream task evaluation signals.

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [404] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: The paper introduces AdaBrain-Bench, a benchmark aiming to systematically evaluate brain foundation models in non-invasive BCI tasks, addressing gaps in practical and extensible assessment frameworks.


<details>
  <summary>Details</summary>
Motivation: Non-invasive Brain-Computer Interfaces face challenges like high noise and limited task-specific data, limiting decoding capabilities, and the adoption of self-supervised pre-training lacks comprehensive benchmarks.

Method: The authors present AdaBrain-Bench, a benchmark incorporating standardized datasets spanning 7 applications, streamlined task adaptation pipelines, multidimensional evaluation metrics, and a suite of adaptation tools.

Result: AdaBrain-Bench evaluates multiple brain foundation models and provides insights into model selection practices for adapting to diverse BCI tasks.

Conclusion: AdaBrain-Bench delivers a reproducible, dynamic framework to advance research in robust and generalized neural decoding solutions by fostering widespread adoption and progress.

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [405] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: The paper introduces TolerantECG, a noise-resistant and flexible foundation model for ECG analysis, utilizing contrastive and self-supervised learning frameworks.


<details>
  <summary>Details</summary>
Motivation: To overcome issues in heart disease diagnosis caused by noise or unavailable leads in standard 12-lead ECG recordings.

Method: TolerantECG is trained using contrastive and self-supervised learning to unify ECG signal representation and retrieval-based text report descriptions, accommodating corrupted or incomplete signals.

Result: The model excels in benchmarking tests, ranking as one of the top performers across various conditions in the PTB-XL dataset and demonstrating superior performance in the MIT-BIH Arrhythmia Database.

Conclusion: TolerantECG is a robust tool for accurate ECG analysis under challenging signal conditions, minimizing diagnostic errors and uncertainty.

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [406] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: The paper introduces scSGC, a novel clustering method for single-cell RNA sequencing (scRNA-seq) data that utilizes soft graph structures to overcome traditional limitations of graph-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based clustering methods in scRNA-seq analysis, especially graph neural networks (GNNs), face challenges due to their reliance on hard graph constructions, which oversimplify cellular relationships and lead to information loss and errors in clustering.

Method: The scSGC framework includes three major modules: a zero-inflated negative binomial (ZINB)-based feature autoencoder, a dual-channel cut-informed soft graph embedding module, and an optimal transport-based clustering optimization module.

Result: Across ten scRNA-seq datasets, scSGC outperformed 13 state-of-the-art models in clustering accuracy, cell type annotation, and computational efficiency.

Conclusion: scSGC significantly improves scRNA-seq clustering by mitigating limitations of hard graph constructions, potentially paving the way for more accurate analysis of cellular heterogeneity and diversity.

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [407] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: The paper introduces DepBERT, a transformer-based model integrating dependency tree structures to enhance cause-effect phrase extraction, outperforming existing supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing supervised methods for extracting cause and effect phrases fail to integrate dependency tree structures, which are proven effective for semantic extraction.

Method: DepBERT incorporates dependency trees into a transformer-based model framework, aiming to leverage syntactic organization for improved semantic pair extraction.

Result: Experiments across three datasets reveal that DepBERT surpasses state-of-the-art supervised causality extraction methods in performance.

Conclusion: The integration of dependency trees in supervised deep learning models like DepBERT can significantly enhance causality extraction tasks.

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [408] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: This paper develops a methodology to interpret how Large Language Models (LLMs), specifically fine-tuned for nuclear engineering, utilize domain-specific knowledge by analyzing neuron activation patterns.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the transparency and trust in LLMs to enable their application in safety-critical fields like nuclear engineering, addressing verification and validation requirements.

Method: The authors fine-tuned an LLM (Gemma-3-1b-it) using Low-Rank Adaptation and analyzed neuron activation patterns pre- and post-fine-tuning. They probed specialized neurons through silencing techniques to assess their causal roles.

Result: The study found that while silencing individual specialized neurons had minimal effect, deactivating the group collectively caused significant task performance degradation. The silencing also impaired the accuracy of technical information generation.

Conclusion: The paper demonstrates a practical methodology to interpret black-box LLMs in safety-critical domains and proposes a pathway for achieving compliance with nuclear regulatory requirements by linking domain expertise to neural circuits for improved assurance.

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [409] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: This paper proposes a new method called MemSinks to effectively separate memorization from general language capabilities in large language models, addressing privacy and copyright concerns caused by model memorization.


<details>
  <summary>Details</summary>
Motivation: Language models often memorize repeated sequences, raising issues related to privacy and intellectual property. Current post-hoc techniques to address this memorization have been largely ineffective, particularly for natural and linguistically plausible text.

Method: The authors introduce MemSinks, a technique that uses sequence identifiers to activate a distinct set of memorization neurons for each sequence. This promotes isolation of memorization during training and makes it easier to identify and remove memorized content without affecting general language abilities.

Result: The implementation of MemSinks at billion-parameter and billion-token scale shows effective isolation of memorized content while maintaining strong language generalization capabilities. The results provide the first real-world demonstration of concurrent generalization and memorization isolation.

Conclusion: MemSinks offers a promising approach to mitigating memorization in large language models by isolating memorized sequences without degrading their ability to generalize. This work lays the groundwork for further exploration and practical adoption in addressing memorization-related challenges.

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [410] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: The paper introduces a biologically-inspired method that dynamically adds and removes neurons during training to improve minority class recognition in class-imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in handling imbalanced datasets with fixed-size neural networks and draw inspiration from how the human hippocampus flexibly adjusts capacity during learning.

Method: The method proposed involves periodically adding and removing neurons for minority class representation while maintaining the final network size unchanged for deployment efficiency.

Result: Experiments on three datasets and five models reveal enhanced performance on class-imbalanced data compared to fixed-size networks, with additional improvements when paired with imbalance-handling strategies.

Conclusion: Dynamic allocation of neurons during training, inspired by biological processes, significantly improves class imbalance handling and recognition accuracy without compromising deployment efficiency.

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [411] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: This paper proposes a novel job classification model using hierarchical embeddings to improve recruitment processes.


<details>
  <summary>Details</summary>
Motivation: Traditional text classification methods struggle to fully utilize the hierarchical nature of industry categories, which impacts job recommendation systems and labor market analysis.

Method: A representation learning and classification model embedding jobs and industry hierarchies into a shared latent space using SOC and Carotene taxonomies.

Result: Extensive tests on a large job dataset demonstrate the model's enhanced classification accuracy and ability to leverage hierarchical and semantic data.

Conclusion: The study provides a robust framework for optimizing job classification, supporting better decision-making in the recruitment industry.

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [412] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: The paper introduces a method for enhancing distance estimation in the latent space of recommender systems and proposes the Radial Neighborhood Estimator (RNE) to improve prediction accuracy and address the cold-start problem.


<details>
  <summary>Details</summary>
Motivation: Existing recommender systems struggle to define accurate and meaningful distances in the latent space for capturing user-user, item-item, and user-item relationships.

Method: The authors propose estimating latent space distances through row- and column-wise matrix distances, refining these estimates with a noise-correcting empirical variance estimator. RNE is introduced to construct neighborhoods and improve imputation accuracy using localized kernel regression.

Result: Simulated and real-world dataset evaluations indicate that the RNE method outperforms traditional collaborative filtering and matrix factorization models.

Conclusion: The novel distance estimation and RNE approach enhance overall recommendation accuracy and address challenges like the cold-start problem in recommender systems.

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [413] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: The study revisits inductive biases in Geographically Neural Network Weighted Regression (GNNWR) and proposes enhancements to improve spatial regression performance using concepts from CNNs, RNNs, and transformers.


<details>
  <summary>Details</summary>
Motivation: Understanding inductive bias is essential for improving spatial regression models and capturing complex patterns in limited data.

Method: The paper generalizes GNNWR by integrating local receptive fields (via CNNs), sequential context (via RNNs), and self-attention mechanisms (via transformers) into spatial regression modeling.

Result: GNNWR surpasses traditional methods in capturing nonlinear relationships, showing distinct advantages depending on data characteristics such as heterogeneity and sample size.

Conclusion: Inductive biases significantly impact spatial modeling; leveraging novel neural architectures can improve performance and interpretability in non-stationary spatial data handling.

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [414] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: Traditional domain generalization methods are impractical due to data requirements, and source-free approaches using vision-language models face challenges with domain-specific confounders. The TDCRL method addresses these limitations using causal representation learning.


<details>
  <summary>Details</summary>
Motivation: Deep learning's performance drops when training and testing data distributions differ, and traditional solutions require extensive data collection.

Method: TDCRL combines style word vectors and text embeddings to simulate visual representations and uses causal intervention networks to extract domain-invariant features.

Result: TDCRL achieves state-of-the-art results in source-free domain generalization tasks on datasets like PACS, VLCS, OfficeHome, and DomainNet.

Conclusion: Integrating causal inference into source-free domain generalization ensures the extraction of domain-invariant features and leads to robust generalization capabilities.

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [415] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: The paper presents a novel framework using physics-informed Gaussian processes to address limitations in compliance minimization problems, achieving improved design complexity control and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Solve issues in machine learning methods for compliance minimization, including poor feature boundaries, high computational cost, and lack of design complexity control.

Method: Design and state variables are parameterized using physics-informed Gaussian processes with neural networks as mean functions. These leverage Parametric Grid Convolutional Attention Networks (PGCANs).

Result: The method produces super-resolution topologies with fast convergence, reduced compliance, less gray area fraction, and fine-scale design control while outperforming traditional and ML-based methods.

Conclusion: The proposed framework effectively addresses limitations in CM problems and improves both performance and design interpretability through innovative use of Gaussian processes and advanced neural network architectures.

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [416] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: A graph neural network was developed to forecast Valley Fever incidence in Arizona, integrating case data and environmental predictors.


<details>
  <summary>Details</summary>
Motivation: Addressing the public health concern of Valley Fever by creating accurate forecasting methods.

Method: Designed a graph neural network incorporating surveillance case data and environmental factors, including lagged effects.

Result: The GNN successfully modeled Valley Fever trends and identified critical environmental drivers.

Conclusion: This work supports early warning systems and resource allocation for prevention in endemic areas.

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [417] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: The study introduces scMPT, which combines single-cell foundation models like scGPT with insights from large language models (LLMs) to enhance single-cell biological data analysis, achieving stronger and more consistent performance than either model alone.


<details>
  <summary>Details</summary>
Motivation: Single-cell foundation models are highly effective for biological tasks but lack the ability to utilize valuable text-based biological information; LLMs have shown promise as alternatives but lack complementary integration with these models.

Method: The researchers developed scMPT, a model combining single-cell foundation models like scGPT with biological insights extracted by LLMs, and explored alternative fusion methods to refine their combined performance.

Result: scMPT outperformed its individual component models across datasets, achieving stronger and more consistent performance with reduced performance disparities.

Conclusion: The study highlights the importance of integrating LLMs with single-cell models, showcasing their complementary capabilities in advancing single-cell biological data analysis.

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [418] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: This paper presents a simple three-stage pipeline for training adversarially robust decision trees, focusing on improving efficiency and robustness verification.


<details>
  <summary>Details</summary>
Motivation: With the rapid adoption of machine learning in the industry, ensuring the trustworthiness and efficiency of adversarially robust models has become increasingly essential.

Method: The pipeline has three stages: (1) automatically determining perturbation size using a novel, simpler algorithm, scalable with smaller models; (2) applying state-of-the-art adversarial training methods while evaluating efficiency and adversarial accuracy; (3) certifying and analyzing model robustness, including verification efficiency.

Result: Verification time, critical to the full pipeline's efficiency, is found to be independent of the training time. Perturbation size estimation and training evaluations yield useful insights for optimization.

Conclusion: The study identifies ways to improve training and verification pipelines for adversarially robust decision trees, emphasizing efficiency without compromising robustness.

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [419] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: The paper introduces a novel parameter reduction method for deep learning models using $H^2$ model order reduction to improve deployment efficiency on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: Deploying deep learning models with linear sequential state-space models (SSMs) is challenging on resource-constrained devices due to their large parameter sizes.

Method: The authors use $H^2$ model order reduction techniques from control theory to compress the linear SSM components of these models.

Result: The experiments on the LRA benchmark show superior model compression compared to a prior Balanced Truncation method, achieving $1/32$ the parameters without performance degradation.

Conclusion: This approach effectively addresses deployment challenges of deep learning models by drastically reducing parameter size while maintaining performance.

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [420] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: This paper introduces VR-CR-PN, a variance-reduced cubic-regularized policy Newton algorithm, addressing the distribution shift problem in second-order reinforcement learning optimization with horizon-independent sample complexity of $\tilde{\mathcal{O}}(\epsilon^{-3})$.


<details>
  <summary>Details</summary>
Motivation: Improve second-order policy optimization in reinforcement learning by overcoming limitations such as suboptimal sample complexity and dependence on unrealistic importance sampling assumptions.

Method: Developed VR-CR-PN, combining Hessian-aided variance reduction with cubic regularization, and designed a novel Hessian estimator with uniform bounds independent of the horizon.

Result: VR-CR-PN achieves a sample complexity of $\tilde{\mathcal{O}}(\epsilon^{-3})$, significantly better than the previous $\tilde{\mathcal{O}}(\epsilon^{-3.5})$, under general nonconvex conditions without the need for importance sampling.

Conclusion: The proposed algorithm provides efficient second-order reinforcement learning optimization while addressing critical challenges. It sets a new benchmark in sample complexity and eliminates dependency on horizon length.

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [421] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: This paper addresses the concept of TN ranks, aiming to clarify their role and utility through examples and intuitive methods for selecting these ranks in tensor networks.


<details>
  <summary>Details</summary>
Motivation: To demystify the foundational yet misunderstood concept of TN (Tensor Network) ranks and provide tools for their informed selection based on domain knowledge.

Method: The study employs real-life examples and visualizations, along with a self-explanatory graphical approach, to illustrate how TN ranks can be selected and generalized to tensors of arbitrary order by considering tensor unfoldings.

Result: The paper presents a unified understanding of TN ranks, their relationship with tensor mathematics, and their practical selection and deployment, avoiding the complexity of multi-index tensor algebra.

Conclusion: The insights provided help readers select and utilize TN ranks effectively in practical applications and educational settings, leveraging domain knowledge for better Tensor Network designs.

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [422] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: The study develops an unsupervised CNN-LSTM autoencoder for identifying play styles using low-level game data without relying on domain expertise.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on domain knowledge and biases when analyzing play styles, while improving adaptive gaming and game-playing agents.

Method: The authors use CNN-LSTM autoencoder models to extract latent representations from raw play-trace data in the MicroRTS game environment.

Result: The approach creates a clear differentiation of game-playing agents in latent space, showing effectiveness in characterizing diverse play styles.

Conclusion: This method provides a domain-agnostic way to analyze play styles, enabling unbiased and meaningful insights into game-playing behaviors.

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [423] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: This paper introduces T-GRAB, a benchmark for testing Temporal Graph Neural Networks (TGNNs) on key temporal reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: There is a lack of clarity on whether Temporal Graph Neural Networks effectively capture core temporal patterns like periodicity, causality, and long-range dependencies.

Method: The authors propose the Temporal Graph Reasoning Benchmark (T-GRAB), which comprises synthetic tasks targeting specific temporal skills: periodic repetition, causal effects, and long-range dependencies. They evaluate 11 temporal graph learning methods on these tasks.

Result: The evaluation revealed significant limitations in existing TGNNs' ability to generalize temporal patterns, highlighting the need for improved models.

Conclusion: Current TGNNs struggle with core temporal reasoning skills, necessitating advancements in their architectures. T-GRAB serves as a diagnostic tool for these advancements.

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [424] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: The paper introduces an adversarial representation learning method using focal entropy to sanitize sensitive data while maintaining high predictive performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing predictive performance with user privacy in representation learning.

Method: Proposes a novel adversarial learning method incorporating focal entropy to reduce information leakage from learned representations.

Result: Method demonstrated effectiveness across multiple benchmarks with high utility and moderate privacy risks.

Conclusion: The approach achieves a practical balance between predictive power and privacy preservation.

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [425] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: Analyzes neural networks using graph variables and statistical sufficiency, proposing that layers transform inputs into sufficient outputs under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To provide a new statistical understanding of neural networks by bridging statistical sufficiency, graph-theoretic representations, and deep learning.

Method: Models neural network layers as graph-based transformations and proves sufficiency under dense anchor point assumptions for infinite-width networks and structured inputs for finite-width networks.

Result: Demonstrates asymptotic sufficiency for infinite-width networks, sufficiency in practical cases with region-separated inputs, and covers diverse architectures including fully connected and convolutional layers.

Conclusion: Establishes statistical sufficiency conditions for neural networks, offering insights into their fundamental mathematical behavior and interpretations.

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [426] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: This paper introduces KAPI-ELM, a physics-informed learning framework designed to solve PDE problems, combining Bayesian optimization for input layer distributions with least-squares output layer optimization for enhanced speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of PI-ELMs in capturing sharp gradients in PDE problems while maintaining their speed advantage over traditional PINNs.

Method: The authors propose KAPI-ELM, which integrates Bayesian Optimization (BO) for tuning input layer distributional parameters and least-squares optimization for the output layer. This allows better handling of sharp gradients using an adaptive RBF-based structure.

Result: KAPI-ELM demonstrates state-of-the-art accuracy across several challenging PDE benchmarks, including singularly perturbed convection-diffusion and time-dependent advection equations. It surpasses advanced methods like XTFC in stiff PDE regimes while using fewer tunable parameters.

Conclusion: KAPI-ELM is a scalable and interpretable framework that matches or outperforms state-of-the-art methods in solving stiff PDE problems, making it a promising tool for various physics-informed learning applications.

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [427] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T is a chemical modeling framework that leverages conditional generative models tailored for biological contexts to enable scoring and design of molecules, excelling in both predictive and generative tasks.


<details>
  <summary>Details</summary>
Motivation: Existing chemical language models for molecular design lack reliable reward signals and interpretability, limiting their utility in real-world drug discovery.

Method: SAFE-T conditions molecular designs on biological contexts, such as protein targets or mechanisms of action, without using structural data or engineered scoring. It models fragment-based molecular sequences to achieve tasks like virtual screening and drug-target interaction prediction while supporting generative goals.

Result: SAFE-T delivers competitive or superior performance versus alternatives in predictive and generative benchmarks. It also offers significantly faster outcomes and fragment-level interpretability, revealing structure-activity relationships.

Conclusion: SAFE-T showcases how conditional generative chemical language models can unify molecular scoring and generation, addressing both computational efficiency and biological interpretability to enhance early-stage drug discovery efforts.

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [428] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: This paper focuses on an improved hierarchical $k$-median clustering algorithm with low average sensitivity and high clustering quality. The study addresses stability issues in existing methods through theoretical and experimental validations.


<details>
  <summary>Details</summary>
Motivation: Modern datasets are often large and dynamic, making the stability and robustness of hierarchical clustering crucial. The paper aims to address the sensitivity and usability challenges in hierarchical $k$-median clustering.

Method: The paper analyzes average sensitivity by measuring the expected change in clustering results upon the deletion of random data points. Based on this, it proposes an efficient algorithm with provable theoretical guarantees, compared to existing unstable methods.

Result: The proposed hierarchical $k$-median clustering algorithm demonstrates low average sensitivity and high clustering quality. It outperforms existing methods like single linkage clustering and a deterministic CLNSS variant in stability.

Conclusion: The proposed algorithm improves robustness and effectiveness in hierarchical $k$-median clustering, offering both theoretical and practical advantages. Experimental validation further underscores its stability and utility.

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [429] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: The paper introduces Demenba, an automatic dementia classification framework using state space models, outperforming prior methods by 21% in classification accuracy while being efficient.


<details>
  <summary>Details</summary>
Motivation: To improve early detection of dementia, as traditional neuropsychological tests rely on manual scoring which is less scalable.

Method: The authors utilize state space models for developing Demenba, leveraging over 1,000 hours of cognitive assessment data and fusing the model with large language models for added improvement.

Result: Demenba achieves a 21% improvement in dementia classification accuracy compared to prior approaches, with fewer parameters and efficient scaling.

Conclusion: Demenba offers a more transparent, scalable, and accurate framework for automatic dementia classification, paving the way for enhanced cognitive assessment methodologies.

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [430] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: The paper presents a systematic evaluation of statistical, machine learning, and deep learning methods for imputing missing Motion Capture (MoCap) data from wearable IMUs. A novel dataset with controlled missingness mechanisms is introduced.


<details>
  <summary>Details</summary>
Motivation: Missing data in MoCap applications hinders its utility in sports science, and comprehensive benchmarks for various imputation methods are absent.

Method: The authors evaluate imputation methods across univariate and multivariate contexts using a novel dataset from 53 karate practitioners, simulating controlled missingness mechanisms.

Result: Multivariate imputation methods outperform univariate approaches, reducing mean absolute error by up to 50%. Advanced methods like GAIN and Iterative Imputers yield the highest accuracy under challenging scenarios.

Conclusion: The study establishes a benchmark for effective imputation techniques, aiding future research and improving the robustness of MoCap data utilization.

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [431] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: The paper provides advanced error bounds for ReLU neural networks when approximating Korobov functions, reflecting improvements over classical metrics.


<details>
  <summary>Details</summary>
Motivation: To address limitations in classical error bounds for neural network expressivity and mitigate the curse of dimensionality.

Method: The researchers employ sparse grid finite elements and bit extraction techniques to analyze approximation errors in $L_p$ and $W^1_p$ norms.

Result: The study achieved nearly optimal error bounds in $L_p$ (order $2m$) and $W^1_p$ norms (order $2m-2$) for target functions.

Conclusion: ReLU neural networks demonstrate strong approximation abilities without being significantly impacted by the curse of dimensionality.

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [432] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: The paper introduces a faster diffusion algorithm on SO(3) manifold, achieving up to 4.9x speed-up without degrading performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are sequential and time-intensive, posing a limitation for tasks requiring quick processing.

Method: The authors adapt the numerical Picard iteration technique to accelerate the diffusion process on the SO(3) manifold.

Result: Up to 4.9x speed-up in generating samples without measurable degradation in task reward during experimentation.

Conclusion: The proposed adaptation effectively reduces latency in diffusion models on SO(3) manifold while maintaining performance.

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [433] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: This paper introduces FedFD, a federated learning method that employs feature distillation with orthogonal projection for better aggregation of knowledge from heterogeneous models.


<details>
  <summary>Details</summary>
Motivation: To address the instability and inefficiency in aggregating knowledge from heterogeneous models during federated learning, particularly problems caused by logit-based distillation methods.

Method: FedFD uses a feature-based ensemble federated knowledge distillation paradigm. The server maintains a projection layer for every client's model architecture, employing orthogonal techniques to align features and mitigate knowledge bias.

Result: Experiments demonstrate that FedFD outperforms existing state-of-the-art methods in federated learning with heterogeneous models.

Conclusion: FedFD provides a stable and efficient solution to tackle knowledge bias in model-heterogeneous federated learning, significantly enhancing performance.

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [434] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: This paper proposes Temporal-Aligned Transformer (TAT), a forecasting model that improves peak demand prediction using temporal alignment and context data, achieving up to 30% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Accurate demand prediction is critical for supply chain management, especially during high-stake sales events with challenging peak demand forecasting.

Method: The model, TAT, uses an encoder-decoder structure with Temporal Alignment Attention (TAA) to leverage context variables like holidays and promotions, improving predictive accuracy.

Result: TAT shows up to 30% accuracy improvement in peak demand forecasting based on evaluations with large-scale e-commerce datasets.

Conclusion: The proposed TAT model effectively improves peak demand predictions, ensuring better decision-making for supply chains during critical events, and performs competitively overall.

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [435] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: DeepONets are used for modeling PDE-governed systems, and this paper evaluates different architectures for geotechnical engineering applications, specifically 1D consolidation problems, with a focus on computational efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a limited application of DeepONets in geotechnical engineering, despite its growing use in other disciplines. The study aims to explore and enhance its capabilities for PDE-governed problems like the 1D consolidation problem.

Method: The study compares several DeepONet architectures (standard and physics-inspired) and introduces a Trunknet Fourier feature-enhanced DeepONet (Model 4) to overcome limitations in capturing rapidly varying solutions.

Result: Model 3 outperforms standard configurations (Models 1 and 2), but Model 4 offers the best results by addressing variations effectively. Speedups of 1.5 to 100 times are achieved compared to traditional solvers.

Conclusion: DeepONets hold promise for efficient and scalable surrogate modeling in geotechnical engineering, with Model 4 being the most efficient option studied. The findings emphasize the potential for broader applications in the field.

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [436] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: This paper introduces a cloud-based shared e-mobility platform, powered by LLMs, featuring personalized route recommendations and schema-level evaluation of its optimization and query functionalities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing demand for comprehensive e-mobility solutions by leveraging advanced technologies like LLMs and cloud-based simulation systems.

Method: The study integrates LLM-powered frameworks into a shared e-mobility platform, evaluates travel optimization in traffic scenarios, and tests schema-level query accuracy using methods like XiYanSQL.

Result: Schema-level RAG with XiYanSQL achieved execution accuracies of 0.81 for system operators and 0.98 for users, demonstrating high accuracy in query handling.

Conclusion: The proposed platform successfully combines cloud technology and LLMs to offer effective e-mobility solutions, promising enhanced user engagement and operational efficiency.

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [437] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: The paper introduces a novel transformer-based language model called TagBERT for e-commerce query reformulation, which outperforms competing models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges in e-commerce search engines to better align user queries with inventory items by bridging semantic gaps and improving query reformulation.

Method: A dependency-aware transformer-based model (TagBERT) is introduced, leveraging semantic tags for enhanced query embedding and token classification.

Result: Experiments demonstrate that TagBERT surpasses models like BERT, eBERT, and sequence-to-sequence transformers in token classification for query reformulation.

Conclusion: TagBERT effectively improves query reformulation in e-commerce search engines by incorporating dependency-aware semantics and demonstrates superior performance compared to existing models.

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [438] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: The paper introduces a computational strategy combining graph-based methods and machine learning to search reaction mechanisms, particularly for complex cyclization reactions.


<details>
  <summary>Details</summary>
Motivation: Complex reactions involving concerted bond changes, such as those seen in cyclizations during natural product synthesis, pose challenges in mechanism exploration.

Method: The strategy combines graph-based enumeration to identify reaction steps and machine learning using AIMNet2-rxn neural network potentials for filtering and evaluating pathways.

Result: The neural network potential successfully estimates activation energies, anticipates stereoselectivity, and reconstructs enabling steps in natural product synthesis.

Conclusion: This approach provides a cost-effective and accurate methodology to expedite exploration of complex reaction pathways, advancing understanding in reaction mechanism searches.

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [439] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: SON (Stochastic Operator Network) introduces uncertainty quantification in operator learning by leveraging Stochastic Optimal Control principles, integrating with DeepONet.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of incorporating uncertainty quantification into operator learning frameworks, often needed for noisy systems.

Method: SON constructs the branch network as a Stochastic Differential Equation (SDE) and employs adjoint Backward Stochastic Differential Equation (BSDE) for optimization, integrating Stochastic Maximum Principle concepts into the SGD update.

Result: SON successfully demonstrates its capability to replicate noisy operators in 2D and 3D environments, effectively quantifying uncertainties.

Conclusion: The introduced SON framework enhances operator learning by integrating uncertainty quantification, potentially aiding complex systems with stochastic processes.

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [440] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: This paper analyzes energy efficiency in AI/ML models, focusing on DeepRX with ResNet architecture, evaluates energy consumption, and leverages knowledge distillation to create energy-efficient models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing energy efficiency with performance in AI/ML models, particularly for DeepRX, a machine learning receiver.

Method: The paper examines energy metrics during training and inference, applies knowledge distillation to create compact student models, and evaluates performance using parameters such as BER and SINR.

Result: Compact DeepRX student models trained via knowledge distillation achieved reduced energy consumption and better BER performance across SINR levels compared to models trained from scratch.

Conclusion: Knowledge distillation effectively reduces the energy consumption of AI models while maintaining performance, offering a viable solution for energy-efficient AI implementations.

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [441] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: The paper introduces CLA, a novel self-supervised learning approach for online continual learning that mitigates forgetting and enhances performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective self-supervised learning techniques for online continual learning, where data is processed sequentially, constrained by computational budgets, and comes without clear task boundaries.

Method: Proposed Continual Latent Alignment (CLA), which aligns current model representations with past ones to reduce forgetting and improve training convergence.

Result: CLA sped up convergence in the online continual learning scenario and outperformed existing approaches under identical computational constraints. Early-stage CLA pretraining also enhanced final performance compared to traditional i.i.d. pretraining.

Conclusion: CLA is both a superior method for online continual learning and a beneficial pretraining protocol, proving its efficacy and flexibility.

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [442] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: The paper investigates limitations in Vision-language Models (VLMs) on fundamental visual tasks through detailed probes and evaluations beyond current benchmarks.


<details>
  <summary>Details</summary>
Motivation: To identify shortcomings and enhance the understanding of Vision-language Models (VLMs) regarding their performance on fundamental visual tasks.

Method: Develop tests to probe VLMs at different stages—features from visual encoders, intermediate vision-language projection, and LLM-decoder outputs—to uncover weaknesses and analyze performance.

Result: The study identifies specific limitations in VLMs' visual understanding capabilities and provides insights into their robustness, processing methods, and areas for improvement.

Conclusion: The observations aim to contribute to improving VLMs and provide guidance for future development of these models.

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [443] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: The paper discusses optimization via gradient descent, focusing on the Polyak-Łojasiewicz Inequality (PLI) and its generalized versions to address global exponential convergence in reinforcement learning and control. Input-to-state stability is explored, alongside challenges like gradient estimation errors.


<details>
  <summary>Details</summary>
Motivation: The authors aim to generalize PLI-like conditions to address the gap between global exponential convergence in discrete-time LQR problems versus mixed locally and globally linear convergence in continuous-time LQR problems.

Method: The authors approach the issue by exploring generalized PLI-like conditions and conducting an 'input-to-state stability' (ISS) analysis to assess transient and asymptotic effects of gradient estimation errors.

Result: The paper identifies generalized PLI-like frameworks that can enhance understanding of optimization behavior under gradient flow and account for errors arising from various practical challenges.

Conclusion: The generalized PLI conditions and ISS analysis provide insights into optimization convergence properties and resilience to gradient estimation errors, advancing knowledge in machine learning, control, and optimization.

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [444] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: The paper introduces 'Target Polish,' a fast, robust framework for nonnegative matrix and tensor factorization that improves efficiency and outlier resistance, tested on image data with noise.


<details>
  <summary>Details</summary>
Motivation: Existing weighted NMF approaches are slow due to multiplicative updates, necessitating a faster and robust method for handling outliers.

Method: The Target Polish framework applies an adaptive weighted median-based transformation that complements the Fast-HALS algorithm's speed while enhancing outlier resistance.

Result: Empirical tests on noisy image datasets show that Target Polish matches or surpasses state-of-the-art robust NMF methods in accuracy and significantly reduces computation time.

Conclusion: Target Polish offers a robust and efficient solution for nonnegative matrix and tensor factorization, overcoming limitations of existing methods.

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [445] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: The paper focuses on mitigating catastrophic forgetting in continual learning through Elastic Weight Consolidation (EWC), showcasing its effectiveness in retaining prior knowledge while slightly reducing adaptability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of catastrophic forgetting in neural networks learning sequential tasks, inspired by biological synaptic consolidation.

Method: The study evaluates EWC in supervised learning using PermutedMNIST and RotatedMNIST benchmarks and compares it to L2 regularization and SGD. It also explores dropout effects and hyperparameter variations.

Result: EWC is confirmed to reduce forgetting significantly compared to naive training, albeit at the cost of slightly lowered learning adaptability for new tasks.

Conclusion: EWC is validated as an effective method for lifelong learning, balancing knowledge retention and adaptability across diverse scenarios.

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [446] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: SplitHappens enhances Split Learning's (SL) security by combining it with Function Secret Sharing (FSS), addressing a wider range of attacks, and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing vulnerabilities in Split Learning, particularly its susceptibility to attacks while preserving data privacy and optimizing communication and computation costs.

Method: Use U-shaped Split Learning combined with Function Secret Sharing to improve security and solve limitations tied to label sharing with servers.

Result: The approach improves training efficiency, reduces communication costs, and successfully counters model inversion and label inference attacks, achieving comparable accuracy levels.

Conclusion: By expanding security analyses and optimizing computational demands, SplitHappens offers a practical advancement in protecting SL workflows from emerging attacks.

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [447] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: This paper highlights the need for standardized benchmarks to evaluate AI-driven biological models across domains and modalities, addressing challenges such as data heterogeneity and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the current lack of cross-domain benchmarks, which hampers the development of reliable AI models in biological fields like genomics and proteomics.

Method: The authors convened a workshop with experts to identify bottlenecks and proposed recommendations for building effective benchmarking frameworks.

Result: The paper identifies key bottlenecks like data heterogeneity and biases and proposes solutions like standardized tooling and collaborative platforms.

Conclusion: Standardized benchmarks are essential for ensuring the rigor and trustworthiness of AI-driven Virtual Cell models, advancing biological research and therapeutic insights.

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [448] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: This paper studies how differential privacy optimization algorithms perform under class imbalance with heavy-tail distributions. Gradient Descent struggles with low-frequency classes, but second-order methods like DP-AdamBC improve training accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the optimization behavior of private learning algorithms under challenging class imbalance and heavy-tail distribution scenarios.

Method: The authors analyze optimization behavior using a stylized model and compare optimization mechanisms such as DP-GD and DP-AdamBC. DP-AdamBC adjusts second-order information to mitigate differential privacy bias.

Result: Second-order optimization algorithms outperform DP-GD in learning low-frequency classes of imbalanced distributions. DP-AdamBC shows increases in training accuracy by 8% and 5% for least frequent classes in both controlled setups and real-world data.

Conclusion: Second-order methods (e.g., DP-AdamBC) are crucial for optimizing private learning models in class imbalanced and heavy-tail contexts. These methods address issues in first-order approaches and enhance model performance.

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [449] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: This paper proposes Graph World Model (GWM), a framework combining unstructured and graph-structured data to handle multi-modal tasks using unified token or embedding spaces, outperforming baselines in six diverse domains.


<details>
  <summary>Details</summary>
Motivation: Existing World Models struggle with incorporating structured data often represented as graphs, while graph foundation models lack versatility in dealing with diverse multi-modal data.

Method: The paper introduces GWM, employing a generic message-passing algorithm over multi-modal token or embedding spaces, and incorporates action nodes for diverse tasks via direct reference or similarity computation.

Result: Experiments in six areas, including recommendation, graph prediction, and planning, show GWM's enhanced performance, adaptability to unseen tasks, and benefits from multi-hop structures.

Conclusion: GWM successfully integrates unstructured and structured data in a unified framework, demonstrating versatility and superior performance across multiple domains and tasks.

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [450] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: The paper addresses the inefficiency of relying on a single LLM by introducing FusionBench, a routing benchmark, and FusionFactory, a systematic fusion framework that integrates strengths of multiple models to enhance performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Although a wide variety of LLMs exist with unique capabilities, reliance on a single model leads to inefficiencies and underutilizes the strengths of the broader ecosystem in tasks requiring diverse abilities.

Method: The authors propose FusionBench, a benchmark to assess LLMs across tasks, and FusionFactory, a fusion framework with three levels: query-level routing, thought-level aggregation, and model-level distillation to systematically leverage multiple LLMs.

Result: FusionFactory showed consistent performance improvements over individual LLMs across 14 benchmarks, with the optimal configurations depending on the benchmark.

Conclusion: Systematically combining multiple LLMs via routing, aggregation, and distillation enhances task coverage, performance, and highlights the potential of collaborative use of diverse models.

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [451] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: The paper improves neural-disjunctive normal form (DNF) models by addressing performance losses during post-training symbolic translation via a disentanglement method. This approach delivers more compact and interpretable logical representations.


<details>
  <summary>Details</summary>
Motivation: There is a need to address performance degradation of neural DNF-based models caused by failures in disentangling learned knowledge during the symbolic translation process.

Method: The paper introduces a disentanglement method that splits nodes encoding nested rules into smaller, independent nodes. This ensures better preservation of the pre-translation model's performance.

Result: Experiments on binary, multiclass, and multilabel classification tasks show that the new method leads to compact and interpretable symbolic models with performance closer to the pre-translation models.

Conclusion: The proposed disentanglement approach enhances the preservation of model performance, enabling neural DNF models to maintain their classification abilities while yielding more interpretable logical representations.

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [452] [Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding](https://arxiv.org/abs/2507.09385)
*Kevin Reyes,Vasco Cortez*

Main category: cs.NE

TL;DR: The paper introduces a novel fraud detection method combining Relative Distance Rotating Encoding (ReDRE) with the RoFormer model to enhance the analysis of time series data and improve fraudulent transaction detection.


<details>
  <summary>Details</summary>
Motivation: Fraud detection is crucial for financial systems, especially for payment companies like Flow Payment, who process millions of transactions and seek to ensure security, improve user experience, and build sustainable operations.

Method: A new method integrates Relative Distance Rotating Encoding (ReDRE) into the RoFormer model, enabling the Transformer to better capture time series dependencies and relationships via angle rotation.

Result: The incorporation of ReDRE improves the RoFormer model’s capacity to detect fraudulent transactions by enhancing temporal data characterization.

Conclusion: The proposed method shows potential in advancing fraud detection capabilities and addressing critical security challenges for financial systems.

Abstract: Fraud detection is one of the most important challenges that financial
systems must address. Detecting fraudulent transactions is critical for payment
gateway companies like Flow Payment, which process millions of transactions
monthly and require robust security measures to mitigate financial risks.
Increasing transaction authorization rates while reducing fraud is essential
for providing a good user experience and building a sustainable business. For
this reason, discovering novel and improved methods to detect fraud requires
continuous research and investment for any company that wants to succeed in
this industry. In this work, we introduced a novel method for detecting
transactional fraud by incorporating the Relative Distance Rotating Encoding
(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE
enhances the characterization of time series data within a Transformer, leading
to improved fraud detection by better capturing temporal dependencies and event
relationships.

</details>


### [453] [BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings](https://arxiv.org/abs/2507.09747)
*Dongyang Li,Haoyang Qin,Mingyang Wu,Chen Wei,Quanying Liu*

Main category: cs.NE

TL;DR: The paper presents BrainFLORA, a framework aimed at unifying multimodal neuroimaging data to interpret neural visual representations.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of spatiotemporal misalignment in integrating neuroimaging modalities (EEG, MEG, fMRI) and enable a holistic understanding of neural visual representation.

Method: BrainFLORA uses multimodal large language models enhanced with modality-specific adapters and task decoders to merge data from multiple neuroimaging modalities.

Result: It achieves state-of-the-art results in joint-subject visual retrieval tasks and provides insights into how visual concept representations align with real-world stimuli across neural modalities.

Conclusion: BrainFLORA links neuroscience with machine learning, offering advancements for cognitive neuroscience and BCIs while enabling unified cross-modal neural understanding.

Abstract: Understanding how the brain represents visual information is a fundamental
challenge in neuroscience and artificial intelligence. While AI-driven decoding
of neural data has provided insights into the human visual system, integrating
multimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical
hurdle due to their inherent spatiotemporal misalignment. Current approaches
often analyze these modalities in isolation, limiting a holistic view of neural
representation. In this study, we introduce BrainFLORA, a unified framework for
integrating cross-modal neuroimaging data to construct a shared neural
representation. Our approach leverages multimodal large language models (MLLMs)
augmented with modality-specific adapters and task decoders, achieving
state-of-the-art performance in joint-subject visual retrieval task and has the
potential to extend multitasking. Combining neuroimaging analysis methods, we
further reveal how visual concept representations align across neural
modalities and with real world object perception. We demonstrate that the
brain's structured visual concept representations exhibit an implicit mapping
to physical-world stimuli, bridging neuroscience and machine learning from
different modalities of neural imaging. Beyond methodological advancements,
BrainFLORA offers novel implications for cognitive neuroscience and
brain-computer interfaces (BCIs). Our code is available at
https://github.com/ncclab-sustech/BrainFLORA.

</details>


### [454] [Effective Self-Attention-Based Deep Learning Model with Evolutionary Grid Search for Robust Wave Farm Energy Forecasting](https://arxiv.org/abs/2507.09847)
*Amin Abdollahi Dehkordi,Mehdi Neshat,Nataliia Y. Sergiienko,Zahra Ghasemi,Lei Chen,John Boland,Hamid Moradkhani,Amir H. Gandomi*

Main category: cs.NE

TL;DR: The study presents a predictive model for wave energy output combining deep learning techniques, achieving high accuracy across Australian wave farms.


<details>
  <summary>Details</summary>
Motivation: Wave energy has immense potential but struggles with forecasting challenges, hindering its adoption for carbon neutrality goals.

Method: A hybrid sequential learning model using Self-Attention-enhanced Convolutional Bi-LSTM optimized through hyperparameters.

Result: The model demonstrated high accuracy, outperforming ten other machine learning algorithms, with R2 scores ranging from 82.8% to 91.7%.

Conclusion: The proposed framework enhances wave energy forecasting, aiding integration into power grids and supporting carbon neutrality efforts.

Abstract: Achieving carbon neutrality, a key focus of UN SDG #13, drives the
exploration of wave energy, a renewable resource with the potential to generate
30,000 TWh of clean electricity annually, surpassing global demand. However,
wave energy remains underdeveloped due to technical and economic challenges,
particularly in forecasting wave farm power output, which is vital for grid
stability and commercial viability. This study proposes a novel predictive
framework to enhance wave energy integration into power grids. It introduces a
hybrid sequential learning model combining Self-Attention-enhanced
Convolutional Bi-LSTM with hyperparameter optimization. The model leverages
spatial data from Wave Energy Converters (WECs) and is validated using datasets
from wave farms in Adelaide, Sydney, Perth, and Tasmania, Australia.
Benchmarked against ten machine learning algorithms, the model achieves
superior accuracy, with R2 scores of 91.7% (Adelaide), 88.0% (Perth), 82.8%
(Tasmania), and 91.0% (Sydney). It outperforms conventional ML and deep
learning methods, offering robust and scalable predictions for wave energy
output across diverse marine environments, supporting reliable integration into
energy systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [455] [Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams](https://arxiv.org/abs/2507.09539)
*Anna Bolotina,Christoph M. Kirsch,Stefanie Muroya Lei,Matthias Pleschinger*

Main category: cs.PL

TL;DR: The paper introduces two tools, rotor and bitme, to improve symbolic execution scalability by reasoning at the machine code level and leveraging more advanced solver techniques.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address scalability challenges in symbolic execution, particularly state explosion in control and data flow.

Method: They propose pushing reasoning to the machine code level (RISC-V target) and using tools like rotor and bitme, which incorporate advanced techniques like BDDs and CFLOBDDs for efficient model processing.

Result: Their experiments show state-of-the-art SMT solvers struggle, but their approach demonstrates promise, especially with CFLOBDDs for mitigating state explosion.

Conclusion: Bit-precise reasoning has the potential to scale for software, and tools like rotor and bitme can contribute to this goal by optimizing symbolic execution and solver interactions.

Abstract: Symbolic execution is a powerful technique for analyzing the behavior of
software yet scalability remains a challenge due to state explosion in control
and data flow. Existing tools typically aim at managing control flow
internally, often at the expense of completeness, while offloading reasoning
over data flow to SMT solvers. Moreover, reasoning typically happens on source
code or intermediate representation level to leverage structural information,
making machine code generation part of the trust base. We are interested in
changing the equation in two non-trivial ways: pushing reasoning down to
machine code level, and then offloading reasoning entirely into SMT solvers and
other, possibly more efficient solver technology. In more abstract terms, we
are asking if bit-precise reasoning technology can be made scalable on
software, and not just hardware. For this purpose, we developed two tools
called rotor and bitme for model generation and bounded model checking,
respectively. We chose RISC-V restricted to integer arithmetic as modeling
target for rotor since RISC-V integer semantics is essentially equivalent to
established SMT semantics over bitvectors and arrays of bitvectors. While
state-of-the-art SMT solvers struggle in our experiments, we have evidence that
there is potential for improvement. To show the potential, we have slightly
generalized and then implemented in bitme two types of binary decision diagrams
(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered
binary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input
through models, essentially generalizing constant propagation to domain
propagation. SMT solvers only get involved when model input cannot be
propagated, significanly speeding up SMT solving. We then study the impact on
state explosion of CFLOBDDs, which are potentially more scalable than ADDs.

</details>


### [456] [BeePL: Correct-by-compilation kernel extensions](https://arxiv.org/abs/2507.09883)
*Swarn Priya,Frédéric Besson,Connor Sughrue,Tim Steenvoorden,Jamie Fulford,Freek Verbeek,Binoy Ravindran*

Main category: cs.PL

TL;DR: BeePL introduces a domain-specific language with a formally verified type system for eBPF, addressing verifier limitations by ensuring critical safety properties and providing a verified compilation approach for safe kernel extensions.


<details>
  <summary>Details</summary>
Motivation: To overcome the safety challenges and limitations of the current eBPF verifier, which is overly conservative and unsound, while supporting safe and extensible kernel functionality.

Method: Developed BeePL, a domain-specific language for eBPF, featuring a formally verified type system, static enforcement of safety properties, runtime checks, and a verified compilation strategy using CompCert.

Result: BeePL ensures critical eBPF-specific safety properties like memory safety, termination, and structured control flow, backed by type soundness proofs and runtime checks for dynamic scenarios.

Conclusion: BeePL establishes a reliable and verifiable toolchain for eBPF, making safe kernel extensions feasible while overcoming verifier shortcomings.

Abstract: eBPF is a technology that allows developers to safely extend kernel
functionality without modifying kernel source code or developing loadable
kernel modules. Since the kernel governs critical system operations and
enforces isolation boundaries between user space and privileged data, any
mechanism that modifies its behavior must meet the highest standards of safety
and correctness. To this end, the eBPF toolchain includes a verifier, which
statically checks safety properties such as memory access validity, bounded
loops, and type correctness before loading the program into the kernel.
However, the existing verifier is both overly conservative in some
cases-rejecting valid programs-and unsound in others, permitting unsafe
behavior that violates the intended semantics of the kernel interface.
  To address these challenges, we introduce BeePL, a domain-specific language
for eBPF with a formally verified type system. The BeePL type system, along
with the language design, statically enforces key safety properties such as
type-correct memory access, safe pointer usage, absence of unbounded loops, and
structured control flow. These guarantees are backed by formal type soundness
proofs, ensuring that well-typed programs satisfy the safety invariants
required by the eBPF execution environment. BeePL also proves that well-typed
source programs meet critical eBPF-specific properties related to memory
safety, termination, and control flow, enabling high-level reasoning prior to
compilation. For properties not fully enforceable statically-such as dynamic
bounds and undefined behavior-BeePL inserts semantics-preserving runtime checks
during compilation. We develop a verified compilation strategy that extends
CompCert to generate BPF bytecode from BeePL programs, establishing a
principled foundation for an end-to-end verifiable toolchain for safe kernel
extensions.

</details>


### [457] [Rows and Capabilities as Modal Effects](https://arxiv.org/abs/2507.10301)
*Wenhao Tang,Sam Lindley*

Main category: cs.PL

TL;DR: The paper introduces a uniform framework using modal effect types to analyze and compare row-based and capability-based effect systems for better understanding of effect tracking mechanisms.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding the relationship between disparate effect systems based on row polymorphism and capabilities, and how effect tracking is entangled with other features.

Method: The authors generalize modal effect types into a uniform framework, enabling encodings as macro translations from existing row-based and capability-based systems, preserving types and semantics.

Result: Encodings reveal the essence of effect tracking mechanisms, facilitate direct analysis of differences, and offer insights for language design.

Conclusion: The proposed framework provides a systematic approach to decode and understand various effect systems, offering utility for both comparative analysis and programming language innovation.

Abstract: Effect handlers allow programmers to model and compose computational effects
modularly. Effect systems statically guarantee that all effects are handled.
Several recent practical effect systems are based on either row polymorphism or
capabilities. However, there remains a gap in understanding the precise
relationship between effect systems with such disparate foundations. The main
difficulty is that in both row-based and capability-based systems, effect
tracking is typically entangled with other features such as functions.
  We propose a uniform framework for encoding, analysing, and comparing effect
systems. Our framework exploits and generalises modal effect types, a recent
novel effect system which decouples effect tracking from functions via
modalities. Modalities offer fine-grained control over when and how effects are
tracked, enabling us to express different strategies for effect tracking. We
give encodings as macro translations from existing row-based and
capability-based effect systems into our framework and show that these
encodings preserve types and semantics. Our encodings reveal the essence of
effect tracking mechanisms in different effect systems, enable a direct
analysis on their differences, and provide valuable insights on language
design.

</details>


### [458] [Orthologic Type Systems](https://arxiv.org/abs/2507.10482)
*Simon Guilloud,Viktor Kunčak*

Main category: cs.PL

TL;DR: The paper proposes using orthologic as the foundation for type systems handling intersection, union, negation types, and subtyping. It introduces an extended proof system, algorithms for subtyping decisions, and type normalization.


<details>
  <summary>Details</summary>
Motivation: To address challenges in type systems dealing with complex types such as intersection, union, and negation in the presence of subtyping assumptions.

Method: The authors extend orthologic to include monotonic and antimonotonic functions and develop proof systems with function symbols, enabling partial cut elimination. Algorithms for subtyping relations and type normalization are presented.

Result: They achieve an $O(n^2(1+m))$ algorithm for subtyping decisions under $m$ assumptions, and an $O(n^2)$ normalization algorithm for canonical type simplification.

Conclusion: Orthologic serves as a promising framework for creating robust type systems, offering both algorithmic efficiency and theoretical rigor in handling advanced type operations.

Abstract: We propose to use orthologic as the basis for designing type systems
supporting intersection, union, and negation types in the presence of subtyping
assumptions. We show how to extend orthologic to support monotonic and
antimonotonic functions, supporting the use of type constructors in such type
systems. We present a proof system for orthologic with function symbols,
showing that it admits partial cut elimination. Using these insights, we
present an $\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation
under $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization
algorithm, allowing simplification of types to their minimal canonical form.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [459] [OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation](https://arxiv.org/abs/2507.08851)
*Simon Schwaiger,Stefan Thalhammer,Wilfried Wöber,Gerald Steinbauer-Wagner*

Main category: cs.RO

TL;DR: The paper presents OTAS, a method for open-vocabulary outdoor segmentation, addressing challenges in unstructured environments by clustering and grounding semantic features extracted directly from pretrained vision models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of understanding open-world semantics in unstructured outdoor environments, where traditional object-centric vision-language mappings fail due to issues like semantic ambiguities and unclear class boundaries.

Method: OTAS works by clustering semantically similar token structures from pretrained vision models across single and multiple views, grounding them in language. It reconstructs a geometrically consistent feature field for open-vocabulary segmentation. The method operates zero-shot without scene-specific fine-tuning.

Result: OTAS demonstrated a minor improvement in 2D segmentation IoU on Off-Road Freespace Detection and up to 151% IoU improvement in 3D segmentation on TartanAir. It also runs at ~17 fps and performs well in real-world robotic applications.

Conclusion: OTAS effectively overcomes limitations of traditional and open-vocabulary segmentation methods. Its zero-shot efficiency, competitive performance, and real-time applicability make it suitable for robotic planning in outdoor environments.

Abstract: Understanding open-world semantics is critical for robotic planning and
control, particularly in unstructured outdoor environments. Current
vision-language mapping approaches rely on object-centric segmentation priors,
which often fail outdoors due to semantic ambiguities and indistinct semantic
class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method
for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary
segmentation models by extracting semantic structure directly from the output
tokens of pretrained vision models. By clustering semantically similar
structures across single and multiple views and grounding them in language,
OTAS reconstructs a geometrically consistent feature field that supports
open-vocabulary segmentation queries. Our method operates zero-shot, without
scene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor
IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on
the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU
improvement over open-vocabulary mapping methods in 3D segmentation on
TartanAir. Real-world reconstructions demonstrate OTAS' applicability to
robotic applications. The code and ROS node will be made publicly available
upon paper acceptance.

</details>


### [460] [AirScape: An Aerial Generative World Model with Motion Controllability](https://arxiv.org/abs/2507.08885)
*Baining Zhao,Rongze Tang,Mingyuan Jia,Ziyou Wang,Fanghang Man,Xin Zhang,Yu Shang,Weichen Zhang,Chen Gao,Wei Wu,Xin Wang,Xinlei Chen,Yong Li*

Main category: cs.RO

TL;DR: This paper introduces AirScape, a world model for aerial agents capable of predicting outcomes of motion intentions in 3D space using visual inputs.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the challenge of enabling robots to anticipate the outcomes of their motion intentions in 3D space, an essential aspect of embodied intelligence.

Method: The authors develop AirScape, which involves creating a dataset of 11,000 video-intention pairs and training a world model using a two-phase training schedule.

Result: The resulting model successfully predicts observation sequences in physical 3D environments based on motion intentions and visual cues.

Conclusion: The study demonstrates that the AirScape model effectively improves spatial imagination capabilities for aerial agents, adhering to spatio-temporal constraints.

Abstract: How to enable robots to predict the outcomes of their own motion intentions
in three-dimensional space has been a fundamental problem in embodied
intelligence. To explore more general spatial imagination capabilities, here we
present AirScape, the first world model designed for six-degree-of-freedom
aerial agents. AirScape predicts future observation sequences based on current
visual inputs and motion intentions. Specifically, we construct an dataset for
aerial world model training and testing, which consists of 11k video-intention
pairs. This dataset includes first-person-view videos capturing diverse drone
actions across a wide range of scenarios, with over 1,000 hours spent
annotating the corresponding motion intentions. Then we develop a two-phase
training schedule to train a foundation model -- initially devoid of embodied
spatial knowledge -- into a world model that is controllable by motion
intentions and adheres to physical spatio-temporal constraints.

</details>


### [461] [End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles](https://arxiv.org/abs/2507.08901)
*Zebang Feng,Miao Fan,Bao Liu,Shengtong Xu,Haoyi Xiong*

Main category: cs.RO

TL;DR: EGC-VMAP is a new framework that generates high-precision, city-scale vectorized maps by crowdsourcing vehicles' data, solving the shortcomings of traditional LiDAR and single-vehicle methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate and cost-effective mapping for autonomous driving, as traditional LiDAR methods are expensive and slow, and single-vehicle systems lack robustness.

Method: The paper proposes a Trip-Aware Transformer architecture that fuses multi-vehicle, multi-temporal data directly, alongside hierarchical matching and multi-objective loss for better training and map accuracy.

Result: The method achieves superior map accuracy and robustness, significantly outperforming single-vehicle baselines and reducing manual annotation costs by 90%.

Conclusion: EGC-VMAP offers a scalable and efficient solution for city-scale autonomous driving map generation by leveraging crowdsourced data, ensuring both performance and cost efficiency.

Abstract: High-precision vectorized maps are indispensable for autonomous driving, yet
traditional LiDAR-based creation is costly and slow, while single-vehicle
perception methods lack accuracy and robustness, particularly in adverse
conditions. This paper introduces EGC-VMAP, an end-to-end framework that
overcomes these limitations by generating accurate, city-scale vectorized maps
through the aggregation of data from crowdsourced vehicles. Unlike prior
approaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements
perceived onboard vehicles using a novel Trip-Aware Transformer architecture
within a unified learning process. Combined with hierarchical matching for
efficient training and a multi-objective loss, our method significantly
enhances map accuracy and structural robustness compared to single-vehicle
baselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP
demonstrates superior performance, enabling a scalable, cost-effective solution
for city-wide mapping with a reported 90\% reduction in manual annotation
costs.

</details>


### [462] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: The paper introduces a camera-LiDAR fusion framework utilizing roadside units for HD semantic mapping, supported by RS-seq, a highly annotated multimodal dataset. Results prove this approach's superior performance over unimodal methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of occlusions and limited perspectives in HD semantic mapping at complex intersections using traditional vehicle-based methods.

Method: A two-stage multimodal fusion framework combining camera texture and LiDAR geometric data for semantic integration. RS-seq dataset supports comprehensive evaluation.

Result: The multimodal approach surpasses unimodal methods, improving semantic segmentation mIoU by 4% over camera-only techniques and 18% over LiDAR-only techniques.

Conclusion: The framework establishes a robust methodology for IRU-supported HD semantic mapping, paving the way for infrastructure-aided autonomous driving and future research.

Abstract: High-definition (HD) semantic mapping of complex intersections poses
significant challenges for traditional vehicle-based approaches due to
occlusions and limited perspectives. This paper introduces a novel camera-LiDAR
fusion framework that leverages elevated intelligent roadside units (IRUs).
Additionally, we present RS-seq, a comprehensive dataset developed through the
systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes
precisely labelled camera imagery and LiDAR point clouds collected from
roadside installations, along with vectorized maps for seven intersections
annotated with detailed features such as lane dividers, pedestrian crossings,
and stop lines. This dataset facilitates the systematic investigation of
cross-modal complementarity for HD map generation using IRU data. The proposed
fusion framework employs a two-stage process that integrates modality-specific
feature extraction and cross-modal semantic integration, capitalizing on camera
high-resolution texture and precise geometric data from LiDAR. Quantitative
evaluations using the RS-seq dataset demonstrate that our multimodal approach
consistently surpasses unimodal methods. Specifically, compared to unimodal
baselines evaluated on the RS-seq dataset, the multimodal approach improves the
mean Intersection-over-Union (mIoU) for semantic segmentation by 4\% over the
image-only results and 18\% over the point cloud-only results. This study
establishes a baseline methodology for IRU-based HD semantic mapping and
provides a valuable dataset for future research in infrastructure-assisted
autonomous driving systems.

</details>


### [463] [Towards Human-level Dexterity via Robot Learning](https://arxiv.org/abs/2507.09117)
*Gagan Khandate*

Main category: cs.RO

TL;DR: The paper addresses challenges in robotic dexterous manipulation, building reinforcement learning frameworks and exploring human demonstration-driven learning methods.


<details>
  <summary>Details</summary>
Motivation: Achieving human-level dexterous intelligence using robotic hands, which is key to general embodied intelligence.

Method: Developing structured exploration reinforcement learning frameworks and introducing visuo-tactile human demonstration imitation learning techniques.

Result: Progressively refined reinforcement learning methods, culminating in effective techniques incorporating planning and human demonstration-based methods for dexterity.

Conclusion: The presented approaches mark significant advancements in computational sensorimotor learning for robotic dexterity, pushing closer to human-level skill in manipulation.

Abstract: Dexterous intelligence -- the ability to perform complex interactions with
multi-fingered hands -- is a pinnacle of human physical intelligence and
emergent higher-order cognitive skills. However, contrary to Moravec's paradox,
dexterous intelligence in humans appears simple only superficially. Many
million years were spent co-evolving the human brain and hands including rich
tactile sensing. Achieving human-level dexterity with robotic hands has long
been a fundamental goal in robotics and represents a critical milestone toward
general embodied intelligence. In this pursuit, computational sensorimotor
learning has made significant progress, enabling feats such as arbitrary
in-hand object reorientation. However, we observe that achieving higher levels
of dexterity requires overcoming very fundamental limitations of computational
sensorimotor learning.
  I develop robot learning methods for highly dexterous multi-fingered
manipulation by directly addressing these limitations at their root cause.
Chiefly, through key studies, this disseration progressively builds an
effective framework for reinforcement learning of dexterous multi-fingered
manipulation skills. These methods adopt structured exploration, effectively
overcoming the limitations of random exploration in reinforcement learning. The
insights gained culminate in a highly effective reinforcement learning that
incorporates sampling-based planning for direct exploration. Additionally, this
thesis explores a new paradigm of using visuo-tactile human demonstrations for
dexterity, introducing corresponding imitation learning techniques.

</details>


### [464] [Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning](https://arxiv.org/abs/2507.09123)
*Ziyan Gao,Lijun Wang,Yuntao Kong,Nak Young Chong*

Main category: cs.RO

TL;DR: The paper addresses limitations in current deep-reinforcement-learning methods for the Online Bin Packing Problem (OBPP) by proposing a framework integrating structural stability validation and heuristic planning to improve bin stability and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing deep-reinforcement-learning methods for OBPP, which fail to ensure structural stability of bins or provide mechanisms to reconfigure them safely when items cannot be directly placed.

Method: The framework introduces Load Bearable Convex Polygon (LBCP) for stability validation and Stable Rearrangement Planning (SRP) for rearranging items to accommodate new ones while ensuring stability.

Result: Experiments on OBPP benchmarks show that LBCP effectively validates structural stability, and SRP identifies effort-saving rearrangement strategies, outperforming existing methods.

Conclusion: The proposed method enhances packing stability and adaptability, offering a robust and practical solution for automated packing in industrial and logistics applications.

Abstract: The Online Bin Packing Problem (OBPP) is a sequential decision-making task in
which each item must be placed immediately upon arrival, with no knowledge of
future arrivals. Although recent deep-reinforcement-learning methods achieve
superior volume utilization compared with classical heuristics, the learned
policies cannot ensure the structural stability of the bin and lack mechanisms
for safely reconfiguring the bin when a new item cannot be placed directly. In
this work, we propose a novel framework that integrates packing policy with
structural stability validation and heuristic planning to overcome these
limitations. Specifically, we introduce the concept of Load Bearable Convex
Polygon (LBCP), which provides a computationally efficient way to identify
stable loading positions that guarantee no bin collapse. Additionally, we
present Stable Rearrangement Planning (SRP), a module that rearranges existing
items to accommodate new ones while maintaining overall stability. Extensive
experiments on standard OBPP benchmarks demonstrate the efficiency and
generalizability of our LBCP-based stability validation, as well as the
superiority of SRP in finding the effort-saving rearrangement plans. Our method
offers a robust and practical solution for automated packing in real-world
industrial and logistics applications.

</details>


### [465] [Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization](https://arxiv.org/abs/2507.09160)
*Jialei Huang,Shuo Wang,Fanqi Lin,Yihang Hu,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: The paper introduces Tactile-VLA, a framework that integrates vision, language, action, and tactile sensing to enable robots to adapt in fine-grained, contact-rich scenarios using prior knowledge for zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of precise grounding of generalist robotic agents into physical interactions, especially in contact-rich scenarios requiring fine-grained force control.

Method: The framework combines a hybrid position-force controller with a reasoning module, leveraging tactile sensing to connect vision-language models' implicit knowledge with physical action.

Result: Experiments confirm Tactile-VLA's generalizability and effectiveness in instruction following, commonsense utilization, and adaptive reasoning based on tactile feedback.

Conclusion: By connecting vision-language models to tactile sensors with few demonstrations, Tactile-VLA activates prior knowledge for zero-shot generalization in physical tasks.

Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements,
driven by the rich implicit knowledge of their vision-language components.
However, achieving generalist robotic agents demands precise grounding into
physical interactions, especially in contact-rich scenarios where fine-grained
force control is essential. We advance VLAs' implicit knowledge beyond
identifying what to do, towards guiding how to physically interact with real
world. This paper introduces Tactile-VLA, a novel framework that deeply fuses
vision, language, action, and tactile sensing. This framework incorporates a
hybrid position-force controller to translate the model's intentions into
precise physical actions and a reasoning module that allows the robot to adapt
its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's
effectiveness and generalizability in three key aspects: (1) enabling
tactile-aware instruction following, (2) utilizing tactile-relevant
commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key
finding is that the VLM's prior knowledge already contains semantic
understanding of physical interaction; by connecting it to the robot's tactile
sensors with only a few demonstrations, we can activate this prior knowledge to
achieve zero-shot generalization in contact-rich tasks.

</details>


### [466] [PRAG: Procedural Action Generator](https://arxiv.org/abs/2507.09167)
*Michal Vavrecka,Radoslav Skoviera,Gabriela Sejnova,Karla Stepanova*

Main category: cs.RO

TL;DR: The paper proposes a generator for procedurally creating multi-step solvable manipulation tasks in robotics, ensuring logical and physical validation.


<details>
  <summary>Details</summary>
Motivation: Robotics often requires solvable and efficient manipulation tasks that are realistic and tailored to the robotic environment.

Method: The generator uses symbolic validation for logical consistency and physical validation for solvability within the robotic setup, ensuring only functional tasks are retained.

Result: The generator successfully produced millions of unique, multi-step solvable tasks, with up to 15 actions per sequence.

Conclusion: The approach offers a scalable way to generate rich, solvable robotic tasks, enhancing datasets and training frameworks for manipulation tasks using dense rewards.

Abstract: We present a novel approach for the procedural construction of multi-step
contact-rich manipulation tasks in robotics. Our generator takes as input
user-defined sets of atomic actions, objects, and spatial predicates and
outputs solvable tasks of a given length for the selected robotic environment.
The generator produces solvable tasks by constraining all possible
(nonsolvable) combinations by symbolic and physical validation. The symbolic
validation checks each generated sequence for logical and operational
consistency, and also the suitability of object-predicate relations. Physical
validation checks whether tasks can be solved in the selected robotic
environment. Only the tasks that passed both validators are retained. The
output from the generator can be directly interfaced with any existing
framework for training robotic manipulation tasks, or it can be stored as a
dataset of curated robotic tasks with detailed information about each task.
This is beneficial for RL training as there are dense reward functions and
initial and goal states paired with each subgoal. It allows the user to measure
the semantic similarity of all generated tasks. We tested our generator on
sequences of up to 15 actions resulting in millions of unique solvable
multi-step tasks.

</details>


### [467] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Main category: cs.RO

TL;DR: This paper introduces a new targetless approach for calibrating multiple LiDARs, even with non-overlapping fields of view or imprecise initial setups. It achieves high accuracy and reliability without requiring manual annotation or special patterns.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges of accurately calibrating multi-LiDAR systems, especially in situations where the sensors have non-overlapping fields of view or lack precise initial parameter estimates.

Method: The authors propose a unified optimization framework that combines LiDAR bundle adjustment (LBA) with robust iterative refinement. This method constructs a reference point cloud map via continuous scanning paired with a sliding-window LBA and formulates the extrinsic calibration as a joint LBA optimization problem.

Result: Experiments in both simulation (CARLA) and real-world environments show superior performance compared to existing methods, achieving an average translational error of 5 mm and a rotational error of 0.2° even with significant initial error tolerances.

Conclusion: The proposed framework provides precise, outlier-resistant calibration for multi-LiDAR systems without requiring manual intervention or specialized infrastructure, making it a valuable contribution to 3D map reconstruction systems.

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


### [468] [Informed Hybrid Zonotope-based Motion Planning Algorithm](https://arxiv.org/abs/2507.09309)
*Peng Xie,Johannes Betz,Amr Alanwar*

Main category: cs.RO

TL;DR: This paper introduces HZ-MP, a novel motion planner utilizing hybrid zonotopes and ellipsotope heuristics for solving challenging path planning in nonconvex free spaces.


<details>
  <summary>Details</summary>
Motivation: Path planning in nonconvex free spaces is difficult due to NP-hard complexity when modeled as mixed-integer linear programs (MILPs). Existing planners struggle in narrow gaps or boxed-goal environments.

Method: The approach involves space decomposition paired with low-dimensional face sampling informed by ellipsotope heuristics, aiming to focus exploration on promising regions while reducing excessive and unreachable sampling.

Result: HZ-MP is proven probabilistically complete, asymptotically optimal, converges to near-optimal trajectories in finite time, and scales effectively to cluttered, high-dimensional scenarios.

Conclusion: HZ-MP improves upon informed motion planners by achieving efficient and focused exploration in complex nonconvex spaces, offering better scalability and optimality guarantees.

Abstract: Optimal path planning in nonconvex free spaces is notoriously challenging, as
formulating such problems as mixed-integer linear programs (MILPs) is NP-hard.
We propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an
alternative approach that decomposes the obstacle-free space and performs
low-dimensional face sampling guided by an ellipsotope heuristic, enabling
focused exploration along promising transit regions. This structured
exploration eliminates the excessive, unreachable sampling that degrades
existing informed planners such as AIT* and EIT* in narrow gaps or boxed-goal
scenarios. We prove that HZ-MP is probabilistically complete and asymptotically
optimal. It converges to near-optimal trajectories in finite time and scales to
high-dimensional cluttered scenes.

</details>


### [469] [Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics](https://arxiv.org/abs/2507.09340)
*Hongyu Nie,Xingyu Li,Xu Liu,Zhaotong Tan,Sen Mei,Wenbo Su*

Main category: cs.RO

TL;DR: This paper introduces RMRP and RPATR to tackle challenges in autonomous navigation for UAVs and UGVs. The approach emphasizes efficient mapping and safe navigation.


<details>
  <summary>Details</summary>
Motivation: Current methods for autonomous navigation in mobile robots are computationally demanding, ineffective in large-scale complex environments, and lack perception-aware strategies.

Method: The authors propose Random Mapping and Random Projection (RMRP) for lightweight mapping, alongside the RPATR framework for navigation. UAV methods unify grid and ESDF maps, while UGV methods focus on terrain characterization.

Result: RMRP and RPATR significantly improved mapping in terms of memory, execution time, and accuracy, enabling efficient and safe trajectory planning for UAVs and UGVs.

Conclusion: The proposed method offers a robust, scalable solution for autonomous navigation, with theoretical guarantees and practical validations. Open-source code will enhance community engagement.

Abstract: Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.

</details>


### [470] [C-ZUPT: Stationarity-Aided Aerial Hovering](https://arxiv.org/abs/2507.09344)
*Daniel Engelsman,Itzik Klein*

Main category: cs.RO

TL;DR: This paper introduces a novel Controlled Zero-Velocity Update (C-ZUPT) approach to enhance navigation and control in aerial systems by mitigating inertial drift and improving energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of drift-prone state estimation in environments where satellite-based and camera systems are unavailable, and inertial sensors suffer from noise and biases.

Method: The paper presents C-ZUPT, which leverages quasi-static equilibria to identify uncertainty thresholds and provide velocity updates for aerial systems, without depending on surface contact.

Result: The C-ZUPT method reduced inertial drift, decreased control effort, mitigated filter divergence, and improved energy efficiency, all of which extend the sustained flight capability of aerial systems.

Conclusion: C-ZUPT offers a significant improvement in drift resilience and navigation stability, posing a valuable advancement for autonomous aerial systems operating in resource-constrained environments.

Abstract: Autonomous systems across diverse domains have underscored the need for
drift-resilient state estimation. Although satellite-based positioning and
cameras are widely used, they often suffer from limited availability in many
environments. As a result, positioning must rely solely on inertial sensors,
leading to rapid accuracy degradation over time due to sensor biases and noise.
To counteract this, alternative update sources-referred to as information
aiding-serve as anchors of certainty. Among these, the zero-velocity update
(ZUPT) is particularly effective in providing accurate corrections during
stationary intervals, though it is restricted to surface-bound platforms. This
work introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and
control, independent of surface contact. By defining an uncertainty threshold,
C-ZUPT identifies quasi-static equilibria to deliver precise velocity updates
to the estimation filter. Extensive validation confirms that these
opportunistic, high-quality updates significantly reduce inertial drift and
control effort. As a result, C-ZUPT mitigates filter divergence and enhances
navigation stability, enabling more energy-efficient hovering and substantially
extending sustained flight-key advantages for resource-constrained aerial
systems.

</details>


### [471] [Constrained Style Learning from Imperfect Demonstrations under Task Optimality](https://arxiv.org/abs/2507.09371)
*Kehan Wen,Chenhao Li,Junzhe He,Marco Hutter*

Main category: cs.RO

TL;DR: The paper proposes using constrained Markov Decision Processes (CMDP) for balancing stylistic imitation and task performance in robots, validated by experiments across platforms including ANYmal-D hardware.


<details>
  <summary>Details</summary>
Motivation: Learning from demonstrations for robotics often struggles with balancing style and task performance, especially when demonstrations are incomplete or unrealistic.

Method: Formulated as a CMDP, the method optimizes a style imitation goal while enforcing constraints for task performance using an adaptive Lagrangian multiplier.

Result: Experiments demonstrate improved task performance and successfully captured stylistic nuances. Results include efficient energy usage and agile gait patterns on ANYmal-D hardware.

Conclusion: The approach ensures robust task performance while preserving high-quality stylistic behaviors, validated across diverse robots and tasks.

Abstract: Learning from demonstration has proven effective in robotics for acquiring
natural behaviors, such as stylistic motions and lifelike agility, particularly
when explicitly defining style-oriented reward functions is challenging.
Synthesizing stylistic motions for real-world tasks usually requires balancing
task performance and imitation quality. Existing methods generally depend on
expert demonstrations closely aligned with task objectives. However, practical
demonstrations are often incomplete or unrealistic, causing current methods to
boost style at the expense of task performance. To address this issue, we
propose formulating the problem as a constrained Markov Decision Process
(CMDP). Specifically, we optimize a style-imitation objective with constraints
to maintain near-optimal task performance. We introduce an adaptively
adjustable Lagrangian multiplier to guide the agent to imitate demonstrations
selectively, capturing stylistic nuances without compromising task performance.
We validate our approach across multiple robotic platforms and tasks,
demonstrating both robust task performance and high-fidelity style learning. On
ANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile
gait pattern, showcasing real-world benefits.

</details>


### [472] [Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields](https://arxiv.org/abs/2507.09383)
*Wondmgezahu Teshome,Kian Behzad,Octavia Camps,Michael Everett,Milad Siami,Mario Sznaier*

Main category: cs.RO

TL;DR: This paper introduces a motion planning system combining diffusion models and artificial potential fields for robust trajectory generation in complex environments, focusing on pursuit-evasion situations.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenge of pursuit-evasion problems where robust, real-time trajectory planning is needed in complex and dynamic environments.

Method: The method integrates energy-based diffusion models with classifier-free guidance training and local artificial potential fields. It processes obstacle information from point clouds and iteratively refines trajectories in dynamic situations.

Result: The system successfully generates initial trajectories and effectively refines them in pursuit-evasion scenarios, even under partial observability.

Conclusion: The framework demonstrates its utility for real-time and robust motion planning, offering efficient obstacle avoidance and adaptability in pursuit-evasion contexts.

Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.

</details>


### [473] [Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems](https://arxiv.org/abs/2507.09463)
*Anoop Kiran,Nora Ayanian,Kenneth Breuer*

Main category: cs.RO

TL;DR: This paper extensively studies the downwash effects in multi-quadrotor systems to improve performance and functionality in dense environments.


<details>
  <summary>Details</summary>
Motivation: Current multi-quadrotor systems struggle with aerodynamic disturbances like downwash, leading to the need for large operational spaces and limiting their deployment capabilities.

Method: The study employs data-driven approaches, measuring forces, torques, and using particle image velocimetry (PIV) to analyze downwash effects on single and multiple quadrotor setups.

Result: The analysis offers detailed insights into the aerodynamic interactions, providing valuable data for optimizing formations, enhancing coordination, and improving system robustness.

Conclusion: The findings enable the development of physics-based strategies to address downwash challenges, paving the way for more efficient and compact multi-quadrotor deployments in constrained environments.

Abstract: Flying multiple quadrotors in close proximity presents a significant
challenge due to complex aerodynamic interactions, particularly downwash
effects that are known to destabilize vehicles and degrade performance.
Traditionally, multi-quadrotor systems rely on conservative strategies, such as
collision avoidance zones around the robot volume, to circumvent this effect.
This restricts their capabilities by requiring a large volume for the operation
of a multi-quadrotor system, limiting their applicability in dense
environments. This work provides a comprehensive, data-driven analysis of the
downwash effect, with a focus on characterizing, analyzing, and understanding
forces, moments, and velocities in both single and multi-quadrotor
configurations. We use measurements of forces and torques to characterize
vehicle interactions, and particle image velocimetry (PIV) to quantify the
spatial features of the downwash wake for a single quadrotor and an interacting
pair of quadrotors. This data can be used to inform physics-based strategies
for coordination, leverage downwash for optimized formations, expand the
envelope of operation, and improve the robustness of multi-quadrotor control.

</details>


### [474] [Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm](https://arxiv.org/abs/2507.09464)
*Azfar Azdi Arfakhsyad,Aufa Nasywa Rahman,Larasati Kinanti,Ahmad Ataka Awwalur Rizqi,Hannan Nur Muhammad*

Main category: cs.RO

TL;DR: This paper presents software for precise UAV modeling using cost-effective sensors, filtering algorithms, and sensor fusion techniques to improve data quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the demand for accurate modeling of UAVs to support efficient developmental testing processes.

Method: Data from cost-effective sensors are processed using filtering algorithms and sensor fusion techniques. IMU data is used for UAV orientation with Quaternion Representation, and GPS and accelerometer sensors are combined for real-time position updates.

Result: The proposed software demonstrates high accuracy and fluid rendering of UAV orientation and location during flight operations.

Conclusion: The software successfully enhances UAV modeling by integrating cost-effective sensors, ensuring reliable orientation and position tracking.

Abstract: Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving
the demand for accurate modeling to support developmental testing. This paper
proposes data-driven modeling software for UAV. Emphasizes the utilization of
cost-effective sensors to obtain orientation and location data subsequently
processed through the application of data filtering algorithms and sensor
fusion techniques to improve the data quality to make a precise model
visualization on the software. UAV's orientation is obtained using processed
Inertial Measurement Unit (IMU) data and represented using Quaternion
Representation to avoid the gimbal lock problem. The UAV's location is
determined by combining data from the Global Positioning System (GPS), which
provides stable geographic coordinates but slower data update frequency, and
the accelerometer, which has higher data update frequency but integrating it to
get position data is unstable due to its accumulative error. By combining data
from these two sensors, the software is able to calculate and continuously
update the UAV's real-time position during its flight operations. The result
shows that the software effectively renders UAV orientation and position with
high degree of accuracy and fluidity

</details>


### [475] [mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization](https://arxiv.org/abs/2507.09469)
*Haoyang Wang,Jingao Xu,Xinyu Luo,Ting Zhang,Xuecheng Chen,Ruiyang Duan,Jialong Chen,Yunhao Liu,Jianfeng Zheng,Weijie Hong,Xinlei Chen*

Main category: cs.RO

TL;DR: The paper introduces mmE-Loc, a ground-based system for precise drone landings using event cameras and mmWave radar to achieve superior accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: Address the system throughput bottlenecks in traditional drone localization setups caused by mismatched sampling frequencies of frame cameras and mmWave radar.

Method: Leverages event cameras (harmonized sampling frequency with mmWave radar) and proposes two modules: (i) Consistency-instructed Collaborative Tracking for precise measurement extraction and (ii) Graph-informed Adaptive Joint Optimization for efficient sensor fusion and localization.

Result: Experiments with a drone delivery company demonstrate that mmE-Loc outperforms existing methods in both accuracy and latency during drone landings.

Conclusion: The proposed mmE-Loc system enhances precise and efficient drone landings, showcasing its potential in real-world applications by addressing conventional limitations.

Abstract: For precise, efficient, and safe drone landings, ground platforms should
real-time, accurately locate descending drones and guide them to designated
spots. While mmWave sensing combined with cameras improves localization
accuracy, lower sampling frequency of traditional frame cameras compared to
mmWave radar creates bottlenecks in system throughput. In this work, we upgrade
traditional frame camera with event camera, a novel sensor that harmonizes in
sampling frequency with mmWave radar within ground platform setup, and
introduce mmE-Loc, a high-precision, low-latency ground localization system
designed for precise drone landings. To fully exploit the \textit{temporal
consistency} and \textit{spatial complementarity} between these two modalities,
we propose two innovative modules: \textit{(i)} the Consistency-instructed
Collaborative Tracking module, which further leverages the drone's physical
knowledge of periodic micro-motions and structure for accurate measurements
extraction, and \textit{(ii)} the Graph-informed Adaptive Joint Optimization
module, which integrates drone motion information for efficient sensor fusion
and drone localization. Real-world experiments conducted in landing scenarios
with a drone delivery company demonstrate that mmE-Loc significantly
outperforms state-of-the-art methods in both accuracy and latency.

</details>


### [476] [TruckV2X: A Truck-Centered Perception Dataset](https://arxiv.org/abs/2507.09505)
*Tenghui Xie,Zhiying Song,Fuxi Wen,Jun Li,Guangzhao Liu,Zijian Zhao*

Main category: cs.RO

TL;DR: TruckV2X introduces the first cooperative perception dataset focusing on heavy-duty trucks with multi-modal and multi-agent scenarios, addressing challenges like blind spots and occlusions.


<details>
  <summary>Details</summary>
Motivation: To improve safety and cost-efficiency in autonomous trucking while addressing limitations in perception due to truck size and trailer dynamics.

Method: Developed TruckV2X, a large-scale dataset with LiDAR and camera sensing data, involving collaboration among tractors, trailers, and other agents.

Result: TruckV2X provides benchmarks for truck-oriented collaborative perception and insights into improving occlusion handling in autonomous heavy vehicles.

Conclusion: The dataset lays the groundwork for advancing cooperative perception systems and accelerating autonomous truck development.

Abstract: Autonomous trucking offers significant benefits, such as improved safety and
reduced costs, but faces unique perception challenges due to trucks' large size
and dynamic trailer movements. These challenges include extensive blind spots
and occlusions that hinder the truck's perception and the capabilities of other
road users. To address these limitations, cooperative perception emerges as a
promising solution. However, existing datasets predominantly feature light
vehicle interactions or lack multi-agent configurations for heavy-duty vehicle
scenarios. To bridge this gap, we introduce TruckV2X, the first large-scale
truck-centered cooperative perception dataset featuring multi-modal sensing
(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and
RSUs). We further investigate how trucks influence collaborative perception
needs, establishing performance benchmarks while suggesting research priorities
for heavy vehicle perception. The dataset provides a foundation for developing
cooperative perception systems with enhanced occlusion handling capabilities,
and accelerates the deployment of multi-agent autonomous trucking systems. The
TruckV2X dataset is available at
https://huggingface.co/datasets/XieTenghu1/TruckV2X.

</details>


### [477] [Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles](https://arxiv.org/abs/2507.09537)
*Yangang Ren,Guojian Zhan,Chen Lv,Jun Li,Fenghua Liang,Keqiang Li*

Main category: cs.RO

TL;DR: The paper introduces Plan-MAE, a pretraining framework using masked autoencoders to improve prediction and planning for automated vehicles.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing automated vehicle systems which focus too narrowly on imitation learning against ground truth metrics, disregarding holistic scene understanding.

Method: Plan-MAE uses three tasks—reconstructing masked road networks, agent trajectories, and navigation routes—along with a local sub-planning task for better contextual understanding and trajectory prediction. It fine-tunes this pretrained model for downstream tasks.

Result: Plan-MAE significantly outperforms existing methods in key planning metrics in experiments on large-scale datasets.

Conclusion: Plan-MAE proves to be a crucial pre-training approach for advanced learning-based motion planning in automated vehicles.

Abstract: Predicting the future of surrounding agents and accordingly planning a safe,
goal-directed trajectory are crucial for automated vehicles. Current methods
typically rely on imitation learning to optimize metrics against the ground
truth, often overlooking how scene understanding could enable more holistic
trajectories. In this paper, we propose Plan-MAE, a unified pretraining
framework for prediction and planning that capitalizes on masked autoencoders.
Plan-MAE fuses critical contextual understanding via three dedicated tasks:
reconstructing masked road networks to learn spatial correlations, agent
trajectories to model social interactions, and navigation routes to capture
destination intents. To further align vehicle dynamics and safety constraints,
we incorporate a local sub-planning task predicting the ego-vehicle's near-term
trajectory segment conditioned on earlier segment. This pretrained model is
subsequently fine-tuned on downstream tasks to jointly generate the prediction
and planning trajectories. Experiments on large-scale datasets demonstrate that
Plan-MAE outperforms current methods on the planning metrics by a large margin
and can serve as an important pre-training step for learning-based motion
planner.

</details>


### [478] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Main category: cs.RO

TL;DR: The paper explores the use of spiking neural networks (SNNs) for direct robot navigation and obstacle avoidance using LIDAR data, demonstrating that careful tuning of neuron membrane leakage achieves performance comparable to conventional CNNs.


<details>
  <summary>Details</summary>
Motivation: Spiking Neural Networks (SNNs) can offer high-precision processing with low computational and memory requirements, which is ideal for battery and payload-constrained robotics applications like drones and rovers.

Method: The study involves setting up a custom robot with LIDAR, collecting a dataset of LIDAR data labeled with human-operated control commands, and examining the impact of tuning the membrane leakage constant in Leaky Integrate-and-Fire (LIF) neurons within the SNN.

Result: Carefully tuning the membrane potential leakage enables SNNs to match the robot control precision of traditional CNNs for obstacle avoidance tasks.

Conclusion: SNNs, with tuned neuron properties, are a viable alternative to traditional non-spiking approaches like CNNs for obstacle avoidance in resource-constrained robots. The authors also release an open-source LIDAR dataset to support future research.

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [479] [IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance](https://arxiv.org/abs/2507.09714)
*Yifan Zeng,Yihan Li,Suiyi He,Koushil Sreenath,Jun Zeng*

Main category: cs.RO

TL;DR: The paper introduces IteraOptiRacing, a strategy for autonomous racing that achieves time-optimal and collision-free trajectories by improving traditional i2LQR methods.


<details>
  <summary>Details</summary>
Motivation: To create a robust autonomous racing system capable of optimizing lap time performance and avoiding collisions with dynamic and competitive obstacles.

Method: The method is based on i2LQR, utilizing the ego car's iterative historical data for obstacle avoidance and time cost optimization. Parallel computing enables real-time operation in dynamic scenarios.

Result: Simulations demonstrate the strategy's superiority over existing methods, achieving better maneuverability and performance in randomly generated competitive scenarios.

Conclusion: This unified approach allows autonomous racing vehicles to achieve reliable and efficient performance against dynamic obstacles in competitive environments.

Abstract: This paper presents a unified planning-control strategy for competing with
other racing cars called IteraOptiRacing in autonomous racing environments.
This unified strategy is proposed based on Iterative Linear Quadratic Regulator
for Iterative Tasks (i2LQR), which can improve lap time performance in the
presence of surrounding racing obstacles. By iteratively using the ego car's
historical data, both obstacle avoidance for multiple moving cars and time cost
optimization are considered in this unified strategy, resulting in
collision-free and time-optimal generated trajectories. The algorithm's
constant low computation burden and suitability for parallel computing enable
real-time operation in competitive racing scenarios. To validate its
performance, simulations in a high-fidelity simulator are conducted with
multiple randomly generated dynamic agents on the track. Results show that the
proposed strategy outperforms existing methods across all randomly generated
autonomous racing scenarios, enabling enhanced maneuvering for the ego racing
car.

</details>


### [480] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: The paper introduces a biologically-inspired autonomous navigation system mimicking ant behavior, leveraging a Mushroom Body model for visual homing in outdoor environments on a compact robot.


<details>
  <summary>Details</summary>
Motivation: To develop a resource-efficient, biologically inspired system for autonomous visual navigation using minimal sensory input, inspired by ants.

Method: The authors implemented a lateralized Mushroom Body architecture on a robot, enabling visual encoding during simulated and real-world experiments, including angular path integration and goal-memory bank categorization.

Result: The system successfully demonstrated robust visual homing behavior during four experiments, including precise goal-stop behavior in natural settings with minimal computational requirements.

Conclusion: Through biologically grounded mechanisms, the system showcases efficient and accurate visual homing capabilities, paving the way for advancements in autonomous navigation technologies.

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [481] [Active Probing with Multimodal Predictions for Motion Planning](https://arxiv.org/abs/2507.09822)
*Darshan Gadginmath,Farhad Nawaz,Minjun Sung,Faizan M Tariq,Sangjae Bae,David Isele,Fabio Pasqualetti,Jovin Dsa*

Main category: cs.RO

TL;DR: The paper presents a unified framework for autonomous navigation in dynamic environments by integrating trajectory planning, multimodal predictions, and active probing to handle uncertainty.


<details>
  <summary>Details</summary>
Motivation: The aim is to enable autonomous systems to navigate dynamic environments effectively by addressing uncertainties in the behavior of other agents.

Method: The authors develop a novel risk metric compatible with Gaussian mixture models, allowing a closed-form solution. They employ active probing to strategically reduce prediction ambiguities while maintaining multimodal uncertainty handling.

Result: Extensive evaluations in the MetaDrive simulation environment show the framework's ability to skillfully navigate complex traffic scenarios and perform well across different agent behavior models.

Conclusion: The proposed framework is robust and broadly applicable, offering enhanced decision-making for autonomous navigation in uncertain and dynamic settings.

Abstract: Navigation in dynamic environments requires autonomous systems to reason
about uncertainties in the behavior of other agents. In this paper, we
introduce a unified framework that combines trajectory planning with multimodal
predictions and active probing to enhance decision-making under uncertainty. We
develop a novel risk metric that seamlessly integrates multimodal prediction
uncertainties through mixture models. When these uncertainties follow a
Gaussian mixture distribution, we prove that our risk metric admits a
closed-form solution, and is always finite, thus ensuring analytical
tractability. To reduce prediction ambiguity, we incorporate an active probing
mechanism that strategically selects actions to improve its estimates of
behavioral parameters of other agents, while simultaneously handling multimodal
uncertainties. We extensively evaluate our framework in autonomous navigation
scenarios using the MetaDrive simulation environment. Results demonstrate that
our active probing approach successfully navigates complex traffic scenarios
with uncertain predictions. Additionally, our framework shows robust
performance across diverse traffic agent behavior models, indicating its broad
applicability to real-world autonomous navigation challenges. Code and videos
are available at
https://darshangm.github.io/papers/active-probing-multimodal-predictions/.

</details>


### [482] [Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems](https://arxiv.org/abs/2507.09836)
*Vindula Jayawardana,Sirui Li,Yashar Farid,Cathy Wu*

Main category: cs.RO

TL;DR: This paper introduces Multi-Residual Mixture of Expert Learning (MRMEL), a method to enhance Lagrangian traffic control for autonomous vehicles by combining mixture of experts and residual reinforcement learning, leading to improved eco-driving performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of designing robust and effective Lagrangian traffic control policies for autonomous vehicles, given the diversity and complexity in real-world traffic scenarios.

Method: The method is based on a novel framework called MRMEL, which augments a suboptimal nominal policy with a learned residual and dynamically selects the most suitable nominal policy from a pool of policies, modeled as a mixture of experts and conditioned on traffic scenarios.

Result: Using real-world traffic scenarios and a case study in cooperative eco-driving at signalized intersections, MRMEL achieved 4%-9% additional reductions in aggregate vehicle emissions compared to the strongest baseline policies.

Conclusion: MRMEL demonstrates a robust solution to manage diverse traffic scenarios, proving its effectiveness in enhancing autonomous vehicle control for sustainable transportation goals while outperforming baseline approaches.

Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their
applications now extending beyond just a mode of transportation to serving as
mobile actuators of a traffic flow to control flow dynamics. This contrasts
with traditional fixed-location actuators, such as traffic signals, and is
referred to as Lagrangian traffic control. However, designing effective
Lagrangian traffic control policies for AVs that generalize across traffic
scenarios introduces a major challenge. Real-world traffic environments are
highly diverse, and developing policies that perform robustly across such
diverse traffic scenarios is challenging. It is further compounded by the joint
complexity of the multi-agent nature of traffic systems, mixed motives among
participants, and conflicting optimization objectives subject to strict
physical and external constraints. To address these challenges, we introduce
Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for
Lagrangian traffic control that augments a given suboptimal nominal policy with
a learned residual while explicitly accounting for the structure of the traffic
scenario space. In particular, taking inspiration from residual reinforcement
learning, MRMEL augments a suboptimal nominal AV control policy by learning a
residual correction, but at the same time dynamically selects the most suitable
nominal policy from a pool of nominal policies conditioned on the traffic
scenarios and modeled as a mixture of experts. We validate MRMEL using a case
study in cooperative eco-driving at signalized intersections in Atlanta, Dallas
Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.
The results show that MRMEL consistently yields superior performance-achieving
an additional 4%-9% reduction in aggregate vehicle emissions relative to the
strongest baseline in each setting.

</details>


### [483] [AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective](https://arxiv.org/abs/2507.09857)
*Xiaofei Wang,Mingliang Han,Tianyu Hao,Cegang Li,Yunbo Zhao,Keke Tang*

Main category: cs.RO

TL;DR: The paper introduces AdvGrasp, a framework for physical adversarial attacks on robotic grasping, targeting lift capability and grasp stability by deforming object shapes to compromise grasp performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate and enhance the robustness of robotic grasping systems by addressing physical principles often overlooked in neural network-focused studies.

Method: AdvGrasp uses object shape deformation to increase gravitational torque and diminish wrench space stability margin, systematically affecting grasp capability and stability.

Result: Experiments across diverse scenarios and real-world validations demonstrate the effectiveness, robustness, and practical applicability of the AdvGrasp framework.

Conclusion: AdvGrasp successfully provides insights to evaluate and improve robotic grasping robustness, showcasing a physically-grounded approach to adversarial attacks.

Abstract: Adversarial attacks on robotic grasping provide valuable insights into
evaluating and improving the robustness of these systems. Unlike studies that
focus solely on neural network predictions while overlooking the physical
principles of grasping, this paper introduces AdvGrasp, a framework for
adversarial attacks on robotic grasping from a physical perspective.
Specifically, AdvGrasp targets two core aspects: lift capability, which
evaluates the ability to lift objects against gravity, and grasp stability,
which assesses resistance to external disturbances. By deforming the object's
shape to increase gravitational torque and reduce stability margin in the
wrench space, our method systematically degrades these two key grasping
metrics, generating adversarial objects that compromise grasp performance.
Extensive experiments across diverse scenarios validate the effectiveness of
AdvGrasp, while real-world validations demonstrate its robustness and practical
applicability

</details>


### [484] [Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths](https://arxiv.org/abs/2507.09858)
*Shuaikang Wang,Tiecheng Guo,Meng Guo*

Main category: cs.RO

TL;DR: The paper proposes a novel method to customize the topological properties of harmonic potential fields for robot navigation in complex environments such as forest worlds.


<details>
  <summary>Details</summary>
Motivation: There is a lack of existing approaches that allow customization of harmonic potential fields and the resulting paths' topological properties for safe robot navigation.

Method: They introduced a hybrid optimization technique that automatically determines homotopy classes of paths, optimizes tree structures, and fine-tunes weight parameters using diffomorphic transformations and gradient descent.

Result: The proposed method was validated through simulations and hardware experiments, showcasing the capability to customize navigation paths with desired homotopic properties.

Conclusion: The approach simplifies robot navigation design, ensures safety and convergence, and allows for flexible adaptations to complex environments.

Abstract: Safe navigation within a workspace is a fundamental skill for autonomous
robots to accomplish more complex tasks. Harmonic potentials are artificial
potential fields that are analytical, globally convergent and provably free of
local minima. Thus, it has been widely used for generating safe and reliable
robot navigation control policies. However, most existing methods do not allow
customization of the harmonic potential fields nor the resulting paths,
particularly regarding their topological properties. In this paper, we propose
a novel method that automatically finds homotopy classes of paths that can be
generated by valid harmonic potential fields. The considered complex workspaces
can be as general as forest worlds consisting of numerous overlapping
star-obstacles. The method is based on a hybrid optimization algorithm that
searches over homotopy classes, selects the structure of each tree-of-stars
within the forest, and optimizes over the continuous weight parameters for each
purged tree via the projected gradient descent. The key insight is to transform
the forest world to the unbounded point world via proper diffeomorphic
transformations. It not only facilitates a simpler design of the
multi-directional D-signature between non-homotopic paths, but also retain the
safety and convergence properties. Extensive simulations and hardware
experiments are conducted for non-trivial scenarios, where the navigation
potentials are customized for desired homotopic properties. Project page:
https://shuaikang-wang.github.io/CustFields.

</details>


### [485] [Demonstrating the Octopi-1.5 Visual-Tactile-Language Model](https://arxiv.org/abs/2507.09985)
*Samson Yu,Kelvin Lin,Harold Soh*

Main category: cs.RO

TL;DR: Octopi-1.5, an improved visual-tactile-language model, introduces multi-part tactile signal processing and retrieval-augmented generation to enhance task performance and learning. It includes a user-friendly handheld tactile interface for practical demonstrations.


<details>
  <summary>Details</summary>
Motivation: To advance robotic manipulation and material identification through touch-based models and provide accessible setups for non-robot users.

Method: Octopi-1.5 uses tactile signals from multiple object parts and incorporates a retrieval-augmented generation module for enhanced adaptability. It can interact via a handheld tactile-enabled interface equipped with GelSight and TAC-02 sensors.

Result: The model demonstrates improved inference capabilities, learning new objects on-the-fly, and showcases practical interaction through a Guessing Game and teaching new items.

Conclusion: Octopi-1.5 highlights both progress and current limitations in visual-tactile-language models, aiming to encourage further research in touch-based AI technologies.

Abstract: Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.

</details>


### [486] [Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy](https://arxiv.org/abs/2507.10003)
*Mohit Singh,Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: This study integrates advanced autonomy solutions into Ariel, an underwater robot, using vision-based systems and refraction-aware state estimation methods. Field tests demonstrate successful performance in challenging underwater environments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of underwater exploration and inspection, particularly in visually degraded environments, by equipping robots with more robust vision-driven autonomy capabilities.

Method: The authors developed and integrated a vision-driven autonomy system into Ariel, using a multi-camera visual-inertial state estimation and a learning-based proprioceptive velocity prediction method. This approach improves functionality even in challenging visual conditions.

Result: Field tests in Trondheim's submarine dry dock demonstrated the robustness of the system in challenging environments and verified the generalizability of the path planning techniques across different robot designs.

Conclusion: The integration of refraction-aware visual-inertial state estimation with learning-based predictive models and autonomous exploration techniques successfully provides aerial drone-level autonomy underwater, making it viable for future underwater applications.

Abstract: This work presents a vision-based underwater exploration and inspection
autonomy solution integrated into Ariel, a custom vision-driven underwater
robot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a
refraction-aware multi-camera visual-inertial state estimation method aided by
a learning-based proprioceptive robot velocity prediction method that enhances
robustness against visual degradation. Furthermore, our previously developed
and extensively field-verified autonomous exploration and general visual
inspection solution is integrated on Ariel, providing aerial drone-level
autonomy underwater. The proposed system is field-tested in a submarine dry
dock in Trondheim under challenging visual conditions. The field demonstration
shows the robustness of the state estimation solution and the generalizability
of the path planning techniques across robot embodiments.

</details>


### [487] [Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots](https://arxiv.org/abs/2507.10030)
*Marco Calì,Alberto Sinigaglia,Niccolò Turcato,Ruggero Carli,Gian Antonio Susto*

Main category: cs.RO

TL;DR: The paper combines Deep Reinforcement Learning (RL) with Evolutionary Strategies (ES) to fine-tune robotic control policies, achieving better performance and robustness for underactuated systems.


<details>
  <summary>Details</summary>
Motivation: To improve the performance and robustness of Deep RL policies in controlling underactuated robotic systems, especially for specific task objectives.

Method: The approach involves initial policy training with Soft-Actor Critic (SAC) using a surrogate reward function, followed by refinement through the Separable Natural Evolution Strategy (SNES), targeting the original scoring metric.

Result: Experimental tests during the 2nd AI Olympics using RealAIGym showcased that the fine-tuned controllers outperformed baseline approaches and achieved competitive competition scores.

Conclusion: Combining RL with ES leads to enhanced control performance and robustness, making it a valuable strategy for underactuated robotic tasks.

Abstract: Deep Reinforcement Learning (RL) has emerged as a powerful method for
addressing complex control problems, particularly those involving underactuated
robotic systems. However, in some cases, policies may require refinement to
achieve optimal performance and robustness aligned with specific task
objectives. In this paper, we propose an approach for fine-tuning Deep RL
policies using Evolutionary Strategies (ES) to enhance control performance for
underactuated robots. Our method involves initially training an RL agent with
Soft-Actor Critic (SAC) using a surrogate reward function designed to
approximate complex specific scoring metrics. We subsequently refine this
learned policy through a zero-order optimization step employing the Separable
Natural Evolution Strategy (SNES), directly targeting the original score.
Experimental evaluations conducted in the context of the 2nd AI Olympics with
RealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning
significantly improves agent performance while maintaining high robustness. The
resulting controllers outperform established baselines, achieving competitive
scores for the competition tasks.

</details>


### [488] [MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks](https://arxiv.org/abs/2507.10047)
*Marc Kaufeld,Mattia Piccinini,Johannes Betz*

Main category: cs.RO

TL;DR: The paper introduces MP-RBFN, a new method for generating motion primitives using Radial Basis Function Networks (RBFNs) to improve efficiency and accuracy in autonomous driving motion planning.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving motion planning faces challenges like computational inefficiency in optimization-based methods and geometric constraints in sampling-based approaches. The paper aims to address these limitations.

Method: MP-RBFN integrates high-fidelity trajectory generation from sampling-based methods with accurate vehicle dynamics modeling using RBFNs. The approach was validated via empirical testing.

Result: MP-RBFN achieves seven times higher accuracy in optimized motion primitive generation compared to existing semi-analytic methods and offers low inference times, demonstrating superior performance.

Conclusion: The method effectively combines the strengths of optimization and sampling-based planning and is practically applicable to trajectory planners. The software is open-source for broader use.

Abstract: This research introduces MP-RBFN, a novel formulation leveraging Radial Basis
Function Networks for efficiently learning Motion Primitives derived from
optimal control problems for autonomous driving. While traditional motion
planning approaches based on optimization are highly accurate, they are often
computationally prohibitive. In contrast, sampling-based methods demonstrate
high performance but impose constraints on the geometric shape of trajectories.
MP-RBFN combines the strengths of both by coupling the high-fidelity trajectory
generation of sampling-based methods with an accurate description of vehicle
dynamics. Empirical results show compelling performance compared to previous
methods, achieving a precise description of motion primitives at low inference
times. MP-RBFN yields a seven times higher accuracy in generating optimized
motion primitives compared to existing semi-analytic approaches. We demonstrate
the practical applicability of MP-RBFN for motion planning by integrating the
method into a sampling-based trajectory planner. MP-RBFN is available as
open-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.

</details>


### [489] [Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems](https://arxiv.org/abs/2507.10055)
*Muhtadin,I Wayan Agus Darmawan,Muhammad Hilmi Rusydiansyah,I Ketut Eddy Purnama,Chastine Fatichah,Mauridhi Hery Purnomo*

Main category: cs.RO

TL;DR: A lightweight deep learning model was developed for hand gesture recognition in human-robot collaboration, achieving high accuracy while being optimized for edge devices.


<details>
  <summary>Details</summary>
Motivation: Simplify and enhance human-robot collaboration by enabling intuitive control without relying on extra devices.

Method: Developed an extremely compact neural network model with 1,103 parameters, optimized using TensorFlow Lite quantization and pruning, and tested on the Universal Robot UR5 utilizing ROS2.

Result: The system achieved 93.5% gesture recognition accuracy and reduced the model size to just 7 KB for real-world deployment.

Conclusion: The study demonstrates that efficient, compact deep learning models can enable natural and precise human-robot interaction, paving the way for real-world implementations.

Abstract: Direct and natural interaction is essential for intuitive human-robot
collaboration, eliminating the need for additional devices such as joysticks,
tablets, or wearable sensors. In this paper, we present a lightweight deep
learning-based hand gesture recognition system that enables humans to control
collaborative robots naturally and efficiently. This model recognizes eight
distinct hand gestures with only 1,103 parameters and a compact size of 22 KB,
achieving an accuracy of 93.5%. To further optimize the model for real-world
deployment on edge devices, we applied quantization and pruning using
TensorFlow Lite, reducing the final model size to just 7 KB. The system was
successfully implemented and tested on a Universal Robot UR5 collaborative
robot within a real-time robotic framework based on ROS2. The results
demonstrate that even extremely lightweight models can deliver accurate and
responsive hand gesture-based control for collaborative robots, opening new
possibilities for natural human-robot interaction in constrained environments.

</details>


### [490] [TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic](https://arxiv.org/abs/2507.10075)
*Jie Pan,Tianyi Wang,Yangyang Wang,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: The paper presents a trust-aware game-theoretic lane-changing framework for automated vehicles that accounts for human-driver trust levels to improve interaction, predictability, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Automated vehicles struggle to interact cooperatively with human-driven vehicles due to limitations in predicting human driver behavior, stemming from overlooked dynamic trust levels.

Method: The study introduces a trust-aware lane-changing framework by modeling a multi-vehicle coalition game, implementing real-time trust evaluations, and considering social dynamics in decision-making.

Result: Human-in-the-loop experiments show AVs dynamically adapt to varying human driver trust levels, improving efficiency, safety, and transparency in lane-changing interactions.

Conclusion: Incorporating trust levels into lane-changing strategies enhances AV-HV cooperation, ensuring safe, efficient, and predictable integration in mixed traffic settings.

Abstract: Automated vehicles (AVs) face a critical need to adopt socially compatible
behaviors and cooperate effectively with human-driven vehicles (HVs) in
heterogeneous traffic environment. However, most existing lane-changing
frameworks overlook HVs' dynamic trust levels, limiting their ability to
accurately predict human driver behaviors. To address this gap, this study
proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.
First, we formulate a multi-vehicle coalition game, incorporating fully
cooperative interactions among AVs and partially cooperative behaviors from HVs
informed by real-time trust evaluations. Second, we develop an online trust
evaluation method to dynamically estimate HVs' trust levels during
lane-changing interactions, guiding AVs to select context-appropriate
cooperative maneuvers. Lastly, social compatibility objectives are considered
by minimizing disruption to surrounding vehicles and enhancing the
predictability of AV behaviors, thereby ensuring human-friendly and
context-adaptive lane-changing strategies. A human-in-the-loop experiment
conducted in a highway on-ramp merging scenario validates our TGLD approach.
Results show that AVs can effectively adjust strategies according to different
HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism
significantly improves lane-changing efficiency, maintains safety, and
contributes to transparent and adaptive AV-HV interactions.

</details>


### [491] [Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications](https://arxiv.org/abs/2507.10082)
*Amit Levy,Itzik Klein*

Main category: cs.RO

TL;DR: This paper proposes an innovative method for propagating sigma points in Unscented Kalman Filters, enhancing navigation performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges with accurately predicting mean and covariance propagation in nonlinear navigation applications within the Unscented Kalman Filter.

Method: A novel approach to propagate sigma points according to the nonlinear dynamic model of the navigation error state vector.

Result: Improved filter accuracy and better navigation performance demonstrated using real sensor data from an autonomous underwater vehicle.

Conclusion: The proposed method enhances the effectiveness and reliability of Unscented Kalman Filters for complex navigation scenarios.

Abstract: The unscented Kalman filter is a nonlinear estimation algorithm commonly used
in navigation applications. The prediction of the mean and covariance matrix is
crucial to the stable behavior of the filter. This prediction is done by
propagating the sigma points according to the dynamic model at hand. In this
paper, we introduce an innovative method to propagate the sigma points
according to the nonlinear dynamic model of the navigation error state vector.
This improves the filter accuracy and navigation performance. We demonstrate
the benefits of our proposed approach using real sensor data recorded by an
autonomous underwater vehicle during several scenarios.

</details>


### [492] [Foundation Model Driven Robotics: A Comprehensive Review](https://arxiv.org/abs/2507.10087)
*Muhammad Tayyab Khan,Ammar Waheed*

Main category: cs.RO

TL;DR: A critical review of foundation models in robotics, focusing on applications, trends, limitations, and future directions.


<details>
  <summary>Details</summary>
Motivation: Foundation models like LLMs and VLMs are transforming robotics, necessitating a structured review of their applications and challenges.

Method: The paper categorizes advancements, identifies enabling trends, and discusses bottlenecks while evaluating real-world feasibility.

Result: Core strengths such as semantic understanding and multimodal reasoning are recognized, but critical limitations like safety risks and computational constraints are highlighted.

Conclusion: The paper proposes a roadmap for bridging semantic reasoning and physical intelligence, emphasizing robust and interpretable models for advancement.

Abstract: The rapid emergence of foundation models, particularly Large Language Models
(LLMs) and Vision-Language Models (VLMs), has introduced a transformative
paradigm in robotics. These models offer powerful capabilities in semantic
understanding, high-level reasoning, and cross-modal generalization, enabling
significant advances in perception, planning, control, and human-robot
interaction. This critical review provides a structured synthesis of recent
developments, categorizing applications across simulation-driven design,
open-world execution, sim-to-real transfer, and adaptable robotics. Unlike
existing surveys that emphasize isolated capabilities, this work highlights
integrated, system-level strategies and evaluates their practical feasibility
in real-world environments. Key enabling trends such as procedural scene
generation, policy generalization, and multimodal reasoning are discussed
alongside core bottlenecks, including limited embodiment, lack of multimodal
data, safety risks, and computational constraints. Through this lens, this
paper identifies both the architectural strengths and critical limitations of
foundation model-based robotics, highlighting open challenges in real-time
operation, grounding, resilience, and trust. The review concludes with a
roadmap for future research aimed at bridging semantic reasoning and physical
intelligence through more robust, interpretable, and embodied models.

</details>


### [493] [Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots](https://arxiv.org/abs/2507.10105)
*Ines Sorrentino,Giulio Romualdi,Lorenzo Moretti,Silvio Traversaro,Daniele Pucci*

Main category: cs.RO

TL;DR: The paper introduces a framework for sensorless torque control in humanoid robots using Physics-Informed Neural Networks (PINNs) and Unscented Kalman Filtering (UKF), achieving better torque tracking and robustness.


<details>
  <summary>Details</summary>
Motivation: Current methods rely heavily on joint torque sensors, which can be limiting in terms of cost and complexity, prompting the need for an efficient sensorless approach.

Method: The framework uses PINNs to model friction and integrates UKF for torque estimation within a real-time control system.

Result: Experimental validation on the ergoCub robot shows improved torque tracking, energy efficiency, and disturbance rejection over existing methods.

Conclusion: The proposed method is proven scalable, adaptable, and practical for sensorless torque control, with consistent performance across varying robot hardware configurations.

Abstract: This paper presents a novel framework for whole-body torque control of
humanoid robots without joint torque sensors, designed for systems with
electric motors and high-ratio harmonic drives. The approach integrates
Physics-Informed Neural Networks (PINNs) for friction modeling and Unscented
Kalman Filtering (UKF) for joint torque estimation, within a real-time torque
control architecture. PINNs estimate nonlinear static and dynamic friction from
joint and motor velocity readings, capturing effects like motor actuation
without joint movement. The UKF utilizes PINN-based friction estimates as
direct measurement inputs, improving torque estimation robustness. Experimental
validation on the ergoCub humanoid robot demonstrates improved torque tracking
accuracy, enhanced energy efficiency, and superior disturbance rejection
compared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using
a dynamic balancing experiment. The framework's scalability is shown by
consistent performance across robots with similar hardware but different
friction characteristics, without re-identification. Furthermore, a comparative
analysis with position control highlights the advantages of the proposed torque
control approach. The results establish the method as a scalable and practical
solution for sensorless torque control in humanoid robots, ensuring torque
tracking, adaptability, and stability in dynamic environments.

</details>


### [494] [Simulations and experiments with assemblies of fiber-reinforced soft actuators](https://arxiv.org/abs/2507.10121)
*Seung Hyun Kim,Jiamiao Guo,Arman Tekinalp,Heng-Sheng Chang,Ugur Akcal,Tixian Wang,Darren Biskup,Benjamin Walt,Girish Chowdhary,Girish Krishnan,Prashant G. Mehta,Mattia Gazzola*

Main category: cs.RO

TL;DR: This paper introduces a simulation framework for soft continuum arms (SCAs) combined with experimental video-tracking systems to address their non-linear behavior challenges.


<details>
  <summary>Details</summary>
Motivation: Soft continuum arms are highly versatile, but their non-linear behavior makes real-world application and control challenging.

Method: The framework integrates a simulation model for SCAs, using fiber reinforced elastomeric enclosures (FREEs), with an experimental video-tracking system to analyze and control their behavior.

Result: The simulation framework and video-tracking integration allow better testing and control design for SCAs.

Conclusion: This research advances the practical usability of SCAs by combining modular simulation and experimental methodologies to tackle their non-linear dynamics.

Abstract: Soft continuum arms (SCAs) promise versatile manipulation through mechanical
compliance, for assistive devices, agriculture, search applications, or
surgery. However, SCAs' real-world use is challenging, partly due to their
hard-to-control non-linear behavior. Here, a simulation framework for SCAs
modularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is
developed and integrated with a video-tracking system for experimental testing
and control design.

</details>


### [495] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: The paper introduces GUIDER, a probabilistic framework for robots to infer human intent in navigation and manipulation tasks, achieving significant performance improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: To enable seamless human-robot collaboration by accurately inferring human intent without constraining human actions or causing conflicts.

Method: GUIDER tracks navigation and manipulation goals through a dual-phase probabilistic framework, integrating tools like Synergy Map, U2Net saliency, FastSAM, and geometric kinematics-awareness for real-time object probability evolution.

Result: In evaluations, GUIDER demonstrated superior stability in navigation (93-100%) and manipulation (94-100%), outperforming baselines by 39.5% and 31.4% in specific scenarios while recognizing object intent three times faster.

Conclusion: The dual-phase framework of GUIDER significantly enhances intent inference in mobile manipulation, validating its approach and presenting potential for effective human-robot collaboration.

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [496] [Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains](https://arxiv.org/abs/2507.10164)
*Egor Maslennikov,Eduard Zaliaev,Nikita Dudorov,Oleg Shamanin,Karanov Dmitry,Gleb Afanasev,Alexey Burkov,Egor Lygin,Simeon Nedelchev,Evgeny Ponomarev*

Main category: cs.RO

TL;DR: The paper introduces an RL framework for bipedal robots with closed-chain dynamics, achieving better sim-to-real transfer and stability in locomotion.


<details>
  <summary>Details</summary>
Motivation: Bipedal robots with closed kinematic chains pose unique training challenges due to the simplification of dynamics in RL approaches, hindering sim-to-real transfer.

Method: An RL framework incorporating closed-chain dynamics with symmetry-aware loss functions, adversarial training, and network regularization was used.

Result: The proposed method achieved superior locomotion stability across diverse terrains compared to simplified kinematic models.

Conclusion: Incorporating closed-chain dynamics in RL frameworks improves sim-to-real transfer and robustness in bipedal robot locomotion.

Abstract: Developing robust locomotion controllers for bipedal robots with closed
kinematic chains presents unique challenges, particularly since most
reinforcement learning (RL) approaches simplify these parallel mechanisms into
serial models during training. We demonstrate that this simplification
significantly impairs sim-to-real transfer by failing to capture essential
aspects such as joint coupling, friction dynamics, and motor-space control
characteristics. In this work, we present an RL framework that explicitly
incorporates closed-chain dynamics and validate it on our custom-built robot
TopA. Our approach enhances policy robustness through symmetry-aware loss
functions, adversarial training, and targeted network regularization.
Experimental results demonstrate that our integrated approach achieves stable
locomotion across diverse terrains, significantly outperforming methods based
on simplified kinematic models.

</details>


### [497] [REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles](https://arxiv.org/abs/2507.10204)
*Abdelhakim Amer,Mohit Mehindratta,Yury Brodskiy,Bilal Wehbe,Erdal Kayacan*

Main category: cs.RO

TL;DR: The REACT framework introduces real-time path planning to prevent tether entanglement for underwater vehicles, ensuring safer and faster inspection missions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of tether entanglement in underwater vehicle inspections, which can hinder the safety and efficiency of operations.

Method: The REACT framework employs a geometry-based tether model, leveraging the signed distance field (SDF) map to ensure real-time simulation and tether length constraints. It integrates this model with coverage path planning for proactive entanglement avoidance.

Result: REACT achieves faster mission completion (20% faster in simulations) while maintaining tether constraints. In real-world tests, it successfully avoids entanglement, unlike conventional planners.

Conclusion: REACT effectively prevents tether entanglement during underwater inspections, ensuring efficient, safe, and complete coverage missions.

Abstract: Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.

</details>


### [498] [Prompt Informed Reinforcement Learning for Visual Coverage Path Planning](https://arxiv.org/abs/2507.10284)
*Venkat Margapuri*

Main category: cs.RO

TL;DR: The paper introduces Prompt-Informed Reinforcement Learning (PIRL), combining LLMs like GPT-3.5 with RL for optimizing visual coverage path planning by UAVs.


<details>
  <summary>Details</summary>
Motivation: Traditional RL struggles with adaptability due to fixed, environment-specific reward systems. The study aims to improve RL by leveraging semantic adaptability from LLMs within UAV visual coverage applications.

Method: Proposed a PIRL framework where feedback from GPT-3.5 shapes rewards dynamically for the PPO agent in UAV motion and camera control, tested across various settings.

Result: PIRL outperformed baselines, achieving up to 27% higher coverage, 25% better battery efficiency, and 18% lower redundancy in simulated environments.

Conclusion: LLM-integrated reward shaping, as demonstrated by PIRL, is effective for spatial exploration in robotics and opens new directions for incorporating natural language into RL.

Abstract: Visual coverage path planning with unmanned aerial vehicles (UAVs) requires
agents to strategically coordinate UAV motion and camera control to maximize
coverage, minimize redundancy, and maintain battery efficiency. Traditional
reinforcement learning (RL) methods rely on environment-specific reward
formulations that lack semantic adaptability. This study proposes
Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates
the zero-shot reasoning ability and in-context learning capability of large
language models with curiosity-driven RL. PIRL leverages semantic feedback from
an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal
Policy Optimization (PPO) RL policy guiding the agent in position and camera
adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI
Gym and evaluated in various environments. Furthermore, the sim-to-real-like
ability and zero-shot generalization of the agent are tested by operating the
agent in Webots simulator which introduces realistic physical dynamics. Results
show that PIRL outperforms multiple learning-based baselines such as PPO with
static rewards, PPO with exploratory weight initialization, imitation learning,
and an LLM-only controller. Across different environments, PIRL outperforms the
best-performing baseline by achieving up to 14% higher visual coverage in
OpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and
up to 18\% lower redundancy, depending on the environment. The results
highlight the effectiveness of LLM-guided reward shaping in complex spatial
exploration tasks and suggest a promising direction for integrating natural
language priors into RL for robotics.

</details>


### [499] [TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity](https://arxiv.org/abs/2507.10290)
*Jiajun Yu,Nanhe Chen,Guodong Liu,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: The paper proposes a novel trajectory optimization framework using CADMM-based algorithm tailored for parallel computing, achieving faster and smoother results compared to existing methods, especially for large-scale trajectories.


<details>
  <summary>Details</summary>
Motivation: Traditional trajectory optimization methods struggle with efficiency and scalability, especially for large-scale trajectories. The paper seeks to address these limitations by leveraging parallel computing.

Method: The authors use a CADMM-based algorithm that decomposes trajectories into multiple segments, solving subproblems concurrently. This includes a closed-form solution for linear and quadratic constraints and numerical solutions for inequality constraints.

Result: Experiments verify that the proposed method achieves over a tenfold speed-up and improved smoothness for large-scale trajectories compared to SOTA methods. GPU deployment further showcases high performance with thousands of segments.

Conclusion: CADMM-based parallel trajectory optimization enables scalable and efficient solutions for large-scale trajectories, demonstrating significant advancements in speed and smoothness while leveraging modern computing architectures.

Abstract: Optimization has been widely used to generate smooth trajectories for motion
planning. However, existing trajectory optimization methods show weakness when
dealing with large-scale long trajectories. Recent advances in parallel
computing have accelerated optimization in some fields, but how to efficiently
solve trajectory optimization via parallelism remains an open question. In this
paper, we propose a novel trajectory optimization framework based on the
Consensus Alternating Direction Method of Multipliers (CADMM) algorithm, which
decomposes the trajectory into multiple segments and solves the subproblems in
parallel. The proposed framework reduces the time complexity to O(1) per
iteration to the number of segments, compared to O(N) of the state-of-the-art
(SOTA) approaches. Furthermore, we introduce a closed-form solution that
integrates convex linear and quadratic constraints to speed up the
optimization, and we also present numerical solutions for general inequality
constraints. A series of simulations and experiments demonstrate that our
approach outperforms the SOTA approach in terms of efficiency and smoothness.
Especially for a large-scale trajectory, with one hundred segments, achieving
over a tenfold speedup. To fully explore the potential of our algorithm on
modern parallel computing architectures, we deploy our framework on a GPU and
show high performance with thousands of segments.

</details>


### [500] [Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic](https://arxiv.org/abs/2507.10310)
*Michael Schröder,Eric Schöneberg,Daniel Görges,Hans D. Schotten*

Main category: cs.RO

TL;DR: The paper resolves the incompatibility between spatially discrete cost-maps and model predictive control (MPC) by re-formulating occupancy grids into continuously differentiable functions using fuzzy logic, enabling their use in trajectory planning and beyond.


<details>
  <summary>Details</summary>
Motivation: The need to enable compatibility between spatially discrete occupancy grid maps and model predictive control (MPC), as obstacles defined on grids pose challenges for continuous differentiability required by MPC solvers.

Method: The method involves redefining obstacles as polygons represented by intersections of half-spaces and using fuzzy logic to turn logical operators into inequality constraints, making them compatible with MPC formulations.

Result: The approach is tested successfully in simulation for MPC-based trajectory planning, demonstrating its feasibility and effectiveness.

Conclusion: The work enables obstacle avoidance in MPC by ensuring compatibility via fuzzy logic and extends its applicability to other domains that require incorporation of logical constraints into MPC frameworks.

Abstract: In practice, navigation of mobile robots in confined environments is often
done using a spatially discrete cost-map to represent obstacles. Path following
is a typical use case for model predictive control (MPC), but formulating
constraints for obstacle avoidance is challenging in this case. Typically the
cost and constraints of an MPC problem are defined as closed-form functions and
typical solvers work best with continuously differentiable functions. This is
contrary to spatially discrete occupancy grid maps, in which a grid's value
defines the cost associated with occupancy. This paper presents a way to
overcome this compatibility issue by re-formulating occupancy grid maps to
continuously differentiable functions to be embedded into the MPC scheme as
constraints. Each obstacle is defined as a polygon -- an intersection of
half-spaces. Any half-space is a linear inequality representing one edge of a
polygon. Using AND and OR operators, the combined set of all obstacles and
therefore the obstacle avoidance constraints can be described. The key
contribution of this paper is the use of fuzzy logic to re-formulate such
constraints that include logical operators as inequality constraints which are
compatible with standard MPC formulation. The resulting MPC-based trajectory
planner is successfully tested in simulation. This concept is also applicable
outside of navigation tasks to implement logical or verbal constraints in MPC.

</details>


### [501] [Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions](https://arxiv.org/abs/2507.10376)
*Mohammadhossein Talebi,Pragyan Dahal,Davide Possenti,Stefano Arrigoni,Francesco Braghin*

Main category: cs.RO

TL;DR: A deep learning-based motion estimator combining visual, inertial, and radar data improves odometry accuracy in harsh weather conditions like snow and rain.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving sensors struggle under harsh weather and environmental challenges, especially visual sensors.

Method: The study employs advanced deep learning sensor fusion, dynamically balancing sensor contributions (e.g., radar compensating for visual limitations).

Result: The proposed model demonstrates robust performance in diverse conditions using the Boreas dataset.

Conclusion: Radar can significantly enhance pose estimation, especially when visual sensors fail, making it a vital tool in autonomous systems.

Abstract: Autonomous driving systems are highly dependent on sensors like cameras,
LiDAR, and inertial measurement units (IMU) to perceive the environment and
estimate their motion. Among these sensors, perception-based sensors are not
protected from harsh weather and technical failures. Although existing methods
show robustness against common technical issues like rotational misalignment
and disconnection, they often degrade when faced with dynamic environmental
factors like weather conditions. To address these problems, this research
introduces a novel deep learning-based motion estimator that integrates visual,
inertial, and millimeter-wave radar data, utilizing each sensor strengths to
improve odometry estimation accuracy and reliability under adverse
environmental conditions such as snow, rain, and varying light. The proposed
model uses advanced sensor fusion techniques that dynamically adjust the
contributions of each sensor based on the current environmental condition, with
radar compensating for visual sensor limitations in poor visibility. This work
explores recent advancements in radar-based odometry and highlights that radar
robustness in different weather conditions makes it a valuable component for
pose estimation systems, specifically when visual sensors are degraded.
Experimental results, conducted on the Boreas dataset, showcase the robustness
and effectiveness of the model in both clear and degraded environments.

</details>


### [502] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: The paper introduces Scene-Aware Conversational ADAS (SC-ADAS), a framework combining AI and sensor-based insights to provide adaptive, dialogue-based driver assistance.


<details>
  <summary>Details</summary>
Motivation: Current ADAS systems cannot engage in natural language interaction or adapt dynamically, limiting their functionality.

Method: The SC-ADAS framework integrates Generative AI tools like large language models and vision-to-text systems to provide real-time scene-aware assistance.

Result: The system was implemented in the CARLA simulator, with evaluations showcasing trade-offs like latency during scene retrieval and the impact of dialogue history.

Conclusion: SC-ADAS proves the feasibility of advancing driver assistance systems through conversational reasoning and modular control, signaling a step towards intelligent ADAS solutions.

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


### [503] [MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation](https://arxiv.org/abs/2507.10543)
*Juyi Sheng,Ziyi Wang,Peiming Li,Mengyuan Liu*

Main category: cs.RO

TL;DR: MP1 introduces MeanFlow-based generative robot learning for action trajectory creation, tackling limitations in speed and accuracy compared to existing methods like DP3 and FlowPolicy.


<details>
  <summary>Details</summary>
Motivation: Generative methods for robot manipulation face challenges in balancing inference speed and model constraints, needing improvements for both performance and generalization.

Method: MP1 pairs 3D point-cloud inputs with the MeanFlow paradigm to create fast and precise action trajectories using interval-averaged velocity, CFG for controllability, and a Dispersive Loss for better generalization.

Result: MP1 outperformed state-of-the-art methods with a 10.2% success rate improvement over DP3 and a 7.3% improvement over FlowPolicy, while also demonstrating significant speed gains (19x faster than DP3 and 2x faster than FlowPolicy).

Conclusion: MP1 addresses critical limitations in generative robot manipulation models, achieving better accuracy, generalization, and speed, paving the way for more efficient robotic learning solutions.

Abstract: In robot manipulation, robot learning has become a prevailing approach.
However, generative models within this field face a fundamental trade-off
between the slow, iterative sampling of diffusion models and the architectural
constraints of faster Flow-based methods, which often rely on explicit
consistency losses. To address these limitations, we introduce MP1, which pairs
3D point-cloud inputs with the MeanFlow paradigm to generate action
trajectories in one network function evaluation (1-NFE). By directly learning
the interval-averaged velocity via the MeanFlow Identity, our policy avoids any
additional consistency constraints. This formulation eliminates numerical
ODE-solver errors during inference, yielding more precise trajectories. MP1
further incorporates CFG for improved trajectory controllability while
retaining 1-NFE inference without reintroducing structural constraints. Because
subtle scene-context variations are critical for robot learning, especially in
few-shot learning, we introduce a lightweight Dispersive Loss that repels state
embeddings during training, boosting generalization without slowing inference.
We validate our method on the Adroit and Meta-World benchmarks, as well as in
real-world scenarios. Experimental results show MP1 achieves superior average
task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its
average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster
than FlowPolicy. Our code is available at https://mp1-2254.github.io/.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [504] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Main category: cs.SE

TL;DR: Large language models (LLMs) are explored as tools to assist in performance-oriented software configuration tasks like identifying relevant options and recommending configurations. While LLMs show promise in aligning with expert knowledge, they also exhibit limitations such as hallucinations or superficial reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of navigating complex configuration spaces in software systems that influence performance metrics, where current methods require domain expertise or are computationally expensive with machine learning techniques.

Method: The study investigates the capabilities of LLMs through prompt-based tasks such as identifying relevant options, ranking configurations, and recommending performant configurations on systems like compilers, video encoders, and SAT solvers.

Result: Preliminary results indicate that LLMs can align well with expert knowledge for some tasks and systems. However, their performance is inconsistent and they may display hallucinations or shallow reasoning in certain scenarios.

Conclusion: LLMs hold potential for assisting in software configuration tasks, but further systematic evaluation and refinement are required to overcome their limitations and enhance their reliability.

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [505] [Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches](https://arxiv.org/abs/2507.08943)
*Pedro Lopes,Paola Accioly,Paulo Borba,Vitor Menezes*

Main category: cs.SE

TL;DR: The paper explores Git workflows, focusing on branch-based and trunk-based workflows, and studies their suitability for different team types.


<details>
  <summary>Details</summary>
Motivation: The authors aim to fill the research gap on the effectiveness of Git workflows for different team contexts.

Method: Conducted semi-structured interviews and surveys with software developers, particularly in Brazil.

Result: Trunk-based workflows suit fast-paced projects with smaller, experienced teams, while branch-based workflows are better for larger, less experienced teams but face management challenges.

Conclusion: Different Git workflows have specific advantages depending on the project pace, team size, and experience.

Abstract: Git has become one of the most widely used version control systems today.
Among its distinguishing features, its ability to easily and quickly create
branches stands out, allowing teams to customize their workflows. In this
context, various formats of collaborative development workflows using Git have
emerged and gained popularity among software engineers. We can categorize such
workflows into two main types: branch-based workflows and trunk-based
workflows. Branch-based workflows typically define a set of remote branches
with well-defined objectives, such as feature branches, a branch for feature
integration, and a main branch. The goal is to migrate changes from the most
isolated branch to the main one shared by all as the code matures. In this
category, GitFlow stands out as the most popular example. In contrast,
trunk-based workflows have a single remote branch where developers integrate
their changes directly. In this range of options, choosing a workflow that
maximizes team productivity while promoting software quality becomes a
non-trivial task. Despite discussions on forums, social networks, and blogs,
few scientific articles have explored this topic. In this work, we provide
evidence on how Brazilian developers work with Git workflows and what factors
favor or hinder the use of each model. To this end, we conducted
semi-structured interviews and a survey with software developers. Our results
indicate that trunk-based development favors fast-paced projects with
experienced and smaller teams, while branch-based development suits less
experienced and larger teams better, despite posing management challenges.

</details>


### [506] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: This paper introduces automated methods for segmenting R code, which aid in navigating and understanding codebases, by using Large and Small Language Models (LLMs/SLMs). Results favor smaller, fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of segmenting R code in growing repositories, especially for low-resource languages, to aid efficient codebase navigation and knowledge retrieval.

Method: The study explores two segmentation approaches: context-based line-by-line analysis and range-based segmentation, using both pre-trained and fine-tuned language models on a dataset called StatCodeSeg.

Result: Context-based line-by-line analysis is more effective, and smaller fine-tuned models like CodeBERT outperform LLMs, even without prior exposure to R code.

Conclusion: Fine-tuned, smaller models offer a domain-specific, efficient solution for code segmentation, proving advantageous despite their limited exposure during pre-training.

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [507] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: Tippy is an AI framework aiming to revolutionize drug discovery by automating laboratory workflows using specialized AI agents to enhance efficiency and scientific rigor within the DMTA cycle.


<details>
  <summary>Details</summary>
Motivation: The pharmaceutical industry faces difficulties meeting modern therapeutic development demands due to limitations in traditional drug discovery approaches.

Method: Tippy employs a multi-agent AI system with five specialized agents operating collaboratively within the DMTA cycle, supervised with Safety Guardrail oversight.

Result: Significant improvements were observed in workflow efficiency, decision-making speed, and cross-disciplinary coordination during drug discovery processes.

Conclusion: Tippy establishes a new paradigm in AI-assisted drug discovery, emphasizing automation, efficiency, and scientific precision in laboratory workflows.

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [508] [Towards Extracting Software Requirements from App Reviews using Seq2seq Framework](https://arxiv.org/abs/2507.09039)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: This paper introduces a new method to extract requirements from mobile app reviews by using a Seq2seq Named Entity Recognition (NER) framework, outperforming existing methods on large datasets.


<details>
  <summary>Details</summary>
Motivation: Extracting requirements from app reviews is vital for software evolution, but current methods struggle due to informal language, errors, and irrelevant information in the reviews.

Method: The paper proposes a Seq2seq framework with a BiLSTM encoder, LSTM decoder, self-attention mechanism, GloVe embeddings, and a CRF model to perform Named Entity Recognition.

Result: The approach achieved a high F1 score of 0.96 on a large dataset (Dataset 2) and a moderate score of 0.47 on a smaller manually annotated dataset (Dataset 1), exceeding the performance of state-of-the-art methods.

Conclusion: The proposed framework effectively extracts user requirements from app reviews, addressing challenges of informal language and errors, and proves to be a valuable tool for software improvement.

Abstract: Mobile app reviews are a large-scale data source for software improvements. A
key task in this context is effectively extracting requirements from app
reviews to analyze the users' needs and support the software's evolution.
Recent studies show that existing methods fail at this task since app reviews
usually contain informal language, grammatical and spelling errors, and a large
amount of irrelevant information that might not have direct practical value for
developers. To address this, we propose a novel reformulation of requirements
extraction as a Named Entity Recognition (NER) task based on the
sequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a
Seq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced
with a self-attention mechanism, GloVe embeddings, and a CRF model. We
evaluated our framework on two datasets: a manually annotated set of 1,000
reviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The
quantitative evaluation of our framework showed that it outperformed existing
state-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved
comparable performance on Dataset 1 with an F1 score of 0.47.

</details>


### [509] [CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews](https://arxiv.org/abs/2507.09049)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: The paper presents CMER, a tool that uses Natural Language Inference (NLI) and a Large Language Model (LLM) to efficiently extract ethical concern-related app reviews, such as privacy and security, from large datasets without labeled data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the increasing prevalence of ethics-related concerns in app reviews, which are often expressed through domain-specific language and overshadowed by generic feedback categories. They seek an efficient method to extract these critical insights at scale.

Method: The study combines NLI for domain-specific context awareness and a decoder-only LLM for classification without requiring labeled data. The approach was tested on over 382K app reviews, with evaluations comparing different NLI and LLM models.

Result: The proposed CMER system identified 2,178 additional privacy and security-related reviews that were overlooked by a previous keyword-based approach, showcasing its enhanced extraction capabilities.

Conclusion: CMER successfully demonstrated its ability to mine ethical issues like privacy and security from app reviews at scale, offering actionable insights that can aid in software engineering processes.

Abstract: With the increasing proliferation of mobile applications in our daily lives,
the concerns surrounding ethics have surged significantly. Users communicate
their feedback in app reviews, frequently emphasizing ethical concerns, such as
privacy and security. Incorporating these reviews has proved to be useful for
many areas of software engineering (e.g., requirement engineering, testing,
etc.). However, app reviews related to ethical concerns generally use
domain-specific language and are typically overshadowed by more generic
categories of user feedback, such as app reliability and usability. Thus,
making automated extraction a challenging and time-consuming effort.
  This study proposes CMER (A \underline{C}ontext-Aware Approach for
\underline{M}ining \underline{E}thical Concern-related App
\underline{R}eviews), a novel approach that combines Natural Language Inference
(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract
ethical concern-related app reviews at scale. In CMER, NLI provides
domain-specific context awareness by using domain-specific hypotheses, and the
Llama-like LLM eliminates the need for labeled data in the classification task.
We evaluated the validity of CMER by mining privacy and security-related
reviews (PSRs) from the dataset of more than 382K app reviews of mobile
investment apps. First, we evaluated four NLI models and compared the results
of domain-specific hypotheses with generic hypotheses. Next, we evaluated three
LLMs for the classification task. Finally, we combined the best NLI and LLM
models (CMER) and extracted 2,178 additional PSRs overlooked by the previous
study using a keyword-based approach, thus demonstrating the effectiveness of
CMER. These reviews can be further refined into actionable requirement
artifacts.

</details>


### [510] [SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps](https://arxiv.org/abs/2507.09051)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: This study introduces SAGE, a methodology leveraging GPT and NLI to automatically identify privacy-related reviews in mental health apps, achieving high performance without fine-tuning and extracting actionable insights.


<details>
  <summary>Details</summary>
Motivation: Sensitive data collection practices in MH apps lead to privacy concerns that are often overlooked amidst other user feedback like usability and reliability.

Method: The paper presents SAGE, which employs Natural Language Inference (NLI) with domain-specific privacy hypotheses and GPT models to identify privacy-relevant reviews, eliminating the need for fine-tuning.

Result: SAGE achieves an F1 score of 0.85 on a dataset of 204K app reviews, outperforming baseline classifiers, and identifies 748 additional privacy reviews missed by keyword-based methods.

Conclusion: SAGE is highly effective for mining privacy-relevant user reviews, offering valuable insights for addressing privacy concerns without requiring extensive model modifications.

Abstract: Mental health (MH) apps often require sensitive user data to customize
services for mental wellness needs. However, such data collection practices in
some MH apps raise significant privacy concerns for users. These concerns are
often mentioned in app reviews, but other feedback categories, such as
reliability and usability, tend to take precedence. This poses a significant
challenge in automatically identifying privacy requirements-relevant reviews
(privacy reviews) that can be utilized to extract privacy requirements and
address users' privacy concerns. Thus, this study introduces SAGE, a
context-aware approach to automatically mining privacy reviews from MH apps
using Natural Language Inference (NLI) with MH domain-specific privacy
hypotheses (provides domain-specific context awareness) and a GPT model
(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a
dataset of 204K app reviews achieved an F1 score of 0.85 without any
fine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.
Furthermore, SAGE extracted 748 privacy reviews previously overlooked by
keyword-based methods, demonstrating its effectiveness through qualitative
evaluation. These reviews can later be refined into actionable privacy
requirement artifacts.

</details>


### [511] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: SetupBench evaluates LLM agents on real-world environment setup tasks in a bare Linux sandbox, exposing limitations in current agent capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the gap in benchmarks for evaluating LLM agents’ ability to bootstrap environments in real-world software tasks.

Method: SetupBench introduces a benchmark composed of 93 tasks across diverse ecosystems, assessing agents’ ability to install dependencies, initialize databases, and configure services.

Result: OpenHands showcases low success rates, revealing inefficiencies in task execution and systematic flaws like unnecessary actions and hallucinated constraints.

Conclusion: SetupBench highlights critical deficiencies in current LLM agents for practical environment setup, emphasizing areas for improvement in future agents.

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [512] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper introduces SPICE, a scalable automated pipeline to label datasets for foundation models in software engineering, significantly reducing costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Creating high-quality labeled datasets for software engineering foundation models is expensive and time-consuming.

Method: SPICE uses automated context-aware code navigation, rationale-driven prompting, and multi-pass consensus to approximate expert annotations efficiently.

Result: SPICE achieved strong agreement with human-labeled data while reducing costs to $5.10 for 1,000 instances, down from $100,000 using manual annotations.

Conclusion: SPICE demonstrates potential for cost-effective dataset creation at scale and provides a new large dataset and tool for the SE community.

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [513] [Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps](https://arxiv.org/abs/2507.09135)
*Yalong Du,Chaozheng Wang,Huaijin Wang*

Main category: cs.SE

TL;DR: Large Language Models excel in code generation, but integrating Programming Language techniques is essential to overcome drawbacks like syntax errors and reliability issues.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges like syntax errors and reliability concerns due to their statistical nature.

Method: Propose integrating PL techniques such as structured representations, formal correctness, and verification mechanisms.

Result: Enhancement in the reliability and trustworthiness of code generated by LLMs.

Conclusion: Integrating PL techniques will make LLM-generated code functionally correct, interpretable, and trustworthy.

Abstract: Large Language Models have demonstrated remarkable capabilities in automated
code generation, yet their statistical nature and black-box characteristics
create significant semantic gaps manifested through syntax errors, semantic
hallucinations, and reliability concerns. This position paper argues that
principled integration of Programming Language (PL) techniques is essential for
bridging these gaps. Through structured program representations, formal
correctness guarantees, and robust verification mechanisms, PL techniques can
elevate LLM-generated code from statistical pattern matching to truly reliable
and trustworthy levels. This integration is crucial for developing systems that
generate code that is not only functionally correct but also interpretable,
verifiable, and ultimately trustworthy.

</details>


### [514] [OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research](https://arxiv.org/abs/2507.09186)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.SE

TL;DR: OpenCAMS is an open-source co-simulation framework integrating SUMO, CARLA, and OMNeT++ for transportation research, focusing on safety, mobility, and cybersecurity.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for a comprehensive simulation platform that integrates traffic modeling, 3D perception, and network communication to advance research in intelligent transportation systems.

Method: The authors constructed a time-synchronized, bidirectional coupling architecture between SUMO, CARLA, and OMNeT++ to ensure modularity and coherent simulation progression across different domains.

Result: OpenCAMS successfully combines large-scale traffic modeling, high-fidelity 3D vehicle simulation, and event-driven network communication, demonstrating seamless co-simulation across these domains.

Conclusion: The platform is open-source, expandable, and future-proof, providing a flexible tool for collaborative transportation research and innovation.

Abstract: We introduce OpenCAMS (Open-Source Connected and Automated Mobility
Co-Simulation Platform), an open-source, synchronized, and extensible
co-simulation framework that tightly couples three best-in-class simulation
tools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support
advanced research in transportation safety, mobility, and cybersecurity by
combining the strengths of each simulation domain. Specifically, SUMO provides
large-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D
perception, vehicle dynamics, and control simulation; and OMNeT++ enables
modular, event-driven network communication, such as cellular
vehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,
bidirectional coupling architecture that ensures coherent simulation
progression across traffic, perception, and communication domains while
preserving modularity and reproducibility. For example, CARLA can simulate and
render a subset of vehicles that require detailed sensor emulation and control
logic; SUMO orchestrates network-wide traffic flow, vehicle routing, and
traffic signal management; and OMNeT++ dynamically maps communication nodes to
both mobile entities (e.g., vehicles) and static entities (e.g., roadside
units) to enable C-V2X communication. While these three simulators form the
foundational core of OpenCAMS, the platform is designed to be expandable and
future-proof, allowing additional simulators to be integrated on top of this
core without requiring fundamental changes to the system architecture. The
OpenCAMS platform is fully open-source and publicly available through its
GitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,
providing the research community with an accessible, flexible, and
collaborative environment for advancing next-generation intelligent
transportation systems.

</details>


### [515] [Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval](https://arxiv.org/abs/2507.09199)
*Huihui Huang,Ratnadira Widyasari,Ting Zhang,Ivana Clairine Irsan,Jieke Shi,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,Hong Jin Kang,David Lo*

Main category: cs.SE

TL;DR: The paper addresses the limitations of existing issue-commit linking methods by proposing a more realistic evaluation setting called Realistic Distribution Setting (RDS) and introducing a method, EasyLink, which significantly outperforms state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Existing issue-commit linking evaluations fail to account for challenges posed by repositories with numerous irrelevant commits, leading to unrealistic performance results.

Method: The paper introduces the Realistic Distribution Setting (RDS) to develop a more realistic dataset and proposes EasyLink, leveraging a vector database and large language model for issue-commit linking.

Result: The evaluation demonstrates that state-of-the-art methods perform poorly under RDS, while EasyLink achieves a Precision@1 of 75.91%, significantly higher than existing methods.

Conclusion: EasyLink provides a robust improvement in issue-commit linking, emphasizing the need for realistic evaluation methodologies. The paper offers practical research guidance to enhance this domain further.

Abstract: Issue-commit linking, which connects issues with commits that fix them, is
crucial for software maintenance. Existing approaches have shown promise in
automatically recovering these links. Evaluations of these techniques assess
their ability to identify genuine links from plausible but false links.
However, these evaluations overlook the fact that, in reality, when a
repository has more commits, the presence of more plausible yet unrelated
commits may interfere with the tool in differentiating the correct fix commits.
To address this, we propose the Realistic Distribution Setting (RDS) and use it
to construct a more realistic evaluation dataset that includes 20 open-source
projects. By evaluating tools on this dataset, we observe that the performance
of the state-of-the-art deep learning-based approach drops by more than half,
while the traditional Information Retrieval method, VSM, outperforms it.
  Inspired by these observations, we propose EasyLink, which utilizes a vector
database as a modern Information Retrieval technique. To address the
long-standing problem of the semantic gap between issues and commits, EasyLink
leverages a large language model to rerank the commits retrieved from the
database. Under our evaluation, EasyLink achieves an average Precision@1 of
75.91%, improving over the state-of-the-art by over four times. Additionally,
this paper provides practical guidelines for advancing research in issue-commit
link recovery.

</details>


### [516] [AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction](https://arxiv.org/abs/2507.10338)
*Enyuan Tian,Yiwei Ci,Qiusong Yang,Yufeng Li,Zhichao Lyu*

Main category: cs.SE

TL;DR: The paper introduces AssertCoder, a framework for automatically generating hardware verification assertions from multimodal specifications, enhancing functional correctness and error detection over existing methods.


<details>
  <summary>Details</summary>
Motivation: Writing SystemVerilog Assertions (SVAs) for hardware verification is labor-intensive, error-prone, and inefficient. A solution is needed to automate and improve this process.

Method: AssertCoder uses a custom pipeline with modality-sensitive preprocessing to handle diverse input formats. It employs semantic analyzers for structured data extraction, CoT prompting for assertion synthesis, and a mutation-based evaluation for refinement.

Result: AssertCoder showed improved results in hardware verification, achieving an 8.4% increase in functional correctness and a 5.8% gain in mutation detection compared to state-of-the-art approaches, based on tests across three real-world RTL designs.

Conclusion: AssertCoder provides a significant advancement in hardware design verification by improving the automation and quality of assertion generation, offering a more effective alternative to manual or other automated methods.

Abstract: Assertion-Based Verification (ABV) is critical for ensuring functional
correctness in modern hardware systems. However, manually writing high-quality
SVAs remains labor-intensive and error-prone. To bridge this gap, we propose
AssertCoder, a novel unified framework that automatically generates
high-quality SVAs directly from multimodal hardware design specifications.
AssertCoder employs a modality-sensitive preprocessing to parse heterogeneous
specification formats (text, tables, diagrams, and formulas), followed by a set
of dedicated semantic analyzers that extract structured representations aligned
with signal-level semantics. These representations are utilized to drive
assertion synthesis via multi-step chain-of-thought (CoT) prompting. The
framework incorporates a mutation-based evaluation approach to assess assertion
quality via model checking and further refine the generated assertions.
Experimental evaluation across three real-world Register-Transfer Level (RTL)
designs demonstrates AssertCoder's superior performance, achieving an average
increase of 8.4% in functional correctness and 5.8% in mutation detection
compared to existing state-of-the-art approaches.

</details>


### [517] [Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation](https://arxiv.org/abs/2507.09220)
*Syed Tauhid Ullah Shah,Mohammad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TL;DR: The paper examines challenges posed by non-explainable AI tools in requirements engineering, particularly in safety-critical industries, and proposes practical improvements to enhance transparency, usability, and compliance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limited adoption of AI tools in regulated industries due to challenges related to explainability, transparency, and traceability.

Method: Through semi-structured interviews with ten practitioners from safety-critical industries, the authors studied workflow integrations and challenges related to AI's lack of explainability.

Result: Findings show that non-explainable AI tools lead to reduced trust, manual validation, difficulties with domain-specific terminology, disrupted collaboration, and compliance risks, undermining efficiency.

Conclusion: Key improvements identified include source tracing, decision justification, domain-specific support, and compliance validation, offering a roadmap for improving adoption in regulated industries.

Abstract: Artificial Intelligence (AI) tools for automating design artifact generation
are increasingly used in Requirements Engineering (RE) to transform textual
requirements into structured diagrams and models. While these AI tools,
particularly those based on Natural Language Processing (NLP), promise to
improve efficiency, their adoption remains limited in regulated industries
where transparency and traceability are essential. In this paper, we
investigate the explainability gap in AI-driven design artifact generation
through semi-structured interviews with ten practitioners from safety-critical
industries. We examine how current AI-based tools are integrated into workflows
and the challenges arising from their lack of explainability. We also explore
mitigation strategies, their impact on project outcomes, and features needed to
improve usability. Our findings reveal that non-explainable AI outputs
necessitate extensive manual validation, reduce stakeholder trust, struggle to
handle domain-specific terminology, disrupt team collaboration, and introduce
regulatory compliance risks, often negating the anticipated efficiency
benefits. To address these issues, we identify key improvements, including
source tracing, providing clear justifications for tool-generated decisions,
supporting domain-specific adaptation, and enabling compliance validation. This
study outlines a practical roadmap for improving the transparency, reliability,
and applicability of AI tools in requirements engineering workflows,
particularly in regulated and safety-critical environments where explainability
is crucial for adoption and certification.

</details>


### [518] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Main category: cs.SE

TL;DR: SCELM is an automated framework addressing challenges in managing software changes efficiently to minimize risks and failures in modern online services.


<details>
  <summary>Details</summary>
Motivation: Mitigate risks associated with frequent software changes in online services and reduce service failures and economic losses.

Method: Introduces SCELM, an automated and comprehensive framework for managing software changes.

Result: The framework improves software change management, contributing to fewer service failures and reduced economic losses.

Conclusion: SCELM demonstrates the importance of efficient and precise software change management to enhance service reliability and economic outcomes.

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [519] [Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs](https://arxiv.org/abs/2507.09414)
*Khizra Sohail,Atif Aftab Ahmed Jilani,Nigar Azhar Butt*

Main category: cs.SE

TL;DR: This paper improves automated test generation in Scratch games by enhancing NEATEST with a branch coverage fitness function, which outperforms statement coverage in fault detection and complex branch exploration.


<details>
  <summary>Details</summary>
Motivation: The need for improved fault detection and control structure exploration in game-like programs, where existing methods relying only on statement coverage are inadequate.

Method: Introduce a branch coverage-based fitness function into the NEATEST framework; evaluate the approach empirically on 25 Scratch games and use mutation analysis to compare performance against statement coverage.

Result: Branch coverage-based testing achieved higher coverage in 13 of 25 games and showed better fault detection, particularly in programs with complex conditions, with fewer false positives.

Conclusion: Branch coverage testing enhances the effectiveness of automated game testing by improving branch exploration and fault detection compared to statement coverage methods.

Abstract: Automated test generation for game-like programs presents unique challenges
due to their non-deterministic behavior and complex control structures. The
NEATEST framework has been used for automated testing in Scratch games,
employing neuroevolution-based test generation optimized for statement
coverage. However, statement coverage alone is often insufficient for fault
detection, as it does not guarantee execution of all logical branches. This
paper introduces a branch coverage-based fitness function to enhance test
effectiveness in automated game testing. We extend NEATEST by integrating a
branch fitness function that prioritizes control-dependent branches, guiding
the neuroevolution process to maximize branch exploration. To evaluate the
effectiveness of this approach, empirical experiments were conducted on 25
Scratch games, comparing Neatest with Statement Coverage (NSC) against Neatest
with Branch Coverage (NBC). A mutation analysis was also performed to assess
the fault detection capabilities of both techniques. The results demonstrate
that NBC achieves higher branch coverage than NSC in 13 out of 25 games,
particularly in programs with complex conditional structures. Moreover, NBC
achieves a lower false positive rate in mutation testing, making it a more
reliable approach for identifying faulty behavior in game programs. These
findings confirm that branch coverage-based test generation improves test
coverage and fault detection in Scratch programs.

</details>


### [520] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: StateGen is an automated framework to test LLM abilities with sequential API interactions by generating diverse tasks and translating them into natural language. It introduces the StateEval benchmark with 120 test cases.


<details>
  <summary>Details</summary>
Motivation: LLMs' capabilities have grown by integrating APIs, but current evaluation methods are insufficient for testing complex sequential API interactions.

Method: StateGen uses state-machine-based API constraint solving, energy-based sampling, control-flow injection, and LLM cooperation to generate and translate executable programs.

Result: StateGen created the StateEval benchmark containing 120 test cases across API task scenarios such as Session Service and Tensor Operation.

Conclusion: StateGen improves the evaluation of LLMs by addressing gaps in testing sequential API interactions, providing a basis to enhance LLM capabilities.

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [521] [Towards LLM-Based Automatic Playtest](https://arxiv.org/abs/2507.09490)
*Yan Zhao,Chiwei Tang*

Main category: cs.SE

TL;DR: The paper introduces "Lap," a new approach using ChatGPT for automating playtesting in match-3 games, ensuring higher code coverage and more crashes detected compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: Manual playtesting in video games is costly and time-consuming; automating it is challenging due to the need for domain knowledge and problem-solving, which conventional tools and LLMs currently lack for non-text games.

Method: Lap automates playtesting by converting game board snapshots into numeric matrices, utilizing ChatGPT for move suggestions, applying those moves iteratively, and testing them on a match-3 game environment.

Result: In testing with CasseBonbons, an open-source match-3 game, Lap achieved better code coverage and more program crash detections than three other existing tools.

Conclusion: Lap demonstrates the potential of using LLMs like ChatGPT for effective automatic playtesting, highlighting future possibilities for broader applications in gaming quality assurance.

Abstract: Playtesting is the process in which people play a video game for testing. It
is critical for the quality assurance of gaming software. Manual playtesting is
time-consuming and expensive. However, automating this process is challenging,
as playtesting typically requires domain knowledge and problem-solving skills
that most conventional testing tools lack. Recent advancements in artificial
intelligence (AI) have opened up new possibilities for applying Large Language
Models (LLMs) to playtesting. However, significant challenges remain: current
LLMs cannot visually perceive game environments, and most existing research
focuses on text-based games or games with robust APIs. Many non-text games lack
APIs to provide textual descriptions of game states, making it almost
impossible to naively apply LLMs for playtesting. This paper introduces Lap,
our novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to
test match-3 games, a category of games where players match three or more
identical tiles in a row or column to earn points. Lap encompasses three key
phases: processing of game environments, prompting-based action generation, and
action execution. Given a match-3 game, Lap takes a snapshot of the game board
and converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to
suggest moves based on that matrix and tentatively applies the suggested moves
to earn points and trigger changes in the game board. It repeats the
above-mentioned three steps iteratively until timeout. For evaluation, we
conducted a case study using Lap on an open-source match-3 game, CasseBonbons,
and empirically compared it with three existing tools. Our results are
promising: Lap outperformed existing tools by achieving higher code coverage
and triggering more program crashes. This research sheds light on the future of
automatic testing and LLM applications.

</details>


### [522] [It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective](https://arxiv.org/abs/2507.09529)
*Yunqian Wang,Xiaohong Li,Yao Zhang,Yuekang Li,Zhiping Zhou,Ruitao Feng*

Main category: cs.SE

TL;DR: The paper introduces VulTegra, an analysis framework for evaluating deep learning-based models in vulnerability detection, identifying key challenges and proposing improvements.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved issues of consistency, real-world effectiveness, and adaptability in DL-based vulnerability detectors, which impact the reliability and accuracy of detection systems.

Method: The authors develop and employ VulTegra, a multidimensional evaluation framework, to analyze both scratch-trained and pre-trained DL models. The framework compares these detectors across various performance dimensions and identifies influential factors.

Result: The study finds low consistency and scalability challenges in current state-of-the-art detectors. Pre-trained models do not consistently outperform scratch-trained models, and key factors influencing performance are uncovered. Adjusting one such factor improves recall and F1 scores significantly.

Conclusion: The research highlights critical weaknesses in DL-based detectors, the limited utility of CWE-based classification, and emphasizes the need to integrate vulnerability types and code features into effective detection strategies.

Abstract: With the growing threat of software vulnerabilities, deep learning (DL)-based
detectors have gained popularity for vulnerability detection. However, doubts
remain regarding their consistency within declared CWE ranges, real-world
effectiveness, and applicability across scenarios. These issues may lead to
unreliable detection, high false positives/negatives, and poor adaptability to
emerging vulnerabilities. A comprehensive analysis is needed to uncover
critical factors affecting detection and guide improvements in model design and
deployment. In this paper, we present VulTegra, a novel evaluation framework
that conducts a multidimensional comparison of scratch-trained and
pre-trained-based DL models for vulnerability detection. VulTegra reveals that
state-of-the-art (SOTA) detectors still suffer from low consistency, limited
real-world capabilities, and scalability challenges. Contrary to common belief,
pre-trained models are not consistently better than scratch-trained models but
exhibit distinct strengths in specific contexts.Importantly, our study exposes
the limitations of relying solely on CWE-based classification and identifies
key factors that significantly affect model performance. Experimental results
show that adjusting just one such factor consistently improves recall across
all seven evaluated detectors, with six also achieving better F1 scores. Our
findings provide deeper insights into model behavior and emphasize the need to
consider both vulnerability types and inherent code features for effective
detection.

</details>


### [523] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Main category: cs.SE

TL;DR: The paper introduces a cost-efficient, serverless system using Google's Gemini API for real-time stock analysis. It emphasizes system design, debugging challenges, and human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: To democratize financial data analysis using accessible LLMs while minimizing costs.

Method: Implemented a serverless system leveraging Gemini API, GitHub Actions for automation, and a static frontend for visualizing results. Incrementally refined through debugging and architectural iterations.

Result: Built a near-zero-cost, robust platform for AI-driven stock analysis that is publicly accessible and open-sourced.

Conclusion: LLMs can transform financial analysis, and debugging plays a critical role in developing efficient human-AI collaborative systems.

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [524] [How to Define Design in Industrial Control and Automation Software](https://arxiv.org/abs/2507.09594)
*Aydin Homay*

Main category: cs.SE

TL;DR: The paper analyzes existing definitions of design within software and industrial control systems, proposes scientific approaches, and distinguishes between ad-hoc and systematic methodologies.


<details>
  <summary>Details</summary>
Motivation: To tackle inefficiencies and lack of innovation caused by the absence of a scientific foundation in design, particularly in software and industrial control system industries.

Method: The paper reviews existing design definitions, evaluates misconceptions, incorporates design theory insights, and compares ad-hoc and systematic design approaches.

Result: Clarified misconceptions about design, identified characteristics of good design, and defined differences between design and design language scientifically.

Conclusion: Adopting systematic approaches and addressing both operational and evolutionary concerns are essential for achieving efficient and innovative design solutions.

Abstract: Design is a fundamental aspect of engineering, enabling the creation of
products, systems, and organizations to meet societal and/or business needs.
However, the absence of a scientific foundation in design often results in
subjective decision-making, reducing both efficiency and innovation. This
challenge is particularly evident in the software industry and, by extension,
in the domain of industrial control and automation systems (iCAS).
  In this study, first we review the existing design definitions within the
software industry, challenge prevailing misconceptions about design, review
design definition in the field of design theory and address key questions such
as: When does design begin? How can design be defined scientifically? What
constitutes good design? and the difference between design and design language
by relying on advancements in the field of design theory. We also evaluate the
distinction between ad-hoc and systematic design approaches, and present
arguments on how to balance complementary operational concerns while resolving
conflicting evolutionary concerns.

</details>


### [525] [The Mythical Good Software](https://arxiv.org/abs/2507.09596)
*Aydin Homay*

Main category: cs.SE

TL;DR: The paper investigates the notion that high cohesion and low coupling in software design may not universally correlate with optimal design, explaining potential flaws in this belief.


<details>
  <summary>Details</summary>
Motivation: To challenge the widely held notion in software engineering that high cohesion and low coupling are universally ideal characteristics of good software design.

Method: The study critically examines the concepts and relationship between cohesion and coupling, and their trade-offs, through theoretical analysis, rather than treating them as unrelated principles.

Result: The study provides a deeper understanding of cohesion and coupling, showing they are interconnected principles varying in space and time, and discusses cases where prioritizing cohesion might not be ideal.

Conclusion: Cohesion and coupling should not be dogmatically pursued without accounting for their costs and context-dependent trade-offs in software design.

Abstract: Good software has high cohesion and low coupling is clumsy, obscure, and in
some certain cases could be actually a harmful state of being. It is clumsy
because there is no perfect correlation between higher cohesiveness and optimum
design, and it is obscure because it conveys the message that coupling and
cohesion are two distinct design principles, while there are in principle the
same design approaches, and only the time and space differ between them, and it
could also be a harmful state of being because we should not always aim for
higher cohesiveness without considering its cost.
  In the course of this study, we aim to elucidate for the readers the meaning
and underlying philosophy of the aforementioned paragraph.

</details>


### [526] [Complexity and Coupling: A Functional Domain Approach](https://arxiv.org/abs/2507.09599)
*Aydin Homay*

Main category: cs.SE

TL;DR: The paper refines the concepts of complexity and coupling in industrial control and automation systems (iCAS), emphasizing their interpretation in the functional domain over physical definitions.


<details>
  <summary>Details</summary>
Motivation: Ambiguity in defining complexity and coupling often leads to confusion and inconsistency in various industries, requiring a clearer and functional-domain-based understanding.

Method: The authors analyze examples across software engineering, industrial automation, and mechanical design to establish the functional-domain perspective, differentiating it from size or component-based metrics.

Result: The paper shows complexity isn't tied directly to system size, and coupling occurs in the functional rather than physical domain, challenging traditional beliefs.

Conclusion: To reduce complexity and ensure effective design, coupling and complexity must be addressed within the functional domain rather than the physical one.

Abstract: This paper provides a precise and scientific definition of complexity and
coupling, grounded in the functional domain, particularly within industrial
control and automation systems (iCAS). We highlight the widespread ambiguity in
defining complexity and coupling, emphasizing that many existing definitions
rooted in physical attributes lead to confusion and inconsistencies.
Furthermore, we re-exhibit why coupled design inherently increases complexity
and how potentially this complexity could be reduced. Drawing on examples from
various disciplines, such as software engineering, industrial automation, and
mechanical design, we demonstrate that complexity does not necessarily
correlate with system size or the number of components, and coupling, unlike
common belief in software engineering, actually does not occur in the physical
domain but in the functional domain. We conclude that effective design
necessitates addressing coupling and complexity within the functional domain.

</details>


### [527] [Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review](https://arxiv.org/abs/2507.09637)
*Lo Gullstrand Heander,Emma Söderberg,Christofer Rydenfält*

Main category: cs.SE

TL;DR: The paper presents a cognitive model for code review processes, called CRDM, identifying two key phases (orientation and analytical) and their associated decision-making actions.


<details>
  <summary>Details</summary>
Motivation: To address misalignments and frustrations in traditional code review tools and processes while maintaining interpersonal benefits such as knowledge transfer and shared ownership.

Method: Conducted an ethnographic think-aloud study with 10 participants performing 34 code reviews, followed by thematic, statistical, temporal, and sequential analysis to build a cognitive model of code review.

Result: Introduced the CRDM (Code Review as Decision-Making) model, which highlights two primary phases in code reviews: the orientation phase to establish context and an analytical phase for assessment and planning.

Conclusion: Understanding cognitive processes in code review can help develop better tools and workflows that enhance efficiency and enjoyment while retaining interpersonal benefits.

Abstract: Code review is a well-established and valued practice in the software
engineering community contributing to both code quality and interpersonal
benefits. However, there are challenges in both tools and processes that give
rise to misalignments and frustrations. Recent research seeks to address this
by automating code review entirely, but we believe that this risks losing the
majority of the interpersonal benefits such as knowledge transfer and shared
ownership.
  We believe that by better understanding the cognitive processes involved in
code review, it would be possible to improve tool support, with out without AI,
and make code review both more efficient, more enjoyable, while increasing or
maintaining all of its benefits. In this paper, we conduct an ethnographic
think-aloud study involving 10 participants and 34 code reviews. We build a
cognitive model of code review bottom up through thematic, statistical,
temporal, and sequential analysis of the transcribed material. Through the
data, the similarities between the cognitive process in code review and
decision-making processes, especially recognition-primed decision-making,
become apparent.
  The result is the Code Review as Decision-Making (CRDM) model that shows how
the developers move through two phases during the code review; first an
orientation phase to establish context and rationale and then an analytical
phase to understand, assess, and plan the rest of the review. Throughout the
process several decisions must be taken, on writing comments, finding more
information, voting, running the code locally, verifying continuous integration
results, etc.
  Analysis software and process-coded data publicly available at:
https://doi.org/10.5281/zenodo.15758266

</details>


### [528] [Is Quantization a Deal-breaker? Empirical Insights from Large Code Models](https://arxiv.org/abs/2507.09665)
*Saima Afrin,Bowen Xu,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: The paper explores how model quantization, specifically Activation-aware Weight Quantization (AWQ), impacts functional correctness and qualitative aspects of code generated by large language models tailored for software engineering.


<details>
  <summary>Details</summary>
Motivation: Reducing computational resource demands and environmental impact of large language models, while preserving software quality in automatically generated code.

Method: Use AWQ quantization on two code models (CodeLlama and DeepSeekCoder) to generate Java and Python code, followed by evaluation using metrics like cyclomatic complexity, cognitive complexity, and lines of code.

Result: Quantized models maintain their functional correctness while retaining essential code quality attributes like maintainability and simplicity.

Conclusion: Quantization proves to be a viable technique for optimizing code generation models without compromising software quality.

Abstract: The growing scale of large language models (LLMs) not only demands extensive
computational resources but also raises environmental concerns due to their
increasing carbon footprint. Model quantization emerges as an effective
approach that can reduce the resource demands of LLMs by decreasing parameter
precision without substantially affecting performance (e.g., 16 bit to 4 bit).
While recent studies have established quantization as a promising approach for
optimizing large code models (LCMs), a specialized subset of LLMs tailored for
automated software engineering, their findings offer only limited insights into
its practical implications. Specifically, current investigations focus only on
the functional correctness of the code generated by quantized models,
neglecting how quantization impacts critical aspects of code quality such as
reliability, maintainability, and security. To bridge this gap, our study
investigates the effects of quantization on the qualitative aspects of
automatically generated code. We apply Activation-aware Weight Quantization
(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate
Java and Python code. Using state-of-the-art static analysis tools, we evaluate
software quality metrics and static features including cyclomatic complexity,
cognitive complexity, and lines of code. Our findings reveal that quantization
is a robust technique that not only preserves functional correctness, but also
retains key qualitative code attributes sought after by developers, such as
maintainability and structural simplicity.

</details>


### [529] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: The paper presents OrQstrator, a modular framework using Deep Reinforcement Learning for quantum circuit optimization tailored to NISQ-era hardware constraints.


<details>
  <summary>Details</summary>
Motivation: Current quantum circuits face challenges such as noise impact and inefficient optimization, especially in the NISQ era, necessitating advanced strategies to enhance their scalability and hardware efficiency.

Method: OrQstrator employs three optimization modules: a DRL-based circuit rewriter, a domain-specific resynthesizer, and a parameterized circuit optimizer, coordinated by an orchestration engine that integrates hardware-aware features.

Result: The proposed system intelligently selects optimization strategies, delivering circuits optimized for depth, gate count, fidelity, and backend-specific constraints.

Conclusion: OrQstrator effectively addresses NISQ-era challenges by leveraging modular optimization and adaptive orchestration, providing a pathway for more efficient quantum circuit design and execution.

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [530] [Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications](https://arxiv.org/abs/2507.09820)
*Jia Yi Goh,Shaun Khoo,Nyx Iskandar,Gabriel Chua,Leanne Tan,Jessica Foo*

Main category: cs.SE

TL;DR: The paper introduces a practical framework for evaluating safety at the application-level in large language model (LLM) systems, addressing gaps between theoretical AI safety and operational deployment.


<details>
  <summary>Details</summary>
Motivation: The authors noticed a lack of safety evaluation methods for application-level LLM systems, focusing on real-world deployment factors like prompts and guardrails.

Method: The framework includes principles for building safety risk taxonomies and practices for evaluating LLM application safety, tested through internal pilot studies in real-world use cases.

Result: The framework was successfully validated during its internal deployment, proving its utility and offering a guideline for scaling safety efforts.

Conclusion: This work emphasizes the need for practical safety measures in LLM applications and provides actionable solutions to enhance the safe deployment of such systems.

Abstract: Most safety testing efforts for large language models (LLMs) today focus on
evaluating foundation models. However, there is a growing need to evaluate
safety at the application level, as components such as system prompts,
retrieval pipelines, and guardrails introduce additional factors that
significantly influence the overall safety of LLM applications. In this paper,
we introduce a practical framework for evaluating application-level safety in
LLM systems, validated through real-world deployment across multiple use cases
within our organization. The framework consists of two parts: (1) principles
for developing customized safety risk taxonomies, and (2) practices for
evaluating safety risks in LLM applications. We illustrate how the proposed
framework was applied in our internal pilot, providing a reference point for
organizations seeking to scale their safety testing efforts. This work aims to
bridge the gap between theoretical concepts in AI safety and the operational
realities of safeguarding LLM applications in practice, offering actionable
guidance for safe and scalable deployment.

</details>


### [531] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: This paper introduces LiveRepoReflection, a benchmark for evaluating code understanding and generation in multi-file repository contexts, and RepoReflection-Instruct, a dataset for training models in code reflection tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in evaluating LLMs for modifying and reflecting on code in multi-file repository contexts, which remains challenging due to a lack of benchmarks and to avoid data contamination.

Method: The authors propose LiveRepoReflection, a benchmark with 1,888 test cases in multiple programming languages, and RepoReflection-Instruct, an instruction-tuning dataset. They also introduce RepoReflectionCoder, a model trained via two-turn dialogues for code generation and repair.

Result: The study presents a leaderboard where over 40 LLMs are evaluated, demonstrating their performance on repository-based code reflection tasks.

Conclusion: The research highlights improved assessment tools for LLMs in repository settings and lays the foundation for enhancing LLM reflection capabilities in complex programming scenarios.

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [532] [PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths](https://arxiv.org/abs/2507.09892)
*Zimu Chen,Di Wang*

Main category: cs.SE

TL;DR: PathFuzzing combines fuzzing and symbolic execution for worst-case resource consumption analysis, achieving better performance compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in code coverage and path explosion in the context of Worst-Case Analysis (WCA) using existing fuzzing and symbolic execution techniques.

Method: Proposed PathFuzzing, which transforms code into a symbolic form interpreted as paths and uses evolutionary fuzzing to find high resource consumption scenarios.

Result: Experimental evaluation showed PathFuzzing consistently outperforms existing fuzzing and symbolic execution baselines over a tested benchmark suite.

Conclusion: PathFuzzing is a promising hybrid approach for WCA, effectively combining the strengths of fuzzing and symbolic execution.

Abstract: Estimating worst-case resource consumption is a critical task in software
development. The worst-case analysis (WCA) problem is an optimization-based
abstraction of this task. Fuzzing and symbolic execution are widely used
techniques for addressing the WCA problem. However, improving code coverage in
fuzzing or managing path explosion in symbolic execution within the context of
WCA poses significant challenges. In this paper, we propose PathFuzzing, aiming
to combine the strengths of both techniques to design a WCA method. The key
idea is to transform a program into a symbolic one that takes an execution path
(encoded as a binary string) and interprets the bits as branch decisions.
PathFuzzing then applies evolutionary fuzzing techniques to the transformed
program to search for binary strings that represent satisfiable path conditions
and lead to high resource consumption. We evaluate the performance of
PathFuzzing experimentally on a benchmark suite that consists of prior work's
benchmarks and some added by us. Results show that PathFuzzing generally
outperforms a fuzzing and a symbolic-execution baseline.

</details>


### [533] [Modelling Interrelations Between Agile Practices: The Agile Map](https://arxiv.org/abs/2507.09907)
*Thomas Hansper,Kevin Phong Pham,Michael Neumann*

Main category: cs.SE

TL;DR: This paper introduces the Agile Map, a theoretical model outlining the relationships between agile practices to assist practitioners in combining them effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge posed by the wide variety and combinatorial use of agile practices, as their interrelations are not clearly understood, potentially limiting their successful application in practice.

Method: The authors employ a systematic approach to identify and describe the interrelationships between different agile practices, resulting in the development of the Agile Map model.

Result: The study delivers the Agile Map, which provides a structured representation of how different agile practices are related and interact.

Conclusion: The Agile Map offers practical guidance for teams by elucidating connections between agile practices, enabling their meaningful selection and combination for effective agile work.

Abstract: Agile methods are defined through guidelines comprising various practices
intended to enable agile ways of working. These guidelines further comprise a
specific set of agile practices aiming to enable teams for an agile way of
working. However, due to its wide-spread use in practice we know that agile
practices are adopted and tailored intensively, which lead to a high variety of
agile practices in terms of their level of detail. Problem: A high variety of
agile practices can be challenging as we do not know how different agile
practices are interrelated with each other. To be more precise, tailoring and
adopting agile practices may lead to the challenge, that the combinatorial use
of several agile practices can only be successful to a limited extent, as
practices support or even require each other for a effective use in practice.
Objective: Our study aims to provide an enabler for this problem. We want to
identify interrelations between agile practices and describe them in a
systematic manner. Contribution: The core contribution of this paper is the
Agile Map, a theoretical model describing relations between agile practices
following a systematic approach aiming to provide an overview of coherences
between agile practices. The model aims to support practitioners in selecting
and combining agile practices in a meaningful way.

</details>


### [534] [When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance](https://arxiv.org/abs/2507.09911)
*Marvin Auf der Landwehr,Julia Topp,Michael Neumann*

Main category: cs.SE

TL;DR: This study analyzes the impact of compressed work schedules, such as the four-day workweek, on IT organizations and provides a framework for adopting these schedules.


<details>
  <summary>Details</summary>
Motivation: Agile IT organizations require adaptive and efficient work environments to enhance value creation; compressed work schedules are seen as a potential solution.

Method: The study performs a systematic literature review and web content analysis to explore four-day workweek conceptualizations and their organizational and social effects.

Result: The study produces a meta-framework that aligns compressed work schedule concepts with their impacts, assisting with tailored implementation by managers.

Conclusion: Adopting compressed work schedules in IT enterprises can provide organizational benefits when adapted to specific managerial circumstances using the proposed meta-framework.

Abstract: Context: Agile IT organizations, which are characterized by self-organization
and collaborative social interactions, require motivating, efficient and
flexible work environments to maximize value creation. Compressed work
schedules such as the four-day workweek have evolved into multiple facets over
the last decades and are associated with various benefits for organizations and
their employees. Objective: Our objective in this study is to deepen our
comprehension of the impact of compressed work schedules on the operational
efficacy of IT enterprises, while concurrently developing a comprehensive
framework delineating the intricacies of compressed work schedules.Method: We
conducted a systematic review of available conceptualizations related to
four-day workweek schedules and elaborate on their organizational and social
effects. To cover scientific and practice-oriented literature, our review
combined a systematic literature review and a web content analysis. Results:
Based on the generated insights, we derive a meta-framework that matches
conceptualizations and effects, finally guiding the adoption of compressed work
schedules based on individual managerial prerequisites and circumstances.

</details>


### [535] [Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks](https://arxiv.org/abs/2507.10054)
*Emir Bosnak,Sahand Moslemi,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: This study investigates how open-source large language models (LLMs) generate vulnerable code upon direct or indirect prompting, revealing high rates of vulnerability production.


<details>
  <summary>Details</summary>
Motivation: To understand the risks associated with open-source LLMs generating insecure code when explicitly prompted, and to evaluate the effectiveness of safety mechanisms in these models.

Method: The authors employ a dual experimental design: Dynamic Prompting to test template-based variations in vulnerability type, persona, and directness; and Reverse Prompting to assess reproduction accuracy using real vulnerable code samples.

Result: All three open-source models (Qwen2, Mistral, Gemma) frequently produced vulnerable code, with Qwen2 generating the most type-correct outputs. Student personas were more likely to induce vulnerabilities, and vulnerability reproduction peaked at moderate cyclomatic complexity levels.

Conclusion: Open-source LLMs lack effective safety mechanisms to prevent vulnerable code generation, even in benign scenarios like educational use, highlighting significant security risks.

Abstract: Large Language Models (LLMs) are increasingly used as code assistants, yet
their behavior when explicitly asked to generate insecure code remains poorly
understood. While prior research has focused on unintended vulnerabilities or
adversarial prompting techniques, this study examines a more direct threat
scenario: open-source LLMs generating vulnerable code when prompted either
directly or indirectly. We propose a dual experimental design: (1) Dynamic
Prompting, which systematically varies vulnerability type, user persona, and
directness across structured templates; and (2) Reverse Prompting, which
derives prompts from real vulnerable code samples to assess vulnerability
reproduction accuracy. We evaluate three open-source 7B-parameter models
(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the
presence of vulnerabilities and the correctness of the generated vulnerability
type. Results show all models frequently produce vulnerable outputs, with Qwen2
achieving highest correctness rates. User persona significantly affects
success, where student personas achieved higher vulnerability rates than
professional roles, while direct prompts were marginally more effective.
Vulnerability reproduction followed an inverted-U pattern with cyclomatic
complexity, peaking at moderate ranges. Our findings expose limitations of
safety mechanisms in open-source models, particularly for seemingly benign
educational requests.

</details>


### [536] [LLMShot: Reducing snapshot testing maintenance via LLMs](https://arxiv.org/abs/2507.10062)
*Ergün Batuhan Kaynak,Mayasah Lami,Sahand Moslemi,Anil Koyuncu*

Main category: cs.SE

TL;DR: The paper presents LLMShot, a framework that uses vision-based large language models to automate the analysis of snapshot test failures in UI testing, significantly reducing manual triage effort.


<details>
  <summary>Details</summary>
Motivation: Snapshot testing for UI validation is labor-intensive due to the need for manually distinguishing between regressions and intentional design changes caused by frequent UI updates.

Method: The authors developed LLMShot, a framework that uses vision-based Large Language Models for hierarchical classification of UI changes in snapshot test failures. They used an iOS application with configurable feature flags to simulate realistic snapshot scenarios, and evaluated its performance using Gemma3 models.

Result: LLMShot, specifically the 12B model, achieved over 84% recall in classifying failure root causes, while the smaller 4B model performed sufficiently for continuous integration environments.

Conclusion: LLMShot offers an automated, semantic analysis approach for snapshot test failures, significantly reducing manual triage efforts and laying groundwork for smarter UI testing paradigms despite some limitations in current prompting-based approaches.

Abstract: Snapshot testing has emerged as a critical technique for UI validation in
modern software development, yet it suffers from substantial maintenance
overhead due to frequent UI changes causing test failures that require manual
inspection to distinguish between genuine regressions and intentional design
changes. This manual triage process becomes increasingly burdensome as
applications evolve, creating a need for automated analysis solutions. This
paper introduces LLMShot, a novel framework that leverages vision-based Large
Language Models to automatically analyze snapshot test failures through
hierarchical classification of UI changes. To evaluate LLMShot's effectiveness,
we developed a comprehensive dataset using a feature-rich iOS application with
configurable feature flags, creating realistic scenarios that produce authentic
snapshot differences representative of real development workflows. Our
evaluation using Gemma3 models demonstrates strong classification performance,
with the 12B variant achieving over 84% recall in identifying failure root
causes while the 4B model offers practical deployment advantages with
acceptable performance for continuous integration environments. However, our
exploration of selective ignore mechanisms revealed significant limitations in
current prompting-based approaches for controllable visual reasoning. LLMShot
represents the first automated approach to semantic snapshot test analysis,
offering developers structured insights that can substantially reduce manual
triage effort and advance toward more intelligent UI testing paradigms.

</details>


### [537] [Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models](https://arxiv.org/abs/2507.10103)
*Hanyang Guo,Xiaoheng Xie,Hong-Ning Dai,Peng Di,Yu Zhang,Bishenghui Tao,Zibin Zheng*

Main category: cs.SE

TL;DR: SelRepair is a novel automated program repair (APR) approach integrating a fine-tuned language model with a dual Retrieval-Augmented-Generation (RAG) module, effectively improving repair performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing Automated Program Repair (APR) methods face challenges in addressing code-specific features, defect types, training data quality, and computational efficiency. There is a need to improve these limitations, especially with increased adoption of Large Language Models (LLMs) in APR tasks.

Method: SelRepair incorporates a fine-tuned LLM trained on a bug-fix pair dataset and uses a dual RAG module to consider semantic and structural similarities through an RAG selection gate, optimizing information retrieval and inference efficiency.

Result: Experiments on Java datasets demonstrate SelRepair significantly outperforms existing APR methods with exact match (EM) rates of 26.29% and 17.64% on different datasets. It also reduces inference time by at least 6.42% with controlled input lengths.

Conclusion: SelRepair successfully addresses previous APR challenges by leveraging code-specific features and achieving improved performance, accuracy, and efficiency in software repair tasks.

Abstract: Automated Program Repair (APR) is essential for ensuring software reliability
and quality while enhancing efficiency and reducing developers' workload.
Although rule-based and learning-based APR methods have demonstrated their
effectiveness, their performance was constrained by the defect type of repair,
the quality of training data, and the size of model parameters. Recently, Large
Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have
been increasingly adopted in APR tasks. However, current code LLMs and RAG
designs neither fully address code repair tasks nor consider code-specific
features. To overcome these limitations, we propose SelRepair, a novel APR
approach with integration of a fine-tuned LLM with a newly-designed dual RAG
module. This approach uses a bug-fix pair dataset for fine-tuning and
incorporates semantic and syntactic/structural similarity information through
an RAG selection gate. This design ensures relevant information is retrieved
efficiently, thereby reducing token length and inference time. Evaluations on
Java datasets show SelRepair outperforms other APR methods, achieving 26.29%
and 17.64% in terms of exact match (EM) on different datasets while reducing
inference time by at least 6.42% with controlled input lengths.

</details>


### [538] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Main category: cs.SE

TL;DR: This paper demonstrates that small, fine-tuned language models can effectively generate postconditions with lower computational demands, rivaling larger models.


<details>
  <summary>Details</summary>
Motivation: Ensuring software correctness requires formal specifications, but manually creating them is tedious and error-prone. Existing large language models are promising but resource-intensive, prompting the need for smaller, efficient models.

Method: A $7$B-parameter code model is fine-tuned on a specialized dataset containing prompts, reasoning logs, and postconditions. This method handles code dependencies and maintains pre-state information to facilitate correct specification.

Result: The fine-tuned compact model matches or exceeds the performance of significantly larger language models in syntax correctness, semantic correctness, and bug-distinguishing capabilities when evaluated using the Defects4J benchmark.

Conclusion: Using targeted fine-tuning on a modest dataset, smaller language models can achieve competitive results, providing a practical alternative to large resource-heavy models for specification generation.

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


### [539] [Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements](https://arxiv.org/abs/2507.10228)
*Hugo Villamizar,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: The paper integrates AMDiRE and PerSpecML approaches for improving requirements engineering in AI-enabled systems.


<details>
  <summary>Details</summary>
Motivation: Address challenges in specifying trustworthiness-related requirements for AI-enabled systems.

Method: Combining AMDiRE for artefact-based RE and PerSpecML for perspective-based ML-specific requirements.

Result: Proposes a pathway to align stakeholder concerns with structured artefact models for operationalizing trustworthiness.

Conclusion: Key research directions and challenges in trustworthiness-related requirements are discussed.

Abstract: Growing concerns around the trustworthiness of AI-enabled systems highlight
the role of requirements engineering (RE) in addressing emergent,
context-dependent properties that are difficult to specify without structured
approaches. In this short vision paper, we propose the integration of two
complementary approaches: AMDiRE, an artefact-based approach for RE, and
PerSpecML, a perspective-based method designed to support the elicitation,
analysis, and specification of machine learning (ML)-enabled systems. AMDiRE
provides a structured, artefact-centric, process-agnostic methodology and
templates that promote consistency and traceability in the results; however, it
is primarily oriented toward deterministic systems. PerSpecML, in turn,
introduces multi-perspective guidance to uncover concerns arising from the
data-driven and non-deterministic behavior of ML-enabled systems. We envision a
pathway to operationalize trustworthiness-related requirements, bridging
stakeholder-driven concerns and structured artefact models. We conclude by
outlining key research directions and open challenges to be discussed with the
RE community.

</details>


### [540] [An Empirical Study of Interaction Bugs in ROS-based Software](https://arxiv.org/abs/2507.10235)
*Zhixiang Chen,Zhuangbin Chen,Xingjie Cai,Wei Li,Zibin Zheng*

Main category: cs.SE

TL;DR: The paper investigates interaction bugs (iBugs) in robotics systems using the Robot Operating System (ROS), analyzing 121 iBugs across ten ROS projects to categorize, understand causes, and propose improvements for more reliable robotic systems.


<details>
  <summary>Details</summary>
Motivation: Robotic systems require interaction between multiple software and hardware components, but interaction bugs, which often occur at the boundaries of these components, are underexplored. This paper aims to fill this gap.

Method: The study conducted an empirical analysis of 121 interaction bugs found in ten actively maintained ROS projects. These bugs were categorized into intra-system, hardware, and environmental iBugs, with an analysis of root causes and fixing strategies.

Result: The categorization and analysis of iBugs revealed their root causes and the impact on robotics systems, providing new insights into their nature. The findings highlight areas for better prevention and detection of these bugs.

Conclusion: The insights derived from the study aim to guide the design of more robust and safer robotic systems by addressing interaction bugs in the ROS framework.

Abstract: Modern robotic systems integrate multiple independent software and hardware
components, each responsible for distinct functionalities such as perception,
decision-making, and execution. These components interact extensively to
accomplish complex end-to-end tasks. As a result, the overall system
reliability depends not only on the correctness of individual components, but
also on the correctness of their interactions. Failures often manifest at the
boundaries between components, yet interaction-related reliability issues in
robotics--referred to here as interaction bugs (iBugs)--remain underexplored.
  This work presents an empirical study of iBugs within robotic systems built
using the Robot Operating System (ROS), a widely adopted open-source robotics
framework. A total of 121 iBugs were analyzed across ten actively maintained
and representative ROS projects. The identified iBugs are categorized into
three major types: intra-system iBugs, hardware iBugs, and environmental iBugs,
covering a broad range of interaction scenarios in robotics. The analysis
includes an examination of root causes, fixing strategies, and the impact of
these bugs. Several findingsa are derived that shed light on the nature of
iBugs and suggest directions for improving their prevention and detection.
These insights aim to inform the design of more robust and safer robotic
systems.

</details>


### [541] [Helveg: Diagrams for Software Documentation](https://arxiv.org/abs/2507.10244)
*Adam Štěpánek,David Kuťák,Barbora Kozlíková,Jan Byška*

Main category: cs.SE

TL;DR: The paper introduces "Helveg," a tool for visualizing codebases using interactive diagrams to improve over traditional, static documentation. It highlights improvements made to the tool after initial feedback and re-evaluates it with users.


<details>
  <summary>Details</summary>
Motivation: Developers often face challenges in understanding codebases, especially when relying on static and predefined documentation. There is a need for more interactive and flexible methods to explore and understand software architectures.

Method: The authors designed a tool named "Helveg" featuring interactive node-link diagrams with glyphs and filtering capabilities to give a high-level overview of codebases. After receiving user feedback on an earlier version, they enhanced the tool by improving its user interface, glyphs, and interaction mechanisms.

Result: The revised version of Helveg was evaluated by the same group as in the initial test. The improvements addressed many issues such as readability, intuitiveness, and overall user experience.

Conclusion: Helveg presents a promising visualization tool for codebase exploration, but iterative improvements and user-centered design are crucial to refining its utility and usability.

Abstract: Software developers often have to gain an understanding of a codebase. Be it
programmers getting onboarded onto a team project or, for example, developers
striving to grasp an external open-source library. In either case, they
frequently turn to the project's documentation. However, documentation in its
traditional textual form is ill-suited for this kind of high-level exploratory
analysis, since it is immutable from the readers' perspective and thus forces
them to follow a predefined path. We have designed an approach bringing aspects
of software architecture visualization to API reference documentation. It
utilizes a highly interactive node-link diagram with expressive node glyphs and
flexible filtering capabilities, providing a high-level overview of the
codebase as well as details on demand. To test our design, we have implemented
a prototype named Helveg, capable of automatically generating diagrams of C\#
codebases. User testing of Helveg confirmed its potential, but it also revealed
problems with the readability, intuitiveness, and user experience of our tool.
Therefore, in this paper, which is an extended version of our VISSOFT paper
with DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems
through major changes to the glyph design, means of interaction, and user
interface of the tool. To assess the improvements, this new version of Helveg
was evaluated again with the same group of participants as the previous
version.

</details>


### [542] [A Grounded Theory on the Teacher and Student Roles in Pair Programming](https://arxiv.org/abs/2507.10305)
*Linus Ververs,Trang Linh Lam,Janina Berger,Lutz Prechelt*

Main category: cs.SE

TL;DR: This paper investigates the conditions under which knowledge transfer negatively impacts pair programming sessions and offers insights on avoiding such harm.


<details>
  <summary>Details</summary>
Motivation: To explore when knowledge transfer in pair programming becomes detrimental, specifically focusing on the implications of power imbalances in developer interactions.

Method: The authors employed Grounded Theory Methodology by analyzing 17 recorded pair programming sessions with developers from 5 German companies and conducting 6 interviews with developers from 4 other German companies.

Result: The study introduces the concepts of student and teacher roles to address knowledge gaps, identifies pitfalls, and formulates a grounded theory focusing on the "Power Gap" in pair programming.

Conclusion: Knowledge transfer in pair programming can harm the process when developers neglect each other's needs and don't manage the Power Gap appropriately, potentially leading to defensive behaviors detrimental to collaboration and code quality.

Abstract: Context: Pair programming is an established (agile) practice and is practiced
throughout the industry. Objective: Understand under what circumstances
knowledge transfer can harm a pair programming session. Method: Grounded Theory
Methodology based on 17 recorded pair programming sessions with 18 developers
from 5 German software companies accompanied, by 6 interviews with different
developers from 4 other German companies. Results: We define the student and
teacher roles to help developers deal with a one-sided knowledge gap. We
describe pitfalls to avoid and develop a grounded theory centered around the
Power Gap in pair programming. Conclusions: Knowledge transfer can be harmful
when developers don't pay attention to their partners needs and desires. If
developers don't pay attention to the Power Gap and keep it in check, Defensive
Behavior may arise that leads to a vicious cycle impacting the knowledge
transfer, the Togetherness and the code quality in a negative way.

</details>


### [543] [Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation](https://arxiv.org/abs/2507.10321)
*Viktor Sinitsyn,Nils Schlautmann,Florian Schwaiger,Florian Holzapfel*

Main category: cs.SE

TL;DR: The paper introduces a novel process and toolchain to improve digital interface design and onboard software development in aerospace, addressing challenges like automation and reducing labor efforts.


<details>
  <summary>Details</summary>
Motivation: The transformative changes in the aerospace industry have brought challenges in handling digital intra-device communications efficiently. Start-ups especially need solutions to streamline integration and functionality.

Method: This research proposes a process and toolchain emphasizing automation, flexibility, and compliance with design assurance standards. It has been validated through practical applications in various projects.

Result: The toolchain has been successfully implemented in multiple projects, proving its effectiveness in improving interface design efficiency and software development.

Conclusion: Adopting this novel approach can guide aerospace start-ups and traditional companies in overcoming digital interface challenges, fostering seamless system integration, and reducing manual labor.

Abstract: The aerospace industry has experienced significant transformations over the
last decade, driven by technological advancements and innovative solutions in
goods and personal transportation. This evolution has spurred the emergence of
numerous start-ups that now face challenges traditionally encountered by
established aerospace companies. Among these challenges is the efficient
processing of digital intra-device communication interfaces for onboard
equipment - a critical component for ensuring seamless system integration and
functionality. Addressing this challenge requires solutions that emphasize
clear and consistent interface descriptions, automation of processes, and
reduced labor-intensive efforts.
  This paper presents a novel process and toolchain designed to streamline the
development of digital interfaces and onboard software, which our team has
successfully applied in several completed projects. The proposed approach
focuses on automation and flexibility while maintaining compliance with design
assurance requirements.

</details>


### [544] [Self-Admitted GenAI Usage in Open-Source Software](https://arxiv.org/abs/2507.10422)
*Tao Xiao,Youmei Fan,Fabio Calefato,Christoph Treude,Raula Gaikovina Kula,Hideaki Hata,Sebastian Baltes*

Main category: cs.SE

TL;DR: This paper studies the role of generative AI tools in software development through the lens of developers' explicit mentions of using such tools, analyzing their practical, ethical, and legal implications.


<details>
  <summary>Details</summary>
Motivation: To understand how generative AI tools like ChatGPT and GitHub Copilot are being integrated into open-source software development and their real-world impact.

Method: The study analyzed over 250,000 GitHub repositories for explicit mentions of generative AI usage, developed a taxonomy of tasks, content types, and purposes, and conducted a developer survey. It also studied policies and guidelines and examined code churn trends.

Result: The paper identified 1,292 explicit mentions of AI usage across 156 repositories, revealing diverse applications. Developers manage AI use with a focus on transparency and quality control. Contrary to popular beliefs, no broad increase in code churn due to AI adoption was observed.

Conclusion: Developers are actively shaping how generative AI is utilized in software development, emphasizing the need for better project-level governance and guidelines to address transparency, attribution, and quality control challenges.

Abstract: The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot
and ChatGPT is transforming software development. Since generated source code
is virtually impossible to distinguish from manually written code, their
real-world usage and impact on open-source software development remain poorly
understood. In this paper, we introduce the concept of self-admitted GenAI
usage, that is, developers explicitly referring to the use of GenAI tools for
content creation in software artifacts. Using this concept as a lens to study
how GenAI tools are integrated into open-source software projects, we analyze a
curated sample of more than 250,000 GitHub repositories, identifying 1,292 such
self-admissions across 156 repositories in commit messages, code comments, and
project documentation. Using a mixed methods approach, we derive a taxonomy of
32 tasks, 10 content types, and 11 purposes associated with GenAI usage based
on 284 qualitatively coded mentions. We then analyze 13 documents with policies
and usage guidelines for GenAI tools and conduct a developer survey to uncover
the ethical, legal, and practical concerns behind them. Our findings reveal
that developers actively manage how GenAI is used in their projects,
highlighting the need for project-level transparency, attribution, and quality
control practices in the new era of AI-assisted software development. Finally,
we examine the longitudinal impact of GenAI adoption on code churn in 151
repositories with self-admitted GenAI usage and find no general increase,
contradicting popular narratives on the impact of GenAI on software
development.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [545] [CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience](https://arxiv.org/abs/2507.09024)
*Marie St-Laurent,Basile Pinsard,Oliver Contier,Elizabeth DuPre,Katja Seeliger,Valentina Borghesani,Julie A. Boyle,Lune Bellec,Martin N. Hebart*

Main category: q-bio.NC

TL;DR: CNeuroMod-THINGS combines two existing projects to create a new large-scale fMRI dataset capturing neural representations of semantic concepts using well-annotated stimuli.


<details>
  <summary>Details</summary>
Motivation: The need for larger, high-quality neuroimaging datasets drives the creation of CNeuroMod-THINGS, facilitating better modeling of neural representations of visual experiences.

Method: The study integrates the annotated image set from THINGS with extensive fMRI data acquisition from CNeuroMod participants exposed to 4000 images across 33-36 sessions.

Result: High-quality behavioral and neuroimaging metrics were reported, demonstrating the dataset's utility for modeling human visual experiences.

Conclusion: CNeuroMod-THINGS enhances the capacity to model the neural basis of human visual experiences by leveraging and uniting existing large-scale resources.

Abstract: Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.
CNeuroMod-THINGS meets this need by capturing neural representations for a wide
set of semantic concepts using well-characterized stimuli in a new
densely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS
exploits synergies between two existing projects: the THINGS initiative
(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has
developed a common set of thoroughly annotated images broadly sampling natural
and man-made objects which is used to acquire a growing collection of
large-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring
hundreds of hours of fMRI data from a core set of participants during
controlled and naturalistic tasks, including visual tasks like movie watching
and videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each
completed 33-36 sessions of a continuous recognition paradigm using
approximately 4000 images from the THINGS stimulus set spanning 720 categories.
We report behavioural and neuroimaging metrics that showcase the quality of the
data. By bridging together large existing resources, CNeuroMod-THINGS expands
our capacity to model broad slices of the human visual experience.

</details>


### [546] [Co-evolutionary Balance State of the Autism inter-Brain Network: A Neurofunctional Framework for Biomarker Discovery](https://arxiv.org/abs/2507.09045)
*S. Rezaei Afshar,H. Pouretemad,G. Reza Jafari*

Main category: q-bio.NC

TL;DR: This study introduces a 'coevolutionary balance paradigm' to identify network-level disruptions in ASD using fMRI-derived energy metrics. The method achieves a 79% classification accuracy.


<details>
  <summary>Details</summary>
Motivation: The lack of objective neurophysiological biomarkers for Autism Spectrum Disorder (ASD) motivates the research.

Method: The study proposes a Hamiltonian-inspired model integrating fALFF and functional connectivity to analyze resting-state fMRI data from adult males with ASD and matched controls. It uses network energy metrics, motif analysis, and classification algorithms.

Result: Findings show lower energy in ASD networks compared to null models, heightened energy in certain brain networks, and correlations with ASD severity scores. The proposed classifier achieved 79% accuracy.

Conclusion: The coevolutionary energy model offers an interpretable framework for identifying network disruptions in ASD and delivers promising diagnostic applications.

Abstract: Autism spectrum disorder (ASD) is a neurodevelopmental condition
characterized by deficits in social communication and repetitive behaviors;
however, objective neurophysiological biomarkers remain lacking. We propose a
coevolutionary balance paradigm that quantifies network level energy via a
Hamiltonian integrating regional activity measured by fractional amplitude of
low frequency fluctuations (fALFF) and resting state functional connectivity
(FC). Analysis of resting state fMRI data from 93 adult males with ASD and 93
matched controls revealed that empirical networks showed lower energy than 1000
topology preserving null models (paired t = -4.12, p less than or equal to
1e-4). Participants with ASD exhibited more negative whole brain energy (t =
-3.239, p = 0.0015), driven by increased agreement links and reduced imbalanced
same motifs. Subnetwork analysis indicated greater energy in the Default Mode
Network after false discovery rate correction (p less than 0.016) and enhanced
energy between the Default Mode, Salience and Dorsal Attention networks (p less
than 0.032). Energy metrics and inter network connectivity correlated with
Autism Diagnostic Interview Revised and Autism Diagnostic Observation Schedule
severity scores (absolute correlation greater than or equal to 0.29, p less
than 0.02). A k nearest neighbors classifier using nine principal features
including motif proportions, global node link alignment, inter network fALFF
weighted and FC strengths, subnetwork magnetization and pairwise energy
achieved an accuracy of 79 percent with balanced sensitivity and specificity.
These results demonstrate that coevolutionary energy detects interpretable
network disruptions and establishes a robust framework for ASD classification.

</details>


### [547] [Cellular Mechanisms of Phase Maintenance in a Pyloric Motif of a Central Pattern Generator](https://arxiv.org/abs/2507.09360)
*Gabrielle O'Brien,Adam L. Weaver,William H. Barnett,Dmitry A. Kozhanov,Gennady S. Cymbalyuk*

Main category: q-bio.NC

TL;DR: The paper investigates how rhythmic neural patterns in crustacean neural networks are maintained over diverse time periods using neuromodulation mechanisms and biophysical models to preserve phase relations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how neural networks preserve consistent rhythmic behaviors across varying time periods, focusing on crustacean pyloric CPGs and the role of neuromodulation.

Method: A biophysical model is developed to simulate a reduced pyloric network motif with interconnected neurons. Cellular mechanisms are analyzed using scenarios like Shilnikov catastrophe and bifurcation dynamics.

Result: Three mechanisms were identified for burst duration, interburst interval, and latency to spiking, with behaviors governed by specific bifurcations such as blue-sky catastrophe and saddle-node scenarios.

Conclusion: Neural phase relations can be preserved across wide temporal ranges through coordinated variations in neuronal properties governed by bifurcation mechanisms.

Abstract: In many neural networks, patterns controlling rhythmic behaviors are
maintained across a wide range of periods. In the crustacean pyloric central
pattern generator (CPG), a constant bursting pattern is preserved over a
three-to-fivefold range of periods. We idescribe how neuromodulation could
adjust neuronal properties to preserve phase relations as the period changes.
We developed a biophysical model implementing a reduced pyloric network motif,
which has a bursting neuron and two follower neurons interconnected through
inhibitory synaptic coupling. We described cellular mechanisms supporting phase
maintenance and investigated possible coordination between these mechanisms in
four dynamically distinct ensembles of a pyloric CPG producing a triphasic
pattern. The coordinated variation of the voltages of half-activation for
potassium (VK2) and hyperpolarization-activated (Vh) currents provides a family
of three mechanisms for control of burst duration, interburst interval, and
latency to spiking. The mechanisms are determined by the Cornerstone
bifurcation, one of the Shilnikov blue sky catastrophe scenarios. In Mechanism
1, in a bursting neuron, the burst duration increases as VK2 nears a blue-sky
catastrophe bifurcation, while the interburst interval grows as Vh approaches a
saddle-node on an invariant circle bifurcation. In Mechanism 2, a silent neuron
responds with a single burst to short input; the burst duration grows as VK2
approaches a saddle-node bifurcation for periodic orbits. In Mechanism 3, a
spiking neuron responds with a pause to short input; the pause duration grows
as Vh nears a saddle-node bifurcation for stationary states. In all three
mechanisms, the measured quantities grow without bound as the bifurcation
parameter nears its critical value, consistent with an inverse-square-root law.

</details>


### [548] [Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding](https://arxiv.org/abs/2507.09513)
*Yanchen Wang,Han Yu,Ari Blau,Yizi Zhang,The International Brain Laboratory,Liam Paninski,Cole Hurwitz,Matt Whiteway*

Main category: q-bio.NC

TL;DR: BEAST is introduced as a self-supervised framework for analyzing behavior videos with vision transformers, aiming to address challenges in neuro-behavioral research while reducing dependence on labeled data.


<details>
  <summary>Details</summary>
Motivation: Understanding brain functionality necessitates analyzing behavior, but current video-based approaches demand significant labeled data, creating scalability challenges.

Method: BEAST uses masked autoencoding and temporal contrastive learning to pretrain vision transformers on unlabeled video data, enabling diverse neuro-behavioral analyses.

Result: BEAST enhances behavioral feature extraction tied to neural activity, poses estimation, and action segmentation across species and settings, outperforming existing models.

Conclusion: The framework provides a scalable solution for behavioral analysis, boosting performance in tasks where labeled data is limited and establishing a versatile model backbone for neuroscience research.

Abstract: The brain can only be fully understood through the lens of the behavior it
generates -- a guiding principle in modern neuroscience research that
nevertheless presents significant technical challenges. Many studies capture
behavior with cameras, but video analysis approaches typically rely on
specialized models requiring extensive labeled data. We address this limitation
with BEAST (BEhavioral Analysis via Self-supervised pretraining of
Transformers), a novel and scalable framework that pretrains
experiment-specific vision transformers for diverse neuro-behavior analyses.
BEAST combines masked autoencoding with temporal contrastive learning to
effectively leverage unlabeled video data. Through comprehensive evaluation
across multiple species, we demonstrate improved performance in three critical
neuro-behavioral tasks: extracting behavioral features that correlate with
neural activity, and pose estimation and action segmentation in both the
single- and multi-animal settings. Our method establishes a powerful and
versatile backbone model that accelerates behavioral analysis in scenarios
where labeled data remains scarce.

</details>


### [549] [Dissociating Cognitive Load and Stress Responses Using Single-Channel EEG: Behavioral and Neural Correlates of Anxiety Across Cognitive States](https://arxiv.org/abs/2507.10093)
*Neta Batya Maimon,Lior Molcho,Talya Zaimer,Ofir Chibotero,Nathan Intrator,Eliezer Yahalom*

Main category: q-bio.NC

TL;DR: This paper evaluates the use of a portable high-density EEG system to identify distinct neural markers for cognitive load and stress. Findings suggest the system is effective in distinguishing these states.


<details>
  <summary>Details</summary>
Motivation: The study aims to develop scalable tools for assessing mental states by identifying neural markers linked to stress and cognitive load.

Method: The study involved 68 participants who completed resting-state EEG recordings, auditory tasks, and exposure to startle stimuli. Frequency bands and machine-learning-derived features were analyzed.

Result: Theta and VC9 increased under cognitive load, while Gamma and A0 were elevated by startle stimuli. Other features correlated with cognitive effort, worry, and self-reported emotional states.

Conclusion: The findings show that portable EEG can reliably identify biomarkers for stress and cognitive load and support applications in clinical, educational, and occupational contexts.

Abstract: Identifying neural markers of stress and cognitive load is key to developing
scalable tools for mental state assessment. This study evaluated whether a
single-channel high-density EEG (hdrEEG) system could dissociate cognitive and
stress-related activity during a brief auditory task-based protocol.
Sixty-eight healthy adults completed resting state recordings, cognitively
demanding auditory tasks, and exposure to unpredictable literalized startle
stimuli. Participants also rated their stress and anxiety using a modified
State-Trait Anxiety Inventory (STAI). EEG analysis focused on frequency bands
(Theta, Gamma, Delta) and machine-learning-derived features (A0, ST4, VC9, T2).
A double dissociation emerged: Theta and VC9 increased under cognitive load but
not startle, supporting their sensitivity to executive function. In contrast,
Gamma and A0 were elevated by the startle stimulus, consistent with stress
reactivity. ST4 tracked cognitive effort and worry, while T2 negatively
correlated with self-reported calmness, indicating relevance to emotional
regulation. These results demonstrate that a short, uniform assessment using
portable EEG can yield multiple reliable biomarkers of cognitive and affective
states. The findings have implications for clinical, occupational, and
educational settings, and may inform future neurofeedback protocols targeting
simultaneous regulation of attention and stress.

</details>


### [550] [The Evaluation of Breathing 5:5 effect on resilience, stress and balance center measured by Single-Channel EEG](https://arxiv.org/abs/2507.10175)
*Eliezer Yahalom,Neta Maimon,Lior Molcho,Talya Zeimer,Ofir Chibotero,Nathan Intrator*

Main category: q-bio.NC

TL;DR: This study demonstrates that guided slow-paced breathing (5:5 rhythm) improves stress-related neural markers and subjective anxiety levels.


<details>
  <summary>Details</summary>
Motivation: To explore the neurophysiological and subjective effects of slow-paced breathing as a scalable method to reduce anxiety and enhance emotional regulation.

Method: Thirty-eight participants were randomized into intervention and control groups, with EEG biomarkers and subjective anxiety assessed pre- and post-intervention. The intervention group practiced 5:5 breathing both in sessions and daily.

Result: Immediate reductions in Gamma power (mental load), and long-term increases in Alpha and Delta power, alongside reduced ST4 activity, correlated with improvements in emotional regulation and subjective calmness.

Conclusion: Guided slow-paced breathing generates both acute and sustained benefits on stress-related neural and cognitive markers, supporting its efficacy in promoting emotional self-regulation via scalable biofeedback methods.

Abstract: Slow-paced breathing is a promising intervention for reducing anxiety and
enhancing emotional regulation through its effects on autonomic and central
nervous system function. This study examined the neurophysiological and
subjective effects of a 5:5 breathing protocol on stress-related EEG biomarkers
using a mobile single-channel EEG system. Thirty-eight healthy adults were
randomly assigned to either an intervention group (n = 20), which completed two
sessions spaced two weeks apart with daily breathing practice, or a control
group (n = 18), which completed one session. In each session, participants
underwent an auditory EEG assessment with resting, mental load, and startle
conditions. The intervention group also completed a guided breathing session
during the first visit and practiced the technique between sessions. EEG
biomarkers (ST4, Alpha, Delta, Gamma, VC0) and subjective anxiety levels (STAI)
were assessed before and after the intervention. A significant reduction in
Gamma power was observed in the intervention group immediately following the
first breathing session during mental load (p = .002), indicating acute stress
reduction. Across sessions, long-term breathing practice led to increased Alpha
and Delta power and reduced ST4 activity, suggesting cumulative improvements in
emotional regulation and cognitive efficiency. Correlational analyses revealed
that changes in VC0 and Alpha were significantly associated with subjective
reports of tension, focus difficulty, and calmness. Guided slow-paced breathing
at a 5:5 rhythm produces both immediate and sustained effects on neural markers
of stress and cognition, with corresponding improvements in subjective anxiety.
These findings support EEG-based monitoring as a scalable method for evaluating
breath-based interventions and promoting real-time emotional self-regulation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [551] [Physics-informed machine learning: A mathematical framework with applications to time series forecasting](https://arxiv.org/abs/2507.08906)
*Nathan Doumèche*

Main category: stat.ML

TL;DR: The paper explores the statistical properties of Physics-Informed Machine Learning (PIML) methods and their industrial applications, including using physical constraints in forecasting energy signals during atypical periods.


<details>
  <summary>Details</summary>
Motivation: To integrate physical knowledge into machine learning models to enhance their effectiveness and reliability. The paper aims to deepen understanding of PIML methods and apply them to industry-relevant forecasting challenges.

Method: Analyzes statistical properties of PIML methods like Physics-Informed Neural Networks (PINNs), reframes PIML problems using kernel methods for efficient GPU implementation, and explores applications in load forecasting using physical constraints.

Result: Presented insights into PINNs' approximation, consistency, overfitting, and convergence. Developed novel algorithms with kernel methods. Achieved forecasting improvements in energy signals and mobility-impact analysis.

Conclusion: PIML methods improved understanding and efficiency through kernel methods and GPU optimization. Industrial applications demonstrated effective use of physical constraints, benefiting forecasting in energy and tourism domains.

Abstract: Physics-informed machine learning (PIML) is an emerging framework that
integrates physical knowledge into machine learning models. This physical prior
often takes the form of a partial differential equation (PDE) system that the
regression function must satisfy. In the first part of this dissertation, we
analyze the statistical properties of PIML methods. In particular, we study the
properties of physics-informed neural networks (PINNs) in terms of
approximation, consistency, overfitting, and convergence. We then show how PIML
problems can be framed as kernel methods, making it possible to apply the tools
of kernel ridge regression to better understand their behavior. In addition, we
use this kernel formulation to develop novel physics-informed algorithms and
implement them efficiently on GPUs. The second part explores industrial
applications in forecasting energy signals during atypical periods. We present
results from the Smarter Mobility challenge on electric vehicle charging
occupancy and examine the impact of mobility on electricity demand. Finally, we
introduce a physics-constrained framework for designing and enforcing
constraints in time series, applying it to load forecasting and tourism
forecasting in various countries.

</details>


### [552] [The Bayesian Approach to Continual Learning: An Overview](https://arxiv.org/abs/2507.08922)
*Tameem Adel*

Main category: stat.ML

TL;DR: This paper surveys Bayesian continual learning for handling sequential data updates without losing past knowledge, offering a taxonomy and analysis of state-of-the-art methods, and exploring links to developmental psychology.


<details>
  <summary>Details</summary>
Motivation: To address challenges in real-world applicability of deep models with a framework that mimics human cognitive processes and maintains prior knowledge during sequential learning.

Method: Classifies and examines Bayesian continual learning algorithms, discusses links to related fields, and provides analogies with developmental psychology.

Result: Provides a comprehensive taxonomy of Bayesian continual learning methods while analyzing existing algorithms and their performance on task-incremental and class-incremental learning problems.

Conclusion: Bayesian continual learning has significant potential for real-world applications, though challenges remain, with promising directions for future research highlighted.

Abstract: Continual learning is an online paradigm where a learner continually
accumulates knowledge from different tasks encountered over sequential time
steps. Importantly, the learner is required to extend and update its knowledge
without forgetting about the learning experience acquired from the past, and
while avoiding the need to retrain from scratch. Given its sequential nature
and its resemblance to the way humans think, continual learning offers an
opportunity to address several challenges which currently stand in the way of
widening the range of applicability of deep models to further real-world
problems. The continual need to update the learner with data arriving
sequentially strikes inherent congruence between continual learning and
Bayesian inference which provides a principal platform to keep updating the
prior beliefs of a model given new data, without completely forgetting the
knowledge acquired from the old data. This survey inspects different settings
of Bayesian continual learning, namely task-incremental learning and
class-incremental learning. We begin by discussing definitions of continual
learning along with its Bayesian setting, as well as the links with related
fields, such as domain adaptation, transfer learning and meta-learning.
Afterwards, we introduce a taxonomy offering a comprehensive categorization of
algorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we
analyze the state-of-the-art while zooming in on some of the most prominent
Bayesian continual learning algorithms to date. Furthermore, we shed some light
on links between continual learning and developmental psychology, and
correspondingly introduce analogies between both fields. We follow that with a
discussion of current challenges, and finally conclude with potential areas for
future research on Bayesian continual learning.

</details>


### [553] [Fixed-Confidence Multiple Change Point Identification under Bandit Feedback](https://arxiv.org/abs/2507.08994)
*Joseph Lazzaro,Ciara Pike-Burke*

Main category: stat.ML

TL;DR: The paper introduces a problem and method for identifying abrupt changes in piecewise constant functions using bandit feedback. It provides theoretical insights, such as optimal sampling strategies, proposes an efficient algorithm, and presents experimental validation.


<details>
  <summary>Details</summary>
Motivation: Rapid identification of change points in piecewise constant functions is vital for applications across various domains like chemistry and manufacturing.

Method: The proposed method uses bandit feedback to sequentially query points, focuses sampling near change points based on magnitude, and introduces a variant of Track-and-Stop algorithm. It provides instance-dependent complexity bounds.

Result: Theoretical findings indicate asymptotic optimality of the proposed method in multiple regimes. Experimental results validate its computational efficiency in synthetic settings.

Conclusion: The study successfully devises an optimal and efficient method for change point identification in piecewise constant functions, aligning theoretical insights with practical validation.

Abstract: Piecewise constant functions describe a variety of real-world phenomena in
domains ranging from chemistry to manufacturing. In practice, it is often
required to confidently identify the locations of the abrupt changes in these
functions as quickly as possible. For this, we introduce a fixed-confidence
piecewise constant bandit problem. Here, we sequentially query points in the
domain and receive noisy evaluations of the function under bandit feedback. We
provide instance-dependent lower bounds for the complexity of change point
identification in this problem. These lower bounds illustrate that an optimal
method should focus its sampling efforts adjacent to each of the change points,
and the number of samples around each change point should be inversely
proportional to the magnitude of the change. Building on this, we devise a
simple and computationally efficient variant of Track-and-Stop and prove that
it is asymptotically optimal in many regimes. We support our theoretical
findings with experimental results in synthetic environments demonstrating the
efficiency of our method.

</details>


### [554] [Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization](https://arxiv.org/abs/2507.09093)
*Aleksandar Armacki,Dragana Bajovic,Dusan Jakovetic,Soummya Kar*

Main category: stat.ML

TL;DR: This paper investigates convergence of SGD-type methods in non-convex optimization under heavy-tailed noise by introducing nonlinear SGD (N-SGD) and two novel estimators to address non-symmetric noise.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing optimization techniques that are hampered by heavy-tailed and non-symmetric noise in stochastic gradients, which affect convergence rates and tail behaviors.

Method: The paper introduces N-SGD, complemented by two novel noise-symmetrization techniques: Symmetrized Gradient Estimator (SGE) and Mini-batch SGE (MSGE). These approaches allow for improved handling of noise with unbounded moments.

Result: N-SGD achieves $
WidETilde{\mathcal{O}}(t^{-1/2})$ convergence and exponentially decaying tails. With SGE and MSGE, the methods extend to handle non-symmetric noise while maintaining strong convergence and optimal or close-to-optimal oracle complexities.

Conclusion: The proposed methods outperform prior approaches under relaxed noise assumptions, offering a unified framework, improved analysis, and better handling of both symmetric and non-symmetric noise with heavy tails.

Abstract: We study convergence in high-probability of SGD-type methods in non-convex
optimization and the presence of heavy-tailed noise. To combat the heavy-tailed
noise, a general black-box nonlinear framework is considered, subsuming
nonlinearities like sign, clipping, normalization and their smooth
counterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the
rate $\widetilde{\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments
and a symmetric probability density function (PDF). Crucially, N-SGD has
exponentially decaying tails, matching the performance of linear SGD under
light-tailed noise. To handle non-symmetric noise, we propose two novel
estimators, based on the idea of noise symmetrization. The first, dubbed
Symmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any
reference point is available at the start of training, while the second, dubbed
Mini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient.
Combined with the nonlinear framework, we get N-SGE and N-MSGE methods,
respectively, both achieving the same convergence rate and exponentially
decaying tails as N-SGD, while allowing for non-symmetric noise with unbounded
moments and PDF satisfying a mild technical condition, with N-MSGE additionally
requiring bounded noise moment of order $p \in (1,2]$. Compared to works
assuming noise with bounded $p$-th moment, our results: 1) are based on a novel
symmetrization approach; 2) provide a unified framework and relaxed moment
conditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly
better than existing works when $p < 2$, while the complexity of N-MSGE is
close to existing works. Compared to works assuming symmetric noise with
unbounded moments, we: 1) provide a sharper analysis and improved rates; 2)
facilitate state-dependent symmetric noise; 3) extend the strong guarantees to
non-symmetric noise.

</details>


### [555] [CoVAE: Consistency Training of Variational Autoencoders](https://arxiv.org/abs/2507.09103)
*Gianluigi Silvestri,Luca Ambrogioni*

Main category: stat.ML

TL;DR: This paper introduces CoVAE, a single-stage generative autoencoding framework that combines VAEs with consistency training, simplifying the generative modeling process while enhancing sample quality.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of the two-stage generative modeling approach, such as computational overhead and increased sampling times, by creating a single-stage model with improved performance.

Method: CoVAE integrates VAE architecture with techniques from consistency models. It uses a progressive series of noisy latent representations controlled by a time-dependent parameter for KL loss scaling and trains the decoder with a combination of consistency and variational regularization losses.

Result: CoVAE generates high-quality samples in a few steps, significantly outperforming traditional VAEs and other single-stage methods without requiring a learned prior.

Conclusion: CoVAE serves as a unified, efficient, and high-performing framework for generative modeling, providing a new pathway for one-step generative high-quality autoencoding.

Abstract: Current state-of-the-art generative approaches frequently rely on a two-stage
training procedure, where an autoencoder (often a VAE) first performs
dimensionality reduction, followed by training a generative model on the
learned latent space. While effective, this introduces computational overhead
and increased sampling times. We challenge this paradigm by proposing
Consistency Training of Variational AutoEncoders (CoVAE), a novel single-stage
generative autoencoding framework that adopts techniques from consistency
models to train a VAE architecture. The CoVAE encoder learns a progressive
series of latent representations with increasing encoding noise levels,
mirroring the forward processes of diffusion and flow matching models. This
sequence of representations is regulated by a time dependent $\beta$ parameter
that scales the KL loss. The decoder is trained using a consistency loss with
variational regularization, which reduces to a conventional VAE loss at the
earliest latent time. We show that CoVAE can generate high-quality samples in
one or few steps without the use of a learned prior, significantly
outperforming equivalent VAEs and other single-stage VAEs methods. Our approach
provides a unified framework for autoencoding and diffusion-style generative
modeling and provides a viable route for one-step generative high-performance
autoencoding. Our code is publicly available at
https://github.com/gisilvs/covae.

</details>


### [556] [A Generalization Theory for Zero-Shot Prediction](https://arxiv.org/abs/2507.09128)
*Ronak Mehta,Zaid Harchaoui*

Main category: stat.ML

TL;DR: The paper theoretically analyzes zero-shot prediction using pre-trained models for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to theoretically understand how zero-shot prediction works and its generalization ability.

Method: A theoretical framework is developed to identify target quantities and conditional independence relationships in zero-shot prediction.

Result: Key insights were provided on the target quantities and conditions enabling zero-shot generalization.

Conclusion: The paper offers a foundational understanding of zero-shot prediction that may inform advancements in its application and theory.

Abstract: A modern paradigm for generalization in machine learning and AI consists of
pre-training a task-agnostic foundation model, generally obtained using
self-supervised and multimodal contrastive learning. The resulting
representations can be used for prediction on a downstream task for which no
labeled data is available. We present a theoretical framework to better
understand this approach, called zero-shot prediction. We identify the target
quantities that zero-shot prediction aims to learn, or learns in passing, and
the key conditional independence relationships that enable its generalization
ability.

</details>


### [557] [A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation](https://arxiv.org/abs/2507.09148)
*Alberto Del Pia,Dekun Zhou*

Main category: stat.ML

TL;DR: The paper introduces a randomized algorithm for Sparse Principal Component Analysis (SPCA) based on SDP relaxation, achieving a bounded approximation ratio under specific assumptions, which is validated numerically.


<details>
  <summary>Details</summary>
Motivation: SPCA is a vital but computationally hard technique for dimensionality reduction, requiring efficient approximation methods.

Method: A randomized approximation algorithm is developed using the basic SDP relaxation. The algorithm's performance is analyzed under specific assumptions regarding SDP solutions.

Result: The algorithm achieves an approximation ratio related to sparsity with high probability and performs near-optimally in a generalized spiked Wishart model. Validation is provided using real-world datasets.

Conclusion: The proposed SPCA algorithm is effective, with bounded average approximation ratios under favorable conditions and practical efficacy demonstrated on real data.

Abstract: Sparse Principal Component Analysis (SPCA) is a fundamental technique for
dimensionality reduction, and is NP-hard. In this paper, we introduce a
randomized approximation algorithm for SPCA, which is based on the basic SDP
relaxation. Our algorithm has an approximation ratio of at most the sparsity
constant with high probability, if called enough times. Under a technical
assumption, which is consistently satisfied in our numerical tests, the average
approximation ratio is also bounded by $\mathcal{O}(\log{d})$, where $d$ is the
number of features. We show that this technical assumption is satisfied if the
SDP solution is low-rank, or has exponentially decaying eigenvalues. We then
present a broad class of instances for which this technical assumption holds.
We also demonstrate that in a covariance model, which generalizes the spiked
Wishart model, our proposed algorithm achieves a near-optimal approximation
ratio. We demonstrate the efficacy of our algorithm through numerical results
on real-world datasets.

</details>


### [558] [Uncovering symmetric and asymmetric species associations from community and environmental data](https://arxiv.org/abs/2507.09317)
*Sara Si-Moussi,Esther Galbrun,Mickael Hedde,Giovanni Poggiato,Matthias Rohr,Wilfried Thuiller*

Main category: stat.ML

TL;DR: The paper introduces a machine-learning framework to analyze spatial species associations by modeling both symmetric and asymmetric biotic interactions using species and environmental data. It outperforms existing methods and is versatile in application.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation of existing models that assume symmetric interactions when studying species associations from spatial data, despite the inherently asymmetric nature of biotic interactions.

Method: The proposed framework models directed influences between species using species-specific latent embeddings to parameterize the effects of source species and responses of target species. It integrates these associations into a multi-species conditional generative model to handle both environmental drivers and biotic interactions.

Result: Using both simulated and real-world datasets, the framework is shown to accurately recover both symmetric and asymmetric associations between species. It outperforms existing approaches like joint species distribution models and probabilistic graphical models in retrieving these interactions.

Conclusion: The paper concludes that the presented framework is intuitive, modular, and capable of broadly applicable across various taxonomic groups, making it a versatile tool for studying species associations.

Abstract: There is no much doubt that biotic interactions shape community assembly and
ultimately the spatial co-variations between species. There is a hope that the
signal of these biotic interactions can be observed and retrieved by
investigating the spatial associations between species while accounting for the
direct effects of the environment. By definition, biotic interactions can be
both symmetric and asymmetric. Yet, most models that attempt to retrieve
species associations from co-occurrence or co-abundance data internally assume
symmetric relationships between species. Here, we propose and validate a
machine-learning framework able to retrieve bidirectional associations by
analyzing species community and environmental data.
  Our framework (1) models pairwise species associations as directed influences
from a source to a target species, parameterized with two species-specific
latent embeddings: the effect of the source species on the community, and the
response of the target species to the community; and (2) jointly fits these
associations within a multi-species conditional generative model with different
modes of interactions between environmental drivers and biotic associations.
Using both simulated and empirical data, we demonstrate the ability of our
framework to recover known asymmetric and symmetric associations and highlight
the properties of the learned association networks. By comparing our approach
to other existing models such as joint species distribution models and
probabilistic graphical models, we show its superior capacity at retrieving
symmetric and asymmetric interactions. The framework is intuitive, modular and
broadly applicable across various taxonomic groups.

</details>


### [559] [An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects](https://arxiv.org/abs/2507.09494)
*Albert Chiu*

Main category: stat.ML

TL;DR: The paper introduces an algorithm to identify interpretable rule-based subgroups with elevated treatment effects.


<details>
  <summary>Details</summary>
Motivation: Current models for estimating CATE often result in results that are high-dimensional or unintelligible, creating demand for simpler, interpretable solutions.

Method: The method uses rule sets to summarize complex CATE models, guided by an objective function balancing subgroup size and effect size, producing Pareto optimal results.

Result: The proposed framework provides interpretable subgroups and is validated through both simulations and empirical examples.

Conclusion: This approach enhances decision-making and policy implementation by offering interpretable insights into treatment effects.

Abstract: We introduce an algorithm for identifying interpretable subgroups with
elevated treatment effects, given an estimate of individual or conditional
average treatment effects (CATE). Subgroups are characterized by ``rule sets''
-- easy-to-understand statements of the form (Condition A AND Condition B) OR
(Condition C) -- which can capture high-order interactions while retaining
interpretability. Our method complements existing approaches for estimating the
CATE, which often produce high dimensional and uninterpretable results, by
summarizing and extracting critical information from fitted models to aid
decision making, policy implementation, and scientific understanding. We
propose an objective function that trades-off subgroup size and effect size,
and varying the hyperparameter that controls this trade-off results in a
``frontier'' of Pareto optimal rule sets, none of which dominates the others
across all criteria. Valid inference is achievable through sample splitting. We
demonstrate the utility and limitations of our method using simulated and
empirical examples.

</details>


### [560] [Signed Graph Learning: Algorithms and Theory](https://arxiv.org/abs/2507.09717)
*Abdullah Karaaslanli,Bisakh Banerjee,Tapabrata Maiti,Selin Aviyente*

Main category: stat.ML

TL;DR: The paper presents a method to learn signed graphs from smooth signed graph signals using the net Laplacian as a key operator. Optimization techniques and an efficient algorithm ensure its practical applicability.


<details>
  <summary>Details</summary>
Motivation: Understanding relationships in biological and social systems often requires capturing positive and negative interactions, which unsigned graphs cannot model effectively.

Method: The approach employs the net Laplacian to define smooth signed graph signals. A non-convex optimization problem minimizes total variation with respect to the net Laplacian, solved via ADMM and a fast algorithm.

Result: The algorithm demonstrates theoretical convergence, bounded estimation error, and promising empirical performance on simulated and gene regulatory network data.

Conclusion: The method advances signed graph learning, offering theoretical rigor, computational efficiency, and practical performance.

Abstract: Real-world data is often represented through the relationships between data
samples, forming a graph structure. In many applications, it is necessary to
learn this graph structure from the observed data. Current graph learning
research has primarily focused on unsigned graphs, which consist only of
positive edges. However, many biological and social systems are better
described by signed graphs that account for both positive and negative
interactions, capturing similarity and dissimilarity between samples. In this
paper, we develop a method for learning signed graphs from a set of smooth
signed graph signals. Specifically, we employ the net Laplacian as a graph
shift operator (GSO) to define smooth signed graph signals as the outputs of a
low-pass signed graph filter defined by the net Laplacian. The signed graph is
then learned by formulating a non-convex optimization problem where the total
variation of the observed signals is minimized with respect to the net
Laplacian. The proposed problem is solved using alternating direction method of
multipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration
complexity from quadratic to linear in the number of nodes is introduced.
Furthermore, theoretical proofs of convergence for the algorithm and a bound on
the estimation error of the learned net Laplacian as a function of sample size,
number of nodes, and graph topology are provided. Finally, the proposed method
is evaluated on simulated data and gene regulatory network inference problem
and compared to existing signed graph learning methods.

</details>


### [561] [Discovering Governing Equations in the Presence of Uncertainty](https://arxiv.org/abs/2507.09740)
*Ridwan Olabiyi,Han Hu,Ashif Iquebal*

Main category: stat.ML

TL;DR: This paper proposes a stochastic inverse physics-discovery (SIP) framework to identify governing equations of dynamical systems while accounting for system variability and measurement noise.


<details>
  <summary>Details</summary>
Motivation: Traditional discovery methods for dynamical systems struggle with pronounced variability and noisy, limited data, making accurate modeling and prediction challenging.

Method: The SIP framework treats unknown coefficients as random variables and infers their posterior distribution by minimizing the Kullback-Leibler divergence between the posterior samples' push-forward and empirical data distribution.

Result: SIP demonstrates superior performance on benchmark problems, improving coefficient root-mean-square error by an average of 82% compared to alternative methods, and yields interpretable models with quantified uncertainty.

Conclusion: SIP is a robust and data-efficient method that consistently discovers governing equations in noisy, variable, and data-limited environments, providing validated uncertainty estimates.

Abstract: In the study of complex dynamical systems, understanding and accurately
modeling the underlying physical processes is crucial for predicting system
behavior and designing effective interventions. Yet real-world systems exhibit
pronounced input (or system) variability and are observed through noisy,
limited data conditions that confound traditional discovery methods that assume
fixed-coefficient deterministic models. In this work, we theorize that
accounting for system variability together with measurement noise is the key to
consistently discover the governing equations underlying dynamical systems. As
such, we introduce a stochastic inverse physics-discovery (SIP) framework that
treats the unknown coefficients as random variables and infers their posterior
distribution by minimizing the Kullback-Leibler divergence between the
push-forward of the posterior samples and the empirical data distribution.
Benchmarks on four canonical problems -- the Lotka-Volterra predator-prey
system (multi- and single-trajectory), the historical Hudson Bay lynx-hare
data, the chaotic Lorenz attractor, and fluid infiltration in porous media
using low- and high-viscosity liquids -- show that SIP consistently identifies
the correct equations and lowers coefficient root-mean-square error by an
average of 82\% relative to the Sparse Identification of Nonlinear Dynamics
(SINDy) approach and its Bayesian variant. The resulting posterior
distributions yield 95\% credible intervals that closely track the observed
trajectories, providing interpretable models with quantified uncertainty. SIP
thus provides a robust, data-efficient approach for consistent physics
discovery in noisy, variable, and data-limited settings.

</details>


### [562] [Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization](https://arxiv.org/abs/2507.09828)
*Shion Takeno,Yu Inatsu,Masayuki Karasuyama,Ichiro Takeuchi*

Main category: stat.ML

TL;DR: This paper analyzes a randomized variant of expected improvement (EI) for Bayesian optimization and demonstrates its effectiveness through theoretical analysis and numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Current theoretical analyses of EI in Bayesian optimization are limited, despite its practical success across many applications.

Method: The paper proposes a randomized version of EI that relies on evaluating the maximum of the posterior sample path. Bayesian cumulative regret bounds are analyzed for cases where the black-box function follows a Gaussian process.

Result: It is shown that the proposed method achieves sublinear Bayesian cumulative regret bounds. Numerical experiments validate the effectiveness of the approach.

Conclusion: The randomized EI variant supported by posterior sampling can be used effectively for optimizing expensive-to-evaluate black-box functions with theoretical guarantees and empirical success.

Abstract: Bayesian optimization is a powerful tool for optimizing an
expensive-to-evaluate black-box function. In particular, the effectiveness of
expected improvement (EI) has been demonstrated in a wide range of
applications. However, theoretical analyses of EI are limited compared with
other theoretically established algorithms. This paper analyzes a randomized
variant of EI, which evaluates the EI from the maximum of the posterior sample
path. We show that this posterior sampling-based random EI achieves the
sublinear Bayesian cumulative regret bounds under the assumption that the
black-box function follows a Gaussian process. Finally, we demonstrate the
effectiveness of the proposed method through numerical experiments.

</details>


### [563] [Simulating Biases for Interpretable Fairness in Offline and Online Classifiers](https://arxiv.org/abs/2507.10154)
*Ricardo Inácio,Zafeiris Kokkinogenis,Vitor Cerqueira,Carlos Soares*

Main category: stat.ML

TL;DR: The paper introduces a framework for generating biased synthetic datasets to study how predictive models reinforce biases and presents a novel explainability technique using second-order Shapley values.


<details>
  <summary>Details</summary>
Motivation: Biases in training data can lead to unfair outcomes in predictive models, making it imperative to study bias embedding processes and develop mitigation strategies.

Method: The authors develop an agent-based model (ABM) to simulate a loan application process and generate synthetic datasets with controllable biases. They apply machine learning classifiers to analyze predictive outcomes and use a novel second-order Shapley values technique for explainability.

Result: The framework demonstrates how biased data embed unfairness into model predictions. Various mitigation strategies are tested at different pipeline stages (e.g., pre-processing, in-processing), validated through offline and online learning experiments.

Conclusion: This work emphasizes the importance of studying bias in predictive models and offers a useful framework and explainability tool to assess and mitigate unfairness in such systems.

Abstract: Predictive models often reinforce biases which were originally embedded in
their training data, through skewed decisions. In such cases, mitigation
methods are critical to ensure that, regardless of the prevailing disparities,
model outcomes are adjusted to be fair. To assess this, datasets could be
systematically generated with specific biases, to train machine learning
classifiers. Then, predictive outcomes could aid in the understanding of this
bias embedding process. Hence, an agent-based model (ABM), depicting a loan
application process that represents various systemic biases across two
demographic groups, was developed to produce synthetic datasets. Then, by
applying classifiers trained on them to predict loan outcomes, we can assess
how biased data leads to unfairness. This highlights a main contribution of
this work: a framework for synthetic dataset generation with controllable bias
injection. We also contribute with a novel explainability technique, which
shows how mitigations affect the way classifiers leverage data features, via
second-order Shapley values. In experiments, both offline and online learning
approaches are employed. Mitigations are applied at different stages of the
modelling pipeline, such as during pre-processing and in-processing.

</details>


### [564] [MF-GLaM: A multifidelity stochastic emulator using generalized lambda models](https://arxiv.org/abs/2507.10303)
*K. Giannoukou,X. Zhu,S. Marelli,B. Sudret*

Main category: stat.ML

TL;DR: This paper introduces multifidelity generalized lambda models (MF-GLaMs) to emulate the conditional response distribution of high-fidelity (HF) stochastic simulators using lower-fidelity (LF) data, achieving accuracy improvements and cost efficiency compared to single-fidelity methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of accurately emulating the conditional probability distributions generated by stochastic simulators, which traditional deterministic surrogate modeling techniques cannot handle effectively and are resource-intensive for high-fidelity simulators.

Method: The proposed method, MF-GLaMs, extends the generalized lambda model (GLaM) to a multifidelity setting, integrating lower-fidelity simulator data to predict the full conditional response distributions of HF stochastic simulators non-intrusively.

Result: The results demonstrate that MF-GLaMs can achieve improved emulation accuracy for stochastic simulators compared to single-fidelity models while significantly reducing computational costs in synthetic and earthquake simulation examples.

Conclusion: MF-GLaMs offer an effective and efficient solution for predicting the full conditional response distribution of high-fidelity stochastic simulators by leveraging lower-fidelity data, saving computational costs while maintaining or improving accuracy.

Abstract: Stochastic simulators exhibit intrinsic stochasticity due to unobservable,
uncontrollable, or unmodeled input variables, resulting in random outputs even
at fixed input conditions. Such simulators are common across various scientific
disciplines; however, emulating their entire conditional probability
distribution is challenging, as it is a task traditional deterministic
surrogate modeling techniques are not designed for. Additionally, accurately
characterizing the response distribution can require prohibitively large
datasets, especially for computationally expensive high-fidelity (HF)
simulators. When lower-fidelity (LF) stochastic simulators are available, they
can enhance limited HF information within a multifidelity surrogate modeling
(MFSM) framework. While MFSM techniques are well-established for deterministic
settings, constructing multifidelity emulators to predict the full conditional
response distribution of stochastic simulators remains a challenge. In this
paper, we propose multifidelity generalized lambda models (MF-GLaMs) to
efficiently emulate the conditional response distribution of HF stochastic
simulators by exploiting data from LF stochastic simulators. Our approach
builds upon the generalized lambda model (GLaM), which represents the
conditional distribution at each input by a flexible, four-parameter
generalized lambda distribution. MF-GLaMs are non-intrusive, requiring no
access to the internal stochasticity of the simulators nor multiple
replications of the same input values. We demonstrate the efficacy of MF-GLaM
through synthetic examples of increasing complexity and a realistic earthquake
application. Results show that MF-GLaMs can achieve improved accuracy at the
same cost as single-fidelity GLaMs, or comparable performance at significantly
reduced cost.

</details>


### [565] [Information Must Flow: Recursive Bootstrapping for Information Bottleneck in Optimal Transport](https://arxiv.org/abs/2507.10443)
*Xin Li*

Main category: stat.ML

TL;DR: The paper introduces the Context-Content Uncertainty Principle (CCUP), a framework for modeling cognition as the flow of information between context and content using recursive inference processes.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand how cognition operates as an inferential process that balances uncertainty in context and content while resolving informational bottlenecks.

Method: The framework employs Rao-Blackwellized variational entropy minimization and formalizes recursive entropy minimization through a Delta Convergence Theorem. This enables hierarchical inference and bootstrapping processes.

Result: The framework stabilizes perceptual schemas and motor plans, while temporal bootstrapping converts episodic traces into semantic knowledge. It also conceptualizes language as a system for externalizing content and aligning cognition across individuals.

Conclusion: The CCUP framework presents recursive inference as a foundational mechanism in both individual and collective intelligence, providing a structured model for cognitive adaptation and alignment.

Abstract: We present the Context-Content Uncertainty Principle (CCUP), a unified
framework that models cognition as the directed flow of information between
high-entropy context and low-entropy content. Inference emerges as a cycle of
bidirectional interactions, bottom-up contextual disambiguation paired with
top-down content reconstruction, which resolves the Information Bottleneck in
Optimal Transport (iBOT). Implemented via Rao-Blackwellized variational entropy
minimization, CCUP steers representations toward minimal joint uncertainty
while preserving inferential directionality. Local cycle completion underpins
temporal bootstrapping, chaining simulations to refine memory, and spatial
bootstrapping, enabling compositional hierarchical inference. We prove a Delta
Convergence Theorem showing that recursive entropy minimization yields
delta-like attractors in latent space, stabilizing perceptual schemas and motor
plans. Temporal bootstrapping through perception-action loops and sleep-wake
consolidation further transforms episodic traces into semantic knowledge.
Extending CCUP, each hierarchical level performs delta-seeded inference:
low-entropy content seeds diffuse outward along goal-constrained paths shaped
by top-down priors and external context, confining inference to task-relevant
manifolds and circumventing the curse of dimensionality. Building on this, we
propose that language emerges as a symbolic transport system, externalizing
latent content to synchronize inference cycles across individuals. Together,
these results establish iBOT as a foundational principle of information flow in
both individual cognition and collective intelligence, positioning recursive
inference as the structured conduit through which minds adapt, align, and
extend.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [566] [Phase transition of the Sinkhorn-Knopp algorithm](https://arxiv.org/abs/2507.09711)
*Kun He*

Main category: cs.DS

TL;DR: The paper investigates the Sinkhorn-Knopp algorithm for matrix scaling, addressing its empirical performance versus theoretical bounds, and identifies a computational phase transition at a density threshold of 1/2.


<details>
  <summary>Details</summary>
Motivation: To explain the strong empirical performance of the Sinkhorn-Knopp algorithm in contrast to its pseudopolynomial theoretical bound and to establish a tighter iteration count.

Method: The authors analyze the performance of the Sinkhorn-Knopp algorithm on normalized matrices with a density property, deriving upper bounds for certain densities and establishing tight lower bounds for iteration counts under the $\ell_2$-norm.

Result: They prove that the algorithm runs in $O(\log n - \log \varepsilon)$ iterations for matrices with density $\gamma > 1/2$ and identify $\widetilde{O}(n^2)$ runtime as optimal. Additionally, they prove a lower bound of $\widetilde{\Omega}(n^{1/2}/\varepsilon)$ iterations for matrices with density $\gamma < 1/2$.

Conclusion: The study finds a sharp phase transition in the Sinkhorn-Knopp algorithm's performance at the density threshold $\gamma = 1/2$, shedding light on its theoretical efficiency.

Abstract: The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has
been studied for over 60 years. In practice, the algorithm often yields
high-quality approximations within just a few iterations. Theoretically,
however, the best-known upper bound places it in the class of
pseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound
landscape remains largely unexplored. Two fundamental questions persist: what
accounts for the algorithm's strong empirical performance, and can a tight
bound on its iteration count be established?
  For an $n\times n$ matrix, its normalized version is obtained by dividing
each entry by its largest entry. We say that a normalized matrix has a density
$\gamma$ if there exists a constant $\rho > 0$ such that one row or column has
exactly $\lceil \gamma n \rceil$ entries with values at least $\rho$, and every
other row and column has at least $\lceil \gamma n \rceil$ such entries.
  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a
nearly doubly stochastic matrix in $O(\log n - \log \varepsilon)$ iterations
and $\widetilde{O}(n^2)$ time for all nonnegative square matrices whose
normalized version has a density $\gamma > 1/2$. Such matrices cover both the
algorithm's principal practical inputs and its typical theoretical regime, and
the $\widetilde{O}(n^2)$ runtime is optimal.
  For the lower bound, we establish a tight bound of
$\widetilde{\Omega}\left(n^{1/2}/\varepsilon\right)$ iterations for positive
matrices under the $\ell_2$-norm error measure. Moreover, for every $\gamma <
1/2$, there exists a matrix with density $\gamma$ for which the algorithm
requires $\Omega\left(n^{1/2}/\varepsilon\right)$ iterations.
  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp
algorithm at the density threshold $\gamma = 1/2$.

</details>


### [567] [Covering a Few Submodular Constraints and Applications](https://arxiv.org/abs/2507.09879)
*Tanvi Bajpai,Chandra Chekuri,Pooja Kulkarni*

Main category: cs.DS

TL;DR: The paper focuses on solving the problem of covering multiple submodular constraints when the number of constraints ($r$) is fixed, proposing efficient approximation algorithms and achieving results comparable to single constraint cases.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address computational challenges in covering multiple submodular constraints, particularly in practical applications where $r$ (number of constraints) is a fixed constant rather than a variable.

Method: The authors developed a randomized bi-criteria approximation algorithm for general submodular functions with guarantees on coverage and cost, and a specialized approximation for weighted coverage functions in deletion-closed set systems.

Result: The paper presents a near-optimal randomized approximation algorithm for fixed $r$, achieving coverage guarantees while keeping costs controlled. Additionally, it addresses specialized cases with weighted coverage functions for improved approximation ratios.

Conclusion: The results demonstrate that for fixed $r$, approximation performance approaches that of the simpler single-constraint ($r=1$) case, making the approach highly effective for such scenarios. Potential applications are highlighted and future work is anticipated.

Abstract: We consider the problem of covering multiple submodular constraints. Given a
finite ground set $N$, a cost function $c: N \rightarrow \mathbb{R}_+$, $r$
monotone submodular functions $f_1,f_2,\ldots,f_r$ over $N$ and requirements
$b_1,b_2,\ldots,b_r$ the goal is to find a minimum cost subset $S \subseteq N$
such that $f_i(S) \ge b_i$ for $1 \le i \le r$. When $r=1$ this is the
well-known Submodular Set Cover problem. Previous work
\cite{chekuri2022covering} considered the setting when $r$ is large and
developed bi-criteria approximation algorithms, and approximation algorithms
for the important special case when each $f_i$ is a weighted coverage function.
These are fairly general models and capture several concrete and interesting
problems as special cases. The approximation ratios for these problem are at
least $\Omega(\log r)$ which is unavoidable when $r$ is part of the input. In
this paper, motivated by some recent applications, we consider the problem when
$r$ is a \emph{fixed constant} and obtain two main results. For covering
multiple submodular constraints we obtain a randomized bi-criteria
approximation algorithm that for any given integer $\alpha \ge 1$ outputs a set
$S$ such that $f_i(S) \ge$ $(1-1/e^\alpha -\epsilon)b_i$ for each $i \in [r]$
and $\mathbb{E}[c(S)] \le (1+\epsilon)\alpha \cdot \sf{OPT}$. Second, when the
$f_i$ are weighted coverage functions from a deletion-closed set system we
obtain a $(1+\epsilon)$ $(\frac{e}{e-1})$ $(1+\beta)$-approximation where
$\beta$ is the approximation ratio for the underlying set cover instances via
the natural LP. These results show that one can obtain nearly as good an
approximation for any fixed $r$ as what one would achieve for $r=1$. We mention
some applications that follow easily from these general results and anticipate
more in the future.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [568] [ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching](https://arxiv.org/abs/2507.09318)
*Han Zhu,Wei Kang,Liyong Guo,Zengwei Yao,Fangjun Kuang,Weiji Zhuang,Zhaoqing Li,Zhifeng Han,Dong Zhang,Xin Zhang,Xingchen Song,Long Lin,Daniel Povey*

Main category: eess.AS

TL;DR: The paper presents ZipVoice-Dialog, a model for non-autoregressive spoken dialogue generation, overcoming limitations like slow inference and unstable results in previous models, along with the introduction of a 6.8k-hour OpenDialog dataset.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of realistic turn-taking and distinct speaker timbres in spoken dialogue generation, as well as issues with slow and unstable inference in existing autoregressive models.

Method: The paper introduces ZipVoice-Dialog, which incorporates 1) speaker-turn embeddings for better speaker turn-taking, 2) a curriculum learning strategy for stable speech-text alignment, and 3) techniques for stereo dialogue generation. They also created a large-scale open-source dataset, OpenDialog.

Result: ZipVoice-Dialog demonstrates superior performance over existing models in terms of intelligibility, turn-taking accuracy, speaker similarity, and inference speed, validated experimentally.

Conclusion: ZipVoice-Dialog provides an effective solution for spoken dialogue generation with significant improvements in quality and efficiency, supported by a novel benchmark dataset to foster further research in the field.

Abstract: Generating spoken dialogue is more challenging than monologue text-to-speech
(TTS) due to the need for realistic turn-taking and distinct speaker timbres.
Existing spoken dialogue generation models, being auto-regressive, suffer from
slow and unstable inference. To overcome these limitations, we introduce
ZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation
model built upon flow matching. Key designs include: 1) speaker-turn embeddings
for precise speaker turn-taking; 2) a curriculum learning strategy for stable
speech-text alignment; 3) specialized strategies to enable stereo dialogue
generation. Additionally, recognizing the lack of open-source large-scale
spoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue
dataset from in-the-wild speech data. Furthermore, we established a benchmark
to comprehensively evaluate various models. Experimental results demonstrate
that ZipVoice-Dialog achieves superior performance in intelligibility, speaker
turn-taking accuracy, speaker similarity, and inference speed. Our codes, model
checkpoints, demo samples, and the OpenDialog dataset are all publicly
available at https://github.com/k2-fsa/ZipVoice.

</details>


### [569] [Natural Language-based Assessment of L2 Oral Proficiency using LLMs](https://arxiv.org/abs/2507.10200)
*Stefano Bannò,Rao Ma,Mengjie Qian,Siyuan Tang,Kate Knill,Mark Gales*

Main category: eess.AS

TL;DR: The paper investigates using natural language-based assessment (NLA) to evaluate language proficiency through can-do descriptors applied by a large language model (LLM), Qwen 2.5 72B, achieving competitive performance in a zero-shot setting.


<details>
  <summary>Details</summary>
Motivation: To explore whether large language models can interpret and apply human-designed can-do descriptors for second language assessment effectively and compare their performance with other models.

Method: The study uses an open-source LLM, Qwen 2.5 72B, to assess responses from the S&I Corpus in a zero-shot setting. It benchmarks the model against fine-tuned speech LLMs and a specifically trained BERT-based model.

Result: The approach achieves competitive performance, surpassing the BERT-based model but not fine-tuned speech LLMs, showing strong generalizability and interpretability.

Conclusion: NLA is effective in mismatched task settings, adaptable across data types and languages, and offers transparency through its clear application of language descriptors.

Abstract: Natural language-based assessment (NLA) is an approach to second language
assessment that uses instructions - expressed in the form of can-do descriptors
- originally intended for human examiners, aiming to determine whether large
language models (LLMs) can interpret and apply them in ways comparable to human
assessment. In this work, we explore the use of such descriptors with an
open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available
S&I Corpus in a zero-shot setting. Our results show that this approach -
relying solely on textual information - achieves competitive performance: while
it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it
surpasses a BERT-based model trained specifically for this purpose. NLA proves
particularly effective in mismatched task settings, is generalisable to other
data types and languages, and offers greater interpretability, as it is
grounded in clearly explainable, widely applicable language descriptors.

</details>


### [570] [Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction](https://arxiv.org/abs/2507.09834)
*Shu-wen Yang,Byeonggeun Kim,Kuan-Po Huang,Qingming Tang,Huy Phan,Bo-Ru Lu,Harsha Sundar,Shalini Ghosh,Hung-yi Lee,Chieh-Chi Kao,Chao Wang*

Main category: eess.AS

TL;DR: This paper proposes a novel approach for audio generation by using a token-wise diffusion method instead of discrete tokens for autoregressive modeling, showing significant improvements over previous methods.


<details>
  <summary>Details</summary>
Motivation: To extend the success of autoregressive next-token prediction (used in large language models) to audio, addressing challenges posed by its continuous nature.

Method: The authors use token-wise diffusion to model the continuous distribution for the next audio token and introduce a masked next-token prediction task within the causal language model framework.

Result: The proposed method achieves 20-40% relative gains in Frechet Audio Distance (FAD) and Kullback-Leibler (KL) divergence over AudioGen, and delivers SOTA-level performance with fewer parameters.

Conclusion: The proposed continuous token modeling and masked prediction task present a significant advancement in efficient and high-performing audio generation techniques.

Abstract: Autoregressive next-token prediction with the Transformer decoder has become
a de facto standard in large language models (LLMs), achieving remarkable
success in Natural Language Processing (NLP) at scale. Extending this paradigm
to audio poses unique challenges due to its inherently continuous nature. We
research audio generation with a causal language model (LM) without discrete
tokens. We leverage token-wise diffusion to model the continuous distribution
of the next continuous-valued token. Our approach delivers significant
improvements over previous discrete solution, AudioGen, achieving 20% and 40%
relative gains on AudioCaps in Frechet Audio Distance (FAD) and
Kullback-Leibler (KL) divergence, respectively. Additionally, we propose a
novel masked next-token prediction task that incorporates masked prediction
into the causal LM framework. On AudioCaps, the innovation yields 41% and 33%
relative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)
models, respectively, and is on par with the state-of-the-art (SOTA) diffusion
models. Furthermore, we achieve these results with significantly fewer
parameters -- 193M for our Base and 462M for our Large models.

</details>


### [571] [Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization](https://arxiv.org/abs/2507.09929)
*Haoyang Li,Nana Hou,Yuchen Hu,Jixun Yao,Sabato Marco Siniscalchi,Eng Siong Chng*

Main category: eess.AS

TL;DR: This paper introduces a novel speech enhancement method using Direct Preference Optimization (DPO) with perceptual feedback metrics.


<details>
  <summary>Details</summary>
Motivation: Existing speech enhancement methods using language models often misalign with human perception, compromising speech quality despite low prediction error.

Method: The method employs UTMOS, a neural MOS prediction model, as a proxy for human ratings to guide optimization towards perceptually preferred outputs through DPO.

Result: The proposed approach yields consistent improvements across various speech quality metrics with relative gains of up to 56% on 2020 Deep Noise Suppression Challenge test sets.

Conclusion: DPO applied to SE offers a perceptually aligned training mechanism, representing a promising advancement for LM-based speech enhancement techniques.

Abstract: This work investigates speech enhancement (SE) from the perspective of
language models (LMs). We propose a novel method that leverages Direct
Preference Optimization (DPO) to improve the perceptual quality of enhanced
speech. Using UTMOS, a neural MOS prediction model, as a proxy for human
ratings, our approach guides optimization toward perceptually preferred
outputs. This differs from existing LM-based SE methods that focus on
maximizing the likelihood of clean speech tokens, which may misalign with human
perception and degrade quality despite low prediction error. Experiments on the
2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO
to a pretrained LM-based SE model yields consistent improvements across various
speech quality metrics, with relative gains of up to 56%. To our knowledge,
this is the first application of DPO to SE and the first to incorporate proxy
perceptual feedback into LM-based SE training, pointing to a promising
direction for perceptually aligned SE.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [572] [Solving dynamic portfolio selection problems via score-based diffusion models](https://arxiv.org/abs/2507.09916)
*Ahmad Aghapour,Erhan Bayraktar,Fengyi Yuan*

Main category: q-fin.PM

TL;DR: The paper introduces a model-free approach using generative diffusion models for dynamic mean-variance portfolio selection, achieving superior results when compared to existing portfolio strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of model-free dynamic portfolio selection using limited real-world data in scenarios where the underlying model is unknown.

Method: Utilizing adaptive training and tailored sampling methods for time series to train a generative diffusion model, combined with policy gradient algorithms in a simulated environment.

Result: The generative environment-based algorithm outperformed benchmarks like Markowitz portfolio, equal-weight portfolio, and S&P 500 on both simulated and real-world datasets.

Conclusion: The proposed approach demonstrates robustness, accuracy, and practical efficacy in portfolio optimization, backed by theoretical guarantees and empirical validation.

Abstract: In this paper, we tackle the dynamic mean-variance portfolio selection
problem in a {\it model-free} manner, based on (generative) diffusion models.
We propose using data sampled from the real model $\mathcal P$ (which is
unknown) with limited size to train a generative model $\mathcal Q$ (from which
we can easily and adequately sample). With adaptive training and sampling
methods that are tailor-made for time series data, we obtain quantification
bounds between $\mathcal P$ and $\mathcal Q$ in terms of the adapted
Wasserstein metric $\mathcal A W_2$. Importantly, the proposed adapted sampling
method also facilitates {\it conditional sampling}. In the second part of this
paper, we provide the stability of the mean-variance portfolio optimization
problems in $\mathcal A W _2$. Then, combined with the error bounds and the
stability result, we propose a policy gradient algorithm based on the
generative environment, in which our innovative adapted sampling method
provides approximate scenario generators. We illustrate the performance of our
algorithm on both simulated and real data. For real data, the algorithm based
on the generative environment produces portfolios that beat several important
baselines, including the Markowitz portfolio, the equal weight (naive)
portfolio, and S\&P 500.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [573] [Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks](https://arxiv.org/abs/2507.09055)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.SI

TL;DR: The study introduces three novel centrality metrics that better identify misleading health information spreaders on online networks, achieving superior outcomes over traditional approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by health misinformation on online social networks during crises, such as its impact on public health and trust, and to overcome limitations of traditional centrality metrics in understanding misinformation flow.

Method: The research introduces and evaluates three novel centrality metrics—dynamic influence centrality (DIC), health misinformation vulnerability centrality (MVC), and propagation centrality (PC)—by comparing them with traditional metrics using datasets like FibVID and Monant Medical Misinformation.

Result: The new metrics identified unique influential nodes missed by traditional methods, improved misinformation reduction by 25% during baseline interventions, and were successfully validated on a second dataset, indicating generalizability.

Conclusion: A combination of traditional and novel centrality measures provides a robust framework for analyzing and mitigating health misinformation across diverse online social networks.

Abstract: The rapid spread of health misinformation on online social networks (OSNs)
during global crises such as the COVID-19 pandemic poses challenges to public
health, social stability, and institutional trust. Centrality metrics have long
been pivotal in understanding the dynamics of information flow, particularly in
the context of health misinformation. However, the increasing complexity and
dynamism of online networks, especially during crises, highlight the
limitations of these traditional approaches. This study introduces and compares
three novel centrality metrics: dynamic influence centrality (DIC), health
misinformation vulnerability centrality (MVC), and propagation centrality (PC).
These metrics incorporate temporal dynamics, susceptibility, and multilayered
network interactions. Using the FibVID dataset, we compared traditional and
novel metrics to identify influential nodes, propagation pathways, and
misinformation influencers. Traditional metrics identified 29 influential
nodes, while the new metrics uncovered 24 unique nodes, resulting in 42
combined nodes, an increase of 44.83%. Baseline interventions reduced health
misinformation by 50%, while incorporating the new metrics increased this to
62.5%, an improvement of 25%. To evaluate the broader applicability of the
proposed metrics, we validated our framework on a second dataset, Monant
Medical Misinformation, which covers a diverse range of health misinformation
discussions beyond COVID-19. The results confirmed that the advanced metrics
generalised successfully, identifying distinct influential actors not captured
by traditional methods. In general, the findings suggest that a combination of
traditional and novel centrality measures offers a more robust and
generalisable framework for understanding and mitigating the spread of health
misinformation in different online network contexts.

</details>


### [574] [Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)](https://arxiv.org/abs/2507.09149)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.SI

TL;DR: The study significantly enhances COVID-19 health misinformation detection on social media by integrating ELM-based features into a hybrid CNN-LSTM model, achieving exceptional accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: To combat the challenge of health misinformation during the COVID-19 pandemic and improve the accuracy of misinformation detection methods on social media.

Method: The study utilizes the Elaboration Likelihood Model (ELM) to incorporate features like text readability, sentiment polarity, and heuristic cues into a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model.

Result: The enhanced model reached high performance metrics including 97.37% accuracy, 99.80% ROC-AUC, and strong precision, recall, and F1-scores. A combined approach further boosted effectiveness.

Conclusion: ELM-based features offer significant improvements in misinformation detection, showcasing the applicability of psychological theories in machine learning algorithms to tackle health misinformation challenges.

Abstract: Health misinformation during the COVID-19 pandemic has significantly
challenged public health efforts globally. This study applies the Elaboration
Likelihood Model (ELM) to enhance misinformation detection on social media
using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory
(LSTM) model. The model aims to enhance the detection accuracy and reliability
of misinformation classification by integrating ELM-based features such as text
readability, sentiment polarity, and heuristic cues (e.g., punctuation
frequency). The enhanced model achieved an accuracy of 97.37%, precision of
96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined
model incorporating feature engineering further improved performance, achieving
a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of
99.80%. These findings highlight the value of ELM features in improving
detection performance, offering valuable contextual information. This study
demonstrates the practical application of psychological theories in developing
advanced machine learning algorithms to address health misinformation
effectively.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [575] [Sampling-Based Estimation of Jaccard Containment and Similarity](https://arxiv.org/abs/2507.10019)
*Pranav Joshi*

Main category: stat.CO

TL;DR: The paper introduces a binomial model to estimate set containment and similarity using random samples, without needing sketches or full data.


<details>
  <summary>Details</summary>
Motivation: To improve containment and similarity estimation between large sets when only partial or sampled data is accessible.

Method: Proposes a binomial model to predict sample overlap and analyzes its error bounds, statistical properties, and sample size requirements.

Result: The model is shown to outperform previous methods in accurately estimating set containment and similarity under limited sample conditions.

Conclusion: The binomial model is practical and effective for large-scale systems, offering better accuracy when only sampled data from sets is available.

Abstract: This paper addresses the problem of estimating the containment and similarity
between two sets using only random samples from each set, without relying on
sketches or full data access. The study introduces a binomial model for
predicting the overlap between samples, demonstrating that it is both accurate
and practical when sample sizes are small compared to the original sets. The
paper compares this model to previous approaches and shows that it provides
better estimates under the considered conditions. It also analyzes the
statistical properties of the estimator, including error bounds and sample size
requirements needed to achieve a desired level of accuracy and confidence. The
framework is extended to estimate set similarity, and the paper provides
guidance for applying these methods in large scale data systems where only
partial or sampled data is available.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [576] [SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity](https://arxiv.org/abs/2507.09453)
*Michał Jóźwik,Johan Pouwelse*

Main category: cs.CR

TL;DR: SmartphoneDemocracy is a secure, privacy-focused e-voting system employing smartphones, using technologies like EUDI Wallet, Zero-Knowledge Proofs, and TrustChain blockchain.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in privacy, security, and trust dependencies in electronic voting systems, often countering democratic principles.

Method: The paper introduces the SmartphoneDemocracy e-voting protocol combining EUDI Wallet for identity verification, Zero-Knowledge Proofs for privacy, and TrustChain blockchain for a serverless bulletin board.

Result: Security analysis and performance evaluations show the system is feasible for medium- to large-scale elections without excessive computational or network loads.

Conclusion: SmartphoneDemocracy offers a viable e-voting solution that emphasizes privacy, security, accessibility, and aligns with democratic principles.

Abstract: The digitization of democratic processes promises greater accessibility but
presents challenges in terms of security, privacy, and verifiability. Existing
electronic voting systems often rely on centralized architectures, creating
single points of failure and forcing too much trust in authorities, which
contradicts democratic principles. This research addresses the challenge of
creating a secure, private e-voting system with minimized trust dependencies
designed for the most versatile personal device: the smartphone. We introduce
SmartphoneDemocracy, a novel e-voting protocol that combines three key
technologies: the emerging European Digital Identity (EUDI) Wallet for
Sybil-resistant identity verification, Zero-Knowledge Proofs for
privacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a
resilient, serverless public bulletin board. Our protocol enables voters to
register and cast ballots anonymously and verifiably directly from their
smartphones. We provide a detailed protocol design, a security analysis against
a defined threat model, and a performance evaluation demonstrating that the
computational and network overhead is feasible for medium- to large-scale
elections. By developing and prototyping this system, we demonstrate a viable
path to empower citizens with a trustworthy, accessible, and user-controlled
digital voting experience.

</details>


### [577] [PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets](https://arxiv.org/abs/2507.09579)
*Marc Bara*

Main category: cs.CR

TL;DR: PromptChain utilizes Web3 technologies like IPFS and smart contracts to establish AI prompts as verifiable and monetizable digital assets, addressing issues like attribution and compensation.


<details>
  <summary>Details</summary>
Motivation: Centralized platforms lack mechanisms to ensure proper attribution, quality assurance, and fair compensation for AI prompt creators.

Method: The approach combines IPFS for immutable storage, smart contracts for governance, and token incentives for community curation, alongside metadata schemas and stake-weighted validation mechanisms.

Result: PromptChain achieves decentralized AI prompt ownership, version control, and monetization, while maintaining high efficiency, ownership security, and censorship resistance.

Conclusion: PromptChain sets a foundation for treating AI prompts as standalone assets in the Web3 ecosystem, improving human-AI collaboration through decentralized infrastructure.

Abstract: We present PromptChain, a decentralized Web3 architecture that establishes AI
prompts as first-class digital assets with verifiable ownership, version
control, and monetization capabilities. Current centralized platforms lack
mechanisms for proper attribution, quality assurance, or fair compensation for
prompt creators. PromptChain addresses these limitations through a novel
integration of IPFS for immutable storage, smart contracts for governance, and
token incentives for community curation. Our design includes: (1) a
comprehensive metadata schema for cross-model compatibility, (2) a
stake-weighted validation mechanism to align incentives, and (3) a token
economy that rewards contributors proportionally to their impact. The proposed
architecture demonstrates how decentralized systems could potentially match
centralized alternatives in efficiency while providing superior ownership
guarantees and censorship resistance through blockchain-anchored provenance
tracking. By decoupling prompts from specific AI models or outputs, this work
establishes the foundation for an open ecosystem of human-AI collaboration in
the Web3 era, representing the first systematic treatment of prompts as
standalone digital assets with dedicated decentralized infrastructure.

</details>


### [578] [Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives](https://arxiv.org/abs/2507.08853)
*Victoria L. Lemieux,Rosa Gil,Faith Molosiwa,Qihong Zhou,Binming Li,Roberto Garcia,Luis De La Torre Cubillo,Zehua Wang*

Main category: cs.CR

TL;DR: The paper introduces Clio-X, a privacy-first Web3 solution for AI-enabled archival management, highlighting user interest and barriers such as trust and governance.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks in AI practices, particularly in archival management, by exploring PETs and Web3 technologies.

Method: Development and evaluation of Clio-X prototype, leveraging PETs and Web3, followed by analysis using Diffusion of Innovation theory.

Result: Findings revealed user interest but also barriers related to trust, system transparency, costs, and governance.

Conclusion: A participatory design and decentralized governance model via Clio-X DAO are recommended for ethical AI deployment in heritage archives.

Abstract: As archives turn to artificial intelligence to manage growing volumes of
digital records, privacy risks inherent in current AI data practices raise
critical concerns about data sovereignty and ethical accountability. This paper
explores how privacy-enhancing technologies (PETs) and Web3 architectures can
support archives to preserve control over sensitive content while still being
able to make it available for access by researchers. We present Clio-X, a
decentralized, privacy-first Web3 digital solution designed to embed PETs into
archival workflows and support AI-enabled reference and access. Drawing on a
user evaluation of a medium-fidelity prototype, the study reveals both interest
in the potential of the solution and significant barriers to adoption related
to trust, system opacity, economic concerns, and governance. Using Rogers'
Diffusion of Innovation theory, we analyze the sociotechnical dimensions of
these barriers and propose a path forward centered on participatory design and
decentralized governance through a Clio-X Decentralized Autonomous
Organization. By integrating technical safeguards with community-based
oversight, Clio-X offers a novel model to ethically deploy AI in cultural
heritage contexts.

</details>


### [579] [Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System](https://arxiv.org/abs/2507.08864)
*Poushali Sengupta,Sabita Maharjan,frank Eliassen,Yan Zhang*

Main category: cs.CR

TL;DR: The paper introduces a privacy-preserving algorithm for location-based vehicular traffic management that balances privacy, utility, and fairness using differential privacy techniques.


<details>
  <summary>Details</summary>
Motivation: To address privacy leakage, demographic biases, and inequity in data analysis while managing vehicular traffic data.

Method: The algorithm integrates query-based data access, iterative shuffling, and calibrated noise injection with epsilon-differential privacy standards using the Laplace mechanism.

Result: When applied to vehicular traffic data in Norway, the algorithm ensured data utility for urban planning and fairness in the representation of various geographical areas.

Conclusion: The proposed algorithm effectively safeguards sensitive geographical data while supporting equitable and meaningful traffic management and urban planning.

Abstract: Location-based vehicular traffic management faces significant challenges in
protecting sensitive geographical data while maintaining utility for traffic
management and fairness across regions. Existing state-of-the-art solutions
often fail to meet the required level of protection against linkage attacks and
demographic biases, leading to privacy leakage and inequity in data analysis.
In this paper, we propose a novel algorithm designed to address the challenges
regarding the balance of privacy, utility, and fairness in location-based
vehicular traffic management systems. In this context, utility means providing
reliable and meaningful traffic information, while fairness ensures that all
regions and individuals are treated equitably in data use and decision-making.
Employing differential privacy techniques, we enhance data security by
integrating query-based data access with iterative shuffling and calibrated
noise injection, ensuring that sensitive geographical data remains protected.
We ensure adherence to epsilon-differential privacy standards by implementing
the Laplace mechanism. We implemented our algorithm on vehicular location-based
data from Norway, demonstrating its ability to maintain data utility for
traffic management and urban planning while ensuring fair representation of all
geographical areas without being overrepresented or underrepresented.
Additionally, we have created a heatmap of Norway based on our model,
illustrating the privatized and fair representation of the traffic conditions
across various cities. Our algorithm provides privacy in vehicular traffic

</details>


### [580] [Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models](https://arxiv.org/abs/2507.08878)
*Xinyu Huang,Leming Shen,Zijing Ma,Yuanqing Zheng*

Main category: cs.CR

TL;DR: HomeLLaMA proposes an on-device tailored language model for privacy-preserving smart home systems, with optional cloud interactions for enhanced user satisfaction.


<details>
  <summary>Details</summary>
Motivation: The paper addresses growing privacy concerns regarding existing smart home assistants that rely on transmitting user data to remote servers for personalization.

Method: HomeLLaMA utilizes an on-device small language model, learning from cloud LLMs, to provide personalized and private user interactions. Additionally, PrivShield offers optional cloud-based interactions for handling less-sensitive queries.

Result: Experiments and user studies involving 100 participants confirm HomeLLaMA's effectiveness in providing high-quality, personalized services while ensuring greater privacy.

Conclusion: HomeLLaMA is a viable solution for privacy-conscious smart home users, balancing personalization with privacy through on-device processing and selective cloud interaction.

Abstract: Large Language Models (LLMs) have showcased remarkable generalizability in
language comprehension and hold significant potential to revolutionize
human-computer interaction in smart homes. Existing LLM-based smart home
assistants typically transmit user commands, along with user profiles and home
configurations, to remote servers to obtain personalized services. However,
users are increasingly concerned about the potential privacy leaks to the
remote servers. To address this issue, we develop HomeLLaMA, an on-device
assistant for privacy-preserving and personalized smart home serving with a
tailored small language model (SLM). HomeLLaMA learns from cloud LLMs to
deliver satisfactory responses and enable user-friendly interactions. Once
deployed, HomeLLaMA facilitates proactive interactions by continuously updating
local SLMs and user profiles. To further enhance user experience while
protecting their privacy, we develop PrivShield to offer an optional
privacy-preserving LLM-based smart home serving for those users, who are
unsatisfied with local responses and willing to send less-sensitive queries to
remote servers. For evaluation, we build a comprehensive benchmark DevFinder to
assess the service quality. Extensive experiments and user studies (M=100)
demonstrate that HomeLLaMA can provide personalized services while
significantly enhancing user privacy.

</details>


### [581] [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)
*Tianzhe Zhao,Jiaoyan Chen,Yanchi Ru,Haiping Zhu,Nan Hu,Jun Liu,Qika Lin*

Main category: cs.CR

TL;DR: The paper investigates vulnerabilities in KG-RAG (Knowledge Graph-based Retrieval-Augmented Generation) systems caused by data poisoning, introducing an attack strategy that manipulates knowledge graphs to degrade system performance.


<details>
  <summary>Details</summary>
Motivation: RAG systems, particularly KG-RAG methods, are vulnerable to data poisoning attacks, which can exploit structured nature of KGs to produce incorrect or harmful responses.

Method: The proposed attack identifies adversarial target answers and inserts perturbation triples to create misleading inference chains in KGs, making KG-RAG systems rely on corrupted data during generation.

Result: Experiments on benchmarks and KG-RAG methods show that the attack effectively degrades system performance with minimal perturbation to the KG.

Conclusion: KG-RAG systems face security risks in real-world applications, necessitating deeper exploration of vulnerabilities and robust countermeasures to safeguard these systems.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving external data to mitigate hallucinations and outdated knowledge
issues. Benefiting from the strong ability in facilitating diverse data sources
and supporting faithful reasoning, knowledge graphs (KGs) have been
increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)
methods. Though RAG systems are widely applied in various applications, recent
studies have also revealed its vulnerabilities to data poisoning attacks, where
malicious information injected into external knowledge sources can mislead the
system into producing incorrect or harmful responses. However, these studies
focus exclusively on RAG systems using unstructured textual data sources,
leaving the security risks of KG-RAG largely unexplored, despite the fact that
KGs present unique vulnerabilities due to their structured and editable nature.
In this work, we conduct the first systematic investigation of the security
issue of KG-RAG methods through data poisoning attacks. To this end, we
introduce a practical, stealthy attack setting that aligns with real-world
implementation. We propose an attack strategy that first identifies adversarial
target answers and then inserts perturbation triples to complete misleading
inference chains in the KG, increasing the likelihood that KG-RAG methods
retrieve and rely on these perturbations during generation. Through extensive
experiments on two benchmarks and four recent KG-RAG methods, our attack
strategy demonstrates strong effectiveness in degrading KG-RAG performance,
even with minimal KG perturbations. In-depth analyses are also conducted to
understand the safety threats within the internal stages of KG-RAG systems and
to explore the robustness of LLMs against adversarial knowledge.

</details>


### [582] [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Jaafar Chbili*

Main category: cs.CR

TL;DR: The paper introduces an unsupervised framework to automatically detect, cluster, and prioritize cybersecurity events from hacker forums, using Transformer-based embeddings and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Hacker forums hold critical information on emerging cybersecurity threats, but their unstructured and noisy nature makes actionable intelligence extraction difficult.

Method: The approach involves Transformer-based embeddings fine-tuned with contrastive learning to cluster related discussions into security event groups. A daily ranking mechanism prioritizes events using metrics like timeliness and source credibility.

Result: Experimental evaluation on real-world hacker forum data shows the framework significantly reduces noise and identifies high-priority threats effectively.

Conclusion: The framework converts unstructured discussions into actionable intelligence, addressing challenges in automated threat detection and enabling proactive cybersecurity responses.

Abstract: Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

</details>


### [583] [A Mixture of Linear Corrections Generates Secure Code](https://arxiv.org/abs/2507.09508)
*Weichen Yu,Ravi Mangal,Terry Zhuo,Matt Fredrikson,Corina S. Pasareanu*

Main category: cs.CR

TL;DR: The paper explores how large language models (LLMs) internally encode security-related knowledge and presents a method to improve the security and functionality of code generated by LLMs leveraging those representations.


<details>
  <summary>Details</summary>
Motivation: Large language models excel at code generation but struggle with identifying and mitigating code vulnerabilities, posing risks for secure software development.

Method: The study investigates LLMs' internal representations regarding code vulnerabilities and introduces a token-generation steering method, called a mixture of corrections (MoC), to guide LLM outputs toward more secure coding practices.

Result: The MoC technique improves both code security and functionality in model outputs, notably enhancing Qwen2.5-Coder-7B's security by 8.9% and code functionality by 2.1%.

Conclusion: LLMs do encode concepts pertinent to security, and harnessing these representations at inference time offers a practical solution to reducing vulnerabilities in AI-generated code without sacrificing code quality.

Abstract: Large language models (LLMs) have become proficient at sophisticated
code-generation tasks, yet remain ineffective at reliably detecting or avoiding
code vulnerabilities. Does this deficiency stem from insufficient learning
about code vulnerabilities, or is it merely a result of ineffective prompting?
Using representation engineering techniques, we investigate whether LLMs
internally encode the concepts necessary to identify code vulnerabilities. We
find that current LLMs encode precise internal representations that distinguish
vulnerable from secure code--achieving greater accuracy than standard prompting
approaches. Leveraging these vulnerability-sensitive representations, we
develop an inference-time steering technique that subtly modulates the model's
token-generation probabilities through a mixture of corrections (MoC). Our
method effectively guides LLMs to produce less vulnerable code without
compromising functionality, demonstrating a practical approach to controlled
vulnerability management in generated code. Notably, MoC enhances the security
ratio of Qwen2.5-Coder-7B by 8.9\%, while simultaneously improving
functionality on HumanEval pass@1 by 2.1\%.

</details>


### [584] [Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing](https://arxiv.org/abs/2507.09860)
*Nguyen Van Duc,Bui Duc Manh,Quang-Trung Luu,Dinh Thai Hoang,Van-Linh Nguyen,Diep N. Nguyen*

Main category: cs.CR

TL;DR: This paper integrates Homomorphic Encryption (HE) with advanced neural networks for UAV-based face detection to secure facial data during processing without significantly affecting accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by privacy concerns related to UAVs' ability to perform extensive surveillance through high-resolution imagery and sophisticated face detection.

Method: The proposed method incorporates the Cheon-Kim-Kim-Song (CKKS) encryption scheme to enable computation on encrypted data, utilizes a specialized data encoding technique, and constructs secure inference algorithms.

Result: Experimental findings show the approach effectively ensures privacy while maintaining detection accuracy within 1% of unencrypted systems.

Conclusion: The framework optimizes privacy protection and efficiency, offering a practical solution for secure UAV-based facial recognition systems.

Abstract: This paper aims to propose a novel machine learning (ML) approach
incorporating Homomorphic Encryption (HE) to address privacy limitations in
Unmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related
to distance, altitude, and face orientation, high-resolution imagery and
sophisticated neural networks enable accurate face recognition in dynamic
environments. However, privacy concerns arise from the extensive surveillance
capabilities of UAVs. To resolve this issue, we propose a novel framework that
integrates HE with advanced neural networks to secure facial data throughout
the inference phase. This method ensures that facial data remains secure with
minimal impact on detection accuracy. Specifically, the proposed system
leverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly
on encrypted data, optimizing computational efficiency and security.
Furthermore, we develop an effective data encoding method specifically designed
to preprocess the raw facial data into CKKS form in a
Single-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a
secure inference algorithm to compute on ciphertext without needing decryption.
This approach not only protects data privacy during the processing of facial
data but also enhances the efficiency of UAV-based face detection systems.
Experimental results demonstrate that our method effectively balances privacy
protection and detection performance, making it a viable solution for UAV-based
secure face detection. Significantly, our approach (while maintaining data
confidentially with HE encryption) can still achieve an accuracy of less than
1% compared to the benchmark without using encryption.

</details>


### [585] [Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix](https://arxiv.org/abs/2507.09990)
*Ming Wen,Jiaqi Zhu,Yuedong Xu,Yipeng Zhou,Dingding Han*

Main category: cs.CR

TL;DR: FedASK addresses the privacy challenges in federated fine-tuning of LLMs using LoRA by introducing a novel sketching pipeline for robust differential privacy.


<details>
  <summary>Details</summary>
Motivation: To address privacy leakage issues and efficiency challenges in federated LoRA fine-tuning for domain-specific tasks while ensuring differential privacy.

Method: FedASK uses a two-stage sketching pipeline inspired by randomized SVD, enabling privacy-preserving local update aggregation and effective global adapter reconstruction.

Result: Experimental evaluations show that FedASK outperforms baseline methods consistently in privacy settings and various data distributions.

Conclusion: FedASK provides a practical and theoretically robust solution to secure federated LLM adaptation without compromising learnability or privacy.

Abstract: Large language models (LLMs) typically require fine-tuning for
domain-specific tasks, and LoRA offers a computationally efficient approach by
training low-rank adapters. LoRA is also communication-efficient for federated
LLMs when multiple users collaboratively fine-tune a global LLM model without
sharing their proprietary raw data. However, even the transmission of local
adapters between a server and clients risks serious privacy leakage. Applying
differential privacy (DP) to federated LoRA encounters a dilemma: adding noise
to both adapters amplifies synthetic noise on the model, while fixing one
adapter impairs the learnability of fine-tuning. In this paper, we propose
FedASK (Differentially Private Federated Low Rank Adaptation with Double
Sketching) , a novel federated LoRA framework to enable effective updating of
both low-rank adapters with robust differential privacy. Inspired by randomized
SVD, our key idea is a two-stage sketching pipeline. This pipeline first
aggregates carefully sketched, privacy-preserving local updates, and then
reconstructs the global matrices on the server to facilitate effective updating
of both adapters. We theoretically prove FedASK's differential privacy
guarantee and its exact aggregation property. Comprehensive experiments
demonstrate that FedASK consistently outperforms baseline methods across a
variety of privacy settings and data distributions.

</details>


### [586] [CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories](https://arxiv.org/abs/2507.09624)
*Xiaojie Lin,Baihe Ma,Xu Wang,Guangsheng Yu,Ying He,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: The paper introduces CAN-Trace, a privacy attack mechanism that uses vehicle CAN messages to reconstruct driving trajectories, showing a high success rate in trajectory identification.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of driving trajectory data to privacy attacks and demonstrate how CAN messages can be exploited beyond traditional GPS methods.

Method: The proposed method transforms vehicle speed and accelerator pedal position data from CAN messages into weighted graphs and uses graph-matching algorithms to uncover driving trajectories, with allowances for data gaps.

Result: CAN-Trace achieves up to 90.59% attack success in urban areas and 99.41% in suburban areas, validated under diverse real-world conditions.

Conclusion: CAN-Trace poses a significant privacy risk, demonstrating the effectiveness of leveraging vehicle CAN data to compromise driver privacy through trajectory reconstruction.

Abstract: Driving trajectory data remains vulnerable to privacy breaches despite
existing mitigation measures. Traditional methods for detecting driving
trajectories typically rely on map-matching the path using Global Positioning
System (GPS) data, which is susceptible to GPS data outage. This paper
introduces CAN-Trace, a novel privacy attack mechanism that leverages
Controller Area Network (CAN) messages to uncover driving trajectories, posing
a significant risk to drivers' long-term privacy. A new trajectory
reconstruction algorithm is proposed to transform the CAN messages,
specifically vehicle speed and accelerator pedal position, into weighted graphs
accommodating various driving statuses. CAN-Trace identifies driving
trajectories using graph-matching algorithms applied to the created graphs in
comparison to road networks. We also design a new metric to evaluate matched
candidates, which allows for potential data gaps and matching inaccuracies.
Empirical validation under various real-world conditions, encompassing
different vehicles and driving regions, demonstrates the efficacy of CAN-Trace:
it achieves an attack success rate of up to 90.59% in the urban region, and
99.41% in the suburban region.

</details>


### [587] [Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems](https://arxiv.org/abs/2507.10457)
*Hammad Atta,Ken Huang,Manish Bhatt,Kamal Ahmed,Muhammad Aziz Ul Haq,Yasir Mehmood*

Main category: cs.CR

TL;DR: The paper introduces Logic-Layer Prompt Control Injection (LPCI), a new attack on enterprise systems using LLMs by embedding delayed payloads in memory or tool outputs.


<details>
  <summary>Details</summary>
Motivation: To address and highlight a critical security vulnerability in enterprise systems using LLMs, specifically regarding undetected logic-layer attacks.

Method: The authors propose the concept of LPCI, detailing how encoded, delayed, and conditionally triggered payloads can subvert security by bypassing input filters.

Result: LPCI payloads can remain hidden across sessions and trigger unauthorized behavior, bypassing standard defenses.

Conclusion: Enterprise systems incorporating LLMs are vulnerable to sophisticated attacks like LPCI, necessitating better detection and defense mechanisms.

Abstract: The integration of large language models (LLMs) into enterprise systems has
created a new class of covert security vulnerabilities, particularly within
logic-execution layers and persistent-memory contexts. In this paper, we
introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category
in which encoded, delayed, and conditionally triggered payloads are embedded in
memory, vector stores, or tool outputs. These payloads can bypass conventional
input filters and trigger unauthorised behaviour across sessions.

</details>


### [588] [DNS Tunneling: Threat Landscape and Improved Detection Solutions](https://arxiv.org/abs/2507.10267)
*Novruz Amirov,Baran Isik,Bilal Ihsan Tuncer,Serif Bahtiyar*

Main category: cs.CR

TL;DR: The study proposes a machine learning-based approach for detecting DNS tunneling, which overcomes the limitations of traditional rule-based methods.


<details>
  <summary>Details</summary>
Motivation: The need to address the limitation of rule-based or signature-matching methods in detecting covert DNS tunneling activities, which are often used to hide harmful actions.

Method: A novel detection system using machine learning algorithms to analyze DNS traffic by extracting features from it.

Result: The analysis demonstrates the effectiveness of the proposed machine learning approach in detecting DNS tunneling accurately.

Conclusion: Machine learning offers an effective alternative to conventional methods for identifying DNS tunneling, showcasing its potential as a reliable detection mechanism.

Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in
security due to its capacity to hide harmful actions within DNS traffic that
appears to be normal and legitimate. Traditional detection methods are based on
rule-based approaches or signature matching methods that are often insufficient
to accurately identify such covert communication channels. This research is
about effectively detecting DNS tunneling. We propose a novel approach to
detect DNS tunneling with machine learning algorithms. We combine machine
learning algorithms to analyze the traffic by using features extracted from DNS
traffic. Analyses results show that the proposed approach is a good candidate
to detect DNS tunneling accurately.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [589] [Bridging Literature and the Universe Via A Multi-Agent Large Language Model System](https://arxiv.org/abs/2507.08958)
*Xiaowen Zhang,Zhenyu Bi,Xuan Wang,Tiziana Di Matteo,Rupert A. C. Croft*

Main category: astro-ph.IM

TL;DR: SimAgents automates cosmological simulation parameter extraction and analysis, improving efficiency and reducing errors in physics research.


<details>
  <summary>Details</summary>
Motivation: Physicists face challenges in extracting simulation parameters from diverse academic papers and translating them into executable scripts, which is time-consuming and error-prone.

Method: The paper introduces SimAgents, a multi-agent system powered by specialized LLM agents capable of physics reasoning, software validation, and tool execution. These agents collaborate for structured communication ensuring meaningful and compliant parameter extraction.

Result: SimAgents demonstrated strong performance on a cosmological parameter extraction evaluation dataset created from over 40 simulations in academic papers, showcasing its effectiveness.

Conclusion: SimAgents has the potential to significantly accelerate cosmological research, making the simulation process more efficient and reducing errors in parameter extraction.

Abstract: As cosmological simulations and their associated software become increasingly
complex, physicists face the challenge of searching through vast amounts of
literature and user manuals to extract simulation parameters from dense
academic papers, each using different models and formats. Translating these
parameters into executable scripts remains a time-consuming and error-prone
process. To improve efficiency in physics research and accelerate the
cosmological simulation process, we introduce SimAgents, a multi-agent system
designed to automate both parameter configuration from the literature and
preliminary analysis for cosmology research. SimAgents is powered by
specialized LLM agents capable of physics reasoning, simulation software
validation, and tool execution. These agents collaborate through structured
communication, ensuring that extracted parameters are physically meaningful,
internally consistent, and software-compliant. We also construct a cosmological
parameter extraction evaluation dataset by collecting over 40 simulations in
published papers from Arxiv and leading journals that cover diverse simulation
types. Experiments on the dataset demonstrate a strong performance of
SimAgents, highlighting its effectiveness and potential to accelerate
scientific research for physicists. Our demonstration video is available at:
https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly
available at https://github.com/xwzhang98/SimAgents.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [590] [Physics-Based Machine Learning Closures and Wall Models for Hypersonic Transition-Continuum Boundary Layer Predictions](https://arxiv.org/abs/2507.08986)
*Ashish S. Nair,Narendra Singh,Marco Panesi,Justin Sirignano,Jonathan F. MacArt*

Main category: physics.flu-dyn

TL;DR: The paper introduces a physics-constrained machine learning framework to improve hypersonic flow modeling in transition-continuum regimes (Knudsen number 0.1–10) where conventional continuum approaches fail.


<details>
  <summary>Details</summary>
Motivation: Classical Navier-Stokes-Fourier models cannot accurately predict nonequilibrium effects in rarefied hypersonic flows, necessitating advanced methods for modeling such regimes.

Method: The study proposes deep learning PDE models for stress and heat flux coupled with a skewed-Gaussian based wall model to replace empirical boundary conditions. These models are trained with adjoint-based optimization.

Result: Enhanced accuracy is achieved using anisotropic viscosity and skewed-Gaussian wall models, particularly for high-Mach and high-Knudsen number flows. Parallel training and high-Mach data inclusion further boost generalization.

Conclusion: The approach provides a data-driven, physics-informed strategy to extend continuum solver applicability in nonequilibrium hypersonic regimes, showing promise for scenarios beyond conventional models.

Abstract: Modeling rarefied hypersonic flows remains a fundamental challenge due to the
breakdown of classical continuum assumptions in the transition-continuum
regime, where the Knudsen number ranges from approximately 0.1 to 10.
Conventional Navier-Stokes-Fourier (NSF) models with empirical slip-wall
boundary conditions fail to accurately predict nonequilibrium effects such as
velocity slip, temperature jump, and shock structure deviations. We develop a
physics-constrained machine learning framework that augments transport models
and boundary conditions to extend the applicability of continuum solvers in
nonequilibrium hypersonic regimes. We employ deep learning PDE models (DPMs)
for the viscous stress and heat flux embedded in the governing PDEs and trained
via adjoint-based optimization. We evaluate these for two-dimensional
supersonic flat-plate flows across a range of Mach and Knudsen numbers.
Additionally, we introduce a wall model based on a mixture of skewed Gaussian
approximations of the particle velocity distribution function. This wall model
replaces empirical slip conditions with physically informed, data-driven
boundary conditions for the streamwise velocity and wall temperature. Our
results show that a trace-free anisotropic viscosity model, paired with the
skewed-Gaussian distribution function wall model, achieves significantly
improved accuracy, particularly at high-Mach and high-Knudsen number regimes.
Strategies such as parallel training across multiple Knudsen numbers and
inclusion of high-Mach data during training are shown to enhance model
generalization. Increasing model complexity yields diminishing returns for
out-of-sample cases, underscoring the need to balance degrees of freedom and
overfitting. This work establishes data-driven, physics-consistent strategies
for improving hypersonic flow modeling for regimes in which conventional
continuum approaches are invalid.

</details>


### [591] [WellPINN: Accurate Well Representation for Transient Fluid Pressure Diffusion in Subsurface Reservoirs with Physics-Informed Neural Networks](https://arxiv.org/abs/2507.09330)
*Linus Walter,Qingkai Kong,Sara Hanson-Hedgecock,Víctor Vilarrasa*

Main category: physics.flu-dyn

TL;DR: Introduces WellPINN, a workflow using sequentially trained Physics-Informed Neural Networks (PINNs) to improve well pressure modeling in reservoir simulations.


<details>
  <summary>Details</summary>
Motivation: Existing PINN-based approaches struggle to capture fluid pressure near wells after injection begins, limiting their effectiveness in reservoir modeling.

Method: WellPINN iteratively trains multiple PINNs across subdomains, shrinking the equivalent well radius stepwise, to approximate the real well dimensions more accurately.

Result: WellPINN demonstrates improved accuracy in fluid pressure inference throughout injection periods, addressing limitations of current PINN-based models.

Conclusion: WellPINN enhances PINN-based reservoir modeling, especially for operational scenarios and inverse modeling; data/code are made available for further research.

Abstract: Accurate representation of wells is essential for reliable reservoir
characterization and simulation of operational scenarios in subsurface flow
models. Physics-informed neural networks (PINNs) have recently emerged as a
promising method for reservoir modeling, offering seamless integration of
monitoring data and governing physical equations. However, existing PINN-based
studies face major challenges in capturing fluid pressure near wells,
particularly during the early stage after injection begins. To address this, we
propose WellPINN, a modeling workflow that combines the outputs of multiple
sequentially trained PINN models to accurately represent wells. This workflow
iteratively approximates the radius of the equivalent well to match the actual
well dimensions by decomposing the domain into stepwise shrinking subdomains
with a simultaneously reducing equivalent well radius. Our results demonstrate
that sequential training of superimposing networks around the pumping well is
the first workflow that focuses on accurate inference of fluid pressure from
pumping rates throughout the entire injection period, significantly advancing
the potential of PINNs for inverse modeling and operational scenario
simulations. All data and code for this paper will be made openly available at
https://github.com/linuswalter/WellPINN.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [592] [Extending Defeasibility for Propositional Standpoint Logics](https://arxiv.org/abs/2507.10133)
*Nicholas Leisegang,Thomas Meyer,Ivan Varzinczak*

Main category: cs.LO

TL;DR: The paper introduces a new defeasible version of propositional standpoint logic, offering a robust framework to handle defeasibility across different logical constructs.


<details>
  <summary>Details</summary>
Motivation: To enhance standpoint logic by incorporating defeasible reasoning, making it more flexible and applicable to situations involving conditional and modal implications.

Method: The authors integrate defeasible conditionals, specific notions of necessity and possibility, and approaches to defeasibility into propositional standpoint logic, developing a preferential semantics and a sound and complete tableaux calculus.

Result: A logical framework enabling defeasibility in implications, modal operators, and standpoint statements was developed, and the tableaux calculus was proven computationally efficient in PSpace complexity.

Conclusion: The paper successfully expands standpoint logic through defeasible reasoning, providing useful tools and efficient computational methods for logical reasoning.

Abstract: In this paper, we introduce a new defeasible version of propositional
standpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz
and Varzinczak's notions of defeasible necessity and distinct possibility,
along with Leisegang et al.'s approach to defeasibility into the standpoint
logics of G\'omez \'Alvarez and Rudolph. The resulting logical framework allows
for the expression of defeasibility on the level of implications, standpoint
modal operators, and standpoint-sharpening statements. We provide a
preferential semantics for this extended language and propose a tableaux
calculus, which is shown to be sound and complete with respect to preferential
entailment. We also establish the computational complexity of the tableaux
procedure to be in PSpace.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [593] [Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood](https://arxiv.org/abs/2507.08896)
*Byunghee Lee,Hye Yeon Sin,Joonsung Kang*

Main category: stat.ME

TL;DR: This paper introduces an integrated framework combining HMM and MTGCN for predictive causal inference, focusing on spatiotemporal complexities in biomedical data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of conventional single-model approaches in predictive causal inference, especially in handling spatiotemporal complexities in clinical data.

Method: The framework integrates a Hidden Markov Model (HMM) for spatial health state estimation with a Multi Task and Multi Graph Convolutional Network (MTGCN) for temporal outcome trajectories. It uses an asymmetric treatment of spatial and temporal data for doubly robust treatment effect estimation.

Result: Simulation studies evaluated the framework's ability to emulate latent disease dynamics and perform under varying conditions.

Conclusion: The framework enhances predictive causal inference by structurally addressing spatiotemporal complexities, particularly in clinical domains like cancer, dementia, and Parkinson's disease.

Abstract: This study introduces an integrated framework for predictive causal inference
designed to overcome limitations inherent in conventional single model
approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial
health state estimation with a Multi Task and Multi Graph Convolutional Network
(MTGCN) for capturing temporal outcome trajectories. The framework
asymmetrically treats temporal and spatial information regarding them as
endogenous variables in the outcome regression, and exogenous variables in the
propensity score model, thereby expanding the standard doubly robust treatment
effect estimation to jointly enhance bias correction and predictive accuracy.
To demonstrate its utility, we focus on clinical domains such as cancer,
dementia, and Parkinson disease, where treatment effects are challenging to
observe directly. Simulation studies are conducted to emulate latent disease
dynamics and evaluate the model performance under varying conditions. Overall,
the proposed framework advances predictive causal inference by structurally
adapting to spatiotemporal complexities common in biomedical data.

</details>


### [594] [Convex Clustering](https://arxiv.org/abs/2507.09077)
*Eric C. Chi,Aaron J. Molstad,Zheming Gao*

Main category: stat.ME

TL;DR: This paper surveys convex clustering, a stable and globally optimal clustering method with unique advantages over existing approaches.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional clustering methods, such as spurious local minima and instability in outputs, by exploring a convex optimization-based clustering approach.

Method: The authors study convex clustering, an approach based on solving a convex optimization problem that ensures global optimality, stability, and flexible parameter tuning.

Result: Convex clustering is demonstrated to be robust, stable, computationally scalable, and versatile in integration with other inferential methods.

Conclusion: Convex clustering offers a novel and compelling framework for clustering, combining theoretical strengths with practical applicability across various domains.

Abstract: This survey reviews a clustering method based on solving a convex
optimization problem. Despite the plethora of existing clustering methods,
convex clustering has several uncommon features that distinguish it from prior
art. The optimization problem is free of spurious local minima, and its unique
global minimizer is stable with respect to all its inputs, including the data,
a tuning parameter, and weight hyperparameters. Its single tuning parameter
controls the number of clusters and can be chosen using standard techniques
from penalized regression. We give intuition into the behavior and theory for
convex clustering as well as practical guidance. We highlight important
algorithms and give insight into how their computational costs scale with the
problem size. Finally, we highlight the breadth of its uses and flexibility to
be combined and integrated with other inferential methods.

</details>


### [595] [Sharp Trade-Offs in High-Dimensional Inference via 2-Level SLOPE](https://arxiv.org/abs/2507.09110)
*Zhiqi Bu,Jason M. Klusowski,Cynthia Rush,Ruijia Wu*

Main category: stat.ME

TL;DR: 2-level SLOPE simplifies the hyperparameter tuning of the SLOPE method for high-dimensional linear regression while retaining its advantages and computational benefits.


<details>
  <summary>Details</summary>
Motivation: The complexity of tuning a monotone penalty sequence in high-dimensional regression settings makes general SLOPE challenging to deploy. There is a need for a simplified yet effective alternative.

Method: The authors focus on 2-level SLOPE, an important subclass of SLOPE with only three hyperparameters, and analyze its theoretical and empirical performance.

Result: 2-level SLOPE maintains the benefits of general SLOPE while reducing computational complexity. It provides tight theoretical characterizations of trade-offs like true positive proportion and false discovery proportion.

Conclusion: 2-level SLOPE is a robust, scalable alternative to both LASSO and general SLOPE for high-dimensional data analysis, especially in challenging settings like high noise or non-sparse signals.

Abstract: Among techniques for high-dimensional linear regression, Sorted L-One
Penalized Estimation (SLOPE) generalizes the LASSO via an adaptive $l_1$
regularization that applies heavier penalties to larger coefficients in the
model. To achieve such adaptivity, SLOPE requires the specification of a
complex hierarchy of penalties, i.e., a monotone penalty sequence in $R^p$, in
contrast to a single penalty scalar for LASSO. Tuning this sequence when $p$ is
large poses a challenge, as brute force search over a grid of values is
computationally prohibitive. In this work, we study the 2-level SLOPE, an
important subclass of SLOPE, with only three hyperparameters. We demonstrate
both empirically and analytically that 2-level SLOPE not only preserves the
advantages of general SLOPE -- such as improved mean squared error and
overcoming the Donoho-Tanner power limit -- but also exhibits computational
benefits by reducing the penalty hyperparameter space. In particular, we prove
that 2-level SLOPE admits a sharp, theoretically tight characterization of the
trade-off between true positive proportion (TPP) and false discovery proportion
(FDP), contrasting with general SLOPE where only upper and lower bounds are
known. Empirical evaluations further underscore the effectiveness of 2-level
SLOPE in settings where predictors exhibit high correlation, when the noise is
large, or when the underlying signal is not sparse. Our results suggest that
2-level SLOPE offers a robust, scalable alternative to both LASSO and general
SLOPE, making it particularly suited for practical high-dimensional data
analysis.

</details>


### [596] [A Moment-Based Generalization to Post-Prediction Inference](https://arxiv.org/abs/2507.09119)
*Stephen Salerno,Kentaro Hoffman,Awan Afiaz,Anna Neufeld,Tyler H. McCormick,Jeffrey T. Leek*

Main category: stat.ME

TL;DR: This paper revisits Wang et al.'s (2020) post-prediction inference method, extends it with relaxed assumptions, and demonstrates improved bias reduction and calibration.


<details>
  <summary>Details</summary>
Motivation: To address the biases and errors that arise when AI/ML-predicted data is naively used as true observations for analysis and to build upon Wang et al.’s method for more reliable inference.

Method: The authors extend Wang et al.'s post-prediction inference method by introducing modifications to relax assumptions and include a scaling factor to better handle variability.

Result: The proposed extension achieves unbiased point estimates under standard conditions, maintains Type I error rates, reduces bias, and ensures proper coverage in simulations.

Conclusion: The extended method improves reliability and accuracy in analyses involving ML-predicted data by appropriately calibrating variability and assumptions.

Abstract: Artificial intelligence (AI) and machine learning (ML) are increasingly used
to generate data for downstream analyses, yet naively treating these
predictions as true observations can lead to biased results and incorrect
inference. Wang et al. (2020) proposed a method, post-prediction inference,
which calibrates inference by modeling the relationship between AI/ML-predicted
and observed outcomes in a small, gold-standard sample. Since then, several
methods have been developed for inference with predicted data. We revisit Wang
et al. in light of these recent developments. We reflect on their assumptions
and offer a simple extension of their method which relaxes these assumptions.
Our extension (1) yields unbiased point estimates under standard conditions and
(2) incorporates a simple scaling factor to preserve calibration variability.
In extensive simulations, we show that our method maintains nominal Type I
error rates, reduces bias, and achieves proper coverage.

</details>


### [597] [The BdryMatérn GP: Reliable incorporation of boundary information on irregular domains for Gaussian process modeling](https://arxiv.org/abs/2507.09178)
*Liang Ding,Simon Mak,C. F. Jeff Wu*

Main category: stat.ME

TL;DR: The paper introduces a new Gaussian Process (GP) framework, "BdryMatérn GP," designed to integrate boundary information on irregular domains with smoothness control and error analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional GP models face difficulties with limited training data due to the computational cost of simulations and cannot effectively incorporate boundary information on non-hypercube domains.

Method: The BdryMatérn GP uses a novel covariance kernel obtained through a stochastic partial differential equation formulation, incorporating boundary conditions like Dirichlet, Neumann, and Robin on irregular domains. It also employs finite element modeling for efficient kernel approximation with error analysis.

Result: The model successfully integrates complex boundaries while controlling sample smoothness, demonstrated through numerical experiments on various irregular domains.

Conclusion: BdryMatérn GP offers a reliable and computationally efficient solution for integrating boundary information into surrogate modeling on irregular domains, overcoming key limitations of traditional GP frameworks.

Abstract: Gaussian processes (GPs) are broadly used as surrogate models for expensive
computer simulators of complex phenomena. However, a key bottleneck is that its
training data are generated from this expensive simulator and thus can be
highly limited. A promising solution is to supplement the learning model with
boundary information from scientific knowledge. However, despite recent work on
boundary-integrated GPs, such models largely cannot accommodate boundary
information on irregular (i.e., non-hypercube) domains, and do not provide
sample path smoothness control or approximation error analysis, both of which
are important for reliable surrogate modeling. We thus propose a novel
BdryMat\'ern GP modeling framework, which can reliably integrate Dirichlet,
Neumann and Robin boundaries on an irregular connected domain with a boundary
set that is twice-differentiable almost everywhere. Our model leverages a new
BdryMat\'ern covariance kernel derived in path integral form via a stochastic
partial differential equation formulation. Similar to the GP with Mat\'ern
kernel, we prove that sample paths from the BdryMat\'ern GP satisfy the desired
boundaries with smoothness control on its derivatives. We further present an
efficient approximation procedure for the BdryMat\'ern kernel using finite
element modeling with rigorous error analysis. Finally, we demonstrate the
effectiveness of the BdryMat\'ern GP in a suite of numerical experiments on
incorporating broad boundaries on irregular domains.

</details>


### [598] [Robust Spatiotemporal Epidemic Modeling with Integrated Adaptive Outlier Detection](https://arxiv.org/abs/2507.09380)
*Haoming Shi,Shan Yu,Eric C. Chi*

Main category: stat.ME

TL;DR: The paper introduces RST-GAM, a robust method for addressing outliers in epidemic modeling to improve public health decision-making and identify disease hotspots.


<details>
  <summary>Details</summary>
Motivation: Outliers in epidemic modeling can skew parameter estimation and mislead public health decisions, highlighting a need for methods that both mitigate distortion and identify disease hotspots.

Method: The RST-GAM incorporates a mean-shift parameter to adjust for outliers, uses adaptive Lasso regularization for sparsity modeling, employs spline-based functional approximation, and utilizes a scalable proximal algorithm for parameter estimation with error bounds and convergence guarantees.

Result: Numerical studies confirm the model's robustness under various outlier scenarios, and its application to U.S. county-level COVID-19 data demonstrates its practical utility for public health analysis.

Conclusion: RST-GAM effectively addresses outliers in spatiotemporal epidemic data, improves parameter estimation, and aids in identifying disease hotspots, making it a valuable tool for public health decision-making.

Abstract: In epidemic modeling, outliers can distort parameter estimation and
ultimately lead to misguided public health decisions. Although there are
existing robust methods that can mitigate this distortion, the ability to
simultaneously detect outliers is equally vital for identifying potential
disease hotspots. In this work, we introduce a robust spatiotemporal
generalized additive model (RST-GAM) to address this need. We accomplish this
with a mean-shift parameter to quantify and adjust for the effects of outliers
and rely on adaptive Lasso regularization to model the sparsity of outlying
observations. We use univariate polynomial splines and bivariate penalized
splines over triangulations to estimate the functional forms and a
data-thinning approach for data-adaptive weight construction. We derive a
scalable proximal algorithm to estimate model parameters by minimizing a convex
negative log-quasi-likelihood function. Our algorithm uses adaptive step-sizes
to ensure global convergence of the resulting iterate sequence. We establish
error bounds and selection consistency for the estimated parameters and
demonstrate our model's effectiveness through numerical studies under various
outlier scenarios. Finally, we demonstrate the practical utility of RST-GAM by
analyzing county-level COVID-19 infection data in the United States,
highlighting its potential to inform public health decision-making.

</details>


### [599] [Statistical Inference for Conditional Group Distributionally Robust Optimization with Cross-Entropy Loss](https://arxiv.org/abs/2507.09905)
*Zijian Guo,Zhenyu Wang,Yifan Hu,Francis Bach*

Main category: stat.ME

TL;DR: The paper introduces a Conditional Group Distributionally Robust Optimization (CG-DRO) framework for multi-source unsupervised domain adaptation, addressing distributional shifts with the target domain.


<details>
  <summary>Details</summary>
Motivation: Distributional heterogeneity across domains in multi-source learning challenges the reliability of predictive models when transferring to unseen domains.

Method: The CG-DRO framework minimizes the worst-case cross-entropy loss over convex combinations of conditional outcome distributions from source domains, solved using a Mirror Prox algorithm with a double machine learning procedure.

Result: The framework achieves fast statistical convergence rates and demonstrates nonstandard asymptotics, with a perturbation-based inference procedure ensuring valid uniform statistical inference.

Conclusion: CG-DRO effectively addresses distribution shifts and provides a robust theoretical foundation for reliable predictive modeling in multi-source unsupervised domain adaptation.

Abstract: In multi-source learning with discrete labels, distributional heterogeneity
across domains poses a central challenge to developing predictive models that
transfer reliably to unseen domains. We study multi-source unsupervised domain
adaptation, where labeled data are drawn from multiple source domains and only
unlabeled data from a target domain. To address potential distribution shifts,
we propose a novel Conditional Group Distributionally Robust Optimization
(CG-DRO) framework that learns a classifier by minimizing the worst-case
cross-entropy loss over the convex combinations of the conditional outcome
distributions from the sources. To solve the resulting minimax problem, we
develop an efficient Mirror Prox algorithm, where we employ a double machine
learning procedure to estimate the risk function. This ensures that the errors
of the machine learning estimators for the nuisance models enter only at
higher-order rates, thereby preserving statistical efficiency under covariate
shift. We establish fast statistical convergence rates for the estimator by
constructing two surrogate minimax optimization problems that serve as
theoretical bridges. A distinguishing challenge for CG-DRO is the emergence of
nonstandard asymptotics: the empirical estimator may fail to converge to a
standard limiting distribution due to boundary effects and system instability.
To address this, we introduce a perturbation-based inference procedure that
enables uniformly valid inference, including confidence interval construction
and hypothesis testing.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [600] [Stochastic Approximation with Block Coordinate Optimal Stepsizes](https://arxiv.org/abs/2507.08963)
*Tao Jiang,Lin Xiao*

Main category: math.OC

TL;DR: Proposes adaptive stepsize rules for block-coordinate stochastic approximation, leveraging second-moment online estimates for better optimization.


<details>
  <summary>Details</summary>
Motivation: To improve optimization approaches by addressing the lack of efficient and adaptive stepsize rules for block-coordinate stochastic approximation.

Method: Developed adaptive stepsize rules using online estimation of second moments, introducing a simplified algorithm similar to Adam but with reduced memory and hyper-parameter requirements.

Result: The proposed method matches Adam in performance while being resource-efficient and broadly applicable.

Conclusion: The method converges to a neighborhood of the optimal point under general conditions, enhancing optimization adaptability and resource efficiency.

Abstract: We consider stochastic approximation with block-coordinate stepsizes and
propose adaptive stepsize rules that aim to minimize the expected distance from
the next iterate to an optimal point. These stepsize rules employ online
estimates of the second moment of the search direction along each block
coordinate. The popular Adam algorithm can be interpreted as a particular
heuristic for such estimation. By leveraging a simple conditional estimator, we
derive a new method that obtains comparable performance as Adam but requires
less memory and fewer hyper-parameters. We prove that this family of methods
converges almost surely to a small neighborhood of the optimal point, and the
radius of the neighborhood depends on the bias and variance of the
second-moment estimator. Our analysis relies on a simple aiming condition that
assumes neither convexity nor smoothness, thus has broad applicability.

</details>


### [601] [On the Gradient Domination of the LQG Problem](https://arxiv.org/abs/2507.09026)
*Kasra Fallah,Leonardo F. Toso,James Anderson*

Main category: math.OC

TL;DR: This paper studies linear quadratic Gaussian (LQG) regulators with policy gradient methods and proves global convergence by introducing an alternative parameterization based on history representation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of global convergence guarantees for policy gradient methods in the LQG problem due to the absence of gradient dominance in the classical parameterization.

Method: The authors propose a history-based parameterization, utilizing past input and output data from previous steps, which enables gradient dominance and approximate smoothness for the LQG cost. They analyze both model-based and model-free settings.

Result: The paper establishes global convergence and per-iteration stability guarantees for policy gradient methods in the LQG setting, supported by numerical experiments on an unstable system.

Conclusion: The history representation of controllers resolves the nonconvexity barrier in LQG problems, enabling robust global optimization with policy gradient methods.

Abstract: We consider solutions to the linear quadratic Gaussian (LQG) regulator
problem via policy gradient (PG) methods. Although PG methods have demonstrated
strong theoretical guarantees in solving the linear quadratic regulator (LQR)
problem, despite its nonconvex landscape, their theoretical understanding in
the LQG setting remains limited. Notably, the LQG problem lacks gradient
dominance in the classical parameterization, i.e., with a dynamic controller,
which hinders global convergence guarantees. In this work, we study PG for the
LQG problem by adopting an alternative parameterization of the set of
stabilizing controllers and employing a lifting argument. We refer to this
parameterization as a history representation of the control input as it is
parameterized by past input and output data from the previous p time-steps.
This representation enables us to establish gradient dominance and approximate
smoothness for the LQG cost. We prove global convergence and per-iteration
stability guarantees for policy gradient LQG in model-based and model-free
settings. Numerical experiments on an open-loop unstable system are provided to
support the global convergence guarantees and to illustrate convergence under
different history lengths of the history representation.

</details>


### [602] [A Method for Learning to Solve Parametric Bilevel Optimization with Coupling Constraints](https://arxiv.org/abs/2507.09050)
*James Kotary,Himanshu Sharma,Ethan King,Draguna Vrabie,Ferdinando Fioretto,Jan Drgona*

Main category: math.OC

TL;DR: The paper develops a framework allowing neural networks to efficiently approximate solutions to complex bilevel optimization problems.


<details>
  <summary>Details</summary>
Motivation: Bilevel optimization problems are tough but critical, and existing learning-to-optimize work focuses only on simpler, single-level cases.

Method: The authors leverage differentiation through optimization to train neural networks to approximate solutions for various bilevel problems.

Result: Their framework performs effectively on both synthetic bilevel optimization problems and real-life control system co-design problems.

Conclusion: The approach demonstrates that neural networks can serve as practical tools to address challenging bilevel optimization problems efficiently.

Abstract: Learning to Optimize (L2O) is a subfield of machine learning (ML) in which ML
models are trained to solve parametric optimization problems. The general goal
is to learn a fast approximator of solutions to constrained optimization
problems, as a function of their defining parameters. Prior L2O methods focus
almost entirely on single-level programs, in contrast to the bilevel programs,
whose constraints are themselves expressed in terms of optimization
subproblems. Bilevel programs have numerous important use cases but are
notoriously difficult to solve, particularly under stringent time demands. This
paper proposes a framework for learning to solve a broad class of challenging
bilevel optimization problems, by leveraging modern techniques for
differentiation through optimization problems. The framework is illustrated on
an array of synthetic bilevel programs, as well as challenging control system
co-design problems, showing how neural networks can be trained as efficient
approximators of parametric bilevel optimization.

</details>


### [603] [Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization](https://arxiv.org/abs/2507.09823)
*Ekaterina Borodich,Dmitry Kovalev*

Main category: math.OC

TL;DR: This paper improves adaptive gradient methods like GRAAL by combining them with Nesterov acceleration to achieve optimal convergence rates.


<details>
  <summary>Details</summary>
Motivation: To address the question of whether adaptive gradient methods can be accelerated to match the optimal convergence rate of $\\mathcal{O}(1/k^2)$ for convex optimization problems.

Method: The authors develop a new algorithm, GRAAL with Nesterov acceleration, which adapts to local curvature, computes stepsizes without hyperparameter tuning, and achieves accelerated rates.

Result: The proposed algorithm achieves the optimal convergence rate of $\\mathcal{O}(1/k^2)$ for Lipschitz smooth functions, even with an excessively small initial stepsize, incurring only a logarithmic additive cost in iteration complexity.

Conclusion: GRAAL with Nesterov acceleration successfully combines adaptability and acceleration for minimizing differentiable convex functions, overcoming limitations of prior methods.

Abstract: In this paper, we focus on the problem of minimizing a continuously
differentiable convex objective function $\min_x f(x)$. Recently, several
adaptive gradient methods, including GRAAL (Malitsky, 2020), have been
developed. These methods estimate the local curvature of the objective function
to compute stepsizes, attain the standard convergence rate $\mathcal{O}(1/k)$
of fixed-stepsize gradient descent for Lipschitz-smooth functions, and do not
require any line search procedures or hyperparameter tuning. However, a natural
question arises: is it possible to accelerate the convergence of these
algorithms to match the optimal rate $\mathcal{O}(1/k^2)$ of the accelerated
gradient descent of Nesterov (1983)? Although some attempts have been made (Li
and Lan, 2023), the capabilities of the existing accelerated algorithms to
adapt to the curvature of the objective function are highly limited.
Consequently, we provide a positive answer to this question and develop GRAAL
with Nesterov acceleration. We prove that our algorithm achieves the desired
optimal convergence rate for Lipschitz smooth functions. Moreover, in contrast
to existing methods, it does so with an arbitrary, even excessively small,
initial stepsize at the cost of a logarithmic additive term in the iteration
complexity.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [604] [Sequence-Model-Guided Measurement Selection for Quantum State Learning](https://arxiv.org/abs/2507.09891)
*Jiaxin Huang,Yan Zhu,Giulio Chiribella,Ya-Dong Wu*

Main category: quant-ph

TL;DR: The paper introduces a deep neural network model for optimizing measurement choices in quantum system characterization, leading to better data-driven predictions and insights.


<details>
  <summary>Details</summary>
Motivation: Characterizing quantum systems requires effective measurement strategies, but optimization becomes infeasible for large systems. The paper seeks to address this limitation.

Method: A deep neural network with a sequence model architecture is developed to identify efficient measurement strategies for various quantum state tasks, including prediction, clustering, and tomography.

Result: The neural network's measurement choices consistently outperform random selections, revealing insights like correlations between system boundaries and bulk properties in topological systems.

Conclusion: The approach provides a data-driven way to optimize measurements for quantum systems, uncovering patterns and outperforming traditional random strategies even without embedded quantum knowledge.

Abstract: Characterization of quantum systems from experimental data is a central
problem in quantum science and technology. But which measurements should be
used to gather data in the first place? While optimal measurement choices can
be worked out for small quantum systems, the optimization becomes intractable
as the system size grows large. To address this problem, we introduce a deep
neural network with a sequence model architecture that searches for efficient
measurement choices in a data-driven, adaptive manner. The model can be applied
to a variety of tasks, including the prediction of linear and nonlinear
properties of quantum states, as well as state clustering and state tomography
tasks. In all these tasks, we find that the measurement choices identified by
our neural network consistently outperform the uniformly random choice.
Intriguingly, for topological quantum systems, our model tends to recommend
measurements at the system's boundaries, even when the task is to predict bulk
properties. This behavior suggests that the neural network may have
independently discovered a connection between boundaries and bulk, without
having been provided any built-in knowledge of quantum physics.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [605] [Optimizing External Sources for Controlled Burning Plasma in Tokamaks with Neural Ordinary Differential Equations](https://arxiv.org/abs/2507.09431)
*Zefang Liu,Weston M. Stacey*

Main category: physics.plasm-ph

TL;DR: This paper introduces a neural ODE-based framework, NeuralPlasmaODE, to inversely model and regulate plasma dynamics in tokamaks by computing external inputs like neutral beam injection power to achieve specified plasma behaviors.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of controlling plasma conditions for burning states in tokamaks, precise regulation of particle and energy sources is needed, motivating the development of a computational framework for inverse modeling.

Method: The paper employs Neural Ordinary Differential Equations (Neural ODEs) for modeling plasma dynamics and formulates the control problem as an optimization task using automatic differentiation through the Neural ODE solver.

Result: The proposed NeuralPlasmaODE framework successfully computes external source profiles that drive plasma dynamics toward specified target behaviors, demonstrating practical applicability in controlling tokamak operations.

Conclusion: NeuralPlasmaODE enables control-oriented modeling of plasma and offers an effective approach for regulating external energy sources in fusion devices, paving the way for advancements in both current and future tokamak systems.

Abstract: Achieving controlled burning plasma in tokamaks requires precise regulation
of external particle and energy sources to reach and maintain target core
densities and temperatures. This work presents an inverse modeling approach
using a multinodal plasma dynamics model based on neural ordinary differential
equations (Neural ODEs). Given a desired time evolution of nodal quantities
such as deuteron density or electron temperature, we compute the external
source profiles, such as neutral beam injection (NBI) power, that drive the
plasma toward the specified behavior. The approach is implemented within the
NeuralPlasmaODE framework, which models multi-region, multi-timescale transport
and incorporates physical mechanisms including radiation, auxiliary heating,
and internodal energy exchange. By formulating the control task as an
optimization problem, we use automatic differentiation through the Neural ODE
solver to minimize the discrepancy between simulated and target trajectories.
This framework transforms the forward simulation tool into a control-oriented
model and provides a practical method for computing external source profiles in
both current and future fusion devices.

</details>


### [606] [Sensitivity Analysis of Transport and Radiation in NeuralPlasmaODE for ITER Burning Plasmas](https://arxiv.org/abs/2507.09432)
*Zefang Liu,Weston M. Stacey*

Main category: physics.plasm-ph

TL;DR: This paper uses NeuralPlasmaODE, a machine learning-based model, to analyze how physical parameters affect plasma behavior in ITER, focusing on energy confinement and predictive modeling.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve understanding of key physical influences on burning plasma behavior to ensure the reliable operation of ITER.

Method: The study extends a neural ordinary differential equation-based model, NeuralPlasmaODE, to perform sensitivity analyses of various plasma-related parameters.

Result: Key factors such as magnetic field strength, safety factor, and impurity content are found to dominantly impact energy confinement, with temperature-dependent transport enabling self-regulating behavior.

Conclusion: The findings confirm NeuralPlasmaODE's effectiveness in predictive modeling and optimizing scenarios within ITER burning plasma contexts.

Abstract: Understanding how key physical parameters influence burning plasma behavior
is critical for the reliable operation of ITER. In this work, we extend
NeuralPlasmaODE, a multi-region, multi-timescale model based on neural ordinary
differential equations, to perform a sensitivity analysis of transport and
radiation mechanisms in ITER plasmas. Normalized sensitivities of core and edge
temperatures and densities are computed with respect to transport
diffusivities, electron cyclotron radiation (ECR) parameters, impurity
fractions, and ion orbit loss (IOL) timescales. The analysis focuses on
perturbations around a trained nominal model for the ITER inductive scenario.
Results highlight the dominant influence of magnetic field strength, safety
factor, and impurity content on energy confinement, while also revealing how
temperature-dependent transport contributes to self-regulating behavior. These
findings demonstrate the utility of NeuralPlasmaODE for predictive modeling and
scenario optimization in burning plasma environments.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [607] [Evolution of Fear and Social Rewards in Prey-Predator Relationship](https://arxiv.org/abs/2507.09992)
*Yuji Kanagawa,Kenji Doya*

Main category: q-bio.PE

TL;DR: The study used simulations to explore how fear and other rewards, such as social interaction and food incentives, evolve in prey under different environmental conditions with predators.


<details>
  <summary>Details</summary>
Motivation: To understand why fear evolved in response to predators, alongside other rewards like food and social interaction, in complex natural environments.

Method: The researchers employed distributed evolutionary simulations where prey and predators co-evolved innate reward functions and learned behaviors using reinforcement learning algorithms.

Result: Social reward for observing the same species proved more critical for prey survival. Fear-like traits emerged under specific circumstances, connected to social reward and predator characteristics.

Conclusion: Fear and social rewards are interlinked in prey evolution, influenced by the traits of predators, environmental threats, and food dynamics.

Abstract: Fear is a critical brain function for detecting danger and learning to avoid
specific stimuli that can lead to danger. While fear is believed to have
evolved under pressure from predators, experimentally reproducing the evolution
is challenging. To investigate the relationship between environmental
conditions, the evolution of fear, and the evolution of other rewards, such as
food reward and social reward, we developed a distributed evolutionary
simulation. In our simulation, prey and predator agents co-evolve their innate
reward functions, including a possibly fear-like term for observing predators,
and learn behaviors via reinforcement learning. Surprisingly, our simulation
revealed that social reward for observing the same species is more important
for prey to survive, and fear-like negative reward for observing predators
evolves only after acquiring social reward. We also found that the predator
with increased hunting ability (larger mouth) amplified fear emergence, but
also that fear evolution is more stable with non-evolving predators that are
bad at chasing prey. Additionally, unlike for predators, we found that positive
rewards evolve in opposition to fear for stationary threats, as areas with
abundant leftover food develop around them. These findings suggest that fear
and social reward have had a complex interplay with each other through
evolution, along with the nature of predators and threats.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [608] [Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference](https://arxiv.org/abs/2507.09942)
*Jiaming Cheng,Duong Tung Nguyen*

Main category: cs.NI

TL;DR: This paper develops an optimization model for distributing large language model (LLM) inference workloads across edge data centers to minimize environmental and operational impacts while improving user experience.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to balance the environmental impacts and operational efficiency of distributed LLM inference workloads due to increasing energy consumption and variability in renewable energy availability.

Method: The authors propose a novel optimization model to allocate LLM workloads across edge data centers while considering renewable energy generation, dynamic electricity prices, spatiotemporal variability, and user experience factors.

Result: Numerical results demonstrate that the proposed optimization approach effectively reduces operational costs, energy consumption, carbon emissions, and water usage.

Conclusion: The optimization model helps LLM service providers achieve sustainable operations by minimizing environmental impacts and operational costs, while maintaining high user experience standards.

Abstract: This letter investigates the optimal allocation of large language model (LLM)
inference workloads across heterogeneous edge data centers (DCs) over time.
Each DC features on-site renewable generation and faces dynamic electricity
prices and spatiotemporal variability in renewable availability. The central
question is: how can inference workloads be optimally distributed to the DCs to
minimize energy consumption, carbon emissions, and water usage while enhancing
user experience? This letter proposes a novel optimization model for LLM
service providers to reduce operational costs and environmental impacts.
Numerical results validate the efficacy of the proposed approach.

</details>


### [609] [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](https://arxiv.org/abs/2507.10510)
*Jiangkai Wu,Zhiyuan Ren,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: The paper introduces an AI-focused video communication framework called Artic, tailored to optimize real-time interactions between humans and AI models despite network challenges.


<details>
  <summary>Details</summary>
Motivation: Address the latency and transmission inefficiencies in AI Video Chats where a Multimodal Large Language Model (MLLM) is involved.

Method: Introduced Context-Aware Video Streaming to prioritize relevant video regions and Loss-Resilient Adaptive Frame Rate to handle packet loss and avoid waste. Developed a benchmark named DeViBench to analyze video streaming quality's impact on MLLM accuracy.

Result: The framework effectively reduces bitrate while preserving MLLM interaction quality, ensuring better real-time communication performance.

Conclusion: Artic represents a significant advancement in RTC for AI Video Chats, enabling more fluid and accurate interactions under network uncertainty, with potential for further solutions in the field.

Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [610] [Optimal Differentially Private Ranking from Pairwise Comparisons](https://arxiv.org/abs/2507.09388)
*T. Tony Cai,Abhinav Chakraborty,Yichen Wang*

Main category: math.ST

TL;DR: The paper introduces differentially private algorithms for ranking from pairwise comparisons, achieving optimal privacy-constrained performance with tested practical effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in ranking tasks involving sensitive data, such as recommendation systems and opinion surveys, particularly when the data are incomplete and noisy.

Method: The authors propose algorithms using edge differential privacy and individual differential privacy, including a perturbed maximum likelihood estimator and a noisy count-based method, and analyze their convergence performance.

Result: The developed algorithms achieve minimax optimal rates of convergence under privacy constraints, validated by experiments on simulated and real-world data.

Conclusion: The proposed methods effectively ensure data privacy while achieving optimal ranking performance in pairwise comparison tasks, proving their practicality in sensitive applications.

Abstract: Data privacy is a central concern in many applications involving ranking from
incomplete and noisy pairwise comparisons, such as recommendation systems,
educational assessments, and opinion surveys on sensitive topics. In this work,
we propose differentially private algorithms for ranking based on pairwise
comparisons. Specifically, we develop and analyze ranking methods under two
privacy notions: edge differential privacy, which protects the confidentiality
of individual comparison outcomes, and individual differential privacy, which
safeguards potentially many comparisons contributed by a single individual. Our
algorithms--including a perturbed maximum likelihood estimator and a noisy
count-based method--are shown to achieve minimax optimal rates of convergence
under the respective privacy constraints. We further demonstrate the practical
effectiveness of our methods through experiments on both simulated and
real-world data.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [611] [A Framework for Predictive Directional Trading Based on Volatility and Causal Inference](https://arxiv.org/abs/2507.09347)
*Ivan Letteri*

Main category: q-fin.ST

TL;DR: The paper proposes a framework combining statistical methods and machine learning to identify lead-lag relationships in financial markets, achieving a total return of 15.38% over two months.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic approach for identifying profitable trading opportunities in financial markets using volatility-based causal relationships.

Method: The study uses Gaussian Mixture Models for clustering, followed by causal inference tools such as Granger Causality and Effective Transfer Entropy, concluding with Dynamic Time Warping for optimal trade timing.

Result: The suggested strategy yielded a total return of 15.38%, outperforming Buy-and-Hold by 5% and achieving high Sharpe Ratios and win rates.

Conclusion: The approach offers an effective methodology for data-driven trading, benefiting both academic research and practical algorithmic trading implementations.

Abstract: Purpose: This study introduces a novel framework for identifying and
exploiting predictive lead-lag relationships in financial markets. We propose
an integrated approach that combines advanced statistical methodologies with
machine learning models to enhance the identification and exploitation of
predictive relationships between equities. Methods: We employed a Gaussian
Mixture Model (GMM) to cluster nine prominent stocks based on their mid-range
historical volatility profiles over a three-year period. From the resulting
clusters, we constructed a multi-stage causal inference pipeline, incorporating
the Granger Causality Test (GCT), a customised Peter-Clark Momentary
Conditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to
identify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW)
and a K-Nearest Neighbours (KNN) classifier were utilised to determine the
optimal time lag for trade execution. The resulting strategy was rigorously
backtested. Results: The proposed volatility-based trading strategy, tested
from 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The
portfolio yielded a total return of 15.38%, significantly outperforming the
10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics,
including a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain
pairs, confirmed the strategy's viability. Conclusion: This research
contributes a systematic and robust methodology for identifying profitable
trading opportunities derived from volatility-based causal relationships. The
findings have significant implications for both academic research in financial
modelling and the practical application of algorithmic trading, offering a
structured approach to developing resilient, data-driven strategies.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [612] [AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model](https://arxiv.org/abs/2507.08920)
*Changze Lv,Jiang Zhou,Siyu Long,Lihao Wang,Jiangtao Feng,Dongyu Xue,Yu Pei,Hao Wang,Zherui Zhang,Yuchen Cai,Zhiqiang Gao,Ziyuan Ma,Jiakai Hu,Chaochen Gao,Jingjing Gong,Yuxuan Song,Shuyi Zhang,Xiaoqing Zheng,Deyi Xiong,Lei Bai,Ya-Qin Zhang,Wei-Ying Ma,Bowen Zhou,Hao Zhou*

Main category: q-bio.BM

TL;DR: AMix-1 is a protein foundation model that leverages Bayesian Flow Networks and advanced training methods to achieve groundbreaking protein design and directed evolution.


<details>
  <summary>Details</summary>
Motivation: To develop a robust protein foundation model capable of unifying protein design and enhancing performance through directed evolution.

Method: AMix-1 introduces a systematic methodology combining pretraining scaling laws, emergent capability analysis, in-context learning, and test-time scaling algorithms. It incorporates multiple sequence alignments (MSAs) for in-context learning to generate coherent protein designs.

Result: AMix-1 successfully designed an improved AmeR variant with up to 50× activity enhancement and demonstrated scalable performance gains in directed evolution contexts.

Conclusion: AMix-1 sets the stage for next-generation protein engineering by enabling scalable and efficient protein design through advanced algorithms and evolutionary techniques.

Abstract: We introduce AMix-1, a powerful protein foundation model built on Bayesian
Flow Networks and empowered by a systematic training methodology, encompassing
pretraining scaling laws, emergent capability analysis, in-context learning
mechanism, and test-time scaling algorithm. To guarantee robust scalability, we
establish a predictive scaling law and reveal the progressive emergence of
structural understanding via loss perspective, culminating in a strong
1.7-billion model. Building on this foundation, we devise a multiple sequence
alignment (MSA)-based in-context learning strategy to unify protein design into
a general framework, where AMix-1 recognizes deep evolutionary signals among
MSAs and consistently generates structurally and functionally coherent
proteins. This framework enables the successful design of a dramatically
improved AmeR variant with an up to $50\times$ activity increase over its wild
type. Pushing the boundaries of protein engineering, we further empower AMix-1
with an evolutionary test-time scaling algorithm for in silico directed
evolution that delivers substantial, scalable performance gains as verification
budgets are intensified, laying the groundwork for next-generation
lab-in-the-loop protein design.

</details>


### [613] [Conformation-Aware Structure Prediction of Antigen-Recognizing Immune Proteins](https://arxiv.org/abs/2507.09054)
*Frédéric A. Dreyer,Jan Ludwiczak,Karolis Martinkus,Brennan Abanades,Robert G. Alberstein,Pan Kessel,Pranav Rao,Jae Hyeon Lee,Richard Bonneau,Andrew M. Watkins,Franziska Seeger*

Main category: q-bio.BM

TL;DR: Ibex is a novel structure prediction model for antibodies, nanobodies, and T-cell receptors, delivering high accuracy with computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for antibody and other immunoglobulin structure prediction fail to distinguish bound and unbound conformations, limiting their accuracy in diverse states.

Method: Ibex uses apo and holo structural pairs for training, enabling separate modeling for bound and unbound protein states. It is powered by a private high-resolution dataset for enhanced learning.

Result: Ibex outperforms specialized and general protein prediction tools on accuracy, especially in out-of-distribution scenarios, while also being computationally efficient.

Conclusion: Ibex provides a powerful, accurate, and efficient tool for accelerating molecular design and therapeutic developments in immunology.

Abstract: We introduce Ibex, a pan-immunoglobulin structure prediction model that
achieves state-of-the-art accuracy in modeling the variable domains of
antibodies, nanobodies, and T-cell receptors. Unlike previous approaches, Ibex
explicitly distinguishes between bound and unbound protein conformations by
training on labeled apo and holo structural pairs, enabling accurate prediction
of both states at inference time. Using a comprehensive private dataset of
high-resolution antibody structures, we demonstrate superior
out-of-distribution performance compared to existing specialized and general
protein structure prediction tools. Ibex combines the accuracy of cutting-edge
models with significantly reduced computational requirements, providing a
robust foundation for accelerating large molecule design and therapeutic
development.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [614] [Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture](https://arxiv.org/abs/2507.09158)
*Sunil Munthumoduku Krishna Murthy,Kumar Rajamani,Srividya Tirunellai Rajamani,Yupei Li,Qiyang Sun,Bjoern W. Schuller*

Main category: eess.IV

TL;DR: The study introduces a novel U-Net variation to enhance thoracic vertebrae segmentation in X-Ray images, achieving a 4.1% improvement in Dice score over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional vertebrae contouring for mobility disease analysis is manual, time-consuming, and error-prone. Automation is needed for efficiency and consistency.

Method: A novel 'sandwich' U-Net structure with dual activation functions is used to segment thoracic vertebrae from anteroposterior X-Ray images.

Result: The proposed model improved segmentation accuracy with a 4.1% higher Dice score compared to the baseline U-Net model.

Conclusion: The new U-Net variation enhances segmentation accuracy, offering a reliable and efficient solution for vertebral contouring in mobility disease analysis.

Abstract: In spinal vertebral mobility disease, accurately extracting and contouring
vertebrae is essential for assessing mobility impairments and monitoring
variations during flexion-extension movements. Precise vertebral contouring
plays a crucial role in surgical planning; however, this process is
traditionally performed manually by radiologists or surgeons, making it
labour-intensive, time-consuming, and prone to human error. In particular,
mobility disease analysis requires the individual contouring of each vertebra,
which is both tedious and susceptible to inconsistencies. Automated methods
provide a more efficient alternative, enabling vertebra identification,
segmentation, and contouring with greater accuracy and reduced time
consumption. In this study, we propose a novel U-Net variation designed to
accurately segment thoracic vertebrae from anteroposterior view on X-Ray
images. Our proposed approach, incorporating a ``sandwich" U-Net structure with
dual activation functions, achieves a 4.1\% improvement in Dice score compared
to the baseline U-Net model, enhancing segmentation accuracy while ensuring
reliable vertebral contour extraction.

</details>


### [615] [PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution](https://arxiv.org/abs/2507.09227)
*Sanyam Jain,Bruna Neves de Freitas,Andreas Basse-OConnor,Alexandros Iosifidis,Ruben Pauwels*

Main category: eess.IV

TL;DR: This paper proposes a method combining diffusion-based generation (PanoDiff) and Super-Resolution (SR) to create high-resolution synthetic dental panoramic radiographs (PRs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scarcity of publicly available medical datasets for AI research and education by generating high-quality, synthetic medical images.

Method: The authors introduced a two-step method where a low-resolution PR is generated using PanoDiff and enhanced to high-resolution using a novel transformer-based SR model that learns local and global features.

Result: The generated synthetic images achieved a Frechet inception distance score of 40.69, and clinical experts had an average accuracy of 68.5% in distinguishing real from synthetic images.

Conclusion: The proposed approach successfully creates realistic synthetic medical images that are challenging to differentiate from actual ones, providing a promising avenue for dataset enhancement in medical imaging.

Abstract: There has been increasing interest in the generation of high-quality,
realistic synthetic medical images in recent years. Such synthetic datasets can
mitigate the scarcity of public datasets for artificial intelligence research,
and can also be used for educational purposes. In this paper, we propose a
combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)
for generating synthetic dental panoramic radiographs (PRs). The former
generates a low-resolution (LR) seed of a PR (256 X 128) which is then
processed by the SR model to yield a high-resolution (HR) PR of size 1024 X
512. For SR, we propose a state-of-the-art transformer that learns local-global
relationships, resulting in sharper edges and textures. Experimental results
demonstrate a Frechet inception distance score of 40.69 between 7243 real and
synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for
real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a
diverse group of six clinical experts, all evaluating a mixture of 100
synthetic and 100 real PRs in a time-limited observation, the average accuracy
in distinguishing real from synthetic images was 68.5% (with 50% corresponding
to random guessing).

</details>


### [616] [AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)](https://arxiv.org/abs/2507.09759)
*Abdul Manaf,Nimra Mughal*

Main category: eess.IV

TL;DR: This study created a CNN-based system for accurate pediatric pneumonia diagnosis using chest X-rays, leveraging data augmentation and GANs for enhanced results.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the critical need for accurate and efficient pneumonia diagnosis in children under five, as it is a leading cause of mortality.

Method: A CNN model was trained on 5,863 pediatric X-ray images. Augmentation techniques and GANs were used to enhance data diversity and address class imbalance. The system was deployed via a Flask web app for real-time use.

Result: The model, combining original, augmented, and GAN-generated data, achieved high diagnostic accuracy and efficiency, validated by metrics like accuracy and F1 score.

Conclusion: The system demonstrates the efficacy of deep learning and GANs in pediatric pneumonia diagnosis, with significant potential for application in resource-limited clinical settings.

Abstract: Pneumonia is a leading cause of mortality in children under five, requiring
accurate chest X-ray diagnosis. This study presents a machine learning-based
Pediatric Chest Pneumonia Classification System to assist healthcare
professionals in diagnosing pneumonia from chest X-ray images. The CNN-based
model was trained on 5,863 labeled chest X-ray images from children aged 0-5
years from the Guangzhou Women and Children's Medical Center. To address
limited data, we applied augmentation techniques (rotation, zooming, shear,
horizontal flipping) and employed GANs to generate synthetic images, addressing
class imbalance. The system achieved optimal performance using combined
original, augmented, and GAN-generated data, evaluated through accuracy and F1
score metrics. The final model was deployed via a Flask web application,
enabling real-time classification with probability estimates. Results
demonstrate the potential of deep learning and GANs in improving diagnostic
accuracy and efficiency for pediatric pneumonia classification, particularly
valuable in resource-limited clinical settings
https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification

</details>


### [617] [Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network](https://arxiv.org/abs/2507.08855)
*Yang Ming,Jiang Shi Zhong,Zhou Su Juan*

Main category: eess.IV

TL;DR: The paper introduces an advanced deep learning algorithm with an asymmetric cross-modal cross-attention mechanism to accurately diagnose Alzheimer's Disease using multimodal data, achieving 94.88% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning methods struggle to effectively utilize multimodal data and often lose key information due to simple feature concatenation. This paper aims to enhance the fusion of complementary multimodal data for improved Alzheimer's Disease diagnosis.

Method: The authors propose a novel deep learning framework employing an asymmetric cross-modal cross-attention mechanism to fuse multimodal data from brain PET, MRI, genetic, and clinical sources, which enhances key information detection during inter-modal interactions.

Result: The proposed algorithm achieved an impressive accuracy of 94.88% on the test set, demonstrating its significant improvement over traditional unimodal and multimodal approaches in Alzheimer's Disease diagnosis.

Conclusion: The study highlights the effectiveness of the asymmetric cross-modal cross-attention mechanism for multimodal data fusion, offering a promising tool for assisting medical professionals in detecting Alzheimer's Disease and related conditions.

Abstract: Alzheimer's Disease (AD) is an irreversible neurodegenerative disease
characterized by progressive cognitive decline as its main symptom. In the
research field of deep learning-assisted diagnosis of AD, traditional
convolutional neural networks and simple feature concatenation methods fail to
effectively utilize the complementary information between multimodal data, and
the simple feature concatenation approach is prone to cause the loss of key
information during the process of modal fusion. In recent years, the
development of deep learning technology has brought new possibilities for
solving the problem of how to effectively fuse multimodal features. This paper
proposes a novel deep learning algorithm framework to assist medical
professionals in AD diagnosis. By fusing medical multi-view information such as
brain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance
imaging (MRI), genetic data, and clinical data, it can accurately detect the
presence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).
The innovation of the algorithm lies in the use of an asymmetric cross-modal
cross-attention mechanism, which can effectively capture the key information
features of the interactions between different data modal features. This paper
compares the asymmetric cross-modal cross-attention mechanism with the
traditional algorithm frameworks of unimodal and multimodal deep learning
models for AD diagnosis, and evaluates the importance of the asymmetric
cross-modal cross-attention mechanism. The algorithm model achieves an accuracy
of 94.88% on the test set.

</details>


### [618] [Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans](https://arxiv.org/abs/2507.08952)
*Silas Nyboe Ørting,Kristina Miger,Anne Sophie Overgaard Olesen,Mikael Ploug Boesen,Michael Brun Andersen,Jens Petersen,Olav W. Nielsen,Marleen de Bruijne*

Main category: eess.IV

TL;DR: An AI model was developed to detect acute heart failure indicators in chest CT scans with high accuracy comparable to expert radiologists.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address delays and challenges in radiological interpretations of chest CT scans for diagnosing acute heart failure, utilizing AI as a complementary diagnostic tool.

Method: The study involved training a Boosted Trees model using segmented cardiac and pulmonary CT measurements, with diagnostic labels derived from radiology reports. Shapley Additive values explained feature importance.

Result: The model achieved a ROC-AUC of 0.87, with expert review indicating errors were often due to inaccuracies in radiology reports rather than model predictions.

Conclusion: The AI model demonstrates potential for enhancing diagnostic precision by providing explainable and accurate predictions comparable to radiologist evaluations.

Abstract: Introduction: Chest CT scans are increasingly used in dyspneic patients where
acute heart failure (AHF) is a key differential diagnosis. Interpretation
remains challenging and radiology reports are frequently delayed due to a
radiologist shortage, although flagging such information for emergency
physicians would have therapeutic implication. Artificial intelligence (AI) can
be a complementary tool to enhance the diagnostic precision. We aim to develop
an explainable AI model to detect radiological signs of AHF in chest CT with an
accuracy comparable to thoracic radiologists.
  Methods: A single-center, retrospective study during 2016-2021 at Copenhagen
University Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees
model was trained to predict AHF based on measurements of segmented cardiac and
pulmonary structures from acute thoracic CT scans. Diagnostic labels for
training and testing were extracted from radiology reports. Structures were
segmented with TotalSegmentator. Shapley Additive explanations values were used
to explain the impact of each measurement on the final prediction.
  Results: Of the 4,672 subjects, 49% were female. The final model incorporated
twelve key features of AHF and achieved an area under the ROC of 0.87 on the
independent test set. Expert radiologist review of model misclassifications
found that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false
negatives were actually correct model predictions, with the errors originating
from inaccuracies in the initial radiology reports.
  Conclusion: We developed an explainable AI model with strong discriminatory
performance, comparable to thoracic radiologists. The AI model's stepwise,
transparent predictions may support decision-making.

</details>


### [619] [VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models](https://arxiv.org/abs/2507.08982)
*Hanene F. Z. Brachemi Meftah,Wassim Hamidouche,Sid Ahmed Fezza,Olivier Déforges*

Main category: eess.IV

TL;DR: The paper proposes a novel adversarial attack technique for Vision-Language Models (VLMs) to preserve user privacy by concealing sensitive regions in images while retaining the semantic integrity of the rest of the image.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of Vision-Language Models has raised concerns about user privacy, particularly regarding the inadvertent exposure of private visual information.

Method: The proposed approach involves selectively concealing information in specific Regions of Interest (ROIs) within images to prevent VLMs from accessing sensitive content. The technique ensures coherence in unmasked areas while disrupting targeted regions.

Result: The approach achieved up to 98% reduction in detecting sensitive ROIs in three state-of-the-art VLMs—LLaVA, Instruct-BLIP, and BLIP2-T5—while maintaining high semantic similarity between clean and adversarial outputs.

Conclusion: This work offers a method for privacy-conscious use of VLMs by masking sensitive content effectively and ensures its practical utility for future research by making the source code publicly available.

Abstract: Recent years have witnessed remarkable progress in developing Vision-Language
Models (VLMs) capable of processing both textual and visual inputs. These
models have demonstrated impressive performance, leading to their widespread
adoption in various applications. However, this widespread raises serious
concerns regarding user privacy, particularly when models inadvertently process
or expose private visual information. In this work, we frame the preservation
of privacy in VLMs as an adversarial attack problem. We propose a novel attack
strategy that selectively conceals information within designated Region Of
Interests (ROIs) in an image, effectively preventing VLMs from accessing
sensitive content while preserving the semantic integrity of the remaining
image. Unlike conventional adversarial attacks that often disrupt the entire
image, our method maintains high coherence in unmasked areas. Experimental
results across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and
BLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while
maintaining global image semantics intact, as confirmed by high similarity
scores between clean and adversarial outputs. We believe that this work
contributes to a more privacy conscious use of multimodal models and offers a
practical tool for further research, with the source code publicly available
at: https://github.com/hbrachemi/Vlm_defense-attack.

</details>


### [620] [prNet: Data-Driven Phase Retrieval via Stochastic Refinement](https://arxiv.org/abs/2507.09608)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: The paper introduces a novel phase retrieval framework using Langevin dynamics for efficient posterior sampling, achieving balanced reconstructions in distortion and perceptual quality.


<details>
  <summary>Details</summary>
Motivation: Existing phase retrieval methods often overemphasize pixel-wise accuracy, leaving room for improvement in balancing distortion and perceptual quality.

Method: The approach combines Langevin inference, adaptive noise scheduling, parallel reconstruction sampling, and warm-start initialization.

Result: Experiments confirm state-of-the-art performance in fidelity and perceptual quality across various benchmarks.

Conclusion: This framework provides a principled tradeoff between distortion and perception while leveraging advanced computational techniques like Langevin dynamics.

Abstract: We propose a novel framework for phase retrieval that leverages Langevin
dynamics to enable efficient posterior sampling, yielding reconstructions that
explicitly balance distortion and perceptual quality. Unlike conventional
approaches that prioritize pixel-wise accuracy, our method navigates the
perception-distortion tradeoff through a principled combination of stochastic
sampling, learned denoising, and model-based updates. The framework comprises
three variants of increasing complexity, integrating theoretically grounded
Langevin inference, adaptive noise schedule learning, parallel reconstruction
sampling, and warm-start initialization from classical solvers. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
across multiple benchmarks, both in terms of fidelity and perceptual quality.

</details>


### [621] [I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.09609)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: The paper introduces a novel phase retrieval method leveraging image-to-image diffusion models for enhanced initialization and iterative refinement, surpassing classical techniques.


<details>
  <summary>Details</summary>
Motivation: The need to improve phase retrieval performance, especially under conditions sensitive to initialization and measurement noise.

Method: A hybrid iterative technique for initialization coupled with an image-to-image diffusion-based refinement pipeline.

Result: Substantial improvements in training efficiency and reconstruction quality compared to existing techniques.

Conclusion: The proposed method delivers superior phase retrieval results, showcasing its potential for diverse applications.

Abstract: Phase retrieval involves recovering a signal from intensity-only
measurements, crucial in many fields such as imaging, holography, optical
computing, crystallography, and microscopy. Although there are several
well-known phase retrieval algorithms, including classical iterative solvers,
the reconstruction performance often remains sensitive to initialization and
measurement noise. Recently, image-to-image diffusion models have gained
traction in various image reconstruction tasks, yielding significant
theoretical insights and practical breakthroughs. In this work, we introduce a
novel phase retrieval approach based on an image-to-image diffusion framework
called Inversion by Direct Iteration. Our method begins with an enhanced
initialization stage that leverages a hybrid iterative technique, combining the
Hybrid Input-Output and Error Reduction methods and incorporating a novel
acceleration mechanism to obtain a robust crude estimate. Then, it iteratively
refines this initial crude estimate using the learned image-to-image pipeline.
Our method achieves substantial improvements in both training efficiency and
reconstruction quality. Furthermore, our approach utilizes aggregation
techniques to refine quality metrics and demonstrates superior results compared
to both classical and contemporary techniques. This highlights its potential
for effective and efficient phase retrieval across various applications.

</details>


### [622] [Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging](https://arxiv.org/abs/2507.09731)
*Robby Hoover,Nelly Elsayed,Zag ElSayed,Chengcheng Li*

Main category: eess.IV

TL;DR: This paper evaluates the robustness of pre-trained deep learning models (ResNet50, VGG16, EfficientNetv2) in classifying bone fractures from X-ray images under varying noise conditions.


<details>
  <summary>Details</summary>
Motivation: To address global healthcare disparities by assessing how the performance of pre-trained deep learning models is affected by noise in X-ray images, simulating real-world diagnostic challenges.

Method: Three pre-trained deep learning models (ResNet50, VGG16, EfficientNetv2) were tested on X-ray images with progressive noise degradation to examine their robustness in bone fracture classification.

Result: The study provides empirical evidence on how noise impacts the performance and robustness of the models, highlighting variations in their ability to classify bone fractures under degraded conditions.

Conclusion: This research establishes a practical framework for evaluating AI model degradation due to noise, offering insights into their generalizability and robustness for global healthcare applications.

Abstract: Medical Imagings are considered one of the crucial diagnostic tools for
different bones-related diseases, especially bones fractures. This paper
investigates the robustness of pre-trained deep learning models for classifying
bone fractures in X-ray images and seeks to address global healthcare disparity
through the lens of technology. Three deep learning models have been tested
under varying simulated equipment quality conditions. ResNet50, VGG16 and
EfficientNetv2 are the three pre-trained architectures which are compared.
These models were used to perform bone fracture classification as images were
progressively degraded using noise. This paper specifically empirically studies
how the noise can affect the bone fractures detection and how the pre-trained
models performance can be changes due to the noise that affect the quality of
the X-ray images. This paper aims to help replicate real world challenges
experienced by medical imaging technicians across the world. Thus, this paper
establishes a methodological framework for assessing AI model degradation using
transfer learning and controlled noise augmentation. The findings provide
practical insight into how robust and generalizable different pre-trained deep
learning powered computer vision models can be when used in different contexts.

</details>


### [623] [Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images](https://arxiv.org/abs/2507.09898)
*Alireza Golkarieha,Kiana Kiashemshakib,Sajjad Rezvani Boroujenic,Nasibeh Asadi Isakand*

Main category: eess.IV

TL;DR: The study explores U-Net models with ResNet50, VGG16, and Xception CNN backbones for lung cancer detection and segmentation in CT images, achieving high diagnostic accuracy and outperforming previous models.


<details>
  <summary>Details</summary>
Motivation: Address the need for accurate and automated diagnostic tools for lung cancer detection through effective segmentation and classification of chest CT images.

Method: The research employed U-Net architectures with different CNN backbones (ResNet50, VGG16, Xception) for segmentation, preprocessing images with CLAHE and resizing, and evaluated CNN-based and hybrid classification models using machine learning approaches. Performance was measured with metrics such as dice coefficient, accuracy, and F1-score.

Result: U-Net with ResNet50 achieved high performance in cancerous segmentation (Dice: 0.9495, Accuracy: 0.9735), U-Net with VGG16 was best for non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513), and Xception-based models achieved the highest classification accuracy (99.1 percent). Hybrid models also performed well but slightly underperformed compared to pure CNNs.

Conclusion: Integrating U-Net with advanced CNN backbones is a robust method for lung cancer segmentation and classification, advancing early diagnosis capabilities in clinical settings.

Abstract: This study investigates the effectiveness of U-Net architectures integrated
with various convolutional neural network (CNN) backbones for automated lung
cancer detection and segmentation in chest CT images, addressing the critical
need for accurate diagnostic tools in clinical settings. A balanced dataset of
832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed
using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to
128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,
VGG16, and Xception, to segment lung regions. After segmentation, CNN-based
classifiers and hybrid models combining CNN feature extraction with traditional
machine learning classifiers (Support Vector Machine, Random Forest, and
Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics
included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.
U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:
0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for
non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For
classification, the CNN model using U-Net with Xception achieved 99.1 percent
accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid
CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent
F1-score. Compared to prior methods, our framework consistently outperformed
existing models. In conclusion, combining U-Net with advanced CNN backbones
provides a powerful method for both segmentation and classification of lung
cancer in CT scans, supporting early diagnosis and clinical decision-making.

</details>


### [624] [Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction](https://arxiv.org/abs/2507.09872)
*Shengjie Liu,Lu Zhang,Siqin Wang*

Main category: eess.IV

TL;DR: The paper introduces a physics-guided deep learning framework to reconstruct high-resolution temperature data by integrating coarse Earth system models and satellite observations.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between spatial and temporal resolution in temperature data, especially under limitations influenced by current satellite technology and weather conditions.

Method: A convolutional neural network that incorporates the annual temperature cycle and uses a linear amplification term to refine coarse data from Earth system models into fine-scale temperature values.

Result: The framework demonstrated effective temperature reconstruction using two satellites (GOES-16 and Landsat) and was validated with hold-out and in situ data across four datasets.

Conclusion: This approach enables global, all-weather generation of high-resolution temperature data, improving spatiotemporal coverage beyond current constraints.

Abstract: Central to Earth observation is the trade-off between spatial and temporal
resolution. For temperature, this is especially critical because real-world
applications require high spatiotemporal resolution data. Current technology
allows for hourly temperature observations at 2 km, but only every 16 days at
100 m, a gap further exacerbated by cloud cover. Earth system models offer
continuous hourly temperature data, but at a much coarser spatial resolution
(9-31 km). Here, we present a physics-guided deep learning framework for
temperature data reconstruction that integrates these two data sources. The
proposed framework uses a convolutional neural network that incorporates the
annual temperature cycle and includes a linear term to amplify the coarse Earth
system model output into fine-scale temperature values observed from
satellites. We evaluated this framework using data from two satellites, GOES-16
(2 km, hourly) and Landsat (100 m, every 16 days), and demonstrated effective
temperature reconstruction with hold-out and in situ data across four datasets.
This physics-guided deep learning framework opens new possibilities for
generating high-resolution temperature data across spatial and temporal scales,
under all weather conditions and globally.

</details>


### [625] [IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution](https://arxiv.org/abs/2507.09923)
*Sejin Park,Sangmin Lee,Kyong Hwan Jin,Seung-Won Jung*

Main category: eess.IV

TL;DR: IM-LUT is introduced as an efficient solution for arbitrary-scale image super-resolution by leveraging interpolation blending and look-up tables to reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Address the high computational demand and memory usage of current methods for arbitrary-scale image super-resolution (ASISR), while overcoming limitations of fixed-scale LUT-based approaches.

Method: Propose a framework, IM-LUT, that uses IM-Net to predict interpolation mixing weights and then converts the weights into lightweight LUTs for efficient image reconstruction.

Result: IM-LUT demonstrates a superior balance of image quality and efficiency compared to existing ASISR techniques, validated on benchmark datasets.

Conclusion: IM-LUT is a promising approach for ASISR, especially in resource-limited settings, due to its efficient use of interpolation blending and LUTs while maintaining high reconstruction quality.

Abstract: Super-resolution (SR) has been a pivotal task in image processing, aimed at
enhancing image resolution across various applications. Recently, look-up table
(LUT)-based approaches have attracted interest due to their efficiency and
performance. However, these methods are typically designed for fixed scale
factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing
ASISR techniques often employ implicit neural representations, which come with
considerable computational cost and memory demands. To address these
limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework
that operates ASISR by learning to blend multiple interpolation functions to
maximize their representational capacity. Specifically, we introduce IM-Net, a
network trained to predict mixing weights for interpolation functions based on
local image patterns and the target scale factor. To enhance efficiency of
interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are
employed to replace computationally expensive operations, enabling lightweight
and fast inference on CPUs while preserving reconstruction quality.
Experimental results on several benchmark datasets demonstrate that IM-LUT
consistently achieves a superior balance between image quality and efficiency
compared to existing methods, highlighting its potential as a promising
solution for resource-constrained applications.

</details>


### [626] [A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion](https://arxiv.org/abs/2507.09966)
*Mingda Zhang*

Main category: eess.IV

TL;DR: The paper introduces a multi-level fusion model integrating pixel, feature, and semantic information for accurate brain tumor segmentation using MRI, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current brain tumor segmentation techniques rely heavily on visual MRI features, overlooking semantic knowledge embedded in medical reports. This limits performance due to tumor complexity and variability.

Method: The method combines 3D U-Net spatial feature extraction with semantic understanding from CLIP models using three innovative mechanisms: 3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based attention.

Result: Testing on the BraTS 2020 dataset shows the model achieved a Dice coefficient of 0.8567, a 4.8% overall improvement and a 7.3% improvement in enhancing tumor segmentation compared to traditional 3D U-Net.

Conclusion: The proposed multi-level fusion architecture effectively integrates semantic knowledge and visual features, showing significant segmentation improvements, especially for clinically critical tumor regions.

Abstract: Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is
essential for neuro-oncology diagnosis and treatment planning. Despite advances
in deep learning methods, automatic segmentation remains challenging due to
tumor morphological heterogeneity and complex three-dimensional spatial
relationships. Current techniques primarily rely on visual features extracted
from MRI sequences while underutilizing semantic knowledge embedded in medical
reports. This research presents a multi-level fusion architecture that
integrates pixel-level, feature-level, and semantic-level information,
facilitating comprehensive processing from low-level data to high-level
concepts. The semantic-level fusion pathway combines the semantic understanding
capabilities of Contrastive Language-Image Pre-training (CLIP) models with the
spatial feature extraction advantages of 3D U-Net through three mechanisms:
3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based
attention mechanisms. Experimental validation on the BraTS 2020 dataset
demonstrates that the proposed model achieves an overall Dice coefficient of
0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with
a 7.3% Dice coefficient increase in the clinically important enhancing tumor
(ET) region.

</details>


### [627] [Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)](https://arxiv.org/abs/2507.09995)
*Guohao Huo,Ruiting Dai,Hao Tang*

Main category: eess.IV

TL;DR: The paper introduces EdgeIMLocSys, a system using human feedback and the GMLN-BTS model to improve brain tumor segmentation under varying MRI scanner conditions. It achieves high accuracy (Dice score 85.1%) with a lightweight, resource-efficient design.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of varying MRI imaging quality across scanners that affects the generalization ability of brain tumor segmentation models.

Method: The proposed EdgeIMLocSys system incorporates Continuous Learning from Human Feedback with the GMLN-BTS model, which integrates a Modality-Aware Adaptive Encoder (M2AE), a Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM), and a Voxel Refinement UpSampling Module (VRUM).

Result: The GMLN-BTS model achieved a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million parameters, making it more efficient and accurate compared to existing methods.

Conclusion: EdgeIMLocSys demonstrates that lightweight, accurate, and resource-efficient models can significantly enhance brain tumor segmentation, making them suitable for real-world clinical deployment.

Abstract: Brain tumor segmentation plays a critical role in clinical diagnosis and
treatment planning, yet the variability in imaging quality across different MRI
scanners presents significant challenges to model generalization. To address
this, we propose the Edge Iterative MRI Lesion Localization System
(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to
adaptively fine-tune segmentation models based on clinician feedback, thereby
enhancing robustness to scanner-specific imaging characteristics. Central to
this system is the Graph-based Multi-Modal Interaction Lightweight Network for
Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive
Encoder (M2AE) to extract multi-scale semantic features efficiently, and a
Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model
complementary cross-modal relationships via graph structures. Additionally, we
introduce a novel Voxel Refinement UpSampling Module (VRUM) that
synergistically combines linear interpolation and multi-scale transposed
convolutions to suppress artifacts while preserving high-frequency details,
improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves
a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million
parameters, representing a 98% reduction compared to mainstream 3D Transformer
models, and significantly outperforms existing lightweight approaches. This
work demonstrates a synergistic breakthrough in achieving high-accuracy,
resource-efficient brain tumor segmentation suitable for deployment in
resource-constrained clinical environments.

</details>


### [628] [DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology](https://arxiv.org/abs/2507.10250)
*Ashkan Shakarami,Lorenzo Nicole,Rocco Cappellesso,Angelo Paolo Dei Tos,Stefano Ghidoni*

Main category: eess.IV

TL;DR: This paper introduces DepViT-CAD, an AI system for diagnosing multiple cancers from histopathological slides, leveraging a novel Multi-Attention Vision Transformer (MAViT).


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve accuracy and reliability in cancer diagnosis through advanced AI models, addressing the challenge of interpreting diverse tumor types in histopathology.

Method: The authors developed MAViT, a Multi-Attention Vision Transformer, trained on 1008 whole-slide images to diagnose 11 categories of cancer. They tested the system on independent datasets for real-world validation.

Result: DepViT-CAD demonstrated high diagnostic sensitivity: 94.11% on The Cancer Genome Atlas data and 92% on routine clinical samples.

Conclusion: The study proposes a robust, scalable, and transparent AI solution for cancer diagnostics, making the code and software publicly available to ensure reproducibility.

Abstract: Accurate and timely cancer diagnosis from histopathological slides is vital
for effective clinical decision-making. This paper introduces DepViT-CAD, a
deployable AI system for multi-class cancer diagnosis in histopathology. At its
core is MAViT, a novel Multi-Attention Vision Transformer designed to capture
fine-grained morphological patterns across diverse tumor types. MAViT was
trained on expert-annotated patches from 1008 whole-slide images, covering 11
diagnostic categories, including 10 major cancers and non-tumor tissue.
DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer
Genome Atlas and 50 routine clinical cases from pathology labs, achieving
diagnostic sensitivities of 94.11% and 92%, respectively. By combining
state-of-the-art transformer architecture with large-scale real-world
validation, DepViT-CAD offers a robust and scalable approach for AI-assisted
cancer diagnostics. To support transparency and reproducibility, software and
code will be made publicly available at GitHub.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [629] [KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.09647)
*Peican Zhu,Yubo Jing,Le Cheng,Keke Tang,Yangming Guo*

Main category: cs.MM

TL;DR: The paper proposes KEN, a novel model leveraging knowledge augmentation and emotion guidance to improve multimodal fake news detection by enhancing semantics understanding and considering emotional types.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of understanding image semantics and tailored emotional analysis in fake news detection, which are overlooked by prior research.

Method: Introduced KEN, which utilizes LVLM for semantic enhancement and world knowledge integration, along with balanced learning for modeling emotional type differences.

Result: KEN demonstrated superior performance in multimodal fake news detection through experiments on two real-world datasets.

Conclusion: The proposed KEN model effectively improves fake news detection by addressing semantic gaps and emotional type diversity, proving its superiority in experiments.

Abstract: In recent years, the rampant spread of misinformation on social media has
made accurate detection of multimodal fake news a critical research focus.
However, previous research has not adequately understood the semantics of
images, and models struggle to discern news authenticity with limited textual
information. Meanwhile, treating all emotional types of news uniformly without
tailored approaches further leads to performance degradation. Therefore, we
propose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On
the one hand, we effectively leverage LVLM's powerful semantic understanding
and extensive world knowledge. For images, the generated captions provide a
comprehensive understanding of image content and scenes, while for text, the
retrieved evidence helps break the information silos caused by the closed and
limited text and context. On the other hand, we consider inter-class
differences between different emotional types of news through balanced
learning, achieving fine-grained modeling of the relationship between emotional
types and authenticity. Extensive experiments on two real-world datasets
demonstrate the superiority of our KEN.

</details>


### [630] [ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization](https://arxiv.org/abs/2507.09945)
*Huilai Li,Yonghao Dang,Ying Xing,Yiming Wang,Jianqin Yin*

Main category: cs.MM

TL;DR: This research introduces ESG-Net, a framework addressing dense audio-visual event localization (DAVE). It improves event distinction by tackling semantic gaps and capturing event relationships.


<details>
  <summary>Details</summary>
Motivation: The authors aim to close the semantic gap between audio and visual modalities during the event localization process and to enable the system to model complex event relationships in untrimmed videos.

Method: The ESG-Net combines a multi-stage semantic guidance system (ESI) for hierarchical understanding and a MoDE module for learning dependencies, through early fusion and adaptive weight allocation mechanisms.

Result: The proposed ESG-Net method outperforms state-of-the-art approaches in terms of accuracy, while simultaneously reducing parameters and computational costs.

Conclusion: By addressing cross-modal semantic gaps and integrating multi-event dependency modeling, ESG-Net offers an efficient and accurate solution for audio-visual event localization.

Abstract: Dense audio-visual event localization (DAVE) aims to identify event
categories and locate the temporal boundaries in untrimmed videos. Most studies
only employ event-related semantic constraints on the final outputs, lacking
cross-modal semantic bridging in intermediate layers. This causes modality
semantic gap for further fusion, making it difficult to distinguish between
event-related content and irrelevant background content. Moreover, they rarely
consider the correlations between events, which limits the model to infer
concurrent events among complex scenarios. In this paper, we incorporate
multi-stage semantic guidance and multi-event relationship modeling, which
respectively enable hierarchical semantic understanding of audio-visual events
and adaptive extraction of event dependencies, thereby better focusing on
event-related information. Specifically, our eventaware semantic guided network
(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of
dependency experts (MoDE) module. ESI applys multi-stage semantic guidance to
explicitly constrain the model in learning semantic information through
multi-modal early fusion and several classification loss functions, ensuring
hierarchical understanding of event-related content. MoDE promotes the
extraction of multi-event dependencies through multiple serial mixture of
experts with adaptive weight allocation. Extensive experiments demonstrate that
our method significantly surpasses the state-of-the-art methods, while greatly
reducing parameters and computational load. Our code will be released on
https://github.com/uchiha99999/ESG-Net.

</details>


### [631] [LayLens: Improving Deepfake Understanding through Simplified Explanations](https://arxiv.org/abs/2507.10066)
*Abhijeet Narang,Parul Gupta,Liuyijia Su,Abhinav Dhall*

Main category: cs.MM

TL;DR: LayLens is a tool designed to improve deepfake understanding by simplifying technical explanations and providing visual aids, making it accessible to users of all educational backgrounds.


<details>
  <summary>Details</summary>
Motivation: Deepfake forensics often present information in technical jargon, making it challenging for non-expert users to understand. LayLens aims to bridge this gap by presenting simplified explanations and visual comparisons.

Method: The tool uses a three-stage pipeline: (1) explainable deepfake detection via forgery localization models, (2) technical explanation simplification using vision-language models, and (3) visual reconstruction of the original image by guided editing.

Result: A user study involving 15 participants demonstrated that LayLens significantly enhances clarity and reduces cognitive load. Most users reported improved confidence in identifying deepfakes.

Conclusion: LayLens represents progress toward transparent and user-friendly deepfake detection tools, promoting trust and understanding in forensic applications.

Abstract: This demonstration paper presents $\mathbf{LayLens}$, a tool aimed to make
deepfake understanding easier for users of all educational backgrounds. While
prior works often rely on outputs containing technical jargon, LayLens bridges
the gap between model reasoning and human understanding through a three-stage
pipeline: (1) explainable deepfake detection using a state-of-the-art forgery
localization model, (2) natural language simplification of technical
explanations using a vision-language model, and (3) visual reconstruction of a
plausible original image via guided image editing. The interface presents both
technical and layperson-friendly explanations in addition to a side-by-side
comparison of the uploaded and reconstructed images. A user study with 15
participants shows that simplified explanations significantly improve clarity
and reduce cognitive load, with most users expressing increased confidence in
identifying deepfakes. LayLens offers a step toward transparent, trustworthy,
and user-centric deepfake forensics.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [632] [Dynamical stability for dense patterns in discrete attractor neural networks](https://arxiv.org/abs/2507.10383)
*Uri Cohen,Máté Lengyel*

Main category: cond-mat.dis-nn

TL;DR: The paper develops a theory for analyzing the stability of fixed points in neural networks modeling biological memory, emphasizing their dependence on neural activity statistics and activation functions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenge of guaranteeing the dynamical stability of fixed points in neural networks storing discrete attractors, an essential feature for modeling biological memory.

Method: The authors derive a new theory to analyze the local stability of fixed points by studying the Jacobian spectrum, including its bulk and outliers, in networks with graded neural activities and noise.

Result: The study established that fixed points are stable below a critical load, different from classical critical capacity, and identified key influences like neural activity statistics and single-neuron activation functions.

Conclusion: Threshold-linear activation functions and sparse-like patterns offer computational advantages, enhancing the stability and performance of neural networks storing multiple discrete attractors.

Abstract: Neural networks storing multiple discrete attractors are canonical models of
biological memory. Previously, the dynamical stability of such networks could
only be guaranteed under highly restrictive conditions. Here, we derive a
theory of the local stability of discrete fixed points in a broad class of
networks with graded neural activities and in the presence of noise. By
directly analyzing the bulk and outliers of the Jacobian spectrum, we show that
all fixed points are stable below a critical load that is distinct from the
classical \textit{critical capacity} and depends on the statistics of neural
activities in the fixed points as well as the single-neuron activation
function. Our analysis highlights the computational benefits of
threshold-linear activation and sparse-like patterns.

</details>


### [633] [A CLuP algorithm to practically achieve $\sim 0.76$ SK--model ground state free energy](https://arxiv.org/abs/2507.09247)
*Mihailo Stojnic*

Main category: cond-mat.dis-nn

TL;DR: The paper proposes an algorithm (CLuP-SK) to determine the ground state free energy of the Sherrington-Kirkpatrick spin glass model, achieving near-optimal results for large $n$ and effectively making the problem typically easy.


<details>
  <summary>Details</summary>
Motivation: To address whether the constant computational gap in approximating the ground state free energy of SK spin glass models can be closed, making the problem computationally easier in practice.

Method: The study introduces a Controlled Loosening-up (CLuP-SK) algorithm specifically for non-planted SK models and analyzes its success using random duality theory and theoretical predictions.

Result: The CLuP-SK algorithm achieved $
≈ 0.76$ ground state free energy for values of n in the order of thousands, closely approaching the theoretical limit of $≈ 0.763$ as $n$ approaches infinity.

Conclusion: The results demonstrate that the problem of computing SK model's near ground state free energy is rendered typically easy for practical purposes through the proposed algorithm, supported by strong agreement between theoretical predictions and computational results.

Abstract: We consider algorithmic determination of the $n$-dimensional
Sherrington-Kirkpatrick (SK) spin glass model ground state free energy. It
corresponds to a binary maximization of an indefinite quadratic form and under
the \emph{worst case} principles of the classical NP complexity theory it is
hard to approximate within a $\log(n)^{const.}$ factor. On the other hand, the
SK's random nature allows (polynomial) spectral methods to \emph{typically}
approach the optimum within a constant factor. Naturally one is left with the
fundamental question: can the residual (constant) \emph{computational gap} be
erased?
  Following the success of \emph{Controlled Loosening-up} (CLuP) algorithms in
planted models, we here devise a simple practical CLuP-SK algorithmic procedure
for (non-planted) SK models. To analyze the \emph{typical} success of the
algorithm we associate to it (random) CLuP-SK models. Further connecting to
recent random processes studies [94,97], we characterize the models and CLuP-SK
algorithm via fully lifted random duality theory (fl RDT) [98]. Moreover,
running the algorithm we demonstrate that its performance is in an excellent
agrement with theoretical predictions. In particular, already for $n$ on the
order of a few thousands CLuP-SK achieves $\sim 0.76$ ground state free energy
and remarkably closely approaches theoretical $n\rightarrow\infty$ limit
$\approx 0.763$. For all practical purposes, this renders computing SK model's
near ground state free energy as a \emph{typically} easy problem.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [634] [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](https://arxiv.org/abs/2507.10201)
*Gleb Shishaev,Vasily Demyanov,Daniel Arnold*

Main category: stat.AP

TL;DR: The paper introduces a graph-based variational autoencoder for reservoir modeling that can handle geological uncertainty using lower-dimensional latent space.


<details>
  <summary>Details</summary>
Motivation: To better manage uncertainty in geological scenarios by introducing a graph-based modeling method as an alternative to traditional lattice-based approaches.

Method: Employing a graph-based generative model combined with Geodesic metrics to control geological realism and analyzing latent space using PCA, t-SNE, and TDA.

Result: Demonstrated the methodology's viability through experiments with synthetic datasets representing two geological scenarios, successfully analyzing latent space structure.

Conclusion: Graph-based variational autoencoder approach was effective in managing geological uncertainty and provided insightful latent space analysis.

Abstract: The graph-based variational autoencoder represents an architecture that can
handle the uncertainty of different geological scenarios, such as depositional
or structural, through the concept of a lowerdimensional latent space. The main
difference from recent studies is utilisation of a graph-based approach in
reservoir modelling instead of the more traditional lattice-based deep learning
methods. We provide a solution to implicitly control the geological realism
through the latent variables of a generative model and Geodesic metrics. Our
experiments of AHM with synthetic dataset that consists of 3D realisations of
channelised geological representations with two distinct scenarios with one and
two channels shows the viability of the approach. We offer in-depth analysis of
the latent space using tools such as PCA, t-SNE, and TDA to illustrate its
structure.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [635] [Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing](https://arxiv.org/abs/2507.09776)
*Mihir Kavishwar,Naresh Shanbhag*

Main category: eess.SP

TL;DR: This paper improves the energy efficiency of analog in-memory computing (AIMC) by proposing a new approach to optimize analog-to-digital converter (ADC) parameters, significantly reducing precision requirements while maintaining computational accuracy.


<details>
  <summary>Details</summary>
Motivation: The high energy cost of column ADCs in AIMC systems limits their energy efficiency, and current methods overestimate ADC precision requirements, leading to inefficiencies.

Method: The authors developed analytical expressions for compute signal-to-noise ratio (CSNR) and proposed a new algorithm, CACTUS, to determine CSNR-optimal ADC parameters.

Result: CACTUS reduces ADC precision requirements by 3 bits and achieves 6dB higher CSNR compared to traditional methods using a behavioral model of SRAM-based AIMC in a 28nm CMOS process.

Conclusion: Adopting CSNR-optimal ADCs instead of SQNR-optimal ADCs can enhance computational accuracy and energy efficiency, making AIMC systems more practical for machine learning and signal processing.

Abstract: Analog in-memory computing (AIMC) is an energy-efficient alternative to
digital architectures for accelerating machine learning and signal processing
workloads. However, its energy efficiency is limited by the high energy cost of
the column analog-to-digital converters (ADCs). Reducing the ADC precision is
an effective approach to lowering its energy cost. However, doing so also
reduces the AIMC's computational accuracy thereby making it critical to
identify the minimum precision required to meet a target accuracy. Prior works
overestimate the ADC precision requirements by modeling quantization error as
input-independent noise, maximizing the signal-to-quantization-noise ratio
(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address
these limitations by developing analytical expressions for estimating the
compute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and
propose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a
circuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we
show that for a 256-dimensional binary dot product, CACTUS reduces the ADC
precision requirements by 3b while achieving 6dB higher CSNR over prior
methods. We also delineate operating conditions under which our proposed
CSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.

</details>


### [636] [Intrinsic frequency distribution characterises neural dynamics](https://arxiv.org/abs/2507.10145)
*Ryohei Fukuma,Yoshinobu Kawahara,Okito Yamashita,Kei Majima,Haruhiko Kishima,Takufumi Yanagisawa*

Main category: eess.SP

TL;DR: The paper introduces a novel application of dynamic mode decomposition (DMD) to characterize neural activities using distributions of intrinsic frequencies.


<details>
  <summary>Details</summary>
Motivation: Understanding and distinguishing nonlinear spatiotemporal dynamics in multivariate time series, such as brain activity, is essential for medical and scientific applications.

Method: Dynamic mode decomposition (DMD) was applied to electroencephalograms (EEGs) of healthy subjects and patients, deriving the distribution of intrinsic frequencies and comparing them to discrete Fourier transform (DFT) amplitude spectra.

Result: The distribution of intrinsic frequencies obtained through DMD distinguished patients with dementia and Parkinson's from healthy subjects with higher accuracy than traditional DFT methods.

Conclusion: The distribution of DMD frequencies is a promising biomarker for neural activity characterization, offering advantages over traditional Fourier-based approaches.

Abstract: Decomposing multivariate time series with certain basic dynamics is crucial
for understanding, predicting and controlling nonlinear spatiotemporally
dynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method
for decomposing nonlinear spatiotemporal dynamics into several basic dynamics
(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,
unlike Fourier transform-based methods, which are used to decompose a
single-channel signal into the amplitudes of sinusoidal waves with discrete
frequencies at a regular interval, DMD can derive the intrinsic frequencies of
a multichannel signal on the basis of the available data; furthermore, it can
capture nonstationary components such as alternations between states with
different intrinsic frequencies. Here, we propose the use of the distribution
of intrinsic frequencies derived from DMDs (DM frequencies) to characterise
neural activities. The distributions of DM frequencies in the
electroencephalograms of healthy subjects and patients with dementia or
Parkinson's disease in a resting state were evaluated. By using the
distributions, these patients were distinguished from healthy subjects with
significantly greater accuracy than when using amplitude spectra derived by
discrete Fourier transform. This finding suggests that the distribution of DM
frequencies exhibits distinct behaviour from amplitude spectra, and therefore,
the distribution may serve as a new biomarker by characterising the nonlinear
spatiotemporal dynamics of electrophysiological signals.

</details>


### [637] [LNN-powered Fluid Antenna Multiple Access](https://arxiv.org/abs/2507.08821)
*Pedro D. Alvim,Hugerles S. Silva,Ugo S. Dias,Osamah S. Badarneh,Felipe A. P. Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: The paper applies liquid neural networks (LNNs) to optimize port selection in fluid antenna systems, achieving better performance with lower outage probabilities.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of best-port selection in fluid antenna systems under limited port observations, particularly in the context of multiple-access scenarios.

Method: The researchers use LNNs for optimal port prediction in fluid antenna systems and optimize LNN architectures for various observation scenarios through hyperparameter tuning.

Result: The proposed approach achieves lower outage probability compared to existing methods, improving system performance.

Conclusion: Liquid neural networks are effective in addressing port selection challenges in fluid antenna systems, demonstrating significant improvements in reliability and efficiency.

Abstract: Fluid antenna systems represent an innovative approach in wireless
communication, recently applied in multiple access to optimize the
signal-to-interference-plus-noise ratio through port selection. This letter
frames the port selection problem as a multi-label classification task for the
first time, improving best-port selection with limited port observations. We
address this challenge by leveraging liquid neural networks (LNNs) to predict
the optimal port under emerging fluid antenna multiple access scenarios
alongside a more general $\alpha$-$\mu$ fading model. We also apply
hyperparameter optimization to refine LNN architectures for different
observation scenarios. Our approach yields lower outage probability values than
existing methods.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [638] [Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers](https://arxiv.org/abs/2507.08882)
*Janaki Viswanathan,Alexander Blatt,Konrad Hagemann,Dietrich Klakow*

Main category: cs.SD

TL;DR: This paper addresses stress detection in air traffic control (ATC) through deep learning models applied to anonymized voice data, achieving high accuracy while ensuring privacy compliance.


<details>
  <summary>Details</summary>
Motivation: Stress detection in ATC is critical due to its high-stakes environment, but privacy laws, like GDPR, impose restrictions on using voice data. The paper explores if effective stress detection is possible while complying with such regulations.

Method: The study anonymized ATCO voice data and evaluated different deep learning architectures for stress detection. They tested performance on the SUSAS dataset and an in-house ATC simulation dataset, both anonymized.

Result: The proposed models achieved a stress detection accuracy of 93.6% on the SUSAS dataset and 80.1% on the ATC simulation dataset, demonstrating effective stress detection with anonymized data.

Conclusion: The results highlight that anonymization can preserve privacy without significantly hindering the performance of deep-learning-based stress detection models.

Abstract: Air traffic control (ATC) demands multi-tasking under time pressure with high
consequences of an error. This can induce stress. Detecting stress is a key
point in maintaining the high safety standards of ATC. However, processing ATC
voice data entails privacy restrictions, e.g. the General Data Protection
Regulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with
these restrictions. In this paper, different architectures for stress detection
for anonymized ATCO speech are evaluated. Our best networks reach a stress
detection accuracy of 93.6% on an anonymized version of the Speech Under
Simulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our
anonymized ATC simulation dataset. This shows that privacy does not have to be
an impediment in building well-performing deep-learning-based models.

</details>


### [639] [Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning](https://arxiv.org/abs/2507.09310)
*Dominika Woszczyk,Manuel Sam Ribeiro,Thomas Merritt,Daniel Korzekwa*

Main category: cs.SD

TL;DR: The paper develops and evaluates voice conversion (VC) models for Lombard speaking style transfer, aiming to improve intelligibility and speaker similarity.


<details>
  <summary>Details</summary>
Motivation: Lombard TTS systems enhance intelligibility in challenging conditions but require extensive data, which is difficult to obtain due to recording challenges. VC provides a way to augment such data.

Method: The study explores implicit and explicit acoustic feature conditioning in VC models to transfer Lombard speaking style while preserving speaker identity.

Result: The proposed implicit conditioning approach achieves intelligibility gains comparable to explicit conditioning while maintaining speaker similarity.

Conclusion: Implicit feature conditioning in VC models is both effective and efficient for transferring Lombard speaking style, offering a viable alternative to explicit conditioning.

Abstract: Text-to-Speech (TTS) systems in Lombard speaking style can improve the
overall intelligibility of speech, useful for hearing loss and noisy
conditions. However, training those models requires a large amount of data and
the Lombard effect is challenging to record due to speaker and noise
variability and tiring recording conditions. Voice conversion (VC) has been
shown to be a useful augmentation technique to train TTS systems in the absence
of recorded data from the target speaker in the target speaking style. In this
paper, we are concerned with Lombard speaking style transfer. Our goal is to
convert speaker identity while preserving the acoustic attributes that define
the Lombard speaking style. We compare voice conversion models with implicit
and explicit acoustic feature conditioning. We observe that our proposed
implicit conditioning strategy achieves an intelligibility gain comparable to
the model conditioned on explicit acoustic features, while also preserving
speaker similarity.

</details>


### [640] [Evaluating Fake Music Detection Performance Under Audio Augmentations](https://arxiv.org/abs/2507.10447)
*Tomasz Sroka,Tomasz Wężowicz,Dominik Sidorczuk,Mateusz Modrzejewski*

Main category: cs.SD

TL;DR: Generative audio models make detecting fake music challenging. This study tests the robustness of fake detection systems against audio modifications, revealing weaknesses in existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing difficulty in identifying generated (fake) music due to the advancement in generative audio models.

Method: A dataset of real and synthetic music, generated by multiple systems, was subjected to diverse audio transformations. A state-of-the-art detection model was then evaluated under these conditions.

Result: The performance of the musical deepfake detection model drops substantially even with minor audio augmentations.

Conclusion: Current detection models lack robustness against audio augmentations, indicating the need for improved systems to reliably classify fake and real music.

Abstract: With the rapid advancement of generative audio models, distinguishing between
human-composed and generated music is becoming increasingly challenging. As a
response, models for detecting fake music have been proposed. In this work, we
explore the robustness of such systems under audio augmentations. To evaluate
model generalization, we constructed a dataset consisting of both real and
synthetic music generated using several systems. We then apply a range of audio
transformations and analyze how they affect classification accuracy. We test
the performance of a recent state-of-the-art musical deepfake detection model
in the presence of audio augmentations. The performance of the model decreases
significantly even with the introduction of light augmentations.

</details>


### [641] [MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients](https://arxiv.org/abs/2507.09750)
*Enric Gusó,Joanna Luberadzka,Umut Sayin,Xavier Serra*

Main category: cs.SD

TL;DR: This paper explores enhancing ecological validity in synthetic room impulse response datasets for monoaural speech enhancement. By examining strategies like multiband absorption coefficients, directional sources, and mesh-based responses, they assess improved performance using DeepFilternet3.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in ecological validity in synthetic RIR datasets for monoaural speech enhancement tasks, impacting performance evaluations on real-world scenarios.

Method: They implement three features—multiband absorption coefficients, source directivity, and receiver directivity—on ISM-based RIRs and also explore mesh-based RIRs from SoundSpaces. Performance is tested using DeepFilternet3 on both objective and subjective metrics.

Result: Using frequency-dependent absorption coefficients for RIRs resulted in better performance: +0.51dB SDR and +8.9 MUSHRA score compared to traditional methods.

Conclusion: Frequency-dependent acoustic absorption improves alignment between synthetic and real RIR datasets. Their MB-RIR dataset achieves notable gains and is publicly available for future research.

Abstract: We investigate the effects of four strategies for improving the ecological
validity of synthetic room impulse response (RIR) datasets for monoaural Speech
Enhancement (SE). We implement three features on top of the traditional image
source method-based (ISM) shoebox RIRs: multiband absorption coefficients,
source directivity and receiver directivity. We additionally consider
mesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3
model for each RIR dataset and evaluate the performance on a test set of real
RIRs both objectively and subjectively. We find that RIRs which use
frequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain
+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs
dataset is publicly available for free download.

</details>


### [642] [AudioMAE++: learning better masked audio representations with SwiGLU FFNs](https://arxiv.org/abs/2507.10464)
*Sarthak Yadav,Sergios Theodoridis,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: AudioMAE++ is a revamped masked autoencoder for audio spectrograms, enhancing audio representation learning with architectural advancements, and demonstrating superior performance across diverse tasks compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing audio masked autoencoders that rely on traditional transformer architecture and to incorporate modern transformer advancements.

Method: Proposes AudioMAE++ incorporating macaron-style transformer blocks and gated linear units, pretrained on the AudioSet dataset.

Result: AudioMAE++ outperformed existing MAE approaches on 10 downstream tasks, excelling in both audio classification and speech-based benchmarks.

Conclusion: AudioMAE++ leverages architectural enhancements to set new standards in self-supervised audio representation learning with better performance and scalability.

Abstract: Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged
as a prominent approach for learning self-supervised audio representations.
While several recent papers have evaluated key aspects of training MAEs on
audio data, the majority of these approaches still leverage vanilla transformer
building blocks, whereas the transformer community has seen steady integration
of newer architectural advancements. In this work, we propose AudioMAE++, a
revamped audio masked autoencoder with two such enhancements, namely
macaron-style transformer blocks with gated linear units. When pretrained on
the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE
based approaches on 10 diverse downstream tasks, demonstrating excellent
performance on audio classification and speech-based benchmarks. The proposed
AudioMAE++ models also demonstrate excellent scaling characteristics,
outperforming directly comparable standard MAE baselines with up to 4x more
parameters.

</details>


### [643] [WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling](https://arxiv.org/abs/2507.10534)
*Qihui Yang,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Main category: cs.SD

TL;DR: WildFX provides a solution for AI-driven modeling of professional Digital Signal Processing (DSP) workflows using a Docker pipeline with rich audio mixing datasets and enhanced integration capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges faced by AI approaches in replicating the complex signal flows and parameter interactions within professional DSP workflows.

Method: The authors introduce a Docker-powered pipeline called WildFX, incorporating multi-track audio mixing datasets, a professional DAW backend, and support for standard plugin formats to enable detailed structural complexity and efficient parallelized processing.

Result: Experiments show that WildFX can perform blind estimations of mixing graphs and gain parameters, and successfully bridge the gap between AI research and practical DSP needs.

Conclusion: WildFX provides a robust framework that enhances AI-driven DSP workflows, making it comparable to professional mixing practices and accessible for further AI research and development.

Abstract: Despite rapid progress in end-to-end AI music generation, AI-driven modeling
of professional Digital Signal Processing (DSP) workflows remains challenging.
In particular, while there is growing interest in neural black-box modeling of
audio effect graphs (e.g. reverb, compression, equalization), AI-based
approaches struggle to replicate the nuanced signal flow and parameter
interactions used in professional workflows. Existing differentiable plugin
approaches often diverge from real-world tools, exhibiting inferior performance
relative to simplified neural controllers under equivalent computational
constraints. We introduce WildFX, a pipeline containerized with Docker for
generating multi-track audio mixing datasets with rich effect graphs, powered
by a professional Digital Audio Workstation (DAW) backend. WildFX supports
seamless integration of cross-platform commercial plugins or any plugins in the
wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,
sidechains, crossovers) and achieving efficient parallelized processing. A
minimalist metadata interface simplifies project/plugin configuration.
Experiments demonstrate the pipeline's validity through blind estimation of
mixing graphs, plugin/gain parameters, and its ability to bridge AI research
with practical DSP demands. The code is available on:
https://github.com/IsaacYQH/WildFX.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [644] [Toolsuite for Implementing Multiagent Systems Based on Communication Protocols](https://arxiv.org/abs/2507.10324)
*Amit K. Chopra,Samuel H. Christie V,Munindar P. Singh*

Main category: cs.MA

TL;DR: This paper discusses Interaction-Oriented Programming (IOP) for multiagent systems, introducing a software suite for protocol verification and agent implementation.


<details>
  <summary>Details</summary>
Motivation: To enhance multiagent system development by focusing on interaction modeling through flexible protocols and ease their verification and implementation.

Method: Development of a comprehensive software suite including tools for protocol verification (liveness, safety) and middleware for agent implementation.

Result: The software suite facilitates effective and efficient development of multiagent systems, ensuring properties like liveness and safety.

Conclusion: IOP and the accompanying software support simplify the creation and management of multiagent systems by focusing on interaction protocol efficiency and agent interaction implementation.

Abstract: Interaction-Oriented Programming (IOP) is an approach to building a
multiagent system by modeling the interactions between its roles via a flexible
interaction protocol and implementing agents to realize the interactions of the
roles they play in the protocol.
  In recent years, we have developed an extensive suite of software that
enables multiagent system developers to apply IOP. These include tools for
efficiently verifying protocols for properties such as liveness and safety and
middleware that simplifies the implementation of agents. This paper presents
some of that software suite.

</details>


### [645] [AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design](https://arxiv.org/abs/2507.09965)
*Weiyu Chen,Chengjie Liu,Wenhao Huang,Jinyang Lyu,Mingqian Yang,Yuan Du,Li Du,Jun Yang*

Main category: cs.MA

TL;DR: AnalogTester leverages large language models (LLMs) to automate testbench generation for analog circuits, addressing a key manual bottleneck in circuit design.


<details>
  <summary>Details</summary>
Motivation: To overcome the time-intensive and manual nature of testbench construction, especially in replicating designs from academic papers, and to advance automation in analog circuit design.

Method: Utilization of an LLM-powered pipeline consisting of domain knowledge integration, paper information extraction, simulation scheme synthesis, and testbench code generation.

Result: Automatic testbench generation was successfully demonstrated for operational amplifiers (op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs).

Conclusion: AnalogTester streamlines automation in analog circuit design and creates foundational datasets for LLM specialization in this domain.

Abstract: Recent advancements have demonstrated the significant potential of large
language models (LLMs) in analog circuit design. Nevertheless, testbench
construction for analog circuits remains manual, creating a critical bottleneck
in achieving fully automated design processes. Particularly when replicating
circuit designs from academic papers, manual Testbench construction demands
time-intensive implementation and frequent adjustments, which fails to address
the dynamic diversity and flexibility requirements for automation. AnalogTester
tackles automated analog design challenges through an LLM-powered pipeline: a)
domain-knowledge integration, b) paper information extraction, c) simulation
scheme synthesis, and d) testbench code generation with Tsinghua Electronic
Design (TED). AnalogTester has demonstrated automated Testbench generation
capabilities for three fundamental analog circuit types: operational amplifiers
(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while
maintaining a scalable framework for adaptation to broader circuit topologies.
Furthermore, AnalogTester can generate circuit knowledge data and TED code
corpus, establishing fundamental training datasets for LLM specialization in
analog circuit design automation.

</details>


### [646] [Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents](https://arxiv.org/abs/2507.08944)
*Enhao Zhang,Erkang Zhu,Gagan Bansal,Adam Fourney,Hussein Mozannar,Jack Gerrits*

Main category: cs.MA

TL;DR: M1-Parallel leverages parallel multi-agent teams with asynchronous messaging to reduce latency or improve task completion rates in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent systems excel at complex task handling but suffer from high latency due to iterative reasoning cycles.

Method: The framework runs multiple multi-agent teams concurrently with an event-driven, asynchronous communication model to explore various solution paths.

Result: M1-Parallel achieved up to 2.2× speedup or higher task completion rates in experiments on complex tasks.

Conclusion: Parallel execution strategies can optimize multi-agent systems for high-complexity tasks, though diversity strategies beyond repeated sampling showed no additional benefits.

Abstract: Large language model (LLM)-based multi-agent systems have demonstrated
remarkable promise for tackling complex tasks by breaking them down into
subtasks that are iteratively planned, executed, observed, and refined. Despite
their effectiveness, these systems often incur high latency because real-world
problems frequently demand multiple iterative cycles of reasoning steps. To
address this challenge, we propose M1-Parallel, a framework that concurrently
runs multiple multi-agent teams in parallel to uncover distinct solution paths.
By leveraging an event-driven communication model with asynchronous messaging,
M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to
either reduce end-to-end latency or boost task completion rates. Our
experiments on complex tasks show that M1-Parallel with early termination
achieves up to $2.2\times$ speedup while preserving accuracy, and that
M1-Parallel with aggregation yields higher task completion rates. We further
investigate strategies aimed at encouraging diverse execution plans but observe
no additional performance gains over repeated sampling. Overall, these findings
underscore the potential of parallel plan execution for optimizing multi-agent
systems for real-world, high-complexity reasoning tasks.

</details>


### [647] [How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08960)
*Andrew Estornell,Jean-Francois Ton,Muhammad Faaiz Taufiq,Hang Li*

Main category: cs.MA

TL;DR: The paper introduces a hierarchical multi-agent framework where a single leader LLM is trained to coordinate other untrained models, leading to improved performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: While large language models (LLMs) perform strongly on complex reasoning tasks, there is room for improvement by leveraging multiple models. Existing multi-agent frameworks, however, are computationally expensive to train and run.

Method: The authors propose Multi-agent guided Leader Policy Optimization (MLPO), which trains a single leader LLM to synthesize agent inputs and responses without needing auxiliary networks or explicit feedback.

Result: The approach successfully improves performance on benchmarks like Big-Bench Hard (BBH), MATH, and MMLU, outperforming both single-agent and multi-agent baselines.

Conclusion: The study demonstrates the efficiency and efficacy of training an adaptable leader LLM for coordinating collaborative reasoning in multi-agent systems.

Abstract: Large Language Models (LLMs) have achieved strong performance on a wide range
of complex reasoning tasks, yet further gains are often possible by leveraging
the complementary strengths of multiple models. While multi-agent frameworks
can improve solution quality by leveraging multiple LLMs, existing methods are
often computationally expensive, both at training and inference time. In this
work, we introduce a hierarchical multi-agent framework that addresses these
challenges by training only a single leader LLM to coordinate a team of
untrained peer agents. To this end, we propose Multi-agent guided Leader Policy
\textbf{O}ptimization (MLPO), a novel approach which trains the leader to
evaluate and synthesize agent responses without auxiliary value networks or
explicit agent feedback. Leaders trained with MLPO exhibit improved performance
not only when interacting with the agent team at inference time, but also enjoy
improved performance when deployed in single-agent settings without the team.
Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our
framework achieves substantial performance improvements over both single-agent
and multi-agent baselines. Our results highlight the effectiveness and
efficiency of training a single, flexible leader for collaborative reasoning in
multi-agent LLM systems.

</details>


### [648] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Main category: cs.MA

TL;DR: The paper introduces TinyTroupe, an LLM-powered MAS toolkit for detailed human behavior simulation.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in existing MAS tools, such as lack of fine-grained persona specification, population sampling, experimentation support, and validation, essential for behavioral studies and simulations.

Method: TinyTroupe enables detailed persona definitions and programmatic control through LLM-driven mechanisms, with representative examples provided for its application.

Result: The toolkit's quantitative and qualitative evaluations highlight its capabilities, limitations, and trade-offs in behavioral problem formulation and solution.

Conclusion: TinyTroupe is a novel simulation toolkit, offering practical solutions for social simulations, and is available as an open-source Python library.

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


### [649] [Large Population Models](https://arxiv.org/abs/2507.09901)
*Ayush Chopra*

Main category: cs.MA

TL;DR: The paper introduces Large Population Models (LPMs) for simulating societies at scale using advanced computational, mathematical, and privacy-preserving methods.


<details>
  <summary>Details</summary>
Motivation: To address societal challenges arising from collective behavior, such as pandemics, climate change, and supply chain issues.

Method: LPMs utilize efficient computational simulations for millions of agents, real-world data learning frameworks, and privacy-preserving communication protocols.

Result: LPMs enable researchers to analyze system-level outcomes from agent interactions and test interventions virtually before physical implementation.

Conclusion: LPMs provide a new AI approach focusing on "digital societies" to better understand collective phenomena, complementing current advances in individual agent capabilities.

Abstract: Many of society's most pressing challenges, from pandemic response to supply
chain disruptions to climate adaptation, emerge from the collective behavior of
millions of autonomous agents making decisions over time. Large Population
Models (LPMs) offer an approach to understand these complex systems by
simulating entire populations with realistic behaviors and interactions at
unprecedented scale. LPMs extend traditional modeling approaches through three
key innovations: computational methods that efficiently simulate millions of
agents simultaneously, mathematical frameworks that learn from diverse
real-world data streams, and privacy-preserving communication protocols that
bridge virtual and physical environments. This allows researchers to observe
how agent behavior aggregates into system-level outcomes and test interventions
before real-world implementation. While current AI advances primarily focus on
creating "digital humans" with sophisticated individual capabilities, LPMs
develop "digital societies" where the richness of interactions reveals emergent
phenomena. By bridging individual agent behavior and population-scale dynamics,
LPMs offer a complementary path in AI research illuminating collective
intelligence and providing testing grounds for policies and social innovations
before real-world deployment. We discuss the technical foundations and some
open problems here. LPMs are implemented by the AgentTorch framework
(github.com/AgentTorch/AgentTorch)

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [650] [Physics-informed neural networks for high-dimensional solutions and snaking bifurcations in nonlinear lattices](https://arxiv.org/abs/2507.09782)
*Muhammad Luthfi Shahab,Fidya Almira Suheri,Rudy Kusdiantara,Hadi Susanto*

Main category: math.NA

TL;DR: The paper proposes using physics-informed neural networks (PINNs) for solving challenges in nonlinear lattice systems, such as solution approximation, bifurcation diagram construction, and stability analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods face limitations in addressing high-dimensional nonlinear lattice systems due to computational inefficiencies.

Method: The framework employs PINNs optimized with the Levenberg-Marquardt algorithm and integrates stochastic sampling for efficiency. PINNs are extended with continuation methods for bifurcation analysis and adapted for stability assessments using eigenvector computations.

Result: Experiments conducted using the discrete Allen-Cahn equation show that the framework achieves accuracy comparable to or superior to traditional methods, especially in high-dimensional cases.

Conclusion: The proposed PINN-based framework demonstrates scalability and efficiency, positioning it as a promising tool for analyzing complex nonlinear lattice systems.

Abstract: This paper introduces a framework based on physics-informed neural networks
(PINNs) for addressing key challenges in nonlinear lattices, including solution
approximation, bifurcation diagram construction, and linear stability analysis.
We first employ PINNs to approximate solutions of nonlinear systems arising
from lattice models, using the Levenberg-Marquardt algorithm to optimize
network weights for greater accuracy. To enhance computational efficiency in
high-dimensional settings, we integrate a stochastic sampling strategy. We then
extend the method by coupling PINNs with a continuation approach to compute
snaking bifurcation diagrams, incorporating an auxiliary equation to
effectively track successive solution branches. For linear stability analysis,
we adapt PINNs to compute eigenvectors, introducing output constraints to
enforce positivity, in line with Sturm-Liouville theory. Numerical experiments
are conducted on the discrete Allen-Cahn equation with cubic and quintic
nonlinearities in one to five spatial dimensions. The results demonstrate that
the proposed approach achieves accuracy comparable to, or better than,
traditional numerical methods, especially in high-dimensional regimes where
computational resources are a limiting factor. These findings highlight the
potential of neural networks as scalable and efficient tools for the study of
complex nonlinear lattice systems.

</details>


### [651] [Energy Dissipation Rate Guided Adaptive Sampling for Physics-Informed Neural Networks: Resolving Surface-Bulk Dynamics in Allen-Cahn Systems](https://arxiv.org/abs/2507.09757)
*Chunyan Li,Wenkai Yu,Qi Wang*

Main category: math.NA

TL;DR: This paper introduces EDRAS, an adaptive sampling method that improves PINNs for solving thermodynamically consistent PDEs, demonstrating significant error reduction using the Allen-Cahn model.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of Physics-Informed Neural Networks (PINNs) in solving PDEs related to thermodynamic processes over complex geometries.

Method: The authors developed EDRAS, leveraging the energy dissipation rate density to dynamically and adaptively re-sample collocation points during training.

Result: EDRAS showed up to sixfold reduction in relative mean square error compared to traditional methods when applied to the Allen-Cahn model in irregular geometries.

Conclusion: EDRAS enhances the physical accuracy, computational efficiency, and versatility of PINN frameworks, making them robust tools for solving complex thermodynamic equations over arbitrary domains.

Abstract: We introduce the Energy Dissipation Rate guided Adaptive Sampling (EDRAS)
strategy, a novel method that substantially enhances the performance of
Physics-Informed Neural Networks (PINNs) in solving thermodynamically
consistent partial differential equations (PDEs) over arbitrary domains. EDRAS
leverages the local energy dissipation rate density as a guiding metric to
identify and adaptively re-sample critical collocation points from both the
interior and boundary of the computational domain. This dynamical sampling
approach improves the accuracy of residual-based PINNs by aligning the training
process with the underlying physical structure of the system. In this study, we
demonstrate the effectiveness of EDRAS using the Allen-Cahn phase field model
in irregular geometries, achieving up to a sixfold reduction in the relative
mean square error compared to traditional residual-based adaptive refinement
(RAR) methods. Moreover, we compare EDRAS with other residual-based adaptive
sampling approaches and show that EDRAS is not only computationally more
efficient but also more likely to identify high-impact collocation points.
Through numerical solutions of the Allen-Cahn equation with both static
(Neumann) and dynamic boundary conditions in 2D disk- and ellipse-shaped
domains solved using PINN coupled with EDRAS, we gain significant insights into
how dynamic boundary conditions influence bulk phase evolution and
thermodynamic behavior. The proposed approach offers an effective, physically
informed enhancement to PINN frameworks for solving thermodynamically
consistent models, making PINN a robust and versatile computational tool for
investigating complex thermodynamic processes in arbitrary geometries.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [652] [Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis](https://arxiv.org/abs/2507.09378)
*Mohammadsaleh Refahi,Mahdi Abavisani,Bahrad A. Sokhansanj,James R. Brown,Gail Rosen*

Main category: q-bio.GN

TL;DR: CARMANIA, a new framework, improves long-range and higher-order dependency modeling in nucleotide sequence analysis using a novel transition-matrix (TM) loss to enhance predictions based on evolutionary constraints and functional structures.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of standard self-attention in transformer models for long nucleotide sequences and their tendency to rely on local context windows instead of global structural dependencies.

Method: CARMANIA combines next-token prediction with a transition-matrix loss derived from n-gram statistics, aligning token transitions with sequence-specific structures via self-supervised pretraining.

Result: It outperforms prior long-context models by at least 7%, matches the state-of-the-art for short sequences, and significantly improves functional classification tasks, achieving up to a 34% absolute gain in Matthews correlation coefficient for enhancer prediction.

Conclusion: CARMANIA demonstrates the effectiveness of integrating context-aware regularization for capturing long-range and organism-specific dependencies, paving the way for improved genomic analysis across diverse tasks.

Abstract: Transformers have revolutionized nucleotide sequence analysis, yet capturing
long-range dependencies remains challenging. Recent studies show that
autoregressive transformers often exhibit Markovian behavior by relying on
fixed-length context windows for next-token prediction. However, standard
self-attention mechanisms are computationally inefficient for long sequences
due to their quadratic complexity and do not explicitly enforce global
transition consistency.
  We introduce CARMANIA (Context-Aware Regularization with Markovian
Integration for Attention-Based Nucleotide Analysis), a self-supervised
pretraining framework that augments next-token (NT) prediction with a
transition-matrix (TM) loss. The TM loss aligns predicted token transitions
with empirically derived n-gram statistics from each input sequence,
encouraging the model to capture higher-order dependencies beyond local
context. This integration enables CARMANIA to learn organism-specific sequence
structures that reflect both evolutionary constraints and functional
organization.
  We evaluate CARMANIA across diverse genomic tasks, including regulatory
element prediction, functional gene classification, taxonomic inference,
antimicrobial resistance detection, and biosynthetic gene cluster
classification. CARMANIA outperforms the previous best long-context model by at
least 7 percent, matches state-of-the-art on shorter sequences (exceeding prior
results on 20 out of 40 tasks while running approximately 2.5 times faster),
and shows particularly strong improvements on enhancer and housekeeping gene
classification tasks, including up to a 34 percent absolute gain in Matthews
correlation coefficient (MCC) for enhancer prediction. The TM loss boosts
accuracy in 33 of 40 tasks, especially where local motifs or regulatory
patterns drive prediction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [653] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Main category: cs.GR

TL;DR: Introducing a large-scale dataset and fine-tuned large language models for automating CAD design using natural language descriptions.


<details>
  <summary>Details</summary>
Motivation: To address the time-intensive, manual nature of CAD modeling, and to explore the potential of large language models (LLMs) for efficient CAD sequence automation.

Method: A dataset of 170k CAD models with human-like descriptions generated via GPT-4 is introduced. Fine-tuned code-LLMs use JSON-based CAD sequence representations for text-conditioned CAD generation. Geometric and topological metrics are proposed to evaluate quality.

Result: Fine-tuned LLMs successfully automate CAD design, drastically accelerating the process and exhibiting effectiveness in both synthetic and human-annotated cases. New evaluation metrics provide richer structural insights.

Conclusion: The work demonstrates the viability of LLMs for automating CAD design, offering a significant improvement in efficiency. The dataset and models are released for community use.

Abstract: Computer-aided design (CAD) is the digital construction of 2D and 3D objects,
and is central to a wide range of engineering and manufacturing applications
like automobile and aviation. Despite its importance, CAD modeling remains
largely a time-intensive, manual task. Recent works have attempted to automate
this process with small transformer-based models and handcrafted CAD sequence
representations. However, there has been little effort to leverage the
potential of large language models (LLMs) for sequential CAD design. In this
work, we introduce a new large-scale dataset of more than 170k CAD models
annotated with high-quality, human-like descriptions generated with our
pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs
to generate CAD sequences represented in a JSON-based format from natural
language descriptions, demonstrating the viability and effectiveness of this
approach for text-conditioned CAD generation. Because simple metrics often fail
to reflect the quality of generated objects, we introduce geometric and
topological metrics based on sphericity, mean curvature, and Euler
characteristic to provide richer structural insights. Our experiments and
ablation studies on both synthetic and human-annotated data demonstrate that
CADmium is able to automate CAD design, drastically speeding up the design of
new objects. The dataset, code, and fine-tuned models are available online.

</details>


### [654] [RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling](https://arxiv.org/abs/2507.09441)
*Ankit Sanjyal*

Main category: cs.GR

TL;DR: The paper addresses quality degradation issues in high-resolution image synthesis with diffusion models by proposing adaptive guidance schedules, improving stability and image fidelity.


<details>
  <summary>Details</summary>
Motivation: To address energy instabilities and guidance artifacts in high-resolution image generation using diffusion models, which affect visual quality.

Method: The authors analyze the latent energy landscape during sampling and develop energy-aware adaptive classifier-free guidance (CFG) schedules to stabilize energy trajectories over time.

Result: The proposed approach achieves superior stability scores (0.9998) and consistency metrics (0.9873), producing sharper and more accurate images compared to fixed-guidance methods.

Conclusion: Energy-aware CFG scheduling improves the performance of diffusion models by reducing artifacts and enhancing image quality, with a profiling framework as a diagnostic tool for optimization.

Abstract: High-resolution image synthesis with diffusion models often suffers from
energy instabilities and guidance artifacts that degrade visual quality. We
analyze the latent energy landscape during sampling and propose adaptive
classifier-free guidance (CFG) schedules that maintain stable energy
trajectories. Our approach introduces energy-aware scheduling strategies that
modulate guidance strength over time, achieving superior stability scores
(0.9998) and consistency metrics (0.9873) compared to fixed-guidance
approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling
yields optimal performance, providing sharper, more faithful images while
reducing artifacts. Our energy profiling framework serves as a powerful
diagnostic tool for understanding and improving diffusion model behavior.

</details>


### [655] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Main category: cs.GR

TL;DR: This paper proposes ScaffoldAvatar, a method for generating ultra-high fidelity, expressive, and photorealistic 3D head avatars using a patch-based approach with 3D Gaussian splatting.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of creating realistic and expressive 3D head avatars for applications like telepresence and movies, particularly focusing on capturing facial microfeatures and detailed motions.

Method: The authors use a patch-based geometric 3D face model combined with 3D Gaussian splatting, extracting patch-level local expressions and linking them to hierarchical scene anchor points for realistic dynamic rendering.

Result: ScaffoldAvatar achieves high resolution training (3K images) and state-of-the-art performance, producing visually natural facial motions and diverse expressions in real-time.

Conclusion: The proposed method successfully enables photorealistic, expressive 3D avatar generation, improving fidelity and motion realism while achieving efficient performance in diverse scenarios.

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [656] [THOR: Transformer Heuristics for On-Demand Retrieval](https://arxiv.org/abs/2507.09592)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.DB

TL;DR: The THOR Module is a Text-to-SQL tool that converts natural language into secure SQL for enterprise use, integrating schema awareness, fault-tolerance, and user-friendly insights.


<details>
  <summary>Details</summary>
Motivation: To simplify and democratize access to enterprise databases for non-technical users while ensuring security, reliability, and compliance.

Method: The module uses a decoupled architecture with a Supervisor Agent to route queries, schema retrieval for metadata injection, a SQL generation agent for query creation, and a self-correction loop for improving outputs. The result interpretation agent enables human-readable insights.

Result: Smoke tests across various scenarios like finance and sales confirmed that the tool provides reliable querying and automated reporting.

Conclusion: The THOR Module enables non-technical users to perform secure and efficient querying of enterprise databases with ease, ensuring scalability and safety.

Abstract: We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)
Module, designed and implemented by eSapiens, a secure, scalable engine that
transforms natural-language questions into verified, read-only SQL analytics
for enterprise databases. The Text-to-SQL module follows a decoupled
orchestration/execution architecture: a Supervisor Agent routes queries, Schema
Retrieval dynamically injects table and column metadata, and a SQL Generation
Agent emits single-statement SELECT queries protected by a read-only guardrail.
An integrated Self-Correction & Rating loop captures empty results, execution
errors, or low-quality outputs and triggers up to five LLM-driven regeneration
attempts. Finally, a Result Interpretation Agent produces concise,
human-readable insights and hands raw rows to the Insight & Intelligence engine
for visualization or forecasting.
  Smoke tests across finance, sales, and operations scenarios demonstrate
reliable ad-hoc querying and automated periodic reporting. By embedding schema
awareness, fault-tolerant execution, and compliance guardrails, the THOR Module
empowers non-technical users to access live data with zero-SQL simplicity and
enterprise-grade safety.

</details>


### [657] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: Tracer is a novel Video Database Management System (VDBMS) that improves object re-identification and tracking across large camera networks using adaptive query processing.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve object re-identification in large camera networks due to the limitations of existing systems like Spatula in terms of accuracy and recall.

Method: Tracer employs a recurrent neural network to select optimal cameras for processing while optimizing query speed and recall through a probabilistic adaptive search model. It also introduces a synthetic benchmark for generating realistic Re-ID datasets.

Result: Tracer outperforms the state-of-the-art system by achieving a 3.9x improvement in performance across diverse datasets.

Conclusion: Tracer addresses key challenges in object re-identification and tracking by providing a scalable and efficient solution for large camera networks while meeting high recall requirements.

Abstract: Efficiently re-identifying and tracking objects across a network of cameras
is crucial for applications like traffic surveillance. Spatula is the
state-of-the-art video database management system (VDBMS) for processing Re-ID
queries. However, it suffers from two limitations. Its spatio-temporal
filtering scheme has limited accuracy on large camera networks due to localized
camera history. It is not suitable for critical video analytics applications
that require high recall due to a lack of support for adaptive query
processing.
  In this paper, we present Tracer, a novel VDBMS for efficiently processing
Re-ID queries using an adaptive query processing framework. Tracer selects the
optimal camera to process at each time step by training a recurrent network to
model long-term historical correlations. To accelerate queries under a high
recall constraint, Tracer incorporates a probabilistic adaptive search model
that processes camera feeds in incremental search windows and dynamically
updates the sampling probabilities using an exploration-exploitation strategy.
To address the paucity of benchmarks for the Re-ID task due to privacy
concerns, we present a novel synthetic benchmark for generating multi-camera
Re-ID datasets based on real-world traffic distribution. Our evaluation shows
that Tracer outperforms the state-of-the-art cross-camera analytics system by
3.9x on average across diverse datasets.

</details>


### [658] [HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving](https://arxiv.org/abs/2507.09138)
*Zhengding Hu,Vibha Murthy,Zaifeng Pan,Wanlu Li,Xiaoyi Fang,Yufei Ding,Yuke Wang*

Main category: cs.DB

TL;DR: HedraRAG optimizes heterogeneous RAG serving with a graph-based runtime system, achieving up to 5x speedups.


<details>
  <summary>Details</summary>
Motivation: Efficient execution in heterogeneous RAG systems is challenging due to complex workflows and varied requests.

Method: Proposes HedraRAG, utilizing graph-based abstractions and dynamic graph transformations for optimization.

Result: Demonstrated speedups of 1.5x to 5x across diverse RAG workflows.

Conclusion: HedraRAG successfully improves resource utilization and latency in serving environments through coordinated optimization strategies.

Abstract: This paper addresses emerging system-level challenges in heterogeneous
retrieval-augmented generation (RAG) serving, where complex multi-stage
workflows and diverse request patterns complicate efficient execution. We
present HedraRAG, a runtime system built on a graph-based abstraction that
exposes optimization opportunities across stage-level parallelism,
intra-request similarity, and inter-request skewness. These opportunities are
realized through dynamic graph transformations, such as node splitting,
reordering, edge addition, and dependency rewiring, applied to wavefronts of
subgraphs spanning concurrent requests. The resulting execution plans are
mapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce
latency. Evaluations across a wide range of RAG workflows demonstrate speedups
exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the
effectiveness of coordinated generation and retrieval in serving environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [659] [Counterfactual optimization for fault prevention in complex wind energy systems](https://arxiv.org/abs/2507.08849)
*Emilio Carrizosa,Martina Fischetti,Roshell Haaker,Juan Miguel Morales*

Main category: eess.SY

TL;DR: The paper proposes a methodology to optimize control strategies for complex systems by solving counterfactual problems and demonstrates its effectiveness in managing offshore wind turbine systems.


<details>
  <summary>Details</summary>
Motivation: To advance beyond detecting anomalies in complex systems and identify optimal control strategies for restoring these systems to a safe state with minimal disruption.

Method: The authors use a mathematical model to compute optimal counterfactual adjustments to system control variables, while respecting system-specific constraints.

Result: The approach was tested on real-world data from offshore wind turbine systems, achieving significant savings of approximately 3 million euros per year in a typical farm.

Conclusion: This work successfully applies counterfactual optimization to a novel domain and provides industrial-scale savings, opening the door to broader research opportunities in complex energy systems.

Abstract: Machine Learning models are increasingly used in businesses to detect faults
and anomalies in complex systems. In this work, we take this approach a step
further: beyond merely detecting anomalies, we aim to identify the optimal
control strategy that restores the system to a safe state with minimal
disruption. We frame this challenge as a counterfactual problem: given a
Machine Learning model that classifies system states as either good or
anomalous, our goal is to determine the minimal adjustment to the system's
control variables (i.e., its current status) that is necessary to return it to
the good state. To achieve this, we leverage a mathematical model that finds
the optimal counterfactual solution while respecting system specific
constraints. Notably, most counterfactual analysis in the literature focuses on
individual cases where a person seeks to alter their status relative to a
decision made by a classifier, such as for loan approval or medical diagnosis.
Our work addresses a fundamentally different challenge: optimizing
counterfactuals for a complex energy system, specifically an offshore wind
turbine oil type transformer. This application not only advances counterfactual
optimization in a new domain but also opens avenues for broader research in
this area. Our tests on real world data provided by our industrial partner show
that our methodology easily adapts to user preferences and brings savings in
the order of 3 million euros per year in a typical farm.

</details>


### [660] [Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO](https://arxiv.org/abs/2507.09864)
*Hossein Nejatbakhsh Esfahani,Javad Mohammadpour Velni*

Main category: eess.SY

TL;DR: This paper integrates MPC-based reinforcement learning with multi-objective Bayesian optimization to enhance convergence, parameter tuning, and safety in control systems.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of slow convergence, limited parameterization, and safety issues in MPC-based RL methods.

Method: Combines MPC-RL with MOBO using noisy evaluations and the Expected Hypervolume Improvement acquisition function.

Result: Demonstrated sample-efficient, stable, and high-performance learning through numerical experiments.

Conclusion: The novel framework improves closed-loop performance efficiently and safely, even with model imperfections.

Abstract: Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a
structured and interpretable alternative to Deep Neural Network (DNN)-based RL
methods, with lower computational complexity and greater transparency. However,
standard MPC-RL approaches often suffer from slow convergence, suboptimal
policy learning due to limited parameterization, and safety issues during
online adaptation. To address these challenges, we propose a novel framework
that integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The
proposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its
gradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)
approach, and incorporates them into a MOBO algorithm using the Expected
Hypervolume Improvement (EHVI) acquisition function. This fusion enables
efficient and safe tuning of the MPC parameters to achieve improved closed-loop
performance, even under model imperfections. A numerical example demonstrates
the effectiveness of the proposed approach in achieving sample-efficient,
stable, and high-performance learning for control systems.

</details>


### [661] [Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem](https://arxiv.org/abs/2507.09503)
*Zhentong Shao,Jingtao Qin,Nanpeng Yu*

Main category: eess.SY

TL;DR: The paper introduces a neural approach to solve a large-scale two-stage stochastic unit commitment (2S-SUC) problem efficiently by embedding a trained deep neural network into a mixed-integer linear program (MILP).


<details>
  <summary>Details</summary>
Motivation: The paper addresses the computational inefficiency and scalability issues of conventional methods for solving 2S-SUC problems under high-dimensional uncertainty.

Method: A deep neural network is trained to approximate the second-stage recourse costs and is embedded into the first-stage problem as a MILP. A scenario-embedding network is used for dimensionality reduction and scenario reduction.

Result: The method achieves an optimality gap of less than 1% and significantly faster computation (orders-of-magnitude speedup) compared to traditional methods, while maintaining scalability as the number of scenarios grows.

Conclusion: This neural optimization approach offers a computationally efficient, scalable, and accurate method for solving large-scale 2S-SUC problems.

Abstract: This paper proposes a neural stochastic optimization method for efficiently
solving the two-stage stochastic unit commitment (2S-SUC) problem under
high-dimensional uncertainty scenarios. The proposed method approximates the
second-stage recourse problem using a deep neural network trained to map
commitment decisions and uncertainty features to recourse costs. The trained
network is subsequently embedded into the first-stage UC problem as a
mixed-integer linear program (MILP), allowing for explicit enforcement of
operational constraints while preserving the key uncertainty characteristics. A
scenario-embedding network is employed to enable dimensionality reduction and
feature aggregation across arbitrary scenario sets, serving as a data-driven
scenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and
118-bus systems demonstrate that the proposed neural two-stage stochastic
optimization method achieves solutions with an optimality gap of less than 1%,
while enabling orders-of-magnitude speedup compared to conventional MILP
solvers and decomposition-based methods. Moreover, the model's size remains
constant regardless of the number of scenarios, offering significant
scalability for large-scale stochastic unit commitment problems.

</details>


### [662] [Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control](https://arxiv.org/abs/2507.09685)
*Yutong Li,Ilya Kolmanovsky*

Main category: eess.SY

TL;DR: The paper presents a noninvasive and symptom-based framework for optimizing Proton Pump Inhibitor (PPI) therapy, offering personalized dosing based on patient-reported symptoms and Bayesian Neural Network predictions, while reducing drug consumption.


<details>
  <summary>Details</summary>
Motivation: Chronic high doses of PPIs pose significant risks, and the current approach lacks precision due to impractical gastric acid monitoring methods and inter-patient variability.

Method: A Bayesian Neural Network generates symptom forecasts from historical data, which informs a chance-constrained Model Predictive Control algorithm to dynamically optimize PPI doses without invasive gastric acid measurements.

Result: In silico studies show a 65% reduction in total PPI consumption compared to fixed regimens, while maintaining acid suppression with at least 95% probability.

Conclusion: The approach demonstrates potential for personalized PPI therapy, enabling significant drug reduction and overdose risk minimization without sacrificing efficacy or requiring invasive monitoring tools.

Abstract: Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid
disorders but carry significant risks when administered chronically at high
doses. Precise long-term control of gastric acidity is challenged by the
impracticality of invasive gastric acid monitoring beyond 72 hours and wide
inter-patient variability. We propose a noninvasive, symptom-based framework
that tailors PPI dosing solely on patient-reported reflux and digestive symptom
patterns. A Bayesian Neural Network prediction model learns to predict patient
symptoms and quantifies its uncertainty from historical symptom scores, meal,
and PPIs intake data. These probabilistic forecasts feed a chance-constrained
Model Predictive Control (MPC) algorithm that dynamically computes future PPI
doses to minimize drug usage while enforcing acid suppression with high
confidence - without any direct acid measurement. In silico studies over
diverse dietary schedules and virtual patient profiles demonstrate that our
learning-augmented MPC reduces total PPI consumption by 65 percent compared to
standard fixed regimens, while maintaining acid suppression with at least 95
percent probability. The proposed approach offers a practical path to
personalized PPI therapy, minimizing treatment burden and overdose risk without
invasive sensors.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [663] [Precomputed Dominant Resource Fairness](https://arxiv.org/abs/2507.08846)
*Serdar Metin*

Main category: cs.GT

TL;DR: The paper revisits Dominant Resource Fairness (DRF) for multiple resource allocation and proposes a streamlined algorithm called Precomputed Dominant Resource Fairness (PDRF).


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for efficient resource allocation in distributed systems with multiple resource types and heterogeneous user demands, focusing on improving the Dominant Resource Fairness algorithm.

Method: The paper provides a detailed analysis of DRF and formulates an alternative algorithm, PDRF, that aims to approximate DRF with fewer computational steps.

Result: The resulting algorithm, PDRF, is proposed as a more computationally efficient method for achieving similar fairness properties as DRF in resource allocation.

Conclusion: Precomputed Dominant Resource Fairness enhances DRF by reducing computational complexity while maintaining desirable fairness properties in distributed systems.

Abstract: Although resource allocation is a well studied problem in computer science,
until the prevalence of distributed systems, such as computing clouds and data
centres, the question had been addressed predominantly for single resource type
scenarios. At the beginning of the last decade, with the introuction of
Dominant Resource Fairness, the studies of the resource allocation problem has
finally extended to the multiple resource type scenarios. Dominant Resource
Fairness is a solution, addressing the problem of fair allocation of multiple
resource types, among users with heterogeneous demands. Based on Max-min
Fairness, which is a well established algorithm in the literature for
allocating resources in the single resource type scenarios, Dominant Resource
Fairness generalises the scheme to the multiple resource case. It has a number
of desirable properties that makes it preferable over alternatives, such as
Sharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness,
and as such, it is widely adopted in distributed systems. In the present study,
we revisit the original study, and analyse the structure of the algorithm in
closer view, to come up with an alternative algorithm, which approximates the
Dominant Resource Fairness allocation in fewer steps. We name the new algorithm
Precomputed Dominant Resource Fairness, after its main working principle.

</details>


### [664] [A Survey on Bilateral Multi-Round Cloud-SLA Negotiation Strategies](https://arxiv.org/abs/2507.08868)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.GT

TL;DR: The paper surveys and analyzes bilateral multi-round negotiation strategies for trading cloud resources, highlighting trends, gaps, and recommendations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static cloud markets and explore dynamic trading mechanisms with negotiation to provide tailored services and enhance datacenter utilization.

Method: Analyzed peer-reviewed articles to identify trends, gaps, and describe formalism used for bilateral multi-round negotiation strategies in cloud resource trading.

Result: Identified trends, gaps, and similarities in negotiation strategies; reviewed formalism used; derived recommendations for industry implementation.

Conclusion: The paper provides insights and actionable recommendations to foster the integration of negotiation strategies in dynamic cloud marketplaces.

Abstract: Today, static cloud markets where consumers purchase services directly from
providers are dominating. Thus, consumers neither negotiate the price nor the
characteristics of the service. In recent years, providers have adopted more
dynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to
the reservation marketspace and the on-demand marketspace, Amazon offers a spot
marketspace where consumers can bid for virtual machines. This spot marketspace
was extended with spot blocks, and recently Amazon reworked the bidding
options. In addition, other cloud providers, such as Virtustream, adopt dynamic
trading mechanisms. The scientific community envisions autonomous multi-round
negotiations for realizing future cloud marketspaces. Consequently, consumers
and providers exchange offers and counteroffers to reach an agreement. This
helps providers increase the utilization of their datacenters, while consumers
can purchase highly customized cloud services.
  In the paper at hand, we present a survey on multi-round bilateral
negotiation strategies for trading cloud resources. Thus, we analyzed
peer-reviewed articles in order to identify trends, gaps, similarities, and the
scope of such negotiation strategies. In addition, we surveyed the formalism
that the scientific community uses to describe such strategies. Based on these
findings, we derived recommendations for creating and documenting bilateral
multi-round negotiation strategies to foster their implementation in the
industry.

</details>


### [665] [Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints](https://arxiv.org/abs/2507.09473)
*Yan Dai,Negin Golrezaei,Patrick Jaillet*

Main category: cs.GT

TL;DR: The paper proposes a solution for strategic allocation of reusable resources to agents, focusing on maximizing social welfare, handling cost constraints, and ensuring truthful behavior. Standard methods are fragile against strategic manipulation, and the authors present a novel mechanism with robust learning techniques.


<details>
  <summary>Details</summary>
Motivation: Applications in cloud platforms or mobile health units raised concerns about allocating resources when agents act strategically. There's a need for mechanisms that balance welfare, cost constraints, and truthfulness.

Method: The authors propose an incentive-aware framework combining lazy updates and randomized exploration rounds for truthful signaling. They leverage tailored online learning techniques for robust dual updates.

Result: Their mechanism achieves near-optimal social welfare regret, satisfies cost constraints, and aligns agent incentives, offering resilience to strategic manipulations.

Conclusion: The paper demonstrates the framework's effectiveness, ensuring robust resource allocation without sacrificing efficiency or fairness even in strategic settings.

Abstract: Motivated by applications such as cloud platforms allocating GPUs to users or
governments deploying mobile health units across competing regions, we study
the dynamic allocation of a reusable resource to strategic agents with private
valuations. Our objective is to simultaneously (i) maximize social welfare,
(ii) satisfy multi-dimensional long-term cost constraints, and (iii)
incentivize truthful reporting. We begin by numerically evaluating primal-dual
methods widely used in constrained online optimization and find them to be
highly fragile in strategic settings -- agents can easily manipulate their
reports to distort future dual updates for future gain.
  To address this vulnerability, we develop an incentive-aware framework that
makes primal-dual methods robust to strategic behavior. Our design combines
epoch-based lazy updates -- where dual variables remain fixed within each epoch
-- with randomized exploration rounds that extract approximately truthful
signals for learning. Leveraging carefully designed online learning subroutines
that can be of independent interest for dual updates, our mechanism achieves
$\tilde{\mathcal{O}}(\sqrt{T})$ social welfare regret, satisfies all cost
constraints, and ensures incentive alignment. This matches the performance of
non-strategic allocation approaches while being robust to strategic agents.

</details>


### [666] [Learning from Synthetic Labs: Language Models as Auction Participants](https://arxiv.org/abs/2507.09083)
*Anand Shah,Kehang Zhu,Yanchen Jiang,Jeffrey G. Wang,Arif K. Dayi,John J. Horton,David C. Parkes*

Main category: cs.GT

TL;DR: This paper studies AI agents (LLMs) in auctions using a new synthetic data approach, showing their behaviors align with known human patterns and theoretical predictions while introducing a cost-effective experimental framework.


<details>
  <summary>Details</summary>
Motivation: To explore the performance of AI agents (LLMs) as proxies for human behavior in auctions, and to provide a cost-effective method for carrying out auction experiments.

Method: Simulated auction experiments with GPT-4 models were conducted, using a novel synthetic data-generating framework and chain of thought reasoning. Prompts were tailored to analyze responsiveness to different factors.

Result: LLMs replicated known behaviors of risk-averse human bidders, aligned closer to theoretical predictions in strategy-proof auctions, and exhibited the winner's curse in common value auctions. Performance improved with prompts using mental models like Nash deviations.

Conclusion: The study demonstrates the feasibility and cost-efficiency of using LLMs as proxies in auction research, validating their adherence to known patterns and theoretical predictions and paving the way for further experimental studies.

Abstract: This paper investigates the behavior of simulated AI agents (large language
models, or LLMs) in auctions, introducing a novel synthetic data-generating
process to help facilitate the study and design of auctions. We find that LLMs
-- when endowed with chain of thought reasoning capacity -- agree with the
experimental literature in auctions across a variety of classic auction
formats. In particular, we find that LLM bidders produce results consistent
with risk-averse human bidders; that they perform closer to theoretical
predictions in obviously strategy-proof auctions; and, that they succumb to the
winner's curse in common value settings. On prompting, we find that LLMs are
not very sensitive to naive changes in prompts (e.g., language, currency) but
can improve dramatically towards theoretical predictions with the right mental
model (i.e., the language of Nash deviations). We run 1,000$+$ auctions for
less than $\$$400 with GPT-4 models (three orders of magnitude cheaper than
modern auction experiments) and develop a framework flexible enough to run
auction experiments with any LLM model and a wide range of auction design
specifications, facilitating further experimental study by decreasing costs and
serving as a proof-of-concept for the use of LLM proxies.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [667] [Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates](https://arxiv.org/abs/2507.09166)
*Louise Largeau,Erwan Koch,David Leutwyler,Gregoire Mariethoz,Valerie Chavez-Demoulin,Tom Beucler*

Main category: physics.ao-ph

TL;DR: This paper addresses the limitations of coarse-resolution climate models in predicting extreme precipitation by introducing direct super-resolution methods for analyzing probability distributions.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in predicting socially significant variables like extreme precipitation and improve robustness amidst climate change-induced distributional shifts.

Method: The authors propose directly super-resolving the parameters of probability distributions using vector generalized linear and additive models, within a perfect-model framework focused on Switzerland.

Result: The study successfully super-resolves generalized extreme value distributions of summer hourly precipitation, identifies a robustness gap, and establishes a super-resolution limit based on spatial correlations.

Conclusion: The proposed framework enhances the understanding of empirical downscaling methods and their generalization to climate extremes, offering model-agnostic diagnostics for broader applications.

Abstract: The coarse spatial resolution of gridded climate models, such as general
circulation models, limits their direct use in projecting socially relevant
variables like extreme precipitation. Most downscaling methods estimate the
conditional distributions of extremes by generating large ensembles,
complicating the assessment of robustness under distributional shifts, such as
those induced by climate change. To better understand and potentially improve
robustness, we propose super-resolving the parameters of the target variable's
probability distribution directly using analytically tractable mappings. Within
a perfect-model framework over Switzerland, we demonstrate that vector
generalized linear and additive models can super-resolve the generalized
extreme value distribution of summer hourly precipitation extremes from coarse
precipitation fields and topography. We introduce the notion of a "robustness
gap", defined as the difference in predictive error between present-trained and
future-trained models, and use it to diagnose how model structure affects the
generalization of each quantile to a pseudo-global warming scenario. By
evaluating multiple model configurations, we also identify an upper limit on
the super-resolution factor based on the spatial auto- and cross-correlation of
precipitation and elevation, beyond which coarse precipitation loses predictive
value. Our framework is broadly applicable to variables governed by parametric
distributions and offers a model-agnostic diagnostic for understanding when and
why empirical downscaling generalizes to climate change and extremes.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [668] [Solving the compute crisis with physics-based ASICs](https://arxiv.org/abs/2507.10463)
*Maxwell Aifer,Zach Belateche,Suraj Bramhavar,Kerem Y. Camsari,Patrick J. Coles,Gavin Crooks,Douglas J. Durian,Andrea J. Liu,Anastasia Marchenkova,Antonio J. Martinez,Peter L. McMahon,Faris Sbahi,Benjamin Weiner,Logan G. Wright*

Main category: cs.ET

TL;DR: Physics-based ASICs leverage intrinsic physical dynamics to solve AI's compute crisis, achieving energy efficiency and high computational throughput.


<details>
  <summary>Details</summary>
Motivation: Current AI demands lead to unsustainable energy usage and CMOS scaling limits, necessitating transformative computing approaches.

Method: Focuses on designing ASICs that capitalize on physical processes instead of enforcing conventional digital abstractions.

Result: Physics-based ASICs demonstrate improved computational efficiency, aligning algorithmic needs with physical system capabilities.

Conclusion: These systems promise a future of specialized, scalable computing platforms for advancing AI and other computational tasks.

Abstract: Escalating artificial intelligence (AI) demands expose a critical "compute
crisis" characterized by unsustainable energy consumption, prohibitive training
costs, and the approaching limits of conventional CMOS scaling. Physics-based
Application-Specific Integrated Circuits (ASICs) present a transformative
paradigm by directly harnessing intrinsic physical dynamics for computation
rather than expending resources to enforce idealized digital abstractions. By
relaxing the constraints needed for traditional ASICs, like enforced
statelessness, unidirectionality, determinism, and synchronization, these
devices aim to operate as exact realizations of physical processes, offering
substantial gains in energy efficiency and computational throughput. This
approach enables novel co-design strategies, aligning algorithmic requirements
with the inherent computational primitives of physical systems. Physics-based
ASICs could accelerate critical AI applications like diffusion models,
sampling, optimization, and neural network inference as well as traditional
computational workloads like scientific simulation of materials and molecules.
Ultimately, this vision points towards a future of heterogeneous,
highly-specialized computing platforms capable of overcoming current scaling
bottlenecks and unlocking new frontiers in computational power and efficiency.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [669] [Advancing network resilience theories with symbolized reinforcement learning](https://arxiv.org/abs/2507.08827)
*Yu Zheng,Jingtao Ding,Depeng Jin,Jianxi Gao,Yong Li*

Main category: physics.soc-ph

TL;DR: A study on network resilience introduces an AI-driven method to account for both topology and system dynamics, improving existing theories.


<details>
  <summary>Details</summary>
Motivation: To create a resilience theory that incorporates both topology and system dynamics to better understand and prevent network collapses.

Method: AI-based self-inductive approach learns network attack strategies and formulates theoretical resilience models.

Result: Discovered the first resilience theory incorporating topology and dynamics, and improved established resilience models by 37.5% in accuracy.

Conclusion: The approach significantly enhances understanding of complex networks and provides tools for preventing collapses, uncovering crucial dynamics and refinements.

Abstract: Many complex networks display remarkable resilience under external
perturbations, internal failures and environmental changes, yet they can
swiftly deteriorate into dysfunction upon the removal of a few keystone nodes.
Discovering theories that measure network resilience offers the potential to
prevent catastrophic collapses--from species extinctions to financial
crise--with profound implications for real-world systems. Current resilience
theories address the problem from a single perspective of topology, neglecting
the crucial role of system dynamics, due to the intrinsic complexity of the
coupling between topology and dynamics which exceeds the capabilities of human
analytical methods. Here, we report an automatic method for resilience theory
discovery, which learns from how AI solves a complicated network dismantling
problem and symbolizes its network attack strategies into theoretical formulas.
This proposed self-inductive approach discovers the first resilience theory
that accounts for both topology and dynamics, highlighting how the correlation
between node degree and state shapes overall network resilience, and offering
insights for designing early warning signals of systematic collapses.
Additionally, our approach discovers formulas that refine existing
well-established resilience theories with over 37.5% improvement in accuracy,
significantly advancing human understanding of complex networks with AI.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [670] [FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios](https://arxiv.org/abs/2507.10448)
*Yingqian Wu,Qiushi Wang,Zefei Long,Rong Ye,Zhongtian Lu,Xianyin Zhang,Bingxuan Li,Wei Chen,Liwen Zhang,Zhongyu Wei*

Main category: cs.CE

TL;DR: The paper introduces FinTeam, a multi-agent financial report generation framework, outperforming existing models in real financial analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM models are inadequate for comprehensive financial analysis, prompting the need for specialized systems.

Method: A four-agent workflow of LLMs (document analyzer, analyst, accountant, and consultant) trained on specific financial expertise datasets.

Result: FinTeam achieved a 62.00% human acceptance rate on financial report generation and improved performance on FinCUGE and FinEval benchmarks.

Conclusion: FinTeam demonstrates the potential of collaborative multi-agent systems for handling complex financial analyses effectively.

Abstract: Financial report generation tasks range from macro- to micro-economics
analysis, also requiring extensive data analysis. Existing LLM models are
usually fine-tuned on simple QA tasks and cannot comprehensively analyze real
financial scenarios. Given the complexity, financial companies often distribute
tasks among departments. Inspired by this, we propose FinTeam, a financial
multi-agent collaborative system, with a workflow with four LLM agents:
document analyzer, analyst, accountant, and consultant. We train these agents
with specific financial expertise using constructed datasets. We evaluate
FinTeam on comprehensive financial tasks constructed from real online
investment forums, including macroeconomic, industry, and company analysis. The
human evaluation shows that by combining agents, the financial reports generate
from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models
like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43%
average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project
is available at https://github.com/FudanDISC/DISC-FinLLM/.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [671] [Generation of structure-guided pMHC-I libraries using Diffusion Models](https://arxiv.org/abs/2507.08902)
*Sergio Mares,Ariel Espinoza Weinberger,Nilah M. Ioannidis*

Main category: q-bio.QM

TL;DR: This paper introduces a structure-guided benchmark using diffusion models to design pMHC-I peptides independent of dataset biases, showing limitations in existing predictors and providing a diverse evaluation resource.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks and models for identifying peptide-MHC class I interactions are biased due to reliance on mass-spectrometry and binding-assay datasets, hindering progress in personalized vaccines and T-cell immunotherapies.

Method: The paper uses diffusion models conditioned on crystal structure interaction distances to design a structure-guided benchmark for pMHC-I peptides, which is independent of known peptides and spans 20 HLA alleles.

Result: The new benchmark reproduces anchor residue preferences while avoiding experimental dataset biases and reveals poor performance of state-of-the-art sequence-based predictors on structurally generated peptides.

Conclusion: The geometry-aware design pipeline improves residue diversity and offers a critical, unbiased resource for advancing pMHC-I peptide prediction models.

Abstract: Personalized vaccines and T-cell immunotherapies depend critically on
identifying peptide-MHC class I (pMHC-I) interactions capable of eliciting
potent immune responses. However, current benchmarks and models inherit biases
present in mass-spectrometry and binding-assay datasets, limiting discovery of
novel peptide ligands. To address this issue, we introduce a structure-guided
benchmark of pMHC-I peptides designed using diffusion models conditioned on
crystal structure interaction distances. Spanning twenty high-priority HLA
alleles, this benchmark is independent of previously characterized peptides yet
reproduces canonical anchor residue preferences, indicating structural
generalization without experimental dataset bias. Using this resource, we
demonstrate that state-of-the-art sequence-based predictors perform poorly at
recognizing the binding potential of these structurally stable designs,
indicating allele-specific limitations invisible in conventional evaluations.
Our geometry-aware design pipeline yields peptides with high predicted
structural integrity and higher residue diversity than existing datasets,
representing a key resource for unbiased model training and evaluation. Our
code, and data are available at: https://github.com/sermare/struct-mhc-dev.

</details>


### [672] [From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research](https://arxiv.org/abs/2507.09028)
*Amgad Muneer,Muhammad Waqas,Maliazurina B Saad,Eman Showkatian,Rukhmini Bandyopadhyay,Hui Xu,Wentao Li,Joe Y Chang,Zhongxing Liao,Cara Haymaker,Luisa Solis Soto,Carol C Wu,Natalie I Vokes,Xiuning Le,Lauren A Byers,Don L Gibbons,John V Heymach,Jianjun Zhang,Jia Wu*

Main category: q-bio.QM

TL;DR: This paper reviews the integration of multimodal datasets in cancer research using foundation models (FMs), exploring advancements, challenges, and tools for data-driven oncology.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of extracting actionable insights from the increasingly vast and heterogeneous multimodal datasets in cancer research.

Method: The study reviews existing literature and methodologies, focusing on the transition from traditional machine learning to foundation models for multimodal data integration in oncology.

Result: The review identifies state-of-the-art foundation models, publicly available multimodal repositories, advanced integration tools, and highlights the emerging trends and challenges.

Conclusion: The paper concludes that current integrative methods lay groundwork for future large-scale, pretrained models, emphasizing their transformative potential for oncology advancements.

Abstract: Cancer research is increasingly driven by the integration of diverse data
modalities, spanning from genomics and proteomics to imaging and clinical
factors. However, extracting actionable insights from these vast and
heterogeneous datasets remains a key challenge. The rise of foundation models
(FMs) -- large deep-learning models pretrained on extensive amounts of data
serving as a backbone for a wide range of downstream tasks -- offers new
avenues for discovering biomarkers, improving diagnosis, and personalizing
treatment. This paper presents a comprehensive review of widely adopted
integration strategies of multimodal data to assist advance the computational
approaches for data-driven discoveries in oncology. We examine emerging trends
in machine learning (ML) and deep learning (DL), including methodological
frameworks, validation protocols, and open-source resources targeting cancer
subtype classification, biomarker discovery, treatment guidance, and outcome
prediction. This study also comprehensively covers the shift from traditional
ML to FMs for multimodal integration. We present a holistic view of recent FMs
advancements and challenges faced during the integration of multi-omics with
advanced imaging data. We identify the state-of-the-art FMs, publicly available
multi-modal repositories, and advanced tools and methods for data integration.
We argue that current state-of-the-art integrative methods provide the
essential groundwork for developing the next generation of large-scale,
pre-trained models poised to further revolutionize oncology. To the best of our
knowledge, this is the first review to systematically map the transition from
conventional ML to advanced FM for multimodal data integration in oncology,
while also framing these developments as foundational for the forthcoming era
of large-scale AI models in cancer research.

</details>


### [673] [A PBN-RL-XAI Framework for Discovering a "Hit-and-Run'' Therapeutic Strategy in Melanoma](https://arxiv.org/abs/2507.10136)
*Zhonglin Liu*

Main category: q-bio.QM

TL;DR: The paper investigates how to overcome resistance to anti-PD-1 therapy in metastatic melanoma using computational modeling and AI-driven intervention strategies.


<details>
  <summary>Details</summary>
Motivation: Understanding and combating innate resistance to anti-PD-1 immunotherapy in metastatic melanoma.

Method: Development of a Probabilistic Boolean Network using transcriptomic data; application of reinforcement learning and explainable AI to identify effective therapeutic strategies.

Result: The most effective intervention was a 4-step, temporary inhibition of LOXL2, identified as erasing the molecular resistance signature.

Conclusion: This computational approach offers novel, time-dependent intervention methods for overcoming immunotherapy resistance, applicable to complex biological systems.

Abstract: Innate resistance to anti-PD-1 immunotherapy remains a major clinical
challenge in metastatic melanoma, with the underlying molecular networks being
poorly understood. To address this, we constructed a dynamic Probabilistic
Boolean Network model using transcriptomic data from patient tumor biopsies to
elucidate the regulatory logic governing therapy response. We then employed a
reinforcement learning agent to systematically discover optimal, multi-step
therapeutic interventions and used explainable artificial intelligence to
mechanistically interpret the agent's control policy. The analysis revealed
that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2
protein (LOXL2) was the most effective strategy. Our explainable analysis
showed that this ``hit-and-run" intervention is sufficient to erase the
molecular signature driving resistance, allowing the network to self-correct
without requiring sustained intervention. This study presents a novel,
time-dependent therapeutic hypothesis for overcoming immunotherapy resistance
and provides a powerful computational framework for identifying non-obvious
intervention protocols in complex biological systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [674] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: This paper proposes a lightweight deep learning framework for efficient cascaded channel estimation in XL-MIMO and RIS-enabled 6G systems, significantly improving scalability and reducing computational complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the challenges in channel estimation for XL-MIMO and RIS systems in 6G wireless technologies, where scalability, computational complexity, and energy efficiency are critical issues.

Method: The authors developed a lightweight deep learning framework using spatial correlations in the wireless channel. It introduces a patch-based training mechanism to reduce the dimensionality of the inputs while retaining crucial information, making the model scalable and efficient.

Result: The proposed framework achieved improved channel estimation accuracy across various conditions and significantly reduced computational complexity, making it suitable for resource-constrained edge devices.

Conclusion: The research provides a practical and efficient solution for channel estimation in large-scale 6G systems, overcoming scalability and deployment challenges, and paving the way for real-world implementation.

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [675] [A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation](https://arxiv.org/abs/2507.08879)
*Max-Paul Förster,Luca Deck,Raimund Weidlich,Niklas Kühl*

Main category: cs.CY

TL;DR: The paper examines the challenges of deepfake technologies under EU regulations and proposes a multi-level strategy for marking, detecting, and labeling them.


<details>
  <summary>Details</summary>
Motivation: To address the risks that deepfakes pose to democratic societies, particularly in political communication, and to assess current EU regulatory measures.

Method: A multivocal literature review was conducted to evaluate existing methods for marking, detecting, and labeling deepfakes and their alignment with EU regulations.

Result: The study found that existing methods are inadequate alone but can be more effective when combined in a multi-level strategy. This strategy includes scalability, adaptability, and a scoring mechanism.

Conclusion: A multi-level strategy is necessary to overcome shortcomings of individual methods and effectively address deepfake risks under EU regulations.

Abstract: The growing availability and use of deepfake technologies increases risks for
democratic societies, e.g., for political communication on online platforms.
The EU has responded with transparency obligations for providers and deployers
of Artificial Intelligence (AI) systems and online platforms. This includes
marking deepfakes during generation and labeling deepfakes when they are
shared. However, the lack of industry and enforcement standards poses an
ongoing challenge. Through a multivocal literature review, we summarize methods
for marking, detecting, and labeling deepfakes and assess their effectiveness
under EU regulation. Our results indicate that individual methods fail to meet
regulatory and practical requirements. Therefore, we propose a multi-level
strategy combining the strengths of existing methods. To account for the masses
of content on online platforms, our multi-level strategy provides scalability
and practicality via a simple scoring mechanism. At the same time, it is
agnostic to types of deepfake technology and allows for context-specific risk
weighting.

</details>


### [676] [The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions](https://arxiv.org/abs/2507.08881)
*Zhang MingDa,Xu Qing*

Main category: cs.CY

TL;DR: The study examines the challenges of integrating large language models (LLMs) into judicial systems, focusing on the gap between technical consistency and social acceptance.


<details>
  <summary>Details</summary>
Motivation: The global use of LLMs in judicial systems has exposed a critical paradox: while they are technically consistent, this consistency can lead to both benefits and drawbacks depending on societal acceptance.

Method: The paper analyzes recent data on judicial applications of LLMs (2023-2025) and proposes a governance framework called DTDMR-LJGF for intelligent task classification and stakeholder interaction.

Result: The DTDMR-LJGF framework offers a way to manage the complexity of LLM performance in judicial systems, addressing both technical efficiency and social legitimacy.

Conclusion: The research provides theoretical and practical strategies for balancing the technical advances of LLMs with the need for societal acceptance in judicial contexts.

Abstract: The integration of large language model (LLM) technology into judicial
systems is fundamentally transforming legal practice worldwide. However, this
global transformation has revealed an urgent paradox requiring immediate
attention. This study introduces the concept of ``consistency-acceptability
divergence'' for the first time, referring to the gap between technical
consistency and social acceptance. While LLMs achieve high consistency at the
technical level, this consistency demonstrates both positive and negative
effects. Through comprehensive analysis of recent data on LLM judicial
applications from 2023--2025, this study finds that addressing this challenge
requires understanding both task and stakeholder dimensions. This study
proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance
Framework (DTDMR-LJGF), which enables intelligent task classification and
meaningful interaction among diverse stakeholders. This framework offers both
theoretical insights and practical guidance for building an LLM judicial
ecosystem that balances technical efficiency with social legitimacy.

</details>


### [677] [The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations](https://arxiv.org/abs/2507.08908)
*M. Z. Naser*

Main category: cs.CY

TL;DR: The paper investigates how engineers can navigate legal and ethical considerations when using machine learning in their practice, proposing a framework based on analogical reasoning and established liability doctrines.


<details>
  <summary>Details</summary>
Motivation: The adoption of machine learning in engineering is hindered by uncertainty regarding legal and regulatory frameworks governing its use.

Method: The authors explore analogical reasoning and analyze legal precedents, liability doctrines, and legislative guidance to align ML with professional engineering standards.

Result: The analysis identifies established liability doctrines like negligence and product liability, alongside insights from legislative bodies, to furnish guidance on the use of ML in engineering.

Conclusion: Integrating machine learning into engineering practice requires a legal framework that aligns ethical guidance and professional accountability with regulatory standards regarding liabilities and benefits.

Abstract: Despite the widespread interest in machine learning (ML), the engineering
industry has not yet fully adopted ML-based methods, which has left engineers
and stakeholders uncertain about the legal and regulatory frameworks that
govern their decisions. This gap remains unaddressed as an engineer's
decision-making process, typically governed by professional ethics and
practical guidelines, now intersects with complex algorithmic outputs. To
bridge this gap, this paper explores how engineers can navigate legal
principles and legislative justifications that support and/or contest the
deployment of ML technologies. Drawing on recent precedents and experiences
gained from other fields, this paper argues that analogical reasoning can
provide a basis for embedding ML within existing engineering codes while
maintaining professional accountability and meeting safety requirements. In
exploring these issues, the discussion focuses on established liability
doctrines, such as negligence and product liability, and highlights how courts
have evaluated the use of predictive models. We further analyze how legislative
bodies and standard-setting organizations can furnish explicit guidance
equivalent to prior endorsements of emergent technologies. This exploration
stresses the vitality of understanding the interplay between technical
justifications and legal precedents for shaping an informed stance on ML's
legitimacy in engineering practice. Finally, our analysis catalyzes a legal
framework for integrating ML through which stakeholders can critically assess
the responsibilities, liabilities, and benefits inherent in ML-driven
engineering solutions.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [678] [DiffNMR: Diffusion Models for Nuclear Magnetic Resonance Spectra Elucidation](https://arxiv.org/abs/2507.08854)
*Qingsong Yang,Binglan Wu,Xuwei Liu,Bo Chen,Wei Li,Gen Long,Xin Chen,Mingjun Xiao*

Main category: physics.chem-ph

TL;DR: The paper introduces DiffNMR, a novel framework using diffusion models for molecular structure interpretation from NMR spectra.


<details>
  <summary>Details</summary>
Motivation: Interpreting NMR spectra to deduce molecular structures remains challenging due to data complexity and large chemical space.

Method: DiffNMR employs a conditional discrete diffusion model, integrates pretraining strategies (Diff-AE and contrastive learning), retrieval initialization, NMR encoders, and RBF encoding for enhanced spectral-molecular alignment.

Result: DiffNMR delivers competitive performance in NMR-based molecular structure elucidation, ensuring global consistency and reducing errors.

Conclusion: DiffNMR provides an efficient and robust automated molecular analysis framework, addressing key challenges in NMR spectral interpretation.

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a central characterization
method for molecular structure elucidation, yet interpreting NMR spectra to
deduce molecular structures remains challenging due to the complexity of
spectral data and the vastness of the chemical space. In this work, we
introduce DiffNMR, a novel end-to-end framework that leverages a conditional
discrete diffusion model for de novo molecular structure elucidation from NMR
spectra. DiffNMR refines molecular graphs iteratively through a diffusion-based
generative process, ensuring global consistency and mitigating error
accumulation inherent in autoregressive methods. The framework integrates a
two-stage pretraining strategy that aligns spectral and molecular
representations via diffusion autoencoder (Diff-AE) and contrastive learning,
the incorporation of retrieval initialization and similarity filtering during
inference, and a specialized NMR encoder with radial basis function (RBF)
encoding for chemical shifts, preserving continuity and chemical correlation.
Experimental results demonstrate that DiffNMR achieves competitive performance
for NMR-based structure elucidation, offering an efficient and robust solution
for automated molecular analysis.

</details>


### [679] [Accurate generation of chemical reaction transition states by conditional flow matching](https://arxiv.org/abs/2507.10530)
*Ping Tuo,Jiale Chen,Ju Li*

Main category: physics.chem-ph

TL;DR: This paper introduces TS-GEN, a generative model for accurately and efficiently predicting chemical transition-state geometries, offering substantial improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Transition-state structures are critical to understanding chemical reactivity, but their experimental inaccessibility and the high computational cost of traditional methods create a need for more efficient and accurate solutions.

Method: TS-GEN utilizes a conditional flow-matching generative model that maps latent Gaussian noise directly to transition-state geometries in a deterministic and non-iterative manner. The model leverages reactant and product conformations as conditioning data, employing optimal-transport principles.

Result: The model achieves high accuracy with a root-mean-square deviation of 0.004 Å, a barrier-height error of 1.019 kcal/mol, and a computation time of 0.06 seconds per inference on a GPU. Over 87% of generated transition states meet chemical-accuracy standards, outperforming existing methods significantly.

Conclusion: TS-GEN provides a transformative approach to high-throughput exploration of reaction networks by combining exceptional speed, precision, and wide applicability, enabling the investigation of novel chemical mechanisms efficiently.

Abstract: Transition state (TS) structures define the critical geometries and energy
barriers underlying chemical reactivity, yet their fleeting nature renders them
experimentally elusive and drives the reliance on costly, high-throughput
density functional theory (DFT) calculations. Here, we introduce TS-GEN, a
conditional flow-matching generative model that maps samples from a simple
Gaussian prior directly to transition-state saddle-point geometries in a
single, deterministic pass. By embedding both reactant and product
conformations as conditioning information, TS-GEN learns to transport latent
noise to true TS structures via an optimal-transport path, effectively
replacing the iterative optimization common in nudged-elastic band or
string-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a
root-mean-square deviation of $0.004\ \rm{\mathring{A}}$ (vs. $0.103\
\rm{\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error
of $1.019\ {\rm kcal/mol}$ (vs. $2.864\ {\rm kcal/mol}$), while requiring only
$0.06\ {\rm s}$ GPU time per inference. Over 87% of generated TSs meet
chemical-accuracy criteria ($<1.58\ {\rm kcal/mol}$ error), substantially
outpacing existing methods. TS-GEN also exhibits strong transferability to
out-of-distribution reactions from a larger database. By uniting sub-angstrom
precision, sub-second speed, and broad applicability, TS-GEN will be highly
useful for high-throughput exploration of complex reaction networks, paving the
way to the exploration of novel chemical reaction mechanisms.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [680] [Mind the Gap: Navigating Inference with Optimal Transport Maps](https://arxiv.org/abs/2507.08867)
*Malte Algren,Tobias Golling,Francesco Armando Di Bello,Christopher Pollard*

Main category: physics.data-an

TL;DR: The paper proposes a calibration method using optimal transport to address discrepancies between simulation and experimental data in machine learning applications for particle physics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of machine learning methods caused by discrepancies between high-quality simulations and experimental data in particle physics, which hinder progress.

Method: The authors develop a calibration method based on optimal transport and apply it to high-dimensional simulations. They demonstrate its effectiveness on latent representations in jet tagging using a CMS-inspired dataset.

Result: The calibrated latent representation is shown to improve calibration for various derived quantities, enabling new applications of jet flavor information in LHC analyses.

Conclusion: The method represents a significant step toward properly-calibrated foundation models for particle physics, with potential applications across various scientific domains.

Abstract: Machine learning (ML) techniques have recently enabled enormous gains in
sensitivity across the sciences. In particle physics, much of this progress has
relied on excellent simulations of a wide range of physical processes. However,
due to the sophistication of modern machine learning (ML) algorithms and their
reliance on high-quality training samples, discrepancies between simulation and
experimental data can significantly limit the effectiveness of ML techniques.
In this work, we present a solution to this ``mis-specification'' problem: a
calibration approach based on optimal transport, which we apply to
high-dimensional simulations for the first time. We demonstrate the performance
of our approach through jet tagging, using a CMS-inspired dataset. A
128-dimensional internal jet representation from a powerful general-purpose
classifier is studied; after calibrating this internal ``latent''
representation, we find that a wide variety of quantities derived from it for
downstream tasks are also properly calibrated: using this calibrated
high-dimensional representation, powerful new applications of jet flavor
information can be utilized in LHC analyses. This is a key step toward allowing
properly-calibrated ``foundation models'' in particle physics. More broadly,
this calibration framework has broad applications for correcting
high-dimensional simulations across the sciences.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [681] [ReDemon UI: Reactive Synthesis by Demonstration for Web UI](https://arxiv.org/abs/2507.10099)
*Jay Lee,Gyuhyeok Oh,Joongwon Ahn,Xiaokang Qiu*

Main category: cs.HC

TL;DR: ReDemon UI allows designers and non-expert programmers to easily build React applications by synthesizing UI behavior from user demonstrations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simplify the process of UI design for users without extensive coding expertise by leveraging standard prototyping workflows and eliminating technical barriers.

Method: Users provide static mockup sketches and demonstrate desired runtime behaviors. It employs enumerative synthesis for basic UIs and LLMs (Large Language Models) for complex ones.

Result: The system identifies reactive data and synthesizes React programs equipped with correct state update logic.

Conclusion: ReDemon UI bridges the gap for non-programmers and designers, making UI creation more accessible while seamlessly integrating into existing workflows.

Abstract: ReDemon UI synthesizes React applications from user demonstrations, enabling
designers and non-expert programmers to create UIs that integrate with standard
UI prototyping workflows. Users provide a static mockup sketch with event
handler holes and demonstrate desired runtime behaviors by interacting with the
rendered mockup and editing the sketch. ReDemon UI identifies reactive data and
synthesizes a React program with correct state update logic. We utilize
enumerative synthesis for simple UIs and LLMs for more complex UIs.

</details>


### [682] [Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads](https://arxiv.org/abs/2507.10427)
*Jing Li,Felix Schijve,Sheng Li,Yuye Yang,Jun Hu,Emilia Barakova*

Main category: cs.HC

TL;DR: This study introduces a social robot leveraging large language models (LLMs) to assist parents and neurodivergent children in emotion regulation, showing positive interaction impacts from pilot tests.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on integrating LLMs with socially assistive robots to aid emotion co-regulation between parents and neurodivergent children.

Method: A LLM-powered social robot using the MiRo-E platform was developed with a speech communication module, integrating LLM prompts and robotic behaviors. Pilot tests were performed with two parent-child dyads, followed by qualitative analysis.

Result: Results demonstrated the robot's positive impact on parent-child interaction dynamics and its potential to assist in emotion regulation, despite noting design and technical challenges.

Conclusion: The study highlights the potential of LLM-powered SAR in mental health applications and provides design recommendations for advancing this technology.

Abstract: Socially Assistive Robotics (SAR) has shown promise in supporting emotion
regulation for neurodivergent children. Recently, there has been increasing
interest in leveraging advanced technologies to assist parents in co-regulating
emotions with their children. However, limited research has explored the
integration of large language models (LLMs) with SAR to facilitate emotion
co-regulation between parents and children with neurodevelopmental disorders.
To address this gap, we developed an LLM-powered social robot by deploying a
speech communication module on the MiRo-E robotic platform. This supervised
autonomous system integrates LLM prompts and robotic behaviors to deliver
tailored interventions for both parents and neurodivergent children. Pilot
tests were conducted with two parent-child dyads, followed by a qualitative
analysis. The findings reveal MiRo-E's positive impacts on interaction dynamics
and its potential to facilitate emotion regulation, along with identified
design and technical challenges. Based on these insights, we provide design
implications to advance the future development of LLM-powered SAR for mental
health applications.

</details>


### [683] [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
*Mohammad Abolnejadian,Shakiba Amirshahi,Matthew Brehmer,Anamaria Crisan*

Main category: cs.HC

TL;DR: The paper explores a conversational user interface leveraging retrieval-based LLM agents to assist experts in leveraging historical data for real-time decision-making in scenarios like doctor-patient interactions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of utilizing extensive historical data during real-time decision-making in expert conversations.

Method: A prototype conversational interface using a retrieval-based Large Language Model (LLM) agent that listens to conversations, extracts key information, and retrieves insights from embedded datasets.

Result: Evaluation showed the system's effectiveness in generating relevant insights from Health Canada datasets during simulated doctor-patient dialogues, while highlighting some challenges.

Conclusion: The study demonstrated the feasibility of integrating historical data insights into real-time decision-making, providing groundwork for future research and improvements.

Abstract: In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

</details>


### [684] [SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations](https://arxiv.org/abs/2507.09664)
*Zoe Kaputa,Anika Rajaram,Vryan Almanon Feliciano,Zhuoyue Lyu,Maneesh Agrawala,Hari Subramonyam*

Main category: cs.HC

TL;DR: The paper introduces a framework called Chain-of-Abstractions (CoA) that enhances programming-by-prompting with generative AI, aimed particularly at helping educators create learning content using natural language.


<details>
  <summary>Details</summary>
Motivation: To enable non-programmers, particularly educators, to create interactive learning content using natural language while retaining programming's core affordances such as traceability and refinement.

Method: The method involves decomposing the synthesis process into cognitively meaningful representations, instantiated as the SimStep authoring environment with intermediate abstractions. It uses an inverse correction mechanism to address model assumptions and ambiguities.

Result: Evaluations with educators show that the CoA framework provides better authoring control and interpretability in programming-by-prompting workflows.

Conclusion: CoA and SimStep enhance the flexibility and usability of programming-by-prompting, making it practical and beneficial for educators to create simulations effectively without prior programming knowledge.

Abstract: Programming-by-prompting with generative AI offers a new paradigm for
end-user programming, shifting the focus from syntactic fluency to semantic
intent. This shift holds particular promise for non-programmers such as
educators, who can describe instructional goals in natural language to generate
interactive learning content. Yet in bypassing direct code authoring, many of
programming's core affordances - such as traceability, stepwise refinement, and
behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)
framework as a way to recover these affordances while preserving the expressive
flexibility of natural language. CoA decomposes the synthesis process into a
sequence of cognitively meaningful, task-aligned representations that function
as checkpoints for specification, inspection, and refinement. We instantiate
this approach in SimStep, an authoring environment for teachers that scaffolds
simulation creation through four intermediate abstractions: Concept Graph,
Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address
ambiguities and misalignments, SimStep includes an inverse correction process
that surfaces in-filled model assumptions and enables targeted revision without
requiring users to manipulate code. Evaluations with educators show that CoA
enables greater authoring control and interpretability in
programming-by-prompting workflows.

</details>


### [685] [Visual Analytics for Explainable and Trustworthy Artificial Intelligence](https://arxiv.org/abs/2507.10240)
*Angelos Chatzimparmpas*

Main category: cs.HC

TL;DR: AI systems are advancing in accuracy and efficiency, but their lack of transparency undermines trust. Visual analytics (VA) seeks to address this by using interactive visualizations that make AI processes more interpretable and trustworthy.


<details>
  <summary>Details</summary>
Motivation: Improve transparency and trust in AI systems essential in critical domains like medical diagnosis, which could save lives and reduce significant economic burdens.

Method: Define and categorize how visual analytics can enhance trust at different stages of the AI pipeline. Provide a design space for visualization innovation and review previous VA tools developed.

Result: Demonstrated how VA dashboards can assist in tasks ranging from data processing to comparing models, making AI systems more transparent and trustworthy.

Conclusion: Visual analytics addresses critical transparency deficiencies of AI systems, enhancing trust and enabling better collaboration between AI and human expertise.

Abstract: Our society increasingly depends on intelligent systems to solve complex
problems, ranging from recommender systems suggesting the next movie to watch
to AI models assisting in medical diagnoses for hospitalized patients. With the
iterative improvement of diagnostic accuracy and efficiency, AI holds
significant potential to mitigate medical misdiagnoses by preventing numerous
deaths and reducing an economic burden of approximately 450 EUR billion
annually. However, a key obstacle to AI adoption lies in the lack of
transparency: many automated systems function as "black boxes," providing
predictions without revealing the underlying processes. This opacity can hinder
experts' ability to trust and rely on AI systems. Visual analytics (VA)
provides a compelling solution by combining AI models with interactive
visualizations. These specialized charts and graphs empower users to
incorporate their domain expertise to refine and improve the models, bridging
the gap between AI and human understanding. In this work, we define,
categorize, and explore how VA solutions can foster trust across the stages of
a typical AI pipeline. We propose a design space for innovative visualizations
and present an overview of our previously developed VA dashboards, which
support critical tasks within the various pipeline stages, including data
processing, feature engineering, hyperparameter tuning, understanding,
debugging, refining, and comparing models.

</details>


### [686] [An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments](https://arxiv.org/abs/2507.10469)
*Mikko Korkiakoski,Saeid Sheikhi,Jesper Nyman,Jussi Saariniemi,Kalle Tapio,Panos Kostakos*

Main category: cs.HC

TL;DR: The paper evaluates AI-driven NPCs using GPT-4 Turbo in a VR interrogation simulator, assessing realism, usability, and performance.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of large language models for improving realism and interactivity of NPCs in VR.

Method: Conducted a user study with 18 participants using questionnaires to measure usability, believability, and latency performance.

Result: Believability scored 6.67/10, SUS usability score was 79.44, and average latency was 7 seconds.

Conclusion: Large language models enhance NPC realism but require optimization in latency and emotional depth.

Abstract: Advancements in artificial intelligence (AI) have significantly enhanced the
realism and interactivity of non-player characters (NPCs) in virtual reality
(VR), creating more engaging and believable user experiences. This paper
evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their
perceived realism, usability, and system performance. The simulator features
two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage
participants in a scenario to determine the suspect's guilt or innocence. A
user study with 18 participants assessed the system using the System Usability
Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent
Believability Questionnaire, alongside latency measurements for speech-to-text
(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.
Results showed an average cycle latency of 7 seconds, influenced by the
increasing conversational context. Believability scored 6.67 out of 10, with
high ratings in behavior, social relationships, and intelligence but moderate
scores in emotion and personality. The system achieved a SUS score of 79.44,
indicating good usability. These findings demonstrate the potential of large
language models to improve NPC realism and interaction in VR while highlighting
challenges in reducing system latency and enhancing emotional depth. This
research contributes to the development of more sophisticated AI-driven NPCs,
revealing the need for performance optimization to achieve increasingly
immersive virtual experiences.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [687] [Overview of the TREC 2023 deep learning track](https://arxiv.org/abs/2507.08890)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Hossein A. Rahmani,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: The TREC Deep Learning track, in its fifth and final year, evaluated passage and document ranking using large language models (LLMs) and synthetic queries, showing that LLMs outperform prior methods like "nnlm."


<details>
  <summary>Details</summary>
Motivation: The motivation was to leverage human-annotated training data (MS MARCO datasets) to evaluate passage and document ranking tasks, exploring both primary and secondary tasks and comparing traditional and emerging techniques like large language model prompting.

Method: The study employed MS MARCO datasets with human-annotated labels for training, along with synthetic queries generated through fine-tuned T5 and GPT-4 models. Runs were conducted, and human relevance assessments were applied to both types of queries. The evaluation also examined potential biases and system performance correlation across human and synthetic queries.

Result: The experiments showed that runs using LLM prompting outperformed the "nnlm" approach, which was the top-performing method in prior years. Synthetic queries demonstrated results similar to human queries, displaying strong evaluation agreement ($\tau=0.8487$). Bias in synthetic evaluation methods was not clearly observed.

Conclusion: Large language models have surpassed the performance of prior methodologies in ranking tasks, and synthetic queries can serve as complements to human-constructed datasets. Future advancements in prompt-based ranking are anticipated to occur outside the TREC track, as this marks the track's conclusion.

Abstract: This is the fifth year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of
human-annotated training labels available for both passage and document ranking
tasks. We mostly repeated last year's design, to get another matching test set,
based on the larger, cleaner, less-biased v2 passage and document set, with
passage ranking as primary and document ranking as a secondary task (using
labels inferred from passage). As we did last year, we sample from MS MARCO
queries that were completely held out, unused in corpus construction, unlike
the test queries in the first three years. This approach yields a more
difficult test with more headroom for improvement. Alongside the usual MS MARCO
(human) queries from MS MARCO, this year we generated synthetic queries using a
fine-tuned T5 model and using a GPT-4 prompt.
  The new headline result this year is that runs using Large Language Model
(LLM) prompting in some way outperformed runs that use the "nnlm" approach,
which was the best approach in the previous four years. Since this is the last
year of the track, future iterations of prompt-based ranking can happen in
other tracks. Human relevance assessments were applied to all query types, not
just human MS MARCO queries. Evaluation using synthetic queries gave similar
results to human queries, with system ordering agreement of $\tau=0.8487$.
However, human effort was needed to select a subset of the synthetic queries
that were usable. We did not see clear evidence of bias, where runs using GPT-4
were favored when evaluated using synthetic GPT-4 queries, or where runs using
T5 were favored when evaluated on synthetic T5 queries.

</details>


### [688] [DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate](https://arxiv.org/abs/2507.09090)
*Anthony Miyaguchi,Conor Johnston,Aaryan Potdar*

Main category: cs.IR

TL;DR: This paper investigates Large Language Models (LLMs) in structured debating and evaluation tasks, showcasing LLMs as effective albeit verbose debaters with consistent evaluation.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the potential of LLMs in structured debates and their evaluation capabilities.

Method: Six LLMs from three providers are tested using Retrieval-Augmented Debate and Evaluation framework, with metrics such as Quality, Quantity, Manner, and Relation.

Result: LLMs handle debate tasks effectively with related arguments, although responses are verbose. Evaluation by LLMs is consistent.

Conclusion: LLMs exhibit strong conversational and evaluative capabilities in debates, despite verbosity in their responses.

Abstract: Large Language Models (LLMs) demonstrate strong conversational abilities. In
this Working Paper, we study them in the context of debating in two ways: their
ability to perform in a structured debate along with a dataset of arguments to
use and their ability to evaluate utterances throughout the debate. We deploy
six leading publicly available models from three providers for the
Retrieval-Augmented Debate and Evaluation. The evaluation is performed by
measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout
this task, we found that although LLMs perform well in debates when given
related arguments, they tend to be verbose in responses yet consistent in
evaluation. The accompanying source code for this paper is located at
https://github.com/dsgt-arc/touche-2025-rad.

</details>


### [689] [GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval](https://arxiv.org/abs/2507.08945)
*Savini Kashmira,Jayanaka L. Dantanarayana,Krisztián Flautner,Lingjia Tang,Jason Mars*

Main category: cs.IR

TL;DR: GraphRunner addresses limitations of traditional LLM graph-based retrieval, offering significant improvements in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional RAG struggles with retrieval in structured datasets like knowledge graphs due to errors and hallucinations in LLM reasoning.

Method: GraphRunner operates through planning, verification, and execution stages, using high-level multi-hop traversal actions and validation to reduce errors.

Result: GraphRunner outperforms existing methods, improving performance by 10-50%, reducing inference costs by 3.0-12.9x, and accelerating response times by 2.5-7.1x.

Conclusion: GraphRunner is a more robust, accurate, and efficient framework for retrieval in interconnected graph datasets than prior methods.

Abstract: Conventional Retrieval Augmented Generation (RAG) approaches are common in
text-based applications. However, they struggle with structured, interconnected
datasets like knowledge graphs, where understanding underlying relationships is
crucial for accurate retrieval. A common direction in graph-based retrieval
employs iterative, rule-based traversal guided by Large Language Models (LLMs).
Such existing iterative methods typically combine reasoning with single hop
traversal at each step, making them vulnerable to LLM reasoning errors and
hallucinations that ultimately hinder the retrieval of relevant information.
  To address these limitations, we propose GraphRunner, a novel graph-based
retrieval framework that operates in three distinct stages: planning,
verification, and execution. This introduces high-level traversal actions that
enable multi-hop exploration in a single step. It also generates a holistic
traversal plan, which is verified against the graph structure and pre-defined
traversal actions, reducing reasoning errors and detecting hallucinations
before execution. GraphRunner significantly reduces LLM reasoning errors and
detects hallucinations through validation. Our evaluation using the GRBench
dataset shows that GraphRunner consistently outperforms existing approaches,
achieving 10-50% performance improvements over the strongest baseline while
reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,
making it significantly more robust and efficient for graph-based retrieval
tasks.

</details>


### [690] [MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora](https://arxiv.org/abs/2507.09924)
*Tuan-Luc Huynh,Thuy-Trang Vu,Weiqing Wang,Trung Le,Dragan Gašević,Yuan-Fang Li,Thanh-Toan Do*

Main category: cs.IR

TL;DR: MixLoRA-DSI efficiently updates model-based indexes for generative retrieval by selectively adding experts based on Out-of-Distribution documents, achieving better performance with lower costs.


<details>
  <summary>Details</summary>
Motivation: Updating model-based indexes with new documents is computationally expensive, making full retraining impractical.

Method: MixLoRA-DSI uses a mixture of Low-Rank Adaptation experts combined with a layer-wise OOD-driven strategy to ensure selective and resource-efficient model expansion.

Result: Experiments showed that MixLoRA-DSI outperformed full-model updates in accuracy, while significantly reducing parameter overhead and training costs.

Conclusion: MixLoRA-DSI offers a scalable and efficient solution for generative retrieval tasks, allowing updates to indexes with minimized resource usage and enhanced performance.

Abstract: Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

</details>


### [691] [PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization](https://arxiv.org/abs/2507.10057)
*Sangwoo Park,Jinheon Baek,Soyeong Jeong,Sung Ju Hwang*

Main category: cs.IR

TL;DR: PRISM introduces a fine-grained approach for document-to-document scientific paper retrieval, outperforming previous methods by 4.3%.


<details>
  <summary>Details</summary>
Motivation: Previous scientific paper retrieval methods relied heavily on abstracts, neglecting the rich details contained in full texts.

Method: PRISM decomposes query papers into aspect-specific views and embeds them individually for matching against similarly segmented candidate papers, leveraging multidimensional context.

Result: PRISM achieves an average performance improvement of 4.3% compared to existing retrieval baselines.

Conclusion: By employing fine-grained representations and a new benchmark, PRISM advances the effectiveness of scientific paper retrieval methods.

Abstract: Scientific paper retrieval, particularly framed as document-to-document
retrieval, aims to identify relevant papers in response to a long-form query
paper, rather than a short query string. Previous approaches to this task have
focused on abstracts, embedding them into dense vectors as surrogates for full
documents and calculating similarity across them, although abstracts provide
only sparse and high-level summaries. To address this, we propose PRISM, a
novel document-to-document retrieval method that introduces multiple,
fine-grained representations for both the query and candidate papers. In
particular, each query paper is decomposed into multiple aspect-specific views
and individually embedded, which are then matched against candidate papers
similarity segmented to consider their multifaceted dimensions. Moreover, we
present SciFullBench, a novel benchmark in which the complete and segmented
context of full papers for both queries and candidates is available. Then,
experimental results show that PRISM improves performance by an average of 4.3%
over existing retrieval baselines.

</details>


### [692] [Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems](https://arxiv.org/abs/2507.09566)
*Timo Wilm,Philipp Normann*

Main category: cs.IR

TL;DR: The paper proposes a method to identify offline metrics aligned with online impact in recommender systems, evaluated via a large-scale experiment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in understanding the relationship between offline and online metrics in recommender systems, ensuring offline tests better predict real-world performance.

Method: The method involves leveraging Pareto front approximation and deploying a single model to analyze multiple offline metrics across test groups in online experiments. The approach is model-agnostic for neural network-based systems.

Result: The approach is validated through an online experiment on the OTTO e-commerce platform, demonstrating significant alignment between offline metrics and key online indicators like click-through rate, conversion rate, and units sold.

Conclusion: The strategy offers a practical tool for aligning offline metrics with online performance, aiding industry practitioners in making data-driven decisions.

Abstract: A critical challenge in recommender systems is to establish reliable
relationships between offline and online metrics that predict real-world
performance. Motivated by recent advances in Pareto front approximation, we
introduce a pragmatic strategy for identifying offline metrics that align with
online impact. A key advantage of this approach is its ability to
simultaneously serve multiple test groups, each with distinct offline
performance metrics, in an online experiment controlled by a single model. The
method is model-agnostic for systems with a neural network backbone, enabling
broad applicability across architectures and domains. We validate the strategy
through a large-scale online experiment in the field of session-based
recommender systems on the OTTO e-commerce platform. The online experiment
identifies significant alignments between offline metrics and real-word
click-through rate, post-click conversion rate and units sold. Our strategy
provides industry practitioners with a valuable tool for understanding
offline-to-online metric relationships and making informed, data-driven
decisions.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [693] [The Second Machine Turn: From Checking Proofs to Creating Concepts](https://arxiv.org/abs/2507.10179)
*Asvin G*

Main category: math.HO

TL;DR: This paper discusses AI's role in automating the creation of mathematical concepts and explores implications, methods, and future scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance AI capabilities from proof-checking to the automation of creating mathematical concepts.

Method: Exploration of current AI technologies, obstacles, proposed solutions, and an attempt to mathematize concept creation.

Result: Preliminary demonstration of AI creating mathematical concepts and identification of challenges in the field.

Conclusion: AI's capacity to create mathematical concepts could significantly impact mathematics and human-machine collaboration, with multiple potential future pathways.

Abstract: We identify a second machine turn in the process of mathematical discovery:
after automating proof-checking, AI is now poised to automate the *creation* of
mathematical concepts themselves. We discuss the current state of the art,
obstacles and potential solutions as well as a preliminary attempt at
mathematizing the creation of concepts itself. The paper ends with an
assessment of how these capabilities could reshape mathematics and
human-machine collaboration, and a few different futures we might find
ourselves in.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [694] [Surprisingly High Redundancy in Electronic Structure Data](https://arxiv.org/abs/2507.09001)
*Sazzad Hossain,Ponkrshnan Thiagarajan,Shashank Pathrudkar,Stephanie Taylor,Abhijeet S. Gangan,Amartya S. Banerjee,Susanta Ghosh*

Main category: cond-mat.mtrl-sci

TL;DR: The paper reveals high redundancy in electronic structure datasets, showing that data can be pruned significantly with minimal loss to ML performance.


<details>
  <summary>Details</summary>
Motivation: To challenge the belief that large exhaustive datasets are required for accurate ML predictions of electronic structure.

Method: The authors analyzed redundancy in datasets and compared random, coverage-based, and importance-based pruning strategies to assess their impact on model accuracy and generalizability.

Result: Coverage-based pruning retained chemical accuracy and used 100x less data with reduced training time, while importance-based pruning performed poorly at higher pruning levels.

Conclusion: Electronic structure datasets have high redundancy, enabling the creation of minimal, efficient datasets for ML without compromising accuracy.

Abstract: Machine Learning (ML) models for electronic structure rely on large datasets
generated through expensive Kohn-Sham Density Functional Theory simulations.
This study reveals a surprisingly high level of redundancy in such datasets
across various material systems, including molecules, simple metals, and
complex alloys. Our findings challenge the prevailing assumption that large,
exhaustive datasets are necessary for accurate ML predictions of electronic
structure. We demonstrate that even random pruning can substantially reduce
dataset size with minimal loss in predictive accuracy, while a state-of-the-art
coverage-based pruning strategy retains chemical accuracy and model
generalizability using up to 100-fold less data and reducing training time by
threefold or more. By contrast, widely used importance-based pruning methods,
which eliminate seemingly redundant data, can catastrophically fail at higher
pruning factors, possibly due to the significant reduction in data coverage.
This heretofore unexplored high degree of redundancy in electronic structure
data holds the potential to identify a minimal, essential dataset
representative of each material class.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [695] [Machine-Precision Prediction of Low-Dimensional Chaotic Systems](https://arxiv.org/abs/2507.09652)
*Christof Schötz,Niklas Boers*

Main category: nlin.CD

TL;DR: This paper presents a methodology to accurately learn dynamics from noise-free observations in low-dimensional chaotic systems, achieving unprecedented prediction times.


<details>
  <summary>Details</summary>
Motivation: The study aims to evaluate the efficacy of system-agnostic methods for learning the dynamics of low-dimensional chaotic systems, such as the Lorenz-63 model, using noise-free data.

Method: The paper employs ordinary least squares regression applied to high-degree polynomial features with 512-bit arithmetic, allowing for machine-precision accuracy.

Result: The proposed method achieves valid prediction times of up to 105 Lyapunov times for the Lorenz-63 system, outperforming previous approaches which capped at 13 Lyapunov times. Results are validated on more complex chaotic systems like Thomas' Cyclically Symmetric Attractor and the higher-dimensional Lorenz-96 model.

Conclusion: Learning dynamics from noise-free observations in low-dimensional chaotic systems has reached a highly accurate and reliable stage, indicating that such problems are effectively solved under these conditions.

Abstract: Low-dimensional chaotic systems such as the Lorenz-63 model are commonly used
to benchmark system-agnostic methods for learning dynamics from data. Here we
show that learning from noise-free observations in such systems can be achieved
up to machine precision: using ordinary least squares regression on high-degree
polynomial features with 512-bit arithmetic, our method exceeds the accuracy of
standard 64-bit numerical ODE solvers of the true underlying dynamical systems.
Depending on the configuration, we obtain valid prediction times of 32 to 105
Lyapunov times for the Lorenz-63 system, dramatically outperforming prior work
that reaches 13 Lyapunov times at most. We further validate our results on
Thomas' Cyclically Symmetric Attractor, a non-polynomial chaotic system that is
considerably more complex than the Lorenz-63 model, and show that similar
results extend also to higher dimensions using the spatiotemporally chaotic
Lorenz-96 model. Our findings suggest that learning low-dimensional chaotic
systems from noise-free data is a solved problem.

</details>
