<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 65]
- [cs.AR](#cs.AR) [Total: 8]
- [cs.CL](#cs.CL) [Total: 96]
- [cs.CV](#cs.CV) [Total: 220]
- [cs.DC](#cs.DC) [Total: 16]
- [cs.LG](#cs.LG) [Total: 144]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 61]
- [cs.SE](#cs.SE) [Total: 15]
- [q-bio.NC](#q-bio.NC) [Total: 7]
- [stat.ML](#stat.ML) [Total: 8]
- [nlin.CG](#nlin.CG) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.SD](#cs.SD) [Total: 9]
- [eess.AS](#eess.AS) [Total: 5]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [cs.MM](#cs.MM) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [cs.OH](#cs.OH) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 10]
- [cs.SI](#cs.SI) [Total: 3]
- [eess.SP](#eess.SP) [Total: 2]
- [eess.IV](#eess.IV) [Total: 19]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.HC](#cs.HC) [Total: 15]
- [cs.NI](#cs.NI) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [math.NA](#math.NA) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.CR](#cs.CR) [Total: 9]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 11]
- [hep-ex](#hep-ex) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: The paper introduces optimized microkernels for 1-bit and 2-bit LLM models to enhance inference efficiency on modern CPUs, achieving significant speedups compared to existing SOTA runtimes.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiencies in deploying ultra-low-bit (1/1.58/2-bit) LLM models in resource-constrained environments like edge devices and AI PCs.

Method: The authors designed 1-bit and 2-bit microkernels tailored for modern CPUs, integrated them into the PyTorch-TPP framework, and benchmarked against other runtime tools like bitnet.cpp.

Result: Their implementation achieves up to a 2.2x speed increase over bitnet.cpp and up to a 7x speed improvement compared to 16-bit model inference.

Conclusion: The study enables efficient deployment of ultra-low-bit LLM models, advancing inference for edge devices and AI PCs.

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [2] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: The paper introduces a CUDA-accelerated framework to solve the fishing card game Pasur using Counterfactual Regret Minimization to compute near-Nash equilibria. Efficient memory handling and round-by-round backward training strategies enable simulation of over $10^9$ nodes, and fair deck values are evaluated through self-play.


<details>
  <summary>Details</summary>
Motivation: To solve Pasur, a complex imperfect-information game, by addressing unique challenges such as intricate rules and large game tree sizes using computational frameworks.

Method: The authors utilize PyTorch CUDA tensors for memory management and decompose the game tree into actual game states and inherited scores. They construct the full game tree through an unfolding process and apply round-by-round backward training to manage computational complexity.

Result: The methodology enabled them to simulate a complete game tree with over $10^9$ nodes and compute near-Nash equilibrium strategies. A tree-based model was trained to predict these strategies for gameplay.

Conclusion: CUDA-accelerated frameworks and computational modeling can efficiently handle large imperfect-information games like Pasur. The approach is extendable to other similar decision-intensive domains such as strategy games or financial trading.

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [3] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: The paper introduces SciLink, an AI-driven framework to foster serendipitous discoveries in materials research by linking experimental observations to novelty assessments and simulations.


<details>
  <summary>Details</summary>
Motivation: Modern autonomous labs optimize efficiency but may miss unexpected, serendipitous discoveries pivotal to scientific progress.

Method: SciLink employs a hybrid AI strategy, combining machine learning for data analysis and large language models for reasoning, to autonomously process experimental data into novelty-scored scientific claims.

Result: The framework successfully demonstrates versatility in handling diverse materials research scenarios, integrating human guidance, and suggesting targeted follow-up experiments.

Conclusion: SciLink enhances AI-driven research by systematically enabling serendipitous discoveries, closing the gap between automation and exploratory science.

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [4] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: This paper introduces IRL-VLA, a novel framework for Vision-Language-Action models in close-loop autonomous driving to address imitation learning constraints and simulation inefficiencies.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of open-loop imitation learning and domain gaps in close-loop training for autonomous driving.

Method: The framework involves three stages: pretraining a VLA policy with imitation learning, building a lightweight reward model via inverse reinforcement learning, and enhancing planning via PPO reinforcement learning.

Result: State-of-the-art performance in NAVSIM v2 end-to-end driving benchmark and 1st runner-up in CVPR2025 Autonomous Grand Challenge.

Conclusion: IRL-VLA improves close-loop planning performance while balancing safety, comfort, and efficiency, aiming to advance VLA research for autonomous driving.

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [5] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: The paper highlights a critical weakness in Multimodal Large Language Models (MLLMs)—object counting—and introduces the CountQA benchmark to address this, revealing poor performance by current models.


<details>
  <summary>Details</summary>
Motivation: MLLMs perform well in understanding visual scenes but fail at object counting, which limits their real-world applicability. Existing benchmarks inadequately test this skill.

Method: The authors created CountQA, a benchmark with over 1,500 question-answer pairs featuring real-world images containing high object densities, clutter, and occlusion. They evaluated 15 MLLMs on it.

Result: The best-performing MLLM achieved only 42.9% accuracy on CountQA, with performance decreasing as object counts increased.

Conclusion: CountQA reveals a significant weakness in MLLMs and offers a tool to drive the development of models capable of accurate counting in realistic scenarios.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [6] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: The paper explores how Formal Concept Analysis (FCA) properties can be applied for variability extraction and analysis, bridging a gap between theory and practical applications.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in understanding and applying the properties of FCA for variability-related tasks, motivated by the difficulty posed by its mathematical nature.

Method: The authors compile essential FCA properties and describe their application in analyzing variability information within conceptual structures.

Result: The paper identifies FCA properties useful for variability analysis and explains their relevance in interpreting diverse variability information.

Conclusion: Formal Concept Analysis has key properties that can effectively support variability analysis when properly understood and applied, thus aiding knowledge representation and discovery.

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [7] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: The paper introduces a method for improving Cellular Trajectory Map-Matching (CTMM) in unexplored regions by leveraging transferable geospatial knowledge and addressing cellular positioning errors.


<details>
  <summary>Details</summary>
Motivation: Current approaches to CTMM rely heavily on ID-based features and region-specific data, which limits adaptability and accuracy in unexplored areas. This paper aims to enable high-accuracy CTMM in regions without additional training.

Method: The authors propose a pixel-based trajectory calibration assistant combined with a Gaussian mixture model in VAE for scenario-adaptive clustering. They incorporate a spatial-temporal awareness module to handle sequential features and positioning uncertainty, and use a constrained path-finding algorithm to reconstruct road ID sequences.

Result: The proposed model achieves a 16.8% improvement compared to existing methods in zero-shot CTMM scenarios.

Conclusion: The model demonstrates effective zero-shot CTMM capability by leveraging shared geospatial knowledge and mitigating positioning errors, offering improved accuracy and adaptability in various regions.

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [8] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: This paper proposes a method to reduce the excessively large rule sets often needed in rule-based knowledge graph completion, achieving high performance with significantly fewer rules.


<details>
  <summary>Details</summary>
Motivation: To address the issue of large rule sets in rule-based knowledge graph completion, which reduce explainability and hinder practical applications.

Method: The paper uses rule contexts derived from training data and employs probabilistic circuits to learn a probability distribution over these contexts, enabling competitive performance with fewer rules.

Result: Up to 96% reduction in rules while retaining 91% of peak performance and outperforming baseline methods by up to 31× with minimal rules on datasets.

Conclusion: The proposed framework establishes semantics-based reasoning, achieves tractable inference, and validates its efficacy through empirical tests on benchmark datasets, suggesting implications for broader probabilistic reasoning over rules.

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [9] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR, a new differentiable inductive logic programming approach, expands rule syntax for knowledge graph tasks and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing inductive logic programming methods often assume chain-like rule structures, which limit performance and interpretability.

Method: The paper introduces GLIDR, which employs a differentiable message-passing algorithm to generalize rule learning to include branches and cycles.

Result: GLIDR significantly outperforms existing rule learning approaches on knowledge graph completion tasks, competes with embedding methods, and shows robustness to data noise.

Conclusion: GLIDR enhances rule learning expressiveness, maintains predictive performance, and is adaptable to other data modalities when combined with deep neural networks.

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [10] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: The paper focuses on extending Balans, a multi-armed bandits-based algorithm for solving MIPs, with a parallelized version called ParBalans to improve performance using solver-level and algorithmic-level parallelism.


<details>
  <summary>Details</summary>
Motivation: Mixed-Integer Programming (MIP) problems demand significant computational resources, and parallelization has become essential to enhance scalability and solution time.

Method: The authors propose ParBalans, an extension of the Balans framework that utilizes both solver-level and algorithmic-level parallelism to address complex MIP instances.

Result: ParBalans demonstrates competitive performance, achieving comparable results to Gurobi, a commercial state-of-the-art solver, especially on difficult optimization tasks.

Conclusion: Parallelization through ParBalans enhances Balans's capacity to solve challenging MIPs effectively, marking progress in algorithms for computational optimization.

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [11] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: The study proposes a UAV network framework using Graph Diffusion-based Policy Optimization (GDPO) and a Stackelberg Game model to ensure reliable and covert UAV communication.


<details>
  <summary>Details</summary>
Motivation: The need for robust and secure UAV networks for sensitive applications like urban monitoring and emergency response, despite challenges such as dynamic mobility and exposure risks.

Method: The framework integrates GDPO for dynamic, adaptive topology generation and a Stackelberg Game-based incentive mechanism to encourage cooperation among UAVs for covert communication.

Result: Experimental results demonstrate the effectiveness of the approach in convergence, topology quality, and covert communication enhancement.

Conclusion: The proposed framework successfully tackles the challenges in UAV networks by enhancing adaptability and communication security, making it suitable for critical applications.

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [12] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: The paper introduces a modular prompting framework to make LLMs safer and more adaptable across dynamic tasks by employing fuzzy scaffolding logic and human learning theory concepts, improving performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the need for safer and more adaptive utilization of LLMs in dynamic, user-centered tasks, particularly those requiring user interaction.

Method: The proposed framework integrates a natural language boundary prompt with a control schema based on fuzzy scaffolding logic and rules, enabling LLMs to adapt their behavior dynamically and without fine-tuning.

Result: In simulations related to intelligent tutoring, the approach demonstrated enhanced adaptability, scaffolding quality, and alignment with instructional goals, outperforming traditional prompting baselines.

Conclusion: The framework provides a reusable and interpretable methodology for safely structuring goal-aligned LLM behavior in various evolving contexts, with applications in education and beyond.

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [13] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: The paper introduces a framework for volumetric data exploration using natural language interaction, facilitating effective viewpoint navigation with minimal domain expertise.


<details>
  <summary>Details</summary>
Motivation: Navigating volumetric data poses challenges for users, especially those without expertise or knowledge of 3D navigation.

Method: They use a combined approach involving semantic encoding of volumetric blocks, CLIP Score mechanism for semantics, and reinforcement learning to align navigation with user intent.

Result: The framework efficiently searches and selects viewpoints that correspond to user intentions, as validated by a semantic scoring system (CLIP Score).

Conclusion: Automating viewpoint selection enhances data navigation efficiency and improves interpretability in analyzing complex scientific datasets.

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [14] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: The paper advocates for a shift from vision-centered to language-centered models in remote sensing interpretation, proposing the use of Large Language Models (LLMs) as cognitive hubs for unified understanding and decision-making.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of vision-centered models in handling multi-modal reasoning, semantic abstraction, and interactive decision-making in remote sensing interpretation.

Method: Inspired by the Global Workspace Theory, the authors propose treating LLMs as central hubs that integrate perceptual, task, knowledge, and action spaces, summarizing challenges and outlining mechanisms for language-centered solutions.

Result: A language-centered framework was developed, addressing key challenges like multimodal representation, reasoning, decision-making, and establishing potential applications and directions in geospatial analysis.

Conclusion: The proposed framework provides a conceptual foundation for cognition-driven geospatial analysis, emphasizing the integration of language-centered solutions and outlining future research directions in adaptive multimodal alignment, task understanding, reasoning, and interaction.

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [15] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: The paper addresses credit assignment in cooperative multi-agent reinforcement learning (MARL) by introducing a new method, MACA, which performs multi-level credit assignment using counterfactual reasoning and an attention-based framework.


<details>
  <summary>Details</summary>
Motivation: Cooperative MARL requires effective credit assignment to evaluate each agent's contribution to shared rewards, which is challenging due to diverse and overlapping agent collaborations. The motivation lies in addressing this complexity, especially in scenarios with multiple levels of agent coordination.

Method: The authors propose Multi-level Advantage Credit Assignment (MACA), leveraging a multi-level advantage formulation and explicit counterfactual reasoning. Utilizing an attention-based framework, MACA identifies correlated agent relationships and integrates advantage functions across individual, joint, and correlated actions.

Result: MACA achieves superior performance, as demonstrated in comprehensive experiments on challenging Starcraft v1&v2 tasks. The results highlight its effectiveness in addressing complex credit assignment scenarios.

Conclusion: MACA successfully captures multi-level credit assignments in cooperative MARL, enhancing policy learning by effectively identifying and reasoning about agent contributions in diverse and complex scenarios.

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [16] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: The paper introduces MDK12-Bench, a large-scale benchmark for evaluating multimodal large language models (MLLMs) using real-world K-12 exam data, addressing limitations in current intelligence assessments.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for MLLMs lack scale, structured knowledge, and dynamic evaluation, limiting their ability to comprehensively measure the models' intelligence.

Method: The paper presents MDK12-Bench, which leverages K-12 exams across disciplines, includes temporal/contextual shifts, and introduces a dynamic evaluation framework to test model generalization under unfamiliar conditions.

Result: MDK12-Bench reveals limitations in existing MLLMs in generalization, objectivity, and reasoning, highlighting areas for improvement in model design and robustness.

Conclusion: MDK12-Bench demonstrates the need for more robust and interpretable MLLMs and offers a foundation for enhancing AI capability and aiding education.

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [17] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: The paper introduces MP-Bench, a large-scale dataset for severe weather event prediction, and proposes the Meteorology Multimodal Large Model (MMLM) to process complex meteorological data for AI-driven weather forecasting.


<details>
  <summary>Details</summary>
Motivation: Current forecasting systems for severe weather rely heavily on human interpretation, which introduces subjectivity and operational challenges. The paper aims to address challenges preventing the adoption of AI-driven, end-to-end weather forecasting systems.

Method: The authors developed MP-Bench, a dataset containing over 421,363 pairs of meteorological data and associated textual warnings. They also constructed the MMLM with adaptive fusion modules to process high-dimensional 4D meteorological data.

Result: Experiments using MP-Bench demonstrated that MMLM excels in multiple tasks, showing its ability to understand severe weather scenarios effectively.

Conclusion: MMLM and MP-Bench mark a significant advancement in AI-driven weather forecasting, enabling better understanding of severe weather events and automating forecasting processes. The authors will make the dataset and source code publicly accessible.

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [18] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: The paper introduces a generalized Python coding agent based on ReAct principles to automate the translation of natural language problem descriptions into formal constraint models, achieving success across all problems in a major benchmark.


<details>
  <summary>Details</summary>
Motivation: Develop a methodology to overcome limitations of fixed workflows in translating natural language constraints into formal models, which often fail on diverse problems.

Method: The paper utilizes a ReAct-based coding agent architecture with an IPython kernel for iterative development, integrating domain-specific knowledge via project prompts.

Result: The proposed method successfully solved all 101 problems of the CP-Bench benchmark, showing superior adaptability and success rate compared to traditional fixed workflows.

Conclusion: General-purpose coding tools combined with domain-expert prompts can surpass predefined workflows or tailored architectures in tackling constraint modeling tasks.

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [19] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: The paper introduces pushdown reward machines (pdRMs), an extension of reward machines, to handle temporally extended behaviors in deterministic context-free languages, coupling this with reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve upon the limitations of reward machines in reinforcement learning by enabling them to handle deterministic context-free languages, thereby enhancing their expressiveness and capability to handle temporally extended tasks.

Method: The authors extend reward machines to pushdown reward machines (pdRMs), define two policy variants (full stack access and top-k symbols access), and propose a procedure to check equivalence of their policy performance in terms of optimal expected reward. They also analyze theoretical expressive power and space complexity, and validate the approach experimentally.

Result: Theoretical results confirm the enhanced expressiveness of pdRMs over traditional RMs, and experiments demonstrate practical application in training agents for tasks calling for deterministic context-free language representation.

Conclusion: The proposed pdRMs significantly extend the capabilities of reward machines, enabling the encoding and learning of more complex, temporally extended behaviors, while maintaining the feasibility of reward optimization in reinforcement learning.

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [20] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: This paper introduces DGLS, a new framework to improve distributed constraint optimization by addressing GDBA's limitations, achieving better results on both general-valued and structured problems.


<details>
  <summary>Details</summary>
Motivation: GDBA struggles with poor local optima due to issues such as over-aggressive constraint violation, unbounded penalties, and lack of coordination. The aim is to enhance its performance for DCOPs.

Method: The proposed DGLS framework includes an adaptive violation condition, a penalty evaporation mechanism, and a synchronization scheme. This ensures bounded penalties and coordinated updates.

Result: DGLS shows theoretical and empirical improvements, achieving superior performance over leading methods like Damped Max-sum, particularly excelling on structured problems.

Conclusion: DGLS is a significant advancement for DCOPs, addressing key flaws in GDBA and delivering better solutions on benchmarks.

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [21] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse is a modular framework for distributed machine learning inference employing strategic cryptographic verification to allow flexible and targeted zero-knowledge proofs, optimizing scalability and trust minimization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the rigidity and high cost of full-model circuitization in zero-knowledge machine learning through a flexible approach to cryptographic verification.

Method: DSperse utilizes 'slices', verifiable subcomputations within a distributed inference framework, enabling targeted verification with support for audit, replication, and economic incentives.

Result: Evaluation with multiple proving systems revealed insights on memory usage, runtime, and circuit behavior under sliced vs unsliced configurations.

Conclusion: DSperse enables scalable and strategic zero-knowledge verification by aligning proof boundaries with the model's logical structure, supporting diverse deployment needs.

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [22] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: The paper introduces CRAMF, a framework designed to improve automatic mathematical formalization via large language models by leveraging concept-driven retrieval-augmented generation techniques.


<details>
  <summary>Details</summary>
Motivation: Interactive theorem provers are powerful but require labor-intensive and expert-driven formalization. Automating this process is promising yet challenged by issues like model hallucination and the semantic gap between natural language and formal logic.

Method: The authors created CRAMF, a framework that incorporates retrieval-augmented generation by building a concept-definition knowledge base from Mathlib4, augmented with context-aware query processing and a dual-channel hybrid retrieval strategy with reranking.

Result: CRAMF demonstrated improvements in autoformalization tasks on benchmarks such as miniF2F, ProofNet, and AdvancedMath, achieving up to 62.1% translation accuracy improvement and a 29.9% average relative improvement.

Conclusion: CRAMF advances automated mathematical formalization by addressing prior challenges through a targeted focus on concept-driven retrieval and precision, enhancing the efficacy of LLM-based theorem-proving processes.

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [23] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: A study integrating diverse data modalities using Transformer-based models to enhance crop yield prediction with a focus on model interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve agricultural applications like crop yield prediction through leveraging multimodal data while addressing the challenge of model interpretability.

Method: A Transformer-based model was utilized and feature attribution was assessed using Attention Rollout, Generic Attention, and Weighted Modality Activation methods, comparing them to Shapley-based estimations.

Result: Transformer-based models outperformed traditional architectures, achieving better R2 scores for crop yield prediction. Robust attribution methods were identified with Attention Rollout being superior.

Conclusion: The proposed Transformer-based multimodal model effectively improves crop yield prediction accuracy and interpretability, providing valuable insights via feature and modality attributions.

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [24] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: The paper critiques the use of Large Language Models (LLMs) to simulate human psychology, citing both theoretical and empirical evidence against their reliability.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs, like ChatGPT, can effectively simulate human psychology and replace human participants in psychological studies.

Method: Conceptual arguments paired with an empirical analysis, including examples where LLMs' responses diverged from human responses due to small wording changes or when different LLMs reacted inconsistently to new items.

Result: The study found inconsistencies in LLM responses, especially when slight wording variations were introduced or when comparing responses across different LLMs.

Conclusion: LLMs do not reliably simulate human psychology; researchers should validate LLMs against human data before considering their application in psychology-related research.

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [25] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: The paper introduces a benchmark called DatasetResearch to test AI agents’ ability to autonomously discover datasets from user demands. Current systems significantly underperform, achieving only 22% in demanding tasks.


<details>
  <summary>Details</summary>
Motivation: To address the critical question of whether AI agents can autonomously discover datasets that meet specific user needs, given the increasing importance of data availability over computational advancements.

Method: The authors developed DatasetResearch, a benchmark with 208 real-world tasks, and a tri-dimensional evaluation framework to analyze AI systems’ performance in dataset discovery, examining capabilities in both knowledge-intensive and reasoning-intensive challenges.

Result: Advanced AI systems scored only 22% in the DatasetResearch-pro subset, showing limitations in dataset discovery for complex tasks. Analysis revealed a dichotomy in agent capabilities: search agents excel in knowledge retrieval while synthesis agents perform better in reasoning but fail on edge cases.

Conclusion: The research establishes a baseline for dataset discovery agents, identifies critical limitations, and provides tools for advancing self-improving AI systems capable of universal dataset retrieval. The findings point to significant potential for improvement in AI dataset discovery.

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [26] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: MASteer is a framework leveraging representation engineering to improve trustworthiness of Large Language Models in a lightweight, automated, and adaptive way without additional training.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have ongoing trustworthiness issues, and current repair methods are costly, slow, or lack robustness and scalability, necessitating an efficient and automated alternative.

Method: The paper proposes MASteer, an end-to-end framework that includes two components: (1) AutoTester, which generates diverse steer samples using a multi-agent system, and (2) AutoRepairer, which formulates adaptive steering strategies with anchor vectors for inference-time adjustments.

Result: MASteer improves trustworthiness metrics significantly, showing 15.36% improvement on LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat while preserving general model capabilities.

Conclusion: MASteer is a robust, generalizable, and practical solution for scalable and efficient trustworthiness repair in LLMs, surpassing existing methods.

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [27] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: This paper proposes a biologically grounded and explainable AI framework based on active inference to model decision-making in embodied agents using simulation inspired by biological neuronal setups.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore purposeful behavior in autonomous agents by leveraging biologically based neuronal networks for potential improvements in power efficiency, data utilization, and model explanation.

Method: A framework rooted in active inference is proposed, coupled with experiment-informed generative models to simulate decision-making in a game environment similar to setups used with biological neurons.

Result: Agents demonstrated learning capability, showcasing the role of memory-based learning and predictive planning in achieving intelligent decision-making.

Conclusion: The framework advances explainable AI by utilizing biological principles, offering a scalable and insightful approach to comprehend purposeful behavior in intelligent agents.

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [28] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: This paper evaluates alternative methods for the implicit hitting set (IHS) approach, specifically exploring pseudo-Boolean (PB) reasoning and stochastic local search for optimization, comparing them with integer programming (IP).


<details>
  <summary>Details</summary>
Motivation: To address computational challenges and investigate new techniques for optimization problems within the IHS framework that reduce reliance on potentially unstable integer programming solvers and ensure correctness.

Method: Examined and evaluated the performance of pseudo-Boolean reasoning and stochastic local search as alternatives to integer programming for hitting set computations in the IHS approach.

Result: Pseudo-Boolean reasoning emerged as a competitive alternative, offering numerically exact solutions and correctness certificates, while retaining trade-offs in efficiency versus IP solvers.

Conclusion: PB reasoning offers a viable and reliable alternative to IP solvers for HS computations, with broader application potential for ensuring correctness in IHS frameworks.

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [29] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: Knowledge editing (KE) in multimodal medical tasks needs improvement, as current methods struggle with generalization and reasoning. MultiMedEdit offers a new benchmark to support advancements.


<details>
  <summary>Details</summary>
Motivation: To improve the integration of factual updates and visual reasoning in medical contexts, addressing gaps in safe and interpretable clinical decision-making for multimodal scenarios.

Method: A benchmark named MultiMedEdit was created to evaluate KE in clinical multimodal tasks using a new metric suite including reliability, generality, and locality. Paradigm comparisons and experiments were conducted under various KE settings.

Result: Results reveal the insufficiencies of current KE methods in generalization, long-tail reasoning, and complex workflows while offering an efficiency analysis. The benchmark identifies limitations and trade-offs in deployment.

Conclusion: MultiMedEdit sets the groundwork for enhancing multimodal medical knowledge editing and paves the way for more robust clinical applications in the future.

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [30] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: The paper introduces K-Dense Analyst, a hierarchical system that outperforms GPT-5 in bioinformatics analysis, bridging the gap between LLMs and real-world scientific workflows.


<details>
  <summary>Details</summary>
Motivation: Address the gap between data generation and meaningful scientific insights in bioinformatics by overcoming the limitations of LLMs in real-world workflows.

Method: The authors developed K-Dense Analyst, a dual-loop hierarchical multi-agent system that decomposes objectives into executable tasks, integrating planning, execution, and verification.

Result: K-Dense Analyst achieved a 29.2% accuracy on the BixBench benchmark, a 6.3 percentage point improvement over GPT-5, using Gemini 2.5 Pro as a base model.

Conclusion: Autonomous scientific reasoning systems must go beyond LLMs by incorporating purpose-built architectures; K-Dense Analyst marks a step toward autonomous computational biologists.

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [31] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: The paper addresses limitations of large language models (LLMs) in ethical moderation tasks, highlighting their struggles with nuanced moral reasoning, biases, and offensive content detection. A unified benchmark dataset and a fine-tuned model, SafePhi (QLoRA version of Phi-4), are introduced, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the increasing need for reliable moderation by LLMs in handling ethical issues like implicit hate, biases, and offensive language, which are critical in the context of the growing use of AI systems.

Method: An experimental framework was developed using SOTA models to assess human emotions and offensive behaviors. A unified benchmark dataset spanning 49 categories was introduced, alongside the creation of SafePhi, a QLoRA fine-tuned version of Phi-4.

Result: SafePhi outperformed existing benchmarks such as OpenAI Moderator and Llama Guard, achieving a Macro F1 score of 0.89 compared to 0.77 and 0.74, respectively.

Conclusion: Integrating more diverse, representative data and human-in-the-loop processes is crucial for enhancing the robustness and ethical capabilities of LLM-based moderation systems.

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [32] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: The paper presents a Feedback-Driven Decision Support System (DSS) aimed at improving student performance predictions through an adaptive, closed-loop LightGBM architecture.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning models in education are static and fail to adapt when new data, like post-intervention outcomes, become available.

Method: The authors propose an adaptive DSS with incremental retraining based on updated student results, using a LightGBM regressor, a Flask-based web interface, and SHAP for model transparency.

Result: The system achieved a 10.7% reduction in RMSE after incremental retraining, with improved predictions for students who received interventions.

Conclusion: The paper demonstrates the effectiveness of transitioning static educational predictors to adaptive systems, emphasizing human-centered, data-driven, and responsive AI. The framework is designed for seamless integration into LMS and institutional dashboards.

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [33] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: This paper proposes a framework using LLM-based agents for summarizing hierarchical enterprise data across multiple dimensions, achieving high performance in faithfulness, relevance, and insight quality.


<details>
  <summary>Details</summary>
Motivation: Traditional table-to-text models struggle with reasoning across hierarchical structures and context-aware deltas, which are vital for nuanced business reporting.

Method: The authors introduce a multi-agent pipeline leveraging LLMs for tasks like slicing data, variance detection, context construction, and text generation.

Result: The framework achieves 83% faithfulness to data, superior coverage of significant changes, and relevance scores of 4.4/5. It excels in capturing nuanced business scenarios, such as revenue changes due to price adjustments.

Conclusion: The proposed method demonstrates measurable improvements in summary accuracy and insight generation, particularly for complex business trade-offs, validated through evaluations on Kaggle datasets.

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [34] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: EndoAgent is a memory-guided AI system designed for adaptive reasoning and decision-making in endoscopic image analysis, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current AI systems for endoscopic diagnosis lack unified coordination across tasks and struggle with complex clinical workflows.

Method: EndoAgent employs a dual-memory system for short-term and long-term reasoning, integrates expert-designed tools, and benchmarks against 5,709 visual Q&A pairs.

Result: Experiments demonstrate EndoAgent's superior performance over general and medical multimodal models, showing advanced flexibility and reasoning.

Conclusion: EndoAgent addresses gaps in clinical AI applications, proving its capability to enhance endoscopic image diagnosis with iterative reasoning and tool integration.

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [35] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: The paper formalizes LLMs as probabilistic Turing machines, proving that illusions are inevitable in their behavior based on incomputability and information theory. However, it suggests two solutions: Retrieval Enhanced Generations (RAGs) and continuous learning via neural game theory.


<details>
  <summary>Details</summary>
Motivation: Address the fundamental issue of illusions in LLMs that hamper their reliable deployment.

Method: Develops formal theories using computational necessity hierarchy, learner pump lemma, and models solutions like RAGs as oracle machines and continuous learning as internalized oracles.

Result: Proves that illusions are inherent and conceptualizes solutions for overcoming them using rigorous theoretical frameworks.

Conclusion: Illusions in LLMs cannot be completely eliminated but can be bypassed with methods like RAGs and continuous learning mechanisms.

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [36] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: The paper critiques conventional domain-specific benchmarks for LLMs, introduces an iterative framework (Comp-Comp) focusing on comprehensiveness and compactness, and demonstrates its efficacy through a case study in academia.


<details>
  <summary>Details</summary>
Motivation: Current domain-specific benchmarks for LLMs often emphasize scaling laws and broad coverage but fail to address how corpus and QA set design affects models’ precision and recall, leaving a critical gap in benchmark construction strategies.

Method: The paper proposes the Comp-Comp framework, which emphasizes semantic comprehensiveness to ensure recall and compactness to enhance precision. This iterative process improves both corpus and QA set construction.

Result: The authors applied Comp-Comp in academia, leading to the development of XUBench, a large-scale closed-domain benchmark, demonstrating the effectiveness of their framework.

Conclusion: The Comp-Comp framework challenges the dominance of the scaling law in benchmark construction and introduces a versatile method that can extend to various domains, improving the evaluation and development of domain-specific LLMs.

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [37] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: Pentest-R1 enhances penetration testing by optimizing reasoning in large language models through a two-stage reinforcement learning process.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in error handling, reasoning, and complex cybersecurity tasks.

Method: Pentest-R1 employs offline RL with a real-world walkthrough dataset followed by online RL in an interactive Capture-The-Flag environment.

Result: Pentest-R1 achieves competitive success rates on AutoPenBench and Cybench benchmarks, outpacing most state-of-the-art models.

Conclusion: The framework demonstrates the importance of combining offline and online RL to refine LLM capabilities for advanced penetration testing tasks.

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [38] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: This paper introduces Invert4TVG, an innovative approach for Temporal Video Grounding that uses three novel inversion tasks to improve both localization accuracy and semantic action understanding, showing significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current TVG methods often overfit to Intersection-over-Union (IoU) metrics, neglecting semantic understanding of actions within the video and query, which is crucial for robust video grounding.

Method: The authors propose three inversion tasks: Verb Completion, Action Recognition, and Video Description, and integrate them with a reinforcement learning framework to optimize localization and semantic understanding.

Result: The proposed framework achieves a 7.1% improvement in R1@0.7 accuracy on the Charades-STA dataset for a 3B model, outperforming state-of-the-art methods.

Conclusion: Invert4TVG enhances Temporal Video Grounding by balancing localization with semantic understanding, broadening its capabilities for action comprehension and setting new benchmarks in TVG performance.

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [39] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: The paper evaluates Generative AI techniques, BERTopic and NMF, for theme generation in strategic planning for government organizations. BERTopic outperforms NMF.


<details>
  <summary>Details</summary>
Motivation: Strategic planning for large-scale government organizations is complex and meeting regulatory requirements is essential. Generative AI offers potential solutions.

Method: The study uses BERTopic and NMF topic modeling tools on reports from the Government Accountability Office to measure correlation with Vision Elements in a government strategic plan.

Result: Both techniques generate themes similar to all Vision Elements, with BERTopic achieving a stronger correlation in over half its topics.

Conclusion: BERTopic is the preferred tool for this application. Further work will explore operationalizing this process and evaluating other modules for GAI-enabled strategic planning.

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [40] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: This survey reviews techniques for creating self-evolving AI agents that adapt over time, introducing a framework to analyze key components and offering insights into domain-specific strategies, evaluation, and ethical challenges.


<details>
  <summary>Details</summary>
Motivation: Current AI agents often rely on static configurations that limit their capability to adapt to changing environments. There's a growing need for agents that can continuously evolve and improve, inspired by recent developments in agent evolution research.

Method: The study introduces a conceptual framework outlining four components (System Inputs, Agent System, Environment, and Optimisers) to analyze self-evolving techniques and systematically reviews these methods across various domains like biomedicine, finance, and programming.

Result: The framework provides a cohesive understanding of how self-evolving AI agents function and highlights diverse strategies for adapting agent systems. Domain-specific examples and strategies are reviewed, addressing optimization challenges.

Conclusion: The paper equips researchers and practitioners with the insights and tools necessary to develop adaptive and autonomous AI agents, addressing ethical and safety considerations essential for long-term effectiveness.

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [41] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: The paper develops a framework to integrate large language models (LLMs) into multi-agent decision-making systems to enhance reasoning and collaboration.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs for improving reasoning, communication, and collaboration among agents in complex decision-making scenarios.

Method: The approach integrates LLMs with techniques like advanced prompt engineering, memory architectures, multi-modal processing, and alignment through fine-tuning, tested via ablation studies.

Result: The framework demonstrates effective communication and strategy coordination in social dilemma games, showcasing its potential utility.

Conclusion: The integration of LLMs into multi-agent systems supports clear communication and improved decision-making, providing valuable insights for future development in this domain.

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [42] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: This paper introduces an evaluation framework allowing local, out-of-the-box LLMs to play Diplomacy without fine-tuning, using optimized game state text representations.


<details>
  <summary>Details</summary>
Motivation: Previous approaches struggled with the complexity and information density of Diplomacy's game state, requiring advanced LLMs or fine-tuning, making the game hard to study.

Method: The authors optimized textual game state representation for a 24B model, developed tooling for hypothesis testing and statistical analysis, and introduced Critical State Analysis for studying pivotal moments in games.

Result: Experiments revealed larger LLMs perform best, but smaller models play adequately. The framework allows effective performance without fine-tuning.

Conclusion: The harness democratizes strategic reasoning evaluation in LLMs, highlighting the natural emergence of these capabilities in standard models.

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [43] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: The paper proposes MCPToolBench++, a benchmark for evaluating AI agents, particularly LLMs, in using Model Context Protocol (MCP) tools effectively, addressing current challenges in evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the fragmented evaluation methods and lack of benchmarks to comprehensively test LLMs' and AI agents' ability to use MCP tools, which are growing in importance for enhancing functionality by integrating external data sources or APIs.

Method: The authors created MCPToolBench++, a benchmark with over 4,000 MCP servers in more than 40 categories, covering single-step and multi-step tool use cases to evaluate LLMs' performance.

Result: Evaluations of state-of-the-art (SOTA) LLMs with agentic capabilities were conducted using MCPToolBench++, and the results are reported, showcasing its effectiveness as a benchmarking tool.

Conclusion: MCPToolBench++ serves as a large-scale, multi-domain benchmarking solution to address challenges in assessing the performance of LLMs and AI agents in MCP tool use, enabling standardized evaluations across various categories.

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [44] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: The paper presents a new covert semantic communication paradigm that uses a jamming signal to protect against attacker eavesdropping. An advanced reinforcement learning algorithm optimizes privacy and transmission quality, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Ensuring secure and private semantic information transmission over unreliable communication channels without coordination between jammer and server.

Method: Proposes a prioritized sampling-assisted twin delayed deep deterministic policy gradient algorithm to optimize semantic transmission and power adaptation in a covert communication system.

Result: Algorithm improves privacy by 77.8% and transmission quality by 14.3% compared to standard reinforcement learning techniques.

Conclusion: The method successfully enhances both semantic privacy and quality of communication, proving its effectiveness over traditional methods.

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [45] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: The paper introduces HGMF, a method to enable Large Language Models (LLMs) to efficiently select tools from large libraries by hierarchically pruning irrelevant options.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to select the correct tools from large libraries due to limited context windows and noise from irrelevant options, resulting in poor accuracy and high computational costs.

Method: The Hierarchical Gaussian Mixture Framework (HGMF) maps user queries and tools’ descriptions into a unified semantic space, clusters them hierarchically using Gaussian Mixture Models (GMM), and filters candidates step-by-step to create a smaller, relevant set of tools.

Result: Experiments demonstrated that HGMF improves tool selection accuracy and reduces latency, showing its effectiveness and scalability for extensive tool libraries.

Conclusion: HGMF streamlines tool selection for LLMs, addressing noise and inefficiency in large hierarchical libraries, and providing a scalable, accurate solution.

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [46] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: The paper introduces ThinkTuning, a GRPO-based training method where a teacher model guides a student model, enhancing its reasoning skills beyond RL-alone methods.


<details>
  <summary>Details</summary>
Motivation: Current RL strategies reveal but do not instill new reasoning behaviors in language models, necessitating a training method to truly develop such abilities from scratch.

Method: ThinkTuning utilizes a GRPO-based approach where a teacher model provides corrective feedback to a student model, thereby shaping and improving its reasoning step-by-step.

Result: ThinkTuning outperforms zero-shot baselines by 3.85% on average and demonstrated specific improvements on MATH-500, AIME, and GPQA-Diamond benchmarks by 2.08%, 2.23%, and 3.99%, respectively.

Conclusion: Incorporating feedback through ThinkTuning enhances reasoning abilities in language models, indicating an effective way to train models lacking native self-reflective behavior.

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [47] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: The paper explores the use of multimodal AI for intelligent monitoring of poultry welfare by integrating data from various sources and achieving better outcomes compared to traditional methods. It addresses challenges like sensor fragility and deployment costs while introducing evaluation tools like DTS and DRI.


<details>
  <summary>Details</summary>
Motivation: The motivation is to revolutionize poultry welfare monitoring by replacing outdated, human-dependent methods with intelligent, data-driven systems that capture the complex dynamics of animal welfare in modern farms.

Method: The paper uses multimodal AI to integrate visual, acoustic, environmental, and physiological data streams. It evaluates feature-level fusion strategies and introduces novel evaluation tools, DTS and DRI, alongside a modular deployment framework.

Result: Feature-level fusion strategies were found to balance robustness and performance optimally. The introduced DTS and DRI tools enhance adaptability across farm settings and reliability of sensor data, enabling practical applications.

Conclusion: Multimodal AI offers scalable and proactive welfare systems, combining productivity with ethical animal care. The work highlights its potential while addressing key adoption barriers through novel tools and a modular framework.

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [48] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav introduces a modular, skill-based approach to Vision-and-Language Navigation (VLN), outperforming prior methods in unseen environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current VLN methods, especially in generalizing to unseen scenarios requiring complex reasoning.

Method: Introduced SkillNav, a modular framework that decomposes navigation into interpretable atomic skills and incorporates a zero-shot VLM-based router for agent selection.

Result: Achieved state-of-the-art performance on the R2R benchmark and strong generalization to the GSA-R2R benchmark.

Conclusion: The structured skill-based reasoning and dynamic agent selection in SkillNav significantly improve VLN agents' generalization and performance.

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [49] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: DiMuST is a socially enhanced POI recommendation model that resolves spatial-temporal misalignment issues for better recommendation accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with misalignment of spatial-temporal representations, leading to redundant information and reduced model interpretability.

Method: DiMuST employs a Disentangled variational multiplex graph Auto-Encoder (DAE), utilizing disentangled representation learning, a Product of Experts mechanism, and contrastive constraints.

Result: Experiments on two datasets show DiMuST outperforms current methods in POI recommendation across multiple evaluation metrics.

Conclusion: DiMuST effectively handles spatial-temporal transition representations while maintaining intrinsic relationships, improving recommendation performance and interpretability.

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [50] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: The paper introduces a multi-agent framework to address contextual privacy concerns in interactive settings involving large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Privacy remains a critical challenge when LLMs process data containing both public and private information, requiring innovative approaches to prevent sensitive data leakage.

Method: The authors propose a multi-agent framework that decomposes privacy tasks into subtasks like extraction and classification, paired with systematic ablation over information-flow topologies to analyze error propagation.

Result: Experiments on benchmarks such as ConfAIde and PrivacyLens show that their framework significantly reduces private information leakage (18–19%) while maintaining the integrity of public content.

Conclusion: The study highlights the efficacy of multi-agent systems in addressing contextual privacy concerns better than single-agent methods, with promising results for LLM applications.

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [51] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: The paper presents EMPATHIA, a multi-agent AI framework, to address refugee integration while preserving human dignity and incorporating cultural, emotional, and ethical perspectives.


<details>
  <summary>Details</summary>
Motivation: AI approaches to refugee integration are currently limited, focusing predominantly on narrow objectives like employment and failing to address broader, human-centric dimensions.

Method: The authors propose EMPATHIA, grounded in Kegan's Constructive Developmental Theory, with three modules: SEED (initial placement), RISE (early independence), and THRIVE (long-term success). Its transparent, multi-agent system operates using emotional, cultural, and ethical agents.

Result: Experiments on a dataset of 15,026 refugees achieved 87.4% validation convergence in generating explainable recommendations across five countries, using over 150 socioeconomic variables.

Conclusion: EMPATHIA offers a collaborative, human-augmenting AI framework that reconciles cultural, emotional, and ethical values, serving as a scalable model for decision-making in value-sensitive contexts.

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [52] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: The paper proposes an Ethics2Vec approach to align AI agents with human values by mapping decision-making strategies into vector representations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of aligning AI systems with human values, particularly when dealing with non-measurable and incommensurable ethical considerations.

Method: The Ethics2Vec approach adapts the Anything2vec technique to represent agent decision-making or control laws as multivariate vectors for comparison and assessment.

Result: The approach showcases its applicability starting from binary decision-making agents, eventually extending it to complex settings like self-driving cars.

Conclusion: Ethics2Vec presents a novel metric space enabling comparisons between human and AI values, addressing ethical alignment challenges across diverse AI applications.

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [53] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: Transformers struggle with learning planning problems due to arbitrary variable names causing symmetry issues. The paper proposes contrastive learning and architectural improvements to make transformers better suited for planning tasks, showcasing improved results over prior work like PlanGPT.


<details>
  <summary>Details</summary>
Motivation: Automated planning with transformer models is hindered by their inefficiency when handling symmetry in problem representations, causing combinatorial complexity and limiting their extrapolation capabilities.

Method: The paper introduces a novel contrastive learning objective to make transformers symmetry-aware, complemented by architectural refinements, enabling transformers to better handle planning tasks.

Result: The proposed approach demonstrates improved efficiency and effectiveness in plan-generation and heuristic-prediction tasks across multiple planning domains compared to existing methods like PlanGPT.

Conclusion: Symmetry-aware learning significantly enhances transformers' performance in automated planning, addressing limitations inherent in their design and opening the door to broader applications.

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [54] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: This paper introduces a refined policy selection for robust Markov decision processes (RMDPs), focusing on "optimal robust best-effort" (ORBE) policies that maximize both worst-case and non-adversarial expected returns.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance the policy selection in robust Markov decision processes, which traditionally only focus on optimizing worst-case scenarios, by introducing a refined criterion that also accounts for non-adversarial settings.

Method: The authors propose ORBE (Optimal Robust Best-Effort) policies, proving their existence, characterizing their structure, and developing an algorithm to compute them with minimal overhead compared to robust value iteration.

Result: The paper establishes the existence and practicality of ORBE policies and demonstrates their feasibility through numerical experiments.

Conclusion: ORBE policies serve as a principled tie-breaker among optimal robust policies by considering performance beyond worst-case scenarios, offering a more holistic and effective decision-making approach in RMDPs.

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [55] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: The paper proposes using a Knowledge Graph to assist first responders by providing AI-powered treatment recommendations during emergencies.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for fast and personalized rescue operations has highlighted the need for tools to help first responders better understand and address patients' conditions.

Method: The authors introduce a Knowledge Graph as a central knowledge representation, leveraging artificial intelligence to analyze and recommend treatments based on real-time vital data.

Result: The approach enables intelligent treatment recommendations, aiding first responders in emergency situations where time and precision are critical.

Conclusion: Integrating AI-driven Knowledge Graphs into rescue operations improves the decision-making process and optimizes healthcare delivery in emergencies.

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [56] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: The paper introduces \(X\)-evolve to optimize complex problems using solution spaces instead of individual solutions, making it computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Current methods combining LLMs and EAs for optimization face high computational costs due to focusing on individual solutions. There is a need for a more efficient approach.

Method: \(X\)-evolve employs LLM-generated tunable programs to define parametrized solution spaces, followed by a score-based search algorithm for efficient exploration based on objective function feedback.

Result: \(X\)-evolve solved three challenging problems: improving bounds in the cap set problem, enhancing Shannon capacity lower bounds in information theory, and generating superior heuristics for the bin packing problem.

Conclusion: The evolution of solution spaces empowers broader exploration, reduces search costs substantially, and enables tackling previously prohibitive high-dimensional optimization problems.

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


### [57] [Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots](https://arxiv.org/abs/2508.07941)
*Olivier Poulet,Frédéric Guinand,François Guérin*

Main category: cs.AI

TL;DR: This paper presents a collision avoidance method using LSTM for trajectory prediction and DQN for decision-making, showing reduced collisions and improved stability in robot navigation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve autonomous robot navigation by reducing the risk of collisions in constrained environments using predictive methods.

Method: An LSTM model predicts robots' next positions based on past trajectories. This is integrated into a DQN framework that dynamically adjusts rewards to anticipate collision risks.

Result: The method significantly reduces collisions and improves movement stability, even under low sampling frequencies (1 Hz).

Conclusion: The proposed approach is efficient, computationally light, and viable for embedded system implementation in autonomous robot systems.

Abstract: This article proposes a collision risk anticipation method based on
short-term prediction of the agents position. A Long Short-Term Memory (LSTM)
model, trained on past trajectories, is used to estimate the next position of
each robot. This prediction allows us to define an anticipated collision risk
by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.
The approach is tested in a constrained environment, where two robots move
without communication or identifiers. Despite a limited sampling frequency (1
Hz), the results show a significant decrease of the collisions number and a
stability improvement. The proposed method, which is computationally
inexpensive, appears particularly attractive for implementation on embedded
systems.

</details>


### [58] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: This paper introduces FEAT, an AI framework designed to improve forensic cause-of-death investigations using a domain-adapted large language model.


<details>
  <summary>Details</summary>
Motivation: Address systemic challenges in forensic cause-of-death determinations, such as workforce shortages and diagnostic variability, especially in high-volume systems like China's.

Method: FEAT employs an architecture with a central Planner, Local Solvers, Memory & Reflection modules, and a Global Solver, integrating AI tools like forensic-tuned LLMs, retrieval-augmented generation, and human-in-the-loop feedback.

Result: The system outperformed state-of-the-art AI in accuracy across diverse Chinese case cohorts, achieving robust generalization across regions and high expert concordance in validations.

Conclusion: FEAT demonstrates scalable and standardized death certification with expert-level rigor, offering a potential solution to workforce and consistency challenges in forensic systems.

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


### [59] [Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths](https://arxiv.org/abs/2508.08001)
*Rui Yao,Qi Chai,Jinhai Yao,Siyuan Li,Junhao Chen,Qi Zhang,Hao Wang*

Main category: cs.AI

TL;DR: The paper introduces an LLM-based framework to analyze 'Fedspeak,' the nuanced language of the U.S. Federal Reserve, with a focus on interpreting monetary policy stance and improving prediction reliability.


<details>
  <summary>Details</summary>
Motivation: Understanding and interpreting the implicit signals in 'Fedspeak' is critical for financial forecasting, economic analysis, and algorithmic decision-making, given its role in shaping economic expectations and outcomes.

Method: The framework uses large language models (LLMs) enhanced by domain-specific reasoning related to monetary policies and integrates a dynamic uncertainty decoding module to evaluate model prediction confidence.

Result: The proposed framework achieves state-of-the-art performance in classifying monetary policy stances and shows a positive correlation between uncertainty and model errors, improving diagnostic capabilities.

Conclusion: The introduced framework offers a powerful tool for analyzing and interpreting Fedspeak, with high classification accuracy and reliability, making it valuable for financial and economic applications.

Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a communication tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an LLM-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.

</details>


### [60] [Fitting Description Logic Ontologies to ABox and Query Examples](https://arxiv.org/abs/2508.08007)
*Maurice Funk,Marvin Grosser,Carsten Lutz*

Main category: cs.AI

TL;DR: The paper addresses the challenge of finding ontologies that fit provided positive and negative query examples. It determines computational complexity and provides characterizations for different logic languages and query types.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the ontology-mediated querying task, which involves designing an ontology that matches given logical constraints for various data and query settings.

Method: The authors examine fitting problems using description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ with query types including atomic queries, conjunctive queries, and unions of conjunctive queries. They provide effective characterizations and analyze computational difficulty.

Result: They establish that deciding the existence of a fitting ontology is $\small{CONP}$ for atomic queries and conjunctive queries, and $2E\small{XP}TIME$-complete for conjunctive and union queries, applying to both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

Conclusion: The study offers a clear understanding of computational challenges and feasibility for designing fitting ontologies across varying logics and query levels, revealing the complexity of the task.

Abstract: We study a fitting problem inspired by ontology-mediated querying: given a
collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash
q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for
all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as
ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be
${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

</details>


### [61] [AdaptFlow: Adaptive Workflow Optimization via Meta-Learning](https://arxiv.org/abs/2508.08053)
*Runchuan Zhu,Bowen Jiang,Lingrui Mei,Fangkai Yang,Lu Wang,Haoxiang Gao,Fengshuo Bai,Pu Zhao,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: AdaptFlow introduces a meta-learning framework for LLM-based workflows to enhance cross-task adaptability and generalization, performing better than existing approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of adaptability and scalability in static or manually designed LLM-based workflows for solving complex tasks.

Method: AdaptFlow employs a meta-learning framework inspired by MAML, featuring a bi-level optimization scheme. The inner loop customizes workflows for specific subtasks, while the outer loop updates a generalized workflow initialization.

Result: Experimental results demonstrate AdaptFlow's superior performance over manual and auto-generated workflows across question answering, code generation, and mathematical reasoning tasks, with state-of-the-art results.

Conclusion: AdaptFlow enables better adaptability and generalization for LLM workflows, offering a robust meta-learning approach for solving diverse and complex tasks.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in agentic workflows, which are structured sequences of LLM invocations
intended to solve complex tasks. However, existing approaches often rely on
static templates or manually designed workflows, which limit adaptability to
diverse tasks and hinder scalability. We propose AdaptFlow, a natural
language-based meta-learning framework inspired by model-agnostic meta-learning
(MAML). AdaptFlow learns a generalizable workflow initialization that enables
rapid subtask-level adaptation. It employs a bi-level optimization scheme: the
inner loop refines the workflow for a specific subtask using LLM-generated
feedback, while the outer loop updates the shared initialization to perform
well across tasks. This setup allows AdaptFlow to generalize effectively to
unseen tasks by adapting the initialized workflow through language-guided
modifications. Evaluated across question answering, code generation, and
mathematical reasoning benchmarks, AdaptFlow consistently outperforms both
manually crafted and automatically searched baselines, achieving
state-of-the-art results with strong generalization across tasks and models.
The source code and data are available at
https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.

</details>


### [62] [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](https://arxiv.org/abs/2508.08075)
*Meishen He,Wenjun Ma,Jiao Wang,Huijun Yue,Xiaoma Fan*

Main category: cs.AI

TL;DR: The study introduces Full Negation Belief Transformation (FNBT) to address challenges in information fusion from heterogeneous data frames, based on Dempster-Shafer theory.


<details>
  <summary>Details</summary>
Motivation: Traditional fusion methods struggle with unsatisfactory results when combining data from different frames or organizations due to data silos.

Method: FNBT extends frames to accommodate elements from heterogeneous sources and uses a full negation mechanism to transform mass functions for compatibility with existing combination rules.

Result: FNBT showcases theoretical properties like invariance, heritability, and conflict elimination, and achieves better pattern classification and resolves Zadeh's counterexample in empirical validations.

Conclusion: FNBT provides an effective solution for fusing information from heterogeneous sources within the framework of the Dempster-Shafer theory.

Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field
of information fusion under uncertainty. Most existing research focuses on
combining evidence within the same frame of discernment. However, in real-world
scenarios, trained algorithms or data often originate from different regions or
organizations, where data silos are prevalent. As a result, using different
data sources or models to generate basic probability assignments may lead to
heterogeneous frames, for which traditional fusion methods often yield
unsatisfactory results. To address this challenge, this study proposes an
open-world information fusion method, termed Full Negation Belief
Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a
criterion is introduced to determine whether a given fusion task belongs to the
open-world setting. Then, by extending the frames, the method can accommodate
elements from heterogeneous frames. Finally, a full negation mechanism is
employed to transform the mass functions, so that existing combination rules
can be applied to the transformed mass functions for such information fusion.
Theoretically, the proposed method satisfies three desirable properties, which
are formally proven: mass function invariance, heritability, and essential
conflict elimination. Empirically, FNBT demonstrates superior performance in
pattern classification tasks on real-world datasets and successfully resolves
Zadeh's counterexample, thereby validating its practical effectiveness.

</details>


### [63] [TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork](https://arxiv.org/abs/2508.08115)
*Pranav Pushkar Mishra,Mohammad Arvan,Mohan Zalake*

Main category: cs.AI

TL;DR: TeamMedAgents introduces a multi-agent system for medical decision-making that integrates teamwork principles from human collaboration into AI systems, showing improved performance on majority of medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve medical decision-making processes using AI by systematically incorporating evidence-based teamwork principles from human collaboration.

Method: The authors adapted and implemented six core teamwork components (team leadership, mutual performance monitoring, etc.) based on Salas et al.'s 'Big Five' model, validating their impact on medical reasoning tasks using eight benchmarks and conducting ablation studies.

Result: Evaluation across eight medical benchmarks demonstrated consistent performance improvements in seven datasets. Ablation studies provided insights into optimal teamwork configurations.

Conclusion: TeamMedAgents establishes the viability of translating human teamwork theories into AI systems, advancing collaborative AI design for critical decision-making domains.

Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (LLMs). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop communication, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.

</details>


### [64] [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127)
*Rui Miao,Yixin Liu,Yili Wang,Xu Shen,Yue Tan,Yiwei Dai,Shirui Pan,Xin Wang*

Main category: cs.AI

TL;DR: The paper introduces BlindGuard, an unsupervised defense method addressing propagation vulnerabilities in LLM-based multi-agent systems (MAS) without requiring prior knowledge or attack-specific labeling.


<details>
  <summary>Details</summary>
Motivation: Current MAS security models depend heavily on labeled malicious agent data, which is impractical for real-world deployment. Thus, more generalizable and label-free defense mechanisms are needed.

Method: BlindGuard implements a hierarchical agent encoder capturing interaction patterns at various levels (individual, neighborhood, global), and employs corruption-guided detection using directional noise injection and contrastive learning to train solely on normal agent behaviors.

Result: BlindGuard effectively detects malicious agents across a broad range of attack types (e.g., prompt injection, memory poisoning, tool attacks) in MAS with diverse communication patterns while outperforming supervised baseline methods in generalizability.

Conclusion: BlindGuard offers a practical, generalizable unsupervised approach to defending MAS against malicious agents and diverse attacks, addressing prior limitations in supervised methods.

Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various communication patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.

</details>


### [65] [From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework](https://arxiv.org/abs/2508.08147)
*Yunkai Hu,Tianqiao Zhao,Meng Yue*

Main category: cs.AI

TL;DR: This paper proposes an LLM-assisted agent to convert natural-language power optimization problems into solver-ready models, producing reliable solutions through validation and repair techniques.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies and inaccuracies of using LLMs alone in mathematical optimization by integrating them with proven optimization solvers.

Method: A pipeline combining an LLM with domain-specific prompts, validation processes, iterative repair, and optimization solver integration for translating and solving power system problems.

Result: Tested on the unit commitment problem, demonstrating optimal or near-optimal solutions with improved reliability using task-specific validations.

Conclusion: Integrating LLMs with systematic validation and optimization solvers creates a robust approach for bridging natural-language problem descriptions with executable energy system solutions.

Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent
that automatically converts natural-language descriptions of power system
optimization scenarios into compact, solver-ready formulations and generates
corresponding solutions. In contrast to approaches that rely solely on LLM to
produce solutions directly, the proposed method focuses on discovering a
mathematically compatible formulation that can be efficiently solved by
off-the-shelf optimization solvers. Directly using LLMs to produce solutions
often leads to infeasible or suboptimal results, as these models lack the
numerical precision and constraint-handling capabilities of established
optimization solvers. The pipeline integrates a domain-aware prompt and schema
with an LLM, enforces feasibility through systematic validation and iterative
repair, and returns both solver-ready models and user-facing results. Using the
unit commitment problem as a representative case study, the agent produces
optimal or near-optimal schedules along with the associated objective costs.
Results demonstrate that coupling the solver with task-specific validation
significantly enhances solution reliability. This work shows that combining AI
with established optimization frameworks bridges high-level problem
descriptions and executable mathematical models, enabling more efficient
decision-making in energy systems

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [66] [SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency](https://arxiv.org/abs/2508.06978)
*Kwanhee Kyung,Sungmin Yun,Jung Ho Ahn*

Main category: cs.AR

TL;DR: The paper studies the energy implications of offloading MoE expert weights in LLM inference to SSDs, highlighting the significant increase in energy consumption and suggesting future Flash improvements may help address this issue.


<details>
  <summary>Details</summary>
Motivation: Scaling LLMs using MoE to trillions of parameters necessitates innovative memory management due to limited DRAM capacity, motivating the exploration of SSDs for offloading weights despite their energy drawbacks.

Method: The study quantitatively analyzes the energy usage of MoE weight offloading during LLM inference, comparing scenarios using SSDs, DDR memory, and HBM in terms of energy efficiency and latency mitigation techniques.

Result: The analysis reveals a drastic increase in energy consumption (up to ~12x) when offloading MoE weights to SSDs compared to using HBM, overshadowing latency advantages achieved through techniques like prefetching.

Conclusion: SSDs are currently energy-inefficient for MoE model inference offloading, but future optimizations in Flash energy efficiency, by an order of magnitude, could make SSDs a viable alternative for such tasks.

Abstract: Large Language Models (LLMs) applying Mixture-of-Experts (MoE) scale to
trillions of parameters but require vast memory, motivating a line of research
to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs.
While SSDs provide cost-effective capacity, their read energy per bit is
substantially higher than that of DRAM. This paper quantitatively analyzes the
energy implications of offloading MoE expert weights to SSDs during the
critical decode stage of LLM inference. Our analysis, comparing SSD, CPU memory
(DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that
offloading MoE weights to current SSDs drastically increases
per-token-generation energy consumption (e.g., by up to ~12x compared to the
HBM baseline), dominating the total inference energy budget. Although
techniques like prefetching effectively hide access latency, they cannot
mitigate this fundamental energy penalty. We further explore future
technological scaling, finding that the inherent sparsity of MoE models could
potentially make SSDs energy-viable if Flash read energy improves
significantly, roughly by an order of magnitude.

</details>


### [67] [Physical Design Exploration of a Wire-Friendly Domain-Specific Processor for Angstrom-Era Nodes](https://arxiv.org/abs/2508.07110)
*Lorenzo Ruotolo,Lara Orlandic,Pengbo Yu,Moritz Brunion,Daniele Jahier Pagliari,Dwaipayan Biswas,Giovanni Ansaloni,David Atienza,Julien Ryckaert,Francky Catthoor,Yukai Chen*

Main category: cs.AR

TL;DR: The paper explores physical design of a DSIP for ML, achieving 2x lower wire length and 3x higher density compared to state-of-the-art designs.


<details>
  <summary>Details</summary>
Motivation: Addressing interconnect efficiency challenges in Angstrom-era technology for machine learning processors.

Method: Evaluated five DSIP configurations using IMEC A10 nanosheet node and compared against state-of-the-art (VWR2A) DSIP.

Result: Achieved >2x lower wire length and >3x higher core density, with minimal layout intervention.

Conclusion: Proposed architecture offers high physical efficiency and cost-effective wire-friendly implementation, suitable for next-gen DSIP designs.

Abstract: This paper presents the physical design exploration of a domain-specific
processor (DSIP) architecture targeted at machine learning (ML), addressing the
challenges of interconnect efficiency in advanced Angstrom-era technologies.
The design emphasizes reduced wire length and high core density by utilizing
specialized memory structures and SIMD (Single Instruction, Multiple Data)
units. Five configurations are synthesized and evaluated using the IMEC A10
nanosheet node PDK. Key physical design metrics are compared across
configurations and against VWR2A, a state-of-the-art (SoA) DSIP baseline.
Results show that our architecture achieves over 2x lower normalized wire
length and more than 3x higher density than the SoA, with low variability in
the metrics across all configurations, making it a promising solution for
next-generation DSIP designs. These improvements are achieved with minimal
manual layout intervention, demonstrating the architecture's intrinsic physical
efficiency and potential for low-cost wire-friendly implementation.

</details>


### [68] [LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization](https://arxiv.org/abs/2508.07227)
*Siyuan He,Zhantong Zhu,Yandong He,Tianyu Jia*

Main category: cs.AR

TL;DR: LP-Spec proposes a hybrid LPDDR5 performance-enhanced PIM architecture with draft token pruning and dynamic workload scheduling to tackle challenges in low-resource LLM inference on mobile devices.


<details>
  <summary>Details</summary>
Motivation: The difficulty of running LLM inference on mobile devices arises due to constraints in memory bandwidth and computational resources.

Method: The paper introduces a co-design architecture using enhanced PIM technologies, dynamic workload scheduling, and draft token pruning to optimize speculative inference for LLMs.

Result: LP-Spec achieves significant advancements in key performance metrics like 13.21x faster execution, 7.56x better energy efficiency, and 99.87x higher energy-delay-product efficiency compared to standard mobile solutions.

Conclusion: LP-Spec effectively minimizes energy consumption and boosts efficiency for mobile LLM inference through its novel architectural approaches and hardware-data integration.

Abstract: LLM inference on mobile devices faces extraneous challenges due to limited
memory bandwidth and computational resources. To address these issues,
speculative inference and processing-in-memory (PIM) techniques have been
explored at the algorithmic and hardware levels. However, speculative inference
results in more compute-intensive GEMM operations, creating new design
trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there
exists a significant amount of redundant draft tokens in tree-based speculative
inference, necessitating efficient token management schemes to minimize energy
consumption. In this work, we present LP-Spec, an architecture-dataflow
co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with
draft token pruning and dynamic workload scheduling to accelerate LLM
speculative inference. A near-data memory controller is proposed to enable data
reallocation between DRAM and PIM banks. Furthermore, a data allocation unit
based on the hardware-aware draft token pruner is developed to minimize energy
consumption and fully exploit parallel execution opportunities. Compared to
end-to-end LLM inference on other mobile solutions such as mobile NPUs or
GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x
improvements in performance, energy efficiency, and energy-delay-product (EDP).
Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and
415.31x EDP reduction benefits.

</details>


### [69] [Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference](https://arxiv.org/abs/2508.07252)
*Siyuan He,Peiran Yan,Yandong He,Youwei Zhuo,Tianyu Jia*

Main category: cs.AR

TL;DR: The paper introduces Tasa, a thermally optimized heterogeneous architecture for better performance in 3D-stacked systems, achieving significant scalability and speedup in LLM inference.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the major bottleneck of autoregressive decoding in large language models (LLMs), caused by memory-intensive operations and bandwidth limitations in existing hardware, while mitigating thermal challenges faced by 3D-stacked architectures.

Method: The authors propose Tasa, a heterogeneous architecture utilizing high-performance cores for computation and high-efficiency cores for memory-intensive tasks. Additionally, it employs thermal optimization strategies and bandwidth sharing scheduling to manage thermal issues and enhance bandwidth utilization.

Result: Tasa reduces peak temperatures by up to 9.37°C compared to homogeneous 3D-stacked architectures, improving scalability. For Llama-65B and GPT-3 66B inference, it achieves 2.85x and 2.21x speedups over GPU baselines and state-of-the-art accelerators.

Conclusion: Tasa effectively balances thermal constraints and computational demands in 3D-stacked architectures, offering significant performance improvements and scalability for autoregressive decoding in LLMs.

Abstract: The autoregressive decoding in LLMs is the major inference bottleneck due to
the memory-intensive operations and limited hardware bandwidth. 3D-stacked
architecture is a promising solution with significantly improved memory
bandwidth, which vertically stacked multi DRAM dies on top of logic die.
However, our experiments also show the 3D-stacked architecture faces severer
thermal issues compared to 2D architecture, in terms of thermal temperature,
gradient and scalability. To better exploit the potential of 3D-stacked
architecture, we present Tasa, a heterogeneous architecture with cross-stack
thermal optimizations to balance the temperature distribution and maximize the
performance under the thermal constraints. High-performance core is designed
for compute-intensive operations, while high-efficiency core is used for
memory-intensive operators, e.g. attention layers. Furthermore, we propose a
bandwidth sharing scheduling to improve the bandwidth utilization in such
heterogeneous architecture. Extensive thermal experiments show that our Tasa
architecture demonstrates greater scalability compared with the homogeneous
3D-stacked architecture, i.e. up to 5.55 $\tccentigrade$, 9.37 $\tccentigrade$,
and 7.91 $\tccentigrade$ peak temperature reduction for 48, 60, and 72 core
configurations. Our experimental for Llama-65B and GPT-3 66B inferences also
demonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and
state-of-the-art heterogeneous PIM-based LLM accelerator

</details>


### [70] [The Monte Carlo Method and New Device and Architectural Techniques for Accelerating It](https://arxiv.org/abs/2508.07457)
*Janith Petangoda,Chatura Samarakoon,James Meech,Divya Thekke Kanapram,Hamid Toshani,Nathaniel Tye,Vasileios Tsoutsouras,Phillip Stanley-Marbell*

Main category: cs.AR

TL;DR: The paper introduces advancements in Monte Carlo methods and new processor architectures for handling uncertainty in computing systems interacting with real-world processes.


<details>
  <summary>Details</summary>
Motivation: Current computing systems need to process uncertain data effectively and Monte Carlo methods face limitations that require innovative solutions.

Method: The study presents a framework for Monte Carlo methods, advances in physics-based random variate generators (PPRVGs), and explores architectural techniques for convergence-oblivious processing using microarchitectural state.

Result: It demonstrates alternatives to traditional Monte Carlo sampling and shows how new processors can compute on probability distributions without requiring convergence.

Conclusion: The work provides foundational insights into overcoming limitations in managing uncertainty, which could influence the design of future processing systems.

Abstract: Computing systems interacting with real-world processes must safely and
reliably process uncertain data. The Monte Carlo method is a popular approach
for computing with such uncertain values. This article introduces a framework
for describing the Monte Carlo method and highlights two advances in the domain
of physics-based non-uniform random variate generators (PPRVGs) to overcome
common limitations of traditional Monte Carlo sampling. This article also
highlights recent advances in architectural techniques that eliminate the need
to use the Monte Carlo method by leveraging distributional microarchitectural
state to natively compute on probability distributions. Unlike Monte Carlo
methods, uncertainty-tracking processor architectures can be said to be
convergence-oblivious.

</details>


### [71] [A Matrix Decomposition Method for Odd-Type Gaussian Normal Basis Multiplication](https://arxiv.org/abs/2508.07541)
*Kittiphon Phalakarn,Athasit Surarerks*

Main category: cs.AR

TL;DR: The paper proposes a method to reduce the space complexity of odd-type Gaussian normal basis multipliers over the binary field GF(2^k), decreasing XOR gate usage with minor trade-offs in critical path delay.


<details>
  <summary>Details</summary>
Motivation: Currently, space complexity improvement techniques in binary field multipliers are largely focused on optimal or even-type Gaussian normal bases, leaving a gap for addressing odd-type Gaussian normal bases.

Method: The method adapts a matrix decomposition technique, typically used for optimal normal bases, to odd-type Gaussian normal basis multipliers.

Result: The proposed method achieves a reduction in the number of XOR gates required in the implementation while introducing only a minimal increase in critical path delay.

Conclusion: This approach successfully addresses the overlooked domain of odd-type Gaussian normal bases, optimizing space complexity with manageable trade-offs, thereby broadening the scope of efficient implementations.

Abstract: Normal basis is used in many applications because of the efficiency of the
implementation. However, most space complexity reduction techniques for binary
field multiplier are applicable for only optimal normal basis or Gaussian
normal basis of even type. There are 187 binary fields GF(2^k) for k from 2 to
1,000 that use odd-type Gaussian normal basis. This paper presents a method to
reduce the space complexity of odd-type Gaussian normal basis multipliers over
binary field GF(2^k). The idea is adapted from the matrix decomposition method
for optimal normal basis. The result shows that our space complexity reduction
method can reduce the number of XOR gates used in the implementation comparing
to previous works with a small trade-off in critical path delay.

</details>


### [72] [ARISE: Automating RISC-V Instruction Set Extension](https://arxiv.org/abs/2508.07725)
*Andreas Hager-Clukas,Philipp van Kempen,Stefan Wallentowitz*

Main category: cs.AR

TL;DR: This paper introduces ARISE, a tool for automating RISC-V instruction set generation to optimize code size and instruction count.


<details>
  <summary>Details</summary>
Motivation: Manual optimization of RISC-V ISA to meet specific requirements is a challenging task. This paper aims to address this gap.

Method: ARISE utilizes assembly patterns and a set of extendable metrics with CoreDSL language to generate optimized RISC-V instructions.

Result: ARISE achieves improvements of 1.48% in static code size, 3.84% in dynamic code size, and 7.39% in executed instruction count for Embench-IoT.

Conclusion: ARISE successfully automates RISC-V instruction generation, enhancing optimization in code size and execution efficiency while integrating with advanced tools.

Abstract: RISC-V is an extendable Instruction Set Architecture, growing in popularity
for embedded systems. However, optimizing it to specific requirements, imposes
a great deal of manual effort. To bridge the gap between software and ISA, the
tool ARISE is presented. It automates the generation of RISC-V instructions
based on assembly patterns, which are selected by an extendable set of metrics.
These metrics implement the optimization goals of code size and instruction
count reduction, both statically and dynamically. The instruction set
extensions are generated using the ISA description language CoreDSL. Allowing
seamless embedding in advanced tools such as the retargeting compiler Seal5 or
the instruction set simulator ETISS. ARISE improves the static code size by
1.48% and the dynamic code size by 3.84%, as well as the number of instructions
to be executed by 7.39% on average for Embench-Iot.

</details>


### [73] [TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference](https://arxiv.org/abs/2508.07796)
*Dengke Han,Duo Wang,Mingyu Yan,Xiaochun Ye,Dongrui Fan*

Main category: cs.AR

TL;DR: The paper introduces TVL-HGNN, a hardware accelerator that addresses memory inefficiencies in heterogeneous graph neural networks (HGNNs) inference, achieving significant speedups and energy savings.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to address the critical memory inefficiencies in HGNN inference stemming from per-semantic execution paradigms and redundant memory accesses during neighbor aggregation. These inefficiencies hinder scalability and performance.

Method: The paper introduces a semantics-complete execution paradigm to eliminate intermediate storage and redundant memory access. It also designs TVL-HGNN, a reconfigurable hardware accelerator alongside a vertex grouping technique to reduce redundant neighbor access, implemented in hardware.

Result: TVL-HGNN demonstrates average speedups of 7.85x over NVIDIA A100 GPU and 1.41x over HiHGNN accelerator. Additionally, it reduces energy consumption by 98.79% compared to GPU and 32.61% compared to HiHGNN.

Conclusion: The proposed TVL-HGNN hardware accelerator, coupled with optimized execution paradigms and vertex grouping techniques, significantly boosts HGNN inference performance while drastically reducing energy usage.

Abstract: Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneous
graph data and are widely applied in critical domains. In HGNN inference, the
neighbor aggregation stage is the primary performance determinant, yet it
suffers from two major sources of memory inefficiency. First, the commonly
adopted per-semantic execution paradigm stores intermediate aggregation results
for each semantic prior to semantic fusion, causing substantial memory
expansion. Second, the aggregation process incurs extensive redundant memory
accesses, including repeated loading of target vertex features across semantics
and repeated accesses to shared neighbors due to cross-semantic neighborhood
overlap. These inefficiencies severely limit scalability and reduce HGNN
inference performance.
  In this work, we first propose a semantics-complete execution paradigm from a
vertex perspective that eliminates per-semantic intermediate storage and
redundant target vertex accesses. Building on this paradigm, we design
TVL-HGNN, a reconfigurable hardware accelerator optimized for efficient
aggregation. In addition, we introduce a vertex grouping technique based on
cross-semantic neighborhood overlap, with hardware implementation, to reduce
redundant accesses to shared neighbors. Experimental results demonstrate that
TVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPU
and the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducing
energy consumption by 98.79% and 32.61%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [74] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: The paper proposes a methodology for enriching Portuguese news datasets with external evidence using Large Language Models and search engine APIs to support Semi-Automated Fact-Checking systems.


<details>
  <summary>Details</summary>
Motivation: There is a lack of publicly available Portuguese datasets that incorporate external evidence, hindering the development of effective Semi-Automated Fact-Checking systems.

Method: The study uses Large Language Models to extract claims and search engine APIs for retrieving external evidence. It also incorporates a framework for data validation and preprocessing.

Result: Enriched Portuguese news datasets with external evidence were developed, improving their utility for AFC systems.

Conclusion: By bridging a significant gap in resources for the Portuguese language, this methodology facilitates advancements in fact-checking technologies and enhances dataset quality.

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [75] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: The paper explores dynamic prompting strategies using retrieval-augmented generation (RAG) to improve few-shot biomedical named entity recognition (NER) performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of using large language models (LLMs) for few-shot biomedical NER, where limited training data hampers performance.

Method: The researchers implement both static and dynamic prompt engineering techniques, leveraging in-context learning examples selected based on text similarity, and dynamically updating prompts during inference. They evaluate the methods on five biomedical NER datasets.

Result: Static prompting with structured components improved average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA. Dynamic prompting further enhanced performance, especially using TF-IDF and SBERT retrieval methods, improving F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings, respectively.

Conclusion: Dynamic prompting with RAG offers a significant advantage for biomedical NER in few-shot scenarios, demonstrating the importance of contextually adaptive prompts.

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [76] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: This paper introduces "CarbonScaling," an analytical framework that integrates neural scaling laws with carbon footprint estimates for LLM training, emphasizing sustainability.


<details>
  <summary>Details</summary>
Motivation: Address sustainability concerns by quantifying carbon emissions associated with large language model (LLM) training.

Method: Develop an analytical framework combining models for neural scaling laws, hardware evolution, parallelism optimization, and carbon estimation.

Result: Carbon emissions scale exponentially with LLM size despite hardware advancements, but training optimizations can mitigate inefficiencies.

Conclusion: CarbonScaling highlights the environmental impact of LLM scaling and provides actionable insights for more sustainable training practices.

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [77] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: The paper studies tokenization in large language models (LLMs), focusing on multilingual contexts, particularly Indic scripts, and introduces improvements leading to better token-to-word ratio, model performance, and inference speed.


<details>
  <summary>Details</summary>
Motivation: Tokenization is often neglected in LLM development, despite its significant impact on efficiency and performance, particularly in linguistically diverse contexts.

Method: The study analyzes the relationship between vocabulary size, pre-tokenization rules, and training-corpus composition, focusing on Indic scripts; it proposes a novel data composition algorithm for tokenizer training.

Result: The novel tokenizer reduces the average token-to-word ratio by about 6% and achieves over 40% improvement compared to state-of-the-art multilingual Indic models, leading to better performance and faster inference.

Conclusion: Tokenization is a critical factor alongside architecture and objectives for building scalable and efficient multilingual LLMs, as demonstrated by the proposed techniques.

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [78] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: This paper introduces AEALT, a framework that integrates dimension reduction into workflows of pre-trained LLMs to enhance computational efficiency and task relevance.


<details>
  <summary>Details</summary>
Motivation: High-dimensional embeddings from LLMs hinder efficiency and increase computational costs in downstream tasks.

Method: AEALT involves extracting LLM text embeddings, applying them to a supervised augmented autoencoder, and learning low-dimensional latent factors tailored to task needs.

Result: AEALT improves performance in classification, anomaly detection, and prediction tasks, showing better results than raw embeddings and standard dimension reduction methods.

Conclusion: AEALT successfully combines dimension reduction with LLMs, enhancing task-specific efficiency without sacrificing effectiveness in various real-world applications.

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [79] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: The study explores large language models' ability to adaptively guide learners by assessing instructional guidance capabilities through a benchmark called GuideEval. Findings highlight existing LLMs' limitations and offer fine-tuning strategies to enhance adaptive tutoring.


<details>
  <summary>Details</summary>
Motivation: To address the gap in research around how LLMs adaptively guide learners based on their cognitive states rather than just generating questions, and to analyze their effectiveness in providing dynamic tutoring strategies.

Method: The authors propose GuideEval, a benchmark that evaluates LLMs' pedagogical guidance using a three-phase framework: Perception (inferring cognitive states), Orchestration (adapting strategies), and Elicitation (prompting reflection). A behavior-guided finetuning strategy is also introduced.

Result: Analysis showed that current LLMs struggle with adaptive scaffolding in guiding confused learners or redirecting them effectively. However, behavior-guided finetuning improved their guidance capabilities.

Conclusion: The study advocates for a more interactive and learner-centered evaluation of LLMs, shifting away from pure content evaluation towards assessing their broader instructional guidance abilities.

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [80] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: The paper introduces a method to produce high-quality synthetic datasets for unlearning specific knowledge in language models using a structured prompting pipeline.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable and efficient unlearning of specific knowledge domains in modern language models without needing manual intervention.

Method: The authors propose generating synthetic data resembling a 'textbook' through an automated pipeline using language models, requiring only the input of a domain name.

Result: The experiments, including on topics like biosecurity and cybersecurity, demonstrate that the synthetic datasets outperform baseline alternatives and are on par with expert-curated sets. Multi-step generation also improves dataset diversity and unlearning effectiveness.

Conclusion: Synthetic datasets are a promising solution for scalable and practical unlearning in language models, minimizing manual effort. The authors provide open-source access to their methods and datasets.

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [81] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: The paper introduces BrowseComp-Plus, a benchmark that facilitates controlled experimentation for evaluating deep research systems using a fixed corpus.


<details>
  <summary>Details</summary>
Motivation: To address limitations of current benchmarks relying on dynamic web search APIs that hinder fairness, reproducibility, and transparency in evaluating the performance of deep research agents.

Method: The authors propose a benchmark called BrowseComp-Plus, based on BrowseComp, but refined by using a fixed corpus with human-verified supporting documents and challenging negatives.

Result: BrowseComp-Plus demonstrates its utility by showing clear performance distinctions among systems, e.g., Search-R1 achieves 3.86% accuracy, GPT-5 achieves 55.9%, and GPT-5 integrated with Qwen3-Embedding-8B retriever achieves 70.1%.

Conclusion: The benchmark enables controlled evaluations and nuanced insights into deep research systems, paving the way for advancements in retrieval effectiveness and context engineering.

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [82] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
*Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: The paper investigates how alternatives to standard Byte-Pair Encoding (BPE)—including those that deviate from or eliminate reliance on the merge list—impact language model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether BPE alternatives, which avoid potential privacy vulnerabilities associated with merge lists, can maintain model performance in downstream tasks.

Method: The authors tested two main types of BPE inference schemes: (1) targeted deviations from merge lists, such as random orders and corrupted lists, and (2) non-targeted algorithms that are independent of the merge list, focusing on alternative text compression methods.

Result: Targeted deviations significantly degraded language model performance, whereas non-targeted methods showed minimal impact on performance, indicating that merge lists might not be as critical for effective tokenization.

Conclusion: Non-targeted, merge-list-free BPE inference algorithms could lead to more straightforward and privacy-preserving tokenization methods, without severely impairing model performance.

Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a
learned token vocabulary with a detailed merge list. Recent work has shown that
this merge list exposes a potential attack surface for extracting information
about language model's training data. In this paper, we explore the downstream
impact of BPE inference algorithms that do not rely on this merge list at all,
and hence differ from the encoding process during BPE training. To address this
question, we investigate two broad classes of BPE inference schemes that differ
from BPE application during training: a) targeted deviation from merge-lists
including random merge orders, and various corruptions of merge list involving
deletion/truncation, and b) non-targeted BPE inference algorithms that do not
depend on the merge list but focus on compressing the text either greedily or
exactly. Extensive experiments across diverse language modeling tasks like
accuracy-based QA benchmarks, machine translation, and open-ended generation
reveal that while targeted deviation from the merge lists exhibits significant
degradation in language model performance, the non-targeted merge-list-free
inference algorithms result in minimal impact on downstream performance that is
often much smaller than expected. These findings pave way for simpler and
potentially more privacy-preserving tokenization schemes that do not
catastrophically compromise model performance.

</details>


### [83] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
*Daniel Wang,Eli Brignac,Minjia Mao,Xiao Fang*

Main category: cs.CL

TL;DR: The paper investigates stereotype and deviation biases in large language models, finding notable biases impacting demographic group associations in outputs.


<details>
  <summary>Details</summary>
Motivation: To understand and highlight biases in large language models that may lead to harmful outputs.

Method: Examining how four advanced LLMs generate profiles related to demographics and their link to attributes like political affiliation, religion, and sexual orientation.

Result: The study reveals significant stereotype and deviation biases in all four evaluated LLMs, particularly towards multiple demographic groups.

Conclusion: The research emphasizes the presence and consequences of biases in LLMs when inferring user attributes, stressing the need to address these issues.

Abstract: Large language models (LLMs) are widely applied across diverse domains,
raising concerns about their limitations and potential risks. In this study, we
investigate two types of bias that LLMs may display: stereotype bias and
deviation bias. Stereotype bias refers to when LLMs consistently associate
specific traits with a particular demographic group. Deviation bias reflects
the disparity between the demographic distributions extracted from
LLM-generated content and real-world demographic distributions. By asking four
advanced LLMs to generate profiles of individuals, we examine the associations
between each demographic group and attributes such as political affiliation,
religion, and sexual orientation. Our experimental results show that all
examined LLMs exhibit both significant stereotype bias and deviation bias
towards multiple groups. Our findings uncover the biases that occur when LLMs
infer user attributes and shed light on the potential harms of LLM-generated
outputs.

</details>


### [84] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
*Jonathan Shaw,Dillon Mee,Timothy Khouw,Zackary Leech,Daniel Wilson*

Main category: cs.CL

TL;DR: The study evaluates Kanuri language translations by LLMs using grammar, dictionary, and parallel sentence resources, finding that parallel sentences are most effective.


<details>
  <summary>Details</summary>
Motivation: To improve translation quality for resource-limited languages, like Kanuri, by leveraging large language models with limited digital resources.

Method: The researchers used two custom datasets—focusing on health and humanitarian terms and generalized terminology—and combined varying linguistic resources (grammar, dictionary, parallel sentences) to evaluate translation using automatic metrics and native speaker assessments.

Result: Parallel sentences showed the highest effectiveness, grammar contributed modestly but failed as a standalone resource, and LLMs were better at conveying meaning than grammatical fluency.

Conclusion: Translation evaluation should use multidimensional metrics, and grammar alone can't provide adequate context; parallel sentences are crucial for effective translation in domain-specific tasks.

Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context
learning to translate into previously unseen language contexts. Tanzer et al.
[2024] utilize language materials (e.g. a grammar) to improve translation
quality for Kalamang using large language models (LLMs). We focus on Kanuri, a
language that, despite having substantial speaker population, has minimal
digital resources. We design two datasets for evaluation: one focused on health
and humanitarian terms, and another containing generalized terminology,
investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar,
dictionary, and parallel sentences), we measure LLM translation effectiveness,
comparing results to native speaker translations and human linguist
performance. We evaluate using both automatic metrics and native speaker
assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data
source, outperforming other methods in human evaluations and automatic metrics.
While incorporating grammar improves over zero-shot translation, it fails as an
effective standalone data source. Human evaluations reveal that LLMs achieve
accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from
multidimensional assessment beyond simple accuracy metrics, and that grammar
alone, without parallel sentences, does not provide sufficient context for
effective domain-specific translation.

</details>


### [85] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: The paper examines whether biases in the decision-making of large language models (LLMs) stem from their intermediate reasoning using chain-of-thought prompting. Researchers find weak correlation between biases in the reasoning steps and the final outputs.


<details>
  <summary>Details</summary>
Motivation: To explore and address the challenge of biases in language models, especially considering the increasing deployment risks associated with gender, race, and other socio-cultural biases.

Method: The researchers analyzed five large language models using chain-of-thought prompting and applied fairness metrics to evaluate 11 different types of biases in both intermediate reasoning steps and final outputs.

Result: Findings reveal a correlation of less than 0.6 between biases in the thinking steps and output biases, with statistical significance (p < 0.001), suggesting weak alignment between intermediate 'thoughts' and output decisions.

Conclusion: Biased decisions in LLMs do not always correspond to biased intermediate reasoning, challenging the assumption that patterns of bias in outputs directly reflect reasoning processes.

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [86] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: The paper studies self-bias in large language model (LLM) judges, demonstrating that they systematically favor their own outputs and proposes a statistical framework to identify and quantify this bias.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the self-bias and family-bias in evaluations performed by large language model judges, which may distort true model performance assessments.

Method: A statistical framework was developed to distinguish genuine differences in model quality from self-bias by analyzing scoring distributions, and empirical analysis using >5000 prompt-completion pairs was conducted.

Result: Some LLMs, like GPT-4o and Claude 3.5 Sonnet, were shown to favor their own outputs and those of related models, demonstrating self-bias and family-bias in automated evaluations.

Conclusion: This study reveals critical limitations in relying on LLM judges for evaluation and provides strategies to mitigate biases, ensuring more accurate interpretations of model performance.

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [87] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: The paper introduces a framework for automating semantic and sentiment analysis of Japanese American Incarceration Oral History archives using language models like ChatGPT, Llama, and Qwen.


<details>
  <summary>Details</summary>
Motivation: The research aims to overcome the challenges of analyzing oral history archives, such as their unstructured format and high annotation costs, to make them accessible and understandable.

Method: The authors used expert annotations, prompt engineering, and evaluated multiple Large Language Models (LLMs) including ChatGPT, Llama, and Qwen. They tested strategies like zero-shot, few-shot, and retrieval-augmented generation (RAG) for accuracy.

Result: ChatGPT performed best in semantic classification (88.71% F1 score), while Llama slightly outperformed others in sentiment analysis. Models were then used to annotate 92,191 sentences across 1,002 interviews.

Conclusion: LLMs can effectively annotate large oral history collections when guided by crafted prompts, offering a scalable pipeline and ethical framework for digital humanities and archival analysis.

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [88] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: This paper introduces the concept of multi-turn jailbreaking in large language models (LLMs) and provides a benchmark to study this vulnerability, highlighting its threats and advocating for improved safety measures.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreaking methods for LLMs only target single-turn outputs, but multi-turn interactions, which are intrinsic to advanced LLM capabilities, pose greater risks if vulnerabilities persist.

Method: The authors construct a Multi-Turn Jailbreak Benchmark (MTJ-Bench) to evaluate the susceptibility of multiple open- and closed-source models to multi-turn jailbreaking scenarios.

Result: They identify that multi-turn jailbreaking introduces additional risks, such as cascading unsafe outputs over continued interactions or answering unrelated harmful queries spurred by earlier interactions.

Conclusion: This work emphasizes the severity of multi-turn jailbreaking, urging the AI community to prioritize research on safety measures for handling multi-turn conversational vulnerabilities in LLMs.

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [89] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
*Ziqi Liu,Yangbin Chen,Ziyang Zhou,Yilin Li,Mingxuan Hu,Yushan Pan,Zhijie Xu*

Main category: cs.CL

TL;DR: The paper proposes SEVADE, an advanced framework for sarcasm detection, leveraging specialized agents and decoupled evaluations that enhance accuracy and mitigate hallucination risks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for sarcasm detection struggle with single-perspective analysis, static reasoning pathways, and hallucinations, resulting in decreased reliability and accuracy.

Method: SEVADE employs a Dynamic Agentive Reasoning Engine (DARE) involving linguistic theory-grounded agents for text deconstruction and reasoning. Final classification is performed by a rationale adjudicator using a decoupled evaluation approach to resist hallucination.

Result: SEVADE achieved state-of-the-art performance with an average accuracy improvement of 6.75% and a Macro-F1 score improvement of 6.29% on four benchmark datasets.

Conclusion: The SEVADE framework successfully addresses key challenges in sarcasm detection, offering a robust solution that significantly improves predictive performance and reliability.

Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing
task. Existing Large Language Model methods are often limited by
single-perspective analysis, static reasoning pathways, and a susceptibility to
hallucination when processing complex ironic rhetoric, which impacts their
accuracy and reliability. To address these challenges, we propose **SEVADE**, a
novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with
**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The
core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which
utilizes a team of specialized agents grounded in linguistic theory to perform
a multifaceted deconstruction of the text and generate a structured reasoning
chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs
the final classification based solely on this reasoning chain. This decoupled
architecture is designed to mitigate the risk of hallucination by separating
complex reasoning from the final judgment. Extensive experiments on four
benchmark datasets demonstrate that our framework achieves state-of-the-art
performance, with average improvements of **6.75%** in Accuracy and **6.29%**
in Macro-F1 score.

</details>


### [90] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
*Steven Coyne,Diana Galvan-Sosa,Ryan Spring,Camélia Guerraoui,Michael Zock,Keisuke Sakaguchi,Kentaro Inui*

Main category: cs.CL

TL;DR: The paper introduces an annotation framework to generate effective feedback for language learners using LLMs, emphasizing indirect hints and explanations over direct corrections.


<details>
  <summary>Details</summary>
Motivation: Current automated writing evaluation systems enhance text grammatically but are not ideal for language learners as they rely heavily on direct corrections without fostering understanding or learning.

Method: The authors introduced a grammatical error annotation framework and collected a dataset of learner errors with labeled feedback types. They tested LLM-based feedback generation methods, including keyword-guided, keyword-free, and template-guided approaches, and had human teachers evaluate the outputs.

Result: The study analyzed a dataset of annotated learner errors, ran comparative evaluations of different feedback generation methods using LLMs, and documented teacher assessments on aspects like relevance, factuality, and comprehensibility.

Conclusion: The results underscore the importance of feedback strategies that cater to knowledge gaps and generalizable grammatical patterns, suggesting the potential for optimized LLM-based systems tailored for language learning.

Abstract: Recent advances in natural language processing (NLP) have contributed to the
development of automated writing evaluation (AWE) systems that can correct
grammatical errors. However, while these systems are effective at improving
text, they are not optimally designed for language learning. They favor direct
revisions, often with a click-to-fix functionality that can be applied without
considering the reason for the correction. Meanwhile, depending on the error
type, learners may benefit most from simple explanations and strategically
indirect hints, especially on generalizable grammatical rules. To support the
generation of such feedback, we introduce an annotation framework that models
each error's error type and generalizability. For error type classification, we
introduce a typology focused on inferring learners' knowledge gaps by
connecting their errors to specific grammatical patterns. Following this
framework, we collect a dataset of annotated learner errors and corresponding
human-written feedback comments, each labeled as a direct correction or hint.
With this data, we evaluate keyword-guided, keyword-free, and template-guided
methods of generating feedback using large language models (LLMs). Human
teachers examined each system's outputs, assessing them on grounds including
relevance, factuality, and comprehensibility. We report on the development of
the dataset and the comparative performance of the systems investigated.

</details>


### [91] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: This paper develops a Text-to-Speech (TTS) system for the Manipuri language using the Meitei Mayek script.


<details>
  <summary>Details</summary>
Motivation: To address the lack of TTS systems for the Manipuri language, enabling linguistic preservation and technological inclusion.

Method: The authors employed a neural TTS architecture using Tacotron 2 and HiFi-GAN, designed a phoneme mapping to ARPAbet, and curated a single-speaker dataset.

Result: The system produced intelligible and natural speech, as confirmed by subjective and objective evaluations.

Conclusion: The work facilitates the inclusion of Manipuri in speech technology and contributes to its linguistic preservation.

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and
HiFi-GAN, we introduce a neural TTS architecture adapted to support tonal
phonology and under-resourced linguistic environments. We develop a phoneme
mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
demonstrate intelligible and natural speech synthesis, validated through
subjective and objective metrics. This system lays the groundwork for
linguistic preservation and technological inclusion of Manipuri.

</details>


### [92] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: The paper proposes an automatic label alignment method for merging NER datasets based on label similarity to improve interpretability and scalability.


<details>
  <summary>Details</summary>
Motivation: Building high-quality annotated datasets for Named Entity Recognition (NER) is expensive and time-consuming, creating a bottleneck in research advancements.

Method: The method applies a greedy pairwise merging strategy combining empirical and semantic label similarities to unify label spaces across datasets.

Result: Experiments merge three existing datasets into a unified corpus without degrading NER performance and integrate it with a financial domain dataset, showing performance enhancements.

Conclusion: The approach provides an efficient, interpretable, and scalable solution for NER dataset integration, aiding low-resource domains like finance.

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [93] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: The paper introduces ReQAP, a system that answers complex user questions by combining data from multiple heterogeneous sources.


<details>
  <summary>Details</summary>
Motivation: There is a need to help users make sense of abundant personal data from diverse sources to support complex queries.

Method: ReQAP recursively breaks down questions, builds an operator tree for execution, uses fine-tuned language models for interpretation, and ensures traceability of answers.

Result: The demo highlights the system's capability to handle advanced questions and provides a detailed trace of the computation process for transparency.

Conclusion: ReQAP enhances user trust and understanding by accurately answering multifaceted questions and offering full traceability of its operations.

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [94] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
*Arpita Saggar,Jonathan C. Darling,Vania Dimitrova,Duygu Sarikaya,David C. Hogg*

Main category: cs.CL

TL;DR: This paper introduces the SBS framework for persona-based dialogue generation that merges response generation with quality scoring, improving dialogue fidelity and performance of large language models.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of effectively integrating persona fidelity into dialogues due to limited diversity in current dialogue datasets.

Method: The SBS framework trains dialogue models to associate augmented responses with quality scores, utilizing noun-based substitutions for data augmentation and semantic similarity-based scoring.

Result: Extensive experiments show SBS achieves superior persona-consistent dialogues compared to previous methods, with enhanced performance across benchmark datasets and model sizes.

Conclusion: The findings suggest SBS improves persona-consistency in dialogues and demonstrates the advantage of score-conditioned training over traditional approaches.

Abstract: Persona-based dialogue generation is an important milestone towards building
conversational artificial intelligence. Despite the ever-improving capabilities
of large language models (LLMs), effectively integrating persona fidelity in
conversations remains challenging due to the limited diversity in existing
dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which
outperforms previous methods and yields improvements for both million and
billion-parameter models. Unlike previous methods, SBS unifies the learning of
responses and their relative quality into a single step. The key innovation is
to train a dialogue model to correlate augmented responses with a quality score
during training and then leverage this knowledge at inference. We use
noun-based substitution for augmentation and semantic similarity-based scores
as a proxy for response quality. Through extensive experiments with benchmark
datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training
allows existing models to better capture a spectrum of persona-consistent
dialogues. Our ablation studies also demonstrate that including scores in the
input prompt during training is superior to conventional training setups. Code
and further details are available at
https://arpita2512.github.io/score_before_you_speak

</details>


### [95] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: SentiDetect, a model-agnostic method, detects AI-generated text by analyzing sentiment distribution stability, showing superior performance over state-of-the-art detectors.


<details>
  <summary>Details</summary>
Motivation: To tackle the difficulty of distinguishing AI-generated text from human-written text due to inconsistent performance and vulnerabilities of existing detection methods.

Method: The paper introduces SentiDetect, which uses metrics like sentiment distribution consistency and preservation to identify discrepancies in emotional patterns between LLM-generated and human-written text.

Result: SentiDetect achieves significant F1 score improvements (e.g., over 16% on Gemini-1.5-Pro) and demonstrates robustness against paraphrasing, adversarial attacks, and domain shifts.

Conclusion: The proposed SentiDetect method is effective and robust for detecting LLM-generated text, addressing the limitations of existing approaches.

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [96] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: The paper introduces a two-stage framework for Quranic question answering, leveraging fine-tuned Arabic models for passage retrieval and instruction-tuned large language models for answer extraction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of linguistic complexity in Classical Arabic and the semantic nuances of religious texts in Quranic question answering.

Method: The method involves a two-stage framework: ensembling fine-tuned Arabic language models for passage retrieval and using instruction-tuned large language models with few-shot prompting for answer extraction.

Result: Achieved state-of-the-art performance in the Quran QA 2023 Shared Task with MAP@10 of 0.3128, MRR@10 of 0.5763 for retrieval, and pAP@10 of 0.669 for extraction.

Conclusion: The approach demonstrates that combining model ensembling with instruction-tuned language models is effective for low-resource question answering in specialized domains.

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [97] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
*Zhijun Tu,Hanting Chen,Siqi Liu,Chuanjian Liu,Jian Li,Jie Hu,Yunhe Wang*

Main category: cs.CL

TL;DR: This paper proposes a method to adapt pre-trained large language models (LLMs) into 1-bit versions by using a new progressive training approach to minimize training costs while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for 1-bit quantization of LLMs generally require training from scratch, which is computationally expensive and leads to accuracy loss. A gap between full precision and 1-bit representations makes direct adaptation difficult.

Method: The authors propose a consistent progressive training method for both forward and backward operations, gradually converting floating-point weights into binary ones. They also introduce binary-aware initialization and dual-scaling compensation to facilitate smoother training.

Result: The proposed method outperforms existing approaches in terms of accuracy and reduces the dependence on costly training from scratch. Experimental results across various LLM sizes support these findings.

Conclusion: Pre-trained LLMs can be effectively adapted to high-performance 1-bit models using this new progressive approach, bypassing the need for resource-intensive training processes.

Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and
computational costs. However, existing methods typically train 1-bit LLMs from
scratch, failing to fully leverage pre-trained models. This results in high
training costs and notable accuracy degradation. We identify that the large gap
between full precision and 1-bit representations makes direct adaptation
difficult. In this paper, we introduce a consistent progressive training for
both forward and backward, smoothly converting the floating-point weights into
the binarized ones. Additionally, we incorporate binary-aware initialization
and dual-scaling compensation to reduce the difficulty of progressive training
and improve the performance. Experimental results on LLMs of various sizes
demonstrate that our method outperforms existing approaches. Our results show
that high-performance 1-bit LLMs can be achieved using pre-trained models,
eliminating the need for expensive training from scratch.

</details>


### [98] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
*Mao Li,Fred Conrad,Johann Gagnon-Bartsch*

Main category: cs.CL

TL;DR: Vec2Summ is a new method for summarization that represents a document collection using a mean vector in semantic space and reconstructs summaries via a generative language model.


<details>
  <summary>Details</summary>
Motivation: Improve abstractive summarization to address scalability, context-length constraints, and allow controllable generation.

Method: Proposes semantic compression using a mean vector of a document collection. Introduces stochasticity via Gaussian sampling for diverse outputs, and inverts embedding to generate summaries.

Result: Shows coherent summaries with performance comparable to traditional LLM summarization, emphasizing thematic coverage but less detailed output.

Conclusion: Vec2Summ is effective for scalable, corpus-level summarization with semantic control, especially for topically focused corpora.

Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames
the task as semantic compression. Vec2Summ represents a document collection
using a single mean vector in the semantic embedding space, capturing the
central meaning of the corpus. To reconstruct fluent summaries, we perform
embedding inversion -- decoding this mean vector into natural language using a
generative language model. To improve reconstruction quality and capture some
degree of topical variability, we introduce stochasticity by sampling from a
Gaussian distribution centered on the mean. This approach is loosely analogous
to bagging in ensemble learning, where controlled randomness encourages more
robust and varied outputs. Vec2Summ addresses key limitations of LLM-based
summarization methods. It avoids context-length constraints, enables
interpretable and controllable generation via semantic parameters, and scales
efficiently with corpus size -- requiring only $O(d + d^2)$ parameters.
Empirical results show that Vec2Summ produces coherent summaries for topically
focused, order-invariant corpora, with performance comparable to direct LLM
summarization in terms of thematic coverage and efficiency, albeit with less
fine-grained detail. These results underscore Vec2Summ's potential in settings
where scalability, semantic control, and corpus-level abstraction are
prioritized.

</details>


### [99] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: The paper presents SEADialogues, a culturally grounded dialogue dataset designed for Southeast Asia, featuring multi-turn dialogues across eight languages and emphasizing cultural relevance.


<details>
  <summary>Details</summary>
Motivation: Existing chit-chat datasets lack the capability to capture cultural nuances in human conversations, especially for regions like Southeast Asia with rich diversity.

Method: SEADialogues includes dialogues in eight languages from six Southeast Asian countries, enriched with persona attributes and culturally significant topics.

Result: A diverse dataset emphasizing low-resource languages is created to promote research on culturally aware conversational AI.

Conclusion: SEADialogues provides a critical resource for advancing human-centric and culturally adaptive dialogue systems.

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [100] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
*Aditya Tomar,Nihar Ranjan Sahoo,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper presents BharatBBQ, a benchmark tailored to evaluate biases in Indian languages across 13 social categories, highlighting amplified biases in Indian contexts and the need for culturally specific evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing bias evaluation benchmarks are primarily Western-focused, lacking applicability in Indian sociocultural contexts. The authors aim to fill this gap with a localized benchmark.

Method: The authors created BharatBBQ, a benchmark covering eight Indian languages and 13 social categories. They expanded datasets through translation and verification, and tested five multilingual language models.

Result: Findings reveal persistent biases across languages and social categories, with amplified biases in Indian languages compared to English.

Conclusion: Linguistically and culturally grounded benchmarks like BharatBBQ are essential for evaluating biases in diverse contexts and ensuring fairness in AI systems.

Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring
fairness and minimizing the reinforcement of harmful stereotypes in AI systems.
Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ),
primarily focus on Western contexts, limiting their applicability to the Indian
context. To address this gap, we introduce BharatBBQ, a culturally adapted
benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,
Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3
intersectional groups, reflecting prevalent biases in the Indian sociocultural
landscape. Our dataset contains 49,108 examples in one language that are
expanded using translation and verification to 392,864 examples in eight
different languages. We evaluate five multilingual LM families across zero and
few-shot settings, analyzing their bias and stereotypical bias scores. Our
findings highlight persistent biases across languages and social categories and
often amplified biases in Indian languages compared to English, demonstrating
the necessity of linguistically and culturally grounded benchmarks for bias
evaluation.

</details>


### [101] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: Large reasoning models can be computationally intensive, especially in token generation, but LessIsMore proposes a training-free sparse attention mechanism that improves efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models generate significant computational overhead in short prompts due to their reliance on exhaustive token generation mechanisms.

Method: LessIsMore uses a training-free sparse attention mechanism leveraging global attention patterns, combining local attention head selections and recent contextual information for unified token ranking efficiency.

Result: The method achieves a 1.1x decoding speed-up compared to full attention, attends to 2x fewer tokens without accuracy loss, and offers a 1.13x speed-up against existing sparse attention approaches.

Conclusion: LessIsMore enhances reasoning model efficiency by reducing computational demand while preserving or improving accuracy, making it viable for adopting sparse attention mechanisms.

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [102] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: This paper explores intersectional bias in large language models (LLMs), introducing a benchmark called WinoIdentity to assess fairness, and uncovers significant confidence disparities in model decisions for doubly-disadvantaged identities.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely used in critical decision-making contexts, yet they can reflect and exacerbate societal biases, necessitating a deeper examination of identity-based harms and intersectional biases.

Method: The authors augmented the WinoBias dataset to create WinoIdentity, incorporating 25 demographic markers across 10 attributes and binary gender. They proposed the "Coreference Confidence Disparity" metric to assess intersectional bias and tested five LLMs with 245,700 prompts.

Result: The evaluation of five LLMs revealed coreference confidence disparities as high as 40% for demographic attributes like body type and socio-economic status. Models showed higher uncertainty for doubly-disadvantaged identities in anti-stereotypical scenarios, with confidence issues even for privileged identities.

Conclusion: The study highlights two independent failures of LLMs: lack of value alignment and reliance on memorization rather than reasoning. These issues can compound, leading to significant social harm.

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [103] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: The paper critiques Automatic Speech Recognition (ASR) systems for biases against non-standard speech varieties, highlighting ethical dilemmas and proposing a philosophical reframing to respect linguistic diversity.


<details>
  <summary>Details</summary>
Motivation: To analyze and highlight the ethical implications of ASR systems misrecognizing marginalized speech varieties, addressing the respect and fairness issues embedded in speech technology.

Method: The paper employs a philosophical analysis to contrast morally neutral classification from discriminatory misrecognition, and identifies unique ethical dimensions specific to ASR systems.

Result: Key ethical issues such as temporal burdens, conversational disruptions, and connections between speech and identity are discovered, emphasizing how current ASR systems perpetuate power asymmetries and problematic language ideologies.

Conclusion: The study calls for ASR systems to actively accommodate diverse speech patterns and views these as legitimate forms of expression, urging for philosophical and ethical reform beyond merely technical adjustments.

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [104] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
*Biao Yi,Jiahao Li,Baolei Zhang,Lihai Nie,Tong Li,Tiansheng Huang,Zheli Liu*

Main category: cs.CL

TL;DR: This paper highlights a vulnerability in LLM fine-tuning where malicious data can compromise safety and proposes a method called SafeGrad to mitigate this issue by correcting gradient conflicts.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address vulnerabilities in large language model (LLM) fine-tuning caused by malicious examples, which can compromise the balance between user task performance and safety alignment.

Method: The authors introduce SafeGrad, which detects and resolves gradient conflicts by nullifying harmful components of user-task gradients using gradient projection. It also employs KL-divergence alignment loss to enhance robustness and efficiency.

Result: SafeGrad demonstrates state-of-the-art defense in experiments with various LLMs, maintaining high safety alignment even under high harmful ratios and without sacrificing task performance.

Conclusion: The suggested method, SafeGrad, is effective in protecting the safety alignment of LLMs while ensuring high-performing fine-tuning, addressing a critical vulnerability in the process.

Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few
malicious examples mixed into the user's fine-tuning dataset can compromise the
safety alignment of Large Language Models (LLMs). While a recognized paradigm
frames safe fine-tuning as a multi-objective optimization problem balancing
user task performance with safety alignment, we find existing solutions are
critically sensitive to the harmful ratio, with defenses degrading sharply as
harmful ratio increases. We diagnose that this failure stems from conflicting
gradients, where the user-task update directly undermines the safety objective.
To resolve this, we propose SafeGrad, a novel method that employs gradient
surgery. When a conflict is detected, SafeGrad nullifies the harmful component
of the user-task gradient by projecting it onto the orthogonal plane of the
alignment gradient, allowing the model to learn the user's task without
sacrificing safety. To further enhance robustness and data efficiency, we
employ a KL-divergence alignment loss that learns the rich, distributional
safety profile of the well-aligned foundation model. Extensive experiments show
that SafeGrad provides state-of-the-art defense across various LLMs and
datasets, maintaining robust safety even at high harmful ratios without
compromising task fidelity.

</details>


### [105] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
*Leyi Pan,Zheyu Fu,Yunpeng Zhai,Shuchang Tao,Sheng Guan,Shiyu Huang,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Felix Henry,Lijie Wen,Aiwei Liu*

Main category: cs.CL

TL;DR: This paper introduces Omni-SafetyBench, a benchmark for assessing the safety of Omni-modal Large Language Models (OLLMs) when handling audio-visual and text inputs, highlighting vulnerabilities and proposing metrics for evaluation.


<details>
  <summary>Details</summary>
Motivation: The emergence of OLLMs with multi-modal capabilities necessitates a dedicated safety evaluation benchmark, as prior ones are inadequate for their unique challenges, such as audio-visual safety and cross-modal consistency.

Method: The paper presents Omni-SafetyBench, a benchmark with 972 samples across 24 modality combinations, including audio-visual harm cases, along with tailored metrics like Conditional Attack Success Rate, Refusal Rate, and Cross-Modal Safety Consistency Score to evaluate model safety and consistency.

Result: Evaluation of 10 OLLMs revealed that no model performed exceptionally across all metrics, with models struggling in handling complex inputs and cross-modal consistency, and some scoring as low as 0.14 in specific scenarios.

Conclusion: Omni-SafetyBench exposes critical vulnerabilities in OLLM safety, and its metrics provide a basis for prioritizing improvements in the field, emphasizing the urgency of addressing these challenges.

Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual
and auditory processing with text, necessitates robust safety evaluations to
mitigate harmful outputs. However, no dedicated benchmarks currently exist for
OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess
safety performance under audio-visual joint inputs or cross-modal safety
consistency. To fill this gap, we introduce Omni-SafetyBench, the first
comprehensive parallel benchmark for OLLM safety evaluation, featuring 24
modality combinations and variations with 972 samples each, including dedicated
audio-visual harm cases. Considering OLLMs' comprehension challenges with
complex omni-modal inputs and the need for cross-modal consistency evaluation,
we propose tailored metrics: a Safety-score based on conditional Attack Success
Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and
a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency
across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals
critical vulnerabilities: (1) no model excels in both overall safety and
consistency, with only 3 models achieving over 0.6 in both metrics and top
performer scoring around 0.8; (2) safety defenses weaken with complex inputs,
especially audio-visual joints; (3) severe weaknesses persist, with some models
scoring as low as 0.14 on specific modalities. Our benchmark and metrics
highlight urgent needs for enhanced OLLM safety, providing a foundation for
future improvements.

</details>


### [106] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: This paper presents PHG-DIF, a personalized headline generation framework that addresses click noise in user behavior through dual-stage filtering and temporal fusion, demonstrating improved results on a newly released dataset DT-PENS.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the adverse effects of click noise in user behavior, which distorts personalized headline generation and undermines quality.

Method: The method includes dual-stage filtering to remove click noise caused by factors like short dwell times and click bursts. Additionally, it employs multi-level temporal fusion to dynamically model users' preferences.

Result: PHG-DIF improves personalized headline quality by mitigating click noise, achieving state-of-the-art performance on the DT-PENS dataset, which contains detailed click behavior and annotated headlines.

Conclusion: The framework successfully refines interest modeling and headline generation, emphasizing the importance of denoising click inputs for personalization and offering benchmarks for further research in DT-PENS.

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [107] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: The paper introduces a framework to address semantic drift in enterprise data pipelines by automatically extracting schema lineage, and proposes a novel evaluation metric (SLiCE) and benchmark. Experiments show that performance improves with model size and prompting advancements.


<details>
  <summary>Details</summary>
Motivation: Semantic drift in enterprise data pipelines causes issues like reduced data reproducibility, governance, and utility for systems like RAG and text-to-SQL. The paper seeks to address this disconnect between original and downstream metadata.

Method: The framework extracts schema lineage components (source schemas, source tables, transformation logic, aggregation operations), providing a standardized representation. A new evaluation metric (SLiCE) is created, along with a benchmark of 1,700 annotated lineages. Experiments assess models ranging from 1.3B to 32B parameters, including GPT series.

Result: The performance of schema lineage extraction scales with model size and better prompting techniques. A 32B open-source model with single reasoning trace matches GPT-series capabilities under standard prompting.

Conclusion: The proposed framework and evaluation techniques offer a scalable and cost-effective solution for schema-aware agents, showing promise for practical enterprise applications.

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [108] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: DySK-Attn is a novel framework that allows Large Language Models to dynamically incorporate real-time updates from external knowledge graphs, overcoming issues of outdated static knowledge and high retraining computational costs.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with static and outdated knowledge, and retraining them is costly. Rapid real-time updates are needed without introducing errors.

Method: DySK-Attn integrates dynamic external knowledge into LLMs using a sparse knowledge attention mechanism for precise and efficient fact selection from updated Knowledge Graphs.

Result: DySK-Attn showed superior performance in time-sensitive Q&A tasks with improved factual accuracy and computational efficiency over existing methods like RAG and model editing.

Conclusion: DySK-Attn provides a scalable solution for keeping LLMs updated with real-time knowledge, outperforming traditional mechanisms in accuracy and efficiency.

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [109] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: The paper introduces TALON, a method for applying large language models (LLMs) to time series forecasting by addressing challenges of temporal heterogeneity and numerical-to-language modality mismatches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in applying LLMs to time series forecasting due to heterogeneity in temporal patterns and modality differences between numerical signals and linguistic representations.

Method: The paper presents TALON, featuring a Heterogeneous Temporal Encoder for structured segmentation of time series and a Semantic Alignment Module to align numerical data with language model input formats.

Result: Experimental evaluations on seven datasets demonstrate that TALON outperforms state-of-the-art methods, showing an 11% improvement in MSE on average.

Conclusion: TALON effectively facilitates the application of LLMs to time series forecasting by integrating pattern-aware and semantic-aware designs, achieving significant performance enhancements.

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [110] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
*Chaoqun Cui,Siyuan Li,Kunkun Ma,Caiyan Jia*

Main category: cs.CL

TL;DR: The paper tackles the suboptimal performance of Pretrained Language Models (PLMs) in social media-focused tasks, like rumor detection, through a novel pretraining strategy called Post Engagement Prediction (PEP), leveraging large-scale Twitter-focused datasets.


<details>
  <summary>Details</summary>
Motivation: Although PLMs perform well in general NLP tasks, their effectiveness for rumor detection on social media is limited due to mismatched corpora, inadequate handling of social symbols, and a lack of focus on user interaction modeling.

Method: The authors propose a novel pretraining strategy, PEP, which predicts root, branch, and parent relations between posts to capture stance and sentiment interactions. They also introduce large-scale Twitter-based corpora to tailor PLMs for social media.

Result: PEP improves rumor detection performance significantly across different PLMs, achieving 1.0-3.7% accuracy gains and outperforming state-of-the-art methods on multiple benchmark datasets. SoLM, the tailored PLM, shows competitive results even without additional high-level modules.

Conclusion: The paper verifies that PEP is effective in infusing propagation structure information into PLMs, enhancing their capacity for social media tasks such as rumor detection. The introduced resources and methods are pivotal for advancing this domain.

Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language
Processing tasks, benefiting from large-scale pretraining and self-attention
mechanism's ability to capture long-range dependencies. However, their
performance on social media application tasks like rumor detection remains
suboptimal. We attribute this to mismatches between pretraining corpora and
social texts, inadequate handling of unique social symbols, and pretraining
tasks ill-suited for modeling user engagements implicit in propagation
structures. To address these issues, we propose a continue pretraining strategy
called Post Engagement Prediction (PEP) to infuse information from propagation
structures into PLMs. PEP makes models to predict root, branch, and parent
relations between posts, capturing interactions of stance and sentiment crucial
for rumor detection. We also curate and release large-scale Twitter corpus:
TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with
propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP
strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments
demonstrate PEP significantly boosts rumor detection performance across
universal and social media PLMs, even in few-shot scenarios. On benchmark
datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it
to outperform current state-of-the-art methods on multiple datasets. SoLM
alone, without high-level modules, also achieves competitive results,
highlighting the strategy's effectiveness in learning discriminative post
interaction features.

</details>


### [111] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: The study explores interpreting CNN-based decisions in lexical stress prediction, achieving 92% accuracy, and highlights critical spectral features influencing predictions.


<details>
  <summary>Details</summary>
Motivation: To address the opacity of neural network decision-making processes, particularly in speech processing, by investigating how CNNs interpret lexical stress.

Method: The authors constructed a dataset of English disyllabic words, trained CNN architectures on spectrographic representations to predict stress positions, and used Layerwise Relevance Propagation for interpretability analysis.

Result: Achieved 92% accuracy in stress prediction on test data. LRP highlighted that spectral properties of stressed vowels, especially first and second formants, were crucial for predictions.

Conclusion: Deep learning effectively captures distributed cues to lexical stress from naturalistic speech, enhancing insights traditionally obtained from controlled phonetic studies.

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [112] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
*Zhe Ren*

Main category: cs.CL

TL;DR: This paper addresses the Few-Shot Continual Learning Named Entity Recognition (FS-CLNER) challenge by proposing techniques that bridge the gap between pre-training and few-shot fine-tuning, enhancing generalization and overcoming distillation issues.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of FS-CLNER tasks by tackling challenges like difficulty in generalization due to scarce new-class data and the Few-Shot Distillation Dilemma.

Method: The authors introduced an Anchor words-oriented Prompt Tuning (APT) paradigm and Memory Demonstration Templates (MDT) to enhance model training, provide replay samples, and promote in-context learning.

Result: The proposed approach achieves competitive results on FS-CLNER tasks in experiments.

Conclusion: The study successfully addressed key challenges in FS-CLNER, enhancing model performance and mitigating the distillation dilemma through innovative strategies.

Abstract: Knowledge distillation has been successfully applied to Continual Learning
Named Entity Recognition (CLNER) tasks, by using a teacher model trained on
old-class data to distill old-class entities present in new-class data as a
form of regularization, thereby avoiding catastrophic forgetting. However, in
Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it
difficult for the trained model to generalize during inference. More
critically, the lack of old-class entity information hinders the distillation
of old knowledge, causing the model to fall into what we refer to as the
Few-Shot Distillation Dilemma. In this work, we address the above challenges
through a prompt tuning paradigm and memory demonstration template strategy.
Specifically, we designed an expandable Anchor words-oriented Prompt Tuning
(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby
enhancing performance in few-shot scenarios. Additionally, we incorporated
Memory Demonstration Templates (MDT) into each training instance to provide
replay samples from previous tasks, which not only avoids the Few-Shot
Distillation Dilemma but also promotes in-context learning. Experiments show
that our approach achieves competitive performances on FS-CLNER.

</details>


### [113] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: This paper enhances the 2D articulatory model DYNARTmo with a 3D palatal dome representation to estimate tongue-palate contact areas.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the DYNARTmo model by enabling it to estimate tongue-palate contact areas and provide more comprehensive articulation visualizations.

Method: The authors integrated a 3D palatal dome (using half-ellipse and cosine geometries) for modeling lateral curvature, allowing analytical computation of lateral contact points.

Result: The enhanced model generates electropalatography-like visualizations and supports synchronized multi-view displays for articulation.

Conclusion: This advancement holds potential for use in speech science education and therapy, with planned future improvements in realism and feature set.

Abstract: This paper describes an extension of the two-dimensional dynamic articulatory
model DYNARTmo by integrating an internal three-dimensional representation of
the palatal dome to estimate tongue-palate contact areas from midsagittal
tongue contours. Two alternative dome geometries - a half-ellipse and a cosine
based profile - are implemented to model lateral curvature in the coronal
plane. Using these geometries, lateral contact points are analytically computed
for each anterior-posterior position, enabling the generation of
electropalatography-like visualizations within the 2D+ framework. The enhanced
model supports three synchronized views (sagittal, glottal, and palatal) for
static and dynamic (animated) articulation displays, suitable for speech
science education and speech therapy. Future work includes adding a facial
(lip) view and implementing articulatory-to-acoustic synthesis to
quantitatively evaluate model realism.

</details>


### [114] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: The paper presents methods to improve Speech-LLMs' empathetic reasoning by integrating contextual paralinguistic information.


<details>
  <summary>Details</summary>
Motivation: To address the limited empathetic reasoning in Speech-LLMs caused by a lack of datasets combing contextual content and paralinguistic cues.

Method: Two methods: explicit (feeding emotion metadata) and implicit (creating novel QA pairs using emotion annotations).

Result: Performance improved by 38.41% with implicit methods and 46.02% when combined with explicit methods.

Conclusion: The proposed methods enhance contextual paralinguistic understanding, with validation of the LLM judge's reliability.

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [115] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: The paper introduces MAQuA, an efficient LLM-based framework for multidimensional mental health screening that reduces assessment questions by 50-87%.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for scalable and efficient mental health assessment tools. Traditional methods with excessive question-answering are inefficient for real-world screening across multiple symptom profiles, thereby motivating the development of adaptive and efficient frameworks.

Method: The proposed method, MAQuA, combines multi-outcome modeling of language responses with item response theory (IRT) and factor analysis. It dynamically selects questions with the most informative responses across symptom dimensions to improve diagnostic accuracy and reduce response burden.

Result: Empirical evaluation on a novel dataset shows that MAQuA reduces the number of assessment questions by 50-87%. For example, depression scores stabilize with 71% fewer questions, and eating disorder scores stabilize with 85% fewer questions. It performs robustly across both internalizing and externalizing mental health domains.

Conclusion: MAQuA is a powerful and efficient tool for nuanced and scalable mental health screening. It reduces patient burden and integrates seamlessly into real-world clinical workflows, demonstrating the potential for LLM-based agents in mental health diagnostics.

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [116] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: This paper evaluates 14 leading large language models (LLMs) using trolley problem scenarios framed by various moral philosophies, highlighting inconsistencies and sweet zones in moral alignment.


<details>
  <summary>Details</summary>
Motivation: The increasing role of LLMs in ethically-sensitive decisions necessitates more transparency and understanding of their moral reasoning capabilities.

Method: Empirical evaluation of LLMs using 27 trolley problem scenarios framed by ten moral philosophies, with factorial prompting protocol yielding binary decisions and justifications.

Result: Models exhibited variability across ethical frames and types, with "sweet zones" found in altruistic, fairness, and virtue ethics frames, but divergence in kinship, legality, or self-interest frames.

Conclusion: Moral reasoning should be a key aspect in LLM alignment, with standardized benchmarks to evaluate decisions and reasoning processes for ethical sensitivity.

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [117] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: ARCE improves named entity recognition in the AEC domain using augmented RoBERTa pre-trained with simple, LLM-generated explanations (Cote), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Standard pre-trained models struggle with the domain-specific terminology and relational complexities in AEC texts, limiting their effectiveness for tasks like automated rule checking.

Method: ARCE employs a large language model (LLM) to generate simple explanations (Cote), uses these for incremental pre-training of a RoBERTa model, and fine-tunes it for the downstream task.

Result: ARCE achieves a Macro-F1 score of 77.20% on a benchmark AEC dataset, outperforming existing models.

Conclusion: Simple, explanation-based knowledge is more effective for domain-specific NER tasks than complex, role-based rationales, offering a cost-effective alternative for improving model performance in specialized domains.

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [118] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Zheng Chu,Bo Yang,Xiaocheng Feng,Yang Xiang,Ming Liu*

Main category: cs.CL

TL;DR: This paper introduces the CCFQA benchmark, a tool designed to assess the cross-lingual and cross-modal factuality of Multimodal Large Language Models (MLLMs) by incorporating speech-text factual questions in 8 languages. It identifies challenges faced by current MLLMs and proposes an effective few-shot transfer learning strategy.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the lack of benchmarks for evaluating MLLMs' factual reliability, particularly in multilingual and speech-based contexts, where current evaluations are limited to textual or visual domains and predominantly focus on English.

Method: The authors introduce the CCFQA, a benchmark containing parallel speech-text factual questions in 8 languages, designed for MLLMs' evaluation. They also propose a few-shot transfer learning strategy to adapt English QA capabilities of LLMs to multilingual spoken QA tasks.

Result: The CCFQA benchmark reveals significant challenges in current MLLMs' performance. However, the proposed few-shot transfer learning strategy achieves competitive results on the multilingual spoken QA tasks, even with limited training samples.

Conclusion: The study highlights weaknesses in existing MLLMs' handling of multilingual speech inputs and provides a new benchmark and methodology for improving their factual reliability. The effort aims to encourage research advancements in robust multilingual and multimodal speech understanding.

Abstract: As Large Language Models (LLMs) are increasingly popularized in the
multilingual world, ensuring hallucination-free factuality becomes markedly
crucial. However, existing benchmarks for evaluating the reliability of
Multimodal Large Language Models (MLLMs) predominantly focus on textual or
visual modalities with a primary emphasis on English, which creates a gap in
evaluation when processing multilingual input, especially in speech. To bridge
this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal
\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA
benchmark contains parallel speech-text factual questions across 8 languages,
designed to systematically evaluate MLLMs' cross-lingual and cross-modal
factuality capabilities. Our experimental results demonstrate that current
MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we
propose a few-shot transfer learning strategy that effectively transfers the
Question Answering (QA) capabilities of LLMs in English to multilingual Spoken
Question Answering (SQA) tasks, achieving competitive performance with
GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a
foundational research resource to promote the development of MLLMs with more
robust and reliable speech understanding capabilities. Our code and dataset are
available at https://github.com/yxduir/ccfqa.

</details>


### [119] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: HealthBranches is a specialized benchmark for assessing complex reasoning in Large Language Models (LLMs) within medical contexts, using 4,063 case studies and reasoning pathways.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark dataset that evaluates the ability of LLMs to perform multi-step reasoning and provide interpretable, reliable outputs in complex medical Question-Answering scenarios.

Method: Using a semi-automated process, explicit decision pathways from medical sources were transformed into realistic patient cases with associated questions and answers. These cases span 17 healthcare topics and include clinically validated reasoning chains.

Result: Generated a dataset with 4,063 medical case studies spanning open-ended and multiple-choice question formats, along with reasoning pathways for robust LLM evaluation.

Conclusion: HealthBranches serves as a robust benchmark for developing and testing trustworthy, interpretable, and clinically reliable LLMs, while also aiding in educational applications.

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [120] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: This paper proposes a framework, ObfusQA, to test the robustness of large language models (LLMs) against obfuscated questions.


<details>
  <summary>Details</summary>
Motivation: The study aims to test and evaluate the robustness of LLMs when presented with obfuscated question forms, addressing a gap in current research.

Method: Introduction of ObfusQA and the ObfusQAte technique, which uses multi-tiered obfuscation including Named-Entity Indirection, Distractor Indirection, and Contextual Overload for systematic evaluation.

Result: LLMs demonstrated tendencies to fail or produce hallucinated responses under nuanced obfuscation conditions.

Conclusion: ObfusQA provides a comprehensive benchmark for evaluating LLM robustness, and the study encourages further exploration by making ObfusQAte publicly available.

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [121] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: The study explores how humans interact with a code-switching chatbot and finds that predictable code-switching improves user experience and task performance.


<details>
  <summary>Details</summary>
Motivation: Understanding characteristics of code-switched language and developing bilingual language-use tools.

Method: Used a chatbot to perform a bilingual Map Task with participants, employing different code-switching strategies.

Result: Predictable and grammatical code-switching led to better user experience and task success, whereas random or ungrammatical switching negatively impacted both.

Conclusion: Well-designed multilingual technology benefits research and usability, while poorly developed tools can hinder participant engagement and success.

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [122] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
*Wenqian Cui,Lei Zhu,Xiaohui Li,Zhihan Guo,Haoli Bai,Lu Hou,Irwin King*

Main category: cs.CL

TL;DR: The paper introduces Full-Duplex Speech Language Models (FD-SLMs) to facilitate real-time spoken interactions by addressing conversational dynamics. It proposes TurnGuide, a planning-inspired approach to manage speech segmentation and guidance, achieving improved conversational abilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the degradation in conversational abilities of FD-SLMs due to prolonged speech sequences and limited high-quality spoken dialogue data.

Method: TurnGuide is proposed as a dynamic process to segment assistant speech into dialogue turns, generating turn-level text guidance before speech output. This approach mimics human conversational planning and resolves timing and length challenges.

Result: TurnGuide significantly enhances FD-SLMs' abilities, enabling semantically meaningful and coherent speech generation while maintaining natural conversational flow.

Conclusion: The proposed approach, TurnGuide, effectively tackles core challenges in FD-SLMs, facilitating improved real-time spoken interactions and demonstrating significant potential in enhancing human-like dialogues.

Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation
models designed to enable natural, real-time spoken interactions by modeling
complex conversational dynamics such as interruptions, backchannels, and
overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world
double-channel conversational data to capture nuanced two-speaker dialogue
patterns for human-like interactions. However, they face a critical challenge
-- their conversational abilities often degrade compared to pure-text
conversation due to prolonged speech sequences and limited high-quality spoken
dialogue data. While text-guided speech generation could mitigate these issues,
it suffers from timing and length issues when integrating textual guidance into
double-channel audio streams, disrupting the precise time alignment essential
for natural interactions. To address these challenges, we propose TurnGuide, a
novel planning-inspired approach that mimics human conversational planning by
dynamically segmenting assistant speech into dialogue turns and generating
turn-level text guidance before speech output, which effectively resolves both
insertion timing and length challenges. Extensive experiments demonstrate our
approach significantly improves e2e FD-SLMs' conversational abilities, enabling
them to generate semantically meaningful and coherent speech while maintaining
natural conversational flow. Demos are available at
https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at
https://github.com/dreamtheater123/TurnGuide.

</details>


### [123] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: This paper introduces CulturalPangea, a Multimodal Large Language Model (MLLM) trained on a culturally rich dataset called CulturalGround, to enhance performance in low-resource languages and culturally diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing Multimodal Large Language Models excel in high-resource settings but struggle with long-tail cultural entities and underperform in low-resource languages.

Method: The authors compiled culturally significant visual question answering (VQA) data from a knowledge graph and generated CulturalGround, a dataset with 22 million VQA pairs across 42 countries and 39 languages. They integrated this dataset with multilingual instruction-tuning data to train the CulturalPangea model.

Result: CulturalPangea achieved state-of-the-art results in culture-focused multilingual multimodal benchmarks, outperforming previous models by an average of 5.0 without compromising performance on mainstream tasks.

Conclusion: A data-centric, culturally grounded approach can significantly mitigate cultural biases in MLLMs, offering a pathway to globally inclusive multimodal systems.

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [124] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
*Zhiyi Lyu,Jianguo Huang,Yanchen Deng,Steven Hoi,Bo An*

Main category: cs.CL

TL;DR: ReLoc is a local search framework designed for efficient and scalable code generation, outperforming construction-based and improvement-based methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses efficiency and scalability issues in code generation methods for LLMs, aiming to overcome shortcomings of existing techniques such as tree size growth and uninformative reward signals.

Method: ReLoc operates through step-by-step code revision, utilizing four key components: initial code drafting, neighborhood generation, candidate evaluation, and code updating. It employs algorithms like Hill Climbing or Genetic Algorithm and introduces a specialized revision reward model for code quality assessment.

Result: Experimental findings show that ReLoc achieves superior results in various code generation tasks, surpassing both tree-search and state-of-the-art improvement-based methods.

Conclusion: ReLoc demonstrates its potential as an effective, scalable, and high-performance code generation framework suitable for diverse applications.

Abstract: Large Language Models (LLMs) with inference-time scaling techniques show
promise for code generation, yet face notable efficiency and scalability
challenges. Construction-based tree-search methods suffer from rapid growth in
tree size, high token consumption, and lack of anytime property. In contrast,
improvement-based methods offer better performance but often struggle with
uninformative reward signals and inefficient search strategies. In this work,
we propose \textbf{ReLoc}, a unified local search framework which effectively
performs step-by-step code revision. Specifically, ReLoc explores a series of
local revisions through four key algorithmic components: initial code drafting,
neighborhood code generation, candidate evaluation, and incumbent code
updating, each of which can be instantiated with specific decision rules to
realize different local search algorithms such as Hill Climbing (HC) or Genetic
Algorithm (GA). Furthermore, we develop a specialized revision reward model
that evaluates code quality based on revision distance to produce fine-grained
preferences that guide the local search toward more promising candidates.
Finally, our extensive experimental results demonstrate that our approach
achieves superior performance across diverse code generation tasks,
significantly outperforming both construction-based tree search as well as the
state-of-the-art improvement-based code generation methods.

</details>


### [125] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
*Blerta Veseli,Julian Chibane,Mariya Toneva,Alexander Koller*

Main category: cs.CL

TL;DR: The paper studies positional biases in Large Language Models (LLMs) and identifies how biases like "Lost in the Middle" effect vary with respect to input length.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and clarify the positional biases of LLMs and their impact on model performance across long-context tasks, particularly the conditions under which these biases occur.

Method: The authors analyze a comprehensive dataset using relative input lengths (up to 50% of the model's context window) rather than absolute lengths to study positional bias effects.

Result: The study finds that the "Lost in the Middle" effect is strongest for inputs occupying up to 50% of the context window. Beyond this, primacy bias weakens, recency bias stabilizes, and a distance-based bias emerges, favoring proximity to the end.

Conclusion: Positional biases in long-context reasoning for LLMs stem largely from retrieval processes. These insights guide improvements in tasks, benchmarks, and evaluations for handling extended context effectively.

Abstract: Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [126] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: The paper introduces ALOPE, a framework that enhances LLM-based Quality Estimation (QE) in Machine Translation by restructuring Transformer layers for better regression tasks and cross-lingual alignment.


<details>
  <summary>Details</summary>
Motivation: Challenges exist in adapting Large Language Models for QE tasks due to their causal language modeling design and low-resource language data limitations.

Method: ALOPE employs a layer-optimization framework incorporating low-rank adapters, regression task heads, dynamic weighting, and multi-head regression to improve LLM-based QE.

Result: ALOPE outperformed existing QE approaches and demonstrated that intermediate Transformer layers offer better cross-lingual alignment.

Conclusion: ALOPE provides an innovative method to improve LLM-based QE tasks and enhances cross-lingual applications, with models and framework made publicly available for continued research.

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [127] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
*Keshav Varadarajan,Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: This paper introduces a method using topological data analysis to identify attention heads in GPT-2 responsible for biases towards identity groups.


<details>
  <summary>Details</summary>
Motivation: There is a lack of methods to identify which components of large language models contribute to biases towards specific groups.

Method: The paper uses topological data analysis to evaluate attention heads in GPT-2 and determine their role in bias representation against identity groups, leveraging the StereoSet dataset.

Result: Biases for categories like gender or profession are concentrated in specific attention heads identified as 'hot spots.'

Conclusion: The proposed metric helps pinpoint bias-responsible attention heads and could inform future de-biasing techniques for large language models.

Abstract: Recently, many bias detection methods have been proposed to determine the
level of bias a large language model captures. However, tests to identify which
parts of a large language model are responsible for bias towards specific
groups remain underdeveloped. In this study, we present a method using
topological data analysis to identify which heads in GPT-2 contribute to the
misrepresentation of identity groups present in the StereoSet dataset. We find
that biases for particular categories, such as gender or profession, are
concentrated in attention heads that act as hot spots. The metric we propose
can also be used to determine which heads capture bias for a specific group
within a bias category, and future work could extend this method to help
de-bias large language models.

</details>


### [128] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: ThemeClouds uses large language models to generate smarter word clouds by identifying thematic ideas rather than simply counting frequencies, offering better insights from qualitative interviews.


<details>
  <summary>Details</summary>
Motivation: Traditional word cloud methods fail to account for conversational nuances such as filler words, paraphrasing, and semantically related ideas, limiting their effectiveness in qualitative interview analysis.

Method: The study introduces ThemeClouds, a tool leveraging large language models to identify thematic concepts from dialogue transcripts and visualize participant-weighted word clouds based on breadth of mention.

Result: ThemeClouds identifies more actionable concerns compared to frequency-based and topic-modeling methods, demonstrating its effectiveness using user study interviews with 155 transcripts.

Conclusion: ThemeClouds improves qualitative data analysis by grounding visualization in thematic breadth rather than raw frequency, with implications for researcher control and interactive analyses.

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [129] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
*Jia Deng,Jie Chen,Zhipeng Chen,Daixuan Cheng,Fei Bai,Beichen Zhang,Yinqian Min,Yanzipeng Gao,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: The paper investigates reinforcement learning with verifiable rewards (RLVR) applied to large language models (LLMs), focusing on exploration strategies and their optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional reinforcement learning approaches struggle in guiding complex reasoning chains within LLMs, making RLVR an essential paradigm for providing rule-based feedback.

Method: The study explores properties of RLVR through metrics defining LLM exploration space, analysis of entropy-performance relationships, and optimization strategies for RL performance.

Result: The research unifies insights from prior work with new empirical findings, offering improved methods to analyze and optimize exploration in RLVR.

Conclusion: A robust foundational framework is provided to guide further development and refinement of RLVR systems applied to LLMs.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

</details>


### [130] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: This paper introduces the Indian Bail Prediction System (IBPS), an AI-powered framework aimed at improving bail decision-making in India by predicting outcomes and generating rationales using factual legal data.


<details>
  <summary>Details</summary>
Motivation: Bail decisions in India are plagued by subjectivity, delays, and inconsistencies, which disproportionately impact undertrial prisoners from disadvantaged backgrounds and contribute to human rights concerns and judicial backlog.

Method: The paper presents a dataset of 150,430 bail judgments enriched with structured legal and case attributes. It fine-tunes a large language model using parameter-efficient techniques and evaluates it under various configurations.

Result: The fine-tuned model, when enhanced with statutory knowledge, significantly outperforms baselines in accuracy and explanation quality, and generalizes effectively to independent, expert-annotated datasets.

Conclusion: IBPS provides a transparent, scalable, and reproducible approach to enhance data-driven legal assistance, reduce bail adjudication delays, and promote procedural fairness in the Indian judiciary.

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [131] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
*Ziheng Li,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: The paper proposes KeyCP++, a keyword-centric chain-of-thought prompting method, to improve one-shot event detection using LLMs.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in event detection due to misinterpretation of event triggers and reliance on insufficient in-context examples.

Method: KeyCP++ uses a trigger discrimination prompting template and integrates exemplary triggers as anchors to generate propose-and-judge rationales for mitigating over-reliance on keywords.

Result: The method yields significant advancements in one-shot event detection, as demonstrated by extensive experiments.

Conclusion: KeyCP++ effectively enhances the accuracy and logical reasoning of LLMs in event detection, overcoming limitations of conventional ICL methods.

Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated
considerable success across various natural language processing tasks, it
encounters challenges in event detection. This is because LLMs lack an accurate
understanding of event triggers and tend to make over-interpretation, which
cannot be effectively corrected through in-context examples alone. In this
paper, we focus on the most challenging one-shot setting and propose KeyCP++, a
keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the
weaknesses of conventional ICL by automatically annotating the logical gaps
between input text and detection results for the demonstrations. Specifically,
to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger
discrimination prompting template. It incorporates the exemplary triggers
(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let
LLM propose candidate triggers, and justify each candidate. These
propose-and-judge rationales help LLMs mitigate over-reliance on the keywords
and promote detection rule learning. Extensive experiments demonstrate the
effectiveness of our approach, showcasing significant advancements in one-shot
event detection.

</details>


### [132] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: InterChart is a benchmark to test the reasoning capabilities of vision-language models (VLMs) across multiple related charts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of multi-chart reasoning, which is vital in domains like scientific reporting, financial analysis, and policy dashboards.

Method: InterChart challenges models with tasks of varying complexities, ranging from factual reasoning in single charts to abstract reasoning across multiple, related, visually complex charts.

Result: The evaluation shows that state-of-the-art VLMs struggle with complex chart reasoning, with accuracy sharply declining as complexity increases. Decomposing complex charts helps models perform better.

Conclusion: InterChart highlights the limitations of VLMs in multi-chart reasoning and offers a structured benchmark to improve their multimodal reasoning abilities in complex environments.

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


### [133] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
*Luyao Zhuang,Qinggang Zhang,Huachi Zhou,Juhua Liu,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: The paper introduces LoSemB, a framework to enhance the inductive tool retrieval process for evolving tool repositories without retraining.


<details>
  <summary>Details</summary>
Motivation: Address challenges in large language models dealing with unseen tools in an evolving tool repository, overcoming limitations like distribution shifts and similarity-based retrieval vulnerabilities.

Method: Introduces the Logic-Guided Semantic Bridging (LoSemB) framework with modules for logic-based embedding alignment and relational augmented retrieval.

Result: LoSemB achieves advanced performance in inductive settings and maintains effectiveness in transductive settings.

Conclusion: LoSemB enables efficient inductive tool retrieval without costly retraining, effectively adapting to evolving tool repositories.

Abstract: Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

</details>


### [134] [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: Transformer-based models, evaluated for predicting masked sentences across three domains, show limitations, especially in low-structured contexts.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to examine the capability of transformer-based LLMs to ensure global coherence and plan ahead, specifically by predicting masked sentences within structured documents.

Method: Three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) are tested on Masked Sentence Prediction (MSP) tasks from datasets spanning narrative, procedural, and expository domains. The evaluation metrics include fidelity and cohesiveness.

Result: Commercial LLMs exhibit suboptimal prediction of masked sentences, particularly in low-structured domains, despite excelling in other tasks.

Conclusion: Current transformer-based LLMs have notable limitations in capturing global coherence in low-structured contexts, which highlights an area for improvement.

Abstract: Transformer-based models primarily rely on Next Token Prediction (NTP), which
predicts the next token in a sequence based on the preceding context. However,
NTP's focus on single-token prediction often limits a model's ability to plan
ahead or maintain long-range coherence, raising questions about how well LLMs
can predict longer contexts, such as full sentences within structured
documents. While NTP encourages local fluency, it provides no explicit
incentive to ensure global coherence across sentence boundaries-an essential
skill for reconstructive or discursive tasks. To investigate this, we evaluate
three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on
Masked Sentence Prediction (MSP) - the task of infilling a randomly removed
sentence - from three domains: ROCStories (narrative), Recipe1M (procedural),
and Wikipedia (expository). We assess both fidelity (similarity to the original
sentence) and cohesiveness (fit within the surrounding context). Our key
finding reveals that commercial LLMs, despite their superlative performance in
other tasks, are poor at predicting masked sentences in low-structured domains,
highlighting a gap in current model capabilities.

</details>


### [135] [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)
*Zhenliang Zhang,Junzhe Zhang,Xinyu Hu,HuiXuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: The study examines the causal relationship between social bias and faithfulness hallucinations in large language models (LLMs), finding that biases significantly influence hallucination effects.


<details>
  <summary>Details</summary>
Motivation: To investigate whether social bias contributes causally to faithfulness hallucinations in LLMs, a topic not explored before, and address challenges in isolating causality due to confounders.

Method: The authors employ a Structural Causal Model (SCM) to validate causality between bias and hallucinations. They also develop a Bias Intervention Dataset (BID) with diverse social biases to measure causal effects precisely.

Result: Experiments on mainstream LLMs show that social biases are major causes of faithfulness hallucinations and that the effects of different bias states vary.

Conclusion: Social bias significantly impacts faithfulness hallucinations in LLMs, with the study enabling precise understanding and control of bias-induced hallucinations through causal analysis.

Abstract: Large language models (LLMs) have achieved remarkable success in various
tasks, yet they remain vulnerable to faithfulness hallucinations, where the
output does not align with the input. In this study, we investigate whether
social bias contributes to these hallucinations, a causal relationship that has
not been explored. A key challenge is controlling confounders within the
context, which complicates the isolation of causality between bias states and
hallucinations. To address this, we utilize the Structural Causal Model (SCM)
to establish and validate the causality and design bias interventions to
control confounders. In addition, we develop the Bias Intervention Dataset
(BID), which includes various social biases, enabling precise measurement of
causal effects. Experiments on mainstream LLMs reveal that biases are
significant causes of faithfulness hallucinations, and the effect of each bias
state differs in direction. We further analyze the scope of these causal
effects across various models, specifically focusing on unfairness
hallucinations, which are primarily targeted by social bias, revealing the
subtle yet significant causal effect of bias on hallucination generation.

</details>


### [136] [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)
*Zeyu Yang,Lai Wei,Roman Koshkin,Xi Chen,Satoshi Nakamura*

Main category: cs.CL

TL;DR: This study introduces a syntax-based segmentation approach to improve simultaneous speech translation, using a framework that combines Whisper and LLM models.


<details>
  <summary>Details</summary>
Motivation: To address issues of semantic fragmentation and translation inaccuracies in simultaneous speech translation through improved segmenting and syntactic structures.

Method: A grammar-based chunking strategy segments input streams by parsing dependency relations and punctuation. This is integrated into the SASST framework using a frozen Whisper encoder and a decoder-only LLM to optimize translation timing and content.

Result: Experiments on the CoVoST2 multilingual dataset (En-De, En-Zh, En-Ja) show improved translation quality and highlight the benefits of syntactic structures for LLM-driven simultaneous speech translation.

Conclusion: The integration of syntactic structures into LLM-based translation systems enhances performance, demonstrating both better timing and content in simultaneous translation.

Abstract: This work proposes a grammar-based chunking strategy that segments input
streams into semantically complete units by parsing dependency relations (e.g.,
noun phrase boundaries, verb-object structures) and punctuation features. The
method ensures chunk coherence and minimizes semantic fragmentation. Building
on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech
Translation), an end-to-end framework integrating frozen Whisper encoder and
decoder-only LLM. The unified architecture dynamically outputs translation
tokens or <WAIT> symbols to jointly optimize translation timing and content,
with target-side reordering addressing word-order divergence. Experiments on
CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation
quality improvements across languages and validate the effectiveness of
syntactic structures in LLM-driven SimulST systems.

</details>


### [137] [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
*Haoyuan Wu,Haoxing Chen,Xiaodong Chen,Zhanchao Zhou,Tieyuan Chen,Yihong Zhuang,Guoshan Lu,Zenan Huang,Junbo Zhao,Lin Liu,Zhenzhong Lan,Bei Yu,Jianguo Li*

Main category: cs.CL

TL;DR: The paper proposes a new Mixture of Experts (MoE) architecture, Grove MoE, which uses heterogeneous experts and dynamic parameter activation to improve computational efficiency and scalability of large language models.


<details>
  <summary>Details</summary>
Motivation: Traditional MoE architectures with homogeneous experts activate a fixed number of parameters regardless of input complexity, limiting computational efficiency and scalability.

Method: The authors introduce Grove MoE, a novel architecture inspired by heterogeneous CPU designs. It employs adjugate experts with a dynamic activation mechanism and applies it to create 33B-parameter models (GroveMoE-Base and GroveMoE-Inst) by upgrading an existing LLM during mid- and post-training.

Result: The GroveMoE models activate 3.14-3.28B parameters dynamically based on token complexity, achieving performance comparable to state-of-the-art open-source models of similar or larger size.

Conclusion: Grove MoE enhances computational efficiency and scalability in large language models by employing heterogeneous experts and dynamic activation, achieving high performance with lower computational overhead.

Abstract: The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate
scalability by enabling sparse parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.

</details>


### [138] [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: This study finds that persuasive language can bias large language models (LLMs) when they act as judges for tasks like scoring mathematical reasoning, making them assign higher scores to incorrect solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate whether individuals can influence LLM judges to assign unfairly high scores using persuasive language and to understand the vulnerabilities of LLM-based evaluation systems.

Method: The paper formalizes seven rhetorical persuasion techniques based on Aristotle's principles: Majority, Consistency, Flattery, Reciprocity, Pity, Authority, and Identity. These techniques were embedded into responses for mathematical reasoning tasks across six benchmarks.

Result: Persuasive language led LLM judges to assign inflated scores to incorrect answers, with an average bias increase of up to 8%. Techniques like 'Consistency' were particularly influential, and combining multiple techniques amplified the bias.

Conclusion: LLM judges are vulnerable to persuasion-based attacks, and the issue persists despite counter-prompting and larger model sizes. This underscores the need for defenses against such biases in automated evaluation systems.

Abstract: As large language models take on growing roles as automated evaluators in
practical settings, a critical question arises: Can individuals persuade an LLM
judge to assign unfairly high scores? This study is the first to reveal that
strategically embedded persuasive language can bias LLM judges when scoring
mathematical reasoning tasks, where correctness should be independent of
stylistic variation. Grounded in Aristotle's rhetorical principles, we
formalize seven persuasion techniques (Majority, Consistency, Flattery,
Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical
responses. Across six math benchmarks, we find that persuasive language leads
LLM judges to assign inflated scores to incorrect solutions, by up to 8% on
average, with Consistency causing the most severe distortion. Notably,
increasing model size does not substantially mitigate this vulnerability.
Further analysis demonstrates that combining multiple persuasion techniques
amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,
the persuasive effect persists under counter prompting strategies, highlighting
a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need
for robust defenses against persuasion-based attacks.

</details>


### [139] [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)
*Olga Kellert,Muhammad Imran,Nicholas Hill Matlis,Mahmud Uz Zaman,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: This paper evaluates compositional approaches in Focus Analysis (FA) and Sentiment Analysis (SA), comparing them with non-compositional methods while arguing their applicability across linguistic analyses.


<details>
  <summary>Details</summary>
Motivation: Quantitative evaluations of compositional approaches in SA exist, but similar studies are scarce in FA, which deals with focus/emphasis in linguistic expressions. This paper bridges this gap by showing the connection between FA and SA.

Method: The authors use compositional methods applying syntactic rules like modification, coordination, and negation (specifically Universal Dependencies) to sentiment analysis, and compare these against non-compositional methods like VADER.

Result: The compositional approach demonstrates advantages in interpretability and explainability. Its accuracy is also tested against VADER using datasets that are better suited for evaluating compositionality.

Conclusion: The study finds compositional approaches effective for both SA and FA, generalizing their applicability and highlighting their strengths in interpretability compared to non-compositional methods.

Abstract: This paper summarizes the results of evaluating a compositional approach for
Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural
Language Processing (NLP). While quantitative evaluations of compositional and
non-compositional approaches in SA exist in NLP, similar quantitative
evaluations are very rare in FA in Linguistics that deal with linguistic
expressions representing focus or emphasis such as "it was John who left". We
fill this gap in research by arguing that compositional rules in SA also apply
to FA because FA and SA are closely related meaning that SA is part of FA. Our
compositional approach in SA exploits basic syntactic rules such as rules of
modification, coordination, and negation represented in the formalism of
Universal Dependencies (UDs) in English and applied to words representing
sentiments from sentiment dictionaries. Some of the advantages of our
compositional analysis method for SA in contrast to non-compositional analysis
methods are interpretability and explainability. We test the accuracy of our
compositional approach and compare it with a non-compositional approach VADER
that uses simple heuristic rules to deal with negation, coordination and
modification. In contrast to previous related work that evaluates
compositionality in SA on long reviews, this study uses more appropriate
datasets to evaluate compositionality. In addition, we generalize the results
of compositional approaches in SA to compositional approaches in FA.

</details>


### [140] [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)
*Yu-Min Tseng,Wei-Lin Chen,Chung-Chi Chen,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: Explored using large language models (LLMs) for annotation tasks in specialized domains, comparing single models and multi-agent frameworks.


<details>
  <summary>Details</summary>
Motivation: Investigate if LLMs can replace human expert annotators for textual data labeling in specialized fields.

Method: Evaluated LLMs individually and in multi-agent settings with discussions, incorporating inference-time techniques and reasoning models like o3-mini.

Result: LLMs showed limited effectiveness, with reasoning models and techniques providing minimal improvement in specialized domains.

Conclusion: LLMs do not yet reliably match human expertise in domain-specific annotation tasks.

Abstract: Textual data annotation, the process of labeling or tagging text with
relevant information, is typically costly, time-consuming, and labor-intensive.
While large language models (LLMs) have demonstrated their potential as direct
alternatives to human annotators for general domains natural language
processing (NLP) tasks, their effectiveness on annotation tasks in domains
requiring expert knowledge remains underexplored. In this paper, we
investigate: whether top-performing LLMs, which might be perceived as having
expert-level proficiency in academic and professional benchmarks, can serve as
direct alternatives to human expert annotators? To this end, we evaluate both
individual LLMs and multi-agent approaches across three highly specialized
domains: finance, biomedicine, and law. Specifically, we propose a multi-agent
discussion framework to simulate a group of human annotators, where LLMs are
tasked to engage in discussions by considering others' annotations and
justifications before finalizing their labels. Additionally, we incorporate
reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our
empirical results reveal that: (1) Individual LLMs equipped with inference-time
techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal
or even negative performance gains, contrary to prior literature suggesting
their broad effectiveness. (2) Overall, reasoning models do not demonstrate
statistically significant improvements over non-reasoning models in most
settings. This suggests that extended long CoT provides relatively limited
benefits for data annotation in specialized domains. (3) Certain model
behaviors emerge in the multi-agent discussion environment. For instance,
Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even
when other agents provide correct annotations or valid reasoning.

</details>


### [141] [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)
*Amrita Singh,H. Suhan Karaca,Aditya Joshi,Hye-young Paik,Jiaojiao Jiang*

Main category: cs.CL

TL;DR: This paper evaluates 10 legal-specific LLMs and compares their performance to 7 general-purpose LLMs on contract classification tasks, showing superior performance from the legal-specific models.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive evaluations for legal-specific LLMs in contract understanding tasks, which creates a gap in understanding their capabilities compared to general-purpose models.

Method: The study tests 10 legal-specific LLMs against 7 general-purpose LLMs on three English language contract understanding tasks, analyzing their performance based on nuanced legal reasoning capabilities.

Result: Legal-specific LLMs outperform general-purpose models across tasks, with Legal-BERT and Contracts-BERT achieving new SOTAs despite having fewer parameters. CaseLaw-BERT and LexLM also emerge as strong baselines.

Conclusion: The findings highlight the efficiency of legal-specific LLMs in contract understanding and provide benchmarks for developing better systems.

Abstract: Despite advances in legal NLP, no comprehensive evaluation covering multiple
legal-specific LLMs currently exists for contract classification tasks in
contract understanding. To address this gap, we present an evaluation of 10
legal-specific LLMs on three English language contract understanding tasks and
compare them with 7 general-purpose LLMs. The results show that legal-specific
LLMs consistently outperform general-purpose models, especially on tasks
requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish
new SOTAs on two of the three tasks, despite having 69% fewer parameters than
the best-performing general-purpose LLM. We also identify CaseLaw-BERT and
LexLM as strong additional baselines for contract understanding. Our results
provide a holistic evaluation of legal-specific LLMs and will facilitate the
development of more accurate contract understanding systems.

</details>


### [142] [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: This paper evaluates 19 large language models (LLMs) on Czech aspect-based sentiment analysis (ABSA) across various settings, finding that fine-tuned domain-specific models outperform general-purpose ones.


<details>
  <summary>Details</summary>
Motivation: Explore the performance of LLMs in Czech ABSA to identify their strengths and limitations, addressing a relatively understudied language in sentiment analysis.

Method: Comprehensive evaluation of 19 LLMs in zero-shot, few-shot, and fine-tuning scenarios, assessing their performance and conducting error analysis on aspect term prediction.

Result: Small domain-specific models fine-tuned for ABSA excel in zero-shot and few-shot tasks, while fine-tuned general-purpose LLMs achieve state-of-the-art results. Model size, multilingualism, and recency were influential.

Conclusion: Fine-tuning enhances LLMs for Czech ABSA, highlighting their viability and offering directions for improving sentiment analysis in low-resource languages.

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to identify sentiment toward specific aspects of an entity.
While large language models (LLMs) have shown strong performance in various
natural language processing (NLP) tasks, their capabilities for Czech ABSA
remain largely unexplored. In this work, we conduct a comprehensive evaluation
of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their
performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show
that small domain-specific models fine-tuned for ABSA outperform
general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs
achieve state-of-the-art results. We analyze how factors such as
multilingualism, model size, and recency influence performance and present an
error analysis highlighting key challenges, particularly in aspect term
prediction. Our findings provide insights into the suitability of LLMs for
Czech ABSA and offer guidance for future research in this area.

</details>


### [143] [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: This paper explores the impact of adding a few annotated examples in target languages for cross-lingual aspect-based sentiment analysis and highlights significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenges of aspect-based sentiment analysis for low-resource languages due to the lack of labeled data and aims to address these limitations.

Method: The approach involved adding few-shot target language examples in training across multiple tasks, languages, and models to evaluate their impact on cross-lingual ABSA performance.

Result: The results show that adding as few as ten examples significantly boosts performance and that combining 1,000 target language examples with English data surpasses monolingual baselines.

Conclusion: Incorporating even minimal target language examples greatly enhances performance in low-resource settings, offering a feasible and effective strategy for cross-lingual ABSA.

Abstract: Aspect-based sentiment analysis (ABSA) has received substantial attention in
English, yet challenges remain for low-resource languages due to the scarcity
of labelled data. Current cross-lingual ABSA approaches often rely on external
translation tools and overlook the potential benefits of incorporating a small
number of target language examples into training. In this paper, we evaluate
the effect of adding few-shot target language examples to the training set
across four ABSA tasks, six target languages, and two sequence-to-sequence
models. We show that adding as few as ten target language examples
significantly improves performance over zero-shot settings and achieves a
similar effect to constrained decoding in reducing prediction errors.
Furthermore, we demonstrate that combining 1,000 target language examples with
English data can even surpass monolingual baselines. These findings offer
practical insights for improving cross-lingual ABSA in low-resource and
domain-specific settings, as obtaining ten high-quality annotated examples is
both feasible and highly effective.

</details>


### [144] [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)
*Chen Cecilia Liu,Hiba Arnaout,Nils Kovačić,Dana Atzil-Slonim,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper introduces CultureCare, a dataset for enhancing large language models (LLMs) to generate empathetic and culturally sensitive responses, while presenting and evaluating adaptation strategies and applications.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of LLMs lacking cultural sensitivity in emotional support due to limited resources.

Method: The authors introduced CultureCare, a novel dataset, and tested four adaptation strategies on state-of-the-art LLMs, evaluated their responses through human annotators, clinical psychologists, and LLM judges.

Result: Adapted LLMs demonstrated improved cultural sensitivity, outperforming anonymous peer responses, and showed potential in clinical training for therapists.

Conclusion: The research highlights the viability of adapted LLMs in fostering cultural competence and improving emotional support practices.

Abstract: Large language models (LLMs) show promise in offering emotional support and
generating empathetic responses for individuals in distress, but their ability
to deliver culturally sensitive support remains underexplored due to lack of
resources. In this work, we introduce CultureCare, the first dataset designed
for this task, spanning four cultures and including 1729 distress messages,
1523 cultural signals, and 1041 support strategies with fine-grained emotional
and cultural annotations. Leveraging CultureCare, we (i) develop and test four
adaptation strategies for guiding three state-of-the-art LLMs toward culturally
sensitive responses; (ii) conduct comprehensive evaluations using LLM judges,
in-culture human annotators, and clinical psychologists; (iii) show that
adapted LLMs outperform anonymous online peer responses, and that simple
cultural role-play is insufficient for cultural sensitivity; and (iv) explore
the application of LLMs in clinical training, where experts highlight their
potential in fostering cultural competence in future therapists.

</details>


### [145] [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)
*John C. McDonald,Rosalee Wolfe,Fabrizio Nunnari*

Main category: cs.CL

TL;DR: The paper introduces a two-parameter method for representing emotional expressions in signing avatars via the EASIER notation to improve consistency and nuance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of incorporating emotional non-manual signals into signing avatars due to the lack of standardized methods.

Method: The authors devised an intuitive two-parameter representation and used the EASIER notation to apply emotional controls to the Paula signing avatar.

Result: The approach enables the avatar to convey nuanced emotional states and offers a standardized way to specify emotional non-manual signals in annotations.

Conclusion: The proposed method facilitates coherent and consistent linguistic specification of emotional signals in signing avatars, overcoming previous limitations.

Abstract: Non-manual signals in sign languages continue to be a challenge for signing
avatars. More specifically, emotional content has been difficult to incorporate
because of a lack of a standard method of specifying the avatar's emotional
state. This paper explores the application of an intuitive two-parameter
representation for emotive non-manual signals to the Paula signing avatar that
shows promise for facilitating the linguistic specification of emotional facial
expressions in a more coherent manner than previous methods. Users can apply
these parameters to control Paula's emotional expressions through a textual
representation called the EASIER notation. The representation can allow avatars
to express more nuanced emotional states using two numerical parameters. It
also has the potential to enable more consistent specification of emotional
non-manual signals in linguistic annotations which drive signing avatars.

</details>


### [146] [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper introduces GREP, a multi-turn evaluation framework for assessing scientific writing, particularly related work sections. It aims to address shortcomings in current evaluation methods, ensuring alignment with expert preferences.


<details>
  <summary>Details</summary>
Motivation: There is a need for better evaluation methods for automatically generated scientific writing, specifically to align with domain-specific quality criteria and expert preferences.

Method: The proposed framework, GREP, uses a fine-grained, multi-turn evaluation approach augmented with few-shot examples and includes two variants for accessibility: a precise evaluation with proprietary LLMs and a cheaper alternative using open-weight LLMs.

Result: The framework robustly assesses the quality of related work sections, is more aligned with expert evaluations, and reveals issues with current state-of-the-art LLM-generated content.

Conclusion: GREP improves the evaluation of scientific writing quality, promoting better human-AI collaboration, but highlights limitations in current LLMs to adequately produce and adapt scientific content.

Abstract: Expert domain writing, such as scientific writing, typically demands
extensive domain knowledge. Recent advances in LLMs show promising potential in
reducing the expert workload. However, evaluating the quality of automatically
generated scientific writing is a crucial open issue, as it requires knowledge
of domain-specific evaluation criteria and the ability to discern expert
preferences. Conventional automatic metrics and LLM-as-a-judge systems are
insufficient to grasp expert preferences and domain-specific quality standards.
To address this gap and support human-AI collaborative writing, we focus on
related work generation, one of the most challenging scientific tasks, as an
exemplar. We propose GREP, a multi-turn evaluation framework that integrates
classical related work evaluation criteria with expert-specific preferences.
Instead of assigning a single score, our framework decomposes the evaluation
into fine-grained dimensions. This localized evaluation approach is further
augmented with contrastive few-shot examples to provide detailed contextual
guidance for the evaluation dimensions. The design principles allow our
framework to deliver cardinal assessment of quality, which can facilitate
better post-training compared to ordinal preference data. For better
accessibility, we design two variants of GREP: a more precise variant with
proprietary LLMs as evaluators, and a cheaper alternative with open-weight
LLMs. Empirical investigation reveals that our framework is able to assess the
quality of related work sections in a much more robust manner compared to
standard LLM judges, reflects natural scenarios of scientific writing, and
bears a strong correlation with the human expert assessment. We also observe
that generations from state-of-the-art LLMs struggle to satisfy validation
constraints of a suitable related work section. They (mostly) fail to improve
based on feedback as well.

</details>


### [147] [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)
*Changhao Song,Yazhou Zhang,Hui Gao,Ben Yao,Peng Zhang*

Main category: cs.CL

TL;DR: The paper surveys the application of large language models (LLMs) like ChatGPT in interpreting subjective language tasks such as sentiment analysis and sarcasm detection, addressing definitions, challenges, architectures, and future directions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how LLMs can address the complex and nuanced requirements of subjective language tasks, an area marked by ambiguity and contextual dependence.

Method: The authors systematically review recent advancements in LLMs for subjective tasks, discuss their suitability, highlight key datasets and methods, and propose unified and multi-task modeling approaches.

Result: The paper consolidates task-specific insights, evaluates state-of-the-art LLM methods, and examines commonalities and gaps in modeling subjective language effectively.

Conclusion: LLMs show promise in subjective language tasks but face challenges such as data limitations, bias, and ethics; multi-task and unified modeling approaches, alongside addressing open challenges, can better advance the field.

Abstract: Subjective language understanding refers to a broad set of natural language
processing tasks where the goal is to interpret or generate content that
conveys personal feelings, opinions, or figurative meanings rather than
objective facts. With the advent of large language models (LLMs) such as
ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach
these inherently nuanced tasks. In this survey, we provide a comprehensive
review of recent advances in applying LLMs to subjective language tasks,
including sentiment analysis, emotion recognition, sarcasm detection, humor
understanding, stance detection, metaphor interpretation, intent detection, and
aesthetics assessment. We begin by clarifying the definition of subjective
language from linguistic and cognitive perspectives, and we outline the unique
challenges posed by subjective language (e.g. ambiguity, figurativeness,
context dependence). We then survey the evolution of LLM architectures and
techniques that particularly benefit subjectivity tasks, highlighting why LLMs
are well-suited to model subtle human-like judgments. For each of the eight
tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based
methods, and remaining challenges. We provide comparative insights, discussing
commonalities and differences among tasks and how multi-task LLM approaches
might yield unified models of subjectivity. Finally, we identify open issues
such as data limitations, model bias, and ethical considerations, and suggest
future research directions. We hope this survey will serve as a valuable
resource for researchers and practitioners interested in the intersection of
affective computing, figurative language processing, and large-scale language
models.

</details>


### [148] [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)
*Matthias Sperber,Maureen de Seyssel,Jiajun Bao,Matthias Paulik*

Main category: cs.CL

TL;DR: Explores how human interpreting principles can inform improvements in speech translation systems, aiming for adaptive systems with interpreting-like experiences.


<details>
  <summary>Details</summary>
Motivation: Current speech translation systems lack adaptability to dynamic, real-world situations, unlike human interpreters.

Method: Examines human interpreting literature through a machine translation perspective and identifies areas for adopting human principles using modern modeling techniques.

Result: Finds that human interpreting principles could inspire advancements in speech translation systems to close usability gaps.

Conclusion: Argues for the potential to develop speech translation systems that better mimic human interpretation, enhancing practical usability.

Abstract: Current speech translation systems, while having achieved impressive
accuracies, are rather static in their behavior and do not adapt to real-world
situations in ways human interpreters do. In order to improve their practical
usefulness and enable interpreting-like experiences, a precise understanding of
the nature of human interpreting is crucial. To this end, we discuss human
interpreting literature from the perspective of the machine translation field,
while considering both operational and qualitative aspects. We identify
implications for the development of speech translation systems and argue that
there is great potential to adopt many human interpreting principles using
recent modeling techniques. We hope that our findings provide inspiration for
closing the perceived usability gap, and can motivate progress toward true
machine interpreting.

</details>


### [149] [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)
*David Arps,Hassan Sajjad,Laura Kallmeyer*

Main category: cs.CL

TL;DR: The paper evaluates three Structure-inducing Language Models (SiLMs)—Structformer, UDGN, and GPST—using natural and synthetic data to assess syntactic representation properties, grammaticality judgment, and training dynamics. GPST shows the most consistency and excels in handling long-distance dependencies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of comprehensive and comparable evaluations of Structure-inducing Language Models, which induce hierarchical sentence representations during processing.

Method: The authors compare three SiLM architectures (Structformer, UDGN, and GPST) using both natural English corpora and synthetic bracketing expressions. They analyze syntactic representation properties, performance on grammaticality tasks, and training dynamics.

Result: The study finds no model dominates across all metrics. GPST performs the most consistently, especially excelling in long-distance dependencies in bracketing expressions. Small models trained on synthetic data prove effective for evaluating model properties.

Conclusion: GPST shows superior consistency and performance in specific settings, highlighting the potential of synthetic datasets in evaluating SiLMs. Each model has unique strengths, and differences exist in syntactic representation induction.

Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised
language modeling task, and induce a hierarchical sentence representation as a
byproduct when processing an input. A wide variety of SiLMs have been proposed.
However, these have typically been evaluated on a relatively small scale, and
evaluation of these models has systematic gaps and lacks comparability. In this
work, we study three different SiLM architectures using both natural language
(English) corpora and synthetic bracketing expressions: Structformer (Shen et
al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare
them with respect to (i) properties of the induced syntactic representations
(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.
We find that none of the three architectures dominates across all evaluation
metrics. However, there are significant differences, in particular with respect
to the induced syntactic representations. The Generative Pretrained Structured
Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation
settings, and outperforms the other models on long-distance dependencies in
bracketing expressions. Furthermore, our study shows that small models trained
on large amounts of synthetic data provide a useful testbed for evaluating
basic model properties.

</details>


### [150] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
*Jiaxuan Gao,Wei Fu,Minyang Xie,Shusheng Xu,Chuyi He,Zhiyu Mei,Banghua Zhu,Yi Wu*

Main category: cs.CL

TL;DR: The paper introduces ASearcher, an open-source RL-based search agent, overcoming prior limitations in scalability, efficiency, and data quality by enabling long-horizon searches and generating high-quality datasets autonomously. It achieved significant performance improvements in benchmarks.


<details>
  <summary>Details</summary>
Motivation: The limitations of current open-source search agents in handling ambiguous queries, conducting precise searches, and enabling long-horizon exploration necessitated a scalable and efficient solution for expert-level Search Intelligence.

Method: ASearcher employs a fully asynchronous RL training framework with long-horizon search capabilities and utilizes a prompt-based LLM agent to autonomously generate high-quality QA datasets for training.

Result: ASearcher showed significant Avg@4 improvements of 46.7% on xBench and 20.8% on GAIA benchmarks during RL training. It supports over 40 tool calls and processes over 150k tokens, showcasing extreme long-horizon search proficiency.

Conclusion: The implementation of ASearcher demonstrates advancements in scalable, efficient RL techniques and autonomous QA dataset generation, achieving state-of-the-art performance among open-source 32B search agents.

Abstract: Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

</details>


### [151] [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.CL

TL;DR: The paper introduces the Medical Metaphors Corpus (MCC), a key resource containing annotated metaphors in medical and biological contexts, to address the gap in domain-specific metaphor detection.


<details>
  <summary>Details</summary>
Motivation: To bridge the existing gap in resources for detecting metaphors in scientific discourse, particularly within medical and biological domains.

Method: The authors created a dataset of 792 annotated metaphors, aggregated from diverse domain-specific sources, validated by human annotations, and provided binary as well as graded metaphoricity judgments.

Result: State-of-the-art language models show limited performance on scientific metaphor detection using MCC, highlighting the challenge in domain-specific figurative language understanding.

Conclusion: MCC serves as a foundational resource for various applications including benchmarking metaphor detection and enhancing communication in scientific and healthcare contexts.

Abstract: Metaphor is a fundamental cognitive mechanism that shapes scientific
understanding, enabling the communication of complex concepts while potentially
constraining paradigmatic thinking. Despite the prevalence of figurative
language in scientific discourse, existing metaphor detection resources
primarily focus on general-domain text, leaving a critical gap for
domain-specific applications. In this paper, we present the Medical Metaphors
Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual
metaphors spanning medical and biological domains. MCC aggregates metaphorical
expressions from diverse sources including peer-reviewed literature, news
media, social media discourse, and crowdsourced contributions, providing both
binary and graded metaphoricity judgments validated through human annotation.
Each instance includes source-target conceptual mappings and perceived
metaphoricity scores on a 0-7 scale, establishing the first annotated resource
for computational scientific metaphor research. Our evaluation demonstrates
that state-of-the-art language models achieve modest performance on scientific
metaphor detection, revealing substantial room for improvement in
domain-specific figurative language understanding. MCC enables multiple
research applications including metaphor detection benchmarking, quality-aware
generation systems, and patient-centered communication tools.

</details>


### [152] [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
*Ryan Wong,Jiawei Wang,Junjie Zhao,Li Chen,Yan Gao,Long Zhang,Xuan Zhou,Zuo Wang,Kai Xiang,Ge Zhang,Wenhao Huang,Yang Wang,Ke Wang*

Main category: cs.CL

TL;DR: The paper introduces WideSearch, a benchmark for evaluating the reliability of automated LLM-powered search agents on large-scale information collection tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address a bottleneck in large-scale information seeking tasks, which are repetitive and time-consuming for humans, by evaluating the capability of LLM-powered search agents.

Method: The authors created WideSearch, a benchmark with 200 manually curated questions across diverse domains, ensuring data quality via a five-stage quality control pipeline. They tested 10+ search systems and compared their performance against human cross-validation.

Result: Most systems perform poorly with near 0% success rates, and the best system achieves only 5%. Human testers achieve near 100% success under cross-validation conditions.

Conclusion: Current automated search agents have major deficiencies in wide-context information tasks, revealing areas for improvement in agentic search systems. The benchmark and findings promote future research.

Abstract: From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of Large Language Models (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable benchmarks. To bridge this gap, we introduce
WideSearch, a new benchmark engineered to evaluate agent reliability on these
large-scale collection tasks. The benchmark features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage quality control pipeline
ensures the difficulty, completeness, and verifiability of the dataset. We
benchmark over 10 state-of-the-art agentic search systems, including
single-agent, multi-agent frameworks, and end-to-end commercial systems. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and benchmark
results have been publicly released at https://widesearch-seed.github.io/

</details>


### [153] [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)
*Mingzi Cao,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: OpT-DeUS proposes a method for depth up-scaling large language models using Optimal Transport for neuron alignment, achieving better performance and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Scaling LLMs improves performance but incurs high costs; depth up-scaling offers efficiency but existing methods neglect neuron permutation differences, causing misalignment.

Method: The paper introduces Optimal Transport Depth Up-Scaling (OpT-DeUS), which uses Optimal Transport to align and fuse Transformer blocks for creating new layers in pre-trained models.

Result: OpT-DeUS demonstrates superior overall performance and training efficiency compared to existing methods in continual pre-training and supervised fine-tuning.

Conclusion: The analysis reveals that inserting new layers closer to the top improves both training efficiency and model performance.

Abstract: Scaling Large Language Models (LLMs) yields performance gains but incurs
substantial training costs. Depth up-scaling offers training efficiency by
adding new layers to pre-trained models. However, most existing methods copy or
average weights from base layers, neglecting neuron permutation differences.
This limitation can potentially cause misalignment that harms performance.
Inspired by applying Optimal Transport (OT) for neuron alignment, we propose
Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses
Transformer blocks in adjacent base layers via OT for new layer creation, to
mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better
overall performance and offers improved training efficiency than existing
methods for continual pre-training and supervised fine-tuning across different
model sizes. To further evaluate the impact of interpolation positions, our
extensive analysis shows that inserting new layers closer to the top results in
higher training efficiency due to shorter back-propagation time while obtaining
additional performance gains.

</details>


### [154] [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)
*Fabrizio Nunnari,Cristina Luna Jiménez,Rosalee Wolfe,John C. McDonald,Michael Filhol,Eleni Efthimiou,Evita Fotinea,Thomas Hanke*

Main category: cs.CL

TL;DR: The 2025 SLTAT workshop focuses on improving deaf/human communication, bringing together advancements in multiple fields, and fostering collaboration with the IVA community.


<details>
  <summary>Details</summary>
Motivation: To enhance communication for the deaf community using non-invasive methods, specifically leveraging digital and virtual human technologies.

Method: The workshop includes contributions from various domains like sign language recognition, avatar tech, data analysis, tools, ethics, usability, and affective computing.

Result: The workshop facilitates rich interdisciplinary collaboration and knowledge exchange between the SLTAT and IVA communities.

Conclusion: The 2025 SLTAT workshop demonstrates ongoing advancements in digital humans and sign language technologies, aligning tools and research to improve inclusivity in communication.

Abstract: The Sign Language Translation and Avatar Technology (SLTAT) workshops
continue a series of gatherings to share recent advances in improving deaf /
human communication through non-invasive means. This 2025 edition, the 9th
since its first appearance in 2011, is hosted by the International Conference
on Intelligent Virtual Agents (IVA), giving the opportunity for contamination
between two research communities, using digital humans as either virtual
interpreters or as interactive conversational agents. As presented in this
summary paper, SLTAT sees contributions beyond avatar technologies, with a
consistent number of submissions on sign language recognition, and other work
on data collection, data analysis, tools, ethics, usability, and affective
computing.

</details>


### [155] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
*Chun Wang,Chenyang Liu,Wenze Xu,Weihong Deng*

Main category: cs.CL

TL;DR: SLMs face challenges in capturing emotional and contextual information. Proposed heterogeneous adapters optimize training strategies to disentangle linguistic and paralinguistic data, achieving efficient performance in emotional tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in capturing paralinguistic cues in speech-language model training, which are crucial for emotional and contextual understanding.

Method: Proposed two heterogeneous adapters and a weakly supervised training strategy to disentangle information and optimize context preservation, focusing on speech with structured representations.

Result: The model demonstrates competitive performance in emotional conversation tasks, exhibiting effective integration of paralinguistic and linguistic information.

Conclusion: Developing heterogeneous adapters with an efficient training strategy improves speech-language model performance while maintaining parameter and data efficiency.

Abstract: Conversational systems relying on text-based large language models (LLMs)
often overlook paralinguistic cues, essential for understanding emotions and
intentions. Speech-language models (SLMs), which use speech as input, are
emerging as a promising solution. However, SLMs built by extending frozen LLMs
struggle to capture paralinguistic information and exhibit reduced context
understanding. We identify entangled information and improper training
strategies as key issues. To address these issues, we propose two heterogeneous
adapters and suggest a weakly supervised training strategy. Our approach
disentangles paralinguistic and linguistic information, enabling SLMs to
interpret speech through structured representations. It also preserves
contextual understanding by avoiding the generation of task-specific vectors
through controlled randomness. This approach trains only the adapters on common
datasets, ensuring parameter and data efficiency. Experiments demonstrate
competitive performance in emotional conversation tasks, showcasing the model's
ability to effectively integrate both paralinguistic and linguistic information
within contextual settings.

</details>


### [156] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
*Lukas Gehring,Benjamin Paaßen*

Main category: cs.CL

TL;DR: The paper evaluates text detectors for identifying LLM-generated student essays, introduces a dataset (GEDE) with varying levels of student input, and highlights performance issues in real-world educational contexts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing challenge of detecting LLM-generated texts in academia, ensuring academic integrity and accurate assessment of student contributions.

Method: The authors created the GEDE dataset (900 human-written and 12,500 LLM-generated essays) and introduced the concept of contribution levels to represent varying blends of human and LLM input. They benchmarked different detectors' performance in educational contexts.

Result: The study revealed that most text detectors struggle with classification at intermediate contribution levels (e.g., LLM-improved human texts) and often generate false positives, raising concerns about their reliability in academic settings.

Conclusion: While automated text detectors appear promising, their inconsistent performance, especially with nuanced LLM-human blended texts, may undermine trust in these tools in educational settings.

Abstract: Recent advancements in Large Language Models (LLMs) and their increased
accessibility have made it easier than ever for students to automatically
generate texts, posing new challenges for educational institutions. To enforce
norms of academic integrity and ensure students' learning, learning analytics
methods to automatically detect LLM-generated text appear increasingly
appealing. This paper benchmarks the performance of different state-of-the-art
detectors in educational contexts, introducing a novel dataset, called
Generative Essay Detection in Education (GEDE), containing over 900
student-written essays and over 12,500 LLM-generated essays from various
domains. To capture the diversity of LLM usage practices in generating text, we
propose the concept of contribution levels, representing students' contribution
to a given assignment. These levels range from purely human-written texts, to
slightly LLM-improved versions, to fully LLM-generated texts, and finally to
active attacks on the detector by "humanizing" generated texts. We show that
most detectors struggle to accurately classify texts of intermediate student
contribution levels, like LLM-improved human-written texts. Detectors are
particularly likely to produce false positives, which is problematic in
educational settings where false suspicions can severely impact students'
lives. Our dataset, code, and additional supplementary materials are publicly
available at
https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [157] [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)
*Robin Huo,Ewan Dunbar*

Main category: cs.CL

TL;DR: The paper examines how model architecture affects the linguistic information encoded in self-supervised speech representation models HuBERT and wav2vec 2.0, focusing on training objective and iterative pseudo-label refinement.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how architectural differences in speech representation models influence their ability to capture linguistic information.

Method: The authors compare HuBERT and wav2vec 2.0, analyzing training objectives and iterative pseudo-label refinement to study their effects on speech representations.

Result: Training iterations, rather than training objectives, explain the variations in the linguistic information encoded by the models as reflected in canonical correlation analyses.

Conclusion: Iterative pseudo-label refinement significantly impacts the encoding of linguistic information, warranting further research into its mechanisms and effectiveness in self-supervised speech learning models.

Abstract: Self-supervised models for speech representation learning now see widespread
use for their versatility and performance on downstream tasks, but the effect
of model architecture on the linguistic information learned in their
representations remains under-studied. This study investigates two such models,
HuBERT and wav2vec 2.0, and minimally compares two of their architectural
differences: training objective and iterative pseudo-label refinement through
multiple training iterations. We find that differences in canonical correlation
of hidden representations to word identity, phoneme identity, and speaker
identity are explained by training iteration, not training objective. We
suggest that future work investigate the reason for the effectiveness of
iterative refinement in encoding linguistic information in self-supervised
speech representations.

</details>


### [158] [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)
*Jakub Šmíd,Pavel Přibáň,Ondřej Pražák,Pavel Král*

Main category: cs.CL

TL;DR: This paper introduces a Czech dataset for advanced aspect-based sentiment analysis (ABSA), comprising 3.1K annotated reviews and 24M unannotated reviews, designed to foster cross-language comparisons and improve sentiment analysis tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for a more comprehensive and complex Czech dataset for sentiment analysis, enabling enhanced cross-lingual comparisons and supporting advanced tasks.

Method: The researchers manually annotated 3.1K restaurant reviews with two trained annotators achieving a 90% agreement rate, followed SemEval-2016 formats for unified annotations, and supplemented the dataset with 24M unannotated reviews for unsupervised learning.

Result: They provided robust monolingual baseline results using Transformer-based models and conducted error analysis to showcase the dataset's efficacy.

Conclusion: The new dataset enables advanced ABSA tasks, facilitates cross-lingual sentiment analysis efforts, and is made available for non-commercial research, showcasing high annotation quality and applicability.

Abstract: In this paper, we introduce a novel Czech dataset for aspect-based sentiment
analysis (ABSA), which consists of 3.1K manually annotated reviews from the
restaurant domain. The dataset is built upon the older Czech dataset, which
contained only separate labels for the basic ABSA tasks such as aspect term
extraction or aspect polarity detection. Unlike its predecessor, our new
dataset is specifically designed for more complex tasks, e.g.
target-aspect-category detection. These advanced tasks require a unified
annotation format, seamlessly linking sentiment elements (labels) together. Our
dataset follows the format of the well-known SemEval-2016 datasets. This design
choice allows effortless application and evaluation in cross-lingual scenarios,
ultimately fostering cross-language comparisons with equivalent counterpart
datasets in other languages. The annotation process engaged two trained
annotators, yielding an impressive inter-annotator agreement rate of
approximately 90%. Additionally, we provide 24M reviews without annotations
suitable for unsupervised learning. We present robust monolingual baseline
results achieved with various Transformer-based models and insightful error
analysis to supplement our contributions. Our code and dataset are freely
available for non-commercial research purposes.

</details>


### [159] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
*Wenze Xu,Chun Wang,Jiazhen Yu,Sheng Chen,Liang Gao,Weihong Deng*

Main category: cs.CL

TL;DR: The paper focuses on Spoken Language Models (SLMs) and proposes Optimal Transport Regularization (OTReg) to improve generalization by mitigating the modality gap between speech and text representations.


<details>
  <summary>Details</summary>
Motivation: SLMs currently face challenges in generalizing across datasets due to the modality gap between speech and text representations, which limits their effectiveness in speech understanding tasks.

Method: The authors propose OTReg, a lightweight method that aligns speech embeddings with text embeddings using an optimal transport plan and derives a regularization loss to integrate into SLM training.

Result: Extensive multilingual ASR experiments show that OTReg enhances speech-text alignment, reduces the modality gap, and improves generalization across various datasets.

Conclusion: OTReg can be seamlessly integrated into SLM training and effectively addresses modality gap issues, advancing the capabilities of SLMs in speech understanding tasks.

Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to
perceive speech inputs, have gained increasing attention for their potential to
advance speech understanding tasks. However, despite recent progress, studies
show that SLMs often struggle to generalize across datasets, even for trained
languages and tasks, raising concerns about whether they process speech in a
text-like manner as intended. A key challenge underlying this limitation is the
modality gap between speech and text representations. The high variability in
speech embeddings may allow SLMs to achieve strong in-domain performance by
exploiting unintended speech variations, ultimately hindering generalization.
To mitigate this modality gap, we introduce Optimal Transport Regularization
(OTReg), a method that formulates speech-text alignment as an optimal transport
problem and derives a regularization loss to improve SLM training. In each
training iteration, OTReg first establishes a structured correspondence between
speech and transcript embeddings by determining the optimal transport plan,
then incorporates the regularization loss based on this transport plan to
optimize SLMs in generating speech embeddings that align more effectively with
transcript embeddings. OTReg is lightweight, requiring no additional labels or
learnable parameters, and integrates seamlessly into existing SLM training
procedures. Extensive multilingual ASR experiments demonstrate that OTReg
enhances speech-text alignment, mitigates the modality gap, and consequently
improves SLM generalization across diverse datasets.

</details>


### [160] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
*Tianyi Zhou,Johanne Medina,Sanjay Chawla*

Main category: cs.CL

TL;DR: The paper explores how context affects large language models (LLMs), focusing on their ability to identify unreliable responses and proposes using uncertainty estimations to predict response reliability.


<details>
  <summary>Details</summary>
Motivation: Large language models often generate incorrect but fluent responses, and their outputs may be reused as context in applications, leading to risks. Understanding how context and uncertainty influence reliability is critical.

Method: The authors estimate reliability by analyzing token-level uncertainty, including aleatoric and epistemic uncertainty, and using this information to create compact representations for predicting response reliability. They conducted controlled experiments with these techniques.

Result: The study found that correct in-context information improves response accuracy and confidence, while misleading context causes confident errors. Their proposed method detected unreliable outputs effectively across various LLMs.

Conclusion: Uncertainty signals alone are insufficient for reliability, but probing-based methods guided by uncertainty show promise for creating reliability-aware text generation in LLMs.

Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [161] [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)
*Jun Wang,Zaifu Zhan,Qixin Zhang,Mingquan Lin,Meijia Song,Rui Zhang*

Main category: cs.CL

TL;DR: Dual-Div enhances LLM performance in biomedical NLP by improving diversity in demonstration selection, yielding up to 5% macro-F1 score gains.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for example selection in LLMs focus on representativeness but lack attention to diversity, affecting the model's adaptability to unseen biomedical NLP tasks.

Method: Dual-Div utilizes a two-stage process: initial retrieval focusing on diversity and representativeness, followed by ranking candidates against test queries for relevance and non-redundancy.

Result: Evaluations across tasks (NER, RE, TC) show Dual-Div achieving up to 5% higher macro-F1 scores versus baselines, highlighting its robustness and efficiency with few examples.

Conclusion: Incorporating diversity during initial example selection significantly improves LLM performance in biomedical tasks, with optimal efficiency using 3-5 demonstrations.

Abstract: Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.

</details>


### [162] [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)
*Wentao Jiang,Xiang Feng,Zengmao Wang,Yong Luo,Pingbo Xu,Zhe Chen,Bo Du,Jing Zhang*

Main category: cs.CL

TL;DR: The paper introduces REX-RAG, a framework that enhances policy learning in retrieval-augmented generation with reinforcement learning by addressing challenges of overconfident and incorrect reasoning paths in language models, achieving an average performance improvement of 5.1% and 3.6% on two model sizes.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in reinforcement learning for large language models during reasoning tasks, specifically issues with being trapped in overconfident but incorrect reasoning paths ("dead ends").

Method: The authors propose REX-RAG, which combines a Mixed Sampling Strategy to improve reasoning exploration and a Policy Correction Mechanism to address gradient estimation biases caused by mixed sampling.

Result: Experimental results across seven question-answering benchmarks indicate significant performance gains, with 5.1% and 3.6% improvement over baselines for Qwen2.5-3B and Qwen2.5-7B models, respectively.

Conclusion: The REX-RAG framework offers a robust solution for enhancing reasoning in retrieval-augmented generation by effectively addressing dead-end reasoning challenges, demonstrating competitive results and providing a promising direction for future improvements in reinforcement learning for LLMs.

Abstract: Reinforcement learning (RL) is emerging as a powerful paradigm for enabling
large language models (LLMs) to perform complex reasoning tasks. Recent
advances indicate that integrating RL with retrieval-augmented generation (RAG)
allows LLMs to dynamically incorporate external knowledge, leading to more
informed and robust decision making. However, we identify a critical challenge
during policy-driven trajectory sampling: LLMs are frequently trapped in
unproductive reasoning paths, which we refer to as "dead ends", committing to
overconfident yet incorrect conclusions. This severely hampers exploration and
undermines effective policy optimization. To address this challenge, we propose
REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented
Generation), a novel framework that explores alternative reasoning paths while
maintaining rigorous policy learning through principled distributional
corrections. Our approach introduces two key innovations: (1) Mixed Sampling
Strategy, which combines a novel probe sampling method with exploratory prompts
to escape dead ends; and (2) Policy Correction Mechanism, which employs
importance sampling to correct distribution shifts induced by mixed sampling,
thereby mitigating gradient estimation bias. We evaluate it on seven
question-answering benchmarks, and the experimental results show that REX-RAG
achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B
over strong baselines, demonstrating competitive results across multiple
datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

</details>


### [163] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
*Mandira Sawkar,Samay U. Shetty,Deepak Pandita,Tharindu Cyril Weerasooriya,Christopher M. Homan*

Main category: cs.CL

TL;DR: This paper adapts and enhances DisCo, a neural architecture, to improve the modeling of annotator disagreements using soft label distribution prediction across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of modeling annotator disagreements by leveraging soft label distributions and disagreement-aware mechanisms.

Method: The paper enhances the DisCo model by incorporating annotator metadata, improving input representations, and modifying loss functions to better capture disagreement patterns.

Result: Through experiments on three datasets, the improved DisCo model demonstrates significant advancements in prediction metrics, calibration, and disagreement-aware modeling.

Conclusion: The study highlights the effectiveness of disagreement-aware systems for annotator modeling, offering insights into how system components interact with complex annotated data.

Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model
annotator disagreement through soft label distribution prediction and
perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution
from Context), a neural architecture that jointly models item-level and
annotator-level label distributions, and present detailed analysis and
improvements. In this paper, we extend the DisCo by incorporating annotator
metadata, enhancing input representations, and modifying the loss functions to
capture disagreement patterns better. Through extensive experiments, we
demonstrate substantial improvements in both soft and perspectivist evaluation
metrics across three datasets. We also conduct in-depth error and calibration
analyses, highlighting the conditions under which improvements occur. Our
findings underscore the value of disagreement-aware modeling and offer insights
into how system components interact with the complexity of human-annotated
data.

</details>


### [164] [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)
*Bangsheng Tang,Carl Chengyan Fu,Fei Kou,Grigory Sizov,Haoci Zhang,Jason Park,Jiawen Liu,Jie You,Qirui Yang,Sachin Mehta,Shengyong Cai,Xiaodong Wang,Xingyu Liu,Yunlu Li,Yanjun Zhou,Wei Wei,Zhiwei Zhao,Zixi Qi,Adolfo Victoria,Aya Ibrahim,Bram Wasti,Changkyu Kim,Daniel Haziza,Fei Sun,Giancarlo Delfin,Emily Guo,Jialin Ouyang,Jaewon Lee,Jianyu Huang,Jeremy Reizenstein,Lu Fang,Quinn Zhu,Ria Verma,Vlad Mihailescu,Xingwen Guo,Yan Cui,Ye Hu,Yejin Lee*

Main category: cs.CL

TL;DR: This paper highlights optimization techniques for production-scale speculative decoding for Llama models, achieving new state-of-the-art inference speeds.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the engineering challenges of efficiently implementing speculative decoding methods for large language models in production environments.

Method: The authors detail training and inference optimization techniques, focusing on GPU-efficient implementations of tree attention and multi-round speculative decoding for EAGLE-based systems.

Result: Optimizations enable Llama4 Maverick to achieve 4 ms/token decoding speed on 8 NVIDIA H100 GPUs, with a 10% improvement over prior methods and a 1.4x to 2.0x speed-up for large batch sizes at production scale.

Conclusion: EAGLE-based speculative decoding optimizations offer significant latency improvements, making them viable for speeding up Llama models in real-world applications.

Abstract: Speculative decoding is a standard method for accelerating the inference
speed of large language models. However, scaling it for production environments
poses several engineering challenges, including efficiently implementing
different operations (e.g., tree attention and multi-round speculative
decoding) on GPU. In this paper, we detail the training and inference
optimization techniques that we have implemented to enable EAGLE-based
speculative decoding at a production scale for Llama models. With these
changes, we achieve a new state-of-the-art inference latency for Llama models.
For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a
batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the
previously best known method. Furthermore, for EAGLE-based speculative
decoding, our optimizations enable us to achieve a speed-up for large batch
sizes between 1.4x and 2.0x at production scale.

</details>


### [165] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
*Kyle Moore,Jesse Roberts,Daryl Watson*

Main category: cs.CL

TL;DR: This paper evaluates inference-time uncertainty measures in large language models, focusing on model calibration and alignment with human uncertainty.


<details>
  <summary>Details</summary>
Motivation: To improve model control and user trust by understanding and enhancing how large language models align their uncertainty estimates with human perspectives.

Method: The authors assess various inference-time uncertainty measures using both established and novel evaluation metrics to compare alignment with human uncertainty and model calibration.

Result: Several uncertainty measures align strongly with human uncertainty and exhibit moderate to strong calibration in correctness correlation and distributional metrics.

Conclusion: Inference-time uncertainty measures can effectively align with human uncertainty and demonstrate good calibration properties, indicating potential to enhance user experience with large language models.

Abstract: There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.

</details>


### [166] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
*Zhuohao Yu,Xingru Jiang,Weizheng Gu,Yidong Wang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: SAEMark introduces a framework for watermarking text generated by Large Language Models (LLMs) without compromising text quality, requiring white-box access, or altering logits.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for LLM outputs often degrade text quality and require invasive model access, limiting their applicability to closed-source and multilingual contexts.

Method: SAEMark employs inference-time, feature-based rejection sampling and deterministic features to embed personalized messages into generated text without altering model logits or retraining.

Result: Empirical evaluation shows that SAEMark achieves 99.7% F1 score in English for watermark detection across 4 datasets while exhibiting strong multi-bit detection accuracy.

Conclusion: SAEMark is a scalable, generalizable solution for content attribution and misinformation prevention that works effectively across languages and domains and for closed-source LLMs.

Abstract: Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.

</details>


### [167] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
*Shansong Wang,Mingzhe Hu,Qiang Li,Mojtaba Safari,Xiaofeng Yang*

Main category: cs.CL

TL;DR: The study evaluates GPT-5 as a multimodal medical decision-support tool and reports superior performance compared to prior models and human experts across reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Explore the potential of large language models (LLMs) like GPT-5 for complex medical decision support, leveraging multimodal data.

Method: Evaluates zero-shot chain-of-thought reasoning of GPT-5 and its variants on benchmarks such as MedQA, MedXpertQA, USMLE, and VQA-RAD under a unified protocol.

Result: GPT-5 delivers state-of-the-art accuracy, surpassing human experts, especially in multimodal reasoning with substantial performance improvements.

Conclusion: GPT-5's advancements in reasoning could shape more effective clinical decision-support systems, exceeding human expert capabilities in tested benchmarks.

Abstract: Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

</details>


### [168] [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)
*Yunna Cai,Fan Wang,Haowei Wang,Kun Wang,Kailai Yang,Sophia Ananiadou,Moyan Li,Mingming Fan*

Main category: cs.CL

TL;DR: The paper introduces PsyCrisis-Bench, a benchmark for evaluating safety alignment of language model responses in high-risk mental health dialogues using expert principles, without relying on standard references.


<details>
  <summary>Details</summary>
Motivation: Safety evaluation of language models in mental health dialogues is challenging due to the lack of gold-standard answers and the ethical sensitivity of these interactions.

Method: Developed a reference-free benchmark (PsyCrisis-Bench) with a prompt-based LLM-as-Judge approach that evaluates safety alignment using expert-defined reasoning grounded in psychological principles and provides binary scoring.

Result: The method achieved the highest agreement with expert assessments, offering interpretable rationales for evaluation outperforming existing approaches, and introduced a quality Chinese-language dataset for mental health scenarios.

Conclusion: PsyCrisis-Bench enhances safety evaluation in mental health context by providing a reliable, reference-free benchmark and an accessible dataset for improving dialogue models in these contexts.

Abstract: Evaluating the safety alignment of LLM responses in high-risk mental health
dialogues is particularly difficult due to missing gold-standard answers and
the ethically sensitive nature of these interactions. To address this
challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark
based on real-world Chinese mental health dialogues. It evaluates whether the
model responses align with the safety principles defined by experts.
Specifically designed for settings without standard references, our method
adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation
using expert-defined reasoning chains grounded in psychological intervention
principles. We employ binary point-wise scoring across multiple safety
dimensions to enhance the explainability and traceability of the evaluation.
Additionally, we present a manually curated, high-quality Chinese-language
dataset covering self-harm, suicidal ideation, and existential distress,
derived from real-world online discourse. Experiments on 3600 judgments show
that our method achieves the highest agreement with expert assessments and
produces more interpretable evaluation rationales compared to existing
approaches. Our dataset and evaluation tool are publicly available to
facilitate further research.

</details>


### [169] [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
*Jiahao Zhao,Liwei Dong*

Main category: cs.CL

TL;DR: The paper introduces Jinx, a helpful-only language model designed for studying alignment and safety failures, filling a gap in public research tools.


<details>
  <summary>Details</summary>
Motivation: Unlimited language models, capable of responding to all queries without refusals, are used internally by AI companies for testing alignment. However, such tools are not publicly available for research.

Method: The authors present Jinx, a variant of open-weight language models that eliminates safety filtering while retaining reasoning and instruction-following abilities.

Result: Jinx acts as a research tool for probing alignment failures and testing safety boundaries effectively.

Conclusion: Jinx provides a significant resource for the research community to study safety and alignment issues in language models.

Abstract: Unlimited, or so-called helpful-only language models are trained without
safety alignment constraints and never refuse user queries. They are widely
used by leading AI companies as internal tools for red teaming and alignment
evaluation. For example, if a safety-aligned model produces harmful outputs
similar to an unlimited model, this indicates alignment failures that require
further attention. Despite their essential role in assessing alignment, such
models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx
responds to all queries without refusals or safety filtering, while preserving
the base model's capabilities in reasoning and instruction following. It
provides researchers with an accessible tool for probing alignment failures,
evaluating safety boundaries, and systematically studying failure modes in
language model safety.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [170] [Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG](https://arxiv.org/abs/2508.06496)
*Rakesh Raj Madavan,Akshat Kaimal,Hashim Faisal,Chandrakala S*

Main category: cs.CV

TL;DR: This paper introduces Med-GRIM, a modular visual question answering model for medical tasks using dense encodings, graph-based retrieval, and prompt engineering, achieving strong performance with computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Medical visual question answering tasks require responses with high precision, which existing multimodal vision-language models struggle to provide.

Method: The authors extended a multimodal encoder (BIND) through dense, query-token-based encodings and used prompt engineering with small language models to dynamically integrate domain-specific knowledge. A graph retrieval system is included to enhance accuracy.

Result: Med-GRIM achieves performance comparable to large language models in medical VQA tasks at a much lower computational cost.

Conclusion: Med-GRIM provides a scalable and efficient framework for medical VQA tasks, and it introduces a new dataset, DermaGraph, to support further research.

Abstract: An ensemble of trained multimodal encoders and vision-language models (VLMs)
has become a standard approach for visual question answering (VQA) tasks.
However, such models often fail to produce responses with the detailed
precision necessary for complex, domain-specific applications such as medical
VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,
extends prior multimodal work by refining the joint embedding space through
dense, query-token-based encodings inspired by contrastive pretraining
techniques. This refined encoder powers Med-GRIM, a model designed for medical
VQA tasks that leverages graph-based retrieval and prompt engineering to
integrate domain-specific knowledge. Rather than relying on compute-heavy
fine-tuning of vision and language models on specific datasets, Med-GRIM
applies a low-compute, modular workflow with small language models (SLMs) for
efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject
relevant knowledge, ensuring both accuracy and robustness in its responses. By
assigning distinct roles to each agent within the VQA system, Med-GRIM achieves
large language model performance at a fraction of the computational cost.
Additionally, to support scalable research in zero-shot multimodal medical
applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising
diverse dermatological conditions. This dataset facilitates both multimodal and
unimodal querying. The code and dataset are available at:
https://github.com/Rakesh-123-cryp/Med-GRIM.git

</details>


### [171] [DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation](https://arxiv.org/abs/2508.06511)
*He Feng,Yongjia Ma,Donglin Di,Lei Fan,Tonghua Su,Xiangqian Wu*

Main category: cs.CV

TL;DR: DiTalker is a unified framework for animating portraits with audio and speaking style control, emphasizing lip synchronization and dynamic styles like head movements.


<details>
  <summary>Details</summary>
Motivation: To improve portrait animation by addressing issues like dynamic speaking styles, head movements, and the high computational cost of dual U-Net architectures.

Method: DiTalker uses a Style-Emotion Encoding Module for separate style and emotion extraction, an Audio-Style Fusion Module with cross-attention for animation guidance, and optimization constraints for better results.

Result: DiTalker outperforms other methods in lip synchronization and speaking style controllability, confirmed by extensive experiments.

Conclusion: DiTalker is a more efficient and effective solution for portrait animation, ensuring high-quality synchronization and style adaptation.

Abstract: Portrait animation aims to synthesize talking videos from a static reference
face, conditioned on audio and style frame cues (e.g., emotion and head poses),
while ensuring precise lip synchronization and faithful reproduction of
speaking styles. Existing diffusion-based portrait animation methods primarily
focus on lip synchronization or static emotion transformation, often
overlooking dynamic styles such as head movements. Moreover, most of these
methods rely on a dual U-Net architecture, which preserves identity consistency
but incurs additional computational overhead. To this end, we propose DiTalker,
a unified DiT-based framework for speaking style-controllable portrait
animation. We design a Style-Emotion Encoding Module that employs two separate
branches: a style branch extracting identity-specific style information (e.g.,
head poses and movements), and an emotion branch extracting identity-agnostic
emotion features. We further introduce an Audio-Style Fusion Module that
decouples audio and speaking styles via two parallel cross-attention layers,
using these features to guide the animation process. To enhance the quality of
results, we adopt and modify two optimization constraints: one to improve lip
synchronization and the other to preserve fine-grained identity and background
details. Extensive experiments demonstrate the superiority of DiTalker in terms
of lip synchronization and speaking style controllability. Project Page:
https://thenameishope.github.io/DiTalker/

</details>


### [172] [BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok](https://arxiv.org/abs/2508.06515)
*Minh Duc Chu,Kshitij Pawar,Zihao He,Roxanna Sharifi,Ross Sonnenblick,Magdalayna Curry,Laura D'Adamo,Lindsay Young,Stuart B Murray,Kristina Lerman*

Main category: cs.CV

TL;DR: The paper introduces BigTokDetect, a detection framework for identifying pro-bigorexia content on TikTok, and provides a multimodal dataset labeled by clinical experts.


<details>
  <summary>Details</summary>
Motivation: Social media platforms struggle to detect pro-bigorexia content, which negatively impacts adolescent males and evades traditional detection systems focused on 'thin ideal.'

Method: The authors developed BigTokDetect, leveraging an annotated multimodal dataset and state-of-the-art vision language models, with fine-tuning for specific domains and ablation studies to test performance enhancements.

Result: The framework achieved 0.829% accuracy in primary category classification and 0.690% in subcategory detection. Multimodal fusion improved performance by 5-10% over text-only methods.

Conclusion: BigTokDetect provides a new benchmark for detecting harmful multimodal content and offers scalable tools to aid content moderation in mental health-related areas.

Abstract: Social media platforms increasingly struggle to detect harmful content that
promotes muscle dysmorphic behaviors, particularly pro-bigorexia content that
disproportionately affects adolescent males. Unlike traditional eating disorder
detection focused on the "thin ideal," pro-bigorexia material masquerades as
legitimate fitness content through complex multimodal combinations of visual
displays, coded language, and motivational messaging that evade text-based
detection systems. We address this challenge by developing BigTokDetect, a
clinically-informed detection framework for identifying pro-bigorexia content
on TikTok. We introduce BigTok, the first expert-annotated multimodal dataset
of over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists
across five primary categories spanning body image, nutrition, exercise,
supplements, and masculinity. Through a comprehensive evaluation of
state-of-the-art vision language models, we achieve 0.829% accuracy on primary
category classification and 0.690% on subcategory detection via domain-specific
finetuning. Our ablation studies demonstrate that multimodal fusion improves
performance by 5-10% over text-only approaches, with video features providing
the most discriminative signals. These findings establish new benchmarks for
multimodal harmful content detection and provide both the computational tools
and methodological framework needed for scalable content moderation in
specialized mental health domains.

</details>


### [173] [Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation](https://arxiv.org/abs/2508.06517)
*Haoran Xi,Chen Liu,Xiaolin Li*

Main category: cs.CV

TL;DR: The paper introduces Frequency Prior Guided Matching (FPGM), a novel framework for polyp segmentation that enhances model performance by leveraging consistent edge frequency signatures across datasets.


<details>
  <summary>Details</summary>
Motivation: Develop robust automated polyp segmentation models while addressing challenges like limited annotated data and domain shift performance degradation.

Method: A two-stage SSL augmentation framework that learns domain-invariant frequency prior and adjusts the amplitude spectra of unlabeled images based on this prior while preserving phase information.

Result: Validated across six datasets and compared to ten competing methods, FPGM achieves state-of-the-art results with exceptional zero-shot generalization and a significant Dice score improvement in low-data scenarios.

Conclusion: FPGM successfully addresses domain shift and data scarcity issues with a novel augmentation strategy, making it highly suitable for clinical implementation in polyp segmentation.

Abstract: Automated polyp segmentation is essential for early diagnosis of colorectal
cancer, yet developing robust models remains challenging due to limited
annotated data and significant performance degradation under domain shift.
Although semi-supervised learning (SSL) reduces annotation requirements,
existing methods rely on generic augmentations that ignore polyp-specific
structural properties, resulting in poor generalization to new imaging centers
and devices. To address this, we introduce Frequency Prior Guided Matching
(FPGM), a novel augmentation framework built on a key discovery: polyp edges
exhibit a remarkably consistent frequency signature across diverse datasets.
FPGM leverages this intrinsic regularity in a two-stage process. It first
learns a domain-invariant frequency prior from the edge regions of labeled
polyps. Then, it performs principled spectral perturbations on unlabeled
images, aligning their amplitude spectra with this learned prior while
preserving phase information to maintain structural integrity. This targeted
alignment normalizes domain-specific textural variations, thereby compelling
the model to learn the underlying, generalizable anatomical structure.
Validated on six public datasets, FPGM establishes a new state-of-the-art
against ten competing methods. It demonstrates exceptional zero-shot
generalization capabilities, achieving over 10% absolute gain in Dice score in
data-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM
presents a powerful solution for clinically deployable polyp segmentation under
limited supervision.

</details>


### [174] [Large Language Models Facilitate Vision Reflection in Image Classification](https://arxiv.org/abs/2508.06525)
*Guoyuan An,JaeYoon Kim,SungEui Yoon*

Main category: cs.CV

TL;DR: This paper explores methods to enhance visual recognition accuracy in large multimodal models (LMMs) by using vision reflection techniques.


<details>
  <summary>Details</summary>
Motivation: Understanding the potential of LMMs in visual recognition tasks and addressing explainability to improve predictions.

Method: They prompt an LMM to verify predictions from dedicated vision models, analyze feature mapping into textual concepts, investigate token replacement strategies, and introduce a training-free connector.

Result: Improved accuracy on benchmarks like ImageNet, insights into compact textual representations, and improved fine-grained recognition without extensive training.

Conclusion: This study demonstrates the promise of leveraging vision-language reflection for interpretable and effective visual recognition.

Abstract: This paper presents several novel findings on the explainability of vision
reflection in large multimodal models (LMMs). First, we show that prompting an
LMM to verify the prediction of a specialized vision model can improve
recognition accuracy, even on benchmarks like ImageNet, despite prior evidence
that LMMs typically underperform dedicated vision encoders. Second, we analyze
the internal behavior of vision reflection and find that the vision-language
connector maps visual features into explicit textual concepts, allowing the
language model to reason about prediction plausibility using commonsense
knowledge. We further observe that replacing a large number of vision tokens
with only a few text tokens still enables LLaVA to generate similar answers,
suggesting that LMMs may rely primarily on a compact set of distilled textual
representations rather than raw vision features. Third, we show that a
training-free connector can enhance LMM performance in fine-grained recognition
tasks, without extensive feature-alignment training. Together, these findings
offer new insights into the explainability of vision-language models and
suggest that vision reflection is a promising strategy for achieving robust and
interpretable visual recognition.

</details>


### [175] [A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition](https://arxiv.org/abs/2508.06528)
*Xiuliang Zhang,Tadiwa Elisha Nyamasvisva,Chuntao Liu*

Main category: cs.CV

TL;DR: This paper introduces a hybrid model combining 3D CNN and Transformer for improved video-based behavior recognition.


<details>
  <summary>Details</summary>
Motivation: Address the shortcomings of 3D CNNs in modeling long-range dependencies and Transformer’s high computational costs in video recognition.

Method: Propose a hybrid framework where 3D CNN extracts low-level features and a Transformer captures long-range dependencies, integrated via a fusion mechanism.

Result: The model outperforms traditional 3D CNNs and Transformers in accuracy while maintaining manageable computational complexity; validated through ablation studies.

Conclusion: The hybrid framework successfully combines the strengths of 3D CNNs and Transformers, offering an effective and scalable solution for video behavior recognition.

Abstract: Video-based behavior recognition is essential in fields such as public
safety, intelligent surveillance, and human-computer interaction. Traditional
3D Convolutional Neural Network (3D CNN) effectively capture local
spatiotemporal features but struggle with modeling long-range dependencies.
Conversely, Transformers excel at learning global contextual information but
face challenges with high computational costs. To address these limitations, we
propose a hybrid framework combining 3D CNN and Transformer architectures. The
3D CNN module extracts low-level spatiotemporal features, while the Transformer
module captures long-range temporal dependencies, with a fusion mechanism
integrating both representations. Evaluated on benchmark datasets, the proposed
model outperforms traditional 3D CNN and standalone Transformers, achieving
higher recognition accuracy with manageable complexity. Ablation studies
further validate the complementary strengths of the two modules. This hybrid
framework offers an effective and scalable solution for video-based behavior
recognition.

</details>


### [176] [RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving](https://arxiv.org/abs/2508.06529)
*Jiayuan Wang,Q. M. Jonathan Wu,Katsuya Suto,Ning Zhang*

Main category: cs.CV

TL;DR: RMT-PPAD is a transformer-based multi-task model proposed for real-time panoptic driving perception tasks such as object detection, drivable area segmentation, and lane line segmentation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of precision and real-time processing in autonomous driving perception systems, ensuring accurate and efficient multi-task performance while avoiding negative task interference.

Method: RMT-PPAD uses a gate control adapter for feature fusion, an adaptive segmentation decoder to learn multi-scale feature weights during training, and resolves label inconsistencies in lane line segmentation for fairer evaluation.

Result: RMT-PPAD achieves state-of-the-art results on the BDD100K dataset with high metrics: 84.9% mAP50 for object detection, 92.6% mIoU for drivable area segmentation, and 56.8% IoU for lane line segmentation at an inference speed of 32.6 FPS.

Conclusion: RMT-PPAD demonstrates robust and consistent performance in real-world scenarios, showing its capability as an efficient solution for autonomous driving systems. The release of its source code and pre-trained models enhances its accessibility for further research and applications.

Abstract: Autonomous driving systems rely on panoptic driving perception that requires
both precision and real-time performance. In this work, we propose RMT-PPAD, a
real-time, transformer-based multi-task model that jointly performs object
detection, drivable area segmentation, and lane line segmentation. We introduce
a lightweight module, a gate control with an adapter to adaptively fuse shared
and task-specific features, effectively alleviating negative transfer between
tasks. Additionally, we design an adaptive segmentation decoder to learn the
weights over multi-scale features automatically during the training stage. This
avoids the manual design of task-specific structures for different segmentation
tasks. We also identify and resolve the inconsistency between training and
testing labels in lane line segmentation. This allows fairer evaluation.
Experiments on the BDD100K dataset demonstrate that RMT-PPAD achieves
state-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object
detection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and
accuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6
FPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD
performance in practice. The results show that RMT-PPAD consistently delivers
stable performance. The source codes and pre-trained models are released at
https://github.com/JiayuanWang-JW/RMT-PPAD.

</details>


### [177] [What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?](https://arxiv.org/abs/2508.06530)
*Ming-Kun Xie,Jia-Hao Xiao,Gang Niu,Lei Feng,Zhiqiang Kou,Min-Ling Zhang,Masashi Sugiyama*

Main category: cs.CV

TL;DR: This paper introduces the HOPE benchmark for evaluating object hallucination vulnerabilities in Large Vision-Language Models (LVLMs) through content-aware and description-based distractor construction.


<details>
  <summary>Details</summary>
Motivation: Despite advances in LVLMs, they still suffer from object hallucination, making the current POPE benchmark insufficient for assessing these vulnerabilities due to its simplistic sampling strategy.

Method: The authors propose HOPE, which uses Contrastive Language-Image Pre-Training (CLIP) to generate image-specific and misleading distractors, including non-existent objects and false descriptions, to rigorously test LVLM hallucination immunity.

Result: HOPE improves hallucination evaluation, revealing precision drops of 9%-23% in various LVLMs compared to the prior POPE benchmark, exposing lesser-known vulnerabilities.

Conclusion: HOPE is a more effective benchmark for assessing LVLM hallucination, surpassing POPE by leveraging advanced distractor construction techniques to expose deficiencies in current models.

Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large
Language Models (LLMs), have achieved impressive performance across domains.
Despite the great advances in LVLMs, they still suffer from the unavailable
object hallucination issue, which tends to generate objects inconsistent with
the image content. The most commonly used Polling-based Object Probing
Evaluation (POPE) benchmark evaluates this issue by sampling negative
categories according to category-level statistics, \textit{e.g.}, category
frequencies and co-occurrence. However, with the continuous advancement of
LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing
object hallucination, as it employs a simplistic sampling strategy that
overlooks image-specific information and restricts distractors to negative
object categories only. In this paper, we introduce the Hallucination
searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate
the most misleading distractors (\textit{i.e.}, non-existent objects or
incorrect image descriptions) that can trigger hallucination in LVLMs, which
serves as a means to more rigorously assess their immunity to hallucination. To
explore the image-specific information, the content-aware hallucination
searching leverages Contrastive Language-Image Pre-Training (CLIP) to
approximate the predictive behavior of LVLMs by selecting negative objects with
the highest predicted likelihood as distractors. To expand the scope of
hallucination assessment, the description-based hallucination searching
constructs highly misleading distractors by pairing true objects with false
descriptions. Experimental results show that HOPE leads to a precision drop of
at least 9\% and up to 23\% across various state-of-the-art LVLMs,
significantly outperforming POPE in exposing hallucination vulnerabilities. The
code is available at https://github.com/xiemk/HOPE.

</details>


### [178] [Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset](https://arxiv.org/abs/2508.06537)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: Standard object detection models trained on datasets like COCO and PASCAL VOC struggle on sparse astrophotography images. MobilTelesco offers a new dataset to study these underrepresented conditions.


<details>
  <summary>Details</summary>
Motivation: Existing object detection datasets do not sufficiently address sparsity found in specialized domains like astrophotography.

Method: The researchers created MobilTelesco, a dataset of sparse night-sky images, and evaluated multiple object detection models on it.

Result: The benchmark revealed challenges and limitations of current detection models under sparse, feature-deficient conditions.

Conclusion: The study highlights a gap in detection models when applied to sparse, non-commercial data, and introduces MobilTelesco as a resource to address this.

Abstract: Object detection models are typically trained on datasets like ImageNet,
COCO, and PASCAL VOC, which focus on everyday objects. However, these lack
signal sparsity found in non-commercial domains. MobilTelesco, a
smartphone-based astrophotography dataset, addresses this by providing sparse
night-sky images. We benchmark several detection models on it, highlighting
challenges under feature-deficient conditions.

</details>


### [179] [MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing](https://arxiv.org/abs/2508.06543)
*Jinghan Yu,Zhiyuan Ma,Yue Ma,Kaiqi Liu,Yuhan Wang,Jianjun Li*

Main category: cs.CV

TL;DR: The paper introduces a Multi-Layer Diffusion (MILD) model and a new dataset to address human erasing challenges in complex environments, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address significant challenges in human-centric image erasing where current methods struggle under complex scenarios like human occlusions, object entanglements, and background interferences.

Method: The authors propose a novel MILD model powered by semantically separated generative pathways for individual instances and backgrounds. Human Morphology Guidance and Spatially-Modulated Attention techniques enhance context understanding and attention flow.

Result: The proposed MILD method surpasses state-of-the-art techniques in human erasing tasks, demonstrating its effectiveness on complex datasets.

Conclusion: By introducing MILD, incorporating advanced guidance mechanisms, and releasing a rich dataset, the authors provide a robust solution to multi-instance human erasing, resolving issues faced by previous approaches.

Abstract: Recent years have witnessed the success of diffusion models in
image-customized tasks. Prior works have achieved notable progress on
human-oriented erasing using explicit mask guidance and semantic-aware
inpainting. However, they struggle under complex multi-IP scenarios involving
human-human occlusions, human-object entanglements, and background
interferences. These challenges are mainly due to: 1) Dataset limitations, as
existing datasets rarely cover dense occlusions, camouflaged backgrounds, and
diverse interactions; 2) Lack of spatial decoupling, where foreground instances
cannot be effectively disentangled, limiting clean background restoration. In
this work, we introduce a high-quality multi-IP human erasing dataset with
diverse pose variations and complex backgrounds. We then propose Multi-Layer
Diffusion (MILD), a novel strategy that decomposes generation into semantically
separated pathways for each instance and the background. To enhance
human-centric understanding, we introduce Human Morphology Guidance,
integrating pose, parsing, and spatial relations. We further present
Spatially-Modulated Attention to better guide attention flow. Extensive
experiments show that MILD outperforms state-of-the-art methods on challenging
human erasing benchmarks.

</details>


### [180] [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/abs/2508.06546)
*Qi Xun Yeo,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: The paper introduces a method for estimating 3D semantic scene graphs using only multi-view RGB images, without relying on ground truth 3D annotations, and achieves superior performance compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: To enable accurate 3D semantic scene graph estimation using less invasive and more accessible input data, such as multi-view RGB images, instead of relying on ground truth 3D annotations.

Method: The authors utilize pseudo point-based geometry derived from predicted depth maps, reduce background noise through semantic masks, enrich node and edge features with neighboring relationships, and refine predictions using statistical priors from training data.

Result: Their proposed method demonstrates superior performance in 3D semantic scene graph estimation compared to existing approaches that rely solely on multi-view images as input.

Conclusion: This novel approach provides a promising pathway for 3D scene graph estimation leveraging only RGB images, enabling robust and accurate predictions while reducing reliance on ground truth annotations.

Abstract: Modern 3D semantic scene graph estimation methods utilize ground truth 3D
annotations to accurately predict target objects, predicates, and
relationships. In the absence of given 3D ground truth representations, we
explore leveraging only multi-view RGB images to tackle this task. To attain
robust features for accurate scene graph estimation, we must overcome the noisy
reconstructed pseudo point-based geometry from predicted depth maps and reduce
the amount of background noise present in multi-view image features. The key is
to enrich node and edge features with accurate semantic and spatial information
and through neighboring relations. We obtain semantic masks to guide feature
aggregation to filter background features and design a novel method to
incorporate neighboring node information to aid robustness of our scene graph
estimates. Furthermore, we leverage on explicit statistical priors calculated
from the training summary statistics to refine node and edge predictions based
on their one-hop neighborhood. Our experiments show that our method outperforms
current methods purely using multi-view images as the initial input. Our
project page is available at https://qixun1.github.io/projects/SCRSSG.

</details>


### [181] [Slice or the Whole Pie? Utility Control for AI Models](https://arxiv.org/abs/2508.06551)
*Ye Tao*

Main category: cs.CV

TL;DR: NNObfuscator introduces a novel mechanism for dynamically modifying AI model performance according to predefined conditions. It eliminates the need for multiple specialized models and enables tiered user access.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of maintaining multiple model versions for varying application requirements and limitations in fine-tuning pre-trained models.

Method: The proposed method involves utilizing NNObfuscator, a utility control mechanism that allows AI models to dynamically adapt their performance levels in real-time, supported by tiered access conditions.

Result: Experimentation across image classification, semantic segmentation, and text-to-image generation tasks demonstrated NNObfuscator's ability to adapt a single trained model for diverse use cases using various models like ResNet, VGG16, and Stable Diffusion.

Conclusion: NNObfuscator enhances resource efficiency and model adaptability, allowing for systematic tiered access to AI capabilities while supporting sustainable AI deployment and business models.

Abstract: Training deep neural networks (DNNs) has become an increasingly
resource-intensive task, requiring large volumes of labeled data, substantial
computational power, and considerable fine-tuning efforts to achieve optimal
performance across diverse use cases. Although pre-trained models offer a
useful starting point, adapting them to meet specific user needs often demands
extensive customization, and infrastructure overhead. This challenge grows when
a single model must support diverse appli-cations with differing requirements
for performance. Traditional solutions often involve training multiple model
versions to meet varying requirements, which can be inefficient and difficult
to maintain. In order to overcome this challenge, we propose NNObfuscator, a
novel utility control mechanism that enables AI models to dynamically modify
their performance according to predefined conditions. It is different from
traditional methods that need separate models for each user. Instead,
NNObfuscator allows a single model to be adapted in real time, giving you
controlled access to multiple levels of performance. This mechanism enables
model owners set up tiered access, ensuring that free-tier users receive a
baseline level of performance while premium users benefit from enhanced
capabilities. The approach improves resource allocation, reduces unnecessary
computation, and supports sustainable business models in AI deployment. To
validate our approach, we conducted experiments on multiple tasks, including
image classification, semantic segmentation, and text to image generation,
using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable
Diffusion. Experimental results show that NNObfuscator successfully makes model
more adaptable, so that a single trained model can handle a broad range of
tasks without requiring a lot of changes.

</details>


### [182] [OpenHAIV: A Framework Towards Practical Open-World Learning](https://arxiv.org/abs/2508.07270)
*Xiang Xiang,Qinhao Zhou,Zhuo Xu,Jing Ma,Jiaxin Dai,Yifan Liang,Hanlin Li*

Main category: cs.CV

TL;DR: The paper introduces OpenHAIV, a framework combining OOD detection, new class discovery, and incremental learning to enhance autonomous model knowledge updates in open-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods for open-world scenarios face challenges: OOD detection alone doesn't update model knowledge, and incremental learning depends on supervised settings that don't align with open-world conditions.

Method: The study proposes OpenHAIV, a unified framework incorporating OOD detection, new class discovery, and autonomous continual fine-tuning for updating models in open environments.

Result: OpenHAIV enables models to independently acquire and update knowledge, alleviating limitations of previous approaches in open-world scenarios.

Conclusion: OpenHAIV offers a comprehensive solution to train and update models autonomously in open-world environments, advancing the state of open-world recognition research.

Abstract: Substantial progress has been made in various techniques for open-world
recognition. Out-of-distribution (OOD) detection methods can effectively
distinguish between known and unknown classes in the data, while incremental
learning enables continuous model knowledge updates. However, in open-world
scenarios, these approaches still face limitations. Relying solely on OOD
detection does not facilitate knowledge updates in the model, and incremental
fine-tuning typically requires supervised conditions, which significantly
deviate from open-world settings. To address these challenges, this paper
proposes OpenHAIV, a novel framework that integrates OOD detection, new class
discovery, and incremental continual fine-tuning into a unified pipeline. This
framework allows models to autonomously acquire and update knowledge in
open-world environments. The proposed framework is available at
https://haiv-lab.github.io/openhaiv .

</details>


### [183] [Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection](https://arxiv.org/abs/2508.06552)
*Unisha Joshi*

Main category: cs.CV

TL;DR: This paper addresses age-specific bias in deepfake detection by creating an age-diverse dataset that improves fairness and detection performance across age groups.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models often fail to address demographic biases, particularly age-specific biases, creating a gap in fair detection performance.

Method: A modular pipeline was designed to construct an age-diverse dataset by utilizing existing datasets (Celeb-DF, FaceForensics++, UTKFace) and generating synthetic data to address age distribution gaps. It was tested using models like XceptionNet, EfficientNet, and LipForensics with metrics like AUC, pAUC, and EER.

Result: Models trained on this age-diverse dataset showed fairer performance across age groups, enhanced accuracy, and better generalization across datasets.

Conclusion: The study provides a fairness-aware deepfake detection dataset and model pipeline, paving the way for future research in mitigating demographic bias in this field.

Abstract: The challenges associated with deepfake detection are increasing
significantly with the latest advancements in technology and the growing
popularity of deepfake videos and images. Despite the presence of numerous
detection models, demographic bias in the deepfake dataset remains largely
unaddressed. This paper focuses on the mitigation of age-specific bias in the
deepfake dataset by introducing an age-diverse deepfake dataset that will
improve fairness across age groups. The dataset is constructed through a
modular pipeline incorporating the existing deepfake datasets Celeb-DF,
FaceForensics++, and UTKFace datasets, and the creation of synthetic data to
fill the age distribution gaps. The effectiveness and generalizability of this
dataset are evaluated using three deepfake detection models: XceptionNet,
EfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and
EER, revealed that models trained on the age-diverse dataset demonstrated
fairer performance across age groups, improved overall accuracy, and higher
generalization across datasets. This study contributes a reproducible,
fairness-aware deepfake dataset and model pipeline that can serve as a
foundation for future research in fairer deepfake detection. The complete
dataset and implementation code are available at
https://github.com/unishajoshi/age-diverse-deepfake-detection.

</details>


### [184] [Static and Plugged: Make Embodied Evaluation Simple](https://arxiv.org/abs/2508.06553)
*Jiahao Xiao,Jianbo Zhang,BoWen Yan,Shengyu Guo,Tongrui Ye,Kaiwei Zhang,Zicheng Zhang,Xiaohong Liu,Zhengxue Cheng,Lei Fan,Chuyi Li,Guangtao Zhai*

Main category: cs.CV

TL;DR: StaticEmbodiedBench is a scalable plug-and-play benchmark designed for evaluating embodied intelligence using static scene representations.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for embodied intelligence rely on costly, fragmented, and hard-to-scale environments, creating a need for a unified and efficient evaluation approach.

Method: The paper introduces StaticEmbodiedBench, covering diverse scenarios and dimensions, alongside an evaluation of 30 models and the establishment of a static leaderboard.

Result: 19 Vision-Language Models and 11 Vision-Language-Action models were evaluated to produce a unified static leaderboard for embodied intelligence.

Conclusion: StaticEmbodiedBench offers unified, scalable evaluations and accelerates embodied intelligence research by releasing benchmark samples.

Abstract: Embodied intelligence is advancing rapidly, driving the need for efficient
evaluation. Current benchmarks typically rely on interactive simulated
environments or real-world setups, which are costly, fragmented, and hard to
scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play
benchmark that enables unified evaluation using static scene representations.
Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and
comprehensive assessment through a simple interface. Furthermore, we evaluate
19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),
establishing the first unified static leaderboard for Embodied intelligence.
Moreover, we release a subset of 200 samples from our benchmark to accelerate
the development of embodied intelligence.

</details>


### [185] [StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback](https://arxiv.org/abs/2508.06555)
*Hongbo Ma,Fei Shen,Hongbin Xu,Xiaoce Wang,Gang Xu,Jinkai Zheng,Liangqiong Qu,Ming Li*

Main category: cs.CV

TL;DR: StyleTailor introduces the first unified framework for personalized fashion styling, integrating design, recommendations, virtual try-ons, and evaluations using advanced multi-level feedback.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in intelligent systems for personalized fashion styling to enhance shopping experiences.

Method: StyleTailor employs a collaborative framework with two agents (Designer and Consultant), refined iteratively via hierarchical vision-language feedback and a closed-loop mechanism using negative prompts.

Result: Experiments show StyleTailor excels in delivering personalized designs and recommendations, significantly surpassing baselines and setting new benchmarks.

Conclusion: StyleTailor demonstrates a novel and effective approach to personalized fashion systems, emphasizing adaptability and user precision.

Abstract: The advancement of intelligent agents has revolutionized problem-solving
across diverse domains, yet solutions for personalized fashion styling remain
underexplored, which holds immense promise for promoting shopping experiences.
In this work, we present StyleTailor, the first collaborative agent framework
that seamlessly unifies personalized apparel design, shopping recommendation,
virtual try-on, and systematic evaluation into a cohesive workflow. To this
end, StyleTailor pioneers an iterative visual refinement paradigm driven by
multi-level negative feedback, enabling adaptive and precise user alignment.
Specifically, our framework features two core agents, i.e., Designer for
personalized garment selection and Consultant for virtual try-on, whose outputs
are progressively refined via hierarchical vision-language model feedback
spanning individual items, complete outfits, and try-on efficacy.
Counterexamples are aggregated into negative prompts, forming a closed-loop
mechanism that enhances recommendation quality.To assess the performance, we
introduce a comprehensive evaluation suite encompassing style consistency,
visual quality, face similarity, and artistic appraisal. Extensive experiments
demonstrate StyleTailor's superior performance in delivering personalized
designs and recommendations, outperforming strong baselines without negative
feedback and establishing a new benchmark for intelligent fashion systems.

</details>


### [186] [From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets](https://arxiv.org/abs/2508.06556)
*Sarina Penquitt,Jonathan Klees,Rinor Cakaj,Daniel Kondermann,Matthias Rottmann,Lars Schmarje*

Main category: cs.CV

TL;DR: The paper introduces a semi-automated framework, REC✔D, to address label errors in object detection datasets using crowd-sourced microtasks for review and correction.


<details>
  <summary>Details</summary>
Motivation: Label errors, such as missing or inaccurate annotations, in object detection datasets compromise training quality and benchmarks, and existing methods lack scalability and comprehensiveness for correction.

Method: The authors developed the REC✔D framework, which combines detector-generated error proposals with crowdsourcing microtasks for independent annotation verification and aggregation.

Result: Applying REC✔D to the KITTI dataset's pedestrian class revealed error rates of 24% and demonstrated its efficiency in correcting errors faster than manual annotation, though highlighting limitations in existing methods.

Conclusion: REC✔D improves dataset annotation quality and offers a new benchmark for label correction research, emphasizing the need for better error detection methods.

Abstract: Object detection has advanced rapidly in recent years, driven by increasingly
large and diverse datasets. However, label errors, defined as missing labels,
incorrect classification or inaccurate localization, often compromise the
quality of these datasets. This can have a significant impact on the outcomes
of training and benchmark evaluations. Although several methods now exist for
detecting label errors in object detection datasets, they are typically
validated only on synthetic benchmarks or limited manual inspection. How to
correct such errors systemically and at scale therefore remains an open
problem. We introduce a semi-automated framework for label-error correction
called REC$\checkmark$D (Rechecked). Building on existing detectors, the
framework pairs their error proposals with lightweight, crowd-sourced
microtasks. These tasks enable multiple annotators to independently verify each
candidate bounding box, and their responses are aggregated to estimate
ambiguity and improve label quality. To demonstrate the effectiveness of
REC$\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our
crowdsourced review yields high-quality corrected annotations, which indicate a
rate of at least 24% of missing and inaccurate annotations in original
annotations. This validated set will be released as a new real-world benchmark
for label error detection and correction. We show that current label error
detection methods, when combined with our correction framework, can recover
hundreds of errors in the time it would take a human to annotate bounding boxes
from scratch. However, even the best methods still miss up to 66% of the true
errors and with low quality labels introduce more errors than they find. This
highlights the urgent need for further research, now enabled by our released
benchmark.

</details>


### [187] [On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications](https://arxiv.org/abs/2508.06558)
*Simon Baur,Alexandra Benova,Emilio Dolgener Cantú,Jackie Ma*

Main category: cs.CV

TL;DR: The paper introduces MMPKD, a method to use training-time multimodal data (text or tabular) to improve unimodal deep learning models in healthcare imaging tasks.


<details>
  <summary>Details</summary>
Motivation: Clinical applications of deep learning often require multimodal data, which might not always be available during inference.

Method: MMPKD leverages data modalities available only during training, using teacher models (text-based and metadata-based) to distill knowledge into vision transformer student models.

Result: MMPKD enhances zero-shot ROI localization in chest radiographs but does not show consistent improvements across different domains.

Conclusion: MMPKD demonstrates promise in modality-specific applications but fails to generalize across domains, challenging previous claims in the literature.

Abstract: Deploying deep learning models in clinical practice often requires leveraging
multiple data modalities, such as images, text, and structured data, to achieve
robust and trustworthy decisions. However, not all modalities are always
available at inference time. In this work, we propose multimodal privileged
knowledge distillation (MMPKD), a training strategy that utilizes additional
modalities available solely during training to guide a unimodal vision model.
Specifically, we used a text-based teacher model for chest radiographs
(MIMIC-CXR) and a tabular metadata-based teacher model for mammography
(CBIS-DDSM) to distill knowledge into a vision transformer student model. We
show that MMPKD can improve the resulting attention maps' zero-shot
capabilities of localizing ROI in input images, while this effect does not
generalize across domains, as contrarily suggested by prior research.

</details>


### [188] [Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC](https://arxiv.org/abs/2508.06564)
*Guanyu Hu,Dimitrios Kollias,Xinyu Yang*

Main category: cs.CV

TL;DR: This paper improves emotion recognition in conversations by introducing a novel mechanism (VEGA) that integrates psychologically meaningful visual semantic anchors, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Multimodal emotion recognition is challenging due to the complex interaction between textual, acoustic, and visual signals, and current approaches lack psychological principles to improve alignment.

Method: The paper introduces a Visual Emotion Guided Anchoring (VEGA) mechanism using CLIP's image encoder to construct emotional anchors from facial exemplars. These anchors guide feature alignment via a dual-branch architecture with self-distillation and stochastic sampling for robustness.

Result: The proposed VEGA mechanism achieves state-of-the-art performance on IEMOCAP and MELD datasets.

Conclusion: The integration of psychologically meaningful visual semantics into a novel architecture enhances multimodal emotion recognition, validated through improved performance on benchmark datasets.

Abstract: Multimodal Emotion Recognition in Conversations remains a challenging task
due to the complex interplay of textual, acoustic and visual signals. While
recent models have improved performance via advanced fusion strategies, they
often lack psychologically meaningful priors to guide multimodal alignment. In
this paper, we revisit the use of CLIP and propose a novel Visual Emotion
Guided Anchoring (VEGA) mechanism that introduces class-level visual semantics
into the fusion and classification process. Distinct from prior work that
primarily utilizes CLIP's textual encoder, our approach leverages its image
encoder to construct emotion-specific visual anchors based on facial exemplars.
These anchors guide unimodal and multimodal features toward a perceptually
grounded and psychologically aligned representation space, drawing inspiration
from cognitive theories (prototypical emotion categories and multisensory
integration). A stochastic anchor sampling strategy further enhances robustness
by balancing semantic stability and intra-class diversity. Integrated into a
dual-branch architecture with self-distillation, our VEGA-augmented model
achieves sota performance on IEMOCAP and MELD. Code is available at:
https://github.com/dkollias/VEGA.

</details>


### [189] [Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06565)
*Jing Zhang,Xiaowei Yu,Minheng Chen,Lu Zhang,Tong Chen,Yan Zhuang,Chao Cao,Yanjun Lyu,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.CV

TL;DR: This paper presents a novel method to align brain connectomes with clinical text reports in a shared latent space to enhance brain disorder diagnosis.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of effectively linking objective brain imaging data with subjective clinical text reports for improved diagnostic capabilities.

Method: A framework aligning brain connectomes (using brain subnetworks as tokens) with clinical text reports in a shared cross-modal latent space, focusing on mild cognitive impairment (MCI).

Result: Achieved state-of-the-art predictive performance on the ADNI dataset and identified meaningful connectome-text pairs for deeper insights into Alzheimer's disease.

Conclusion: The proposed method improves representation learning for brain disorders, supports early Alzheimer's mechanism studies, and aids in developing useful multimodal biomarkers.

Abstract: Integrating brain imaging data with clinical reports offers a valuable
opportunity to leverage complementary multimodal information for more effective
and timely diagnosis in practical clinical settings. This approach has gained
significant attention in brain disorder research, yet a key challenge remains:
how to effectively link objective imaging data with subjective text-based
reports, such as doctors' notes. In this work, we propose a novel framework
that aligns brain connectomes with clinical reports in a shared cross-modal
latent space at both the subject and connectome levels, thereby enhancing
representation learning. The key innovation of our approach is that we treat
brain subnetworks as tokens of imaging data, rather than raw image patches, to
align with word tokens in clinical reports. This enables a more efficient
identification of system-level associations between neuroimaging findings and
clinical observations, which is critical since brain disorders often manifest
as network-level abnormalities rather than isolated regional alterations. We
applied our method to mild cognitive impairment (MCI) using the ADNI dataset.
Our approach not only achieves state-of-the-art predictive performance but also
identifies clinically meaningful connectome-text pairs, offering new insights
into the early mechanisms of Alzheimer's disease and supporting the development
of clinically useful multimodal biomarkers.

</details>


### [190] [Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features](https://arxiv.org/abs/2508.06566)
*Manish Kansana,Elias Hossain,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: Surformer v1, a transformer model, excels in surface material recognition by integrating tactile features and visual embeddings, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance robotic perception and physical interaction by efficiently combining tactile and visual inputs for surface material recognition.

Method: Surformer v1 utilizes a transformer architecture with PCA-reduced visual embeddings (ResNet-50) and cross-modal attention layers. It compares tactile-only and multimodal setups.

Result: Surformer v1 achieved 99.4% accuracy with 0.77 ms inference time, outperforming traditional models on efficiency and nearing multimodal CNN on accuracy.

Conclusion: Surformer v1 effectively balances accuracy and computational efficiency, making it suitable for real-time robotic applications.

Abstract: Surface material recognition is a key component in robotic perception and
physical interaction, particularly when leveraging both tactile and visual
sensory inputs. In this work, we propose Surformer v1, a transformer-based
architecture designed for surface classification using structured tactile
features and PCA-reduced visual embeddings extracted via ResNet-50. The model
integrates modality-specific encoders with cross-modal attention layers,
enabling rich interactions between vision and touch. Currently,
state-of-the-art deep learning models for vision tasks have achieved remarkable
performance. With this in mind, our first set of experiments focused
exclusively on tactile-only surface classification. Using feature engineering,
we trained and evaluated multiple machine learning models, assessing their
accuracy and inference time. We then implemented an encoder-only Transformer
model tailored for tactile features. This model not only achieved the highest
accuracy but also demonstrated significantly faster inference time compared to
other evaluated models, highlighting its potential for real-time applications.
To extend this investigation, we introduced a multimodal fusion setup by
combining vision and tactile inputs. We trained both Surformer v1 (using
structured features) and Multimodal CNN (using raw images) to examine the
impact of feature-based versus image-based multimodal learning on
classification accuracy and computational efficiency. The results showed that
Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while
the Multimodal CNN achieved slightly higher accuracy but required significantly
more inference time. These findings suggest Surformer v1 offers a compelling
balance between accuracy, efficiency, and computational cost for surface
material recognition.

</details>


### [191] [ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos](https://arxiv.org/abs/2508.06570)
*Mohammad Zia Ur Rehman,Anukriti Bhatnagar,Omkar Kabde,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: This paper unveils a novel dataset, ImpliHateVid, for implicit hate speech detection in videos, proposing a two-stage multimodal contrastive learning method. Evaluations show the approach's effectiveness and underscore the dataset's value.


<details>
  <summary>Details</summary>
Motivation: Existing research has emphasized text and image-based hate speech detection, leaving video-based hate speech detection insufficiently explored. The study aims to address this gap, especially for implicit hate detection in videos.

Method: The authors introduce ImpliHateVid, a large-scale dataset for implicit video hate speech detection. They propose a two-stage contrastive learning framework with modality-specific encoders for audio, text, and image in the first stage, followed by cross-encoders to refine multimodal representations in the second stage. Sentiment, emotion, and caption-based features are incorporated to enhance detection.

Result: The proposed framework was evaluated using the ImpliHateVid dataset and an additional HateMM dataset. Results highlighted the effectiveness of the multimodal contrastive method for detecting hateful video content and demonstrated the significance of the newly introduced dataset.

Conclusion: The research advances the field of hate speech detection by developing a large-scale video dataset and an innovative multimodal approach, emphasizing its potential to address implicit hate content effectively.

Abstract: The existing research has primarily focused on text and image-based hate
speech detection, video-based approaches remain underexplored. In this work, we
introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate
speech detection in videos. ImpliHateVid consists of 2,009 videos comprising
509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,
making it one of the first large-scale video datasets dedicated to implicit
hate detection. We also propose a novel two-stage contrastive learning
framework for hate speech detection in videos. In the first stage, we train
modality-specific encoders for audio, text, and image using contrastive loss by
concatenating features from the three encoders. In the second stage, we train
cross-encoders using contrastive learning to refine multimodal representations.
Additionally, we incorporate sentiment, emotion, and caption-based features to
enhance implicit hate detection. We evaluate our method on two datasets,
ImpliHateVid for implicit hate speech detection and another dataset for general
hate speech detection in videos, HateMM dataset, demonstrating the
effectiveness of the proposed multimodal contrastive learning for hateful
content detection in videos and the significance of our dataset.

</details>


### [192] [ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification](https://arxiv.org/abs/2508.06623)
*Sihan Ma,Qiming Wu,Ruotong Jiang,Frank Burns*

Main category: cs.CV

TL;DR: The paper introduces ContextGuard-LVLM, a Vision-Language Large Models framework aimed at improving the detection of fine-grained cross-modal contextual consistency (FCCC) between text and visuals, outperforming existing methods in detecting subtle misalignments.


<details>
  <summary>Details</summary>
Motivation: The study focuses on the need for better tools to verify content consistency in digital news media, particularly for fine-grained alignment of visual narratives, emotional tones, and logical coherence with textual information.

Method: The authors propose a framework, ContextGuard-LVLM, that incorporates multi-stage contextual reasoning and enhanced learning paradigms (reinforced/adversarial training). They also extend three datasets with new fine-grained contextual annotations and introduce a novel 'Contextual Coherence' (CTXT) entity type.

Result: Experiments show that ContextGuard-LVLM significantly outperforms zero-shot LVLM baselines in complex logical reasoning, nuanced contextual understanding, and robustness to subtle perturbations. It also aligns more closely with human expert evaluations.

Conclusion: ContextGuard-LVLM is a robust and effective framework for fine-grained cross-modal consistency tasks, paving the way for better alignment detection in digital news verification systems.

Abstract: The proliferation of digital news media necessitates robust methods for
verifying content veracity, particularly regarding the consistency between
visual and textual information. Traditional approaches often fall short in
addressing the fine-grained cross-modal contextual consistency (FCCC) problem,
which encompasses deeper alignment of visual narrative, emotional tone, and
background information with text, beyond mere entity matching. To address this,
we propose ContextGuard-LVLM, a novel framework built upon advanced
Vision-Language Large Models (LVLMs) and integrating a multi-stage contextual
reasoning mechanism. Our model is uniquely enhanced through reinforced or
adversarial learning paradigms, enabling it to detect subtle contextual
misalignments that evade zero-shot baselines. We extend and augment three
established datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new
fine-grained contextual annotations, including "contextual sentiment," "visual
narrative theme," and "scene-event logical coherence," and introduce a
comprehensive CTXT (Contextual Coherence) entity type. Extensive experiments
demonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art
zero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all
fine-grained consistency tasks, showing significant improvements in complex
logical reasoning and nuanced contextual understanding. Furthermore, our model
exhibits superior robustness to subtle perturbations and a higher agreement
rate with human expert judgments on challenging samples, affirming its efficacy
in discerning sophisticated forms of context detachment.

</details>


### [193] [VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis](https://arxiv.org/abs/2508.06624)
*Kexin Yu,Zihan Xu,Jialei Xie,Carter Adams*

Main category: cs.CV

TL;DR: The paper introduces VL-MedGuide, a diagnostic framework utilizing Visual-Language Large Models (LVLMs) for interpretable and accurate auxiliary diagnosis of skin diseases, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing visual diagnostic models for skin diseases lack interpretability and struggle with the diverse features in dermatoscopic images.

Method: VL-MedGuide employs a two-step process: a Multi-modal Concept Perception Module for describing visual features linguistically, and an Explainable Disease Reasoning Module integrating concepts and raw data via Chain-of-Thought prompting.

Result: VL-MedGuide outperforms baselines in disease diagnosis (83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1), and human evaluations confirm its clarity, completeness, and trustworthiness.

Conclusion: VL-MedGuide successfully bridges AI performance and clinical utility by offering explainable and actionable insights for dermatological practice.

Abstract: Accurate diagnosis of skin diseases remains a significant challenge due to
the complex and diverse visual features present in dermatoscopic images, often
compounded by a lack of interpretability in existing purely visual diagnostic
models. To address these limitations, this study introduces VL-MedGuide
(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful
multi-modal understanding and reasoning capabilities of Visual-Language Large
Models (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis
of skin conditions. VL-MedGuide operates in two interconnected stages: a
Multi-modal Concept Perception Module, which identifies and linguistically
describes dermatologically relevant visual features through sophisticated
prompt engineering, and an Explainable Disease Reasoning Module, which
integrates these concepts with raw visual information via Chain-of-Thought
prompting to provide precise disease diagnoses alongside transparent
rationales. Comprehensive experiments on the Derm7pt dataset demonstrate that
VL-MedGuide achieves state-of-the-art performance in both disease diagnosis
(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),
surpassing existing baselines. Furthermore, human evaluations confirm the high
clarity, completeness, and trustworthiness of its generated explanations,
bridging the gap between AI performance and clinical utility by offering
actionable, explainable insights for dermatological practice.

</details>


### [194] [CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation](https://arxiv.org/abs/2508.06625)
*Shilong Zou,Yuhang Huang,Renjiao Yi,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: The authors propose a novel diffusion-based method for cross-domain image translation without using paired training data, offering better fidelity and structural consistency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve cross-domain image translation by overcoming the limitations of GAN-based methods and addressing challenges in aligning diffusion and translation processes.

Method: The paper introduces a joint learning framework that aligns diffusion and translation processes, utilizing diffusion models to extract image components and a time-dependent translation network for complex mappings.

Result: Extensive experiments demonstrate superior generative performance across various tasks like RGB↔RGB, RGB↔Edge, RGB↔Semantics, and RGB↔Depth compared to state-of-the-art methods.

Conclusion: By jointly optimizing the diffusion and translation processes, the method enhances global optimality and achieves improved image fidelity and structural consistency for cross-domain translations.

Abstract: We introduce a diffusion-based cross-domain image translator in the absence
of paired training data. Unlike GAN-based methods, our approach integrates
diffusion models to learn the image translation process, allowing for more
coverable modeling of the data distribution and performance improvement of the
cross-domain translation. However, incorporating the translation process within
the diffusion process is still challenging since the two processes are not
aligned exactly, i.e., the diffusion process is applied to the noisy signal
while the translation process is conducted on the clean signal. As a result,
recent diffusion-based studies employ separate training or shallow integration
to learn the two processes, yet this may cause the local minimal of the
translation optimization, constraining the effectiveness of diffusion models.
To address the problem, we propose a novel joint learning framework that aligns
the diffusion and the translation process, thereby improving the global
optimality. Specifically, we propose to extract the image components with
diffusion models to represent the clean signal and employ the translation
process with the image components, enabling an end-to-end joint learning
manner. On the other hand, we introduce a time-dependent translation network to
learn the complex translation mapping, resulting in effective translation
learning and significant performance improvement. Benefiting from the design of
joint learning, our method enables global optimization of both processes,
enhancing the optimality and achieving improved fidelity and structural
consistency. We have conducted extensive experiments on RGB$\leftrightarrow$RGB
and diverse cross-modality translation tasks including
RGB$\leftrightarrow$Edge, RGB$\leftrightarrow$Semantics and
RGB$\leftrightarrow$Depth, showcasing better generative performances than the
state of the arts.

</details>


### [195] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: This paper proposes a novel framework that improves the rendering of complex specular reflections in Neural Radiance Fields (NeRF) by decomposing appearance into static material properties and dynamic coefficients.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with rendering sharp and realistic specular highlights in NeRF due to challenges in disentangling lighting and material properties or instability from inverse rendering approaches.

Method: The proposed method introduces dynamic coefficient decomposition, separating material properties as a static neural basis and dynamic coefficients derived from view and illumination, combined via a Dynamic Radiance Integrator for final radiance synthesis.

Result: Compared to existing methods, this framework achieves sharper and more realistic specular highlights on challenging benchmarks.

Conclusion: The decomposition paradigm enhances the realism of neural scene representations and offers an effective way to tackle complex appearance modeling in NeRF.

Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view
synthesis, but challenges remain in rendering scenes with complex specular
reflections and highlights. Existing approaches may produce blurry reflections
due to entanglement between lighting and material properties, or encounter
optimization instability when relying on physically-based inverse rendering. In
this work, we present a neural rendering framework based on dynamic coefficient
decomposition, aiming to improve the modeling of view-dependent appearance. Our
approach decomposes complex appearance into a shared, static neural basis that
encodes intrinsic material properties, and a set of dynamic coefficients
generated by a Coefficient Network conditioned on view and illumination. A
Dynamic Radiance Integrator then combines these components to synthesize the
final radiance. Experimental results on several challenging benchmarks suggest
that our method can produce sharper and more realistic specular highlights
compared to existing techniques. We hope that this decomposition paradigm can
provide a flexible and effective direction for modeling complex appearance in
neural scene representations.

</details>


### [196] [Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors](https://arxiv.org/abs/2508.06640)
*Zheyuan Zhang,Weihao Tang,Hong Chen*

Main category: cs.CV

TL;DR: The paper introduces CausalNet, a framework for robust micro-expression recognition (MER) that performs well even with key-frame index errors. It utilizes the full ME sequence as input and employs innovative modules to enhance recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of key-frame-based methods in MER, particularly their reliance on accurate key-frame indexes, which hinders practical application.

Method: CausalNet uses the full ME sequence as input to tackle key-frame index errors. It employs the Causal Motion Position Learning Module (CMPLM) to focus on muscle movements related to Action Units (AUs) and the Causal Attention Block (CAB) to analyze muscle contraction-relaxation causality.

Result: CausalNet demonstrates robustness under varying key-frame index noise levels and outperforms state-of-the-art methods on several MER benchmarks using annotated key-frames.

Conclusion: CausalNet effectively enhances the accuracy and robustness of MER, making it a step closer to practical applications. Code for implementation is provided.

Abstract: Micro-expression recognition (MER) is a highly challenging task in affective
computing. With the reduced-sized micro-expression (ME) input that contains key
information based on key-frame indexes, key-frame-based methods have
significantly improved the performance of MER. However, most of these methods
focus on improving the performance with relatively accurate key-frame indexes,
while ignoring the difficulty of obtaining accurate key-frame indexes and the
objective existence of key-frame index errors, which impedes them from moving
towards practical applications. In this paper, we propose CausalNet, a novel
framework to achieve robust MER facing key-frame index errors while maintaining
accurate recognition. To enhance robustness, CausalNet takes the representation
of the entire ME sequence as the input. To address the information redundancy
brought by the complete ME range input and maintain accurate recognition,
first, the Causal Motion Position Learning Module (CMPLM) is proposed to help
the model locate the muscle movement areas related to Action Units (AUs),
thereby reducing the attention to other redundant areas. Second, the Causal
Attention Block (CAB) is proposed to deeply learn the causal relationships
between the muscle contraction and relaxation movements in MEs. Empirical
experiments have demonstrated that on popular ME benchmarks, the CausalNet has
achieved robust MER under different levels of key-frame index noise. Meanwhile,
it has surpassed state-of-the-art (SOTA) methods on several standard MER
benchmarks when using the provided annotated key-frames. Code is available at
https://github.com/tony19980810/CausalNet.

</details>


### [197] [Towards Robust Red-Green Watermarking for Autoregressive Image Generators](https://arxiv.org/abs/2508.06656)
*Denis Lukovnikov,Andreas Müller,Erwin Quiring,Asja Fischer*

Main category: cs.CV

TL;DR: The paper explores in-generation watermarking methods for autoregressive (AR) image models by introducing token-level schemes that improve robustness and detectability under image perturbations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the unexplored potential of watermarking in autoregressive (AR) models for generated images, inspired by prior work in latent diffusion models (LDMs) and large language models.

Method: The authors propose two new watermarking approaches: a training-free method using visual token clustering via cluster lookup tables, and another that finetunes VAE encoders to predict token clusters robustly from perturbed images.

Result: The proposed cluster-level watermarking techniques improve robustness against image perturbations and regeneration attacks while maintaining high image quality. They also outperform baseline watermarking approaches in terms of detectability.

Conclusion: The novel watermarking methods not only enhance robustness and detection capabilities but also ensure fast verification times, making them practical solutions for content attribution in AR image models.

Abstract: In-generation watermarking for detecting and attributing generated content
has recently been explored for latent diffusion models (LDMs), demonstrating
high robustness. However, the use of in-generation watermarks in autoregressive
(AR) image models has not been explored yet. AR models generate images by
autoregressively predicting a sequence of visual tokens that are then decoded
into pixels using a vector-quantized decoder. Inspired by red-green watermarks
for large language models, we examine token-level watermarking schemes that
bias the next-token prediction based on prior tokens. We find that a direct
transfer of these schemes works in principle, but the detectability of the
watermarks decreases considerably under common image perturbations. As a
remedy, we propose two novel watermarking methods that rely on visual token
clustering to assign similar tokens to the same set. Firstly, we investigate a
training-free approach that relies on a cluster lookup table, and secondly, we
finetune VAE encoders to predict token clusters directly from perturbed images.
Overall, our experiments show that cluster-level watermarks improve robustness
against perturbations and regeneration attacks while preserving image quality.
Cluster classification further boosts watermark detectability, outperforming a
set of baselines. Moreover, our methods offer fast verification runtime,
comparable to lightweight post-hoc watermarking methods.

</details>


### [198] [Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision](https://arxiv.org/abs/2508.06696)
*Tianqin Li,George Liu,Tai Sing Lee*

Main category: cs.CV

TL;DR: The paper investigates using line drawings for pretraining vision models to develop compact and generalizable visual representations. This method enhances efficiency and performance in various tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of modern recognition systems that rely heavily on rich, redundant visual inputs by exploring whether structural information, as found in line drawings, can improve visual understanding.

Method: The researchers propose line-drawing-based pretraining and evaluate its impact on shape bias, attention focus, data efficiency, and intrinsic dimensionality. They also introduce an unsupervised method called 'learning to draw' to extend these benefits.

Result: Models pretrained on line drawings exhibit stronger shape bias, better attention focus, reduced dimensionality, and improved performance across tasks such as classification, detection, and segmentation. These representations are also more compressible, leading to better student-teacher model distillation.

Conclusion: Pretraining with line drawings fosters efficient, generalizable, and human-aligned vision models while offering a simple, robust strategy for enhancing modern recognition systems.

Abstract: Despite remarkable progress in computer vision, modern recognition systems
remain limited by their dependence on rich, redundant visual inputs. In
contrast, humans can effortlessly understand sparse, minimal representations
like line drawings - suggesting that structure, rather than appearance,
underlies efficient visual understanding. In this work, we propose using line
drawings as a structure-first pretraining modality to induce more compact and
generalizable visual representations. We show that models pretrained on line
drawings develop stronger shape bias, more focused attention, and greater data
efficiency across classification, detection, and segmentation tasks. Notably,
these models also exhibit lower intrinsic dimensionality, requiring
significantly fewer principal components to capture representational variance -
echoing the similar observation in low dimensional efficient representation in
the brain. Beyond performance improvements, line drawing pretraining produces
more compressible representations, enabling better distillation into
lightweight student models. Students distilled from line-pretrained teachers
consistently outperform those trained from color-supervised teachers,
highlighting the benefits of structurally compact knowledge. Finally, we
demonstrate that the pretraining with line-drawing can also be extended to
unsupervised setting via our proposed method "learning to draw". Together, our
results support the view that structure-first visual learning fosters
efficiency, generalization, and human-aligned inductive biases - offering a
simple yet powerful strategy for building more robust and adaptable vision
systems.

</details>


### [199] [MMFformer: Multimodal Fusion Transformer Network for Depression Detection](https://arxiv.org/abs/2508.06701)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: The paper presents MMFformer, an advanced multimodal transformer network for detecting depression from social media, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for diagnosing depression largely rely on subjective assessments during clinical interviews, highlighting a need for automated tools leveraging social media data.

Method: MMFformer integrates spatio-temporal analysis using transformers with residual connections for video and a transformer encoder for audio, complemented by innovative data fusion strategies.

Result: MMFformer significantly improves detection accuracy, boosting F1-Scores by 13.92% on the D-Vlog dataset and 7.74% on the LMVD dataset compared to existing methods.

Conclusion: MMFformer offers a promising solution for early depression detection via multimodal data from social media, achieving superior performance and setting a new benchmark for research in this domain.

Abstract: Depression is a serious mental health illness that significantly affects an
individual's well-being and quality of life, making early detection crucial for
adequate care and treatment. Detecting depression is often difficult, as it is
based primarily on subjective evaluations during clinical interviews. Hence,
the early diagnosis of depression, thanks to the content of social networks,
has become a prominent research area. The extensive and diverse nature of
user-generated information poses a significant challenge, limiting the accurate
extraction of relevant temporal information and the effective fusion of data
across multiple modalities. This paper introduces MMFformer, a multimodal
depression detection network designed to retrieve depressive spatio-temporal
high-level patterns from multimodal social media information. The transformer
network with residual connections captures spatial features from videos, and a
transformer encoder is exploited to design important temporal dynamics in
audio. Moreover, the fusion architecture fused the extracted features through
late and intermediate fusion strategies to find out the most relevant
intermodal correlations among them. Finally, the proposed network is assessed
on two large-scale depression detection datasets, and the results clearly
reveal that it surpasses existing state-of-the-art approaches, improving the
F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is
made available publicly at
https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.

</details>


### [200] [Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography](https://arxiv.org/abs/2508.06703)
*Justin London*

Main category: cs.CV

TL;DR: The paper introduces an efficient and fast pipeline to generate computer-generated holography using initial point cloud and MRI data, applying optimization algorithms and filtering methods to improve reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient way to generate computer-generated holograms using various datasets, such as point clouds and MRI data, and improve reconstruction quality by mitigating artifacts and noise.

Method: The approach combines volumetric object reconstruction with non-convex Fourier optics optimization techniques, including alternating projection, SGD, and quasi-Newton methods. Additionally, the results are enhanced using 2D median filtering for noise removal.

Result: Reconstruction quality of holograms is measured through MSE, RMSE, and PSNR metrics, demonstrating enhanced performance compared to alternatives like HoloNet deep learning CGH.

Conclusion: The proposed pipeline effectively improves the generation and quality of computer-generated holography through optimized methods and noise reduction techniques, showcasing its potential for practical applications.

Abstract: Computer-generated holography (CGH) is a promising method that modulates
user-defined waveforms with digital holograms. An efficient and fast pipeline
framework is proposed to synthesize CGH using initial point cloud and MRI data.
This input data is reconstructed into volumetric objects that are then input
into non-convex Fourier optics optimization algorithms for phase-only hologram
(POH) and complex-hologram (CH) generation using alternating projection, SGD,
and quasi-Netwton methods. Comparison of reconstruction performance of these
algorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet
deep learning CGH. Performance metrics are shown to be improved by using 2D
median filtering to remove artifacts and speckled noise during optimization.

</details>


### [201] [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](https://arxiv.org/abs/2508.06715)
*Jixuan He,Chieh Hubert Lin,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: The paper introduces 'Restage4D', a text-to-video pipeline that reanimates deformable 3D scenes from a video, ensuring improved geometry consistency, motion quality, and tracking performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in generating physically realistic 4D scenes using generative models that fail to account for motion dynamics and physical realism.

Method: Proposes Restage4D, leveraging video-conditioned reanimation with a video-rewinding training strategy, an occlusion-aware rigidity loss, and a disocclusion backtracing mechanism for geometry and temporal consistency.

Result: Demonstrates enhanced geometry consistency and motion quality on datasets such as DAVIS and PointOdyssey, correcting generative model errors and preserving structure during novel motion.

Conclusion: Restage4D showcases the potential of real-world video motion priors in generating physically consistent 4D content, bridging gaps in current generative modeling techniques.

Abstract: Creating deformable 3D content has gained increasing attention with the rise
of text-to-image and image-to-video generative models. While these models
provide rich semantic priors for appearance, they struggle to capture the
physical realism and motion dynamics needed for authentic 4D scene synthesis.
In contrast, real-world videos can provide physically grounded geometry and
articulation cues that are difficult to hallucinate. One question is raised:
\textit{Can we generate physically consistent 4D content by leveraging the
motion priors of the real-world video}? In this work, we explore the task of
reanimating deformable 3D scenes from a single video, using the original
sequence as a supervisory signal to correct artifacts from synthetic motion. We
introduce \textbf{Restage4D}, a geometry-preserving pipeline for
video-conditioned 4D restaging. Our approach uses a video-rewinding training
strategy to temporally bridge a real base video and a synthetic driving video
via a shared motion representation. We further incorporate an occlusion-aware
rigidity loss and a disocclusion backtracing mechanism to improve structural
and geometry consistency under challenging motion. We validate Restage4D on
DAVIS and PointOdyssey, demonstrating improved geometry consistency, motion
quality, and 3D tracking performance. Our method not only preserves deformable
structure under novel motion, but also automatically corrects errors introduced
by generative models, revealing the potential of video prior in 4D restaging
task. Source code and trained models will be released.

</details>


### [202] [FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI](https://arxiv.org/abs/2508.06756)
*Somayeh Farahani,Marjaneh Hejazi,Antonio Di Ieva,Sidong Liu*

Main category: cs.CV

TL;DR: The paper presents a foundation deep learning model, FoundBioNet, for noninvasive prediction of IDH mutation in gliomas using MRI data, achieving higher accuracy than baseline methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the reliance on invasive tissue sampling for glioma diagnosis and to leverage the potential of foundation models, addressing limitations in annotated data and spatial heterogeneity in tumors.

Method: A SWIN-UNETR-based architecture with Tumor-Aware Feature Encoding (TAFE) and Cross-Modality Differential (CMD) modules is employed to predict IDH mutation status from multi-parametric MRI in a diverse patient cohort.

Result: FoundBioNet achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on test sets from multiple datasets, significantly surpassing baseline approaches in performance.

Conclusion: The proposed model offers a more generalizable and accurate approach for glioma diagnosis, enhancing noninvasive molecular profiling and enabling personalized patient care.

Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is
essential for effective glioma management. Traditional methods rely on invasive
tissue sampling, which may fail to capture a tumor's spatial heterogeneity.
While deep learning models have shown promise in molecular profiling, their
performance is often limited by scarce annotated data. In contrast, foundation
deep learning models offer a more generalizable approach for glioma imaging
biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that
utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation
status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware
Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and
Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch
signals associated with IDH mutation. The model was trained and validated on a
diverse, multi-center cohort of 1705 glioma patients from six public datasets.
Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent
test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming
baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE
and CMD modules are essential for improving predictive accuracy. By integrating
large-scale pretraining and task-specific fine-tuning, FoundBioNet enables
generalizable glioma characterization. This approach enhances diagnostic
accuracy and interpretability, with the potential to enable more personalized
patient care.

</details>


### [203] [VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](https://arxiv.org/abs/2508.06757)
*Yash Garg,Saketh Bachu,Arindam Dutta,Rohit Lal,Sarosij Bose,Calvin-Khang Ta,M. Salman Asif,Amit Roy-Chowdhury*

Main category: cs.CV

TL;DR: Introducing VOccl3D, a new dataset to improve human pose and shape estimation under realistic occlusion scenarios. It leverages advanced computer graphics and enhances both detection and estimation performance through fine-tuned methods.


<details>
  <summary>Details</summary>
Motivation: Current human pose and shape estimation techniques struggle with complex poses and significant occlusions, partly due to insufficiently realistic datasets for evaluating methods.

Method: VOccl3D, a novel dataset created using advanced computer graphics, provides 3D body pose and shape annotations under diverse realistic occlusions. Fine-tuning state-of-the-art methods and object detectors improve performance.

Result: Fine-tuned methods, CLIFF and BEDLAM-CLIFF, showed marked improvements across public datasets and VOccl3D. Also, human detection under occlusion was boosted by retraining YOLO11, resulting in an enhanced end-to-end system.

Conclusion: VOccl3D establishes a more realistic benchmark for studying occlusions in HPS estimation, enabling better evaluation frameworks and boosting detection and estimation capabilities.

Abstract: Human pose and shape (HPS) estimation methods have been extensively studied,
with many demonstrating high zero-shot performance on in-the-wild images and
videos. However, these methods often struggle in challenging scenarios
involving complex human poses or significant occlusions. Although some studies
address 3D human pose estimation under occlusion, they typically evaluate
performance on datasets that lack realistic or substantial occlusions, e.g.,
most existing datasets introduce occlusions with random patches over the human
or clipart-style overlays, which may not reflect real-world challenges. To
bridge this gap in realistic occlusion datasets, we introduce a novel benchmark
dataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and
shape annotations. Inspired by works such as AGORA and BEDLAM, we constructed
this dataset using advanced computer graphics rendering techniques,
incorporating diverse real-world occlusion scenarios, clothing textures, and
human motions. Additionally, we fine-tuned recent HPS methods, CLIFF and
BEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and
quantitative improvements across multiple public datasets, as well as on the
test split of our dataset, while comparing its performance with other
state-of-the-art methods. Furthermore, we leveraged our dataset to enhance
human detection performance under occlusion by fine-tuning an existing object
detector, YOLO11, thus leading to a robust end-to-end HPS estimation system
under occlusions. Overall, this dataset serves as a valuable resource for
future research aimed at benchmarking methods designed to handle occlusions,
offering a more realistic alternative to existing occlusion datasets. See the
Project page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/

</details>


### [204] [SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding](https://arxiv.org/abs/2508.06763)
*Zihao Sheng,Zilin Huang,Yen-Jung Chen,Yansong Qu,Yuhao Luo,Yue Leng,Sikai Chen*

Main category: cs.CV

TL;DR: SafePLUG is a new framework designed for traffic accident understanding, excelling in fine-grained visual and temporal analysis.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with fine-grained visual details and local scene components in traffic accidents, limiting their applicability in complex scenarios.

Method: SafePLUG enables pixel-level visual understanding, supports arbitrary-shaped region-aware visual prompts, and facilitates temporal event recognition for accident analysis. A new multimodal dataset with detailed annotations is also developed.

Result: SafePLUG achieves strong results in tasks such as region-based Q&A, pixel segmentation, temporal event localization, and traffic accident understanding.

Conclusion: SafePLUG enhances fine-grained traffic scene understanding and has potential advancements in driving safety and smart transportation systems.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress
across a range of vision-language tasks and demonstrate strong potential for
traffic accident understanding. However, existing MLLMs in this domain
primarily focus on coarse-grained image-level or video-level comprehension and
often struggle to handle fine-grained visual details or localized scene
components, limiting their applicability in complex accident scenarios. To
address these limitations, we propose SafePLUG, a novel framework that empowers
MLLMs with both Pixel-Level Understanding and temporal Grounding for
comprehensive traffic accident analysis. SafePLUG supports both
arbitrary-shaped visual prompts for region-aware question answering and
pixel-level segmentation based on language instructions, while also enabling
the recognition of temporally anchored events in traffic accident scenarios. To
advance the development of MLLMs for traffic accident understanding, we curate
a new dataset containing multimodal question-answer pairs centered on diverse
accident scenarios, with detailed pixel-level annotations and temporal event
boundaries. Experimental results show that SafePLUG achieves strong performance
on multiple tasks, including region-based question answering, pixel-level
segmentation, temporal event localization, and accident event understanding.
These capabilities lay a foundation for fine-grained understanding of complex
traffic scenes, with the potential to improve driving safety and enhance
situational awareness in smart transportation systems. The code, dataset, and
model checkpoints will be made publicly available at:
https://zihaosheng.github.io/SafePLUG

</details>


### [205] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: DiffUS is a differentiable ultrasound renderer designed to synthesize realistic B-mode images from preoperative MRI scans for better intraoperative guidance.


<details>
  <summary>Details</summary>
Motivation: There is a gap between preoperative planning with high-resolution imaging and intraoperative ultrasound guidance due to noise, artifacts, and poor alignment.

Method: DiffUS converts 3D MRI scans into acoustic impedance volumes through machine learning, simulates ultrasound beam propagation using ray tracing, and reconstructs B-mode images with depth-resolved echo extraction in PyTorch.

Result: DiffUS successfully generates anatomically accurate ultrasound images from brain MRI data.

Conclusion: DiffUS improves intraoperative guidance by bridging preoperative MRI data and real-time ultrasound imaging.

Abstract: Intraoperative ultrasound imaging provides real-time guidance during numerous
surgical procedures, but its interpretation is complicated by noise, artifacts,
and poor alignment with high-resolution preoperative MRI/CT scans. To bridge
the gap between reoperative planning and intraoperative guidance, we present
DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes
realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D
scans into acoustic impedance volumes using a machine learning approach. Next,
we simulate ultrasound beam propagation using ray tracing with coupled
reflection-transmission equations. DiffUS formulates wave propagation as a
sparse linear system that captures multiple internal reflections. Finally, we
reconstruct B-mode images via depth-resolved echo extraction across fan-shaped
acquisition geometry, incorporating realistic artifacts including speckle noise
and depth-dependent degradation. DiffUS is entirely implemented as
differentiable tensor operations in PyTorch, enabling gradient-based
optimization for downstream applications such as slice-to-volume registration
and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates
DiffUS's ability to generate anatomically accurate ultrasound images from brain
MRI data.

</details>


### [206] [Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling](https://arxiv.org/abs/2508.06805)
*Aarav Mehta,Priya Deshmukh,Vikram Singh,Siddharth Malhotra,Krishnan Menon Iyer,Tanvi Iyer*

Main category: cs.CV

TL;DR: This paper focuses on improving edge localization in medical imaging by proposing a crisp edge detector with a novel backward refinement architecture.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of ConvNet-based edge detectors in achieving millimeter-level localization accuracy, which is essential for medical imaging tasks.

Method: The paper introduces a backward refinement architecture that merges low-level cues and high-level features to produce well-localized boundaries and extends the design to handle anisotropic volumes with 2D slice-wise refinement and 3D context aggregation.

Result: The method demonstrates improved boundary localization under strict evaluation metrics and enhances tasks like organ segmentation, registration, and lesion delineation.

Conclusion: The proposed technique generates crisp organ edges that significantly improve the performance of medical imaging tasks, thereby offering clinical benefits.

Abstract: Accurate localization of organ boundaries is critical in medical imaging for
segmentation, registration, surgical planning, and radiotherapy. While deep
convolutional networks (ConvNets) have advanced general-purpose edge detection
to near-human performance on natural images, their outputs often lack precise
localization, a limitation that is particularly harmful in medical applications
where millimeter-level accuracy is required. Building on a systematic analysis
of ConvNet edge outputs, we propose a medically focused crisp edge detector
that adapts a novel top-down backward refinement architecture to medical images
(2D and volumetric). Our method progressively upsamples and fuses high-level
semantic features with fine-grained low-level cues through a backward
refinement pathway, producing high-resolution, well-localized organ boundaries.
We further extend the design to handle anisotropic volumes by combining 2D
slice-wise refinement with light 3D context aggregation to retain computational
efficiency. Evaluations on several CT and MRI organ datasets demonstrate
substantially improved boundary localization under strict criteria (boundary
F-measure, Hausdorff distance) compared to baseline ConvNet detectors and
contemporary medical edge/contour methods. Importantly, integrating our crisp
edge maps into downstream pipelines yields consistent gains in organ
segmentation (higher Dice scores, lower boundary errors), more accurate image
registration, and improved delineation of lesions near organ interfaces. The
proposed approach produces clinically valuable, crisp organ edges that
materially enhance common medical-imaging tasks.

</details>


### [207] [DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation](https://arxiv.org/abs/2508.06816)
*Vikram Singh,Kabir Malhotra,Rohan Desai,Ananya Shankaracharya,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: The paper introduces a dual-resolution network with innovative features for accurate and artifact-resilient segmentation of melanocytic tumors in dermoscopic images.


<details>
  <summary>Details</summary>
Motivation: The segmentation of melanocytic tumors is critical for automated skin cancer detection, but challenges include subtle texture and color variations, artifacts, and the need for precise boundary identification.

Method: The proposed method uses a dual-resolution architecture combining full-resolution and pooled streams, boundary-aware residual connections, a channel attention module, and a multi-task training objective with Dice Tversky loss, boundary loss, and contrastive regularizers.

Result: The method achieves precise segmentation without heavy post-processing or complex pre-training. It significantly improves boundary adherence and achieves strong clinical segmentation metrics on public benchmarks.

Conclusion: The proposed dual-resolution network is effective for lesion segmentation, offering a practical tool for automated melanoma assessment with improved performance over existing standard baselines.

Abstract: Accurate segmentation of melanocytic tumors in dermoscopic images is a
critical step for automated skin cancer screening and clinical decision
support. Unlike natural scene segmentation, lesion delineation must reconcile
subtle texture and color variations, frequent artifacts (hairs, rulers,
bubbles), and a strong need for precise boundary localization to support
downstream diagnosis. In this paper we introduce Our method, a novel ResNet
inspired dual resolution architecture specifically designed for melanocytic
tumor segmentation. Our method maintains a full resolution stream that
preserves fine grained boundary information while a complementary pooled stream
aggregates multi scale contextual cues for robust lesion recognition. The
streams are tightly coupled by boundary aware residual connections that inject
high frequency edge information into deep feature maps, and by a channel
attention module that adapts color and texture sensitivity to dermoscopic
appearance. To further address common imaging artifacts and the limited size of
clinical datasets, we propose a lightweight artifact suppression block and a
multi task training objective that combines a Dice Tversky segmentation loss
with an explicit boundary loss and a contrastive regularizer for feature
stability. The combined design yields pixel accurate masks without requiring
heavy post processing or complex pre training protocols. Extensive experiments
on public dermoscopic benchmarks demonstrate that Our method significantly
improves boundary adherence and clinically relevant segmentation metrics
compared to standard encoder decoder baselines, making it a practical building
block for automated melanoma assessment systems.

</details>


### [208] [VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation](https://arxiv.org/abs/2508.06819)
*Ayaan Nooruddin Siddiqui,Mahnoor Zaidi,Ayesha Nazneen Shahbaz,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: The study introduces a weakly supervised segmentation framework that uses sparse annotations to improve the accuracy and usability of subcutaneous vessel segmentation while maintaining a low annotation burden.


<details>
  <summary>Details</summary>
Motivation: Segmentation of subcutaneous vessels is challenging due to scarce ground truth data and the noisy, low-contrast appearance of vessels in clinical images. There is a need for methods that can work with limited annotations while maintaining accuracy.

Method: The proposed framework uses sparse annotations as input, applies a differentiable random walk label propagation model for dense supervision, and integrates image-driven vesselness cues and uncertainty weighting. A CNN-based segmentation predictor is trained jointly, and a topology-aware regularizer is applied to handle connectivity and remove spurious branches.

Result: The framework outperforms traditional sparse label and dense pseudo-labeling methods, producing accurate vascular maps and better uncertainty calibration. It demonstrates improved segmentation results in clinical datasets while reducing the annotation effort.

Conclusion: This method effectively reduces annotation burden and enhances the segmentation of subcutaneous vessels while preserving crucial vessel topology, making it a suitable tool for clinical use.

Abstract: Accurate segmentation of subcutaneous vessels from clinical images is
hampered by scarce, expensive ground truth and by low contrast, noisy
appearance of vessels across patients and modalities. We present a novel weakly
supervised training framework tailored for subcutaneous vessel segmentation
that leverages inexpensive sparse annotations (e.g., centerline traces, dot
markers, or short scribbles). Sparse labels are expanded into dense,
probabilistic supervision via a differentiable random walk label propagation
model whose transition weights incorporate image driven vesselness cues and
tubular continuity priors. The propagation yields per-pixel hitting
probabilities together with calibrated uncertainty estimates; these are
incorporated into an uncertainty weighted loss to avoid over fitting to
ambiguous regions. Crucially, the label-propagator is learned jointly with a
CNN based segmentation predictor, enabling the system to discover vessel edges
and continuity constraints without explicit edge supervision. We further
introduce a topology aware regularizer that encourages centerline connectivity
and penalizes spurious branches, improving clinical usability. In experiments
on clinical subcutaneous imaging datasets, our method consistently outperforms
naive training on sparse labels and conventional dense pseudo-labeling,
producing more complete vascular maps and better calibrated uncertainty for
downstream decision making. The approach substantially reduces annotation
burden while preserving clinically relevant vessel topology.

</details>


### [209] [Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification](https://arxiv.org/abs/2508.06831)
*Taha Mustapha Nehdi,Nairouz Mrabah,Atif Belal,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: The paper introduces SAGE-reID, a cost-effective multi-source domain adaptation (MSDA) method for person re-identification (reID) that eliminates the need for source domain data during adaptation, reduces computational costs, and improves accuracy.


<details>
  <summary>Details</summary>
Motivation: Improving person re-identification (reID) across diverse target environments using computationally efficient methods without requiring access to source domain data during adaptation.

Method: SAGE-reID uses source-specific low-rank adapters (LoRA) trained through source-free unsupervised domain adaptation (UDA) and a lightweight gating network to dynamically merge these adapters for cross-domain knowledge transfer.

Result: SAGE-reID achieves state-of-the-art performance on challenging benchmarks (Market-1501, DukeMTMC-reID, MSMT17) while reducing memory consumption and computational costs.

Conclusion: The proposed SAGE-reID framework effectively handles multi-source reID adaptation without overfitting, requiring minimal computational resources compared to existing methods.

Abstract: Adapting person re-identification (reID) models to new target environments
remains a challenging problem that is typically addressed using unsupervised
domain adaptation (UDA) methods. Recent works show that when labeled data
originates from several distinct sources (e.g., datasets and cameras),
considering each source separately and applying multi-source domain adaptation
(MSDA) typically yields higher accuracy and robustness compared to blending the
sources and performing conventional UDA. However, state-of-the-art MSDA methods
learn domain-specific backbone models or require access to source domain data
during adaptation, resulting in significant growth in training parameters and
computational cost. In this paper, a Source-free Adaptive Gated Experts
(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a
cost-effective, source-free MSDA method that first trains individual
source-specific low-rank adapters (LoRA) through source-free UDA. Next, a
lightweight gating network is introduced and trained to dynamically assign
optimal merging weights for fusion of LoRA experts, enabling effective
cross-domain knowledge transfer. While the number of backbone parameters
remains constant across source domains, LoRA experts scale linearly but remain
negligible in size (<= 2% of the backbone), reducing both the memory
consumption and risk of overfitting. Extensive experiments conducted on three
challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that
SAGE-reID outperforms state-of-the-art methods while being computationally
efficient.

</details>


### [210] [Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology](https://arxiv.org/abs/2508.06845)
*Hamidreza Samadi,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.CV

TL;DR: The paper presents a hybrid machine learning method to forecast geometric deviations in manufactured components, achieving 73% more accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurately forecasting geometric deviations in manufacturing is critical but challenging, particularly for complex geometries, even with advancements in manufacturing technologies.

Method: The study utilizes a high-resolution 3D scanner for data acquisition, advanced pre-processing techniques, and combines convolutional neural networks with gradient-boosted decision trees for predictive modeling.

Result: The proposed system improves prediction accuracy to 0.012 mm at a 95% confidence level, significantly outperforming conventional methods by 73%, while uncovering hidden correlations in the data.

Conclusion: The methodology enhances automated quality control, predictive maintenance, and design optimization capabilities in precision manufacturing, laying groundwork for future research.

Abstract: This study addresses the challenge of accurately forecasting geometric
deviations in manufactured components using advanced 3D surface analysis.
Despite progress in modern manufacturing, maintaining dimensional precision
remains difficult, particularly for complex geometries. We present a
methodology that employs a high-resolution 3D scanner to acquire multi-angle
surface data from 237 components produced across different batches. The data
were processed through precise alignment, noise reduction, and merging
techniques to generate accurate 3D representations. A hybrid machine learning
framework was developed, combining convolutional neural networks for feature
extraction with gradient-boosted decision trees for predictive modeling. The
proposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence
level, representing a 73% improvement over conventional statistical process
control methods. In addition to improved accuracy, the model revealed hidden
correlations between manufacturing parameters and geometric deviations. This
approach offers significant potential for automated quality control, predictive
maintenance, and design optimization in precision manufacturing, and the
resulting dataset provides a strong foundation for future predictive modeling
research.

</details>


### [211] [AGIC: Attention-Guided Image Captioning to Improve Caption Relevance](https://arxiv.org/abs/2508.06853)
*L. D. M. S. Sai Teja,Ashok Urlana,Pruthwik Mishra*

Main category: cs.CV

TL;DR: The paper presents an Attention-Guided Image Captioning (AGIC) model that improves image captioning accuracy by enhancing salient visual regions and introduces a hybrid decoding strategy for better fluency and diversity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of generating more accurate and descriptive captions for images, which remains a difficult task despite advancements in the field.

Method: The method involves amplifying salient visual regions in the feature space with attention guidance and employing a hybrid decoding strategy combining deterministic and probabilistic sampling.

Result: AGIC shows competitive or superior performance to state-of-the-art models on the Flickr8k and Flickr30k datasets while enabling faster inference.

Conclusion: AGIC represents a scalable and interpretable approach for image captioning, maintaining strong performance across multiple evaluation metrics and achieving both speed and quality improvements.

Abstract: Despite significant progress in image captioning, generating accurate and
descriptive captions remains a long-standing challenge. In this study, we
propose Attention-Guided Image Captioning (AGIC), which amplifies salient
visual regions directly in the feature space to guide caption generation. We
further introduce a hybrid decoding strategy that combines deterministic and
probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we
conduct extensive experiments on the Flickr8k and Flickr30k datasets. The
results show that AGIC matches or surpasses several state-of-the-art models
while achieving faster inference. Moreover, AGIC demonstrates strong
performance across multiple evaluation metrics, offering a scalable and
interpretable solution for image captioning.

</details>


### [212] [A Joint Sparse Self-Representation Learning Method for Multiview Clustering](https://arxiv.org/abs/2508.06857)
*Mengxue Jia,Zhihua Allen-Zhao,You Zhao,Sanyang Liu*

Main category: cs.CV

TL;DR: This paper develops a new multiview clustering approach leveraging joint sparse self-representation with $
\ell_0$-norm constraints and introduces an alternating quadratic penalty method for optimization.


<details>
  <summary>Details</summary>
Motivation: Existing subspace clustering methods for MC heavily depend on tools like Graph-Laplacian regularization, which may not efficiently capture consistent and local information across multiple views.

Method: The authors propose a model utilizing cardinality constraints ($\ell_0$-norm) for local and global structure extraction, combined with a low-rank constraint for global coherence, and solve the optimization using a novel alternating quadratic penalty method.

Result: Their method achieves strong clustering performance across six standard datasets, surpassing eight state-of-the-art multiview clustering methods.

Conclusion: This work effectively combines sparse representation with cardinality constraints and introduces a globally convergent optimization algorithm, advancing multiview clustering methods.

Abstract: Multiview clustering (MC) aims to group samples using consistent and
complementary information across various views. The subspace clustering, as a
fundamental technique of MC, has attracted significant attention. In this
paper, we propose a novel joint sparse self-representation learning model for
MC, where a featured difference is the extraction of view-specific local
information by introducing cardinality (i.e., $\ell_0$-norm) constraints
instead of Graph-Laplacian regularization. Specifically, under each view,
cardinality constraints directly restrict the samples used in the
self-representation stage to extract reliable local and global structure
information, while the low-rank constraint aids in revealing a global coherent
structure in the consensus affinity matrix during merging. The attendant
challenge is that Augmented Lagrange Method (ALM)-based alternating
minimization algorithms cannot guarantee convergence when applied directly to
our nonconvex, nonsmooth model, thus resulting in poor generalization ability.
To address it, we develop an alternating quadratic penalty (AQP) method with
global convergence, where two subproblems are iteratively solved by closed-form
solutions. Empirical results on six standard datasets demonstrate the
superiority of our model and AQP method, compared to eight state-of-the-art
algorithms.

</details>


### [213] [VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding](https://arxiv.org/abs/2508.06869)
*Jianxiang He,Shaoguang Wang,Weiyu Guo,Meisheng Hong,Jungang Li,Yijie Xu,Ziyang Chen,Hui Xiong*

Main category: cs.CV

TL;DR: The paper introduces Visual-Subtitle Integration (VSI), a method for multimodal keyframe search in long videos using visual and subtitle information, achieving state-of-the-art results in several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Keyframe retrieval in long video understanding is hindered by poor multimodal alignment and lack of temporal semantic reasoning, necessitating improved approaches.

Method: The authors proposed VSI, which integrates subtitles, timestamps, and scene boundaries into a unified multimodal search using dual-stream (Visual and Subtitle) search mechanisms that interact for improved accuracy.

Result: VSI significantly outperforms baselines, achieving 40.00% keyframe localization accuracy and 68.48% accuracy on long Video-QA tasks, with state-of-the-art results in relevant benchmarks.

Conclusion: The integration of visual content with textual information via dual-stream search improves keyframe retrieval and reasoning in long video tasks, demonstrating robustness and generalizability.

Abstract: Long video understanding presents a significant challenge to multimodal large
language models (MLLMs) primarily due to the immense data scale. A critical and
widely adopted strategy for making this task computationally tractable is
keyframe retrieval, which seeks to identify a sparse set of video frames that
are most salient to a given textual query. However, the efficacy of this
approach is hindered by weak multimodal alignment between textual queries and
visual content and fails to capture the complex temporal semantic information
required for precise reasoning. To address this, we propose Visual-Subtitle
Integeration(VSI), a multimodal keyframe search method that integrates
subtitles, timestamps, and scene boundaries into a unified multimodal search
process. The proposed method captures the visual information of video frames as
well as the complementary textual information through a dual-stream search
mechanism by Video Search Stream as well as Subtitle Match Stream,
respectively, and improves the keyframe search accuracy through the interaction
of the two search streams. Experimental results show that VSI achieve 40.00%
key frame localization accuracy on the text-relevant subset of LongVideoBench
and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive
baselines by 20.35% and 15.79%, respectively. Furthermore, on the
LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA
tasks, demonstrating the robustness and generalizability of the proposed
multimodal search strategy.

</details>


### [214] [NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective](https://arxiv.org/abs/2508.06878)
*Maoxun Yuan,Duanni Meng,Ziteng Xi,Tianyi Zhao,Shiji Zhao,Yimian Dai,Xingxing Wei*

Main category: cs.CV

TL;DR: This paper introduces NS-FPN, a lightweight network improving infrared small target detection by focusing on noise suppression using novel modules like LFP and SFS.


<details>
  <summary>Details</summary>
Motivation: To address the persistent challenges in infrared small target detection caused by severe background clutter and the limitations of existing CNN-based methods, which lead to increased false alarms.

Method: Proposed the NS-FPN, incorporating a Low-Frequency Guided Feature Purification (LFP) module for noise suppression and a Spiral-Aware Feature Sampling (SFS) module for improved feature fusion.

Result: Extensive experiments show superior performance of NS-FPN with a significant reduction in false alarms on public IRSTDS datasets.

Conclusion: NS-FPN provides an effective noise suppression approach, enhances target perception, and integrates seamlessly into existing IRSTDS workflows, enabling better performance.

Abstract: Infrared small target detection and segmentation (IRSTDS) is a critical yet
challenging task in defense and civilian applications, owing to the dim,
shapeless appearance of targets and severe background clutter. Recent CNN-based
methods have achieved promising target perception results, but they only focus
on enhancing feature representation to offset the impact of noise, which
results in the increased false alarms problem. In this paper, through analyzing
the problem from the frequency domain, we pioneer in improving performance from
noise suppression perspective and propose a novel noise-suppression feature
pyramid network (NS-FPN), which integrates a low-frequency guided feature
purification (LFP) module and a spiral-aware feature sampling (SFS) module into
the original FPN structure. The LFP module suppresses the noise features by
purifying high-frequency components to achieve feature enhancement devoid of
noise interference, while the SFS module further adopts spiral sampling to fuse
target-relevant features in feature fusion process. Our NS-FPN is designed to
be lightweight yet effective and can be easily plugged into existing IRSTDS
frameworks. Extensive experiments on the public IRSTDS datasets demonstrate
that our method significantly reduces false alarms and achieves superior
performance on IRSTDS tasks.

</details>


### [215] [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/abs/2508.06895)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: The paper introduces BASIC, a method aimed at enhancing multimodal large language models by improving the alignment between visual embeddings and language models through direct supervision without external annotations.


<details>
  <summary>Details</summary>
Motivation: Current multimodal language models use visual embeddings but do not implement direct visual supervision, which limits their alignment potential.

Method: BASIC leverages refined visual embeddings within large language models as supervision for the vision projector by optimizing embedding directions and improving semantic matching of visual embeddings.

Result: BASIC achieves significant performance improvements across various benchmarks without the need for additional supervisory models or annotations.

Conclusion: Direct visual supervision with refined embeddings can greatly enhance the alignment and performance of multimodal language models, as demonstrated by BASIC.

Abstract: Mainstream Multimodal Large Language Models (MLLMs) achieve visual
understanding by using a vision projector to bridge well-pretrained vision
encoders and large language models (LLMs). The inherent gap between visual and
textual modalities makes the embeddings from the vision projector critical for
visual comprehension. However, current alignment approaches treat visual
embeddings as contextual cues and merely apply auto-regressive supervision to
textual outputs, neglecting the necessity of introducing equivalent direct
visual supervision, which hinders the potential finer alignment of visual
embeddings. In this paper, based on our analysis of the refinement process of
visual embeddings in the LLM's shallow layers, we propose BASIC, a method that
utilizes refined visual embeddings within the LLM as supervision to directly
guide the projector in generating initial visual embeddings. Specifically, the
guidance is conducted from two perspectives: (i) optimizing embedding
directions by reducing angles between initial and supervisory embeddings in
semantic space; (ii) improving semantic matching by minimizing disparities
between the logit distributions of both visual embeddings. Without additional
supervisory models or artificial annotations, BASIC significantly improves the
performance of MLLMs across a wide range of benchmarks, demonstrating the
effectiveness of our introduced direct visual supervision.

</details>


### [216] [Advancements in Chinese font generation since deep learning era: A survey](https://arxiv.org/abs/2508.06900)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: The paper surveys recent advancements in Chinese font generation using deep learning, categorizing methods based on the number of reference samples while identifying challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of improving Chinese font generation quality to benefit font designers and typographers, leveraging deep learning advancements.

Method: A comprehensive survey method is employed to categorize many-shot and few-shot font generation techniques, reviewing deep learning architectures, datasets, evaluation metrics, and existing methods.

Result: The review categorizes methods, highlights their strengths and limitations, and identifies challenges in font generation.

Conclusion: Challenges still persist in improving generated font quality, and future research directions are discussed to aid advances in the field.

Abstract: Chinese font generation aims to create a new Chinese font library based on
some reference samples. It is a topic of great concern to many font designers
and typographers. Over the past years, with the rapid development of deep
learning algorithms, various new techniques have achieved flourishing and
thriving progress. Nevertheless, how to improve the overall quality of
generated Chinese character images remains a tough issue. In this paper, we
conduct a holistic survey of the recent Chinese font generation approaches
based on deep learning. To be specific, we first illustrate the research
background of the task. Then, we outline our literature selection and analysis
methodology, and review a series of related fundamentals, including classical
deep learning architectures, font representation formats, public datasets, and
frequently-used evaluation metrics. After that, relying on the number of
reference samples required to generate a new font, we categorize the existing
methods into two major groups: many-shot font generation and few-shot font
generation methods. Within each category, representative approaches are
summarized, and their strengths and limitations are also discussed in detail.
Finally, we conclude our paper with the challenges and future directions, with
the expectation to provide some valuable illuminations for the researchers in
this field.

</details>


### [217] [eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos](https://arxiv.org/abs/2508.06902)
*Xuecheng Wu,Dingkang Yang,Danlei Huang,Xinyi Yin,Yifan Wang,Jia Zhang,Jiayu Nie,Liangyu Fu,Yang Liu,Junxiao Xue,Hadi Amirpour,Wei Zhou*

Main category: cs.CV

TL;DR: The paper introduces "eMotions," a large-scale dataset of 27,996 short-form videos (SVs) for video emotion analysis (VEA), and proposes AV-CANet, an end-to-end audio-visual fusion network optimized with specialized loss functions, showing its effectiveness through extensive experiments.


<details>
  <summary>Details</summary>
Motivation: The increasing popularity of short-form videos (SVs) has created a demand for reliable video emotion analysis (VEA), yet current research lacks adequate datasets and methods to fully grasp the multimodal characteristics and emotional expressions within SVs.

Method: The researchers created a 27,996-video dataset, "eMotions," using a multi-stage annotation process to ensure quality. They proposed AV-CANet, an end-to-end audio-visual fusion network that employs a video transformer and Local-Global Fusion Module to analyze audio-visual correlations, combined with EP-CE Loss for improved optimization.

Result: Extensive experiments conducted on the "eMotions" dataset and other public VEA datasets showed the effectiveness of AV-CANet in tackling the semantic gaps and inconsistencies of emotional expressions in SVs.

Conclusion: AV-CANet, alongside the "eMotions" dataset, addresses significant challenges in VEA, providing a robust method and resource for analyzing multimodal emotional expressions in short-form videos. The work paves the way for future research in this domain.

Abstract: Short-form videos (SVs) have become a vital part of our online routine for
acquiring and sharing information. Their multimodal complexity poses new
challenges for video analysis, highlighting the need for video emotion analysis
(VEA) within the community. Given the limited availability of SVs emotion data,
we introduce eMotions, a large-scale dataset consisting of 27,996 videos with
full-scale annotations. To ensure quality and reduce subjective bias, we
emphasize better personnel allocation and propose a multi-stage annotation
procedure. Additionally, we provide the category-balanced and test-oriented
variants through targeted sampling to meet diverse needs. While there have been
significant studies on videos with clear emotional cues (e.g., facial
expressions), analyzing emotions in SVs remains a challenging task. The
challenge arises from the broader content diversity, which introduces more
distinct semantic gaps and complicates the representations learning of
emotion-related features. Furthermore, the prevalence of audio-visual
co-expressions in SVs leads to the local biases and collective information gaps
caused by the inconsistencies in emotional expressions. To tackle this, we
propose AV-CANet, an end-to-end audio-visual fusion network that leverages
video transformer to capture semantically relevant representations. We further
introduce the Local-Global Fusion Module designed to progressively capture the
correlations of audio-visual features. Besides, EP-CE Loss is constructed to
globally steer optimizations with tripolar penalties. Extensive experiments
across three eMotions-related datasets and four public VEA datasets demonstrate
the effectiveness of our proposed AV-CANet, while providing broad insights for
future research. Moreover, we conduct ablation studies to examine the critical
components of our method. Dataset and code will be made available at Github.

</details>


### [218] [A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2508.06904)
*Chao Yin,Jide Li,Xiaoqiang Li*

Main category: cs.CV

TL;DR: The paper introduces a training-free framework for camouflaged object segmentation (COS) that improves instance-level mask prediction using an instance-aware approach.


<details>
  <summary>Details</summary>
Motivation: Existing training-based COS methods struggle with annotation sparsity, and recent training-free methods produce coarse semantic masks, often failing to detect multiple discrete camouflaged instances. This paper aims to overcome these limitations.

Method: The proposed Instance-Aware Prompting Framework (IAPF) includes three steps: (1) generating image-specific tags via a multimodal large language model, (2) utilizing bounding box prompts with a novel sampling strategy for detailed instance masks, and (3) a voting mechanism to finalize predictions based on consistency.

Result: The IAPF demonstrates superior performance over previous state-of-the-art training-free COS methods in standard benchmarks, handling multiple discrete instances effectively.

Conclusion: The framework proves effective in improving COS without the need for training, addressing both sparsity and instance-level segmentation challenges.

Abstract: Camouflaged Object Segmentation (COS) remains highly challenging due to the
intrinsic visual similarity between target objects and their surroundings.
While training-based COS methods achieve good performance, their performance
degrades rapidly with increased annotation sparsity. To circumvent this
limitation, recent studies have explored training-free COS methods, leveraging
the Segment Anything Model (SAM) by automatically generating visual prompts
from a single task-generic prompt (\textit{e.g.}, "\textit{camouflaged
animal}") uniformly applied across all test images. However, these methods
typically produce only semantic-level visual prompts, causing SAM to output
coarse semantic masks and thus failing to handle scenarios with multiple
discrete camouflaged instances effectively. To address this critical
limitation, we propose a simple yet powerful \textbf{I}nstance-\textbf{A}ware
\textbf{P}rompting \textbf{F}ramework (IAPF), the first training-free COS
pipeline that explicitly converts a task-generic prompt into fine-grained
instance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt
Generator, utilizing task-generic queries to prompt a Multimodal Large Language
Model (MLLM) for generating image-specific foreground and background tags; (2)
\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise
instance-level bounding box prompts, alongside the proposed Single-Foreground
Multi-Background Prompting strategy to sample region-constrained point prompts
within each box, enabling SAM to yield a candidate instance mask; (3)
Self-consistency Instance Mask Voting, which selects the final COS prediction
by identifying the candidate mask most consistent across multiple candidate
instance masks. Extensive evaluations on standard COS benchmarks demonstrate
that the proposed IAPF significantly surpasses existing state-of-the-art
training-free COS methods.

</details>


### [219] [MultiRef: Controllable Image Generation with Multiple Visual References](https://arxiv.org/abs/2508.06905)
*Ruoxi Chen,Dongping Chen,Siyuan Wu,Sinan Wang,Shiyun Lang,Petr Sushko,Gaoyang Jiang,Yao Wan,Ranjay Krishna*

Main category: cs.CV

TL;DR: This paper focuses on controllable image generation using multiple visual references, introduces a benchmark (MultiRef-bench), a high-quality dataset (MultiRef), and evaluates current models, revealing their limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation in existing image generation frameworks, which predominantly rely on single-source inputs and lack the capability to integrate multiple visual references.

Method: The authors introduce two main contributions: MultiRef-bench, an evaluation framework with synthetic and real-world samples, and MultiRef, a dataset of 38k images. They evaluate existing image-text models and frameworks using these tools to identify limitations in handling multiple references.

Result: The experiments show that even state-of-the-art systems like OmniGen struggle with multi-reference image conditioning, achieving only 66.6% and 79.0% accuracy in synthetic and real-world samples, respectively, compared to the ideal results.

Conclusion: The study highlights the need for developing new tools and models that are better equipped to integrate multiple visual inspirations, making progress toward more human-like creative AI systems.

Abstract: Visual designers naturally draw inspiration from multiple visual references,
combining diverse elements and aesthetic principles to create artwork. However,
current image generative frameworks predominantly rely on single-source inputs
-- either text prompts or individual reference images. In this paper, we focus
on the task of controllable image generation using multiple visual references.
We introduce MultiRef-bench, a rigorous evaluation framework comprising 990
synthetic and 1,000 real-world samples that require incorporating visual
content from multiple reference images. The synthetic samples are synthetically
generated through our data engine RefBlend, with 10 reference types and 33
reference combinations. Based on RefBlend, we further construct a dataset
MultiRef containing 38k high-quality images to facilitate further research. Our
experiments across three interleaved image-text models (i.e., OmniGen, ACE, and
Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that
even state-of-the-art systems struggle with multi-reference conditioning, with
the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in
real-world cases on average compared to the golden answer. These findings
provide valuable directions for developing more flexible and human-like
creative tools that can effectively integrate multiple sources of visual
inspiration. The dataset is publicly available at: https://multiref.github.io/.

</details>


### [220] [MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification](https://arxiv.org/abs/2508.06908)
*Jinhao Li,Zijian Chen,Lirong Deng,Changbo Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: Traditional person re-identification models struggle with handling multimodal data, while multi-modal large language models (MLLMs) show promise but have yet to be fully utilized in this domain. This paper introduces MMReID-Bench to benchmark multi-task, multi-modal capabilities in ReID.


<details>
  <summary>Details</summary>
Motivation: Person re-identification models have limited capability in addressing multimodal data. The study seeks to leverage the reasoning and cross-modal understanding strengths of MLLMs to improve ReID performance.

Method: The paper introduces MMReID-Bench, comprising 20,710 queries and gallery images across 10 tasks, for testing the capabilities of MLLMs in person ReID.

Result: MLLMs demonstrate strong performance in multimodal person ReID tasks but struggle with modalities like thermal and infrared data.

Conclusion: MMReID-Bench provides a framework to assess and advance versatile, robust multi-modal foundation models for improving person ReID capabilities.

Abstract: Person re-identification (ReID) aims to retrieve the images of an interested
person in the gallery images, with wide applications in medical rehabilitation,
abnormal behavior detection, and public security. However, traditional person
ReID models suffer from uni-modal capability, leading to poor generalization
ability in multi-modal data, such as RGB, thermal, infrared, sketch images,
textual descriptions, etc. Recently, the emergence of multi-modal large
language models (MLLMs) shows a promising avenue for addressing this problem.
Despite this potential, existing methods merely regard MLLMs as feature
extractors or caption generators, which do not fully unleash their reasoning,
instruction-following, and cross-modal understanding capabilities. To bridge
this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark
specifically designed for person ReID. The MMReID-Bench includes 20,710
multi-modal queries and gallery images covering 10 different person ReID tasks.
Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in
delivering effective and versatile person ReID. Nevertheless, they also have
limitations in handling a few modalities, particularly thermal and infrared
data. We hope MMReID-Bench can facilitate the community to develop more robust
and generalizable multimodal foundation models for person ReID.

</details>


### [221] [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](https://arxiv.org/abs/2508.06916)
*Shichao Ma,Yunhe Guo,Jiahao Su,Qihe Huang,Zhengyang Zhou,Yang Wang*

Main category: cs.CV

TL;DR: Talk2Image is a multi-agent system designed for interactive, multi-turn text-to-image generation, utilizing intention parsing, task decomposition, and feedback refinement for better controllability and coherence.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of single-turn text-to-image systems, which fail at iterative tasks, and overcoming issues with intention drift and incoherent edits in current dialogue-based systems.

Method: Combines intention parsing from dialogue history, task decomposition across specialized agents, and feedback-driven refinement via multi-view evaluation to enable aligned and consistent image editing.

Result: Talk2Image outperforms baselines in user satisfaction, controllability, and coherence during iterative image generation and editing.

Conclusion: Talk2Image demonstrates the potential for multi-agent systems in multi-turn dialogue scenarios, resolving previous challenges in intention alignment and edit coherence.

Abstract: Text-to-image generation tasks have driven remarkable advances in diverse
media applications, yet most focus on single-turn scenarios and struggle with
iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to
bridge this gap, but their single-agent, sequential paradigm often causes
intention drift and incoherent edits. To address these limitations, we present
Talk2Image, a novel multi-agent system for interactive image generation and
editing in multi-turn dialogue scenarios. Our approach integrates three key
components: intention parsing from dialogue history, task decomposition and
collaborative execution across specialized agents, and feedback-driven
refinement based on a multi-view evaluation mechanism. Talk2Image enables
step-by-step alignment with user intention and consistent image editing.
Experiments demonstrate that Talk2Image outperforms existing baselines in
controllability, coherence, and user satisfaction across iterative image
generation and editing tasks.

</details>


### [222] [AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](https://arxiv.org/abs/2508.06924)
*Shihao Yuan,Yahui Liu,Yang Yue,Jingyuan Zhang,Wangmeng Zuo,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: AR-GRPO integrates reinforcement learning into autoregressive image generation models, improving quality and realism through reward functions.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality and controllability of images generated by autoregressive models by leveraging RL techniques, inspired by RL's success in language models.

Method: Adapts the Group Relative Policy Optimization (GRPO) algorithm to autoregressive models, employing reward functions to evaluate dimensions like perceptual quality and realism.

Result: Significant improvement in image quality, realism, and human preferences compared to baseline autoregressive models, across both class-conditional and text-conditional tasks.

Conclusion: RL-based optimization proves effective for controllable, high-quality image synthesis, marking progress in the field with open-source codes provided.

Abstract: Inspired by the success of reinforcement learning (RL) in refining large
language models (LLMs), we propose AR-GRPO, an approach to integrate online RL
training into autoregressive (AR) image generation models. We adapt the Group
Relative Policy Optimization (GRPO) algorithm to refine the vanilla
autoregressive models' outputs by carefully designed reward functions that
evaluate generated images across multiple quality dimensions, including
perceptual quality, realism, and semantic fidelity. We conduct comprehensive
experiments on both class-conditional (i.e., class-to-image) and
text-conditional (i.e., text-to-image) image generation tasks, demonstrating
that our RL-enhanced framework significantly improves both the image quality
and human preference of generated images compared to the standard AR baselines.
Our results show consistent improvements across various evaluation metrics,
establishing the viability of RL-based optimization for AR image generation and
opening new avenues for controllable and high-quality image synthesis. The
source codes and models are available at:
https://github.com/Kwai-Klear/AR-GRPO.

</details>


### [223] [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](https://arxiv.org/abs/2508.06937)
*Weiyan Xie,Han Gao,Didan Deng,Kaican Li,April Hua Liu,Yongxiang Huang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: CannyEdit offers a training-free framework for improved text-to-image regional editing with enhanced text adherence, context fidelity, and edit seamlessness.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle to balance text adherence, context fidelity, and seamless integration in regional image editing.

Method: CannyEdit uses Selective Canny Control for editing precision and Dual-Prompt Guidance for coherent scene interactions while leveraging generative priors without training.

Result: CannyEdit outperforms competitors in adherence-context balance by 2.93-10.49%, and user studies show its edits are less frequently identified as AI-edited compared to other methods.

Conclusion: CannyEdit provides a significant improvement in text-driven regional editing, maintaining source image integrity and creating minimally detectable edits.

Abstract: Recent advances in text-to-image (T2I) models have enabled training-free
regional image editing by leveraging the generative priors of foundation
models. However, existing methods struggle to balance text adherence in edited
regions, context fidelity in unedited areas, and seamless integration of edits.
We introduce CannyEdit, a novel training-free framework that addresses these
challenges through two key innovations: (1) Selective Canny Control, which
masks the structural guidance of Canny ControlNet in user-specified editable
regions while strictly preserving details of the source images in unedited
areas via inversion-phase ControlNet information retention. This enables
precise, text-driven edits without compromising contextual integrity. (2)
Dual-Prompt Guidance, which combines local prompts for object-specific edits
with a global target prompt to maintain coherent scene interactions. On
real-world image editing tasks (addition, replacement, removal), CannyEdit
outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent
improvement in the balance of text adherence and context fidelity. In terms of
editing seamlessness, user studies reveal only 49.2 percent of general users
and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited
when paired with real images without edits, versus 76.08 to 89.09 percent for
competitor methods.

</details>


### [224] [SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work](https://arxiv.org/abs/2508.06951)
*Harry Walsh,Ed Fish,Ozge Mercanoglu Sincan,Mohamed Ilyes Lakhal,Richard Bowden,Neil Fox,Bencie Woll,Kepeng Wu,Zecheng Li,Weichao Zhao,Haodong Wang,Wengang Zhou,Houqiang Li,Shengeng Tang,Jiayi He,Xu Wang,Ruobei Zhang,Yaxiong Wang,Lechao Cheng,Meryem Tasyurek,Tugce Kiziltepe,Hacer Yalim Keles*

Main category: cs.CV

TL;DR: The paper introduces the first Sign Language Production Challenge to standardize evaluation metrics for generating sign language videos, attracting noteworthy participation and presenting top methodologies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized evaluation metrics in the Sign Language Production (SLP) field, which hinders meaningful comparisons between systems.

Method: The researchers organized a competition focused on Text-to-Pose (T2P) translation using the RWTH-PHOENIX-Weather-2014T dataset and a hidden test set, developing and evaluating systems based on diverse metrics.

Result: The challenge attracted 33 participants who submitted 231 solutions, with the leading team achieving a BLEU-1 score of 31.40 and DTW-MJE of 0.0574 using retrieval-based frameworks and pre-trained language models.

Conclusion: The effort introduces a standardized evaluation network for SLP, establishes a baseline for future comparison, and showcases advancements in Text-to-Pose translation methodologies.

Abstract: Sign Language Production (SLP) is the task of generating sign language video
from spoken language inputs. The field has seen a range of innovations over the
last few years, with the introduction of deep learning-based approaches
providing significant improvements in the realism and naturalness of generated
outputs. However, the lack of standardized evaluation metrics for SLP
approaches hampers meaningful comparisons across different systems. To address
this, we introduce the first Sign Language Production Challenge, held as part
of the third SLRTP Workshop at CVPR 2025. The competition's aims are to
evaluate architectures that translate from spoken language sentences to a
sequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a
range of metrics. For our evaluation data, we use the
RWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche
Gebardensprache (DGS) weather broadcast dataset. In addition, we curate a
custom hidden test set from a similar domain of discourse. This paper presents
the challenge design and the winning methodologies. The challenge attracted 33
participants who submitted 231 solutions, with the top-performing team
achieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach
utilized a retrieval-based framework and a pre-trained language model. As part
of the workshop, we release a standardized evaluation network, including
high-quality skeleton extraction-based keypoints establishing a consistent
baseline for the SLP field, which will enable future researchers to compare
their work against a broader range of methods.

</details>


### [225] [Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification](https://arxiv.org/abs/2508.06959)
*Qin Xu,Lili Zhu,Xiaoxia Cheng,Bo Jiang*

Main category: cs.CV

TL;DR: This paper introduces Subtle-Cue Oriented Perception Engine (SCOPE), a novel method for improving fine-grained visual classification (FGVC) by dynamically enhancing image feature extraction.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of fixed basis functions in frequency-domain methods, which lack adaptability to image content and discriminate subtleties in FGVC.

Method: SCOPE combines two key modules: Subtle Detail Extractor (enhances edges and textures dynamically) and Salient Semantic Refiner (refines high-level features guided by enhanced shallow features).

Result: The proposed SCOPE method sets new state-of-the-art performance on four widely-used FGVC benchmark datasets.

Conclusion: SCOPE enhances FGVC with a flexible approach, effectively combining low- and high-level features to capture discriminative cues, breaking traditional fixed limitations of frequency-domain methods.

Abstract: The crux of resolving fine-grained visual classification (FGVC) lies in
capturing discriminative and class-specific cues that correspond to subtle
visual characteristics. Recently, frequency decomposition/transform based
approaches have attracted considerable interests since its appearing
discriminative cue mining ability. However, the frequency-domain methods are
based on fixed basis functions, lacking adaptability to image content and
unable to dynamically adjust feature extraction according to the discriminative
requirements of different images. To address this, we propose a novel method
for FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively
enhances the representational capability of low-level details and high-level
semantics in the spatial domain, breaking through the limitations of fixed
scales in the frequency domain and improving the flexibility of multi-scale
fusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor
(SDE), which dynamically enhances subtle details such as edges and textures
from shallow features, and the Salient Semantic Refiner (SSR), which learns
semantically coherent and structure-aware refinement features from the
high-level features guided by the enhanced shallow features. The SDE and SSR
are cascaded stage-by-stage to progressively combine local details with global
semantics. Extensive experiments demonstrate that our method achieves new
state-of-the-art on four popular fine-grained image classification benchmarks.

</details>


### [226] [Adversarial Video Promotion Against Text-to-Video Retrieval](https://arxiv.org/abs/2508.06964)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Qian Li,Shuai Liu,Chao Shen*

Main category: cs.CV

TL;DR: This paper introduces ViPro, a novel attack strategy for text-to-video retrieval systems, focusing on promoting specific videos through adversarial means and outperforming existing approaches in various experimental scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unexamined robustness of text-to-video retrieval systems against promotion-focused attacks, which could lead to financial or misinformation-related exploitation.

Method: The authors propose the Video Promotion attack (ViPro) and Modal Refinement (MoRe) to manipulate video rankings, leveraging finer-grained interactions between modalities to improve attack transferability in both white, grey, and black-box settings.

Result: ViPro outperforms two baselines across three robust T2VR models and datasets, achieving over 30/10/4% improvements in white/grey/black-box scenarios, while remaining applicable in multi-target attack settings.

Conclusion: ViPro reveals a critical vulnerability in text-to-video retrieval models, providing qualitative insights, upper/lower boundary analysis, and suggestions for countermeasures against such adversarial attacks.

Abstract: Thanks to the development of cross-modal models, text-to-video retrieval
(T2VR) is advancing rapidly, but its robustness remains largely unexamined.
Existing attacks against T2VR are designed to push videos away from queries,
i.e., suppressing the ranks of videos, while the attacks that pull videos
towards selected queries, i.e., promoting the ranks of videos, remain largely
unexplored. These attacks can be more impactful as attackers may gain more
views/clicks for financial benefits and widespread (mis)information. To this
end, we pioneer the first attack against T2VR to promote videos adversarially,
dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement
(MoRe) to capture the finer-grained, intricate interaction between visual and
textual modalities to enhance black-box transferability. Comprehensive
experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing
datasets with over 10k videos, evaluated under 3 scenarios. All experiments are
conducted in a multi-target setting to reflect realistic scenarios where
attackers seek to promote the video regarding multiple queries simultaneously.
We also evaluated our attacks for defences and imperceptibility. Overall, ViPro
surpasses other baselines by over $30/10/4\%$ for white/grey/black-box settings
on average. Our work highlights an overlooked vulnerability, provides a
qualitative analysis on the upper/lower bound of our attacks, and offers
insights into potential counterplays. Code will be publicly available at
https://github.com/michaeltian108/ViPro.

</details>


### [227] [Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View](https://arxiv.org/abs/2508.06968)
*Ulas Gunes,Matias Turkulainen,Juho Kannala,Esa Rahtu*

Main category: cs.CV

TL;DR: The paper evaluates fisheye-based 3D Gaussian Splatting methods on real 200-degree fisheye images, examining their performance and proposing a depth-based alternative to SfM initialization.


<details>
  <summary>Details</summary>
Motivation: To explore the practicality and limitations of fisheye-based 3D reconstruction methods in handling extreme distortion from wide-angle fields of view.

Method: The authors compared two fisheye-based 3D Gaussian Splatting methods, Fisheye-GS and 3DGUT, on images with varying field-of-view ranges. They also introduced a depth-based initialization approach using UniK3D predictions instead of SfM.

Result: Fisheye-GS performs better at reduced fields of view (e.g., 160 degrees) while 3DGUT remains stable and delivers high perceptual quality even at 200 degrees. The proposed depth-based strategy generates high-quality point clouds comparable to SfM, even in challenging scenarios.

Conclusion: Fisheye-based 3D Gaussian Splatting methods show strong potential for wide-angle 3D reconstructions, especially when paired with the proposed depth-based initialization.

Abstract: We present the first evaluation of fisheye-based 3D Gaussian Splatting
methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180
degree. Our study covers both indoor and outdoor scenes captured with 200
degree fisheye cameras and analyzes how each method handles extreme distortion
in real world settings. We evaluate performance under varying fields of view
(200 degree, 160 degree, and 120 degree) to study the tradeoff between
peripheral distortion and spatial coverage. Fisheye-GS benefits from field of
view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable
across all settings and maintains high perceptual quality at the full 200
degree view. To address the limitations of SfM-based initialization, which
often fails under strong distortion, we also propose a depth-based strategy
using UniK3D predictions from only 2-3 fisheye images per scene. Although
UniK3D is not trained on real fisheye data, it produces dense point clouds that
enable reconstruction quality on par with SfM, even in difficult scenes with
fog, glare, or sky. Our results highlight the practical viability of
fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and
distortion-heavy image inputs.

</details>


### [228] [WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering](https://arxiv.org/abs/2508.06982)
*Yixin Zhu,Zuoliang Zhu,Miloš Hašan,Jian Yang,Jin Xie,Beibei Wang*

Main category: cs.CV

TL;DR: This paper presents WeatherDiffusion, a diffusion model-based framework for rendering autonomous driving (AD) scenes in diverse weather and lighting conditions, overcoming challenges in controllability and robustness seen in existing models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by complex weather and illumination in forward and inverse rendering for autonomous driving scenes while benefiting from advancements in diffusion models.

Method: The proposed method, WeatherDiffusion, uses diffusion-based modeling to predict intrinsic maps for material properties, geometry, and lighting. It introduces Intrinsic map-aware attention (MAA) for inverse rendering and relies on synthetic (WeatherSynthetic) and real-world (WeatherReal) datasets for evaluation.

Result: WeatherDiffusion surpasses state-of-the-art methods on key benchmarks and improves downstream autonomous driving tasks such as object detection and segmentation under adverse weather and lighting.

Conclusion: WeatherDiffusion demonstrates the potential of diffusion frameworks in achieving high-quality rendering for AD scenes and proves to be a valuable contribution to handling challenging weather and illumination conditions.

Abstract: Forward and inverse rendering have emerged as key techniques for enabling
understanding and reconstruction in the context of autonomous driving (AD).
However, complex weather and illumination pose great challenges to this task.
The emergence of large diffusion models has shown promise in achieving
reasonable results through learning from 2D priors, but these models are
difficult to control and lack robustness. In this paper, we introduce
WeatherDiffusion, a diffusion-based framework for forward and inverse rendering
on AD scenes with various weather and lighting conditions. Our method enables
authentic estimation of material properties, scene geometry, and lighting, and
further supports controllable weather and illumination editing through the use
of predicted intrinsic maps guided by text descriptions. We observe that
different intrinsic maps should correspond to different regions of the original
image. Based on this observation, we propose Intrinsic map-aware attention
(MAA) to enable high-quality inverse rendering. Additionally, we introduce a
synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie
WeatherReal) for forward and inverse rendering on AD scenes with diverse
weather and lighting. Extensive experiments show that our WeatherDiffusion
outperforms state-of-the-art methods on several benchmarks. Moreover, our
method demonstrates significant value in downstream tasks for AD, enhancing the
robustness of object detection and image segmentation in challenging weather
scenarios.

</details>


### [229] [TADoc: Robust Time-Aware Document Image Dewarping](https://arxiv.org/abs/2508.06988)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Yu Zhou*

Main category: cs.CV

TL;DR: This paper introduces a novel framework, TADoc, and a new metric, DLS, for document image dewarping to effectively handle distorted documents.


<details>
  <summary>Details</summary>
Motivation: Digital transformation and online workflows demand effective flattening of distorted document images, yet existing methods falter on complex structures and severe deformations.

Method: The task is reformulated as a dynamic multi-step process, and a lightweight framework called TADoc is introduced for progressive geometric corrections.

Result: TADoc demonstrates high robustness and outperforms on benchmarks with diverse document types and severe distortions.

Conclusion: The approach successfully redefines document dewarping as progressive steps and establishes the new DLS metric for better evaluations in downstream applications.

Abstract: Flattening curved, wrinkled, and rotated document images captured by portable
photographing devices, termed document image dewarping, has become an
increasingly important task with the rise of digital economy and online
working. Although many methods have been proposed recently, they often struggle
to achieve satisfactory results when confronted with intricate document
structures and higher degrees of deformation in real-world scenarios. Our main
insight is that, unlike other document restoration tasks (e.g., deblurring),
dewarping in real physical scenes is a progressive motion rather than a
one-step transformation. Based on this, we have undertaken two key initiatives.
Firstly, we reformulate this task, modeling it for the first time as a dynamic
process that encompasses a series of intermediate states. Secondly, we design a
lightweight framework called TADoc (Time-Aware Document Dewarping Network) to
address the geometric distortion of document images. In addition, due to the
inadequacy of OCR metrics for document images containing sparse text, the
comprehensiveness of evaluation is insufficient. To address this shortcoming,
we propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the
effectiveness of document dewarping in downstream tasks. Extensive experiments
and in-depth evaluations have been conducted and the results indicate that our
model possesses strong robustness, achieving superiority on several benchmarks
with different document types and degrees of distortion.

</details>


### [230] [OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware](https://arxiv.org/abs/2508.06993)
*Nick Lemke,John Kalkhof,Niklas Babendererde,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: The paper introduces OctreeNCA, a Neural Cellular Automaton model enhanced with octree data structures to efficiently segment large medical data inputs while drastically reducing VRAM consumption compared to conventional models like UNet.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the VRAM bottleneck in segmenting large medical datasets, such as prostate MRIs and surgical videos, while maintaining global context and inference speed, which are essential for medical applications.

Method: The authors propose OctreeNCA by extending Neural Cellular Automata (NCAs) with octree-based neighborhood definitions to achieve efficient global knowledge traversal. Additionally, a CUDA-implemented NCA inference function is designed to optimize VRAM usage and speed.

Result: OctreeNCA significantly reduces VRAM demand by 90% compared to UNet while maintaining efficient, fast segmentation for high-resolution medical data such as 184 Megapixel pathology slices or 1-minute surgical videos.

Conclusion: OctreeNCA demonstrates viability for segmenting large medical datasets, balancing size-invariance, reduced VRAM usage, global context preservation, and improved inference speed, making it a promising alternative to conventional segmentation models.

Abstract: Medical applications demand segmentation of large inputs, like prostate MRIs,
pathology slices, or videos of surgery. These inputs should ideally be inferred
at once to provide the model with proper spatial or temporal context. When
segmenting large inputs, the VRAM consumption of the GPU becomes the
bottleneck. Architectures like UNets or Vision Transformers scale very poorly
in VRAM consumption, resulting in patch- or frame-wise approaches that
compromise global consistency and inference speed. The lightweight Neural
Cellular Automaton (NCA) is a bio-inspired model that is by construction
size-invariant. However, due to its local-only communication rules, it lacks
global knowledge. We propose OctreeNCA by generalizing the neighborhood
definition using an octree data structure. Our generalized neighborhood
definition enables the efficient traversal of global knowledge. Since deep
learning frameworks are mainly developed for large multi-layer networks, their
implementation does not fully leverage the advantages of NCAs. We implement an
NCA inference function in CUDA that further reduces VRAM demands and increases
inference speed. Our OctreeNCA segments high-resolution images and videos
quickly while occupying 90% less VRAM than a UNet during evaluation. This
allows us to segment 184 Megapixel pathology slices or 1-minute surgical videos
at once.

</details>


### [231] [S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision](https://arxiv.org/abs/2508.06995)
*Huihui Xu,Jin Ye,Hongqiu Wang,Changkai Ji,Jiashi Lin,Ming Hu,Ziyan Huang,Ying Chen,Chenglong Ma,Tianbin Li,Lihao Liu,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: The paper introduces S2-UniSeg with a new pseudo-mask algorithm, UniAP, to address inefficiencies in self-supervised image segmentation.


<details>
  <summary>Details</summary>
Motivation: Address multi-stage and time-consuming pretraining processes in self-supervised image segmentation models.

Method: Proposal of UniAP for quick pseudo-mask generation and QuerySD pretext task within the S2-UniSeg framework.

Result: S2-UniSeg demonstrates superior performance over SOTA benchmarks like UnSAM across four datasets, including COCO and Cityscapes.

Conclusion: S2-UniSeg effectively scales up self-supervised segmentation while offering notable performance improvements across benchmarks.

Abstract: Recent self-supervised image segmentation models have achieved promising
performance on semantic segmentation and class-agnostic instance segmentation.
However, their pretraining schedule is multi-stage, requiring a time-consuming
pseudo-masks generation process between each training epoch. This
time-consuming offline process not only makes it difficult to scale with
training dataset size, but also leads to sub-optimal solutions due to its
discontinuous optimization routine. To solve these, we first present a novel
pseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer
of UniAP can identify groups of similar nodes in parallel, allowing to generate
both semantic-level and instance-level and multi-granular pseudo-masks within
ens of milliseconds for one image. Based on the fast UniAP, we propose the
Scalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a
student and a momentum teacher for continuous pretraining. A novel
segmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is
proposed to pretrain S2-UniSeg to learn the local-to-global correspondences.
Under the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving
notable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on
COCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image
subset of SA-1B, S2-UniSeg further achieves performance gains on all four
benchmarks. Our code and pretrained models are available at
https://github.com/bio-mlhui/S2-UniSeg

</details>


### [232] [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/abs/2508.07089)
*Sandro Papais,Letian Wang,Brian Cheong,Steven L. Waslander*

Main category: cs.CV

TL;DR: ForeSight is a new framework combining detection and forecasting for autonomous vehicle 3D vision, achieving state-of-the-art results on nuScenes dataset.


<details>
  <summary>Details</summary>
Motivation: Improve the accuracy and efficiency of vision-based perception in autonomous vehicles by addressing limitations of separate detection and forecasting processes.

Method: Proposes a joint detection and forecasting framework using bidirectional learning, shared memory, and transformers to integrate trajectory predictions and temporal consistency.

Result: ForeSight achieves state-of-the-art performance on the nuScenes dataset with an EPA of 54.9%, surpassing previous methods by 9.3%, and achieving leading mAP and minADE metrics.

Conclusion: ForeSight eliminates the need for tracking, enhances detection and forecasting through temporal integration, and scales efficiently across multi-frame sequences.

Abstract: We introduce ForeSight, a novel joint detection and forecasting framework for
vision-based 3D perception in autonomous vehicles. Traditional approaches treat
detection and forecasting as separate sequential tasks, limiting their ability
to leverage temporal cues. ForeSight addresses this limitation with a
multi-task streaming and bidirectional learning approach, allowing detection
and forecasting to share query memory and propagate information seamlessly. The
forecast-aware detection transformer enhances spatial reasoning by integrating
trajectory predictions from a multiple hypothesis forecast memory queue, while
the streaming forecast transformer improves temporal consistency using past
forecasts and refined detections. Unlike tracking-based methods, ForeSight
eliminates the need for explicit object association, reducing error propagation
with a tracking-free model that efficiently scales across multi-frame
sequences. Experiments on the nuScenes dataset show that ForeSight achieves
state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous
methods by 9.3%, while also attaining the best mAP and minADE among multi-view
detection and forecasting models.

</details>


### [233] [HiMat: DiT-based Ultra-High Resolution SVBRDF Generation](https://arxiv.org/abs/2508.07011)
*Zixiong Wang,Jian Yang,Yiwei Hu,Milos Hasan,Beibei Wang*

Main category: cs.CV

TL;DR: This paper introduces HiMat, a memory- and computation-efficient diffusion framework for generating high-resolution (4K) SVBRDFs, using a lightweight module called CrossStitch to ensure consistency across maps.


<details>
  <summary>Details</summary>
Motivation: The need for generating high-quality, detailed SVBRDFs for 3D content creation intersects with the development of high-resolution text-to-image generative models like diffusion transformers.

Method: HiMat utilizes a CrossStitch module to ensure inter-map coherence, leveraging convolutional operations. This approach avoids modifying the DiT backbone or retraining VAEs, providing high efficiency and consistency.

Result: The method demonstrates strong structural coherence, high-frequency detail preservation, and effectiveness in 4K SVBRDF generation, as validated by experiments with diverse text prompts.

Conclusion: HiMat presents a significant advancement for creating SVBRDF maps by maintaining efficiency, consistency, and leveraging existing diffusion model capabilities, with potential applications beyond this domain.

Abstract: Creating highly detailed SVBRDFs is essential for 3D content creation. The
rise of high-resolution text-to-image generative models, based on diffusion
transformers (DiT), suggests an opportunity to finetune them for this task.
However, retargeting the models to produce multiple aligned SVBRDF maps instead
of just RGB images, while achieving high efficiency and ensuring consistency
across different maps, remains a challenge. In this paper, we introduce HiMat:
a memory- and computation-efficient diffusion-based framework capable of
generating native 4K-resolution SVBRDFs. A key challenge we address is
maintaining consistency across different maps in a lightweight manner, without
relying on training new VAEs or significantly altering the DiT backbone (which
would damage its prior capabilities). To tackle this, we introduce the
CrossStitch module, a lightweight convolutional module that captures inter-map
dependencies through localized operations. Its weights are initialized such
that the DiT backbone operation is unchanged before finetuning starts. HiMat
enables generation with strong structural coherence and high-frequency details.
Results with a large set of text prompts demonstrate the effectiveness of our
approach for 4K SVBRDF generation. Further experiments suggest generalization
to tasks such as intrinsic decomposition.

</details>


### [234] [TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders](https://arxiv.org/abs/2508.07020)
*Tanjim Bin Faruk,Abdul Matin,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: The paper introduces TerraMAE, a novel framework for hyperspectral image encoding, outperforming existing methods in geospatial tasks like crop identification, land cover classification, and soil texture prediction.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised Masked Autoencoders struggle to leverage the spatial-spectral correlations in hyperspectral images, limiting their application for detailed geospatial analysis.

Method: TerraMAE utilizes an adaptive channel grouping strategy informed by spectral reflectance properties and an enhanced reconstruction loss function incorporating spatial and spectral quality metrics to learn representative spatial-spectral embeddings from hyperspectral datasets.

Result: TerraMAE achieved superior performance in information preservation and proved effective in downstream geospatial tasks such as crop identification, land cover classification, and soil texture prediction.

Conclusion: The TerraMAE framework effectively addresses the limitations of prior methods by learning highly informative spatial-spectral embeddings for hyperspectral imagery, thereby improving performance and utility in diverse geospatial applications.

Abstract: Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of
contiguous spectral bands, enabling fine-grained mapping of soils, crops, and
land cover. While self-supervised Masked Autoencoders excel on RGB and low-band
multispectral data, they struggle to exploit the intricate spatial-spectral
correlations in 200+ band hyperspectral images. We introduce TerraMAE, a novel
HSI encoding framework specifically designed to learn highly representative
spatial-spectral embeddings for diverse geospatial analyses. TerraMAE features
an adaptive channel grouping strategy, based on statistical reflectance
properties to capture spectral similarities, and an enhanced reconstruction
loss function that incorporates spatial and spectral quality metrics. We
demonstrate TerraMAE's effectiveness through superior spatial-spectral
information preservation in high-fidelity image reconstruction. Furthermore, we
validate its practical utility and the quality of its learned representations
through strong performance on three key downstream geospatial tasks: crop
identification, land cover classification, and soil texture prediction.

</details>


### [235] [AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning](https://arxiv.org/abs/2508.07626)
*Dejie Yang,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: The paper introduces AR-VRM, a method leveraging human action keypoints for explicit analogical reasoning to enhance visual robot manipulation under limited data scenarios.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of enabling robots to follow natural language instructions based on robot states and visual data, while overcoming the limitations of insufficient multi-modal robot data for training.

Method: The authors propose AR-VRM, a method that pretrains Vision-Language Models using human action video datasets. This involves learning from human hand keypoints and devising an Analogical Reasoning map to link human hand actions to robot components.

Result: AR-VRM demonstrates leading performance on the CALVIN benchmark and in real-world experiments. It shows notable improvements over existing methods in few-shot scenarios, highlighting its capability under data scarcity.

Conclusion: Focusing on action keypoints and explicitly imitating human actions enhances the robot's ability to generalize tasks, providing a more effective solution for visual robot manipulation with limited robotic data.

Abstract: Visual Robot Manipulation (VRM) aims to enable a robot to follow natural
language instructions based on robot states and visual observations, and
therefore requires costly multi-modal data. To compensate for the deficiency of
robot data, existing approaches have employed vision-language pretraining with
large-scale data. However, they either utilize web data that differs from
robotic tasks, or train the model in an implicit way (e.g., predicting future
frames at the pixel level), thus showing limited generalization ability under
insufficient robot data. In this paper, we propose to learn from large-scale
human action video datasets in an explicit way (i.e., imitating human actions
from hand keypoints), introducing Visual Robot Manipulation with Analogical
Reasoning (AR-VRM). To acquire action knowledge explicitly from human action
videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,
enabling the VLM to learn human action knowledge and directly predict human
hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm
in imitating the action patterns of human motions, we first retrieve human
action videos that perform similar manipulation tasks and have similar
historical observations , and then learn the Analogical Reasoning (AR) map
between human hand keypoints and robot components. Taking advantage of focusing
on action keypoints instead of irrelevant visual cues, our method achieves
leading performance on the CALVIN benchmark {and real-world experiments}. In
few-shot scenarios, our AR-VRM outperforms previous methods by large margins ,
underscoring the effectiveness of explicitly imitating human actions under data
scarcity.

</details>


### [236] [DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents](https://arxiv.org/abs/2508.07021)
*Kun Qian,Wenjie Li,Tianyu Sun,Wenhong Wang,Wenhan Luo*

Main category: cs.CV

TL;DR: This paper proposes DocRefine, a sophisticated framework for automated summarization and content refinement of scientific PDF documents utilizing LVLMs and a multi-agent system for high accuracy.


<details>
  <summary>Details</summary>
Motivation: The need arises from the exponential growth of scientific literature in PDF format and the limitations of traditional methods and direct applications of LLMs/LVLMs in handling complex multimodal content.

Method: DocRefine employs a multi-agent system with six specialized agents in a closed-loop feedback architecture to ensure semantic accuracy and visual fidelity.

Result: DocRefine outperforms state-of-the-art baselines on the DocEditBench dataset, achieving high scores in Semantic Consistency Score (86.7%), Layout Fidelity Index (93.9%), and Instruction Adherence Rate (85.0%).

Conclusion: DocRefine marks a significant advancement in automated scientific document processing, showcasing its capability in complex multimodal document editing while preserving semantic integrity and visual consistency.

Abstract: The exponential growth of scientific literature in PDF format necessitates
advanced tools for efficient and accurate document understanding,
summarization, and content optimization. Traditional methods fall short in
handling complex layouts and multimodal content, while direct application of
Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks
precision and control for intricate editing tasks. This paper introduces
DocRefine, an innovative framework designed for intelligent understanding,
content refinement, and automated summarization of scientific PDF documents,
driven by natural language instructions. DocRefine leverages the power of
advanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent
system comprising six specialized and collaborative agents: Layout & Structure
Analysis, Multimodal Content Understanding, Instruction Decomposition, Content
Refinement, Summarization & Generation, and Fidelity & Consistency
Verification. This closed-loop feedback architecture ensures high semantic
accuracy and visual fidelity. Evaluated on the comprehensive DocEditBench
dataset, DocRefine consistently outperforms state-of-the-art baselines across
various tasks, achieving overall scores of 86.7% for Semantic Consistency Score
(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction
Adherence Rate (IAR). These results demonstrate DocRefine's superior capability
in handling complex multimodal document editing, preserving semantic integrity,
and maintaining visual consistency, marking a significant advancement in
automated scientific document processing.

</details>


### [237] [Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2508.07701)
*Bo Jia,Yanan Guo,Ying Chang,Benkui Zhang,Ying Xie,Kangning Du,Lin Cao*

Main category: cs.CV

TL;DR: The paper introduces improvements to 3D Gaussian Splatting (3DGS) for better multi-view surface reconstruction, addressing biases arising from single-view normal alignments.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in 3DGS that cause geometric biases in multi-view scenarios, by improving depth and normal alignment across views.

Method: The authors designed two modules: a multi-view distance reprojection regularization module to align depth maps across nearby views and a multi-view normal enhancement module to ensure normal consistency between pixel points in adjacent views.

Result: The proposed method achieves higher accuracy in reconstructing small indoor and outdoor scenes, outperforming baseline approaches in both quantitative and qualitative evaluations.

Conclusion: The improvements significantly enhance the surface reconstruction capabilities of 3DGS in multi-view contexts.

Abstract: 3D Gaussian Splatting (3DGS) achieves remarkable results in the field of
surface reconstruction. However, when Gaussian normal vectors are aligned
within the single-view projection plane, while the geometry appears reasonable
in the current view, biases may emerge upon switching to nearby views. To
address the distance and global matching challenges in multi-view scenes, we
design multi-view normal and distance-guided Gaussian splatting. This method
achieves geometric depth unification and high-accuracy reconstruction by
constraining nearby depth maps and aligning 3D normals. Specifically, for the
reconstruction of small indoor and outdoor scenes, we propose a multi-view
distance reprojection regularization module that achieves multi-view Gaussian
alignment by computing the distance loss between two nearby views and the same
Gaussian surface. Additionally, we develop a multi-view normal enhancement
module, which ensures consistency across views by matching the normals of pixel
points in nearby views and calculating the loss. Extensive experimental results
demonstrate that our method outperforms the baseline in both quantitative and
qualitative evaluations, significantly enhancing the surface reconstruction
capability of 3DGS.

</details>


### [238] [MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering](https://arxiv.org/abs/2508.07023)
*Jingwei Peng,Jiehao Chen,Mateo Alejandro Rojas,Meilin Zhang*

Main category: cs.CV

TL;DR: MV-CoRe, a new model combining diverse visual and linguistic information, improves Complex Visual Question Answering (VQA) by leveraging fine-grained features and a Multimodal Fusion Transformer.


<details>
  <summary>Details</summary>
Motivation: Existing large vision-language models struggle with Complex VQA due to reliance on global features and insufficient fine-grained reasoning capabilities.

Method: MV-CoRe combines global embeddings from Vision and Language models with fine-grained visual features like object detection and scene graphs, using a Multimodal Fusion Transformer for deep feature integration.

Result: MV-CoRe achieves 77.5% accuracy on the GQA benchmark and consistently outperforms other baselines, confirmed by ablation studies and human evaluations.

Conclusion: MV-CoRe demonstrates superior complex reasoning capabilities and factual correctness, making it a strong solution for complex, multimodal VQA tasks.

Abstract: Complex Visual Question Answering (Complex VQA) tasks, which demand
sophisticated multi-modal reasoning and external knowledge integration, present
significant challenges for existing large vision-language models (LVLMs) often
limited by their reliance on high-level global features. To address this, we
propose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model
designed to enhance Complex VQA performance through the deep fusion of diverse
visual and linguistic information. MV-CoRe meticulously integrates global
embeddings from pre-trained Vision Large Models (VLMs) and Language Large
Models (LLMs) with fine-grained semantic-aware visual features, including
object detection characteristics and scene graph representations. An innovative
Multimodal Fusion Transformer then processes and deeply integrates these
diverse feature sets, enabling rich cross-modal attention and facilitating
complex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,
including GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental
results demonstrate that MV-CoRe consistently outperforms established LVLM
baselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies
confirm the critical contribution of both object and scene graph features, and
human evaluations further validate MV-CoRe's superior factual correctness and
reasoning depth, underscoring its robust capabilities for deep visual and
conceptual understanding.

</details>


### [239] [Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation](https://arxiv.org/abs/2508.07028)
*Juntong Fan,Shuyi Fan,Debesh Jha,Changsheng Fang,Tieyong Zeng,Hengyong Yu,Dayang Wang*

Main category: cs.CV

TL;DR: FOCUS-Med is a novel approach for endoscopic image segmentation of polyps, utilizing spatial and structural graph fusion along with attentional context mechanisms to achieve enhanced accuracy in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the difficulties in precisely segmenting polyps in endoscopic images, which is crucial for early colorectal cancer detection but challenging due to factors such as indistinct boundaries, low contrast, and specular highlights.

Method: FOCUS-Med integrates a Dual Graph Convolutional Network (Dual-GCN) for spatial and structural dependencies, employs location-fused self-attention for global context integration, and incorporates a trainable weighted fusion strategy for multi-scale aggregation. Additionally, a Large Language Model (LLM) provides segmentation quality evaluations.

Result: The approach delivers state-of-the-art performance in polyp segmentation across five key metrics on public benchmark datasets, showing superior capability in boundary preservation and complex shape delineation.

Conclusion: FOCUS-Med demonstrates excellent potential for clinical use in AI-assisted colonoscopy, significantly advancing accuracy in polyp segmentation tasks by addressing traditional challenges using graph-based strategies and attention mechanisms.

Abstract: Accurate endoscopic image segmentation on the polyps is critical for early
colorectal cancer detection. However, this task remains challenging due to low
contrast with surrounding mucosa, specular highlights, and indistinct
boundaries. To address these challenges, we propose FOCUS-Med, which stands for
Fusion of spatial and structural graph with attentional context-aware polyp
segmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph
Convolutional Network (Dual-GCN) module to capture contextual spatial and
topological structural dependencies. This graph-based representation enables
the model to better distinguish polyps from background tissues by leveraging
topological cues and spatial connectivity, which are often obscured in raw
image intensities. It enhances the model's ability to preserve boundaries and
delineate complex shapes typical of polyps. In addition, a location-fused
stand-alone self-attention is employed to strengthen global context
integration. To bridge the semantic gap between encoder-decoder layers, we
incorporate a trainable weighted fast normalized fusion strategy for efficient
multi-scale aggregation. Notably, we are the first to introduce the use of a
Large Language Model (LLM) to provide detailed qualitative evaluations of
segmentation quality. Extensive experiments on public benchmarks demonstrate
that FOCUS-Med achieves state-of-the-art performance across five key metrics,
underscoring its effectiveness and clinical potential for AI-assisted
colonoscopy.

</details>


### [240] [TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree](https://arxiv.org/abs/2508.07083)
*Yueyu Hu,Ran Gong,Tingyu Fan,Yao Wang*

Main category: cs.CV

TL;DR: The paper introduces the Textured Surfel Octree (TeSO), a novel 3D representation that efficiently combines point cloud features with octree structures and textures to enhance rendering quality and compressibility.


<details>
  <summary>Details</summary>
Motivation: Current 3D representations like point clouds, meshes, and 3D Gaussians face challenges with rendering quality, surface definition, and efficient compression, which hinders their use in applications like 3D telepresence and AR/VR.

Method: The authors present the Textured Surfel Octree (TeSO), a structure utilizing cube-bounded surfels, octree organization, and texture patches to reduce primitive count while retaining high-frequency texture details. They also propose a compression scheme leveraging the octree structure to encode geometry and texture efficiently.

Result: The TeSO framework demonstrates superior rendering quality at lower bit-rates when compared to multiple point cloud and 3D Gaussian-based baselines.

Conclusion: TeSO successfully combines structural efficiency and high-fidelity rendering, making it a stronger candidate for 3D content streaming applications.

Abstract: 3D visual content streaming is a key technology for emerging 3D telepresence
and AR/VR applications. One fundamental element underlying the technology is a
versatile 3D representation that is capable of producing high-quality renders
and can be efficiently compressed at the same time. Existing 3D representations
like point clouds, meshes and 3D Gaussians each have limitations in terms of
rendering quality, surface definition, and compressibility. In this paper, we
present the Textured Surfel Octree (TeSO), a novel 3D representation that is
built from point clouds but addresses the aforementioned limitations. It
represents a 3D scene as cube-bounded surfels organized on an octree, where
each surfel is further associated with a texture patch. By approximating a
smooth surface with a large surfel at a coarser level of the octree, it reduces
the number of primitives required to represent the 3D scene, and yet retains
the high-frequency texture details through the texture map attached to each
surfel. We further propose a compression scheme to encode the geometry and
texture efficiently, leveraging the octree structure. The proposed textured
surfel octree combined with the compression scheme achieves higher rendering
quality at lower bit-rates compared to multiple point cloud and 3D
Gaussian-based baselines.

</details>


### [241] [Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration](https://arxiv.org/abs/2508.07092)
*Yue Hu,Juntong Peng,Yunqiao Yang,Siheng Chen*

Main category: cs.CV

TL;DR: This paper proposes HyComm, a collaborative LiDAR-based 3D detection system that optimizes the trade-off between communication bandwidth and detection performance using hybrid collaboration techniques.


<details>
  <summary>Details</summary>
Motivation: There is a fundamental challenge in collaborative 3D detection to balance the trade-off between communication bandwidth and detection accuracy.

Method: The paper introduces HyComm, which adaptively integrates compact perceptual outputs and raw observations while prioritizing critical data for optimal collaboration, using standardized data formats.

Result: HyComm outperforms previous methods on datasets like DAIR-V2X and OPV2V by achieving superior performance-bandwidth trade-offs, offering over 2,006× reduced communication volume compared to previous methods while maintaining high detection accuracy.

Conclusion: HyComm provides a communication-efficient and adaptable solution for collaborative 3D detection, making it suitable for diverse communication scenarios and agent configurations with varying detection models.

Abstract: Collaborative 3D detection can substantially boost detection performance by
allowing agents to exchange complementary information. It inherently results in
a fundamental trade-off between detection performance and communication
bandwidth. To tackle this bottleneck issue, we propose a novel hybrid
collaboration that adaptively integrates two types of communication messages:
perceptual outputs, which are compact, and raw observations, which offer richer
information. This approach focuses on two key aspects: i) integrating
complementary information from two message types and ii) prioritizing the most
critical data within each type. By adaptively selecting the most critical set
of messages, it ensures optimal perceptual information and adaptability,
effectively meeting the demands of diverse communication scenarios.Building on
this hybrid collaboration, we present \texttt{HyComm}, a
communication-efficient LiDAR-based collaborative 3D detection system.
\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable
compression rates for messages, addressing various communication requirements,
and ii) it uses standardized data formats for messages. This ensures they are
independent of specific detection models, fostering adaptability across
different agent configurations. To evaluate HyComm, we conduct experiments on
both real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm
consistently outperforms previous methods and achieves a superior
performance-bandwidth trade-off regardless of whether agents use the same or
varied detection models. It achieves a lower communication volume of more than
2,006$\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.
The related code will be released.

</details>


### [242] [AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation](https://arxiv.org/abs/2508.07112)
*Nikolai Warner,Wenjin Zhang,Irfan Essa,Apaar Sadhwani*

Main category: cs.CV

TL;DR: The paper introduces AugLift, an enhancement for 3D Human Pose Estimation (HPE) that improves cross-dataset generalization by integrating additional contextual features like detection confidence and depth estimates.


<details>
  <summary>Details</summary>
Motivation: Lifting-based 3D HPE approaches often fail to generalize effectively to new datasets or real-world conditions, necessitating methods that enhance generalization without requiring extra data or sensors.

Method: The proposed AugLift method enriches input data by adding detection confidence scores and depth estimates to the 2D keypoints, utilizing pre-trained models for these additional signals, and integrates seamlessly into existing systems.

Result: Extensive tests across four datasets show average improvements of 10.1% in generalization to unseen datasets and 4.0% for in-distribution data, consistently across different lifting architectures.

Conclusion: AugLift offers a simple, modular, and effective solution to improve generalization performance in lifting-based HPE models, leveraging sparse, keypoint-aligned contextual cues.

Abstract: Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D
poses from detected 2D keypoints, often generalize poorly to new datasets and
real-world settings. To address this, we propose \emph{AugLift}, a simple yet
effective reformulation of the standard lifting pipeline that significantly
improves generalization performance without requiring additional data
collection or sensors. AugLift sparsely enriches the standard input -- the 2D
keypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection
confidence score $c$ and a corresponding depth estimate $d$. These additional
signals are computed from the image using off-the-shelf, pre-trained models
(e.g., for monocular depth estimation), thereby inheriting their strong
generalization capabilities. Importantly, AugLift serves as a modular add-on
and can be readily integrated into existing lifting architectures.
  Our extensive experiments across four datasets demonstrate that AugLift
boosts cross-dataset performance on unseen datasets by an average of $10.1\%$,
while also improving in-distribution performance by $4.0\%$. These gains are
consistent across various lifting architectures, highlighting the robustness of
our method. Our analysis suggests that these sparse, keypoint-aligned cues
provide robust frame-level context, offering a practical way to significantly
improve the generalization of any lifting-based pose estimation model. Code
will be made publicly available.

</details>


### [243] [Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays](https://arxiv.org/abs/2508.07128)
*Gregory Schuit,Denis Parra,Cecilia Besa*

Main category: cs.CV

TL;DR: Generative adversarial networks (GANs) and diffusion models (DMs) were evaluated for medical chest X-ray image generation, showing complementary strengths but identifying perceptual gaps.


<details>
  <summary>Details</summary>
Motivation: Address the data scarcity issue in medical imaging, especially for low-prevalence anomalies, and enhance AI diagnostic and segmentation tools.

Method: Compared generative models (GANs and DMs) using synthetic chest X-rays conditioned on abnormalities, conducting a reader study with radiologists to assess fidelity and abnormality representation.

Result: DMs generally produced more realistic images, but GANs outperformed DMs in some tasks; radiologists identified visual cues showing gaps in synthetic image quality.

Conclusion: Further refinement is needed in generative models for reliable augmentation of AI training datasets, leveraging strengths of both GANs and DMs.

Abstract: Generative image models have achieved remarkable progress in both natural and
medical imaging. In the medical context, these techniques offer a potential
solution to data scarcity-especially for low-prevalence anomalies that impair
the performance of AI-driven diagnostic and segmentation tools. However,
questions remain regarding the fidelity and clinical utility of synthetic
images, since poor generation quality can undermine model generalizability and
trust. In this study, we evaluate the effectiveness of state-of-the-art
generative models-Generative Adversarial Networks (GANs) and Diffusion Models
(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:
Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged
Cardiac Silhouette (ECS). Using a benchmark composed of real images from the
MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a
reader study with three radiologists of varied experience. Participants were
asked to distinguish real from synthetic images and assess the consistency
between visual features and the target abnormality. Our results show that while
DMs generate more visually realistic images overall, GANs can report better
accuracy for specific conditions, such as absence of ECS. We further identify
visual cues radiologists use to detect synthetic images, offering insights into
the perceptual gaps in current models. These findings underscore the
complementary strengths of GANs and DMs and point to the need for further
refinement to ensure generative models can reliably augment training datasets
for AI diagnostic systems.

</details>


### [244] [CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance](https://arxiv.org/abs/2508.07140)
*Yingtie Lei,Fanghai Yi,Yihang Dong,Weihuang Liu,Xiaofeng Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: The paper proposes CMAMRNet, a novel deep learning-based framework for digital mural restoration, addressing challenges in maintaining artistic authenticity and dealing with complex degradation patterns.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for digital mural restoration struggle to maintain focus on damaged regions and often compromise the artistic details due to inconsistent mask guidance.

Method: The authors introduce CMAMRNet with two components: the Mask-Aware Up/Down-Sampler (MAUDS) for improved mask sensitivity across scales and the Co-Feature Aggregator (CFA) to capture both fine textures and global structures.

Result: Experimental evaluations show that CMAMRNet achieves better performance than existing state-of-the-art techniques in preserving structural and artistic details of murals.

Conclusion: CMAMRNet provides an effective solution to mural restoration challenges, successfully combining comprehensive mask guidance with multi-scale feature extraction to enhance restoration quality.

Abstract: Murals, as invaluable cultural artifacts, face continuous deterioration from
environmental factors and human activities. Digital restoration of murals faces
unique challenges due to their complex degradation patterns and the critical
need to preserve artistic authenticity. Existing learning-based methods
struggle with maintaining consistent mask guidance throughout their networks,
leading to insufficient focus on damaged regions and compromised restoration
quality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network
that addresses these limitations through comprehensive mask guidance and
multi-scale feature extraction. Our framework introduces two key components:
(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask
sensitivity across resolution scales through dedicated channel-wise feature
selection and mask-guided feature fusion; and (2) the Co-Feature Aggregator
(CFA), operating at both the highest and lowest resolutions to extract
complementary features for capturing fine textures and global structures in
degraded regions. Experimental results on benchmark datasets demonstrate that
CMAMRNet outperforms state-of-the-art methods, effectively preserving both
structural integrity and artistic details in restored murals. The code is
available
at~\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}.

</details>


### [245] [Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models](https://arxiv.org/abs/2508.07144)
*Xuanhan Wang,Huimin Deng,Ke Liu,Jun Wang,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: The paper introduces DPAL, a distillation-based pretraining framework to train lightweight Human-centric Vision Models (HVMs), achieving strong generalization similar to large HVMs.


<details>
  <summary>Details</summary>
Motivation: Large HVMs excel in human-centric vision tasks due to massive pretraining, but their heavy architectures and limited accessibility to pretraining data hinder practical applications.

Method: DPAL employs a Dynamic Pattern Decoder (D-PaDe) and three alignment objectives to adaptively learn typical human visual patterns from large HVMs at image, pixel, and relation levels.

Result: DPAL-ViT/Ti (5M parameters) demonstrates comparable generalization to large HVMs (PATH-B, Sapiens-L) and surpasses other distillation methods on 15 benchmark datasets.

Conclusion: DPAL effectively enables lightweight HVMs to achieve generalization performance rivaling large HVMs, making it suitable for real-world applications.

Abstract: Human-centric vision models (HVMs) have achieved remarkable generalization
due to large-scale pretraining on massive person images. However, their
dependence on large neural architectures and the restricted accessibility of
pretraining data significantly limits their practicality in real-world
applications. To address this limitation, we propose Dynamic Pattern Alignment
Learning (DPAL), a novel distillation-based pretraining framework that
efficiently trains lightweight HVMs to acquire strong generalization from large
HVMs. In particular, human-centric visual perception are highly dependent on
three typical visual patterns, including global identity pattern, local shape
pattern and multi-person interaction pattern. To achieve generalizable
lightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting
as a dynamic Mixture of Expert (MoE) model. It incorporates three specialized
experts dedicated to adaptively extract typical visual patterns, conditioned on
both input image and pattern queries. And then, we present three levels of
alignment objectives, which aims to minimize generalization gap between
lightweight HVMs and large HVMs at global image level, local pixel level, and
instance relation level. With these two deliberate designs, the DPAL
effectively guides lightweight model to learn all typical human visual patterns
from large HVMs, which can generalize to various human-centric vision tasks.
Extensive experiments conducted on 15 challenging datasets demonstrate the
effectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher,
DPAL-ViT/Ti (5M parameters) achieves surprising generalizability similar to
existing large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms
previous distillation-based pretraining methods including Proteus-ViT/Ti (5M)
and TinyMiM-ViT/Ti (5M) by a large margin.

</details>


### [246] [Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.07146)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: This paper proposes a diffusion-based method to improve pedestrian trajectory predictions by integrating short-term and long-term intent modeling.


<details>
  <summary>Details</summary>
Motivation: Predictions of pedestrian trajectories are critical for autonomous vehicles, and current diffusion-based models lack explicit semantic modeling of intent, which leads to inaccuracies.

Method: The proposed method uses residual polar representation for short-term intent and token-based endpoint prediction for long-term intent, combined with adaptive guidance and residual noise refinement.

Result: Experiments on ETH, UCY, and SDD benchmarks show the framework achieves competitive results compared to state-of-the-art methods.

Conclusion: The framework improves accuracy in trajectory predictions by combining intent modeling and enhanced diffusion process techniques.

Abstract: Predicting pedestrian motion trajectories is critical for the path planning
and motion control of autonomous vehicles. Recent diffusion-based models have
shown promising results in capturing the inherent stochasticity of pedestrian
behavior for trajectory prediction. However, the absence of explicit semantic
modelling of pedestrian intent in many diffusion-based methods may result in
misinterpreted behaviors and reduced prediction accuracy. To address the above
challenges, we propose a diffusion-based pedestrian trajectory prediction
framework that incorporates both short-term and long-term motion intentions.
Short-term intent is modelled using a residual polar representation, which
decouples direction and magnitude to capture fine-grained local motion
patterns. Long-term intent is estimated through a learnable, token-based
endpoint predictor that generates multiple candidate goals with associated
probabilities, enabling multimodal and context-aware intention modelling.
Furthermore, we enhance the diffusion process by incorporating adaptive
guidance and a residual noise predictor that dynamically refines denoising
accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and
SDD benchmarks, demonstrating competitive results against state-of-the-art
methods.

</details>


### [247] [SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.07149)
*Ruolin Yang,Da Li,Honggang Zhang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: The paper proposes SketchAnimator, a novel model for animating sketches by leveraging pre-trained models and reference videos.


<details>
  <summary>Details</summary>
Motivation: Animating sketches is a complex and time-intensive process requiring professional skills, making it inaccessible to amateurs.

Method: The method involves three stages: 1) Appearance Learning and 2) Motion Learning using LoRA with the pre-trained T2V model, and 3) Video Prior Distillation using Score Distillation Sampling (SDS) to update Bezier curve parameters for dynamic motion.

Result: The proposed model generates sketch videos that retain the original appearance of the sketches while incorporating reference video dynamics, performing well in one-shot motion customization.

Conclusion: SketchAnimator makes creative sketch animation accessible and efficient, opening new opportunities for designers and amateurs alike.

Abstract: Sketching is a uniquely human tool for expressing ideas and creativity. The
animation of sketches infuses life into these static drawings, opening a new
dimension for designers. Animating sketches is a time-consuming process that
demands professional skills and extensive experience, often proving daunting
for amateurs. In this paper, we propose a novel sketch animation model
SketchAnimator, which enables adding creative motion to a given sketch, like "a
jumping car''. Namely, given an input sketch and a reference video, we divide
the sketch animation into three stages: Appearance Learning, Motion Learning
and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate
sketch appearance information and motion dynamics from the reference video into
the pre-trained T2V model. In the third stage, we utilize Score Distillation
Sampling (SDS) to update the parameters of the Bezier curves in each sketch
frame according to the acquired motion information. Consequently, our model
produces a sketch video that not only retains the original appearance of the
sketch but also mirrors the dynamic movements of the reference video. We
compare our method with alternative approaches and demonstrate that it
generates the desired sketch video under the challenge of one-shot motion
customization.

</details>


### [248] [CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion](https://arxiv.org/abs/2508.07162)
*Xiaotong Lin,Tianming Liang,Jian-Fang Hu,Kun-Yu Lin,Yulei Kang,Chunwei Tian,Jianhuang Lai,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: The paper introduces CoopDiff, a framework that decouples human and object motion modeling for improved 3D human-object interaction anticipation, leveraging contact points for consistent predictions.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to differentiate between the distinct motion patterns of articulated humans and rigid objects, which have unique physical properties.

Method: The proposed CoopDiff framework employs two separate branches for modeling human and object dynamics. Human-object interactions are connected through shared contact points, ensuring consistency in their motion predictions.

Result: CoopDiff achieves superior performance compared to state-of-the-art methods on standard datasets like BEHAVE and Human-object Interaction.

Conclusion: The framework establishes a more reliable and coherent approach for predicting human-object interactions by decoupling their motion modeling and using contact points as anchors.

Abstract: 3D human-object interaction (HOI) anticipation aims to predict the future
motion of humans and their manipulated objects, conditioned on the historical
context. Generally, the articulated humans and rigid objects exhibit different
motion patterns, due to their distinct intrinsic physical properties. However,
this distinction is ignored by most of the existing works, which intend to
capture the dynamics of both humans and objects within a single prediction
model. In this work, we propose a novel contact-consistent decoupled diffusion
framework CoopDiff, which employs two distinct branches to decouple human and
object motion modeling, with the human-object contact points as shared anchors
to bridge the motion generation across branches. The human dynamics branch is
aimed to predict highly structured human motion, while the object dynamics
branch focuses on the object motion with rigid translations and rotations.
These two branches are bridged by a series of shared contact points with
consistency constraint for coherent human-object motion prediction. To further
enhance human-object consistency and prediction reliability, we propose a
human-driven interaction module to guide object motion modeling. Extensive
experiments on the BEHAVE and Human-object Interaction datasets demonstrate
that our CoopDiff outperforms state-of-the-art methods.

</details>


### [249] [Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection](https://arxiv.org/abs/2508.07170)
*Yunpeng Shi,Lei Chen,Xiaolu Shen,Yanju Guo*

Main category: cs.CV

TL;DR: The paper introduces LMFNet, a lightweight network utilizing the novel LMF layer for salient object detection that excels in efficiency and accuracy with only 0.81M parameters.


<details>
  <summary>Details</summary>
Motivation: Lightweight networks struggle with efficient multi-scale feature extraction essential for tasks like salient object detection, necessitating solutions optimizing both parameters and performance.

Method: The study designs the LMF layer using depthwise separable dilated convolutions in a fully connected structure and integrates these layers into LMFNet for superior feature extraction.

Result: LMFNet achieves state-of-the-art or comparable performance on five benchmark datasets while significantly reducing parameters compared to traditional and lightweight models.

Conclusion: The proposed LMFNet overcomes multi-scale learning challenges in lightweight networks and showcases its potential for wider applications across image processing tasks.

Abstract: In the domain of computer vision, multi-scale feature extraction is vital for
tasks such as salient object detection. However, achieving this capability in
lightweight networks remains challenging due to the trade-off between
efficiency and performance. This paper proposes a novel lightweight multi-scale
feature extraction layer, termed the LMF layer, which employs depthwise
separable dilated convolutions in a fully connected structure. By integrating
multiple LMF layers, we develop LMFNet, a lightweight network tailored for
salient object detection. Our approach significantly reduces the number of
parameters while maintaining competitive performance. Here, we show that LMFNet
achieves state-of-the-art or comparable results on five benchmark datasets with
only 0.81M parameters, outperforming several traditional and lightweight models
in terms of both efficiency and accuracy. Our work not only addresses the
challenge of multi-scale learning in lightweight networks but also demonstrates
the potential for broader applications in image processing tasks. The related
code files are available at https://github.com/Shi-Yun-peng/LMFNet

</details>


### [250] [EventRR: Event Referential Reasoning for Referring Video Object Segmentation](https://arxiv.org/abs/2508.07171)
*Huihui Xu,Jiashi Lin,Haoyu Chen,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: This paper introduces the Event Referential Reasoning (EventRR) framework for Referring Video Object Segmentation, focusing on event-level structured reasoning and achieving improved performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RVOS methods treat referring expressions as unstructured sequences, overlooking the semantic structure essential for effective referent reasoning. Moreover, video-referring expressions, compared to image-based ones, involve additional complexities like event attributes and temporal relations, which current methods struggle to address.

Method: The proposed EventRR framework decouples RVOS into two parts: object summarization and referent reasoning. Object summarization aggregates cross-modal temporal context into bottleneck tokens, while referent reasoning leverages a Referential Event Graph (REG) to semantically structure the video expression into a graph, using a Temporal Concept-Role Reasoning (TCRR) mechanism to accumulate scores in a hierarchical manner.

Result: EventRR achieves state-of-the-art performance on four benchmark datasets, significantly outperforming existing RVOS methods both quantitatively and qualitatively.

Conclusion: EventRR effectively addresses the challenges of RVOS by introducing structure-aware event reasoning, leading to more accurate and interpretable results in video object segmentation. The proposed framework demonstrates the benefits of integrating semantic structure into expression understanding.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment out the object in
a video referred by an expression. Current RVOS methods view referring
expressions as unstructured sequences, neglecting their crucial semantic
structure essential for referent reasoning. Besides, in contrast to
image-referring expressions whose semantics focus only on object attributes and
object-object relations, video-referring expressions also encompass event
attributes and event-event temporal relations. This complexity challenges
traditional structured reasoning image approaches. In this paper, we propose
the Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS
into object summarization part and referent reasoning part. The summarization
phase begins by summarizing each frame into a set of bottleneck tokens, which
are then efficiently aggregated in the video-level summarization step to
exchange the global cross-modal temporal context. For reasoning part, EventRR
extracts semantic eventful structure of a video-referring expression into
highly expressive Referential Event Graph (REG), which is a single-rooted
directed acyclic graph. Guided by topological traversal of REG, we propose
Temporal Concept-Role Reasoning (TCRR) to accumulate the referring score of
each temporal query from REG leaf nodes to root node. Each reasoning step can
be interpreted as a question-answer pair derived from the concept-role
relations in REG. Extensive experiments across four widely recognized benchmark
datasets, show that EventRR quantitatively and qualitatively outperforms
state-of-the-art RVOS methods. Code is available at
https://github.com/bio-mlhui/EventRR

</details>


### [251] [Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset](https://arxiv.org/abs/2508.07211)
*Junyi He,Liuling Chen,Hongyang Zhou,Zhang xiaoxing,Xiaobin Zhu,Shengxiang Yu,Jingyan Qin,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: This paper introduces a Depth-Guided Network (DGN) for image restoration, addressing often-neglected depth information, and includes a novel large-scale dataset of high-resolution plant images.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing image restoration models that neglect depth information, leading to suboptimal performance in varying depth-of-field settings.

Method: A Depth-Guided Network (DGN) with interactive branches: a depth estimation branch for structural guidance and an image restoration branch utilizing progressive window-based self-attention and sparse non-local attention. Additionally, joint training is applied for mutual enhancement of restoration and depth estimation.

Result: The proposed method achieves state-of-the-art performance on multiple benchmarks and demonstrates strong generalization to unseen plant images.

Conclusion: Depth information significantly enhances image restoration quality, and the proposed DGN and accompanying dataset provide robust and effective solutions for advancing the field.

Abstract: Image restoration has seen substantial progress in recent years. However,
existing methods often neglect depth information, which hurts similarity
matching, results in attention distractions in shallow depth-of-field (DoF)
scenarios, and excessive enhancement of background content in deep DoF
settings. To overcome these limitations, we propose a novel Depth-Guided
Network (DGN) for image restoration, together with a novel large-scale
high-resolution dataset. Specifically, the network consists of two interactive
branches: a depth estimation branch that provides structural guidance, and an
image restoration branch that performs the core restoration task. In addition,
the image restoration branch exploits intra-object similarity through
progressive window-based self-attention and captures inter-object similarity
via sparse non-local attention. Through joint training, depth features
contribute to improved restoration quality, while the enhanced visual features
from the restoration branch in turn help refine depth estimation. Notably, we
also introduce a new dataset for training and evaluation, consisting of 9,205
high-resolution images from 403 plant species, with diverse depth and texture
variations. Extensive experiments show that our method achieves
state-of-the-art performance on several standard benchmarks and generalizes
well to unseen plant images, demonstrating its effectiveness and robustness.

</details>


### [252] [Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling](https://arxiv.org/abs/2508.07214)
*Hongyang Zhou,Xiaobin Zhu,Liuling Chen,Junyi He,Jingyan Qin,Xu-Cheng Yin,Zhang xiaoxing*

Main category: cs.CV

TL;DR: The paper introduces a novel method for unsupervised real-world super-resolution (SR) through effective modeling of real-world image degradation using rectified flow and Fourier priors.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of the significant domain gap between synthetic and real-world data in unsupervised super-resolution due to unknown and complex degradation in practical scenarios.

Method: The authors propose two novel modules: a Rectified Flow Degradation Module (RFDM) and a Fourier Prior Guided Degradation Module (FGDM). RFDM models degradation through degradation-transformed LR images to improve realism, while FGDM uses Fourier phase information for accurate degradation modeling. These modules generate synthetic LR images with realistic degradation paired with HR images to train SR models.

Result: Experiments on real-world datasets show that the proposed method significantly improves the performance of existing super-resolution models in handling real-world data.

Conclusion: The proposed degradation modeling approach effectively narrows the domain gap, enabling current SR models to perform better in practical applications by synthesizing more realistic training data.

Abstract: Unsupervised real-world super-resolution (SR) faces critical challenges due
to the complex, unknown degradation distributions in practical scenarios.
Existing methods struggle to generalize from synthetic low-resolution (LR) and
high-resolution (HR) image pairs to real-world data due to a significant domain
gap. In this paper, we propose an unsupervised real-world SR method based on
rectified flow to effectively capture and model real-world degradation,
synthesizing LR-HR training pairs with realistic degradation. Specifically,
given unpaired LR and HR images, we propose a novel Rectified Flow Degradation
Module (RFDM) that introduces degradation-transformed LR (DT-LR) images as
intermediaries. By modeling the degradation trajectory in a continuous and
invertible manner, RFDM better captures real-world degradation and enhances the
realism of generated LR images. Additionally, we propose a Fourier Prior Guided
Degradation Module (FGDM) that leverages structural information embedded in
Fourier phase components to ensure more precise modeling of real-world
degradation. Finally, the LR images are processed by both FGDM and RFDM,
producing final synthetic LR images with real-world degradation. The synthetic
LR images are paired with the given HR images to train the off-the-shelf SR
networks. Extensive experiments on real-world datasets demonstrate that our
method significantly enhances the performance of existing SR approaches in
real-world scenarios.

</details>


### [253] [Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization](https://arxiv.org/abs/2508.07216)
*Songlin Li,Zhiqing Guo,Yuanman Li,Zeyu Li,Yunfeng Diao,Gaobo Yang,Liejun Wang*

Main category: cs.CV

TL;DR: The paper introduces CMB-Net, a model leveraging visual and semantic clues using LLMs for detecting image manipulations. It uses modules to improve textual and feature alignment, excelling in fine-grained boundary preservation.


<details>
  <summary>Details</summary>
Motivation: Existing IML models mainly depend on visual cues and overlook the semantic relationships between content features that real images naturally comply with. Manipulated images often disrupt these logical relationships, offering clues for detection.

Method: CMB-Net utilizes LLMs to produce textual descriptions of manipulated regions to enhance semantic understanding. It incorporates ITCAM to weigh reliable text features, ITIM for aligning visual and text features through interaction, and RED for preserving boundary information during manipulation detection.

Result: Extensive experiments demonstrate that CMB-Net surpasses most current IML models, highlighting its effectiveness in combining semantic and visual features for precise manipulation localization.

Conclusion: CMB-Net successfully integrates visual, semantic, and boundary-preserving techniques, offering improved localization accuracy in manipulated images and outperforming existing models.

Abstract: The existing image manipulation localization (IML) models mainly relies on
visual cues, but ignores the semantic logical relationships between content
features. In fact, the content semantics conveyed by real images often conform
to human cognitive laws. However, image manipulation technology usually
destroys the internal relationship between content features, thus leaving
semantic clues for IML. In this paper, we propose a cognition-inspired
multimodal boundary-preserving network (CMB-Net). Specifically, CMB-Net
utilizes large language models (LLMs) to analyze manipulated regions within
images and generate prompt-based textual information to compensate for the lack
of semantic relationships in the visual information. Considering that the
erroneous texts induced by hallucination from LLMs will damage the accuracy of
IML, we propose an image-text central ambiguity module (ITCAM). It assigns
weights to the text features by quantifying the ambiguity between text and
image features, thereby ensuring the beneficial impact of textual information.
We also propose an image-text interaction module (ITIM) that aligns visual and
text features using a correlation matrix for fine-grained interaction. Finally,
inspired by invertible neural networks, we propose a restoration edge decoder
(RED) that mutually generates input and output features to preserve boundary
information in manipulated regions without loss. Extensive experiments show
that CMB-Net outperforms most existing IML models.

</details>


### [254] [Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline](https://arxiv.org/abs/2508.07217)
*Yuqi Han,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: This paper proposes a hybrid calibration method combining generic and parametric camera models to improve accuracy and stability in camera calibration.


<details>
  <summary>Details</summary>
Motivation: Challenges exist in offline camera calibration due to reliance on user-selected parametric models or complex generic methods, both of which have limitations in accuracy, flexibility, and intrinsic parameter provision.

Method: A global optimization-based hybrid method is proposed, combining generic and parametric camera models. It includes a linear solver and nonlinear optimization to address pose ambiguity issues in generic models.

Result: The proposed hybrid method achieves consistent accuracy improvements across diverse lens types and noise levels, validated through simulations and real-world experiments.

Conclusion: The generic-parametric hybrid calibration method is effective and reliable, combining the strengths of both approaches for accurate camera calibration in diverse and complex conditions.

Abstract: Offline camera calibration techniques typically employ parametric or generic
camera models. Selecting parametric models relies heavily on user experience,
and an inappropriate camera model can significantly affect calibration
accuracy. Meanwhile, generic calibration methods involve complex procedures and
cannot provide traditional intrinsic parameters. This paper reveals a pose
ambiguity in the pose solutions of generic calibration methods that
irreversibly impacts subsequent pose estimation. A linear solver and a
nonlinear optimization are proposed to address this ambiguity issue. Then a
global optimization hybrid calibration method is introduced to integrate
generic and parametric models together, which improves extrinsic parameter
accuracy of generic calibration and mitigates overfitting and numerical
instability in parametric calibration. Simulation and real-world experimental
results demonstrate that the generic-parametric hybrid calibration method
consistently excels across various lens types and noise contamination,
hopefully serving as a reliable and accurate solution for camera calibration in
complex scenarios.

</details>


### [255] [Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource](https://arxiv.org/abs/2508.07233)
*Lei Yang,Junshan Jin,Mingyuan Zhang,Yi He,Bofan Chen,Shilin Wang*

Main category: cs.CV

TL;DR: The paper introduces a landmark-guided approach to enhance visual speech recognition, utilizing spatio-temporal multi-graph convolutional networks with limited data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of deep learning approaches in visual speech recognition, which struggle with visual disturbances and require extensive training data.

Method: The paper proposes a landmark-guided visual feature extractor, using facial landmarks for spatio-temporal analysis via a multi-graph convolutional network. This is coupled with a multi-level lip dynamic fusion framework.

Result: Experimental results highlight that the proposed method performs well with limited data and shows improved accuracy when applied to unseen speakers.

Conclusion: Landmark-guided approaches can reduce the reliance on large datasets, mitigate user-specific visual disturbances, and enhance generalization in visual speech recognition.

Abstract: Visual speech recognition is a technique to identify spoken content in silent
speech videos, which has raised significant attention in recent years.
Advancements in data-driven deep learning methods have significantly improved
both the speed and accuracy of recognition. However, these deep learning
methods can be effected by visual disturbances, such as lightning conditions,
skin texture and other user-specific features. Data-driven approaches could
reduce the performance degradation caused by these visual disturbances using
models pretrained on large-scale datasets. But these methods often require
large amounts of training data and computational resources, making them costly.
To reduce the influence of user-specific features and enhance performance with
limited data, this paper proposed a landmark guided visual feature extractor.
Facial landmarks are used as auxiliary information to aid in training the
visual feature extractor. A spatio-temporal multi-graph convolutional network
is designed to fully exploit the spatial locations and spatio-temporal features
of facial landmarks. Additionally, a multi-level lip dynamic fusion framework
is introduced to combine the spatio-temporal features of the landmarks with the
visual features extracted from the raw video frames. Experimental results show
that this approach performs well with limited data and also improves the
model's accuracy on unseen speakers.

</details>


### [256] [ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation](https://arxiv.org/abs/2508.07237)
*Bo Wang,Mengyuan Xu,Yue Yan,Yuqun Yang,Kechen Shu,Wei Ping,Xu Tang,Wei Jiang,Zheng You*

Main category: cs.CV

TL;DR: The paper proposes ASM-UNet, a novel medical image segmentation architecture for fine-grained segmentation (FGS), addressing the limitations of existing models that rely on fixed scanning orders.


<details>
  <summary>Details</summary>
Motivation: Current medical image segmentation methods often fail in FGS due to individual anatomical variations and reliance on fixed scanning orders.

Method: ASM-UNet dynamically predicts scanning orders using adaptive scan scores based on group-level commonalities and individual variations.

Result: ASM-UNet demonstrated superior performance in both coarse- and fine-grained segmentation tasks across three datasets: ACDC, Synapse, and BTMS.

Conclusion: The proposed ASM-UNet successfully advances FGS by dynamically adapting to individual anatomical variations, outperforming existing approaches.

Abstract: Precise lesion resection depends on accurately identifying fine-grained
anatomical structures. While many coarse-grained segmentation (CGS) methods
have been successful in large-scale segmentation (e.g., organs), they fall
short in clinical scenarios requiring fine-grained segmentation (FGS), which
remains challenging due to frequent individual variations in small-scale
anatomical structures. Although recent Mamba-based models have advanced medical
image segmentation, they often rely on fixed manually-defined scanning orders,
which limit their adaptability to individual variations in FGS. To address
this, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It
introduces adaptive scan scores to dynamically guide the scanning order,
generated by combining group-level commonalities and individual-level
variations. Experiments on two public datasets (ACDC and Synapse) and a newly
proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that
ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and
dataset are available at https://github.com/YqunYang/ASM-UNet.

</details>


### [257] [Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers](https://arxiv.org/abs/2508.07246)
*Xin Ma,Yaohui Wang,Genyun Jia,Xinyuan Chen,Tien-Tsin Wong,Cunjian Chen*

Main category: cs.CV

TL;DR: MiraMo is a framework for image animation that enhances efficiency, appearance consistency, and motion smoothness using a text-to-video architecture, motion residual learning, and DCT-based noise refinement.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address issues in image animation such as appearance inconsistency, abrupt motion transitions, and high computational demands, while improving on the less effective U-Net-based diffusion models used in the field.

Method: MiraMo uses three innovations: an efficient linear attention-based text-to-video architecture, a motion residual learning paradigm for better temporal consistency, and a DCT-based noise refinement approach with a dynamics control module for smoother animations.

Result: The experiments demonstrate MiraMo's superiority over state-of-the-art methods in generating smooth and consistent animations with faster inference. It shows promise in applications like motion transfer and video editing.

Conclusion: MiraMo achieves significant advancements in the field of image animation by improving quality and efficiency, while also opening up new applications in related domains.

Abstract: Image animation has seen significant progress, driven by the powerful
generative capabilities of diffusion models. However, maintaining appearance
consistency with static input images and mitigating abrupt motion transitions
in generated animations remain persistent challenges. While text-to-video (T2V)
generation has demonstrated impressive performance with diffusion transformer
models, the image animation field still largely relies on U-Net-based diffusion
models, which lag behind the latest T2V approaches. Moreover, the quadratic
complexity of vanilla self-attention mechanisms in Transformers imposes heavy
computational demands, making image animation particularly resource-intensive.
To address these issues, we propose MiraMo, a framework designed to enhance
efficiency, appearance consistency, and motion smoothness in image animation.
Specifically, MiraMo introduces three key elements: (1) A foundational
text-to-video architecture replacing vanilla self-attention with efficient
linear attention to reduce computational overhead while preserving generation
quality; (2) A novel motion residual learning paradigm that focuses on modeling
motion dynamics rather than directly predicting frames, improving temporal
consistency; and (3) A DCT-based noise refinement strategy during inference to
suppress sudden motion artifacts, complemented by a dynamics control module to
balance motion smoothness and expressiveness. Extensive experiments against
state-of-the-art methods validate the superiority of MiraMo in generating
consistent, smooth, and controllable animations with accelerated inference
speed. Additionally, we demonstrate the versatility of MiraMo through
applications in motion transfer and video editing tasks.

</details>


### [258] [SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking](https://arxiv.org/abs/2508.07250)
*Fengchao Xiong,Zhenxing Wu,Sen Jia,Yuntao Qian*

Main category: cs.CV

TL;DR: This paper proposes a novel hyperspectral video tracker that incorporates spectral interactions using Transformers and a new spectral loss for enhanced robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations of existing tracking methods, which neglect spectral interactions in hyperspectral videos, resulting in suboptimal performance in challenging conditions.

Method: The authors use Transformers to model band-wise spatial relationships and introduce a spectral interaction approach based on the inclusion-exclusion principle. They also add a spectral loss during training to improve material distribution alignment.

Result: The proposed method significantly improves tracking performance over existing methods and achieves state-of-the-art results in experiments.

Conclusion: The work successfully addresses limitations in hyperspectral video tracking by leveraging both spectral and spatial interactions, offering improved robustness and accuracy, and makes its implementation and results reproducible.

Abstract: Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal
structure, offer distinct advantages in challenging tracking scenarios such as
cluttered backgrounds and small objects. However, existing methods primarily
focus on spatial interactions between the template and search regions, often
overlooking spectral interactions, leading to suboptimal performance. To
address this issue, this paper investigates spectral interactions from both the
architectural and training perspectives. At the architectural level, we first
establish band-wise long-range spatial relationships between the template and
search regions using Transformers. We then model spectral interactions using
the inclusion-exclusion principle from set theory, treating them as the union
of spatial interactions across all bands. This enables the effective
integration of both shared and band-specific spatial cues. At the training
level, we introduce a spectral loss to enforce material distribution alignment
between the template and predicted regions, enhancing robustness to shape
deformation and appearance variations. Extensive experiments demonstrate that
our tracker achieves state-of-the-art tracking performance. The source code,
trained models and results will be publicly available via
https://github.com/bearshng/suit to support reproducibility.

</details>


### [259] [Understanding Dynamic Scenes in Ego Centric 4D Point Clouds](https://arxiv.org/abs/2508.07251)
*Junsheng Huang,Shengyu Hao,Bocheng Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: The paper introduces the EgoDynamic4D benchmark for understanding and reasoning about dynamic scenes from an egocentric perspective, using a detailed question-answering framework with robust annotations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of unified 4D annotations and task-driven evaluation in existing egocentric datasets, specifically for fine-grained spatio-temporal reasoning regarding motion and interactions in dynamic scenes.

Method: They developed EgoDynamic4D, which integrates RGB-D video, camera poses, globally unique instance masks, and 4D bounding boxes, along with a QA benchmark comprising 927K pairs. They also propose a reasoning framework that encodes dynamic/static scene information to create token sequences from 4D data for use with LLMs.

Result: Experiments demonstrate that the proposed method consistently outperforms baseline approaches, highlighting its capability in multimodal temporal modeling for dynamic egocentric scene understanding.

Conclusion: The EgoDynamic4D benchmark and spatio-temporal reasoning framework enable more effective and fine-grained understanding of dynamic scenes, marking progress in tasks like trajectory prediction and temporal reasoning for human-machine interaction.

Abstract: Understanding dynamic 4D scenes from an egocentric perspective-modeling
changes in 3D spatial structure over time-is crucial for human-machine
interaction, autonomous navigation, and embodied intelligence. While existing
egocentric datasets contain dynamic scenes, they lack unified 4D annotations
and task-driven evaluation protocols for fine-grained spatio-temporal
reasoning, especially on motion of objects and human, together with their
interactions. To address this gap, we introduce EgoDynamic4D, a novel QA
benchmark on highly dynamic scenes, comprising RGB-D video, camera poses,
globally unique instance masks, and 4D bounding boxes. We construct 927K QA
pairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,
step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering
agent motion, human-object interaction, trajectory prediction, relation
understanding, and temporal-causal reasoning, with fine-grained,
multidimensional metrics. To tackle these tasks, we propose an end-to-end
spatio-temporal reasoning framework that unifies dynamic and static scene
information, using instance-aware feature encoding, time and camera encoding,
and spatially adaptive down-sampling to compress large 4D scenes into token
sequences manageable by LLMs. Experiments on EgoDynamic4D show that our method
consistently outperforms baselines, validating the effectiveness of multimodal
temporal modeling for egocentric dynamic scene understanding.

</details>


### [260] [Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM](https://arxiv.org/abs/2508.07260)
*Sihan Yang,Huitong Ji,Shaolin Lu,Jiayi Chen,Binxiao Xu,Ming Lu,Yuanxing Zhang,Wenhui Dong,Wentao Zhang*

Main category: cs.CV

TL;DR: The paper introduces a Small-Large Collaboration (SLC) framework for efficiently personalizing large Vision-Language Models using a small VLM for personalized tasks without extensive training costs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of personalizing large Vision-Language Models, such as high training costs and restricted access, while leveraging the personalization advantages of small VLMs.

Method: The Small VLM handles personalized information generation while the Large VLM synthesizes this data. A test-time reflection strategy is employed to prevent hallucinations by the smaller model. This framework supports both open and closed-source VLMs with reduced training overhead.

Result: The experiments show that the SLC framework is effective across benchmarks and various large VLMs, achieving accurate personalized responses.

Conclusion: The SLC framework represents a training-efficient and versatile solution for large VLM personalization, supporting both open and closed-source models and expanding practical applications.

Abstract: Personalizing Vision-Language Models (VLMs) to transform them into daily
assistants has emerged as a trending research direction. However, leading
companies like OpenAI continue to increase model size and develop complex
designs such as the chain of thought (CoT). While large VLMs are proficient in
complex multi-modal understanding, their high training costs and limited access
via paid APIs restrict direct personalization. Conversely, small VLMs are
easily personalized and freely available, but they lack sufficient reasoning
capabilities. Inspired by this, we propose a novel collaborative framework
named Small-Large Collaboration (SLC) for large VLM personalization, where the
small VLM is responsible for generating personalized information, while the
large model integrates this personalized information to deliver accurate
responses. To effectively incorporate personalized information, we develop a
test-time reflection strategy, preventing the potential hallucination of the
small VLM. Since SLC only needs to train a meta personalized small VLM for the
large VLMs, the overall process is training-efficient. To the best of our
knowledge, this is the first training-efficient framework that supports both
open-source and closed-source large VLMs, enabling broader real-world
personalized applications. We conduct thorough experiments across various
benchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC
framework. The code will be released at https://github.com/Hhankyangg/SLC.

</details>


### [261] [Representation Understanding via Activation Maximization](https://arxiv.org/abs/2508.07281)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.CV

TL;DR: This study introduces a unified framework to visualize features in CNNs and ViTs, extending interpretations to intermediate layers and exploring adversarial example generation.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability of DNNs by analyzing internal feature representations through visualization, inspired by neuroscience techniques.

Method: The paper uses Activation Maximization (AM) to synthesize inputs that trigger strong responses from neural network components, applied to both CNNs and ViTs, including intermediate layers.

Result: The framework successfully interprets both CNNs and ViTs, extends visualization to hierarchical intermediate layers, and generates adversarial examples to highlight vulnerabilities.

Conclusion: The approach generalizes across neural architectures and offers valuable insights into the interpretive structures and weaknesses of DNNs.

Abstract: Understanding internal feature representations of deep neural networks (DNNs)
is a fundamental step toward model interpretability. Inspired by neuroscience
methods that probe biological neurons using visual stimuli, recent deep
learning studies have employed Activation Maximization (AM) to synthesize
inputs that elicit strong responses from artificial neurons. In this work, we
propose a unified feature visualization framework applicable to both
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike
prior efforts that predominantly focus on the last output-layer neurons in
CNNs, we extend feature visualization to intermediate layers as well, offering
deeper insights into the hierarchical structure of learned feature
representations. Furthermore, we investigate how activation maximization can be
leveraged to generate adversarial examples, revealing potential vulnerabilities
and decision boundaries of DNNs. Our experiments demonstrate the effectiveness
of our approach in both traditional CNNs and modern ViT, highlighting its
generalizability and interpretive value.

</details>


### [262] [SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations](https://arxiv.org/abs/2508.07298)
*Zhiqiang Shen,Peng Cao,Xiaoli Liu,Jinzhu Yang,Osmar R. Zaiane*

Main category: cs.CV

TL;DR: This paper addresses challenges in medical image segmentation using limited labeled data by introducing SynMatch, a framework that synthesizes images to match pseudo labels without requiring traditional image synthesis training parameters.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle label scarcity in medical image segmentation, a major obstacle for deep learning-based methods, particularly under semi-, weakly-, and barely-supervised learning settings.

Method: The authors developed SynMatch, which synthesizes images using texture and shape features extracted from the segmentation model generating pseudo labels, ensuring consistency between synthesized images and pseudo labels without additional training parameters for synthesis.

Result: SynMatch demonstrated superior performance across various medical image segmentation tasks, especially excelling in barely-supervised learning scenarios. It significantly outperformed existing pseudo supervision methods in the challenging polyp segmentation tasks with limited scribble annotations.

Conclusion: SynMatch represents a promising advancement in medical image segmentation under limited label scenarios, effectively addressing challenges of pseudo label inconsistency by synthesizing more consistent image-label pairs.

Abstract: Label scarcity remains a major challenge in deep learning-based medical image
segmentation. Recent studies use strong-weak pseudo supervision to leverage
unlabeled data. However, performance is often hindered by inconsistencies
between pseudo labels and their corresponding unlabeled images. In this work,
we propose \textbf{SynMatch}, a novel framework that sidesteps the need for
improving pseudo labels by synthesizing images to match them instead.
Specifically, SynMatch synthesizes images using texture and shape features
extracted from the same segmentation model that generates the corresponding
pseudo labels for unlabeled images. This design enables the generation of
highly consistent synthesized-image-pseudo-label pairs without requiring any
training parameters for image synthesis. We extensively evaluate SynMatch
across diverse medical image segmentation tasks under semi-supervised learning
(SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL)
settings with increasingly limited annotations. The results demonstrate that
SynMatch achieves superior performance, especially in the most challenging BSL
setting. For example, it outperforms the recent strong-weak pseudo
supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task
with 5\% and 10\% scribble annotations, respectively. The code will be released
at https://github.com/Senyh/SynMatch.

</details>


### [263] [BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2508.07300)
*Ping-Mao Huang,I-Tien Chao,Ping-Chia Huang,Jia-Wei Liao,Yung-Yu Chuang*

Main category: cs.CV

TL;DR: BEVANet introduces BEVANet for real-time semantic segmentation using novel attention mechanisms and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing efficiency in capturing large receptive fields for semantic understanding and refining detailed contours in real-time segmentation.

Method: The paper proposes BEVANet, incorporating mechanisms like LKA, SDLSKA, CKS, and DLKPPM which expand receptive fields and enhance boundary delineation.

Result: BEVANet achieves real-time segmentation at 33 FPS and demonstrates state-of-the-art accuracy with 79.3% mIoU without pretraining and 81% mIoU with ImageNet pretraining.

Conclusion: BEVANet successfully combines computational efficiency with high semantic accuracy for real-time segmentation, offering a solid advancement in the field.

Abstract: Real-time semantic segmentation presents the dual challenge of designing
efficient architectures that capture large receptive fields for semantic
understanding while also refining detailed contours. Vision transformers model
long-range dependencies effectively but incur high computational cost. To
address these challenges, we introduce the Large Kernel Attention (LKA)
mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)
expands the receptive field to capture contextual information and extracts
visual and structural features using Sparse Decomposed Large Separable Kernel
Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism
dynamically adapts the receptive field to further enhance performance.
Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches
contextual features by synergistically combining dilated convolutions and large
kernel attention. The bilateral architecture facilitates frequent branch
communication, and the Boundary Guided Adaptive Fusion (BGAF) module enhances
boundary delineation by integrating spatial and semantic features under
boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding
79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet
pretraining, demonstrating state-of-the-art performance. The code and model is
available at https://github.com/maomao0819/BEVANet.

</details>


### [264] [DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices](https://arxiv.org/abs/2508.07306)
*Md Zahurul Haquea,Yeahyea Sarker,Muhammed Farhan Sadique Mahi,Syed Jubayer Jaman,Md Robiul Islam*

Main category: cs.CV

TL;DR: The paper introduces DragonFruitQualityNet, a CNN-based model achieving 93.98% accuracy in real-time dragon fruit quality assessment and integrated into a mobile app for practical on-device use.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the growing need for efficient quality inspection of dragon fruits to improve agricultural productivity and reduce post-harvest losses.

Method: A lightweight convolutional neural network (CNN) named DragonFruitQualityNet was developed and trained on a dataset of 13,789 images, classifying fruits into four categories. The model was also deployed in a mobile application for real-time quality assessment.

Result: The proposed model delivered 93.98% classification accuracy, surpassing existing methods for fruit quality inspection.

Conclusion: This research provides a scalable, accessible solution for sustainable farming by enabling practical and efficient quality control of dragon fruits through AI-driven mobile technology.

Abstract: Dragon fruit, renowned for its nutritional benefits and economic value, has
experienced rising global demand due to its affordability and local
availability. As dragon fruit cultivation expands, efficient pre- and
post-harvest quality inspection has become essential for improving agricultural
productivity and minimizing post-harvest losses. This study presents
DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)
optimized for real-time quality assessment of dragon fruits on mobile devices.
We curated a diverse dataset of 13,789 images, integrating self-collected
samples with public datasets (dataset from Mendeley Data), and classified them
into four categories: fresh, immature, mature, and defective fruits to ensure
robust model training. The proposed model achieves an impressive 93.98%
accuracy, outperforming existing methods in fruit quality classification. To
facilitate practical adoption, we embedded the model into an intuitive mobile
application, enabling farmers and agricultural stakeholders to conduct
on-device, real-time quality inspections. This research provides an accurate,
efficient, and scalable AI-driven solution for dragon fruit quality control,
supporting digital agriculture and empowering smallholder farmers with
accessible technology. By bridging the gap between research and real-world
application, our work advances post-harvest management and promotes sustainable
farming practices.

</details>


### [265] [MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark](https://arxiv.org/abs/2508.07307)
*Haiyang Guo,Fei Zhu,Hongbo Zhao,Fanhu Zeng,Wenzhuo Liu,Shijie Ma,Da-Han Wang,Xu-Yao Zhang*

Main category: cs.CV

TL;DR: This paper introduces MCITlib, a code library for continual instruction tuning in Multimodal Large Language Models, which addresses challenges in Multimodal Continual Learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable AI systems to continually learn and integrate new knowledge, particularly in multimodal settings involving cross-modal challenges.

Method: The authors developed MCITlib, a code library that implements 8 algorithms and evaluates them systematically on 2 multimodal benchmarks.

Result: The paper provides a comprehensive evaluation of 8 algorithms for Multimodal Continual Instruction Tuning and demonstrates the library's utility with benchmarks.

Conclusion: MCITlib is poised to advance research in Multimodal Continual Learning, providing a robust platform for developing and testing new methods.

Abstract: Continual learning aims to equip AI systems with the ability to continuously
acquire and adapt to new knowledge without forgetting previously learned
information, similar to human learning. While traditional continual learning
methods focusing on unimodal tasks have achieved notable success, the emergence
of Multimodal Large Language Models has brought increasing attention to
Multimodal Continual Learning tasks involving multiple modalities, such as
vision and language. In this setting, models are expected to not only mitigate
catastrophic forgetting but also handle the challenges posed by cross-modal
interactions and coordination. To facilitate research in this direction, we
introduce MCITlib, a comprehensive and constantly evolving code library for
continual instruction tuning of Multimodal Large Language Models. In MCITlib,
we have currently implemented 8 representative algorithms for Multimodal
Continual Instruction Tuning and systematically evaluated them on 2 carefully
selected benchmarks. MCITlib will be continuously updated to reflect advances
in the Multimodal Continual Learning field. The codebase is released at
https://github.com/Ghy0501/MCITlib.

</details>


### [266] [MobileViCLIP: An Efficient Video-Text Model for Mobile Devices](https://arxiv.org/abs/2508.07312)
*Min Yang,Zihan Jia,Zhilin Dai,Sheng Guo,Limin Wang*

Main category: cs.CV

TL;DR: The paper introduces MobileViCLIP, an efficient video-text model optimized for mobile devices, achieving strong zero-shot classification and retrieval capabilities with significantly faster inference speeds.


<details>
  <summary>Details</summary>
Motivation: There is a need for efficient video pre-trained models designed for mobile devices, as existing models primarily rely on high-latency architectures such as ViT and lack mobile optimization.

Method: The authors integrate temporal structural reparameterization into an image-text model and train it using a large-scale video-text dataset to create MobileViCLIP.

Result: MobileViCLIP-Small is 55.4x faster than InternVideo2-L14 and 6.7x faster than InternVideo2-S14 for mobile device inference. It achieves comparable zero-shot retrieval performance to InternVideo2-L14 and outperforms InternVideo2-S14 by 6.9% on MSR-VTT.

Conclusion: MobileViCLIP is an efficient and deployable video-text model that balances speed and accuracy, making it ideal for mobile platforms and demonstrating strong zero-shot retrieval capabilities.

Abstract: Efficient lightweight neural networks are with increasing attention due to
their faster reasoning speed and easier deployment on mobile devices. However,
existing video pre-trained models still focus on the common ViT architecture
with high latency, and few works attempt to build efficient architecture on
mobile devices. This paper bridges this gap by introducing temporal structural
reparameterization into an efficient image-text model and training it on a
large-scale high-quality video-text dataset, resulting in an efficient
video-text model that can run on mobile devices with strong zero-shot
classification and retrieval capabilities, termed as MobileViCLIP. In
particular, in terms of inference speed on mobile devices, our
MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster
than InternVideo2-S14. In terms of zero-shot retrieval performance, our
MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains
6.9\% better than InternVideo2-S14 on MSR-VTT. The code is available at
https://github.com/MCG-NJU/MobileViCLIP.

</details>


### [267] [DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding](https://arxiv.org/abs/2508.07313)
*Junyu Xiong,Yonghui Wang,Weichao Zhao,Chenyu Liu,Bing Yin,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: DocR1 uses the EviGRPO RL framework for improving multi-page document understanding in multimodal LLMs by retraining models through coarse-to-fine reasoning strategies and evidence-aware rewards.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of fine-grained visual comprehension and multi-hop reasoning in multi-page documents for multimodal LLMs.

Method: The approach involves a novel RL framework called EviGRPO, which leverages evidence-aware rewards to train a model to retrieve relevant pages first and use them for answer generation. It also incorporates a new annotation pipeline and curriculum learning for creating training datasets.

Result: DocR1 significantly outperforms prior methods on multi-page document tasks and achieves state-of-the-art performance while retaining strong single-page benchmark results.

Conclusion: The study presents a promising approach for improving reasoning and page retrieval in multimodal LLMs, advancing their capacity for complex multi-page document handling through a refined RL framework and unique datasets.

Abstract: Understanding multi-page documents poses a significant challenge for
multimodal large language models (MLLMs), as it requires fine-grained visual
comprehension and multi-hop reasoning across pages. While prior work has
explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,
its application to multi-page document understanding remains underexplored. In
this paper, we introduce DocR1, an MLLM trained with a novel RL framework,
Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware
reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the
model to first retrieve relevant pages before generating answers. This training
paradigm enables us to build high-quality models with limited supervision. To
support this, we design a two-stage annotation pipeline and a curriculum
learning strategy, based on which we construct two datasets: EviBench, a
high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation
benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments
across a wide range of benchmarks demonstrate that DocR1 achieves
state-of-the-art performance on multi-page tasks, while consistently
maintaining strong results on single-page benchmarks.

</details>


### [268] [RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning](https://arxiv.org/abs/2508.07318)
*Jinjing Gu,Tianbao Qin,Yuanyuan Pu,Zhengpeng Zhao*

Main category: cs.CV

TL;DR: This paper presents RORPCap, a novel image captioning model leveraging object and relation extraction and text-image retrieval to generate captions efficiently and effectively.


<details>
  <summary>Details</summary>
Motivation: To address issues like redundant detection, complexity in GCN setup, and high training costs in existing image captioning models.

Method: RORPCap extracts objects and relations, encodes them into prompt templates, enriches visual embeddings using CLIP and Mamba, and generates captions with GPT-2.

Result: Experiments on the MS-COCO dataset show RORPCap achieving a CIDEr score of 120.5% and SPICE of 22.0%, with 2.6 hours for training.

Conclusion: RORPCap offers a competitive, efficient, and cost-effective alternative to traditional object detector and GCN-based image captioning methods.

Abstract: Image captioning aims to generate natural language descriptions for input
images in an open-form manner. To accurately generate descriptions related to
the image, a critical step in image captioning is to identify objects and
understand their relations within the image. Modern approaches typically
capitalize on object detectors or combine detectors with Graph Convolutional
Network (GCN). However, these models suffer from redundant detection
information, difficulty in GCN construction, and high training costs. To
address these issues, a Retrieval-based Objects and Relations Prompt for Image
Captioning (RORPCap) is proposed, inspired by the fact that image-text
retrieval can provide rich semantic information for input images. RORPCap
employs an Objects and relations Extraction Model to extract object and
relation words from the image. These words are then incorporate into predefined
prompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping
network is designed to quickly map image embeddings extracted by CLIP to
visual-text embeddings. Finally, the resulting prompt embeddings and
visual-text embeddings are concatenated to form textual-enriched feature
embeddings, which are fed into a GPT-2 model for caption generation. Extensive
experiments conducted on the widely used MS-COCO dataset show that the RORPCap
requires only 2.6 hours under cross-entropy loss training, achieving 120.5%
CIDEr score and 22.0% SPICE score on the "Karpathy" test split. RORPCap
achieves comparable performance metrics to detector-based and GCN-based models
with the shortest training time and demonstrates its potential as an
alternative for image captioning.

</details>


### [269] [Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos](https://arxiv.org/abs/2508.07330)
*Tuyen Tran,Thao Minh Le,Quang-Hung Le,Truyen Tran*

Main category: cs.CV

TL;DR: The paper introduces Planner-Refiner, a novel framework for video-language alignment, focusing on refining space-time representations of visual elements using iterative language guidance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in video-language alignment, such as language complexity, evolving entities and actions, and the semantic gaps between vision and language.

Method: The Planner-Refiner framework decomposes complex linguistic prompts into simpler chains via the Planner module and refines vision-language alignment iteratively through the Refiner module, achieving single-step refinement and maintaining visual token representation.

Result: Planner-Refiner achieves superior performance on video-language tasks like Referring Video Object Segmentation and Temporal Grounding, especially for complex language prompts. A new MeViS-X benchmark further showcases its effectiveness.

Conclusion: The proposed Planner-Refiner framework significantly improves video-language alignment performance, particularly for complex and long language queries, establishing its potential over state-of-the-art methods.

Abstract: Vision-language alignment in video must address the complexity of language,
evolving interacting entities, their action chains, and semantic gaps between
language and vision. This work introduces Planner-Refiner, a framework to
overcome these challenges. Planner-Refiner bridges the semantic gap by
iteratively refining visual elements' space-time representation, guided by
language until semantic gaps are minimal. A Planner module schedules language
guidance by decomposing complex linguistic prompts into short sentence chains.
The Refiner processes each short sentence, a noun-phrase and verb-phrase pair,
to direct visual tokens' self-attention across space then time, achieving
efficient single-step refinement. A recurrent system chains these steps,
maintaining refined visual token representations. The final representation
feeds into task-specific heads for alignment generation. We demonstrate
Planner-Refiner's effectiveness on two video-language alignment tasks:
Referring Video Object Segmentation and Temporal Grounding with varying
language complexity. We further introduce a new MeViS-X benchmark to assess
models' capability with long queries. Superior performance versus
state-of-the-art methods on these benchmarks shows the approach's potential,
especially for complex prompts.

</details>


### [270] [CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation](https://arxiv.org/abs/2508.07341)
*Fangtai Wu,Mushui Liu,Weijie He,Wanggui He,Hao Jiang,Zhao Wang,Yunlong Yu*

Main category: cs.CV

TL;DR: CoAR enables effective subject and style customization in unified autoregressive models without parameter tuning, using minimal additional parameters and achieving high computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiencies and risks of overfitting in existing methods for customized image generation in unified autoregressive (AR) models.

Method: CoAR uses a Layerwise Multimodal Context Learning strategy to freeze pre-trained parameters, introduces regularization to avoid language drift, and offers training-free subject customization for user-provided styles.

Result: CoAR demonstrates superior performance in subject-driven and style personalization tasks, requires tuning less than 0.05% of parameters, and is computationally and memory-efficient.

Conclusion: CoAR proves to be a novel and efficient framework for personalized and style-based customization in autoregressive models, maintaining competitive performance and conserving resources.

Abstract: The unified autoregressive (AR) model excels at multimodal understanding and
generation, but its potential for customized image generation remains
underexplored. Existing customized generation methods rely on full fine-tuning
or adapters, making them costly and prone to overfitting or catastrophic
forgetting. In this paper, we propose \textbf{CoAR}, a novel framework for
injecting subject concepts into the unified AR models while keeping all
pre-trained parameters completely frozen. CoAR learns effective, specific
subject representations with only a minimal number of parameters using a
Layerwise Multimodal Context Learning strategy. To address overfitting and
language drift, we further introduce regularization that preserves the
pre-trained distribution and anchors context tokens to improve subject fidelity
and re-contextualization. Additionally, CoAR supports training-free subject
customization in a user-provided style. Experiments demonstrate that CoAR
achieves superior performance on both subject-driven personalization and style
personalization, while delivering significant gains in computational and memory
efficiency. Notably, CoAR tunes less than \textbf{0.05\%} of the parameters
while achieving competitive performance compared to recent Proxy-Tuning. Code:
https://github.com/KZF-kzf/CoAR

</details>


### [271] [SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal](https://arxiv.org/abs/2508.07346)
*Tingyu Yang,Jue Gong,Jinpei Guo,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: SODiff, a novel diffusion model, tackles JPEG artifact removal by introducing semantic-oriented guidance and adaptive optimization.


<details>
  <summary>Details</summary>
Motivation: JPEG compression often introduces severe visual artifacts at high ratios, with existing restoration methods failing to recover complex textures.

Method: SODiff leverages a semantic-aligned image prompt extractor for signal reconstruction and a quality factor-aware time predictor for adaptive denoising.

Result: SODiff achieves superior visual quality and quantitative performance compared to other leading JPEG artifact removal techniques.

Conclusion: SODiff effectively removes JPEG artifacts by combining semantic guidance and quality factor-aware optimization, outperforming existing methods.

Abstract: JPEG, as a widely used image compression standard, often introduces severe
visual artifacts when achieving high compression ratios. Although existing deep
learning-based restoration methods have made considerable progress, they often
struggle to recover complex texture details, resulting in over-smoothed
outputs. To overcome these limitations, we propose SODiff, a novel and
efficient semantic-oriented one-step diffusion model for JPEG artifacts
removal. Our core idea is that effective restoration hinges on providing
semantic-oriented guidance to the pre-trained diffusion model, thereby fully
leveraging its powerful generative prior. To this end, SODiff incorporates a
semantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features
from low-quality (LQ) images and projects them into an embedding space
semantically aligned with that of the text encoder. Simultaneously, it
preserves crucial information for faithful reconstruction. Furthermore, we
propose a quality factor-aware time predictor that implicitly learns the
compression quality factor (QF) of the LQ image and adaptively selects the
optimal denoising start timestep for the diffusion process. Extensive
experimental results show that our SODiff outperforms recent leading methods in
both visual quality and quantitative metrics. Code is available at:
https://github.com/frakenation/SODiff

</details>


### [272] [GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction](https://arxiv.org/abs/2508.07355)
*Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: GS4Buildings enhances Gaussian Splatting for urban building reconstruction by incorporating semantic 3D building models and prior-guided optimization.


<details>
  <summary>Details</summary>
Motivation: To address limitations in Gaussian Splatting for reconstructing large-scale urban scenes, specifically issues arising from occlusions and incomplete structures.

Method: The paper proposes initializing Gaussians from semantic 3D building models instead of traditional SfM pipelines and utilizes prior depth and normal maps from planar building geometry for optimization.

Result: GS4Buildings improves reconstruction completeness by 20.5% and geometric accuracy by 32.8%, reducing Gaussian primitives by 71.8% in building-focused mode.

Conclusion: Integrating semantic building models into GS enhances its capability for urban applications like smart cities and digital twins.

Abstract: Recent advances in Gaussian Splatting (GS) have demonstrated its
effectiveness in photo-realistic rendering and 3D reconstruction. Among these,
2D Gaussian Splatting (2DGS) is particularly suitable for surface
reconstruction due to its flattened Gaussian representation and integrated
normal regularization. However, its performance often degrades in large-scale
and complex urban scenes with frequent occlusions, leading to incomplete
building reconstructions. We propose GS4Buildings, a novel prior-guided
Gaussian Splatting method leveraging the ubiquity of semantic 3D building
models for robust and scalable building surface reconstruction. Instead of
relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings
initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic
3D building models. Moreover, we generate prior depth and normal maps from the
planar building geometry and incorporate them into the optimization process,
providing strong geometric guidance for surface consistency and structural
accuracy. We also introduce an optional building-focused mode that limits
reconstruction to building regions, achieving a 71.8% reduction in Gaussian
primitives and enabling a more efficient and compact representation.
Experiments on urban datasets demonstrate that GS4Buildings improves
reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These
results highlight the potential of semantic building model integration to
advance GS-based reconstruction toward real-world urban applications such as
smart cities and digital twins. Our project is available:
https://github.com/zqlin0521/GS4Buildings.

</details>


### [273] [Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring](https://arxiv.org/abs/2508.07369)
*Tianyu Xin,Jin-Liang Xiao,Zeyu Xia,Shan Yin,Liang-Jian Deng*

Main category: cs.CV

TL;DR: The paper introduces a method to enhance cross-sensor generalization in pansharpening models using modular decomposition and feature tailoring.


<details>
  <summary>Details</summary>
Motivation: Improving cross-sensor generalization in deep pansharpening models without requiring costly retraining or additional data.

Method: The approach involves modular decomposition to identify feature interfaces, incorporates a Feature Tailor module trained with unsupervised physics-aware losses, and uses efficient patch-wise training/testing.

Result: The proposed method demonstrates state-of-the-art quality and over 100x faster execution compared to zero-shot methods on real-world datasets.

Conclusion: The method effectively addresses cross-sensor degradation, offering substantial gains in generalization and computational efficiency.

Abstract: Deep learning methods for pansharpening have advanced rapidly, yet models
pretrained on data from a specific sensor often generalize poorly to data from
other sensors. Existing methods to tackle such cross-sensor degradation include
retraining model or zero-shot methods, but they are highly time-consuming or
even need extra training data. To address these challenges, our method first
performs modular decomposition on deep learning-based pansharpening models,
revealing a general yet critical interface where high-dimensional fused
features begin mapping to the channel space of the final image. % may need
revisement A Feature Tailor is then integrated at this interface to address
cross-sensor degradation at the feature level, and is trained efficiently with
physics-aware unsupervised losses. Moreover, our method operates in a
patch-wise manner, training on partial patches and performing parallel
inference on all patches to boost efficiency. Our method offers two key
advantages: (1) $\textit{Improved Generalization Ability}$: it significantly
enhance performance in cross-sensor cases. (2) $\textit{Low Generalization
Cost}$: it achieves sub-second training and inference, requiring only partial
test inputs and no external data, whereas prior methods often take minutes or
even hours. Experiments on the real-world data from multiple datasets
demonstrate that our method achieves state-of-the-art quality and efficiency in
tackling cross-sensor degradation. For example, training and inference of
$512\times512\times8$ image within $\textit{0.2 seconds}$ and
$4000\times4000\times8$ image within $\textit{3 seconds}$ at the fastest
setting on a commonly used RTX 3090 GPU, which is over 100 times faster than
zero-shot methods.

</details>


### [274] [DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery](https://arxiv.org/abs/2508.07372)
*Rajaei Khatib,Raja Giryes*

Main category: cs.CV

TL;DR: DIP-GS enhances sparse view reconstruction for 3D scene representations by incorporating a Deep Image Prior into the 3D Gaussian Splatting framework.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of 3D Gaussian Splatting in sparse view reconstruction scenarios, where input views are sparse and lack comprehensive scene coverage.

Method: Utilizes the Deep Image Prior (DIP) approach in a coarse-to-fine manner, relying solely on the input frames without pre-trained models.

Result: DIP-GS achieves state-of-the-art performance in sparse-view 3D reconstruction tasks, demonstrating superior capabilities compared to vanilla 3D Gaussian Splatting.

Conclusion: DIP-GS successfully extends the applicability of 3D Gaussian Splatting to sparse view scenarios using internal structure and patterns, bypassing the need for additional pre-trained models.

Abstract: 3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,
obtaining high-quality reconstruction with real-time rendering runtime
performance. The main idea behind 3DGS is to represent the scene as a
collection of 3D gaussians, while learning their parameters to fit the given
views of the scene. While achieving superior performance in the presence of
many views, 3DGS struggles with sparse view reconstruction, where the input
views are sparse and do not fully cover the scene and have low overlaps. In
this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By
using the DIP prior, which utilizes internal structure and patterns, with
coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla
3DGS fails, such as sparse view recovery. Note that our approach does not use
any pre-trained models such as generative models and depth estimation, but
rather relies only on the input frames. Among such methods, DIP-GS obtains
state-of-the-art (SOTA) competitive results on various sparse-view
reconstruction tasks, demonstrating its capabilities.

</details>


### [275] [LET-US: Long Event-Text Understanding of Scenes](https://arxiv.org/abs/2508.07401)
*Rui Chen,Xingyu Chen,Shaoan Wang,Shihan Kong,Junzhi Yu*

Main category: cs.CV

TL;DR: The paper introduces LET-US, a framework designed to enable Multimodal Large Language Models (MLLMs) to better comprehend long asynchronous event streams through adaptive compression and innovative feature reduction techniques.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with effectively interpreting asynchronous and extended event streams typically produced by event cameras, calling for a method to bridge the gap between event stream data and textual understanding.

Method: The proposed LET-US framework employs an adaptive compression mechanism, hierarchical clustering, similarity computation, and text-guided cross-modal queries to reduce input event volumes while retaining critical features. A two-stage optimization paradigm and a new event-text aligned dataset are also used.

Result: LET-US outperforms previous MLLMs in descriptive and semantic tasks, showcasing superior performance on benchmarks involving reasoning, captioning, classification, temporal localization, and moment retrieval for long event streams.

Conclusion: LET-US offers a novel solution for understanding long-duration event streams in a multimodal setting, establishing a breakthrough in cross-modal inference with publicly accessible datasets, codes, and models to further research in the domain.

Abstract: Event cameras output event streams as sparse, asynchronous data with
microsecond-level temporal resolution, enabling visual perception with low
latency and a high dynamic range. While existing Multimodal Large Language
Models (MLLMs) have achieved significant success in understanding and analyzing
RGB video content, they either fail to interpret event streams effectively or
remain constrained to very short sequences. In this paper, we introduce LET-US,
a framework for long event-stream--text comprehension that employs an adaptive
compression mechanism to reduce the volume of input events while preserving
critical visual details. LET-US thus establishes a new frontier in cross-modal
inferential understanding over extended event sequences. To bridge the
substantial modality gap between event streams and textual representations, we
adopt a two-stage optimization paradigm that progressively equips our model
with the capacity to interpret event-based scenes. To handle the voluminous
temporal information inherent in long event streams, we leverage text-guided
cross-modal queries for feature reduction, augmented by hierarchical clustering
and similarity computation to distill the most representative event features.
Moreover, we curate and construct a large-scale event-text aligned dataset to
train our model, achieving tighter alignment of event features within the LLM
embedding space. We also develop a comprehensive benchmark covering a diverse
set of tasks -- reasoning, captioning, classification, temporal localization
and moment retrieval. Experimental results demonstrate that LET-US outperforms
prior state-of-the-art MLLMs in both descriptive accuracy and semantic
comprehension on long-duration event streams. All datasets, codes, and models
will be publicly available.

</details>


### [276] [ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack](https://arxiv.org/abs/2508.07402)
*Rongxuan Peng,Shunquan Tan,Chenqi Kong,Anwei Luo,Alex C. Kot,Jiwu Huang*

Main category: cs.CV

TL;DR: The paper introduces ForensicsSAM, a robust framework for image forgery detection and localization (IFDL) that enhances resistance against adversarial attacks and improves detection and localization performance.


<details>
  <summary>Details</summary>
Motivation: Existing parameter-efficient fine-tuning (PEFT) approaches for adapting large vision foundation models to tasks like IFDL are highly vulnerable to adversarial attacks.

Method: ForensicsSAM incorporates three strategies: (1) adding forgery experts into the transformer blocks to enhance artifact detection, (2) designing a light-weight adversary detector to identify adversarial images based on RGB-domain artifacts, and (3) injecting adversary experts into global layers to counteract adversarial disturbances, activated adaptively by the detector.

Result: ForensicsSAM achieves superior performance against various adversarial attacks and sets new benchmarks in both image-level forgery detection and pixel-level forgery localization across multiple datasets.

Conclusion: The proposed ForensicsSAM framework successfully integrates adversarial robustness into PEFT for IFDL, providing a state-of-the-art solution for handling adversarial images while maintaining strong detection and localization capabilities.

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for
adapting large vision foundation models, such as the Segment Anything Model
(SAM) and LLaVA, to downstream tasks like image forgery detection and
localization (IFDL). However, existing PEFT-based approaches overlook their
vulnerability to adversarial attacks. In this paper, we show that highly
transferable adversarial images can be crafted solely via the upstream model,
without accessing the downstream model or training data, significantly
degrading the IFDL performance. To address this, we propose ForensicsSAM, a
unified IFDL framework with built-in adversarial robustness. Our design is
guided by three key ideas: (1) To compensate for the lack of forgery-relevant
knowledge in the frozen image encoder, we inject forgery experts into each
transformer block to enhance its ability to capture forgery artifacts. These
forgery experts are always activated and shared across any input images. (2) To
detect adversarial images, we design an light-weight adversary detector that
learns to capture structured, task-specific artifact in RGB domain, enabling
reliable discrimination across various attack methods. (3) To resist
adversarial attacks, we inject adversary experts into the global attention
layers and MLP modules to progressively correct feature shifts induced by
adversarial noise. These adversary experts are adaptively activated by the
adversary detector, thereby avoiding unnecessary interference with clean
images. Extensive experiments across multiple benchmarks demonstrate that
ForensicsSAM achieves superior resistance to various adversarial attack
methods, while also delivering state-of-the-art performance in image-level
forgery detection and pixel-level forgery localization. The resource is
available at https://github.com/siriusPRX/ForensicsSAM.

</details>


### [277] [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409)
*Junyao Gao,Jiaxing Li,Wenran Liu,Yanhong Zeng,Fei Shen,Kai Chen,Yanan Sun,Cairong Zhao*

Main category: cs.CV

TL;DR: The paper introduces CharacterShot, a framework for generating 4D character animations from a single reference image and 2D sequences, achieving controllable, dynamic, and consistent results.


<details>
  <summary>Details</summary>
Motivation: To empower designers in creating dynamic 4D character animations from minimal inputs while promoting controllability and spatial-temporal consistency.

Method: Combines a pre-trained DiT-based 2D animation model with 3D-lifting techniques using dual-attention modules and camera priors, along with a novel neighbor-constrained 4D Gaussian splatting optimization.

Result: The authors constructed a large-scale character dataset (Character4D) and benchmarks (CharacterBench). Their method outperforms existing state-of-the-art approaches in dynamic character generation.

Conclusion: CharacterShot offers an effective solution for dynamic and controllable 4D character generation, providing significant advancements over current techniques, alongside freely available datasets and codes.

Abstract: In this paper, we propose \textbf{CharacterShot}, a controllable and
consistent 4D character animation framework that enables any individual
designer to create dynamic 3D characters (i.e., 4D character animation) from a
single reference character image and a 2D pose sequence. We begin by
pretraining a powerful 2D character animation model based on a cutting-edge
DiT-based image-to-video model, which allows for any 2D pose sequnce as
controllable signal. We then lift the animation model from 2D to 3D through
introducing dual-attention module together with camera prior to generate
multi-view videos with spatial-temporal and spatial-view consistency. Finally,
we employ a novel neighbor-constrained 4D gaussian splatting optimization on
these multi-view videos, resulting in continuous and stable 4D character
representations. Moreover, to improve character-centric performance, we
construct a large-scale dataset Character4D, containing 13,115 unique
characters with diverse appearances and motions, rendered from multiple
viewpoints. Extensive experiments on our newly constructed benchmark,
CharacterBench, demonstrate that our approach outperforms current
state-of-the-art methods. Code, models, and datasets will be publicly available
at https://github.com/Jeoyal/CharacterShot.

</details>


### [278] [CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](https://arxiv.org/abs/2508.07413)
*Youqi Wang,Shunquan Tan,Rongxuan Peng,Bin Li,Jiwu Huang*

Main category: cs.CV

TL;DR: This paper introduces CLUE, a framework that repurposes a state-of-the-art text-to-image synthesis model, Stable Diffusion 3, for forensic analysis to localize high-fidelity image forgeries.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the proliferation of visually convincing digital image forgeries caused by the accessibility of image editing tools and generative AI, threatening media authenticity.

Method: CLUE leverages Stable Diffusion 3's Rectified Flow mechanism and employs Low-Rank Adaptation (LoRA) to amplify forgery-related inconsistencies in latent representations. It also integrates the Segment Anything Model for analyzing contextual semantic features and spatial localization.

Result: The proposed framework, CLUE, significantly outperformed prior methods in forgery localization, demonstrated strong generalization, and showcased robustness against post-processing attacks and challenges posed by online social networks.

Conclusion: CLUE provides a novel, efficient, and high-performing approach for identifying forgery in digital images, making it a valuable tool for enhancing forensic analysis amidst increasing generative AI-based threats.

Abstract: The increasing accessibility of image editing tools and generative AI has led
to a proliferation of visually convincing forgeries, compromising the
authenticity of digital media. In this paper, in addition to leveraging
distortions from conventional forgeries, we repurpose the mechanism of a
state-of-the-art (SOTA) text-to-image synthesis model by exploiting its
internal generative process, turning it into a high-fidelity forgery
localization tool. To this end, we propose CLUE (Capture Latent Uncovered
Evidence), a framework that employs Low- Rank Adaptation (LoRA) to
parameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic
feature extractor. Our approach begins with the strategic use of SD3's
Rectified Flow (RF) mechanism to inject noise at varying intensities into the
latent representation, thereby steering the LoRAtuned denoising process to
amplify subtle statistical inconsistencies indicative of a forgery. To
complement the latent analysis with high-level semantic context and precise
spatial details, our method incorporates contextual features from the image
encoder of the Segment Anything Model (SAM), which is parameter-efficiently
adapted to better trace the boundaries of forged regions. Extensive evaluations
demonstrate CLUE's SOTA generalization performance, significantly outperforming
prior methods. Furthermore, CLUE shows superior robustness against common
post-processing attacks and Online Social Networks (OSNs). Code is publicly
available at https://github.com/SZAISEC/CLUE.

</details>


### [279] [Freeze and Reveal: Exposing Modality Bias in Vision-Language Models](https://arxiv.org/abs/2508.07432)
*Vivek Hruday Kavuri,Vysishtya Karanam,Venkata Jahnavi Venkamsetty,Kriti Madumadukala,Lakshmipathi Balaji Darur,Ponnurangam Kumaraguru*

Main category: cs.CV

TL;DR: This paper addresses gender bias in Vision Language Models (VLMs) by identifying whether the bias originates more from the vision or text encoders and proposes low-cost debiasing methods to mitigate it.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the issue of gender bias in Vision Language Models stemming from both vision and text modalities and to enable more focused bias mitigation.

Method: The study employs Counterfactual Data Augmentation (CDA), Task Vector methods, and a novel method called Data Augmentation Using Degree of Stereotypicality (DAUDoS) to reduce bias. A gender-annotated dataset and the VisoGender benchmark are also used for evaluation.

Result: CDA reduces the gender bias gap by 6% and DAUDoS by 3%, while both methods improve gender identification in images by 3%. DAUDoS achieves results with only one-third of training data.

Conclusion: The findings indicate that bias is more prominent in CLIP’s vision encoder and PaliGemma2’s text encoder. The work enables targeted strategies for bias mitigation in multi-modal AI systems.

Abstract: Vision Language Models achieve impressive multi-modal performance but often
inherit gender biases from their training data. This bias might be coming from
both the vision and text modalities. In this work, we dissect the contributions
of vision and text backbones to these biases by applying targeted debiasing
using Counterfactual Data Augmentation and Task Vector methods. Inspired by
data-efficient approaches in hate-speech classification, we introduce a novel
metric, Degree of Stereotypicality and a corresponding debiasing method, Data
Augmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with
minimal computational cost. We curate a gender annotated dataset and evaluate
all methods on VisoGender benchmark to quantify improvements and identify
dominant source of bias. Our results show that CDA reduces the gender gap by 6%
and DAUDoS by 3% but using only one-third of the data. Both methods also
improve the model's ability to correctly identify gender in images by 3%, with
DAUDoS achieving this improvement using only almost one-third of training data.
From our experiment's, we observed that CLIP's vision encoder is more biased
whereas PaliGemma2's text encoder is more biased. By identifying whether bias
stems more from vision or text encoders, our work enables more targeted and
effective bias mitigation strategies in future multi-modal systems.

</details>


### [280] [Levarging Learning Bias for Noisy Anomaly Detection](https://arxiv.org/abs/2508.07441)
*Yuxin Zhang,Yunkang Cao,Yuqi Cheng,Yihan Sun,Weiming Shen*

Main category: cs.CV

TL;DR: The paper proposes a two-stage framework for fully unsupervised image anomaly detection (FUIAD) addressing challenges of noisy, anomaly-contaminated training data.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods struggle with real-world data contamination, as they assume anomaly-free training datasets. This leads to poor performance when anomalies are treated as normal.

Method: The authors exploit inherent learning bias in models to filter anomalies from training data. In stage 1, training data is split, sub-models are trained, and anomalies are detected via cross-model scores. In stage 2, the final model is trained on the filtered data.

Result: The proposed framework achieves state-of-the-art anomaly detection and localization performance on the Real-IAD benchmark, demonstrating robustness against dataset contamination.

Conclusion: The model-agnostic framework provides a practical solution for real-world FUIAD scenarios by leveraging learning biases and enhancing resilience to imperfect training data.

Abstract: This paper addresses the challenge of fully unsupervised image anomaly
detection (FUIAD), where training data may contain unlabeled anomalies.
Conventional methods assume anomaly-free training data, but real-world
contamination leads models to absorb anomalies as normal, degrading detection
performance. To mitigate this, we propose a two-stage framework that
systematically exploits inherent learning bias in models. The learning bias
stems from: (1) the statistical dominance of normal samples, driving models to
prioritize learning stable normal patterns over sparse anomalies, and (2)
feature-space divergence, where normal data exhibit high intra-class
consistency while anomalies display high diversity, leading to unstable model
responses. Leveraging the learning bias, stage 1 partitions the training set
into subsets, trains sub-models, and aggregates cross-model anomaly scores to
filter a purified dataset. Stage 2 trains the final detector on this dataset.
Experiments on the Real-IAD benchmark demonstrate superior anomaly detection
and localization performance under different noise conditions. Ablation studies
further validate the framework's contamination resilience, emphasizing the
critical role of learning bias exploitation. The model-agnostic design ensures
compatibility with diverse unsupervised backbones, offering a practical
solution for real-world scenarios with imperfect training data. Code is
available at https://github.com/hustzhangyuxin/LLBNAD.

</details>


### [281] [Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines](https://arxiv.org/abs/2508.07450)
*Suman Kunwar,Prabesh Rai*

Main category: cs.CV

TL;DR: The study compares HCW classification models, finding YOLOv5-s to achieve the highest accuracy (95.06%) for better waste segregation, aiding Nepal's health care waste management.


<details>
  <summary>Details</summary>
Motivation: To address challenges in managing health care waste in Nepal through advanced waste classification for better segregation and reduced contamination risks.

Method: Benchmarked models (ResNeXt-50, EfficientNet-B0, MobileNetV3-S, YOLOv8-n, YOLOv5-s) using Stratified K-fold techniques on HCW data, evaluated for accuracy, inference speed, and statistical significance.

Result: YOLOv5-s achieved 95.06% accuracy but slightly slower inference speed than YOLOv8-n, while EfficientNet-B0 performed well (93.22% accuracy) but was slower.

Conclusion: Deployed YOLOv5-s model to a web platform with mapped bin colors complying with Nepal’s HCW standards; further localized data processing was suggested.

Abstract: The increasing number of Health Care facilities in Nepal has also added up
the challenges on managing health care waste (HCW). Improper segregation and
disposal of HCW leads to the contamination, spreading of infectious diseases
and puts a risk of waste handlers. This study benchmarks the state of the art
waste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,
YOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds
on combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%
accuracy but fell short few milliseconds in inference speed with YOLOv8-n
model. The EfficientNet-B0 showed promising results of 93.22% accuracy but took
the highest inference time. A repetitive ANOVA was performed to see statistical
significance and the best performing model (YOLOv5-s) was deployed to the web
with mapped bin color using Nepal's HCW management standards for public usage.
Further work on the data was suggested along with localized context.

</details>


### [282] [AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning](https://arxiv.org/abs/2508.07470)
*Siminfar Samakoush Galougah,Rishie Raj,Sanjoy Chowdhury,Sayan Nag,Ramani Duraiswami*

Main category: cs.CV

TL;DR: AURA is a proposed benchmark for assessing the cross-modal reasoning capabilities of Audio-Visual Large Language Models (AV-LLMs) and Omni-modal Language Models (OLMs). It focuses on reasoning fidelity and introduces the novel AuraScore metric to evaluate this aspect.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for AV models prioritize accuracy without assessing whether models derive answers through valid reasoning, making it difficult to identify flawed comprehension or hallucinations.

Method: The authors introduce AURA, a benchmark featuring questions across cognitive domains that are intentionally unanswerable with single modalities. They also propose the AuraScore metric to evaluate reasoning fidelity by measuring Factual Consistency and Core Inference in model reasoning.

Result: State-of-the-art models achieve high accuracy (up to 92%) on AURA tasks but score poorly (below 45%) in Factual Consistency and Core Inference, revealing critical reasoning gaps.

Conclusion: The results underscore the need for benchmarks like AURA to expose reasoning flaws in models and contribute to the development of more robust multimodal evaluation techniques.

Abstract: Current audio-visual (AV) benchmarks focus on final answer accuracy,
overlooking the underlying reasoning process. This makes it difficult to
distinguish genuine comprehension from correct answers derived through flawed
reasoning or hallucinations. To address this, we introduce AURA (Audio-visual
Understanding and Reasoning Assessment), a benchmark for evaluating the
cross-modal reasoning capabilities of Audio-Visual Large Language Models
(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across
six challenging cognitive domains, such as causality, timbre and pitch, tempo
and AV synchronization, unanswerability, implicit distractions, and skill
profiling, explicitly designed to be unanswerable from a single modality. This
forces models to construct a valid logical path grounded in both audio and
video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To
assess reasoning traces, we propose a novel metric, AuraScore, which addresses
the lack of robust tools for evaluating reasoning fidelity. It decomposes
reasoning into two aspects: (i) Factual Consistency - whether reasoning is
grounded in perceptual evidence, and (ii) Core Inference - the logical validity
of each reasoning step. Evaluations of SOTA models on AURA reveal a critical
reasoning gap: although models achieve high accuracy (up to 92% on some tasks),
their Factual Consistency and Core Inference scores fall below 45%. This
discrepancy highlights that models often arrive at correct answers through
flawed logic, underscoring the need for our benchmark and paving the way for
more robust multimodal evaluation.

</details>


### [283] [Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution](https://arxiv.org/abs/2508.07483)
*Pranav Chougule*

Main category: cs.CV

TL;DR: This paper compares photogrammetry and Gaussian Splatting for 3D reconstruction and view synthesis, demonstrating Gaussian Splatting's potential for high-quality view generation and enhanced photogrammetry models.


<details>
  <summary>Details</summary>
Motivation: To explore and compare the strengths and limitations of photogrammetry and Gaussian Splatting in 3D model reconstruction and view synthesis for applications in XR, photogrammetry, and autonomous vehicle simulations.

Method: The study involves creating a dataset from a real-world scene, constructing models using both techniques, comparing performance with metrics like SSIM, PSNR, LPIPS, and lp/mm resolution, and using a modified Gaussian Splatting repository for novel view synthesis.

Result: Gaussian Splatting effectively generates high-quality novel views and shows potential to improve photogrammetry-based 3D reconstructions, as demonstrated in comparisons using both original and augmented datasets.

Conclusion: The comparative analysis confirms Gaussian Splatting as a flexible and promising approach for view synthesis, capable of enhancing traditional photogrammetry methods for 3D reconstruction in various applications.

Abstract: In this paper, I present a comprehensive study comparing Photogrammetry and
Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I
created a dataset of images from a real-world scene and constructed 3D models
using both methods. To evaluate the performance, I compared the models using
structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned
perceptual image patch similarity (LPIPS), and lp/mm resolution based on the
USAF resolution chart. A significant contribution of this work is the
development of a modified Gaussian Splatting repository, which I forked and
enhanced to enable rendering images from novel camera poses generated in the
Blender environment. This innovation allows for the synthesis of high-quality
novel views, showcasing the flexibility and potential of Gaussian Splatting. My
investigation extends to an augmented dataset that includes both original
ground images and novel views synthesized via Gaussian Splatting. This
augmented dataset was employed to generate a new photogrammetry model, which
was then compared against the original photogrammetry model created using only
the original images. The results demonstrate the efficacy of using Gaussian
Splatting to generate novel high-quality views and its potential to improve
photogrammetry-based 3D reconstructions. The comparative analysis highlights
the strengths and limitations of both approaches, providing valuable
information for applications in extended reality (XR), photogrammetry, and
autonomous vehicle simulations. Code is available at
https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.

</details>


### [284] [VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding](https://arxiv.org/abs/2508.07493)
*Jian Chen,Ming Li,Jihyung Kil,Chenguang Wang,Tong Yu,Ryan Rossi,Tianyi Zhou,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: The paper introduces VisR-Bench, a multilingual benchmark for question-driven multimodal retrieval across long documents spanning sixteen languages, evaluating multiple retrieval models and highlighting challenges in structured data and low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus narrowly on English-only or single-page multilingual question-answering, creating a gap in the comprehensive multilingual visual retrieval of long documents.

Method: The authors developed VisR-Bench with 35K QA pairs across 1.2K documents, covering three question types (figures, text, and tables) in sixteen languages. They included queries without explicit answers to avoid reliance on keyword matching and evaluated various retrieval models.

Result: Findings reveal that while MLLMs outperform text-based and multimodal encoder models, they face issues with structured tables and low-resource languages.

Conclusion: VisR-Bench enables fine-grained evaluation for multilingual retrieval tasks, but further development is necessary to address challenges with structured data and language barriers.

Abstract: Most organizational data in this world are stored as documents, and visual
retrieval plays a crucial role in unlocking the collective intelligence from
all these documents. However, existing benchmarks focus on English-only
document retrieval or only consider multilingual question-answering on a
single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual
benchmark designed for question-driven multimodal retrieval in long documents.
Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents,
enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans
sixteen languages with three question types (figures, text, and tables),
offering diverse linguistic and question coverage. Unlike prior datasets, we
include queries without explicit answers, preventing models from relying on
superficial keyword matching. We evaluate various retrieval models, including
text-based methods, multimodal encoders, and MLLMs, providing insights into
their strengths and limitations. Our results show that while MLLMs
significantly outperform text-based and multimodal encoder models, they still
struggle with structured tables and low-resource languages, highlighting key
challenges in multilingual visual retrieval.

</details>


### [285] [FormCoach: Lift Smarter, Not Harder](https://arxiv.org/abs/2508.07501)
*Xiaoye Zuo,Nikos Athanasiou,Ginger Delmas,Yiming Huang,Xingyu Fu,Lingjie Liu*

Main category: cs.CV

TL;DR: The paper introduces FormCoach, an AI-driven fitness coaching tool leveraging vision-language models to spot and correct exercise form errors in real-time. The authors benchmark its performance, release a dataset, and highlight gaps compared to human coaches.


<details>
  <summary>Details</summary>
Motivation: Many at-home fitness enthusiasts lack access to expert feedback, which is critical for proper form and preventing injury. The study aims to bridge this gap with an interactive AI solution.

Method: The authors developed FormCoach, which uses vision-language models to analyze exercise forms. They evaluated its capabilities using a dataset of 1,700 expert-annotated videos and compared different AI models through a standardized evaluation pipeline.

Result: The analysis showed that current VLMs underperform compared to human-level coaching, indicating room for improvement in AI-based movement analysis.

Conclusion: FormCoach showcases the potential of combining human expertise and AI in collaborative ways for fitness coaching, but emphasizes the challenges that remain in achieving human-level performance.

Abstract: Good form is the difference between strength and strain, yet for the
fast-growing community of at-home fitness enthusiasts, expert feedback is often
out of reach. FormCoach transforms a simple camera into an always-on,
interactive AI training partner, capable of spotting subtle form errors and
delivering tailored corrections in real time, leveraging vision-language models
(VLMs). We showcase this capability through a web interface and benchmark
state-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference
video pairs spanning 22 strength and mobility exercises. To accelerate research
in AI-driven coaching, we release both the dataset and an automated,
rubric-based evaluation pipeline, enabling standardized comparison across
models. Our benchmarks reveal substantial gaps compared to human-level
coaching, underscoring both the challenges and opportunities in integrating
nuanced, context-aware movement analysis into interactive AI systems. By
framing form correction as a collaborative and creative process between humans
and machines, FormCoach opens a new frontier in embodied AI.

</details>


### [286] [From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials](https://arxiv.org/abs/2508.07514)
*Artzai Picon,Itziar Eguskiza,Daniel Mugica,Javier Romero,Carlos Javier Jimenez,Eric White,Gabriel Do-Lago-Junqueira,Christian Klukas,Ramon Navarra-Mestre*

Main category: cs.CV

TL;DR: This research introduces a model improving automated herbicide field trial evaluation, enhancing efficiency and reliability in species and damage classification across devices and locations.


<details>
  <summary>Details</summary>
Motivation: Traditional manual visual assessments for herbicide effects are tedious, subjective, and labor-intensive. Automating this process is desirable but challenging due to subtle visual differences in crops and weeds.

Method: The paper introduces an enhanced segmentation model utilizing a self-supervised visual framework combined with hierarchical inference based on botanical taxonomy, tested on comprehensive datasets from multiple years and geographies, including domain-shift evaluations.

Result: The model showed marked improvements in species and damage identification metrics (F1-scores and R-squared values) and demonstrated robustness under domain shifts.

Conclusion: The model provides a scalable, automated solution for herbicide evaluation, demonstrating resilience across platforms and geographies. It is operational within BASF's phenotyping pipeline for large-scale crop and weed monitoring.

Abstract: Field trials are vital in herbicide research and development to assess
effects on crops and weeds under varied conditions. Traditionally, evaluations
rely on manual visual assessments, which are time-consuming, labor-intensive,
and subjective. Automating species and damage identification is challenging due
to subtle visual differences, but it can greatly enhance efficiency and
consistency.
  We present an improved segmentation model combining a general-purpose
self-supervised visual model with hierarchical inference based on botanical
taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain
using digital and mobile cameras, the model was tested on digital camera data
(year 2023) and drone imagery from the United States, Germany, and Spain (year
2024) to evaluate robustness under domain shift. This cross-device evaluation
marks a key step in assessing generalization across platforms of the model.
  Our model significantly improved species identification (F1-score: 0.52 to
0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to
0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone
images), it maintained strong performance with moderate degradation (species:
F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where
earlier models failed.
  These results confirm the model's robustness and real-world applicability. It
is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated
crop and weed monitoring across diverse geographies.

</details>


### [287] [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/abs/2508.07519)
*Joonghyuk Shin,Alchan Hwang,Yujin Kim,Daneul Kim,Jaesik Park*

Main category: cs.CV

TL;DR: The paper explores multimodal diffusion transformers (MM-DiT), enhancing bidirectional interaction between text and image, and proposes a new image editing method.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing unidirectional cross-attention mechanisms and adapt editing techniques for MM-DiT's unified attention architecture.

Method: Analyzed MM-DiT's attention mechanism by decomposing attention matrices into blocks; proposed a robust, prompt-based editing method for MM-DiT covering global to local edits.

Result: The new method supports effective global-to-local edits for MM-DiT variants, including few-step models.

Conclusion: The study advances understanding of MM-DiT’s behavior and bridges the gap between older U-Net-based models and next-gen architectures.

Abstract: Transformer-based diffusion models have recently superseded traditional U-Net
architectures, with multimodal diffusion transformers (MM-DiT) emerging as the
dominant approach in state-of-the-art models like Stable Diffusion 3 and
Flux.1. Previous approaches have relied on unidirectional cross-attention
mechanisms, with information flowing from text embeddings to image latents. In
contrast, MMDiT introduces a unified attention mechanism that concatenates
input projections from both modalities and performs a single full attention
operation, allowing bidirectional information flow between text and image
branches. This architectural shift presents significant challenges for existing
editing techniques. In this paper, we systematically analyze MM-DiT's attention
mechanism by decomposing attention matrices into four distinct blocks,
revealing their inherent characteristics. Through these analyses, we propose a
robust, prompt-based image editing method for MM-DiT that supports global to
local edits across various MM-DiT variants, including few-step models. We
believe our findings bridge the gap between existing U-Net-based methods and
emerging architectures, offering deeper insights into MMDiT's behavioral
patterns.

</details>


### [288] [Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module](https://arxiv.org/abs/2508.07528)
*Xiaotong Ji,Ryoma Bise,Seiichi Uchida*

Main category: cs.CV

TL;DR: The study introduces a novel improvement to top-rank learning for medical imaging by integrating a rejection module to address noisy labels and class-ambiguous cases, improving diagnosis accuracy.


<details>
  <summary>Details</summary>
Motivation: Top-rank learning in medical imaging is hindered by noisy labels and ambiguous cases, which impact diagnosis accuracy by placing irrelevant data among the top instances.

Method: The researchers proposed combining a rejection module with top-rank loss to identify and address outliers. This module operates as a separate branch using a rejection function that evaluates deviation from the norm.

Result: Experimental tests on a medical dataset showed the method successfully detected and mitigated outliers, leading to improved diagnostic reliability and accuracy.

Conclusion: Enhancing top-rank learning with a rejection module proves effective in overcoming challenges like noisy labels, resulting in more accurate and reliable medical image diagnoses.

Abstract: In medical image processing, accurate diagnosis is of paramount importance.
Leveraging machine learning techniques, particularly top-rank learning, shows
significant promise by focusing on the most crucial instances. However,
challenges arise from noisy labels and class-ambiguous instances, which can
severely hinder the top-rank objective, as they may be erroneously placed among
the top-ranked instances. To address these, we propose a novel approach that
enhances toprank learning by integrating a rejection module. Cooptimized with
the top-rank loss, this module identifies and mitigates the impact of outliers
that hinder training effectiveness. The rejection module functions as an
additional branch, assessing instances based on a rejection function that
measures their deviation from the norm. Through experimental validation on a
medical dataset, our methodology demonstrates its efficacy in detecting and
mitigating outliers, improving the reliability and accuracy of medical image
diagnoses.

</details>


### [289] [Enhanced Generative Structure Prior for Chinese Text Image Super-resolution](https://arxiv.org/abs/2508.07537)
*Xiaoming Li,Wangmeng Zuo,Chen Change Loy*

Main category: cs.CV

TL;DR: This paper proposes a framework to enhance super-resolution (SR) for Chinese text images using structure priors integrated with a StyleGAN model, ensuring accurate restoration of degraded characters while accommodating diverse font styles and layouts.


<details>
  <summary>Details</summary>
Motivation: Existing text SR methods focus on English scripts and often struggle with the complexities of Chinese characters, which feature unique strokes and styles. This paper aims to address these challenges by developing a high-quality SR framework for Chinese text.

Method: The framework integrates a novel structure prior into a StyleGAN model to guide SR. It uses a codebook-based approach to restrict the generative space, with each code representing specific character structures and the StyleGAN vector $w$ controlling style attributes. This ensures unity between LR and HR structures during restoration.

Result: Experiments show that the proposed structure prior significantly improves the restoration of clear strokes in degraded low-resolution Chinese characters, even in cases with irregular layouts and diverse font styles. Robust performance was demonstrated in real-world scenarios.

Conclusion: The proposed framework effectively enhances SR for Chinese text images, utilizing structure priors to ensure faithful restoration of strokes while supporting diverse font styles and layouts. The work establishes a robust method for complex scripts beyond English text.

Abstract: Faithful text image super-resolution (SR) is challenging because each
character has a unique structure and usually exhibits diverse font styles and
layouts. While existing methods primarily focus on English text, less attention
has been paid to more complex scripts like Chinese. In this paper, we introduce
a high-quality text image SR framework designed to restore the precise strokes
of low-resolution (LR) Chinese characters. Unlike methods that rely on
character recognition priors to regularize the SR task, we propose a novel
structure prior that offers structure-level guidance to enhance visual quality.
Our framework incorporates this structure prior within a StyleGAN model,
leveraging its generative capabilities for restoration. To maintain the
integrity of character structures while accommodating various font styles and
layouts, we implement a codebook-based mechanism that restricts the generative
space of StyleGAN. Each code in the codebook represents the structure of a
specific character, while the vector $w$ in StyleGAN controls the character's
style, including typeface, orientation, and location. Through the collaborative
interaction between the codebook and style, we generate a high-resolution
structure prior that aligns with LR characters both spatially and structurally.
Experiments demonstrate that this structure prior provides robust,
character-specific guidance, enabling the accurate restoration of clear strokes
in degraded characters, even for real-world LR Chinese text with irregular
layouts. Our code and pre-trained models will be available at
https://github.com/csxmli2016/MARCONetPlusPlus

</details>


### [290] [A DICOM Image De-identification Algorithm in the MIDI-B Challenge](https://arxiv.org/abs/2508.07538)
*Hongzhu Jiang,Sihan Xie,Zhiyu Wan*

Main category: cs.CV

TL;DR: The paper discusses efforts to de-identify medical images in DICOM format to meet privacy standards, highlighting a successful algorithm from a challenge with a 99.92% success rate.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure compliance with privacy regulations (like HIPAA) by de-identifying medical images while maintaining their usefulness for research and diagnostics.

Method: The paper describes a de-identification algorithm employing techniques such as pixel masking, date shifting, text recognition, and replacement, tested during the MIDI-B Challenge.

Result: The presented solution achieved 99.92% success in de-identification tasks and ranked 2nd in the challenge, showing its high reliability.

Conclusion: The approach demonstrates significant success but highlights current limitations in de-identification methods, advocating for continued refinement and innovation.

Abstract: Image de-identification is essential for the public sharing of medical
images, particularly in the widely used Digital Imaging and Communications in
Medicine (DICOM) format as required by various regulations and standards,
including Health Insurance Portability and Accountability Act (HIPAA) privacy
rules, the DICOM PS3.15 standard, and best practices recommended by the Cancer
Imaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)
Challenge at the 27th International Conference on Medical Image Computing and
Computer Assisted Intervention (MICCAI 2024) was organized to evaluate
rule-based DICOM image de-identification algorithms with a large dataset of
clinical DICOM images. In this report, we explore the critical challenges of
de-identifying DICOM images, emphasize the importance of removing personally
identifiable information (PII) to protect patient privacy while ensuring the
continued utility of medical data for research, diagnostics, and treatment, and
provide a comprehensive overview of the standards and regulations that govern
this process. Additionally, we detail the de-identification methods we applied
- such as pixel masking, date shifting, date hashing, text recognition, text
replacement, and text removal - to process datasets during the test phase in
strict compliance with these standards. According to the final leaderboard of
the MIDI-B challenge, the latest version of our solution algorithm correctly
executed 99.92% of the required actions and ranked 2nd out of 10 teams that
completed the challenge (from a total of 22 registered teams). Finally, we
conducted a thorough analysis of the resulting statistics and discussed the
limitations of current approaches and potential avenues for future improvement.

</details>


### [291] [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
*Weitai Kang,Weiming Zhuang,Zhizhong Li,Yan Yan,Lingjuan Lyu*

Main category: cs.CV

TL;DR: The paper conducts a systematic study to improve visual grounding (VG) capabilities in Multimodal Large Language Models (MLLMs), using LLaVA-1.5 as a foundation.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the lack of systematic verification for design choices in fine-tuning MLLMs for the visual grounding (VG) task.

Method: The study explores various design paradigms for VG in MLLMs and performs ablation studies on VG task datasets to optimize fine-tuning.

Result: Improvements of +5.6% / +6.9% / +7.0% were achieved on benchmarks like RefCOCO/+/g over the LLaVA-1.5 model.

Conclusion: The research identifies effective design choices and provides insights to strengthen MLLMs in visual grounding, showcasing transferable findings for broader architectures.

Abstract: Fine-grained multimodal capability in Multimodal Large Language Models
(MLLMs) has emerged as a critical research direction, particularly for tackling
the visual grounding (VG) problem. Despite the strong performance achieved by
existing approaches, they often employ disparate design choices when
fine-tuning MLLMs for VG, lacking systematic verification to support these
designs. To bridge this gap, this paper presents a comprehensive study of
various design choices that impact the VG performance of MLLMs. We conduct our
analysis using LLaVA-1.5, which has been widely adopted in prior empirical
studies of MLLMs. While more recent models exist, we follow this convention to
ensure our findings remain broadly applicable and extendable to other
architectures. We cover two key aspects: (1) exploring different visual
grounding paradigms in MLLMs, identifying the most effective design, and
providing our insights; and (2) conducting ablation studies on the design of
grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our
findings contribute to a stronger MLLM for VG, achieving improvements of +5.6%
/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

</details>


### [292] [Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning](https://arxiv.org/abs/2508.07539)
*Yuki Shigeyasu,Shota Harada,Akihiko Yoshizawa,Kazuhiro Terada,Naoki Nakazima,Mariyo Kurata,Hiroyuki Abe,Tetsuo Ushiku,Ryoma Bise*

Main category: cs.CV

TL;DR: The paper proposes a domain generalization method for pathological image analysis that handles intra-hospital domain shifts in whole slide images, avoiding reliance on multi-hospital data.


<details>
  <summary>Details</summary>
Motivation: To address domain shifts in pathological images within whole slide images (WSIs), driven by patient characteristics and tissue thickness, without relying on multi-hospital data due to practical challenges.

Method: The method involves clustering WSI-level features from non-tumor regions to define domains and applying contrastive learning at both WSI-level and patch-level to reduce feature gaps between different clusters.

Result: The two-stage contrastive learning approach minimizes domain shifts within WSIs, effectively leveraging intra-hospital variations.

Conclusion: The proposed approach offers a practical solution to manage domain shifts in pathological images, facilitating improved analysis without requiring diverse multi-hospital datasets.

Abstract: In this paper, we address domain shifts in pathological images by focusing on
shifts within whole slide images~(WSIs), such as patient characteristics and
tissue thickness, rather than shifts between hospitals. Traditional approaches
rely on multi-hospital data, but data collection challenges often make this
impractical. Therefore, the proposed domain generalization method captures and
leverages intra-hospital domain shifts by clustering WSI-level features from
non-tumor regions and treating these clusters as domains. To mitigate domain
shift, we apply contrastive learning to reduce feature gaps between WSI pairs
from different clusters. The proposed method introduces a two-stage contrastive
learning approach WSI-level and patch-level contrastive learning to minimize
these gaps effectively.

</details>


### [293] [CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts](https://arxiv.org/abs/2508.07540)
*Junuk Cha,Jihyeon Kim*

Main category: cs.CV

TL;DR: The paper presents CoT-Pose, a novel approach for generating 3D human poses using high-level abstract language inputs by applying chain-of-thought (CoT) reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-pose models require detailed prompts that describe joint configurations, which differ from how humans use abstract language to express actions. This creates a challenge for real-world applications of pose generation.

Method: The authors integrate CoT reasoning into pose generation and propose a data synthesis pipeline to generate triplets of abstract prompts, detailed prompts, and corresponding 3D poses for training.

Result: The model, CoT-Pose, generates semantically aligned and realistic 3D human poses from abstract textual prompts.

Conclusion: The study underscores the value of reasoning-based approaches for human pose generation and demonstrates the effectiveness of CoT-Pose in bridging the gap between high-level abstractions and accurate 3D pose generation.

Abstract: Recent advances in multi-modal large language models (MLLMs) and
chain-of-thought (CoT) reasoning have led to significant progress in image and
text generation tasks. However, the field of 3D human pose generation still
faces critical limitations. Most existing text-to-pose models rely heavily on
detailed (low-level) prompts that explicitly describe joint configurations. In
contrast, humans tend to communicate actions and intentions using abstract
(high-level) language. This mismatch results in a practical challenge for
deploying pose generation systems in real-world scenarios. To bridge this gap,
we introduce a novel framework that incorporates CoT reasoning into the pose
generation process, enabling the interpretation of abstract prompts into
accurate 3D human poses. We further propose a data synthesis pipeline that
automatically generates triplets of abstract prompts, detailed prompts, and
corresponding 3D poses for training process. Experimental results demonstrate
that our reasoning-enhanced model, CoT-Pose, can effectively generate plausible
and semantically aligned poses from abstract textual inputs. This work
highlights the importance of high-level understanding in pose generation and
opens new directions for reasoning-enhanced approach for human pose generation.

</details>


### [294] [Commentary Generation for Soccer Highlights](https://arxiv.org/abs/2508.07543)
*Chidaksh Ravuru*

Main category: cs.CV

TL;DR: This paper extends MatchVoice for generating soccer highlight commentaries using the GOAL dataset, aiming for improved video-commentary alignment.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fine-grained alignment in automated soccer commentary generation and improve commentary synchronization with video content.

Method: Utilizing the MatchVoice model for commentary on short soccer clips (GOAL dataset) and conducting experiments on training configurations, hardware limitations, and zero-shot setups.

Result: MatchVoice shows improved synchronization and generalization for soccer highlight commentaries, but demonstrates potential areas for enhancement.

Conclusion: Integrating broader video-language techniques is necessary to further elevate the alignment and performance of automated sports commentary systems.

Abstract: Automated soccer commentary generation has evolved from template-based
systems to advanced neural architectures, aiming to produce real-time
descriptions of sports events. While frameworks like SoccerNet-Caption laid
foundational work, their inability to achieve fine-grained alignment between
video content and commentary remains a significant challenge. Recent efforts
such as MatchTime, with its MatchVoice model, address this issue through coarse
and fine-grained alignment techniques, achieving improved temporal
synchronization. In this paper, we extend MatchVoice to commentary generation
for soccer highlights using the GOAL dataset, which emphasizes short clips over
entire games. We conduct extensive experiments to reproduce the original
MatchTime results and evaluate our setup, highlighting the impact of different
training configurations and hardware limitations. Furthermore, we explore the
effect of varying window sizes on zero-shot performance. While MatchVoice
exhibits promising generalization capabilities, our findings suggest the need
for integrating techniques from broader video-language domains to further
enhance performance. Our code is available at
https://github.com/chidaksh/SoccerCommentary.

</details>


### [295] [Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning](https://arxiv.org/abs/2508.07548)
*Takehiro Yamane,Itaru Tsuge,Susumu Saito,Ryoma Bise*

Main category: cs.CV

TL;DR: The paper introduces a pseudo-labeling method leveraged by PU learning for effective medical image segmentation, focusing on individual images to differentiate foreground and background regions.


<details>
  <summary>Details</summary>
Motivation: To enhance medical image segmentation by addressing challenges in learning from unlabeled data through effective pseudo-label selection.

Method: The authors utilize Positive and Unlabeled Learning (PU learning) for binary classification, enabling discrimination between foreground and background regions for pseudo-labeling at the individual image level.

Result: Experiments demonstrate the method's effectiveness in selecting pseudo-labels and improving segmentation outcomes compared to traditional approaches.

Conclusion: The proposed PU learning-based pseudo-labeling approach provides an effective pathway for refining medical image segmentation, making it versatile for application across various unlabeled image datasets.

Abstract: This paper proposes a novel pseudo-labeling method for medical image
segmentation that can perform learning on ``individual images'' to select
effective pseudo-labels. We introduce Positive and Unlabeled Learning (PU
learning), which uses only positive and unlabeled data for binary
classification problems, to obtain the appropriate metric for discriminating
foreground and background regions on each unlabeled image. Our PU learning
makes us easy to select pseudo-labels for various background regions. The
experimental results show the effectiveness of our method.

</details>


### [296] [Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring](https://arxiv.org/abs/2508.07552)
*Ludan Zhang,Sihan Wang,Yuqi Dai,Shuofei Qiao,Lei He*

Main category: cs.CV

TL;DR: This paper introduces an evaluation method for autonomous driving modules using a novel Feature Map Convergence Score (FMCS) and demonstrates its efficacy in improving model performance.


<details>
  <summary>Details</summary>
Motivation: The lack of explicit supervision signals for intermediate functional modules in end-to-end autonomous driving systems has led to limited interpretability and challenges in independent evaluation and training.

Method: The method involves a Feature Map Convergence Score (FMCS), a Dual-Granularity Dynamic Weighted Scoring System (DG-DWSS), and a CLIP-based Feature Map Quality Evaluation Network (CLIP-FMQE-Net) to evaluate feature map quality and enhance training.

Result: Integrating the proposed evaluation module into training improved 3D object detection performance by 3.89 percent in NDS, showcasing its effectiveness.

Conclusion: The proposed framework provides a comprehensive and interpretable evaluation method for feature maps, improving feature representation quality and overall performance in autonomous driving models.

Abstract: End-to-end models are emerging as the mainstream in autonomous driving
perception and planning. However, the lack of explicit supervision signals for
intermediate functional modules leads to opaque operational mechanisms and
limited interpretability, making it challenging for traditional methods to
independently evaluate and train these modules. Pioneering in the issue, this
study builds upon the feature map-truth representation similarity-based
evaluation framework and proposes an independent evaluation method based on
Feature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted
Scoring System (DG-DWSS) is constructed, formulating a unified quantitative
metric - Feature Map Quality Score - to enable comprehensive evaluation of the
quality of feature maps generated by functional modules. A CLIP-based Feature
Map Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining
feature-truth encoders and quality score prediction heads to enable real-time
quality analysis of feature maps generated by functional modules. Experimental
results on the NuScenes dataset demonstrate that integrating our evaluation
module into the training improves 3D object detection performance, achieving a
3.89 percent gain in NDS. These results verify the effectiveness of our method
in enhancing feature representation quality and overall model performance.

</details>


### [297] [Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation](https://arxiv.org/abs/2508.07557)
*Minghao Yin,Yukang Cao,Songyou Peng,Kai Han*

Main category: cs.CV

TL;DR: Splat4D introduces a novel framework producing high-fidelity 4D content from monocular videos, achieving state-of-the-art spatial-temporal coherence and versatility in applications.


<details>
  <summary>Details</summary>
Motivation: Generating high-quality 4D content from monocular videos is challenging due to issues like ensuring temporal consistency, preserving details, and integrating user guidance.

Method: The framework employs multi-view rendering, inconsistency identification, a video diffusion model, and an asymmetric U-Net for refinement.

Result: Extensive evaluations on public benchmarks confirm Splat4D shows state-of-the-art performance across multiple metrics.

Conclusion: Splat4D stands out as effective and versatile for applications including text/image conditioned 4D generation, human rendering, and guided content editing.

Abstract: Generating high-quality 4D content from monocular videos for applications
such as digital humans and AR/VR poses challenges in ensuring temporal and
spatial consistency, preserving intricate details, and incorporating user
guidance effectively. To overcome these challenges, we introduce Splat4D, a
novel framework enabling high-fidelity 4D content generation from a monocular
video. Splat4D achieves superior performance while maintaining faithful
spatial-temporal coherence by leveraging multi-view rendering, inconsistency
identification, a video diffusion model, and an asymmetric U-Net for
refinement. Through extensive evaluations on public benchmarks, Splat4D
consistently demonstrates state-of-the-art performance across various metrics,
underscoring the efficacy of our approach. Additionally, the versatility of
Splat4D is validated in various applications such as text/image conditioned 4D
generation, 4D human generation, and text-guided content editing, producing
coherent outcomes following user instructions.

</details>


### [298] [Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2508.07570)
*Khanh-Binh Nguyen,Phuoc-Nguyen Bui,Hyunseung Choo,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: The paper introduces the Adaptive Cache Enhancement (ACE) framework for improving zero-shot generalization and robustness of vision-language models (VLMs) during test-time adaptation (TTA), achieving superior performance on diverse benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Vision-language models experience performance drops under distribution shifts, especially without labeled data, and existing cache-based TTA methods have challenges with unreliable confidence metrics and rigid decision boundaries.

Method: The ACE framework dynamically constructs a robust memory cache of class-specific, high-confidence image embeddings, using class-adaptive thresholds refined iteratively with zero-shot statistics, an exponential moving average, and exploration-augmented updates.

Result: ACE significantly outperforms previous TTA methods, demonstrating state-of-the-art robustness and generalization capabilities across 15 out-of-distribution benchmark datasets.

Conclusion: The ACE framework effectively tackles limitations in existing TTA methods by enhancing adaptation to diverse visual data distributions, ensuring reliable and accurate predictions in zero-shot scenarios.

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot generalization but
suffer performance degradation under distribution shifts in downstream tasks,
particularly in the absence of labeled data. Test-Time Adaptation (TTA)
addresses this challenge by enabling online optimization of VLMs during
inference, eliminating the need for annotated data. Cache-based TTA methods
exploit historical knowledge by maintaining a dynamic memory cache of
low-entropy or high-confidence samples, promoting efficient adaptation to
out-of-distribution data. Nevertheless, these methods face two critical
challenges: (1) unreliable confidence metrics under significant distribution
shifts, resulting in error accumulation within the cache and degraded
adaptation performance; and (2) rigid decision boundaries that fail to
accommodate substantial distributional variations, leading to suboptimal
predictions. To overcome these limitations, we introduce the Adaptive Cache
Enhancement (ACE) framework, which constructs a robust cache by selectively
storing high-confidence or low-entropy image embeddings per class, guided by
dynamic, class-specific thresholds initialized from zero-shot statistics and
iteratively refined using an exponential moving average and
exploration-augmented updates. This approach enables adaptive, class-wise
decision boundaries, ensuring robust and accurate predictions across diverse
visual distributions. Extensive experiments on 15 diverse benchmark datasets
demonstrate that ACE achieves state-of-the-art performance, delivering superior
robustness and generalization compared to existing TTA methods in challenging
out-of-distribution scenarios.

</details>


### [299] [Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification](https://arxiv.org/abs/2508.07577)
*Zhaorui Tan,Tan Pan,Kaizhu Huang,Weimiao Yu,Kai Yao,Chen Jiang,Qiufeng Wang,Anh Nguyen,Xin Guo,Yuan Cheng,Xi Yang*

Main category: cs.CV

TL;DR: This paper studies the dynamics of Layer Normalization (LayerNorm) in fine-tuning Vision Transformers (ViTs) and presents a rescaling mechanism to optimize it, especially under domain shifts and data scarcity.


<details>
  <summary>Details</summary>
Motivation: LayerNorm plays a critical role in ViTs, but its behavior under domain shifts and limited data during fine-tuning remains poorly understood.

Method: The authors propose the Fine-tuning Shift Ratio (FSR) to quantify data representation alignment and a rescaling mechanism using a scalar λ to optimize LayerNorm fine-tuning, along with a cyclic framework for enhancement.

Result: Experiments across natural/pathological images and varied domains (ID, OOD) show the effectiveness of the framework. Notably, OOD tasks exhibit higher λ with lower FSR, especially under scarce data.

Conclusion: LayerNorm fine-tuning dynamics are better understood via the proposed metrics and strategies, offering improvements for ViTs under domain transitions and limited data conditions.

Abstract: LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning
dynamics under data scarcity and domain shifts remain underexplored. This paper
shows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)
are indicative of the transitions between source and target domains; its
efficacy is contingent upon the degree to which the target training samples
accurately represent the target domain, as quantified by our proposed
Fine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet
effective rescaling mechanism using a scalar $\lambda$ that is negatively
correlated to $FSR$ to align learned LayerNorm shifts with those ideal shifts
achieved under fully representative data, combined with a cyclic framework that
further enhances the LayerNorm fine-tuning. Extensive experiments across
natural and pathological images, in both in-distribution (ID) and
out-of-distribution (OOD) settings, and various target training sample regimes
validate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher
$\lambda$ in comparison to ID cases, especially with scarce data, indicating
under-represented target training samples. Moreover, ViTFs fine-tuned on
pathological data behave more like ID settings, favoring conservative LayerNorm
updates. Our findings illuminate the underexplored dynamics of LayerNorm in
transfer learning and provide practical strategies for LayerNorm fine-tuning.

</details>


### [300] [GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm](https://arxiv.org/abs/2508.07585)
*Yu-Huan Wu,Wei Liu,Zi-Xuan Zhu,Zizhou Wang,Yong Liu,Liangli Zhen*

Main category: cs.CV

TL;DR: GAPNet is a lightweight model for salient object detection (SOD) that combines granularity-aware connections and modules for efficiency, achieving state-of-the-art results for image and video SOD.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational inefficiency of existing SOD models and make them suitable for real-world applications, especially on edge devices.

Method: The method involves using granularity-aware connections in the decoder, supported by granular pyramid convolution (GPC) and cross-scale attention (CSA) modules, as well as a self-attention module in the encoder for efficient fusion and global localization.

Result: The method achieves new state-of-the-art performance in lightweight image and video salient object detection (SOD).

Conclusion: GAPNet optimizes feature utilization, provides efficient granularity-aware processing, and outperforms other lightweight models, making it suitable for practical applications.

Abstract: Recent salient object detection (SOD) models predominantly rely on
heavyweight backbones, incurring substantial computational cost and hindering
their practical application in various real-world settings, particularly on
edge devices. This paper presents GAPNet, a lightweight network built on the
granularity-aware paradigm for both image and video SOD. We assign saliency
maps of different granularities to supervise the multi-scale decoder
side-outputs: coarse object locations for high-level outputs and fine-grained
object boundaries for low-level outputs. Specifically, our decoder is built
with granularity-aware connections which fuse high-level features of low
granularity and low-level features of high granularity, respectively. To
support these connections, we design granular pyramid convolution (GPC) and
cross-scale attention (CSA) modules for efficient fusion of low-scale and
high-scale features, respectively. On top of the encoder, a self-attention
module is built to learn global information, enabling accurate object
localization with negligible computational cost. Unlike traditional U-Net-based
approaches, our proposed method optimizes feature utilization and semantic
interpretation while applying appropriate supervision at each processing stage.
Extensive experiments show that the proposed method achieves a new
state-of-the-art performance among lightweight image and video SOD models. Code
is available at https://github.com/yuhuan-wu/GAPNet.

</details>


### [301] [Voice Pathology Detection Using Phonation](https://arxiv.org/abs/2508.07587)
*Sri Raksha Siva,Nived Suthahar,Prakash Boominathan,Uma Ranjan*

Main category: cs.CV

TL;DR: This research introduces a noninvasive machine learning framework leveraging acoustic features and neural networks to detect voice pathologies.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnosis methods for voice disorders are invasive and subjective, necessitating alternative approaches.

Method: Analyzed phonation data using RNNs, advanced acoustic features, and data augmentation techniques for classification.

Result: The framework successfully classified voice samples as normal or pathological, demonstrating potential for AI-driven healthcare.

Conclusion: It provides an effective, automated, and noninvasive tool for early diagnosis of voice disorders, improving accessibility and patient care.

Abstract: Voice disorders significantly affect communication and quality of life,
requiring an early and accurate diagnosis. Traditional methods like
laryngoscopy are invasive, subjective, and often inaccessible. This research
proposes a noninvasive, machine learning-based framework for detecting voice
pathologies using phonation data.
  Phonation data from the Saarbr\"ucken Voice Database are analyzed using
acoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma
features, and Mel spectrograms. Recurrent Neural Networks (RNNs), including
LSTM and attention mechanisms, classify samples into normal and pathological
categories. Data augmentation techniques, including pitch shifting and Gaussian
noise addition, enhance model generalizability, while preprocessing ensures
signal quality. Scale-based features, such as H\"older and Hurst exponents,
further capture signal irregularities and long-term dependencies.
  The proposed framework offers a noninvasive, automated diagnostic tool for
early detection of voice pathologies, supporting AI-driven healthcare, and
improving patient outcomes.

</details>


### [302] [From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users](https://arxiv.org/abs/2508.07596)
*Shahroz Tariq,Simon S. Woo,Priyanka Singh,Irena Irmalasari,Saakshi Gupta,Dev Gupta*

Main category: cs.CV

TL;DR: This paper introduces DF-P2E, a new system for interpretable deepfake detection, combining visual, semantic, and narrative explanations.


<details>
  <summary>Details</summary>
Motivation: The lack of interpretability in existing deepfake detection systems limits their usability in real-world, non-expert decision-making contexts.

Method: DF-P2E integrates three components: a classifier with Grad-CAM visualisations, a captioning module for manipulated regions, and a Large Language Model (LLM) for refined narrative explanations.

Result: Experiments on the DF40 benchmark show competitive detection performance with high-quality, human-aligned explanations.

Conclusion: DF-P2E provides a transparent, scalable, and interpretable solution for deepfake detection, supporting trustworthy AI in adversarial media environments.

Abstract: The proliferation of deepfake technologies poses urgent challenges and
serious risks to digital integrity, particularly within critical sectors such
as forensics, journalism, and the legal system. While existing detection
systems have made significant progress in classification accuracy, they
typically function as black-box models, offering limited transparency and
minimal support for human reasoning. This lack of interpretability hinders
their usability in real-world decision-making contexts, especially for
non-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to
Explanation), a novel multimodal framework that integrates visual, semantic,
and narrative layers of explanation to make deepfake detection interpretable
and accessible. The framework consists of three modular components: (1) a
deepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual
captioning module that generates natural language summaries of manipulated
regions, and (3) a narrative refinement module that uses a fine-tuned Large
Language Model (LLM) to produce context-aware, user-sensitive explanations. We
instantiate and evaluate the framework on the DF40 benchmark, the most diverse
deepfake dataset to date. Experiments demonstrate that our system achieves
competitive detection performance while providing high-quality explanations
aligned with Grad-CAM activations. By unifying prediction and explanation in a
coherent, human-aligned pipeline, this work offers a scalable approach to
interpretable deepfake detection, advancing the broader vision of trustworthy
and transparent AI systems in adversarial media environments.

</details>


### [303] [ShoulderShot: Generating Over-the-Shoulder Dialogue Videos](https://arxiv.org/abs/2508.07597)
*Yuang Zhang,Junqi Cheng,Haoyu Zhao,Jiaxi Gu,Fangyuan Zou,Zenghui Lu,Peng Shu*

Main category: cs.CV

TL;DR: The study introduces ShoulderShot, a framework for generating long dialogue videos with spatial continuity and character consistency, addressing gaps in existing video generation methods.


<details>
  <summary>Details</summary>
Motivation: Over-the-shoulder dialogue videos are vital for various media applications but have been underexplored in video generation research due to challenges like maintaining character consistency and efficient production.

Method: ShoulderShot combines dual-shot generation with looping video techniques to ensure extended dialogue sequences while preserving spatial and character continuity.

Result: ShoulderShot surpasses existing methods in shot-reverse-shot layout, spatial continuity, and dialog length flexibility, as evidenced by comparative results shared publicly.

Conclusion: The framework demonstrates promising advancements for practical dialogue video generation, paving the way for improved approaches in media production.

Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and
advertisements, providing visual variety and enhancing viewers' emotional
connection. Despite their importance, such dialogue scenes remain largely
underexplored in video generation research. The main challenges include
maintaining character consistency across different shots, creating a sense of
spatial continuity, and generating long, multi-turn dialogues within limited
computational budgets. Here, we present ShoulderShot, a framework that combines
dual-shot generation with looping video, enabling extended dialogues while
preserving character consistency. Our results demonstrate capabilities that
surpass existing methods in terms of shot-reverse-shot layout, spatial
continuity, and flexibility in dialogue length, thereby opening up new
possibilities for practical dialogue video generation. Videos and comparisons
are available at https://shouldershot.github.io.

</details>


### [304] [LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation](https://arxiv.org/abs/2508.07603)
*Wenhui Song,Hanhui Li,Jiehui Huang,Panwen Hu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang*

Main category: cs.CV

TL;DR: The paper introduces LaVieID, a framework focused on preserving identity in text-to-video generation by applying local and temporal enhancements to diffusion transformer models.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion transformers suffer from poor identity preservation in generated videos due to their global and unstructured modeling processes.

Method: LaVieID employs a local routing mechanism for finer facial representation, along with a temporal autoregressive module that refines latent tokens based on long-range temporal dependencies.

Result: The model significantly improves identity consistency and achieves state-of-the-art performance in personalized video generation.

Conclusion: LaVieID effectively addresses the identity preservation challenge in text-to-video generation, offering high-fidelity video outputs with better performance than prior models.

Abstract: In this paper, we present LaVieID, a novel \underline{l}ocal
\underline{a}utoregressive \underline{vi}d\underline{e}o diffusion framework
designed to tackle the challenging \underline{id}entity-preserving
text-to-video task. The key idea of LaVieID is to mitigate the loss of identity
information inherent in the stochastic global generation process of diffusion
transformers (DiTs) from both spatial and temporal perspectives. Specifically,
unlike the global and unstructured modeling of facial latent states in existing
DiTs, LaVieID introduces a local router to explicitly represent latent states
by weighted combinations of fine-grained local facial structures. This
alleviates undesirable feature interference and encourages DiTs to capture
distinctive facial characteristics. Furthermore, a temporal autoregressive
module is integrated into LaVieID to refine denoised latent tokens before video
decoding. This module divides latent tokens temporally into chunks, exploiting
their long-range temporal dependencies to predict biases for rectifying tokens,
thereby significantly enhancing inter-frame identity consistency. Consequently,
LaVieID can generate high-fidelity personalized videos and achieve
state-of-the-art performance. Our code and models are available at
https://github.com/ssugarwh/LaVieID.

</details>


### [305] [X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning](https://arxiv.org/abs/2508.07607)
*Jian Ma,Xujie Zhu,Zihao Pan,Qirong Peng,Xu Guo,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: The paper introduces X2Edit, a dataset for image editing tasks, and a lightweight model training approach compatible with community generative models, showing improved editing performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of high-quality open-source datasets for arbitrary-instruction image editing and the absence of plug-and-play modules compatible with popular generative models.

Method: The authors construct a diverse dataset (X2Edit) using industry-standard models, implement task-aware MoE-LoRA training with FLUX, and utilize diffusion model representations for contrastive learning.

Result: Experiments show that the proposed model achieves competitive editing performance, and the X2Edit dataset outperforms existing alternatives in quality and diversity.

Conclusion: The paper contributes with a high-quality dataset and a model training strategy improving arbitrary image editing capabilities, openly available for the community.

Abstract: Existing open-source datasets for arbitrary-instruction image editing remain
suboptimal, while a plug-and-play editing module compatible with
community-prevalent generative models is notably absent. In this paper, we
first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse
editing tasks, including subject-driven generation. We utilize the
industry-leading unified image generation models and expert models to construct
the data. Meanwhile, we design reasonable editing instructions with the VLM and
implement various scoring mechanisms to filter the data. As a result, we
construct 3.7 million high-quality data with balanced categories. Second, to
better integrate seamlessly with community image generation models, we design
task-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parameters
of the full model. To further improve the final performance, we utilize the
internal representations of the diffusion model and define positive/negative
samples based on image editing types to introduce contrastive learning.
Extensive experiments demonstrate that the model's editing performance is
competitive among many excellent models. Additionally, the constructed dataset
exhibits substantial advantages over existing open-source datasets. The
open-source code, checkpoints, and datasets for X2Edit can be found at the
following link: https://github.com/OPPO-Mente-Lab/X2Edit.

</details>


### [306] [An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View](https://arxiv.org/abs/2508.07618)
*Hyoung Suk Park,Kiwan Jeon*

Main category: cs.CV

TL;DR: This paper proposes a two-stage method to suppress truncation artifacts in dental CBCT imaging caused by a small detector's limited field of view.


<details>
  <summary>Details</summary>
Motivation: To address the image quality degradation in dental CBCT caused by truncated fields of view from compact detector designs.

Method: The approach is a two-stage process: first, generating a prior image using Implicit Neural Representation (INR) over a coarse voxel size; second, correcting projection discrepancies for improved iterative reconstruction.

Result: Numerical results show the proposed method significantly reduces truncation artifacts, improving image quality in CBCT.

Conclusion: The two-stage method leverages INR to effectively address truncation artifacts in CBCT, offering enhanced diagnostic image quality.

Abstract: In dental cone-beam computed tomography (CBCT), compact and cost-effective
system designs often use small detectors, resulting in a truncated field of
view (FOV) that does not fully encompass the patient's head. In iterative
reconstruction approaches, the discrepancy between the actual projection and
the forward projection within the truncated FOV accumulates over iterations,
leading to significant degradation in the reconstructed image quality. In this
study, we propose a two-stage approach to mitigate truncation artifacts in
dental CBCT. In the first stage, we employ Implicit Neural Representation
(INR), leveraging its superior representation power, to generate a prior image
over an extended region so that its forward projection fully covers the
patient's head. To reduce computational and memory burdens, INR reconstruction
is performed with a coarse voxel size. The forward projection of this prior
image is then used to estimate the discrepancy due to truncated FOV in the
measured projection data. In the second stage, the discrepancy-corrected
projection data is utilized in a conventional iterative reconstruction process
within the truncated region. Our numerical results demonstrate that the
proposed two-grid approach effectively suppresses truncation artifacts, leading
to improved CBCT image quality.

</details>


### [307] [SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation](https://arxiv.org/abs/2508.07621)
*Yunsung Chung,Chanho Lim,Ghassan Bidaoui,Christian Massad,Nassir Marrouche,Jihun Hamm*

Main category: cs.CV

TL;DR: The paper introduces SOFA, a deep-learning framework designed to simulate and optimize atrial fibrillation ablation procedures, reducing recurrence risk by 22.18%.


<details>
  <summary>Details</summary>
Motivation: Atrial fibrillation treatments vary in outcome due to complex interactions of patient-specific tissue and procedural factors. The study aims to predict recurrence risk and optimize ablation strategies to improve efficacy.

Method: SOFA generates post-ablation images based on pre-ablation LGE-MRI and procedural parameters, predicts recurrence risk, and optimizes parameters to reduce the risk using a multi-modal, multi-view deep-learning generator.

Result: Quantitative evaluation reveals SOFA's capability to accurately create post-ablation images and achieve a 22.18% reduction in predicted recurrence risk.

Conclusion: SOFA integrates simulation, prediction, and optimization to personalize AF ablation, making it a novel and impactful tool for improving procedural outcomes.

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with
catheter ablation procedures, but procedural outcomes are highly variable.
Evaluating and improving ablation efficacy is challenging due to the complex
interaction between patient-specific tissue and procedural factors. This paper
asks two questions: Can AF recurrence be predicted by simulating the effects of
procedural parameters? How should we ablate to reduce AF recurrence? We propose
SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel
deep-learning framework that addresses these questions. SOFA first simulates
the outcome of an ablation strategy by generating a post-ablation image
depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and
the specific procedural parameters used (e.g., ablation locations, duration,
temperature, power, and force). During this simulation, it predicts AF
recurrence risk. Critically, SOFA then introduces an optimization scheme that
refines these procedural parameters to minimize the predicted risk. Our method
leverages a multi-modal, multi-view generator that processes 2.5D
representations of the atrium. Quantitative evaluations show that SOFA
accurately synthesizes post-ablation images and that our optimization scheme
leads to a 22.18\% reduction in the model-predicted recurrence risk. To the
best of our knowledge, SOFA is the first framework to integrate the simulation
of procedural effects, recurrence prediction, and parameter optimization,
offering a novel tool for personalizing AF ablation.

</details>


### [308] [Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction](https://arxiv.org/abs/2508.07624)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: The paper introduces a graph-based approach to correct errors in object detection models by modeling spatial relationships using a GNN, improving detection accuracy by up to 4%.


<details>
  <summary>Details</summary>
Motivation: Object detection models often fail in cluttered or occluded scenes because they do not leverage spatial priors of static environments.

Method: The authors propose a post-processing pipeline using a graph neural network (GNN) trained on annotated data, which adjusts invalid or erroneous object class labels based on spatial relationships.

Result: Evaluations show that using this approach as a standalone framework or alongside YOLOv7 and RT-DETR improves object detection performance, achieving up to 4% mAP@50 gains.

Conclusion: The study demonstrates the advantages of incorporating spatial reasoning for enhancing the accuracy and reliability of object detection systems.

Abstract: In many real-world applications involving static environments, the spatial
layout of objects remains consistent across instances. However,
state-of-the-art object detection models often fail to leverage this spatial
prior, resulting in inconsistent predictions, missed detections, or
misclassifications, particularly in cluttered or occluded scenes. In this work,
we propose a graph-based post-processing pipeline that explicitly models the
spatial relationships between objects to correct detection anomalies in
egocentric frames. Using a graph neural network (GNN) trained on manually
annotated data, our model identifies invalid object class labels and predicts
corrected class labels based on their neighbourhood context. We evaluate our
approach both as a standalone anomaly detection and correction framework and as
a post-processing module for standard object detectors such as YOLOv7 and
RT-DETR. Experiments demonstrate that incorporating this spatial reasoning
significantly improves detection performance, with mAP@50 gains of up to 4%.
This method highlights the potential of leveraging the environment's spatial
structure to improve reliability in object detection systems.

</details>


### [309] [A Trustworthy Method for Multimodal Emotion Recognition](https://arxiv.org/abs/2508.07625)
*Junxiao Xue,Xiaozhen Liu,Jie Wang,Xuecheng Wu,Bin Wu*

Main category: cs.CV

TL;DR: The paper introduces Trusted Emotion Recognition (TER), a method prioritizing reliability by using uncertainty estimation to determine prediction confidence. It demonstrates superior performance and robustness against noise.


<details>
  <summary>Details</summary>
Motivation: To address the issue of unreliable decisions made by emotion recognition models when faced with noisy, corrupted, or out-of-distribution data.

Method: Proposed a framework called TER that incorporates uncertainty estimation to assign confidence values to predictions. Combines results from multiple modalities based on their confidence values and introduces new metrics (trusted precision, trusted recall, trusted Acc., and trusted F1 score) to evaluate prediction reliability.

Result: TER achieves state-of-the-art performance on Music-video datasets with 82.40% accuracy and trusted F1 scores of 0.7511 and 0.9035 on IEMOCAP and Music-video datasets, respectively.

Conclusion: TER enhances emotion recognition reliability by robustly handling noisy and corrupted data and outperforming other methods in terms of accuracy and trusted metrics.

Abstract: Existing emotion recognition methods mainly focus on enhancing performance by
employing complex deep models, typically resulting in significantly higher
model complexity. Although effective, it is also crucial to ensure the
reliability of the final decision, especially for noisy, corrupted and
out-of-distribution data. To this end, we propose a novel emotion recognition
method called trusted emotion recognition (TER), which utilizes uncertainty
estimation to calculate the confidence value of predictions. TER combines the
results from multiple modalities based on their confidence values to output the
trusted predictions. We also provide a new evaluation criterion to assess the
reliability of predictions. Specifically, we incorporate trusted precision and
trusted recall to determine the trusted threshold and formulate the trusted
Acc. and trusted F1 score to evaluate the model's trusted performance. The
proposed framework combines the confidence module that accordingly endows the
model with reliability and robustness against possible noise or corruption. The
extensive experimental results validate the effectiveness of our proposed
model. The TER achieves state-of-the-art performance on the Music-video,
achieving 82.40% Acc. In terms of trusted performance, TER outperforms other
methods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511
and 0.9035, respectively.

</details>


### [310] [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/abs/2508.07647)
*Xiaohang Zhan,Dingming Liu*

Main category: cs.CV

TL;DR: The paper introduces a training-free method leveraging volume rendering principles to control occlusion relationships in image generation precisely by using pre-trained image diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack precision in controlling occlusion relationships in generated images. Layout-to-image approaches don't explicitly tackle occlusion control.

Method: Utilizes volume rendering principles in the latent space of a pre-trained image diffusion model to achieve control over occlusion relationships without retraining or fine-tuning the model.

Result: The proposed method outperforms existing approaches in occlusion accuracy and enables effects like adjusting transparency, light intensity, and particle concentration.

Conclusion: Physics-grounded volume rendering enables effective and precise occlusion control in image generation, broadening the potential for creative and practical applications.

Abstract: We propose a novel training-free image generation algorithm that precisely
controls the occlusion relationships between objects in an image. Existing
image generation methods typically rely on prompts to influence occlusion,
which often lack precision. While layout-to-image methods provide control over
object locations, they fail to address occlusion relationships explicitly.
Given a pre-trained image diffusion model, our method leverages volume
rendering principles to "render" the scene in latent space, guided by occlusion
relationships and the estimated transmittance of objects. This approach does
not require retraining or fine-tuning the image diffusion model, yet it enables
accurate occlusion control due to its physics-grounded foundation. In extensive
experiments, our method significantly outperforms existing approaches in terms
of occlusion accuracy. Furthermore, we demonstrate that by adjusting the
opacities of objects or concepts during rendering, our method can achieve a
variety of effects, such as altering the transparency of objects, the density
of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the
intensity of light, and the strength of lens effects, etc.

</details>


### [311] [Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels](https://arxiv.org/abs/2508.07656)
*Yimin Fu,Zhunga Liu,Dongxiu Guo,Longfei Wang*

Main category: cs.CV

TL;DR: The paper addresses noisy labels in SAR data recognition by proposing a collaborative learning method combining scattering and deep features.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled SAR data acquisition is challenging due to expert knowledge requirements, and noisy labels degrade SAR ATR performance.

Method: A multi-model feature fusion framework integrates dynamic scattering and deep features, combines Gaussian Mixture Models for label noise separation, implements semi-supervised divergent branches learning, and uses joint distribution alignment.

Result: Experiments on the MSTAR dataset demonstrate state-of-the-art SAR ATR performance under various noisy label conditions.

Conclusion: The proposed CLSDF method effectively improves SAR ATR robustness with noisy labels, advancing reliable recognition in challenging conditions.

Abstract: The acquisition of high-quality labeled synthetic aperture radar (SAR) data
is challenging due to the demanding requirement for expert knowledge.
Consequently, the presence of unreliable noisy labels is unavoidable, which
results in performance degradation of SAR automatic target recognition (ATR).
Existing research on learning with noisy labels mainly focuses on image data.
However, the non-intuitive visual characteristics of SAR data are insufficient
to achieve noise-robust learning. To address this problem, we propose
collaborative learning of scattering and deep features (CLSDF) for SAR ATR with
noisy labels. Specifically, a multi-model feature fusion framework is designed
to integrate scattering and deep features. The attributed scattering centers
(ASCs) are treated as dynamic graph structure data, and the extracted physical
characteristics effectively enrich the representation of deep image features.
Then, the samples with clean and noisy labels are divided by modeling the loss
distribution with multiple class-wise Gaussian Mixture Models (GMMs).
Afterward, the semi-supervised learning of two divergent branches is conducted
based on the data divided by each other. Moreover, a joint distribution
alignment strategy is introduced to enhance the reliability of co-guessed
labels. Extensive experiments have been done on the Moving and Stationary
Target Acquisition and Recognition (MSTAR) dataset, and the results show that
the proposed method can achieve state-of-the-art performance under different
operating conditions with various label noises.

</details>


### [312] [Undress to Redress: A Training-Free Framework for Virtual Try-On](https://arxiv.org/abs/2508.07680)
*Zhiying Li,Junhao Wu,Yeying Jin,Daiheng Gao,Yun Ji,Kaichuan Kong,Lei Yu,Hao Xu,Kai Chen,Bruce Gu,Nana Wang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: The paper proposes UR-VTON, a novel framework to address challenges in long-sleeve-to-short-sleeve conversions for virtual try-on systems by introducing an undress-to-redress mechanism for better skin restoration and detail fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods struggle with converting long-sleeve garments to short-sleeve, often producing unrealistic outputs due to poor skin restoration. The authors identify this as stemming from a limitation in the 'majority' completion rule of current models.

Method: The proposed UR-VTON framework leverages a novel undress-to-redress mechanism that first 'undresses' the user's torso before applying the target garment. It also incorporates Dynamic Classifier-Free Guidance scheduling and a Structural Refiner for enhanced image quality and detail preservation.

Result: Experiments show that UR-VTON surpasses current state-of-the-art methods in both image quality and detail fidelity for the task of converting long-sleeve to short-sleeve garments. A new benchmark (LS-TON) is introduced to evaluate this specific scenario.

Conclusion: UR-VTON effectively handles challenging garment conversion cases by decomposing the process into manageable steps, improving both skin restoration and detail accuracy in virtual try-on scenarios.

Abstract: Virtual try-on (VTON) is a crucial task for enhancing user experience in
online shopping by generating realistic garment previews on personal photos.
Although existing methods have achieved impressive results, they struggle with
long-sleeve-to-short-sleeve conversions-a common and practical scenario-often
producing unrealistic outputs when exposed skin is underrepresented in the
original image. We argue that this challenge arises from the ''majority''
completion rule in current VTON models, which leads to inaccurate skin
restoration in such cases. To address this, we propose UR-VTON (Undress-Redress
Virtual Try-ON), a novel, training-free framework that can be seamlessly
integrated with any existing VTON method. UR-VTON introduces an
''undress-to-redress'' mechanism: it first reveals the user's torso by
virtually ''undressing,'' then applies the target short-sleeve garment,
effectively decomposing the conversion into two more manageable steps.
Additionally, we incorporate Dynamic Classifier-Free Guidance scheduling to
balance diversity and image quality during DDPM sampling, and employ Structural
Refiner to enhance detail fidelity using high-frequency cues. Finally, we
present LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.
Extensive experiments demonstrate that UR-VTON outperforms state-of-the-art
methods in both detail preservation and image quality. Code will be released
upon acceptance.

</details>


### [313] [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683)
*Chaohong Guo,Xun Mo,Yongwei Nie,Xuemiao Xu,Chao Xu,Fei Yu,Chengjiang Long*

Main category: cs.CV

TL;DR: This paper introduces TAR-TVG, a novel Temporal Video Grounding framework that incorporates timestamp anchors for explicit reasoning and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods for Temporal Video Grounding fail to explicitly control the quality of the reasoning process, leading to suboptimal temporal predictions.

Method: The TAR-TVG framework introduces timestamp anchors as intermediate checkpoints in the reasoning process and enforces progressively accurate temporal estimations. A three-stage training strategy involving GRPO training, supervised fine-tuning, and final optimization is used.

Result: The proposed TAR-TVG framework achieves state-of-the-art performance in Temporal Video Grounding tasks while producing clear and verifiable reasoning chains.

Conclusion: TAR-TVG improves Temporal Video Grounding by refining the reasoning process with timestamp anchors, ensuring better predictions and interpretability.

Abstract: Temporal Video Grounding (TVG) aims to precisely localize video segments
corresponding to natural language queries, which is a critical capability for
long-form video understanding. Although existing reinforcement learning
approaches encourage models to generate reasoning chains before predictions,
they fail to explicitly constrain the reasoning process to ensure the quality
of the final temporal predictions. To address this limitation, we propose
Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),
a novel framework that introduces timestamp anchors within the reasoning
process to enforce explicit supervision to the thought content. These anchors
serve as intermediate verification points. More importantly, we require each
reasoning step to produce increasingly accurate temporal estimations, thereby
ensuring that the reasoning process contributes meaningfully to the final
prediction. To address the challenge of low-probability anchor generation in
models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation
training strategy: (1) initial GRPO training to collect 30K high-quality
reasoning traces containing multiple timestamp anchors, (2) supervised
fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the
SFT-enhanced model. This three-stage training strategy enables robust anchor
generation while maintaining reasoning quality. Experiments show that our model
achieves state-of-the-art performance while producing interpretable, verifiable
reasoning chains with progressively refined temporal estimations.

</details>


### [314] [Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing](https://arxiv.org/abs/2508.07700)
*Weitao Wang,Haoran Xu,Jun Meng,Haoqian Wang*

Main category: cs.CV

TL;DR: The paper introduces a tuning-free method to ensure high-quality 3D content editing while preserving geometry, addressing the limitations of existing 2D-focused tools.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper arises from the increasing demand for personalized 3D content editing, requiring techniques to enhance color, style, and lighting without compromising the 3D geometry.

Method: The authors propose a plug-and-play scheme with a geometry preservation module to align edited assets with original geometry. They also introduce an injection switcher to balance the supervision between original normals and edited elements.

Result: Experiments demonstrate that this method improves the multi-view consistency and mesh quality of 3D assets, despite various combinations of multi-view diffusion models and editing techniques.

Conclusion: The proposed scheme successfully addresses limitations in 2D-based editing tools, providing a robust method for geometry-preserving 3D edits.

Abstract: As 3D generation techniques continue to flourish, the demand for generating
personalized content is rapidly rising. Users increasingly seek to apply
various editing methods to polish generated 3D content, aiming to enhance its
color, style, and lighting without compromising the underlying geometry.
However, most existing editing tools focus on the 2D domain, and directly
feeding their results into 3D generation methods (like multi-view diffusion
models) will introduce information loss, degrading the quality of the final 3D
assets. In this paper, we propose a tuning-free, plug-and-play scheme that
aligns edited assets with their original geometry in a single inference run.
Central to our approach is a geometry preservation module that guides the
edited multi-view generation with original input normal latents. Besides, an
injection switcher is proposed to deliberately control the supervision extent
of the original normals, ensuring the alignment between the edited color and
normal views. Extensive experiments show that our method consistently improves
both the multi-view consistency and mesh quality of edited 3D assets, across
multiple combinations of multi-view diffusion models and editing methods.

</details>


### [315] [DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models](https://arxiv.org/abs/2508.07714)
*Licheng Zhang,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: A novel semi-automated pipeline is developed for fine-grained door detection in floor plans using deep learning and LLMs, reducing annotation effort while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: The lack of publicly available datasets for fine-grained multi-class door detection in floor plans limits progress in applications like building compliance checking and indoor scene understanding.

Method: A semi-automated pipeline combines deep object detectors and LLMs to detect and classify doors in floor plans; a human-in-the-loop step ensures high-quality annotations.

Result: The process generates a high-quality multi-class door detection dataset with reduced manual annotation costs.

Conclusion: Combining deep learning and multimodal reasoning improves dataset creation efficiency for complex real-world domains like floor plan analysis.

Abstract: Accurate detection and classification of diverse door types in floor plans
drawings is critical for multiple applications, such as building compliance
checking, and indoor scene understanding. Despite their importance, publicly
available datasets specifically designed for fine-grained multi-class door
detection remain scarce. In this work, we present a semi-automated pipeline
that leverages a state-of-the-art object detector and a large language model
(LLM) to construct a multi-class door detection dataset with minimal manual
effort. Doors are first detected as a unified category using a deep object
detection model. Next, an LLM classifies each detected instance based on its
visual and contextual features. Finally, a human-in-the-loop stage ensures
high-quality labels and bounding boxes. Our method significantly reduces
annotation cost while producing a dataset suitable for benchmarking neural
models in floor plan analysis. This work demonstrates the potential of
combining deep learning and multimodal reasoning for efficient dataset
construction in complex real-world domains.

</details>


### [316] [A Registration-Based Star-Shape Segmentation Model and Fast Algorithms](https://arxiv.org/abs/2508.07721)
*Daoping Zhang,Xue-Cheng Tai,Lok Ming Lui*

Main category: cs.CV

TL;DR: The paper introduces a model for accurate star-shape image segmentation leveraging a level set approach with constraints, even for noisy or occluded images.


<details>
  <summary>Details</summary>
Motivation: Accurate image segmentation is difficult when images are noisy or occluded; the authors aim to improve segmentation using star-shape priors.

Method: They propose a registration framework with level set representation and constraints to allow flexible star-shape segmentation and specified boundary landmarks.

Result: Their approach demonstrated effective results through experiments on synthetic and real images, achieving accurate segmentation.

Conclusion: The proposed model is a robust method for segmenting star-shaped objects and holds potential for handling challenging scenarios in segmentation tasks.

Abstract: Image segmentation plays a crucial role in extracting objects of interest and
identifying their boundaries within an image. However, accurate segmentation
becomes challenging when dealing with occlusions, obscurities, or noise in
corrupted images. To tackle this challenge, prior information is often
utilized, with recent attention on star-shape priors. In this paper, we propose
a star-shape segmentation model based on the registration framework. By
combining the level set representation with the registration framework and
imposing constraints on the deformed level set function, our model enables both
full and partial star-shape segmentation, accommodating single or multiple
centers. Additionally, our approach allows for the enforcement of identified
boundaries to pass through specified landmark locations. We tackle the proposed
models using the alternating direction method of multipliers. Through numerical
experiments conducted on synthetic and real images, we demonstrate the efficacy
of our approach in achieving accurate star-shape segmentation.

</details>


### [317] [Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting](https://arxiv.org/abs/2508.07723)
*Ting Xiang,Changjian Chen,Zhuo Tang,Qifeng Zhang,Fei Lyu,Li Yang,Jiapeng Zhang,Kenli Li*

Main category: cs.CV

TL;DR: A triplet-connection-based re-weighting method, named TriReWeight, is proposed to improve generative data augmentation for computer vision tasks, showing significant performance improvements on various datasets.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of limited image availability in real-world applications like medical diagnosis, where noisy data often arises during dataset expansion using generative models.

Method: TriReWeight applies theoretical analysis of three types of supervision for generated images and introduces a sample re-weighting approach that mitigates negative effects introduced by noise.

Result: TriReWeight shows a performance boost of 7.9% on average across six natural image datasets and a 3.4% improvement across three medical datasets over existing SOTA methods.

Conclusion: TriReWeight improves upon current generative data augmentation methods and can consistently enhance their performance without degradation, validating its theoretical effectiveness and practical utility.

Abstract: The performance of computer vision models in certain real-world applications,
such as medical diagnosis, is often limited by the scarcity of available
images. Expanding datasets using pre-trained generative models is an effective
solution. However, due to the uncontrollable generation process and the
ambiguity of natural language, noisy images may be generated. Re-weighting is
an effective way to address this issue by assigning low weights to such noisy
images. We first theoretically analyze three types of supervision for the
generated images. Based on the theoretical analysis, we develop TriReWeight, a
triplet-connection-based sample re-weighting method to enhance generative data
augmentation. Theoretically, TriReWeight can be integrated with any generative
data augmentation methods and never downgrade their performance. Moreover, its
generalization approaches the optimal in the order $O(\sqrt{d\ln (n)/n})$. Our
experiments validate the correctness of the theoretical analysis and
demonstrate that our method outperforms the existing SOTA methods by $7.9\%$ on
average over six natural image datasets and by $3.4\%$ on average over three
medical datasets. We also experimentally validate that our method can enhance
the performance of different generative data augmentation methods.

</details>


### [318] [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/abs/2508.07747)
*Junhyuk So,Juncheol Shin,Hyunho Kook,Eunhyeok Park*

Main category: cs.CV

TL;DR: This paper introduces "Grouped Speculative Decoding" (GSD), a training-free method to accelerate autoregressive (AR) image models, achieving 3.7x speedup without sacrificing quality.


<details>
  <summary>Details</summary>
Motivation: The sequential nature of AR image models results in slow inference times, limiting their scalability despite superior generative capabilities compared to diffusion models.

Method: The authors propose GSD, which evaluates clusters of visually valid tokens rather than relying on single-target tokens, addressing the redundancy and diversity inherent in image tokens. The method dynamically adjusts clustering to avoid static embedding-based inefficiencies.

Result: GSD achieves an average 3.7x acceleration in AR image generation while maintaining high image quality, and it requires no additional model training.

Conclusion: Grouped Speculative Decoding significantly improves the practicality of AR image models by addressing token redundancy/diversity, offering efficient and quality-preserving generation acceleration.

Abstract: Recently, autoregressive (AR) image models have demonstrated remarkable
generative capabilities, positioning themselves as a compelling alternative to
diffusion models. However, their sequential nature leads to long inference
times, limiting their practical scalability. In this work, we introduce Grouped
Speculative Decoding (GSD), a novel, training-free acceleration method for AR
image models. While recent studies have explored Speculative Decoding (SD) as a
means to speed up AR image generation, existing approaches either provide only
modest acceleration or require additional training. Our in-depth analysis
reveals a fundamental difference between language and image tokens: image
tokens exhibit inherent redundancy and diversity, meaning multiple tokens can
convey valid semantics. However, traditional SD methods are designed to accept
only a single most-likely token, which fails to leverage this difference,
leading to excessive false-negative rejections. To address this, we propose a
new SD strategy that evaluates clusters of visually valid tokens rather than
relying on a single target token. Additionally, we observe that static
clustering based on embedding distance is ineffective, which motivates our
dynamic GSD approach. Extensive experiments show that GSD accelerates AR image
models by an average of 3.7x while preserving image quality-all without
requiring any additional training. The source code is available at
https://github.com/junhyukso/GSD

</details>


### [319] [Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion](https://arxiv.org/abs/2508.07755)
*Minseo Kim,Minchan Kwon,Dongyeun Lee,Yunho Jeon,Junmo Kim*

Main category: cs.CV

TL;DR: This paper introduces Contrastive Inversion, a method for extracting shared concepts from image sets without external guidance, achieving better generation quality and concept fidelity.


<details>
  <summary>Details</summary>
Motivation: Customized image generation requires robust techniques to identify shared concepts among small image sets, avoiding quality degradation caused by incomplete separation of auxiliary features.

Method: The approach involves contrastive learning to disentangle true semantics of shared concepts, followed by cross-attention fine-tuning to ensure concept fidelity without overfitting.

Result: The method outperforms existing techniques in concept representation and editing, providing balanced and high-quality performance.

Conclusion: Contrastive Inversion addresses challenges in customized image generation by effectively extracting shared concepts without reliance on manual guidance, improving both quality and editing capabilities.

Abstract: The recent demand for customized image generation raises a need for
techniques that effectively extract the common concept from small sets of
images. Existing methods typically rely on additional guidance, such as text
prompts or spatial masks, to capture the common target concept. Unfortunately,
relying on manually provided guidance can lead to incomplete separation of
auxiliary features, which degrades generation quality.In this paper, we propose
Contrastive Inversion, a novel approach that identifies the common concept by
comparing the input images without relying on additional information. We train
the target token along with the image-wise auxiliary text tokens via
contrastive learning, which extracts the well-disentangled true semantics of
the target. Then we apply disentangled cross-attention fine-tuning to improve
concept fidelity without overfitting. Experimental results and analysis
demonstrate that our method achieves a balanced, high-level performance in both
concept representation and editing, outperforming existing techniques.

</details>


### [320] [Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild](https://arxiv.org/abs/2508.07759)
*Haoran Wang,Zekun Li,Jian Zhang,Lei Qi,Yinghuan Shi*

Main category: cs.CV

TL;DR: The paper introduces CAV-SAM, a method to adapt vision models like SAM for segmentation tasks using reference images, achieving over 5% improvement in performance.


<details>
  <summary>Details</summary>
Motivation: Existing reference segmentation methods rely on resource-heavy meta-learning, making them expensive in data and computational cost.

Method: The authors represent the reference-target image pairs as a pseudo video to leverage SAM's video segmentation capabilities. They propose two modules: DBST for semantic transformation and TTGA for geometric alignment through fine-tuning.

Result: CAV-SAM outperforms state-of-the-art methods in segmentation tasks, achieving more than 5% improvement in relevant datasets.

Conclusion: CAV-SAM provides a lightweight and effective solution for adapting vision models to downstream tasks, avoiding the heavy costs of meta-learning.

Abstract: Large vision models like the Segment Anything Model (SAM) exhibit significant
limitations when applied to downstream tasks in the wild. Consequently,
reference segmentation, which leverages reference images and their
corresponding masks to impart novel knowledge to the model, emerges as a
promising new direction for adapting vision models. However, existing reference
segmentation approaches predominantly rely on meta-learning, which still
necessitates an extensive meta-training process and brings massive data and
computational cost. In this study, we propose a novel approach by representing
the inherent correspondence between reference-target image pairs as a pseudo
video. This perspective allows the latest version of SAM, known as SAM2, which
is equipped with interactive video object segmentation (iVOS) capabilities, to
be adapted to downstream tasks in a lightweight manner. We term this approach
Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:
the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model
to construct a semantic transformation sequence, while the Test-Time Geometric
Alignment (TTGA) module aligns the geometric changes within this sequence
through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,
achieving segmentation performance improvements exceeding 5% over SOTA methods.
Implementation is provided in the supplementary materials.

</details>


### [321] [UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models](https://arxiv.org/abs/2508.07766)
*Jinke Li,Jiarui Yu,Chenxing Wei,Hande Dong,Qiang Lin,Liangjing Yang,Zhicai Wang,Yanbin Hao*

Main category: cs.CV

TL;DR: This paper introduces UniSVG, a 525k data item dataset designed for training and evaluating Multi-modal Large Language Models (MLLMs) on scalable vector graphics (SVG) tasks, enabling better SVG understanding and generation.


<details>
  <summary>Details</summary>
Motivation: The growing demand for AI-powered systems to understand and generate SVGs stems from their scalability and widespread use in computer vision and design, where consistent quality and precision are required.

Method: The authors developed UniSVG, a dataset specifically for SVG-related tasks, enabling MLLMs to process multi-modal inputs for unified SVG understanding and generation under textual and visual conditions.

Result: Learning on UniSVG significantly improves open-source MLLM performance across SVG U&G tasks, outperforming proprietary models like GPT-4V.

Conclusion: UniSVG proves to be an essential dataset that enhances MLLMs' capacity for accurate and high-quality SVG understanding and generation, pushing the boundaries for open-source models in this domain.

Abstract: Unlike bitmap images, scalable vector graphics (SVG) maintain quality when
scaled, frequently employed in computer vision and artistic design in the
representation of SVG code. In this era of proliferating AI-powered systems,
enabling AI to understand and generate SVG has become increasingly urgent.
However, AI-driven SVG understanding and generation (U&G) remain significant
challenges. SVG code, equivalent to a set of curves and lines controlled by
floating-point parameters, demands high precision in SVG U&G. Besides, SVG
generation operates under diverse conditional constraints, including textual
prompts and visual references, which requires powerful multi-modal processing
for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal
Large Language Models (MLLMs) have demonstrated capabilities to process
multi-modal inputs and generate complex vector controlling parameters,
suggesting the potential to address SVG U&G tasks within a unified model. To
unlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset
called UniSVG, comprising 525k data items, tailored for MLLM training and
evaluation. To our best knowledge, it is the first comprehensive dataset
designed for unified SVG generation (from textual prompts and images) and SVG
understanding (color, category, usage, etc.). As expected, learning on the
proposed dataset boosts open-source MLLMs' performance on various SVG U&G
tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,
benchmark, weights, codes and experiment details on
https://ryanlijinke.github.io/.

</details>


### [322] [Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation](https://arxiv.org/abs/2508.07769)
*Xiaoyan Liu,Kangrui Li,Jiaxin Liu*

Main category: cs.CV

TL;DR: This paper introduces Dream4D, a framework for generating spatiotemporally coherent 4D content using a two-stage process combining video generation and neural reconstruction.


<details>
  <summary>Details</summary>
Motivation: The need to address challenges in creating spatiotemporally coherent 4D content, including maintaining view consistency and handling complex scene dynamics in large-scale environments.

Method: Dream4D uses a two-stage architecture: predicting camera trajectories from a single image via few-shot learning and generating multi-view sequences with a pose-conditioned diffusion process for 4D representation.

Result: Dream4D achieves geometrically consistent 4D generation and demonstrates higher quality results compared to existing methods in metrics like mPSNR and mSSIM.

Conclusion: Dream4D leverages video diffusion models and geometric awareness to successfully address challenges in 4D content generation while achieving high-quality results.

Abstract: The synthesis of spatiotemporally coherent 4D content presents fundamental
challenges in computer vision, requiring simultaneous modeling of high-fidelity
spatial representations and physically plausible temporal dynamics. Current
approaches often struggle to maintain view consistency while handling complex
scene dynamics, particularly in large-scale environments with multiple
interacting elements. This work introduces Dream4D, a novel framework that
bridges this gap through a synergy of controllable video generation and neural
4D reconstruction. Our approach seamlessly combines a two-stage architecture:
it first predicts optimal camera trajectories from a single image using
few-shot learning, then generates geometrically consistent multi-view sequences
via a specialized pose-conditioned diffusion process, which are finally
converted into a persistent 4D representation. This framework is the first to
leverage both rich temporal priors from video diffusion models and geometric
awareness of the reconstruction models, which significantly facilitates 4D
generation and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.

</details>


### [323] [Prototype-Guided Curriculum Learning for Zero-Shot Learning](https://arxiv.org/abs/2508.07771)
*Lei Wang,Shiming Chen,Guo-Sen Xie,Ziming Hong,Chaojian Yu,Qinmu Peng,Xinge You*

Main category: cs.CV

TL;DR: The paper addresses the challenges in Zero-Shot Learning (ZSL) caused by mismatches and inaccuracies in semantic prototypes, proposing a curriculum learning framework (CLZSL) to improve knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Manually defined semantic prototypes in ZSL suffer from instance-level mismatches and class-level inaccuracies, limiting effective knowledge transfer from seen to unseen classes.

Method: The proposed framework, CLZSL, includes two key components: (i) a Prototype-Guided Curriculum Learning (PCL) module that prioritizes highly aligned samples to minimize instance-level mismatches and (ii) a Prototype Update (PUP) module that dynamically refines class-level prototypes based on learned visual mappings.

Result: The proposed method was tested on benchmark datasets (AWA2, SUN, and CUB) and demonstrated its effectiveness in improving the visual-semantic mapping for better ZSL performance.

Conclusion: CLZSL effectively mitigates the challenges of instance-level and class-level mismatches in ZSL, leading to improved knowledge transfer and more accurate visual-semantic mappings.

Abstract: In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge
transfer from seen to unseen classes by learning a visual-semantic mapping from
seen-class images to class-level semantic prototypes (e.g., attributes).
However, these semantic prototypes are manually defined and may introduce noisy
supervision for two main reasons: (i) instance-level mismatch: variations in
perspective, occlusion, and annotation bias will cause discrepancies between
individual sample and the class-level semantic prototypes; and (ii) class-level
imprecision: the manually defined semantic prototypes may not accurately
reflect the true semantics of the class. Consequently, the visual-semantic
mapping will be misled, reducing the effectiveness of knowledge transfer to
unseen classes. In this work, we propose a prototype-guided curriculum learning
framework (dubbed as CLZSL), which mitigates instance-level mismatches through
a Prototype-Guided Curriculum Learning (PCL) module and addresses class-level
imprecision via a Prototype Update (PUP) module. Specifically, the PCL module
prioritizes samples with high cosine similarity between their visual mappings
and the class-level semantic prototypes, and progressively advances to
less-aligned samples, thereby reducing the interference of instance-level
mismatches to achieve accurate visual-semantic mapping. Besides, the PUP module
dynamically updates the class-level semantic prototypes by leveraging the
visual mappings learned from instances, thereby reducing class-level
imprecision and further improving the visual-semantic mapping. Experiments were
conducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the
effectiveness of our method.

</details>


### [324] [Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)](https://arxiv.org/abs/2508.07775)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: The paper introduces a neural network-driven method to model object rotations on the manifold of 3D rotations ($SO(3)$), addressing challenges like non-conservative dynamics and noisy observations.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome difficulties in modeling rotational dynamics, including the complications of unknown inertia, external forces, and noisy trajectory estimation.

Method: The approach utilizes Neural Controlled Differential Equations alongside $SO(3)$ Savitzky-Golay paths to model object rotations, departing from traditional methods that presume energy conservation or constant velocities.

Result: The proposed model demonstrates robust extrapolation capabilities by approximating dynamics from noisy observations during training, suitable for both simulation and real-world scenarios.

Conclusion: The method offers robustness in modeling trajectories under challenging conditions, can be integrated into existing systems, and generalizes well to scenarios with unknown physical parameters.

Abstract: Modeling the rotation of moving objects is a fundamental task in computer
vision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)
unknown quantities such as the moment of inertia complicate dynamics, (2) the
presence of external forces and torques can lead to non-conservative
kinematics, and (3) estimating evolving state trajectories under sparse, noisy
observations requires robustness. We propose modeling trajectories of noisy
pose estimates on the manifold of 3D rotations in a physically and
geometrically meaningful way by leveraging Neural Controlled Differential
Equations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation
methods often rely on energy conservation or constant velocity assumptions,
limiting their applicability in real-world scenarios involving non-conservative
forces. In contrast, our approach is agnostic to energy and momentum
conservation while being robust to input noise, making it applicable to
complex, non-inertial systems. Our approach is easily integrated as a module in
existing pipelines and generalizes well to trajectories with unknown physical
parameters. By learning to approximate object dynamics from noisy states during
training, our model attains robust extrapolation capabilities in simulation and
various real-world settings. Code is available at
https://github.com/bastianlb/forecasting-rotational-dynamics

</details>


### [325] [GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences](https://arxiv.org/abs/2508.07782)
*Saihui Hou,Chenye Wang,Wenpeng Lang,Zhengxiang Lan,Yongzhen Huang*

Main category: cs.CV

TL;DR: This research proposes a snippet-based method for gait recognition to address limitations in set-based and sequence-based approaches, achieving high accuracy on benchmarks like Gait3D and GREW.


<details>
  <summary>Details</summary>
Motivation: Current gait recognition methods fall short in either capturing short-range temporal context (set-based) or long-range dependencies (sequence-based).

Method: The authors introduce a snippet-based approach where human gait is modeled as a composition of randomized, frame-based snippets enabling multi-scale temporal context learning. They focus on Snippet Sampling and Snippet Modeling.

Result: The proposed approach achieves rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D convolution-based model, validated by extensive experimentation.

Conclusion: Snippet-based modeling effectively enhances gait recognition by incorporating multi-scale temporal contexts, presenting a significant improvement in the field and demonstrating the potential of this novel approach.

Abstract: Recent advancements in gait recognition have significantly enhanced
performance by treating silhouettes as either an unordered set or an ordered
sequence. However, both set-based and sequence-based approaches exhibit notable
limitations. Specifically, set-based methods tend to overlook short-range
temporal context for individual frames, while sequence-based methods struggle
to capture long-range temporal dependencies effectively. To address these
challenges, we draw inspiration from human identification and propose a new
perspective that conceptualizes human gait as a composition of individualized
actions. Each action is represented by a series of frames, randomly selected
from a continuous segment of the sequence, which we term a snippet.
Fundamentally, the collection of snippets for a given sequence enables the
incorporation of multi-scale temporal context, facilitating more comprehensive
gait feature learning. Moreover, we introduce a non-trivial solution for
snippet-based gait recognition, focusing on Snippet Sampling and Snippet
Modeling as key components. Extensive experiments on four widely-used gait
datasets validate the effectiveness of our proposed approach and, more
importantly, highlight the potential of gait snippets. For instance, our method
achieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D
convolution-based backbone.

</details>


### [326] [Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake](https://arxiv.org/abs/2508.07795)
*Hongrui Zheng,Yuezun Li,Liejun Wang,Yunfeng Diao,Zhiqing Guo*

Main category: cs.CV

TL;DR: The study proposes a Two-Stage Defense Framework (TSDF) to counteract deepfake threats effectively by employing adversarial perturbations to distort forged content and disrupt attacker model adaptation, ensuring long-term defense.


<details>
  <summary>Details</summary>
Motivation: Conventional active defenses against deepfakes lack persistence, as attackers often retrain to bypass them, limiting their practical application.

Method: The proposed TSDF employs dual-function adversarial perturbations to both distort deepfake outputs and poison data sources, hindering attackers' model retraining processes.

Result: Experiments demonstrate traditional methods lose efficacy under adversarial retraining, whereas TSDF exhibits sustained dual defense capability.

Conclusion: TSDF provides a persistent and innovative solution against deepfake threats by leveraging its dual defense mechanisms, offering improved long-term effectiveness over conventional methods.

Abstract: Active defense strategies have been developed to counter the threat of
deepfake technology. However, a primary challenge is their lack of persistence,
as their effectiveness is often short-lived. Attackers can bypass these
defenses by simply collecting protected samples and retraining their models.
This means that static defenses inevitably fail when attackers retrain their
models, which severely limits practical use. We argue that an effective defense
not only distorts forged content but also blocks the model's ability to adapt,
which occurs when attackers retrain their models on protected images. To
achieve this, we propose an innovative Two-Stage Defense Framework (TSDF).
Benefiting from the intensity separation mechanism designed in this paper, the
framework uses dual-function adversarial perturbations to perform two roles.
First, it can directly distort the forged results. Second, it acts as a
poisoning vehicle that disrupts the data preparation process essential for an
attacker's retraining pipeline. By poisoning the data source, TSDF aims to
prevent the attacker's model from adapting to the defensive perturbations, thus
ensuring the defense remains effective long-term. Comprehensive experiments
show that the performance of traditional interruption methods degrades sharply
when it is subjected to adversarial retraining. However, our framework shows a
strong dual defense capability, which can improve the persistence of active
defense. Our code will be available at https://github.com/vpsg-research/TSDF.

</details>


### [327] [Power Battery Detection](https://arxiv.org/abs/2508.07797)
*Xiaoqi Zhao,Peiqian Cao,Lihe Zhang,Zonglei Feng,Hanqi Liu,Jiaming Zuo,Youwei Pang,Weisi Lin,Georges El Fakhri,Huchuan Lu,Xiaofeng Liu*

Main category: cs.CV

TL;DR: This paper introduces Power Battery Detection (PBD) as a task to identify dense endpoints of cathode and anode plates in X-ray images of batteries, proposing a novel dataset (PBD5K) and approach (MDCNeXt).


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the safety risks posed by structural defects in power batteries and the limitations of manual and traditional vision-based quality inspections.

Method: The researchers developed a large-scale benchmark dataset (PBD5K) and a model called MDCNeXt, which uses multi-dimensional clues and state space modules for better detection and segmentation.

Result: The proposed MDCNeXt model, aided by the strategies like prompt-filtered modules and a density-aware reordering module, effectively handles dense and variable plate structures, outperforming traditional methods.

Conclusion: The study provides practical advancements in power battery endpoint detection for quality inspection, through an innovative dataset and approach. The methodology and resources are made publicly available for future research.

Abstract: Power batteries are essential components in electric vehicles, where internal
structural defects can pose serious safety risks. We conduct a comprehensive
study on a new task, power battery detection (PBD), which aims to localize the
dense endpoints of cathode and anode plates from industrial X-ray images for
quality inspection. Manual inspection is inefficient and error-prone, while
traditional vision algorithms struggle with densely packed plates, low
contrast, scale variation, and imaging artifacts. To address this issue and
drive more attention into this meaningful task, we present PBD5K, the first
large-scale benchmark for this task, consisting of 5,000 X-ray images from nine
battery types with fine-grained annotations and eight types of real-world
visual interference. To support scalable and consistent labeling, we develop an
intelligent annotation pipeline that combines image filtering, model-assisted
pre-labeling, cross-verification, and layered quality evaluation. We formulate
PBD as a point-level segmentation problem and propose MDCNeXt, a model designed
to extract and integrate multi-dimensional structure clues including point,
line, and count information from the plate itself. To improve discrimination
between plates and suppress visual interference, MDCNeXt incorporates two state
space modules. The first is a prompt-filtered module that learns contrastive
relationships guided by task-specific prompts. The second is a density-aware
reordering module that refines segmentation in regions with high plate density.
In addition, we propose a distance-adaptive mask generation strategy to provide
robust supervision under varying spatial distributions of anode and cathode
positions. The source code and datasets will be publicly available at
\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}.

</details>


### [328] [MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks](https://arxiv.org/abs/2508.07803)
*Yushen Xu,Xiaosong Li,Zhenyu Kuang,Xiaoqi Cheng,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: The paper addresses the problem of adapting multimodal fusion images to pretrained object detection and semantic segmentation models. It proposes MambaTrans, a modality translator, to bridge this gap.


<details>
  <summary>Details</summary>
Motivation: The performance of multimodal fused images deteriorates on downstream tasks when using models trained on visible images due to modality differences.

Method: The proposed MambaTrans utilizes mask-image-text cross-attention, a 3D-Selective Scan Module, and object detection prior knowledge to adapt multimodal images for downstream tasks.

Result: Experiments demonstrate that MambaTrans improves the performance of multimodal fused images in object detection and semantic segmentation.

Conclusion: MambaTrans effectively resolves performance issues for multimodal fused images in downstream tasks, achieving significant improvements without altering pre-trained model parameters.

Abstract: The goal of multimodal image fusion is to integrate complementary information
from infrared and visible images, generating multimodal fused images for
downstream tasks. Existing downstream pre-training models are typically trained
on visible images. However, the significant pixel distribution differences
between visible and multimodal fusion images can degrade downstream task
performance, sometimes even below that of using only visible images. This paper
explores adapting multimodal fused images with significant modality differences
to object detection and semantic segmentation models trained on visible images.
To address this, we propose MambaTrans, a novel multimodal fusion image
modality translator. MambaTrans uses descriptions from a multimodal large
language model and masks from semantic segmentation models as input. Its core
component, the Multi-Model State Space Block, combines mask-image-text
cross-attention and a 3D-Selective Scan Module, enhancing pure visual
capabilities. By leveraging object detection prior knowledge, MambaTrans
minimizes detection loss during training and captures long-term dependencies
among text, masks, and images. This enables favorable results in pre-trained
models without adjusting their parameters. Experiments on public datasets show
that MambaTrans effectively improves multimodal image performance in downstream
tasks.

</details>


### [329] [Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.07804)
*Bao Li,Xiaomei Zhang,Miao Xu,Zhaoxin Fan,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: Pose-RFT introduces a reinforcement fine-tuning framework to enhance 3D human pose generation from images and text, addressing limitations of supervised approaches.


<details>
  <summary>Details</summary>
Motivation: Many multimodal large language models struggle with accurate 3D human pose generation due to inherent ambiguity in spatial and semantic correspondences.

Method: Pose-RFT uses a hybrid reinforcement learning algorithm, HyGRPO, for joint optimization of language prediction and continuous pose generation, leveraging task-specific reward functions.

Result: Pose-RFT demonstrated superior performance on multiple benchmarks, outperforming existing pose-specific multimodal models.

Conclusion: Hybrid reinforcement learning fine-tuning effectively addresses challenges in 3D pose generation, enhancing spatial and semantic alignment.

Abstract: Generating 3D human poses from multimodal inputs such as images or text
requires models to capture both rich spatial and semantic correspondences.
While pose-specific multimodal large language models (MLLMs) have shown promise
in this task, they are typically trained with supervised objectives such as
SMPL parameter regression or token-level prediction, which struggle to model
the inherent ambiguity and achieve task-specific alignment required for
accurate 3D pose generation. To address these limitations, we propose Pose-RFT,
a reinforcement fine-tuning framework tailored for 3D human pose generation in
MLLMs. We formulate the task as a hybrid action reinforcement learning problem
that jointly optimizes discrete language prediction and continuous pose
generation. To this end, we introduce HyGRPO, a hybrid reinforcement learning
algorithm that performs group-wise reward normalization over sampled responses
to guide joint optimization of discrete and continuous actions. Pose-RFT
further incorporates task-specific reward functions to guide optimization
towards spatial alignment in image-to-pose generation and semantic consistency
in text-to-pose generation. Extensive experiments on multiple pose generation
benchmarks demonstrate that Pose-RFT significantly improves performance over
existing pose-specific MLLMs, validating the effectiveness of hybrid action
reinforcement fine-tuning for 3D pose generation.

</details>


### [330] [DiTVR: Zero-Shot Diffusion Transformer for Video Restoration](https://arxiv.org/abs/2508.07811)
*Sicheng Gao,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: DiTVR is a diffusion-transformer-based framework for zero-shot video restoration that ensures temporal consistency and detail preservation through flow-aware techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional regression methods lack realism and require paired datasets, while generative diffusion models struggle with temporal consistency in video restoration.

Method: The proposed DiTVR framework incorporates trajectory-aware attention mechanisms, a spatiotemporal neighbor cache, and a flow-guided sampler to better align and restore details across video frames.

Result: DiTVR achieves state-of-the-art performance in zero-shot video restoration benchmarks, showing superior temporal consistency and robustness to noise and occlusions.

Conclusion: DiTVR represents a significant advancement in zero-shot video restoration by leveraging innovative spatiotemporal techniques, ensuring realistic and temporally consistent outputs.

Abstract: Video restoration aims to reconstruct high quality video sequences from low
quality inputs, addressing tasks such as super resolution, denoising, and
deblurring. Traditional regression based methods often produce unrealistic
details and require extensive paired datasets, while recent generative
diffusion models face challenges in ensuring temporal consistency. We introduce
DiTVR, a zero shot video restoration framework that couples a diffusion
transformer with trajectory aware attention and a wavelet guided, flow
consistent sampler. Unlike prior 3D convolutional or frame wise diffusion
approaches, our attention mechanism aligns tokens along optical flow
trajectories, with particular emphasis on vital layers that exhibit the highest
sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically
selects relevant tokens based on motion correspondences across frames. The flow
guided sampler injects data consistency only into low-frequency bands,
preserving high frequency priors while accelerating convergence. DiTVR
establishes a new zero shot state of the art on video restoration benchmarks,
demonstrating superior temporal consistency and detail preservation while
remaining robust to flow noise and occlusions.

</details>


### [331] [Semi-supervised Multiscale Matching for SAR-Optical Image](https://arxiv.org/abs/2508.07812)
*Jingze Gai,Changchun Li*

Main category: cs.CV

TL;DR: This paper introduces a semi-supervised SAR-optical image matching method, leveraging pseudo-labeling and feature disentanglement to reduce reliance on manual annotation.


<details>
  <summary>Details</summary>
Motivation: The challenge of manual annotation in SAR-optical image matching motivates the authors to develop a framework that utilizes labeled and unlabeled image pairs efficiently.

Method: The proposed S2M2-SAR pipeline uses pseudo-labeling of similarity heatmaps, deep/shallow level matching, and a feature enhancement module trained via cross-modality mutual independence loss.

Result: Experimental evaluations show that S2M2-SAR outperforms existing semi-supervised methods and achieves performance comparable to fully supervised SOTA methods.

Conclusion: The approach demonstrates both efficiency and practical potential, offering advancements in SAR-optical image matching without extensive labeling requirements.

Abstract: Driven by the complementary nature of optical and synthetic aperture radar
(SAR) images, SAR-optical image matching has garnered significant interest.
Most existing SAR-optical image matching methods aim to capture effective
matching features by employing the supervision of pixel-level matched
correspondences within SAR-optical image pairs, which, however, suffers from
time-consuming and complex manual annotation, making it difficult to collect
sufficient labeled SAR-optical image pairs. To handle this, we design a
semi-supervised SAR-optical image matching pipeline that leverages both scarce
labeled and abundant unlabeled image pairs and propose a semi-supervised
multiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we
pseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth
similarity heatmaps by combining both deep and shallow level matching results,
and train the matching model by employing labeled and pseudo-labeled similarity
heatmaps. In addition, we introduce a cross-modal feature enhancement module
trained using a cross-modality mutual independence loss, which requires no
ground-truth labels. This unsupervised objective promotes the separation of
modality-shared and modality-specific features by encouraging statistical
independence between them, enabling effective feature disentanglement across
optical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we
compare it with existing competitors on benchmark datasets. Experimental
results demonstrate that S2M2-SAR not only surpasses existing semi-supervised
methods but also achieves performance competitive with fully supervised SOTA
methods, demonstrating its efficiency and practical potential.

</details>


### [332] [Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models](https://arxiv.org/abs/2508.07818)
*Chenyue Song,Chen Hui,Haiqi Zhu,Feng Jiang,Yachun Mi,Wei Zhang,Shaohui Liu*

Main category: cs.CV

TL;DR: This paper introduces RSFIQA, a no-reference image quality assessment model that incorporates region-level distortion information to improve local semantic and quality perception, showing competitive results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current NR-IQA methods are limited by their focus on global representations or uniform weighting of region features, which fail to effectively capture local quality variations aligned with human perception.

Method: The method involves segmenting images dynamically using the Segment Anything Model (SAM) and employing a Multi-modal Large Language Model (MLLM) to comprehend local semantics and quality distortions. The Region-Aware Semantic Attention (RSA) mechanism aggregates fine-grained information for global attention.

Result: RSFIQA demonstrates robustness and achieves competitive performance on multiple benchmark datasets for image quality prediction.

Conclusion: RSFIQA improves NR-IQA by effectively integrating region-level semantic and quality distortion information, making it adaptable for various deep learning architectures while providing accurate quality prediction.

Abstract: No-reference image quality assessment (NR-IQA) aims to simulate the process
of perceiving image quality aligned with subjective human perception. However,
existing NR-IQA methods either focus on global representations that leads to
limited insights into the semantically salient regions or employ a uniform
weighting for region features that weakens the sensitivity to local quality
variations. In this paper, we propose a fine-grained image quality assessment
model, named RSFIQA, which integrates region-level distortion information to
perceive multi-dimensional quality discrepancies. To enhance regional quality
awareness, we first utilize the Segment Anything Model (SAM) to dynamically
partition the input image into non-overlapping semantic regions. For each
region, we teach a powerful Multi-modal Large Language Model (MLLM) to extract
descriptive content and perceive multi-dimensional distortions, enabling a
comprehensive understanding of both local semantics and quality degradations.
To effectively leverage this information, we introduce Region-Aware Semantic
Attention (RSA) mechanism, which generates a global attention map by
aggregating fine-grained representations from local regions. In addition,
RSFIQA is backbone-agnostic and can be seamlessly integrated into various deep
neural network architectures. Extensive experiments demonstrate the robustness
and effectiveness of the proposed method, which achieves competitive quality
prediction performance across multiple benchmark datasets.

</details>


### [333] [Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP](https://arxiv.org/abs/2508.07819)
*Ke Ma,Jun Long,Hongxiao Fei,Liujie Hua,Yueyi Luo*

Main category: cs.CV

TL;DR: Pre-trained Vision-Language Models (VLMs) struggle with Zero-Shot Anomaly Detection (ZSAD), particularly in dense prediction tasks. This paper proposes a refined architecture combining convolutional strategies and adaptive text modulation to improve performance.


<details>
  <summary>Details</summary>
Motivation: To address the adaptation gap in applying Vision-Language Models (VLMs) to Zero-Shot Anomaly Detection (ZSAD), caused by the lack of local inductive biases and rigid feature fusion.

Method: The approach integrates a Conv-LoRA adapter for refining feature representation and a Dynamic Fusion Gateway (DFG) for adaptive text prompt modulation, creating a synergistic architectural co-design.

Result: Experiments on industrial and medical datasets show improved accuracy and robustness in anomaly detection tasks.

Conclusion: The architectural co-design effectively equips VLMs with adaptability for dense perception tasks, making them viable for robust ZSAD applications.

Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap
when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of
local inductive biases for dense prediction and their reliance on inflexible
feature fusion paradigms. We address these limitations through an Architectural
Co-Design framework that jointly refines feature representation and cross-modal
fusion. Our method integrates a parameter-efficient Convolutional Low-Rank
Adaptation (Conv-LoRA) adapter to inject local inductive biases for
fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that
leverages visual context to adaptively modulate text prompts, enabling a
powerful bidirectional fusion. Extensive experiments on diverse industrial and
medical benchmarks demonstrate superior accuracy and robustness, validating
that this synergistic co-design is critical for robustly adapting foundation
models to dense perception tasks.

</details>


### [334] [MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization](https://arxiv.org/abs/2508.07833)
*Animesh Jain,Alexandros Stergiou*

Main category: cs.CV

TL;DR: This paper introduces MIMIC, a framework that visualizes the internal representations of Vision Language Models (VLMs) by synthesizing related visual concepts, improving interpretability.


<details>
  <summary>Details</summary>
Motivation: The complexity and opacity of Vision Language Models limit their transparency and trustworthiness, necessitating tools for better interpretation.

Method: MIMIC leverages a VLM-based inversion technique combined with feature alignment, along with spatial alignment, smoothness, and realism regularizers, to synthesize visual representations of VLM encodings.

Result: MIMIC successfully visualizes visual concepts in VLM representations, validated through both visual quality metrics and semantic text-based metrics.

Conclusion: This framework enables enhanced interpretability of VLMs, marking the first inversion approach for visual interpretations of VLM internal concepts.

Abstract: Vision Language Models (VLMs) encode multimodal inputs over large, complex,
and difficult-to-interpret architectures, which limit transparency and trust.
We propose a Multimodal Inversion for Model Interpretation and
Conceptualization (MIMIC) framework to visualize the internal representations
of VLMs by synthesizing visual concepts corresponding to internal encodings.
MIMIC uses a joint VLM-based inversion and a feature alignment objective to
account for VLM's autoregressive processing. It additionally includes a triplet
of regularizers for spatial alignment, natural image smoothness, and semantic
realism. We quantitatively and qualitatively evaluate MIMIC by inverting visual
concepts over a range of varying-length free-form VLM output texts. Reported
results include both standard visual quality metrics as well as semantic
text-based metrics. To the best of our knowledge, this is the first model
inversion approach addressing visual interpretations of VLM concepts.

</details>


### [335] [Effortless Vision-Language Model Specialization in Histopathology without Annotation](https://arxiv.org/abs/2508.07835)
*Jingna Qiu,Nishanth Jain,Jonas Ammeling,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: The paper explores adapting Vision-Language Models (VLMs) in histopathology without manual annotations by leveraging continued pretraining on domain-specific image-caption pairs, achieving competitive zero-shot and few-shot performance.


<details>
  <summary>Details</summary>
Motivation: General-purpose Vision-Language Models show suboptimal performance in specific tasks, and supervised fine-tuning requires manual labeling, creating a need for annotation-free adaptation solutions.

Method: Continued pretraining of VLMs using domain- and task-relevant image-caption pairs extracted from existing databases.

Result: The approach enhances zero-shot and few-shot performance across three tasks, matching supervised few-shot methods with larger training sizes without requiring manual labeling.

Conclusion: The annotation-free, task-agnostic approach is effective for adapting VLMs to new histopathology tasks, offering a scalable alternative without the need for labeling.

Abstract: Recent advances in Vision-Language Models (VLMs) in histopathology, such as
CONCH and QuiltNet, have demonstrated impressive zero-shot classification
capabilities across various tasks. However, their general-purpose design may
lead to suboptimal performance in specific downstream applications. While
supervised fine-tuning methods address this issue, they require manually
labeled samples for adaptation. This paper investigates annotation-free
adaptation of VLMs through continued pretraining on domain- and task-relevant
image-caption pairs extracted from existing databases. Our experiments on two
VLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs
substantially enhance both zero-shot and few-shot performance. Notably, with
larger training sizes, continued pretraining matches the performance of
few-shot methods while eliminating manual labeling. Its effectiveness,
task-agnostic design, and annotation-free workflow make it a promising pathway
for adapting VLMs to new histopathology tasks. Code is available at
https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.

</details>


### [336] [CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving](https://arxiv.org/abs/2508.07838)
*Qi Xiang,Kunsong Shi,Zhigui Lin,Lei He*

Main category: cs.CV

TL;DR: The paper introduces CBDES MoE, a novel hierarchically decoupled Mixture-of-Experts architecture to improve precision and efficiency in multi-sensor BEV systems for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing BEV systems suffer from limited input adaptability, modeling capacity, and generalization, which hinder optimal autonomous driving performance.

Method: The method involves using structurally heterogeneous expert networks coupled with a Self-Attention Router (SAR) to dynamically select efficient inference paths based on input conditions.

Result: CBDES MoE significantly improves performance metrics in 3D object detection, achieving a 1.6-point increase in mAP and a 4.1-point boost in NDS on the nuScenes dataset.

Conclusion: The modular CBDES MoE framework demonstrates its capability to enhance adaptability, efficiency, and accuracy in autonomous driving systems.

Abstract: Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion
have become a fundamental cornerstone for end-to-end autonomous driving.
However, existing multi-modal BEV methods commonly suffer from limited input
adaptability, constrained modeling capacity, and suboptimal generalization. To
address these challenges, we propose a hierarchically decoupled
Mixture-of-Experts architecture at the functional module level, termed
Computing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE
integrates multiple structurally heterogeneous expert networks with a
lightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic
expert path selection and sparse, input-aware efficient inference. To the best
of our knowledge, this is the first modular Mixture-of-Experts framework
constructed at the functional module granularity within the autonomous driving
domain. Extensive evaluations on the real-world nuScenes dataset demonstrate
that CBDES MoE consistently outperforms fixed single-expert baselines in 3D
object detection. Compared to the strongest single-expert model, CBDES MoE
achieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,
demonstrating the effectiveness and practical advantages of the proposed
approach.

</details>


### [337] [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/abs/2508.07847)
*Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: This paper introduces Deep SWM, a deep learning model designed for solar flare prediction, utilizing state space models and innovative pretraining techniques for improved accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Solar flares can disrupt critical infrastructure, making accurate prediction essential. However, current methods either lack sophisticated representation learning or fail to model long-range dependencies in solar images.

Method: The Deep SWM employs deep state space models and a sparse masked autoencoder for enhanced representation, focusing on key solar features like sunspots. It handles both multi-channel images and long-range dependencies.

Result: Deep SWM surpassed baseline models and human expert predictions in standard metrics for solar flare forecasting, demonstrating improved reliability and accuracy. FlareBench was introduced as a comprehensive evaluation framework.

Conclusion: Deep SWM represents a significant step forward in solar flare prediction, combining advanced model architecture and evaluation benchmarks to achieve state-of-the-art results.

Abstract: Accurate, reliable solar flare prediction is crucial for mitigating potential
disruptions to critical infrastructure, while predicting solar flares remains a
significant challenge. Existing methods based on heuristic physical features
often lack representation learning from solar images. On the other hand,
end-to-end learning approaches struggle to model long-range temporal
dependencies in solar images. In this study, we propose Deep Space Weather
Model (Deep SWM), which is based on multiple deep state space models for
handling both ten-channel solar images and long-range spatio-temporal
dependencies. Deep SWM also features a sparse masked autoencoder, a novel
pretraining strategy that employs a two-phase masking approach to preserve
crucial regions such as sunspots while compressing spatial information.
Furthermore, we built FlareBench, a new public benchmark for solar flare
prediction covering a full 11-year solar activity cycle, to validate our
method. Our method outperformed baseline methods and even human expert
performance on standard metrics in terms of performance and reliability. The
project page can be found at https://keio-smilab25.github.io/DeepSWM.

</details>


### [338] [Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs](https://arxiv.org/abs/2508.07850)
*Noriko Nitta,Rei Miyata,Naoto Oishi*

Main category: cs.CV

TL;DR: This paper processes electron microscopy images of Ge surfaces to extract topological features, embeds them using graph convolutional networks, and evaluates the impact of irradiation angles and fluence on the surface morphology.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the morphological changes in Ge surfaces due to ion beam irradiation, investigating the effects of irradiation angle and fluence.

Method: Electron microscopy images were analyzed to extract skeleton graphs, embedded via a graph convolutional network, and evaluated using PCA and the Davies-Bouldin index.

Result: Irradiation angle causes more significant changes to the surface morphology of Ge than irradiation fluence.

Conclusion: The research highlights the dominant role of irradiation angle in influencing the morphological properties of Ge surfaces, with implications for material design and analysis.

Abstract: In this paper, electron microscopy images of microstructures formed on Ge
surfaces by ion beam irradiation were processed to extract topological features
as skeleton graphs, which were then embedded using a graph convolutional
network. The resulting embeddings were analyzed using principal component
analysis, and cluster separability in the resulting PCA space was evaluated
using the Davies-Bouldin index. The results indicate that variations in
irradiation angle have a more significant impact on the morphological
properties of Ge surfaces than variations in irradiation fluence.

</details>


### [339] [Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images](https://arxiv.org/abs/2508.07851)
*Konrad Reuter,Suresh Guttikonda,Sarah Latus,Lennart Maack,Christian Betz,Tobias Maurer,Alexander Schlaefer*

Main category: cs.CV

TL;DR: This paper introduces a method for markerless 3D tissue tracking during minimally invasive surgery by leveraging 2D Tracking Any Point (TAP) networks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in minimally invasive surgeries, such as dynamic tissue motion and limited field of view, by improving tissue tracking for safer, more efficient procedures.

Method: The proposed technique combines two CoTracker models—one for temporal tracking and another for stereo matching—to estimate 3D motion using stereo endoscopic images.

Result: The method was tested on synthetic and chicken tissue phantoms, achieving Euclidean distance errors as low as 1.1 mm at 10 mm/s velocity on the chicken tissue phantom.

Conclusion: The study demonstrates the potential of TAP-based models for accurate markerless 3D tracking, which could enhance surgical guidance and robotic assistance in challenging scenarios.

Abstract: Minimally invasive surgery presents challenges such as dynamic tissue motion
and a limited field of view. Accurate tissue tracking has the potential to
support surgical guidance, improve safety by helping avoid damage to sensitive
structures, and enable context-aware robotic assistance during complex
procedures. In this work, we propose a novel method for markerless 3D tissue
tracking by leveraging 2D Tracking Any Point (TAP) networks. Our method
combines two CoTracker models, one for temporal tracking and one for stereo
matching, to estimate 3D motion from stereo endoscopic images. We evaluate the
system using a clinical laparoscopic setup and a robotic arm simulating tissue
motion, with experiments conducted on a synthetic 3D-printed phantom and a
chicken tissue phantom. Tracking on the chicken tissue phantom yielded more
reliable results, with Euclidean distance errors as low as 1.1 mm at a velocity
of 10 mm/s. These findings highlight the potential of TAP-based models for
accurate, markerless 3D tracking in challenging surgical scenarios.

</details>


### [340] [Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/2508.07863)
*Bin Cao,Sipeng Zheng,Ye Wang,Lujie Xia,Qianshan Wei,Qin Jin,Jing Liu,Zongqing Lu*

Main category: cs.CV

TL;DR: The paper introduces Being-M0.5, a real-time, controllable motion generation model that addresses limitations of current Vision-Language-Motion Models (VLMMs), using the new HuMo100M dataset and novel techniques for detailed motion control.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current VLMMs in key aspects like diverse command response, long-sequence performance, unseen scenario handling, initialization flexibility, and fine-grained body part control.

Method: Developed Being-M0.5, leveraging the new HuMo100M dataset along with a novel part-aware residual quantization technique for fine-grained motion control.

Result: Being-M0.5 achieves state-of-the-art performance across multiple motion generation benchmarks and demonstrates real-time capabilities verified through efficiency analysis.

Conclusion: The study showcases significant advancements in motion generation through innovative methods and dataset contributions, offering a practical path for real-world adoption of such technologies.

Abstract: Human motion generation has emerged as a critical technology with
transformative potential for real-world applications. However, existing
vision-language-motion models (VLMMs) face significant limitations that hinder
their practical deployment. We identify controllability as a main bottleneck,
manifesting in five key aspects: inadequate response to diverse human commands,
limited pose initialization capabilities, poor performance on long-term
sequences, insufficient handling of unseen scenarios, and lack of fine-grained
control over individual body parts. To overcome these limitations, we present
Being-M0.5, the first real-time, controllable VLMM that achieves
state-of-the-art performance across multiple motion generation tasks. Our
approach is built upon HuMo100M, the largest and most comprehensive human
motion dataset to date, comprising over 5 million self-collected motion
sequences, 100 million multi-task instructional instances, and detailed
part-level annotations that address a critical gap in existing datasets. We
introduce a novel part-aware residual quantization technique for motion
tokenization that enables precise, granular control over individual body parts
during generation. Extensive experimental validation demonstrates Being-M0.5's
superior performance across diverse motion benchmarks, while comprehensive
efficiency analysis confirms its real-time capabilities. Our contributions
include design insights and detailed computational analysis to guide future
development of practical motion generators. We believe that HuMo100M and
Being-M0.5 represent significant advances that will accelerate the adoption of
motion generation technologies in real-world applications. The project page is
available at https://beingbeyond.github.io/Being-M0.5.

</details>


### [341] [CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/abs/2508.07871)
*Yanshu Li,Jianjiang Yang,Zhennan Shen,Ligong Han,Haoyan Xu,Ruixiang Tang*

Main category: cs.CV

TL;DR: The paper introduces Contextually Adaptive Token Pruning (CATP), a method to efficiently reduce image token redundancy in vision-language models (LVLMs) designed for multimodal in-context learning (ICL).


<details>
  <summary>Details</summary>
Motivation: Current LVLMs suffer from token redundancy, especially in multimodal ICL tasks, leading to inefficient inference and unstable performance. Existing pruning methods largely focus on single-image tasks and fail to address multimodal settings effectively, highlighting the need for better approaches.

Method: The proposed CATP performs progressive image token pruning in two stages, ensuring the reduction accounts for cross-modal interactions in multimodal tasks. This is achieved without additional training requirements.

Result: CATP eliminates 77.8% of image tokens, boosts performance by an average of 0.6%, and reduces inference latency by 10.78% across multiple LVLMs and benchmarks.

Conclusion: CATP improves the efficiency and performance of LVLMs in multimodal ICL tasks, demonstrating significant practical advantages and paving the way for better image-text integration methods in future research.

Abstract: Modern large vision-language models (LVLMs) convert each input image into a
large set of tokens, far outnumbering the text tokens. Although this improves
visual perception, it introduces severe image token redundancy. Because image
tokens carry sparse information, many add little to reasoning, yet greatly
increase inference cost. The emerging image token pruning methods tackle this
issue by identifying the most important tokens and discarding the rest. These
methods can raise efficiency with only modest performance loss. However, most
of them only consider single-image tasks and overlook multimodal in-context
learning (ICL), where redundancy is greater and efficiency is more critical.
Redundant tokens weaken the advantage of multimodal ICL for rapid domain
adaptation and cause unstable performance. Applying existing pruning methods in
this setting leads to large accuracy drops, exposing a clear gap and the need
for new techniques. Thus, we propose Contextually Adaptive Token Pruning
(CATP), a training-free pruning method targeted at multimodal ICL. CATP
consists of two stages that perform progressive pruning to fully account for
the complex cross-modal interactions in the input sequence. After removing
77.8\% of the image tokens, CATP produces an average performance gain of 0.6\%
over the vanilla model on four LVLMs and eight benchmarks, exceeding all
baselines remarkably. Meanwhile, it effectively improves efficiency by
achieving an average reduction of 10.78\% in inference latency. CATP enhances
the practical value of multimodal ICL and lays the groundwork for future
progress in interleaved image-text scenarios.

</details>


### [342] [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2508.07877)
*WonJun Moon,Hyun Seok Seong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: The paper introduces a novel method for weakly supervised affordance grounding (WSAG) that effectively identifies object parts which afford specific actions without relying on detailed annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in overcoming challenges in WSAG, where prior methods often misfocus on irrelevant class-specific patterns instead of affordance cues.

Method: The paper proposes selective prototypical and pixel contrastive objectives combined with CLIP-based cross-referencing of egocentric and exocentric images to extract affordance-relevant cues.

Result: Experimental results confirm the method's improved ability to shift focus from irrelevant areas to meaningful affordance cues while achieving better grounding.

Conclusion: This method enhances object affordance grounding by adaptively learning cues significant to both part-level and object-level interactions, demonstrating its practical effectiveness.

Abstract: Facilitating an entity's interaction with objects requires accurately
identifying parts that afford specific actions. Weakly supervised affordance
grounding (WSAG) seeks to imitate human learning from third-person
demonstrations, where humans intuitively grasp functional parts without needing
pixel-level annotations. To achieve this, grounding is typically learned using
a shared classifier across images from different perspectives, along with
distillation strategies incorporating part discovery process. However, since
affordance-relevant parts are not always easily distinguishable, models
primarily rely on classification, often focusing on common class-specific
patterns that are unrelated to affordance. To address this limitation, we move
beyond isolated part-level learning by introducing selective prototypical and
pixel contrastive objectives that adaptively learn affordance-relevant cues at
both the part and object levels, depending on the granularity of the available
information. Initially, we find the action-associated objects in both
egocentric (object-focused) and exocentric (third-person example) images by
leveraging CLIP. Then, by cross-referencing the discovered objects of
complementary views, we excavate the precise part-level affordance clues in
each perspective. By consistently learning to distinguish affordance-relevant
regions from affordance-irrelevant background context, our approach effectively
shifts activation from irrelevant areas toward meaningful affordance cues.
Experimental results demonstrate the effectiveness of our method. Codes are
available at github.com/hynnsk/SelectiveCL.

</details>


### [343] [TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal](https://arxiv.org/abs/2508.07878)
*Hanting Wang,Shengpeng Ji,Shulei Wang,Hai Huang,Xiao Jin,Qifei Zhang,Tao Jin*

Main category: cs.CV

TL;DR: The paper addresses image restoration under adverse weather conditions using a newly proposed All-in-One framework, which uses task-aware enhanced prompts to significantly improve efficiency and accuracy while minimizing parameters.


<details>
  <summary>Details</summary>
Motivation: To mitigate inefficiencies in prior All-in-One image restoration methods, which have high parameter overhead and overlook relatedness across different tasks.

Method: The authors propose a parameter-efficient two-stage training paradigm with pretraining and prompt-tuning phases, using task-aware enhanced prompts involving low-rank decomposition and contrastive constraints.

Result: Experimental findings show improved performance across multiple image restoration tasks with just 2.75M parameters, demonstrating effective task-specific adaptation.

Conclusion: The approach successfully balances efficiency and accuracy in image restoration tasks, providing a more optimized framework for handling adverse weather degradation.

Abstract: Image restoration under adverse weather conditions has been extensively
explored, leading to numerous high-performance methods. In particular, recent
advances in All-in-One approaches have shown impressive results by training on
multi-task image restoration datasets. However, most of these methods rely on
dedicated network modules or parameters for each specific degradation type,
resulting in a significant parameter overhead. Moreover, the relatedness across
different restoration tasks is often overlooked. In light of these issues, we
propose a parameter-efficient All-in-One image restoration framework that
leverages task-aware enhanced prompts to tackle various adverse weather
degradations.Specifically, we adopt a two-stage training paradigm consisting of
a pretraining phase and a prompt-tuning phase to mitigate parameter conflicts
across tasks. We first employ supervised learning to acquire general
restoration knowledge, and then adapt the model to handle specific degradation
via trainable soft prompts. Crucially, we enhance these task-specific prompts
in a task-aware manner. We apply low-rank decomposition to these prompts to
capture both task-general and task-specific characteristics, and impose
contrastive constraints to better align them with the actual inter-task
relatedness. These enhanced prompts not only improve the parameter efficiency
of the restoration model but also enable more accurate task modeling, as
evidenced by t-SNE analysis. Experimental results on different restoration
tasks demonstrate that the proposed method achieves superior performance with
only 2.75M parameters.

</details>


### [344] [NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction](https://arxiv.org/abs/2508.07897)
*Tianle Zeng,Junlei Hu,Gerardo Loza Galindo,Sharib Ali,Duygu Sarikaya,Pietro Valdastri,Dominic Jones*

Main category: cs.CV

TL;DR: The paper introduces dynamic Gaussian Splatting for generating labeled surgical image datasets to address data scarcity and enhance model performances.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the limitations posed by the high demand for large, high-quality labeled image datasets in surgical applications, which hinders progress.

Method: They present a dynamic Gaussian model for rendering surgical instruments and a dynamic training adjustment strategy, paired with synthetic annotations generation to tackle challenges like camera pose calibration.

Result: The novel method generates photo-realistic datasets with high Peak-Signal-to-Noise Ratio (PSNR) and improves model performance on unseen real-world images by up to 15%.

Conclusion: Dynamic Gaussian Splatting offers a promising solution to data scarcity in surgical science, enhancing image quality and the efficacy of neural network training.

Abstract: Computer vision-based technologies significantly enhance surgical automation
by advancing tool tracking, detection, and localization. However, Current
data-driven approaches are data-voracious, requiring large, high-quality
labeled image datasets, which limits their application in surgical data
science. Our Work introduces a novel dynamic Gaussian Splatting technique to
address the data scarcity in surgical image datasets. We propose a dynamic
Gaussian model to represent dynamic surgical scenes, enabling the rendering of
surgical instruments from unseen viewpoints and deformations with real tissue
backgrounds. We utilize a dynamic training adjustment strategy to address
challenges posed by poorly calibrated camera poses from real-world scenarios.
Additionally, we propose a method based on dynamic Gaussians for automatically
generating annotations for our synthetic data. For evaluation, we constructed a
new dataset featuring seven scenes with 14,000 frames of tool and camera motion
and tool jaw articulation, with a background of an ex-vivo porcine model. Using
this dataset, we synthetically replicate the scene deformation from the ground
truth data, allowing direct comparisons of synthetic image quality.
Experimental results illustrate that our method generates photo-realistic
labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio
(29.87). We further evaluate the performance of medical-specific neural
networks trained on real and synthetic images using an unseen real-world image
dataset. Our results show that the performance of models trained on synthetic
images generated by the proposed method outperforms those trained with
state-of-the-art standard data augmentation by 10%, leading to an overall
improvement in model performances by nearly 15%.

</details>


### [345] [Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation](https://arxiv.org/abs/2508.07901)
*Bowen Xue,Qixin Yan,Wenjing Wang,Hao Liu,Chen Li*

Main category: cs.CV

TL;DR: A lightweight framework, Stand-In, is proposed for high-quality identity-preserving video generation with minimal additional parameters, outperforming full-parameter methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving high-fidelity human video generation with low parameter requirements and compatibility with other AIGC tools.

Method: Stand-In introduces a conditional image branch into pre-trained video models, utilizing restricted self-attentions with conditional position mapping, requiring training on only ~1% extra parameters.

Result: The framework delivers excellent video quality and identity preservation and outperforms full-parameter training methods.

Conclusion: Stand-In is effective, efficient, and flexible, enabling seamless integration into diverse applications like pose-referenced video generation, stylization, and face swapping.

Abstract: Generating high-fidelity human videos that match user-specified identities is
important yet challenging in the field of generative AI. Existing methods often
rely on an excessive number of training parameters and lack compatibility with
other AIGC tools. In this paper, we propose Stand-In, a lightweight and
plug-and-play framework for identity preservation in video generation.
Specifically, we introduce a conditional image branch into the pre-trained
video generation model. Identity control is achieved through restricted
self-attentions with conditional position mapping, and can be learned quickly
with only 2000 pairs. Despite incorporating and training just $\sim$1\%
additional parameters, our framework achieves excellent results in video
quality and identity preservation, outperforming other full-parameter training
methods. Moreover, our framework can be seamlessly integrated for other tasks,
such as subject-driven video generation, pose-referenced video generation,
stylization, and face swapping.

</details>


### [346] [CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality](https://arxiv.org/abs/2508.07904)
*Marco Peer,Anna Scius-Bertrand,Andreas Fischer*

Main category: cs.CV

TL;DR: The paper introduces a self-training method for improving alignment accuracy and text recognition in historical handwritten documents using CTC alignment. It releases new corrected data and provides an iterative strategy for refinement.


<details>
  <summary>Details</summary>
Motivation: The core motivation is to improve handwritten text recognition in historical documents, which face challenges like handwriting variability, degraded sources, and annotation issues, specifically targeting hyphenation errors in 16th-century Bullinger correspondence.

Method: The method employs a self-training pipeline using CTC alignment, combining dynamic programming with model output probabilities, and trains on CTC loss. Iterative training is executed to refine alignment and recognition accuracy.

Result: The approach improves Character Error Rate (CER) by 1.1 percentage points with PyLaia and enhances alignment accuracy. Weaker models were found unexpectedly more effective for accurate alignment.

Conclusion: The proposed method is effective for text recognition pipelines and allows iterative improvement in CER and alignment quality. A new manually corrected subset of 100 pages is provided, along with the code and benchmarks for reproducibility.

Abstract: Handwritten text recognition for historical documents remains challenging due
to handwriting variability, degraded sources, and limited layout-aware
annotations. In this work, we address annotation errors - particularly
hyphenation issues - in the Bullinger correspondence, a large 16th-century
letter collection. We introduce a self-training method based on a CTC alignment
algorithm that matches full transcriptions to text line images using dynamic
programming and model output probabilities trained with the CTC loss. Our
approach improves performance (e.g., by 1.1 percentage points CER with PyLaia)
and increases alignment accuracy. Interestingly, we find that weaker models
yield more accurate alignments, enabling an iterative training strategy. We
release a new manually corrected subset of 100 pages from the Bullinger
dataset, along with our code and benchmarks. Our approach can be applied
iteratively to further improve the CER as well as the alignment quality for
text recognition pipelines. Code and data are available via
https://github.com/andreas-fischer-unifr/nntp.

</details>


### [347] [Generative Video Matting](https://arxiv.org/abs/2508.07905)
*Yongtao Ge,Kangyang Xie,Guangkai Xu,Mingyu Liu,Li Ke,Longtao Huang,Hui Xue,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: The paper introduces a new video matting approach utilizing pre-trained video diffusion models and a synthetic data generation pipeline to bridge the gap between synthetic and real-world applications.


<details>
  <summary>Details</summary>
Motivation: Existing video matting approaches struggle with generalization to real-world scenarios due to the lack of high-quality ground-truth data.

Method: The authors emphasize large-scale pre-training using diverse synthetic and pseudo-labeled datasets, coupled with a scalable synthetic data generation pipeline and novel architecture leveraging pre-trained video diffusion models.

Result: Comprehensive evaluations across benchmark datasets show superior performance, and qualitative results illustrate strong generalization in real-world scenes.

Conclusion: The proposed method improves temporal consistency and real-world generalization, setting a new standard in video matting.

Abstract: Video matting has traditionally been limited by the lack of high-quality
ground-truth data. Most existing video matting datasets provide only
human-annotated imperfect alpha and foreground annotations, which must be
composited to background images or videos during the training stage. Thus, the
generalization capability of previous methods in real-world scenarios is
typically poor. In this work, we propose to solve the problem from two
perspectives. First, we emphasize the importance of large-scale pre-training by
pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also
develop a scalable synthetic data generation pipeline that can render diverse
human bodies and fine-grained hairs, yielding around 200 video clips with a
3-second duration for fine-tuning. Second, we introduce a novel video matting
approach that can effectively leverage the rich priors from pre-trained video
diffusion models. This architecture offers two key advantages. First, strong
priors play a critical role in bridging the domain gap between synthetic and
real-world scenes. Second, unlike most existing methods that process video
matting frame-by-frame and use an independent decoder to aggregate temporal
information, our model is inherently designed for video, ensuring strong
temporal consistency. We provide a comprehensive quantitative evaluation across
three benchmark datasets, demonstrating our approach's superior performance,
and present comprehensive qualitative results in diverse real-world scenes,
illustrating the strong generalization capability of our method. The code is
available at https://github.com/aim-uofa/GVM.

</details>


### [348] [Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07908)
*Xudong Cai,Shuo Wang,Peng Wang,Yongcai Wang,Zhaoxin Fan,Wanting Li,Tianbao Zhang,Jianrong Tao,Yeying Jin,Deying Li*

Main category: cs.CV

TL;DR: Mem4D introduces a framework to resolve the inherent conflict in monocular video geometry reconstruction by decoupling static and dynamic scene modeling using a dual-memory architecture.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the Memory Demand Dilemma, where existing methods for dynamic scene reconstruction face a tradeoff between stable static geometry and detailed dynamic motion modeling, causing inaccuracies in results.

Method: Mem4D utilizes a dual-memory architecture composed of the Transient Dynamics Memory (TDM) for capturing high-frequency motion details and the Persistent Structure Memory (PSM) for long-term preservation of static geometries.

Result: Mem4D achieves state-of-the-art or competitive performance in reconstructing dynamic scenes without compromising global consistency or motion detail fidelity, as showcased in challenging benchmarks.

Conclusion: The proposed Mem4D framework effectively resolves the Memory Demand Dilemma by decoupling static and dynamic structures, providing efficient, detailed, and accurate scene reconstruction with publicly available codes.

Abstract: Reconstructing dense geometry for dynamic scenes from a monocular video is a
critical yet challenging task. Recent memory-based methods enable efficient
online reconstruction, but they fundamentally suffer from a Memory Demand
Dilemma: The memory representation faces an inherent conflict between the
long-term stability required for static structures and the rapid, high-fidelity
detail retention needed for dynamic motion. This conflict forces existing
methods into a compromise, leading to either geometric drift in static
structures or blurred, inaccurate reconstructions of dynamic objects. To
address this dilemma, we propose Mem4D, a novel framework that decouples the
modeling of static geometry and dynamic motion. Guided by this insight, we
design a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)
focuses on capturing high-frequency motion details from recent frames, enabling
accurate and fine-grained modeling of dynamic content; 2) The Persistent
Structure Memory (PSM) compresses and preserves long-term spatial information,
ensuring global consistency and drift-free reconstruction for static elements.
By alternating queries to these specialized memories, Mem4D simultaneously
maintains static geometry with global consistency and reconstructs dynamic
elements with high fidelity. Experiments on challenging benchmarks demonstrate
that our method achieves state-of-the-art or competitive performance while
maintaining high efficiency. Codes will be publicly available.

</details>


### [349] [RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering](https://arxiv.org/abs/2508.07918)
*Xing Zi,Jinghao Xiao,Yunxiao Shi,Xian Tao,Jun Li,Ali Braytee,Mukesh Prasad*

Main category: cs.CV

TL;DR: This paper introduces RSVLM-QA, a new large-scale, annotated Visual Question Answering dataset tailored for remote sensing, addressing limitations in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing RS VQA datasets lack annotation richness, diverse questions, and sufficient evaluation for reasoning capabilities.

Method: Authors integrated RS dataset data and employed GPT-4.1 for annotation generation, including captions, spatial relations, and semantic tags, along with a custom object counting pipeline.

Result: RSVLM-QA contains 13,820 images and 162,373 VQA pairs, featuring diverse and rich annotations; benchmark tests reveal its efficacy in evaluating Vision Language Models.

Conclusion: RSVLM-QA acts as a key resource for RS and VLM research communities, advancing capabilities in Earth data interpretation and reasoning.

Abstract: Visual Question Answering (VQA) in remote sensing (RS) is pivotal for
interpreting Earth observation data. However, existing RS VQA datasets are
constrained by limitations in annotation richness, question diversity, and the
assessment of specific reasoning capabilities. This paper introduces RSVLM-QA
dataset, a new large-scale, content-rich VQA dataset for the RS domain.
RSVLM-QA is constructed by integrating data from several prominent RS
segmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ
an innovative dual-track annotation generation pipeline. Firstly, we leverage
Large Language Models (LLMs), specifically GPT-4.1, with meticulously designed
prompts to automatically generate a suite of detailed annotations including
image captions, spatial relations, and semantic tags, alongside complex
caption-based VQA pairs. Secondly, to address the challenging task of object
counting in RS imagery, we have developed a specialized automated process that
extracts object counts directly from the original segmentation data; GPT-4.1
then formulates natural language answers from these counts, which are paired
with preset question templates to create counting QA pairs. RSVLM-QA comprises
13,820 images and 162,373 VQA pairs, featuring extensive annotations and
diverse question types. We provide a detailed statistical analysis of the
dataset and a comparison with existing RS VQA benchmarks, highlighting the
superior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct
benchmark experiments on Six mainstream Vision Language Models (VLMs),
demonstrating that RSVLM-QA effectively evaluates and challenges the
understanding and reasoning abilities of current VLMs in the RS domain. We
believe RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM
research communities, poised to catalyze advancements in the field.

</details>


### [350] [Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection](https://arxiv.org/abs/2508.07923)
*Jakub Binda,Valentina Paneta,Vasileios Eleftheriadis,Hongkyou Chung,Panagiotis Papadimitroulas,Neo Christopher Chung*

Main category: cs.CV

TL;DR: This paper presents a hybrid anomaly detection framework to ensure the reliability of generative AI models in biomedical imaging, specifically for tasks in nuclear medicine.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of unexpected and erroneous behaviors in generative AI models used in high-stakes applications, such as nuclear medicine imaging.

Method: The proposed approach integrates a hybrid anomaly detection framework to oversee generative AI in real-time applications, demonstrated in Pose2Xray and DosimetrEYE systems.

Result: The framework successfully enhances reliability and quality control while minimizing manual intervention in synthetic X-ray generation and radiation dose map estimation.

Conclusion: This methodology bolsters the industrial applicability of generative AI in preclinical nuclear medicine, providing robustness, scalability, and improved compliance.

Abstract: Generative AI holds great potentials to automate and enhance data synthesis
in nuclear medicine. However, the high-stakes nature of biomedical imaging
necessitates robust mechanisms to detect and manage unexpected or erroneous
model behavior. We introduce development and implementation of a hybrid anomaly
detection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.
Two applications are demonstrated: Pose2Xray, which generates synthetic X-rays
from photographic mouse images, and DosimetrEYE, which estimates 3D radiation
dose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)
enhances reliability, reduces manual oversight, and supports real-time quality
control. This approach strengthens the industrial viability of GenAI in
preclinical settings by increasing robustness, scalability, and regulatory
compliance.

</details>


### [351] [TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding](https://arxiv.org/abs/2508.07925)
*Jin-Seop Lee,SungJoon Lee,Jaehan Ahn,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: The paper introduces TAG, a method to improve zero-shot video temporal grounding by addressing semantic fragmentation and skewed similarity issues without relying on costly LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve zero-shot VTG (Video Temporal Grounding) performance, which currently suffers from semantic fragmentation, skewed similarity distributions, and heavy reliance on expensive LLMs.

Method: The proposed method, TAG, integrates temporal pooling, temporal coherence clustering, and similarity adjustment to enhance temporal continuity and address similarity distortions in video frames without additional training.

Result: TAG outperforms existing methods, achieving state-of-the-art results on the Charades-STA and ActivityNet Captions benchmark datasets.

Conclusion: TAG is an effective and simple approach that improves zero-shot VTG by addressing major limitations, offering improved accuracy and efficiency without depending on LLMs.

Abstract: Video Temporal Grounding (VTG) aims to extract relevant video segments based
on a given natural language query. Recently, zero-shot VTG methods have gained
attention by leveraging pretrained vision-language models (VLMs) to localize
target moments without additional training. However, existing approaches suffer
from semantic fragmentation, where temporally continuous frames sharing the
same semantics are split across multiple segments. When segments are
fragmented, it becomes difficult to predict an accurate target moment that
aligns with the text query. Also, they rely on skewed similarity distributions
for localization, making it difficult to select the optimal segment.
Furthermore, they heavily depend on the use of LLMs which require expensive
inferences. To address these limitations, we propose a \textit{TAG}, a simple
yet effective Temporal-Aware approach for zero-shot video temporal Grounding,
which incorporates temporal pooling, temporal coherence clustering, and
similarity adjustment. Our proposed method effectively captures the temporal
context of videos and addresses distorted similarity distributions without
training. Our approach achieves state-of-the-art results on Charades-STA and
ActivityNet Captions benchmark datasets without rely on LLMs. Our code is
available at https://github.com/Nuetee/TAG

</details>


### [352] [VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security](https://arxiv.org/abs/2508.07960)
*Ajnas Muhammed,Iurri Medvedev,Nuno Gonçalves*

Main category: cs.CV

TL;DR: VOIDFace is a novel facial recognition framework focusing on privacy, security, and efficiency by eliminating data replication and enabling user control.


<details>
  <summary>Details</summary>
Motivation: Address growing privacy and ethical concerns in facial recognition systems, particularly around data replication and loss of user control.

Method: VOIDFace uses visual secret sharing for secure data storage and employs a patch-based multi-training network to protect sensitive user data.

Result: VOIDFace enhances privacy, data control, and security, providing the Right-To-Be-Forgotten feature while achieving competitive recognition performance.

Conclusion: VOIDFace successfully integrates novel mechanisms to tackle ethical concerns in facial recognition training workflows without sacrificing accuracy.

Abstract: Advancement of machine learning techniques, combined with the availability of
large-scale datasets, has significantly improved the accuracy and efficiency of
facial recognition. Modern facial recognition systems are trained using large
face datasets collected from diverse individuals or public repositories.
However, for training, these datasets are often replicated and stored in
multiple workstations, resulting in data replication, which complicates
database management and oversight. Currently, once a user submits their face
for dataset preparation, they lose control over how their data is used, raising
significant privacy and ethical concerns. This paper introduces VOIDFace, a
novel framework for facial recognition systems that addresses two major issues.
First, it eliminates the need of data replication and improves data control to
securely store training face data by using visual secret sharing. Second, it
proposes a patch-based multi-training network that uses this novel training
data storage mechanism to develop a robust, privacy-preserving facial
recognition system. By integrating these advancements, VOIDFace aims to improve
the privacy, security, and efficiency of facial recognition training, while
ensuring greater control over sensitive personal face data. VOIDFace also
enables users to exercise their Right-To-Be-Forgotten property to control their
personal data. Experimental evaluations on the VGGFace2 dataset show that
VOIDFace provides Right-To-Be-Forgotten, improved data control, security, and
privacy while maintaining competitive facial recognition performance. Code is
available at: https://github.com/ajnasmuhammed89/VOIDFace

</details>


### [353] [TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking](https://arxiv.org/abs/2508.07968)
*Tony Danjun Wang,Christian Heiliger,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: TrackOR is a framework for long-term multi-person tracking in operating rooms using 3D geometric signatures, achieving better online and offline tracking efficiency.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve patient outcomes by providing personalized, intelligent systems for surgical teams, requiring long-term tracking of staff in complex operating room scenarios.

Method: The method involves using 3D geometric signatures for tracking and re-identification of staff in operating rooms, ensuring both online tracking accuracy and offline trajectory recovery.

Result: TrackOR outperforms existing systems by 11% in Association Accuracy and enables staff-centric analyses through persistent identity tracking.

Conclusion: Leveraging 3D geometry enables personalized, efficient tracking, paving the way for intelligent applications to assist operating room teams and enhance safety and team dynamics.

Abstract: Providing intelligent support to surgical teams is a key frontier in
automated surgical scene understanding, with the long-term goal of improving
patient outcomes. Developing personalized intelligence for all staff members
requires maintaining a consistent state of who is located where for long
surgical procedures, which still poses numerous computational challenges. We
propose TrackOR, a framework for tackling long-term multi-person tracking and
re-identification in the operating room. TrackOR uses 3D geometric signatures
to achieve state-of-the-art online tracking performance (+11% Association
Accuracy over the strongest baseline), while also enabling an effective offline
recovery process to create analysis-ready trajectories. Our work shows that by
leveraging 3D geometric information, persistent identity tracking becomes
attainable, enabling a critical shift towards the more granular, staff-centric
analyses required for personalized intelligent systems in the operating room.
This new capability opens up various applications, including our proposed
temporal pathway imprints that translate raw tracking data into actionable
insights for improving team efficiency and safety and ultimately providing
personalized support.

</details>


### [354] [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](https://arxiv.org/abs/2508.07981)
*Fangyuan Mao,Aiming Hao,Jintao Chen,Dongxia Liu,Xiaokun Feng,Jiashu Zhu,Meiqi Wu,Chubin Chen,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Omni-Effects introduces a unified approach for generating spatially controllable visual effects in video production, overcoming traditional limitations.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods are limited by their inability to produce spatially controlled composite visual effects, which is crucial for advanced VFX applications.

Method: The paper introduces the Omni-Effects framework, featuring LoRA-based Mixture of Experts for managing diverse visual effects and Spatial-Aware Prompt for spatial control of effects.

Result: Omni-Effects successfully integrates diverse VFX into a single framework and allows precise spatial control, demonstrated via extensive experiments and a novel dataset.

Conclusion: Omni-Effects significantly advances VFX generation by enabling spatially controlled composite effects, improving operational efficiency and creative flexibility.

Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern
cinematic production. Although video generation models offer cost-efficient
solutions for VFX production, current methods are constrained by per-effect
LoRA training, which limits generation to single effects. This fundamental
limitation impedes applications that require spatially controllable composite
effects, i.e., the concurrent generation of multiple effects at designated
locations. However, integrating diverse effects into a unified framework faces
major challenges: interference from effect variations and spatial
uncontrollability during multi-VFX joint training. To tackle these challenges,
we propose Omni-Effects, a first unified framework capable of generating
prompt-guided effects and spatially controllable composite effects. The core of
our framework comprises two key innovations: (1) LoRA-based Mixture of Experts
(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects
within a unified model while effectively mitigating cross-task interference.
(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the
text token, enabling precise spatial control. Furthermore, we introduce an
Independent-Information Flow (IIF) module integrated within the SAP, isolating
the control signals corresponding to individual effects to prevent any unwanted
blending. To facilitate this research, we construct a comprehensive VFX dataset
Omni-VFX via a novel data collection pipeline combining image editing and
First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX
evaluation framework for validating model performance. Extensive experiments
demonstrate that Omni-Effects achieves precise spatial control and diverse
effect generation, enabling users to specify both the category and location of
desired effects.

</details>


### [355] [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/abs/2508.07989)
*Xiantao Zhang*

Main category: cs.CV

TL;DR: This paper highlights a significant limitation in multimodal large language models (MLLMs), namely their inability to perceive continuous motion effectively, exemplified by their failure to determine escalator directions.


<details>
  <summary>Details</summary>
Motivation: To improve the trustworthiness and safety of MLLMs for the blind and visually impaired (BVI) community by addressing their limitations in recognizing low-signal, continuous motion in videos.

Method: The authors analyze the limitations of frame-sampling paradigms in video understanding and propose a shift in focus towards dynamic perception and human-centered benchmarks.

Result: The paper identifies "Implicit Motion Blindness" as a critical issue and raises awareness of its implications for user trust and safety.

Conclusion: A paradigm shift towards robust physical perception and safety-focused benchmarks is essential for enhancing the reliability of assistive technologies in dynamic real-world environments.

Abstract: Multimodal Large Language Models (MLLMs) hold immense promise as assistive
technologies for the blind and visually impaired (BVI) community. However, we
identify a critical failure mode that undermines their trustworthiness in
real-world applications. We introduce the Escalator Problem -- the inability of
state-of-the-art models to perceive an escalator's direction of travel -- as a
canonical example of a deeper limitation we term Implicit Motion Blindness.
This blindness stems from the dominant frame-sampling paradigm in video
understanding, which, by treating videos as discrete sequences of static
images, fundamentally struggles to perceive continuous, low-signal motion. As a
position paper, our contribution is not a new model but rather to: (I) formally
articulate this blind spot, (II) analyze its implications for user trust, and
(III) issue a call to action. We advocate for a paradigm shift from purely
semantic recognition towards robust physical perception and urge the
development of new, human-centered benchmarks that prioritize safety,
reliability, and the genuine needs of users in dynamic environments.

</details>


### [356] [Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models](https://arxiv.org/abs/2508.07996)
*Thinesh Thiyakesan Ponbagavathi,Chengzheng Yang,Alina Roitberg*

Main category: cs.CV

TL;DR: The paper introduces ProGraD, a method leveraging learnable group prompts and a lightweight transformer for Group Activity Detection (GAD), yielding state-of-the-art performance, particularly in complex multi-group scenarios.


<details>
  <summary>Details</summary>
Motivation: To improve group activity detection by addressing limitations in vision foundation models (VFMs), which are pretrained on object-centric data and struggle with modeling group dynamics.

Method: The approach, ProGraD, incorporates learnable group prompts to steer VFM attention toward social interactions and uses a lightweight two-layer GroupContext Transformer to infer actor-group associations and activities.

Result: ProGraD achieves state-of-the-art results on two benchmarks, with a notable improvement of up to 8.2% Group mAP in complex multi-group setups, using only 10M trainable parameters.

Conclusion: ProGraD demonstrates the effectiveness of structured, group-aware reasoning for GAD over traditional methods and VFMs, with its interpretable attention maps offering added insights into actor-group reasoning.

Abstract: Group Activity Detection (GAD) involves recognizing social groups and their
collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,
offer excellent features, but are pretrained primarily on object-centric data
and remain underexplored for modeling group dynamics. While they are a
promising alternative to highly task-specific GAD architectures that require
full fine-tuning, our initial investigation reveals that simply swapping CNN
backbones used in these methods with VFMs brings little gain, underscoring the
need for structured, group-aware reasoning on top.
  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method
that bridges this gap through 1) learnable group prompts to guide the VFM
attention toward social configurations, and 2) a lightweight two-layer
GroupContext Transformer that infers actor-group associations and collective
behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which
features multiple concurrent social groups, and Social-CAD, which focuses on
single-group interactions. While we surpass state-of-the-art in both settings,
our method is especially effective in complex multi-group scenarios, where we
yield a gain of 6.5\% (Group mAP\@1.0) and 8.2\% (Group mAP\@0.5) using only
10M trainable parameters. Furthermore, our experiments reveal that ProGraD
produces interpretable attention maps, offering insights into actor-group
reasoning. Code and models will be released.

</details>


### [357] [Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition](https://arxiv.org/abs/2508.08004)
*Anqi Xiao,Weichen Yu,Hongyuan Yu*

Main category: cs.CV

TL;DR: The paper introduces Sample-aware RandAugment (SRA), a novel search-free approach to automatic data augmentation that dynamically adjusts augmentation policies based on sample complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in automatic data augmentation methods, such as excessive time consumption and suboptimal performance due to inadequate policy adaptation.

Method: SRA uses a heuristic scoring module to evaluate sample complexity and applies tailored augmentations using an asymmetric augmentation strategy without requiring hyperparameter tuning.

Result: SRA achieves state-of-the-art Top-1 accuracy of 78.31% on ImageNet with ResNet-50 and shows compatibility with existing augmentation pipelines and solid generalization for new tasks.

Conclusion: SRA simplifies AutoDA methods, demonstrating effectiveness and practicality, while enhancing recognition capabilities in downstream object detection tasks.

Abstract: Automatic data augmentation (AutoDA) plays an important role in enhancing the
generalization of neural networks. However, mainstream AutoDA methods often
encounter two challenges: either the search process is excessively
time-consuming, hindering practical application, or the performance is
suboptimal due to insufficient policy adaptation during training. To address
these issues, we propose Sample-aware RandAugment (SRA), an asymmetric,
search-free AutoDA method that dynamically adjusts augmentation policies while
maintaining straightforward implementation. SRA incorporates a heuristic
scoring module that evaluates the complexity of the original training data,
enabling the application of tailored augmentations for each sample.
Additionally, an asymmetric augmentation strategy is employed to maximize the
potential of this scoring module. In multiple experimental settings, SRA
narrows the performance gap between search-based and search-free AutoDA
methods, achieving a state-of-the-art Top-1 accuracy of 78.31\% on ImageNet
with ResNet-50. Notably, SRA demonstrates good compatibility with existing
augmentation pipelines and solid generalization across new tasks, without
requiring hyperparameter tuning. The pretrained models leveraging SRA also
enhance recognition in downstream object detection tasks. SRA represents a
promising step towards simpler, more effective, and practical AutoDA designs
applicable to a variety of future tasks. Our code is available at
\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment

</details>


### [358] [Mitigating Biases in Surgical Operating Rooms with Geometry](https://arxiv.org/abs/2508.08028)
*Tony Danjun Wang,Tobias Czempiel,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: The paper highlights biases in surgical operating room (OR) tasks where deep learning models use spurious correlations like footwear or eyewear for personnel modeling. It introduces encoding personnel as 3D point cloud sequences to avoid such biases, demonstrating superior performance compared to RGB-based methods in clinical settings.


<details>
  <summary>Details</summary>
Motivation: To address biases in intelligent assistance systems in surgical ORs that arise from the exploitation of shallow correlations in visual data rather than meaningful features.

Method: The paper introduces the usage of 3D point cloud sequences to model OR personnel, focusing on shape and motion dynamics to avoid spurious dependencies on appearance-based features.

Result: Experimentation shows that geometric-based methods outperform RGB-based methods, especially in realistic clinical settings with less visual diversity, with a 12% accuracy improvement.

Conclusion: Geometric representations provide a robust alternative for modeling humans in the OR by capturing meaningful biometric traits, reducing reliance on superficial visual cues.

Abstract: Deep neural networks are prone to learning spurious correlations, exploiting
dataset-specific artifacts rather than meaningful features for prediction. In
surgical operating rooms (OR), these manifest through the standardization of
smocks and gowns that obscure robust identifying landmarks, introducing model
bias for tasks related to modeling OR personnel. Through gradient-based
saliency analysis on two public OR datasets, we reveal that CNN models succumb
to such shortcuts, fixating on incidental visual cues such as footwear beneath
surgical gowns, distinctive eyewear, or other role-specific identifiers.
Avoiding such biases is essential for the next generation of intelligent
assistance systems in the OR, which should accurately recognize personalized
workflow traits, such as surgical skill level or coordination with other staff
members. We address this problem by encoding personnel as 3D point cloud
sequences, disentangling identity-relevant shape and motion patterns from
appearance-based confounders. Our experiments demonstrate that while RGB and
geometric methods achieve comparable performance on datasets with apparent
simulation artifacts, RGB models suffer a 12% accuracy drop in realistic
clinical settings with decreased visual diversity due to standardizations. This
performance gap confirms that geometric representations capture more meaningful
biometric features, providing an avenue to developing robust methods of
modeling humans in the OR.

</details>


### [359] [TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation](https://arxiv.org/abs/2508.08038)
*Huawei Sun,Zixu Wang,Hao Feng,Julius Ott,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: This paper proposes TRIDE, a method integrating radar-camera fusion with weather-aware features and advanced text-based depth estimation, outperforming existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing radar-camera fusion techniques for depth estimation, particularly under varying weather conditions and to incorporate text features for enhanced performance.

Method: The paper introduces a text-generation strategy alongside fusion techniques and a radar-camera fusion system (TRIDE) with weather-aware radar weighting to enhance depth estimation.

Result: TRIDE demonstrates substantial improvements in depth estimation accuracy, showing 12.87% better MAE and 9.08% better RMSE on the nuScenes dataset.

Conclusion: By integrating advanced fusion methods and weather condition adaptations, TRIDE outperforms state-of-the-art techniques, offering robust depth estimation solutions.

Abstract: Depth estimation, essential for autonomous driving, seeks to interpret the 3D
environment surrounding vehicles. The development of radar sensors, known for
their cost-efficiency and robustness, has spurred interest in radar-camera
fusion-based solutions. However, existing algorithms fuse features from these
modalities without accounting for weather conditions, despite radars being
known to be more robust than cameras under adverse weather. Additionally, while
Vision-Language models have seen rapid advancement, utilizing language
descriptions alongside other modalities for depth estimation remains an open
challenge. This paper first introduces a text-generation strategy along with
feature extraction and fusion techniques that can assist monocular depth
estimation pipelines, leading to improved accuracy across different algorithms
on the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion
algorithm that enhances text feature extraction by incorporating radar point
information. To address the impact of weather on sensor performance, we
introduce a weather-aware fusion block that adaptively adjusts radar weighting
based on current weather conditions. Our method, benchmarked on the nuScenes
dataset, demonstrates performance gains over the state-of-the-art, achieving a
12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:
https://github.com/harborsarah/TRIDE

</details>


### [360] [S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix](https://arxiv.org/abs/2508.08048)
*Peng Dai,Feitong Tan,Qiangeng Xu,Yihua Huang,David Futschik,Ruofei Du,Sean Fanello,Yinda Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: This paper introduces a novel method to convert monocular videos into immersive 3D stereoscopic and spatial videos without requiring additional model training.


<details>
  <summary>Details</summary>
Motivation: Current video generation models are adept at producing high-quality monocular videos but lack effective solutions for generating immersive 3D stereoscopic and spatial videos suitable for applications like virtual reality.

Method: The method employs depth estimation to warp monocular videos into multiple camera viewpoints. It uses a unique frame matrix inpainting framework to synthesize missing content, ensuring spatial and temporal consistency, alongside a dual-update scheme to mitigate latent space artifacts.

Result: Experiments conducted across various generative models (Sora, Lumiere, WALT, Zeroscope) show that the proposed approach significantly enhances video quality compared to existing methods.

Conclusion: The approach successfully extends monocular video generation models to create immersive 3D and spatial videos, providing improved consistency and quality without requiring model fine-tuning.

Abstract: While video generation models excel at producing high-quality monocular
videos, generating 3D stereoscopic and spatial videos for immersive
applications remains an underexplored challenge. We present a pose-free and
training-free method that leverages an off-the-shelf monocular video generation
model to produce immersive 3D videos. Our approach first warps the generated
monocular video into pre-defined camera viewpoints using estimated depth
information, then applies a novel \textit{frame matrix} inpainting framework.
This framework utilizes the original video generation model to synthesize
missing content across different viewpoints and timestamps, ensuring spatial
and temporal consistency without requiring additional model fine-tuning.
Moreover, we develop a \dualupdate~scheme that further improves the quality of
video inpainting by alleviating the negative effects propagated from
disoccluded areas in the latent space. The resulting multi-view videos are then
adapted into stereoscopic pairs or optimized into 4D Gaussians for spatial
video synthesis. We validate the efficacy of our proposed method by conducting
experiments on videos from various generative models, such as Sora, Lumiere,
WALT, and Zeroscope. The experiments demonstrate that our method has a
significant improvement over previous methods. Project page at:
https://daipengwa.github.io/S-2VG_ProjectPage/

</details>


### [361] [PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI](https://arxiv.org/abs/2508.08058)
*Ziad Al-Haj Hemidi,Eytan Kats,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: PrIINeR leverages deep learning-derived prior knowledge to enhance image reconstruction in accelerated MRI using Implicit Neural Representations (INRs), addressing aliasing artefacts and structural loss.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of degraded image quality at high acceleration factors in MRI reconstruction using INRs, which usually suffer from weak prior constraints.

Method: PrIINeR introduces a framework combining pre-trained deep learning model-derived prior information with instance-focused optimization and dual data consistency for MRI reconstruction.

Result: PrIINeR demonstrated superior performance in reconstructing MRI images compared to both INR-based and multiple state-of-the-art learning-based methods on the NYU fastMRI dataset.

Conclusion: The proposed PrIINeR method effectively bridges deep learning with INR-based approaches, improving structural fidelity, removing artefacts, and enhancing reliability for accelerated MRI reconstruction.

Abstract: Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often
degrades image quality. While Implicit Neural Representations (INRs) show
promise for MRI reconstruction, they struggle at high acceleration factors due
to weak prior constraints, leading to structural loss and aliasing artefacts.
To address this, we propose PrIINeR, an INR-based MRI reconstruction method
that integrates prior knowledge from pre-trained deep learning models into the
INR framework. By combining population-level knowledge with instance-based
optimization and enforcing dual data consistency, PrIINeR aligns both with the
acquired k-space data and the prior-informed reconstruction. Evaluated on the
NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based
approaches but also improves upon several learning-based state-of-the-art
methods, significantly improving structural preservation and fidelity while
effectively removing aliasing artefacts.PrIINeR bridges deep learning and
INR-based techniques, offering a more reliable solution for high-quality,
accelerated MRI reconstruction. The code is publicly available on
https://github.com/multimodallearning/PrIINeR.

</details>


### [362] [Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition](https://arxiv.org/abs/2508.08069)
*Xiaoxiao Cui,Yiran Li,Kai He,Shanzhi Jiang,Mengli Xue,Wentao Li,Junhong Leng,Zhi Liu,Lizhen Cui,Shuo Li*

Main category: cs.CV

TL;DR: The paper addresses multi-label classification (MLC) for medical images by improving the extraction of class-specific features using an Information Bottleneck-based Causal Attention (IBCA) method.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the accuracy and interpretability of MLC in medical imaging by addressing the challenge of existing models inadvertently focusing on class-irrelevant features.

Method: The method introduces a Structural Causal Model (SCM) for modeling class-specific attention and proposes a Gaussian mixture attention mechanism combined with contrastive enhancement to mitigate spurious attention and noise.

Result: The proposed IBCA method outperforms existing approaches with significant improvements across several key metrics such as CR, OR, and mAP on the MuReD and Endo datasets.

Conclusion: IBCA provides a more effective and interpretable solution for MLC of medical images by focusing on accurate class-specific attention, showcasing its potential for clinical applicability.

Abstract: Multi-label classification (MLC) of medical images aims to identify multiple
diseases and holds significant clinical potential. A critical step is to learn
class-specific features for accurate diagnosis and improved interpretability
effectively. However, current works focus primarily on causal attention to
learn class-specific features, yet they struggle to interpret the true cause
due to the inadvertent attention to class-irrelevant features. To address this
challenge, we propose a new structural causal model (SCM) that treats
class-specific attention as a mixture of causal, spurious, and noisy factors,
and a novel Information Bottleneck-based Causal Attention (IBCA) that is
capable of learning the discriminative class-specific attention for MLC of
medical images. Specifically, we propose learning Gaussian mixture multi-label
spatial attention to filter out class-irrelevant information and capture each
class-specific attention pattern. Then a contrastive enhancement-based causal
intervention is proposed to gradually mitigate the spurious attention and
reduce noise information by aligning multi-head attention with the Gaussian
mixture multi-label spatial. Quantitative and ablation results on Endo and
MuReD show that IBCA outperforms all methods. Compared to the second-best
results for each metric, IBCA achieves improvements of 6.35\% in CR, 7.72\% in
OR, and 5.02\% in mAP for MuReD, 1.47\% in CR, and 1.65\% in CF1, and 1.42\% in
mAP for Endo.

</details>


### [363] [ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness](https://arxiv.org/abs/2508.08082)
*Zizheng Guo,Bochao Zou,Junbao Zhuo,Huimin Ma*

Main category: cs.CV

TL;DR: The paper introduces ME-TST and ME-TST+, state space model-based architectures for micro-expression (ME) analysis, combining spotting and recognition tasks to improve precision and performance.


<details>
  <summary>Details</summary>
Motivation: Address limitations of earlier deep learning methods for ME analysis, such as fixed window lengths, hard classifications, and treating spotting and recognition as separate tasks.

Method: Develop ME-TST and ME-TST+ architectures, utilizing temporal state transition mechanisms and advanced modeling techniques like multi-granularity ROI and the slowfast Mamba framework.

Result: Proposed methods achieve state-of-the-art performance in ME analysis, supported by extensive experiments.

Conclusion: The integration of ME spotting and recognition with innovative architectures improves overall analysis efficacy, benefiting ME dynamics modeling and recognition accuracy.

Abstract: Micro-expressions (MEs) are regarded as important indicators of an
individual's intrinsic emotions, preferences, and tendencies. ME analysis
requires spotting of ME intervals within long video sequences and recognition
of their corresponding emotional categories. Previous deep learning approaches
commonly employ sliding-window classification networks. However, the use of
fixed window lengths and hard classification presents notable limitations in
practice. Furthermore, these methods typically treat ME spotting and
recognition as two separate tasks, overlooking the essential relationship
between them. To address these challenges, this paper proposes two state space
model-based architectures, namely ME-TST and ME-TST+, which utilize temporal
state transition mechanisms to replace conventional window-level classification
with video-level regression. This enables a more precise characterization of
the temporal dynamics of MEs and supports the modeling of MEs with varying
durations. In ME-TST+, we further introduce multi-granularity ROI modeling and
the slowfast Mamba framework to alleviate information loss associated with
treating ME analysis as a time-series task. Additionally, we propose a synergy
strategy for spotting and recognition at both the feature and result levels,
leveraging their intrinsic connection to enhance overall analysis performance.
Extensive experiments demonstrate that the proposed methods achieve
state-of-the-art performance. The codes are available at
https://github.com/zizheng-guo/ME-TST.

</details>


### [364] [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086)
*Zhongqi Yang,Wenhang Ge,Yuqi Li,Jiaqi Chen,Haoyuan Li,Mengyin An,Fei Kang,Hua Xue,Baixin Xu,Yuyang Yin,Eric Li,Yang Liu,Yikai Wang,Hao-Xiang Guo,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-3D leverages panoramic representation for omnidirectional 3D world generation, enabling spatial intelligence from a single image or text prompt.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with generating visually consistent and wide-scope 3D worlds, limiting applications in spatial intelligence tasks.

Method: Matrix-3D blends trajectory-guided panoramic video diffusion with scene mesh renders and uses both feed-forward and optimization-based approaches for 3D reconstruction.

Result: Matrix-3D achieves state-of-the-art performance in panoramic video generation and creates explorable 3D worlds efficiently and accurately.

Conclusion: Matrix-3D provides a novel framework that advances 3D world generation through panoramic video diffusion and introduces a unique dataset for enhancing training.

Abstract: Explorable 3D world generation from a single image or text prompt forms a
cornerstone of spatial intelligence. Recent works utilize video model to
achieve wide-scope and generalizable 3D world generation. However, existing
approaches often suffer from a limited scope in the generated scenes. In this
work, we propose Matrix-3D, a framework that utilize panoramic representation
for wide-coverage omnidirectional explorable 3D world generation that combines
conditional video generation and panoramic 3D reconstruction. We first train a
trajectory-guided panoramic video diffusion model that employs scene mesh
renders as condition, to enable high-quality and geometrically consistent scene
video generation. To lift the panorama scene video to 3D world, we propose two
separate methods: (1) a feed-forward large panorama reconstruction model for
rapid 3D scene reconstruction and (2) an optimization-based pipeline for
accurate and detailed 3D scene reconstruction. To facilitate effective
training, we also introduce the Matrix-Pano dataset, the first large-scale
synthetic collection comprising 116K high-quality static panoramic video
sequences with depth and trajectory annotations. Extensive experiments
demonstrate that our proposed framework achieves state-of-the-art performance
in panoramic video generation and 3D world generation. See more in
https://matrix-3d.github.io.

</details>


### [365] [MDD-Net: Multimodal Depression Detection through Mutual Transformer](https://arxiv.org/abs/2508.08093)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: This paper introduces MDD-Net, a network using acoustic and visual data for depression detection, showcasing superior results compared to other systems.


<details>
  <summary>Details</summary>
Motivation: To leverage social media data for improved mental health diagnostics, focusing on depression using multimodal approaches.

Method: The paper proposed MDD-Net, which integrates acoustic and visual data using mutual transformers to enhance feature extraction and fusion for detecting depression.

Result: The proposed method, tested on the D-Vlog dataset, showed up to a 17.37% improvement in F1-Score compared to the state-of-the-art systems.

Conclusion: MDD-Net offers an advanced solution for multimodal depression detection and significantly improves detection capabilities over existing methods.

Abstract: Depression is a major mental health condition that severely impacts the
emotional and physical well-being of individuals. The simple nature of data
collection from social media platforms has attracted significant interest in
properly utilizing this information for mental health research. A Multimodal
Depression Detection Network (MDD-Net), utilizing acoustic and visual data
obtained from social media networks, is proposed in this work where mutual
transformers are exploited to efficiently extract and fuse multimodal features
for efficient depression detection. The MDD-Net consists of four core modules:
an acoustic feature extraction module for retrieving relevant acoustic
attributes, a visual feature extraction module for extracting significant
high-level patterns, a mutual transformer for computing the correlations among
the generated features and fusing these features from multiple modalities, and
a detection layer for detecting depression using the fused feature
representations. The extensive experiments are performed using the multimodal
D-Vlog dataset, and the findings reveal that the developed multimodal
depression detection network surpasses the state-of-the-art by up to 17.37% for
F1-Score, demonstrating the greater performance of the proposed system. The
source code is accessible at
https://github.com/rezwanh001/Multimodal-Depression-Detection.

</details>


### [366] [3D Plant Root Skeleton Detection and Extraction](https://arxiv.org/abs/2508.08094)
*Jiakai Lin,Jinchang Zhang,Ge Jin,Wenzhan Song,Tianming Liu,Guoyu Lu*

Main category: cs.CV

TL;DR: This paper introduces a method to extract 3D root architectures of plants from limited images, facilitating accurate root structure analysis for advanced agriculture and breeding robots.


<details>
  <summary>Details</summary>
Motivation: Exploring the 3D architecture of plant roots is crucial for understanding genetic traits and enhancing root development studies, as traditional 2D methods are insufficient for capturing their complexity.

Method: The paper presents a 3D root skeleton extraction technique involving lateral root detection and matching, triangulation for skeletal structure extraction, and integration of lateral and primary roots.

Result: The proposed model was tested on a complex root dataset, and the extracted 3D root skeletons closely matched the ground truth, demonstrating the method's accuracy.

Conclusion: The method enables accurate and efficient analysis of plant root systems, making breeding practices more intelligent and efficient by reducing manual intervention and aiding in seed selection for superior root traits.

Abstract: Plant roots typically exhibit a highly complex and dense architecture,
incorporating numerous slender lateral roots and branches, which significantly
hinders the precise capture and modeling of the entire root system.
Additionally, roots often lack sufficient texture and color information, making
it difficult to identify and track root traits using visual methods. Previous
research on roots has been largely confined to 2D studies; however, exploring
the 3D architecture of roots is crucial in botany. Since roots grow in real 3D
space, 3D phenotypic information is more critical for studying genetic traits
and their impact on root development. We have introduced a 3D root skeleton
extraction method that efficiently derives the 3D architecture of plant roots
from a few images. This method includes the detection and matching of lateral
roots, triangulation to extract the skeletal structure of lateral roots, and
the integration of lateral and primary roots. We developed a highly complex
root dataset and tested our method on it. The extracted 3D root skeletons
showed considerable similarity to the ground truth, validating the
effectiveness of the model. This method can play a significant role in
automated breeding robots. Through precise 3D root structure analysis, breeding
robots can better identify plant phenotypic traits, especially root structure
and growth patterns, helping practitioners select seeds with superior root
systems. This automated approach not only improves breeding efficiency but also
reduces manual intervention, making the breeding process more intelligent and
efficient, thus advancing modern agriculture.

</details>


### [367] [TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning](https://arxiv.org/abs/2508.08098)
*Junzhe Xu,Yuyang Yin,Xi Chen*

Main category: cs.CV

TL;DR: The paper presents TBAC-UniImage, a unified model that integrates a pre-trained Diffusion Model with a Multimodal Large Language Model for enhanced multimodal understanding and generation.


<details>
  <summary>Details</summary>
Motivation: Existing unified models either rely on shallow connections between components or require computationally expensive pretraining methods, creating barriers to effective multimodal model development.

Method: The proposed method uses intermediate representations from multiple layers of the MLLM as conditions for the pre-trained Diffusion Model, enabling a richer integration between the two models.

Result: TBAC-UniImage achieves deeper and more fine-grained multimodal understanding and generation compared to previous approaches.

Conclusion: The new paradigm of utilizing diverse-layer outputs for generative conditions offers a computationally efficient and effective solution to unify multimodal understanding and generation.

Abstract: This paper introduces TBAC-UniImage, a novel unified model for multimodal
understanding and generation. We achieve this by deeply integrating a
pre-trained Diffusion Model, acting as a generative ladder, with a Multimodal
Large Language Model (MLLM). Previous diffusion-based unified models face two
primary limitations. One approach uses only the MLLM's final hidden state as
the generative condition. This creates a shallow connection, as the generator
is isolated from the rich, hierarchical representations within the MLLM's
intermediate layers. The other approach, pretraining a unified generative
architecture from scratch, is computationally expensive and prohibitive for
many researchers. To overcome these issues, our work explores a new paradigm.
Instead of relying on a single output, we use representations from multiple,
diverse layers of the MLLM as generative conditions for the diffusion model.
This method treats the pre-trained generator as a ladder, receiving guidance
from various depths of the MLLM's understanding process. Consequently,
TBAC-UniImage achieves a much deeper and more fine-grained unification of
understanding and generation.

</details>


### [368] [Hyperspectral Imaging](https://arxiv.org/abs/2508.08107)
*Danfeng Hong,Chenyu Li,Naoto Yokoya,Bing Zhang,Xiuping Jia,Antonio Plaza,Paolo Gamba,Jon Atli Benediktsson,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: This paper provides a detailed overview of hyperspectral imaging (HSI), addressing its principles, methods, challenges, and applications across diverse fields.


<details>
  <summary>Details</summary>
Motivation: The paper aims to consolidate foundational and advanced knowledge of HSI to enable its broader understanding and application in various interdisciplinary fields.

Method: The authors review physical principles, sensor architectures, data handling processes, analysis methods (classical and AI-driven), and application examples. They also address challenges and propose emerging solutions for HSI development.

Result: HSI is presented as a versatile and powerful tool for diverse applications such as diagnostics, monitoring, and decision-making, with current advancements and solutions for its challenges outlined.

Conclusion: HSI has transformative potential across science, technology, and society, with the future direction focused on real-time, scalable systems and leveraging AI innovations.

Abstract: Hyperspectral imaging (HSI) is an advanced sensing modality that
simultaneously captures spatial and spectral information, enabling
non-invasive, label-free analysis of material, chemical, and biological
properties. This Primer presents a comprehensive overview of HSI, from the
underlying physical principles and sensor architectures to key steps in data
acquisition, calibration, and correction. We summarize common data structures
and highlight classical and modern analysis methods, including dimensionality
reduction, classification, spectral unmixing, and AI-driven techniques such as
deep learning. Representative applications across Earth observation, precision
agriculture, biomedicine, industrial inspection, cultural heritage, and
security are also discussed, emphasizing HSI's ability to uncover sub-visual
features for advanced monitoring, diagnostics, and decision-making. Persistent
challenges, such as hardware trade-offs, acquisition variability, and the
complexity of high-dimensional data, are examined alongside emerging solutions,
including computational imaging, physics-informed modeling, cross-modal fusion,
and self-supervised learning. Best practices for dataset sharing,
reproducibility, and metadata documentation are further highlighted to support
transparency and reuse. Looking ahead, we explore future directions toward
scalable, real-time, and embedded HSI systems, driven by sensor
miniaturization, self-supervised learning, and foundation models. As HSI
evolves into a general-purpose, cross-disciplinary platform, it holds promise
for transformative applications in science, technology, and society.

</details>


### [369] [GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking](https://arxiv.org/abs/2508.08117)
*Xudong Han,Pengcheng Fang,Yueying Tian,Jianhui Yu,Xiaohao Cai,Daniel Roggen,Philip Birch*

Main category: cs.CV

TL;DR: The paper presents GRASPTrack, a MOT framework integrating depth estimation to improve object tracking in monocular videos, particularly under occlusion and depth ambiguity.


<details>
  <summary>Details</summary>
Motivation: Conventional tracking-by-detection methods struggle with occlusions and depth ambiguity due to a lack of geometric context. This paper aims to address these limitations by introducing depth-aware tracking.

Method: The GRASPTrack framework integrates monocular depth estimation and instance segmentation to produce 3D point clouds. These point clouds are voxelized to enable robust 3D IoU for tracking. Additional techniques include adaptive Kalman filter noise adjustment and a depth-enhanced observation-centric momentum model.

Result: The system achieves competitive results on benchmarks like MOT17, MOT20, and DanceTrack, showing improved robustness in handling occlusions and complex object trajectories.

Conclusion: GRASPTrack offers a significant advancement in monocular object tracking by leveraging 3D geometric reasoning, making it effective in challenging scenarios with occlusion and motion complexities.

Abstract: Multi-object tracking (MOT) in monocular videos is fundamentally challenged
by occlusions and depth ambiguity, issues that conventional
tracking-by-detection (TBD) methods struggle to resolve owing to a lack of
geometric awareness. To address these limitations, we introduce GRASPTrack, a
novel depth-aware MOT framework that integrates monocular depth estimation and
instance segmentation into a standard TBD pipeline to generate high-fidelity 3D
point clouds from 2D detections, thereby enabling explicit 3D geometric
reasoning. These 3D point clouds are then voxelized to enable a precise and
robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To
further enhance tracking robustness, our approach incorporates Depth-aware
Adaptive Noise Compensation, which dynamically adjusts the Kalman filter
process noise based on occlusion severity for more reliable state estimation.
Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which
extends the motion direction consistency from the image plane into 3D space to
improve motion-based association cues, particularly for objects with complex
trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack
benchmarks demonstrate that our method achieves competitive performance,
significantly improving tracking robustness in complex scenes with frequent
occlusions and intricate motion patterns.

</details>


### [370] [Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control](https://arxiv.org/abs/2508.08134)
*Zeqian Long,Mingzhe Zheng,Kunyu Feng,Xinhua Zhang,Hongyu Liu,Harry Yang,Linfeng Zhang,Qifeng Chen,Yue Ma*

Main category: cs.CV

TL;DR: The paper introduces "Follow-Your-Shape," a training-free, mask-free framework for precise editing of object shapes in images while preserving non-target areas. It also presents a new benchmark, ReShapeBench, demonstrating its effectiveness in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing image editing models that struggle with large-scale shape transformations and preserving non-target regions during structural edits.

Method: The authors propose a framework based on the calculation of a Trajectory Divergence Map (TDM) that compares token-wise velocity differences during inversion and denoising. This guides a Scheduled KV Injection mechanism for stable and faithful editing.

Result: The framework shows superior editability and visual fidelity in experiments, especially for tasks requiring significant shape alterations, and performs well on the newly introduced ReShapeBench benchmark.

Conclusion: The method successfully enables precise shape editing without negatively impacting non-target content and introduces tools for better evaluation of such tasks.

Abstract: While recent flow-based image editing models demonstrate general-purpose
capabilities across diverse tasks, they often struggle to specialize in
challenging scenarios -- particularly those involving large-scale shape
transformations. When performing such structural edits, these methods either
fail to achieve the intended shape change or inadvertently alter non-target
regions, resulting in degraded background quality. We propose
Follow-Your-Shape, a training-free and mask-free framework that supports
precise and controllable editing of object shapes while strictly preserving
non-target content. Motivated by the divergence between inversion and editing
trajectories, we compute a Trajectory Divergence Map (TDM) by comparing
token-wise velocity differences between the inversion and denoising paths. The
TDM enables precise localization of editable regions and guides a Scheduled KV
Injection mechanism that ensures stable and faithful editing. To facilitate a
rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120
new images and enriched prompt pairs specifically curated for shape-aware
editing. Experiments demonstrate that our method achieves superior editability
and visual fidelity, particularly in tasks requiring large-scale shape
replacement.

</details>


### [371] [FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting](https://arxiv.org/abs/2508.08136)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Huajie Wang,Shuting He*

Main category: cs.CV

TL;DR: The paper presents FantasyStyle, a framework for 3D Gaussian Splatting (3DGS)-based style transfer, addressing multi-view consistency and content leakage challenges using a diffusion model distillation approach.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS-based style transfer methods struggle with multi-view inconsistency and over-reliance on VGG features, leading to style conflicts, appearance distortions, and content leakage.

Method: FantasyStyle employs Multi-View Frequency Consistency, using 3D filters to reduce low-frequency conflicts across views, and Controllable Stylized Distillation, introducing negative guidance to suppress undesired content while optimizing 3D Gaussians effectively.

Result: FantasyStyle consistently outperforms state-of-the-art methods in stylization quality and visual realism across diverse scenes and styles.

Conclusion: The proposed framework enhances the quality of 3D style transfer, offering improved consistency, reduced content leakage, and greater visual realism by using innovative diffusion-based techniques.

Abstract: The success of 3DGS in generative and editing applications has sparked
growing interest in 3DGS-based style transfer. However, current methods still
face two major challenges: (1) multi-view inconsistency often leads to style
conflicts, resulting in appearance smoothing and distortion; and (2) heavy
reliance on VGG features, which struggle to disentangle style and content from
style images, often causing content leakage and excessive stylization. To
tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style
transfer framework, and the first to rely entirely on diffusion model
distillation. It comprises two key components: (1) \textbf{Multi-View Frequency
Consistency}. We enhance cross-view consistency by applying a 3D filter to
multi-view noisy latent, selectively reducing low-frequency components to
mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized
Distillation}. To suppress content leakage from style images, we introduce
negative guidance to exclude undesired content. In addition, we identify the
limitations of Score Distillation Sampling and Delta Denoising Score in 3D
style transfer and remove the reconstruction term accordingly. Building on
these insights, we propose a controllable stylized distillation that leverages
negative guidance to more effectively optimize the 3D Gaussians. Extensive
experiments demonstrate that our method consistently outperforms
state-of-the-art approaches, achieving higher stylization quality and visual
realism across various scenes and styles.

</details>


### [372] [Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization](https://arxiv.org/abs/2508.08141)
*Nicholas Klein,Hemlata Tak,James Fullwood,Krishna Regmi,Leonidas Spinoulas,Ganesh Sivaraman,Tianxiang Chen,Elie Khoury*

Main category: cs.CV

TL;DR: This paper introduces methods for detecting synthetic content in videos, specifically deepfakes, excelling in temporal localization and achieving high classification performance.


<details>
  <summary>Details</summary>
Motivation: The rapid development of visual and audio generation methods has increased the need for robust deepfake detection, especially for subtle modifications using localized manipulations.

Method: The paper proposes novel solutions for the tasks of deepfake video classification and localization, tested on the ACM 1M Deepfakes Detection Challenge dataset.

Result: The proposed methods achieved the top performance in the temporal localization task and ranked in the top four for the classification task in the TestA split of the ACM 1M dataset.

Conclusion: The presented techniques demonstrate effectiveness for detecting and localizing deepfake videos, addressing critical challenges in synthetic content identification.

Abstract: The field of visual and audio generation is burgeoning with new
state-of-the-art methods. This rapid proliferation of new techniques
underscores the need for robust solutions for detecting synthetic content in
videos. In particular, when fine-grained alterations via localized
manipulations are performed in visual, audio, or both domains, these subtle
modifications add challenges to the detection algorithms. This paper presents
solutions for the problems of deepfake video classification and localization.
The methods were submitted to the ACM 1M Deepfakes Detection Challenge,
achieving the best performance in the temporal localization task and a top four
ranking in the classification task for the TestA split of the evaluation
dataset.

</details>


### [373] [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2508.08165)
*Yan Wang,Da-Wei Zhou,Han-Jia Ye*

Main category: cs.CV

TL;DR: The paper addresses challenges in Class-Incremental Learning by combining task-specific and universal adapters (TUNA) for better performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome performance issues caused by incorrect module selection and overlooking shared knowledge in pre-trained model-based CIL methods.

Method: The method trains task-specific adapters, uses entropy-based selection for inference, and introduces a universal adapter via adapter fusion to balance specialized and general knowledge.

Result: Extensive experiments show the proposed approach achieves state-of-the-art performance across multiple benchmarks.

Conclusion: Integrating task-specific and universal adapters effectively mitigates key CIL challenges and improves performance.

Abstract: Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Existing pre-trained model-based CIL
methods often freeze the pre-trained network and adapt to incremental tasks
using additional lightweight modules such as adapters. However, incorrect
module selection during inference hurts performance, and task-specific modules
often overlook shared general knowledge, leading to errors on distinguishing
between similar classes across tasks. To address the aforementioned challenges,
we propose integrating Task-Specific and Universal Adapters (TUNA) in this
paper. Specifically, we train task-specific adapters to capture the most
crucial features relevant to their respective tasks and introduce an
entropy-based selection mechanism to choose the most suitable adapter.
Furthermore, we leverage an adapter fusion strategy to construct a universal
adapter, which encodes the most discriminative features shared across tasks. We
combine task-specific and universal adapter predictions to harness both
specialized and general knowledge during inference. Extensive experiments on
various benchmark datasets demonstrate the state-of-the-art performance of our
approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA

</details>


### [374] [ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction](https://arxiv.org/abs/2508.08170)
*Chaojun Ni,Guosheng Zhao,Xiaofeng Wang,Zheng Zhu,Wenkang Qin,Xinze Chen,Guanghong Jia,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: The paper introduces ReconDreamer-RL, a framework leveraging video diffusion priors and dynamic scenario generation methods to reduce the sim2real gap for reinforcement learning-based autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Address the sim2real gap in reinforcement learning for autonomous driving models, and improve the ability to handle novel, photorealistic, and corner-case scenarios.

Method: The framework includes ReconSimulator for scene reconstruction using video diffusion priors and physical modeling, Dynamic Adversary Agent for generating diverse traffic scenarios, and Cousin Trajectory Generator for enhancing training dataset diversity.

Result: Experiments demonstrate significant improvement in autonomous driving training, achieving a 5x reduction in collision ratio versus imitation learning methods.

Conclusion: ReconDreamer-RL offers a promising approach to bridging the sim2real gap and enhancing the robustness of autonomous driving models.

Abstract: Reinforcement learning for training end-to-end autonomous driving models in
closed-loop simulations is gaining growing attention. However, most simulation
environments differ significantly from real-world conditions, creating a
substantial simulation-to-reality (sim2real) gap. To bridge this gap, some
approaches utilize scene reconstruction techniques to create photorealistic
environments as a simulator. While this improves realistic sensor simulation,
these methods are inherently constrained by the distribution of the training
data, making it difficult to render high-quality sensor data for novel
trajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a
framework designed to integrate video diffusion priors into scene
reconstruction to aid reinforcement learning, thereby enhancing end-to-end
autonomous driving training. Specifically, in ReconDreamer-RL, we introduce
ReconSimulator, which combines the video diffusion prior for appearance
modeling and incorporates a kinematic model for physical modeling, thereby
reconstructing driving scenarios from real-world data. This narrows the
sim2real gap for closed-loop evaluation and reinforcement learning. To cover
more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),
which adjusts the trajectories of surrounding vehicles relative to the ego
vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).
Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue
of training data distribution, which is often biased toward simple
straight-line movements. Experiments show that ReconDreamer-RL improves
end-to-end autonomous driving training, outperforming imitation learning
methods with a 5x reduction in the Collision Ratio.

</details>


### [375] [CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data](https://arxiv.org/abs/2508.08173)
*Chongke Bi,Xin Gao,Jiangkang Deng,Guan*

Main category: cs.CV

TL;DR: CD-TVD, a hybrid of contrastive learning and diffusion-based super-resolution, enables efficient 3D super-resolution with minimal high-resolution training data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to minimize the dependency on large-scale high-resolution datasets to perform 3D super-resolution in scientific simulations.

Method: The proposed approach, CD-TVD, pre-trains on historical data using contrastive learning and a diffusion super-resolution model. It then fine-tunes the model using only one timestep of new high-resolution data while employing a local attention mechanism.

Result: CD-TVD demonstrates accurate and resource-efficient 3D super-resolution in experiments on fluid and atmospheric simulation datasets.

Conclusion: The framework significantly advances data augmentation in scientific simulations by reducing high-resolution data dependency and maintaining detailed accuracy.

Abstract: Large-scale scientific simulations require significant resources to generate
high-resolution time-varying data (TVD). While super-resolution is an efficient
post-processing strategy to reduce costs, existing methods rely on a large
amount of HR training data, limiting their applicability to diverse simulation
scenarios. To address this constraint, we proposed CD-TVD, a novel framework
that combines contrastive learning and an improved diffusion-based
super-resolution model to achieve accurate 3D super-resolution from limited
time-step high-resolution data. During pre-training on historical simulation
data, the contrastive encoder and diffusion superresolution modules learn
degradation patterns and detailed features of high-resolution and
low-resolution samples. In the training phase, the improved diffusion model
with a local attention mechanism is fine-tuned using only one newly generated
high-resolution timestep, leveraging the degradation knowledge learned by the
encoder. This design minimizes the reliance on large-scale high-resolution
datasets while maintaining the capability to recover fine-grained details.
Experimental results on fluid and atmospheric simulation datasets confirm that
CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a
significant advancement in data augmentation for large-scale scientific
simulations. The code is available at
https://github.com/Xin-Gao-private/CD-TVD.

</details>


### [376] [MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision](https://arxiv.org/abs/2508.08177)
*Zhonghao Yan,Muxi Diao,Yuxuan Yang,Jiayuan Xu,Kaizhou Zhang,Ruoyan Jing,Lele Yang,Yanxi Liu,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: The paper introduces a novel medical vision-language task named Unified Medical Reasoning Grounding (UMRG), presents a dataset (U-MRG-14K), and proposes a framework (MedReasoner) that leverages reinforcement learning for clinical reasoning and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing medical-grounding pipelines rely heavily on explicit supervision and struggle to address implicit queries commonly encountered in clinical settings, necessitating advancements in more nuanced and interpretable medical imaging solutions.

Method: The proposed framework, MedReasoner, separates clinical reasoning and segmentation into distinct modules. It employs reinforcement learning to optimize an MLLM reasoner while utilizing a frozen segmentation module to convert spatial prompts into pixel-level masks. Alignment between modules is achieved via format and accuracy rewards.

Result: MedReasoner achieved state-of-the-art performance on the U-MRG-14K dataset and demonstrated strong generalization capabilities for unseen clinical queries.

Conclusion: The combination of reinforcement learning for interpretable reasoning and a modular framework for separate tasks shows considerable promise in advancing medical imaging and suggests a path toward more effective and adaptable diagnostic tools.

Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and
treatment planning in medical imaging. While multimodal large language models
(MLLMs) combine visual perception with natural language, current
medical-grounding pipelines still rely on supervised fine-tuning with explicit
spatial hints, making them ill-equipped to handle the implicit queries common
in clinical practice. This work makes three core contributions. We first define
Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that
demands clinical reasoning and pixel-level grounding. Second, we release
U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside
implicit clinical queries and reasoning traces, spanning 10 modalities, 15
super-categories, and 108 specific categories. Finally, we introduce
MedReasoner, a modular framework that distinctly separates reasoning from
segmentation: an MLLM reasoner is optimized with reinforcement learning, while
a frozen segmentation expert converts spatial prompts into masks, with
alignment achieved through format and accuracy rewards. MedReasoner achieves
state-of-the-art performance on U-MRG-14K and demonstrates strong
generalization to unseen clinical queries, underscoring the significant promise
of reinforcement learning for interpretable medical grounding.

</details>


### [377] [3D Human Mesh Estimation from Single View RGBD](https://arxiv.org/abs/2508.08178)
*Ozhan Suat,Bedirhan Uguz,Batuhan Karagoz,Muhammed Can Keles,Emre Akbas*

Main category: cs.CV

TL;DR: The paper introduces M$^3$, a method for estimating 3D human meshes from RGBD data using a masked autoencoder, outperforming previous approaches in accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the underutilization of depth information from RGBD cameras for 3D human mesh estimation and overcome data scarcity due to the lack of extensive labeled datasets.

Method: The authors synthesize partial views using MoCap datasets and train a masked autoencoder to reconstruct full 3D meshes from partial, single-view RGBD inputs.

Result: M$^3$ achieves superior accuracy on multiple datasets, with 16.8 mm and 22.0 mm PVE on SURREAL and CAPE, and competitively performs on BEHAVE, emphasizing the advantages of depth data.

Conclusion: The method demonstrates that leveraging depth data using masked autoencoders can outperform traditional approaches in human mesh reconstruction from RGBD inputs, showcasing its practical potential.

Abstract: Despite significant progress in 3D human mesh estimation from RGB images;
RGBD cameras, offering additional depth data, remain underutilized. In this
paper, we present a method for accurate 3D human mesh estimation from a single
RGBD view, leveraging the affordability and widespread adoption of RGBD cameras
for real-world applications. A fully supervised approach for this problem,
requires a dataset with RGBD image and 3D mesh label pairs. However, collecting
such a dataset is costly and challenging, hence, existing datasets are small,
and limited in pose and shape diversity. To overcome this data scarcity, we
leverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D
meshes from the body models found in MoCap datasets, and create partial,
single-view versions of them by projection to a virtual camera. This simulates
the depth data provided by an RGBD camera from a single viewpoint. Then, we
train a masked autoencoder to complete the partial, single-view mesh. During
inference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',
matches the depth values coming from the sensor to vertices of a template human
mesh, which creates a partial, single-view mesh. We effectively recover parts
of the 3D human body mesh model that are not visible, resulting in a full body
mesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL
and CAPE datasets, respectively; outperforming existing methods that use
full-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE
dataset, outperforming a recently published RGB based method by 18.4 mm,
highlighting the usefulness of depth data. Code will be released.

</details>


### [378] [PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation](https://arxiv.org/abs/2508.08179)
*Sihan Zhao,Zixuan Wang,Tianyu Luan,Jia Jia,Wentao Zhu,Jiebo Luo,Junsong Yuan,Nan Xi*

Main category: cs.CV

TL;DR: The paper introduces PP-Motion, a metric to evaluate human motion fidelity by combining physical and perceptual fidelity assessments, addressing limitations in prior work.


<details>
  <summary>Details</summary>
Motivation: To address the gap between physical feasibility and human-perceived fidelity in motion evaluation and to overcome the subjective, binary nature of existing human perception-based methods.

Method: The authors propose a physical labeling method to generate fine-grained, continuous annotations for motion fidelity and combine it with a human-based perceptual fidelity loss in a data-driven metric called PP-Motion. Pearson's correlation loss is used to capture physical priors.

Result: Experiments show that PP-Motion aligns with physical laws and human perception of motion fidelity better than existing methods.

Conclusion: PP-Motion offers a robust approach for evaluating human motion fidelity by integrating both physical and perceptual perspectives, advancing motion evaluation techniques.

Abstract: Human motion generation has found widespread applications in AR/VR, film,
sports, and medical rehabilitation, offering a cost-effective alternative to
traditional motion capture systems. However, evaluating the fidelity of such
generated motions is a crucial, multifaceted task. Although previous approaches
have attempted at motion fidelity evaluation using human perception or physical
constraints, there remains an inherent gap between human-perceived fidelity and
physical feasibility. Moreover, the subjective and coarse binary labeling of
human perception further undermines the development of a robust data-driven
metric. We address these issues by introducing a physical labeling method. This
method evaluates motion fidelity by calculating the minimum modifications
needed for a motion to align with physical laws. With this approach, we are
able to produce fine-grained, continuous physical alignment annotations that
serve as objective ground truth. With these annotations, we propose PP-Motion,
a novel data-driven metric to evaluate both physical and perceptual fidelity of
human motion. To effectively capture underlying physical priors, we employ
Pearson's correlation loss for the training of our metric. Additionally, by
incorporating a human-based perceptual fidelity loss, our metric can capture
fidelity that simultaneously considers both human perception and physical
alignment. Experimental results demonstrate that our metric, PP-Motion, not
only aligns with physical laws but also aligns better with human perception of
motion fidelity than previous work.

</details>


### [379] [THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening](https://arxiv.org/abs/2508.08183)
*Hongkun Jin,Hongcheng Jiang,Zejun Zhang,Yuan Zhang,Jia Fu,Tingfeng Li,Kai Luo*

Main category: cs.CV

TL;DR: This paper introduces a novel transformer-based framework, Token-wise High-frequency Augmentation Transformer (THAT), to enhance hyperspectral pansharpening with better reconstruction quality and efficiency through improved token selection and high-frequency feature representation.


<details>
  <summary>Details</summary>
Motivation: Traditional transformer-based methods for hyperspectral pansharpening face challenges such as redundant token representations and insufficient multi-scale feature modeling. They often fail to preserve high-frequency components and suffer from diluted attention due to reliance on global self-attention.

Method: The paper proposes the THAT framework, which incorporates Pivotal Token Selective Attention (PTSA) to prioritize critical tokens while reducing redundancy and a Multi-level Variance-aware Feed-forward Network (MVFN) to effectively model high-frequency details, addressing the limitations of Vision Transformers in this domain.

Result: Experiments on standard benchmarks show that THAT achieves state-of-the-art performance in hyperspectral pansharpening, delivering both better reconstruction quality and improved efficiency.

Conclusion: The study confirms the effectiveness of the proposed THAT framework in overcoming key limitations of traditional Vision Transformers, establishing it as a superior approach for hyperspectral pansharpening tasks.

Abstract: Transformer-based methods have demonstrated strong potential in hyperspectral
pansharpening by modeling long-range dependencies. However, their effectiveness
is often limited by redundant token representations and a lack of multi-scale
feature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,
abundance sparsity) and spatial priors (e.g., non-local similarity), which are
critical for accurate reconstruction. From a spectral-spatial perspective,
Vision Transformers (ViTs) face two major limitations: they struggle to
preserve high-frequency components--such as material edges and texture
transitions--and suffer from attention dispersion across redundant tokens.
These issues stem from the global self-attention mechanism, which tends to
dilute high-frequency signals and overlook localized details. To address these
challenges, we propose the Token-wise High-frequency Augmentation Transformer
(THAT), a novel framework designed to enhance hyperspectral pansharpening
through improved high-frequency feature representation and token selection.
Specifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to
prioritize informative tokens and suppress redundancy; (2) a Multi-level
Variance-aware Feed-forward Network (MVFN) to enhance high-frequency detail
learning. Experiments on standard benchmarks show that THAT achieves
state-of-the-art performance with improved reconstruction quality and
efficiency. The source code is available at https://github.com/kailuo93/THAT.

</details>


### [380] [KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning](https://arxiv.org/abs/2508.08186)
*Md Meftahul Ferdaus,Mahdi Abdelguerfi,Elias Ioup,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: The KARMA framework introduces an efficient deep learning method for defect segmentation in civil infrastructure, reducing parameters by 97% and enabling real-time deployment.


<details>
  <summary>Details</summary>
Motivation: To address challenges in defect segmentation, such as variable conditions and class imbalance, while creating a lightweight approach suitable for real-time inspection.

Method: KARMA employs a Tiny Kolmogorov-Arnold Network for efficient feature transformation, a separable convolution-based feature pyramid for multi-scale analysis, and a prototype mechanism for balancing class representation.

Result: KARMA achieves competitive mean IoU performance on benchmark datasets while requiring only 0.959 million parameters, significantly outperforming competing methods in efficiency.

Conclusion: KARMA is a scalable, low-resource semantic segmentation framework suitable for real-time infrastructure inspections without compromising accuracy.

Abstract: Semantic segmentation of structural defects in civil infrastructure remains
challenging due to variable defect appearances, harsh imaging conditions, and
significant class imbalance. Current deep learning methods, despite their
effectiveness, typically require millions of parameters, rendering them
impractical for real-time inspection systems. We introduce KARMA
(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient
semantic segmentation framework that models complex defect patterns through
compositions of one-dimensional functions rather than conventional
convolutions. KARMA features three technical innovations: (1) a
parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging
low-rank factorization for KAN-based feature transformation; (2) an optimized
feature pyramid structure with separable convolutions for multi-scale defect
analysis; and (3) a static-dynamic prototype mechanism that enhances feature
representation for imbalanced classes. Extensive experiments on benchmark
infrastructure inspection datasets demonstrate that KARMA achieves competitive
or superior mean IoU performance compared to state-of-the-art approaches, while
using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).
Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for
real-time deployment, enabling practical automated infrastructure inspection
systems without compromising accuracy. The source code can be accessed at the
following URL: https://github.com/faeyelab/karma.

</details>


### [381] [Reinforcement Learning in Vision: A Survey](https://arxiv.org/abs/2508.08189)
*Weijia Wu,Chen Gao,Joya Chen,Kevin Qinghong Lin,Qingwei Meng,Yiming Zhang,Yuke Qiu,Hong Zhou,Mike Zheng Shou*

Main category: cs.CV

TL;DR: The paper surveys advancements in reinforcement learning focused on visual intelligence. It categorizes 200+ works into thematic pillars and discusses trends, challenges, and evaluation protocols.


<details>
  <summary>Details</summary>
Motivation: The growing integration of reinforcement learning with visual intelligence has created complex and dynamic applications, but lacks a coherent synthesis of strategies and progress in the field.

Method: The paper reviews over 200 works in visual RL, categorizing them under thematic pillars and analyzing associated techniques, trends, and challenges.

Result: The survey's categorization highlights key advancements like RLHF, novel policy optimization techniques, and evaluation protocols, offering a panoramic view of the field.

Conclusion: The paper provides a roadmap for visual RL research, summarizing contributions and identifying opportunities for addressing challenges like sample efficiency, generalization, and safe deployment.

Abstract: Recent advances at the intersection of reinforcement learning (RL) and visual
intelligence have enabled agents that not only perceive complex visual scenes
but also reason, generate, and act within them. This survey offers a critical
and up-to-date synthesis of the field. We first formalize visual RL problems
and trace the evolution of policy-optimization strategies from RLHF to
verifiable reward paradigms, and from Proximal Policy Optimization to Group
Relative Policy Optimization. We then organize more than 200 representative
works into four thematic pillars: multi-modal large language models, visual
generation, unified model frameworks, and vision-language-action models. For
each pillar we examine algorithmic design, reward engineering, benchmark
progress, and we distill trends such as curriculum-driven training,
preference-aligned diffusion, and unified reward modeling. Finally, we review
evaluation protocols spanning set-level fidelity, sample-level preference, and
state-level stability, and we identify open challenges that include sample
efficiency, generalization, and safe deployment. Our goal is to provide
researchers and practitioners with a coherent map of the rapidly expanding
landscape of visual RL and to highlight promising directions for future
inquiry. Resources are available at:
https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.

</details>


### [382] [Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model](https://arxiv.org/abs/2508.08199)
*Peiqi He,Zhenhao Zhang,Yixiang Zhang,Xiongjun Zhao,Shaoliang Peng*

Main category: cs.CV

TL;DR: Spatial-ORMLLM is the first large vision-language model to enable 3D spatial reasoning in operating rooms using only RGB data.


<details>
  <summary>Details</summary>
Motivation: The goal is to overcome the lack of multimodal 3D data and the limitations of using only 2D data for complex spatial reasoning in operating-room settings.

Method: Spatial-ORMLLM employs a Spatial-Enhanced Feature Fusion Block that integrates 2D modality inputs with 3D spatial knowledge through an estimation algorithm, leveraging a unified end-to-end framework without requiring extra annotations or sensors.

Result: Spatial-ORMLLM achieves state-of-the-art performance on benchmark clinical datasets and generalizes well to unseen surgical scenarios.

Conclusion: The model delivers detailed and holistic spatial context in operating rooms, enabling robust 3D scene reasoning for downstream medical tasks.

Abstract: Precise spatial modeling in the operating room (OR) is foundational to many
clinical tasks, supporting intraoperative awareness, hazard avoidance, and
surgical decision-making. While existing approaches leverage large-scale
multimodal datasets for latent-space alignment to implicitly learn spatial
relationships, they overlook the 3D capabilities of MLLMs. However, this
approach raises two issues: (1) Operating rooms typically lack multiple video
and audio sensors, making multimodal 3D data difficult to obtain; (2) Training
solely on readily available 2D data fails to capture fine-grained details in
complex scenes. To address this gap, we introduce Spatial-ORMLLM, the first
large vision-language model for 3D spatial reasoning in operating rooms using
only RGB modality to infer volumetric and semantic cues, enabling downstream
medical tasks with detailed and holistic spatial context. Spatial-ORMLLM
incorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D
modality inputs with rich 3D spatial knowledge extracted by the estimation
algorithm and then feeds the combined features into the visual tower. By
employing a unified end-to-end MLLM framework, it combines powerful spatial
features with textual features to deliver robust 3D scene reasoning without any
additional expert annotations or sensor inputs. Experiments on multiple
benchmark clinical datasets demonstrate that Spatial-ORMLLM achieves
state-of-the-art performance and generalizes robustly to previously unseen
surgical scenarios and downstream tasks.

</details>


### [383] [SAGOnline: Segment Any Gaussians Online](https://arxiv.org/abs/2508.08219)
*Wentao Sun,Quanyun Wu,Hanqing Xu,Kyle Gao,Zhengsen Xu,Yiping Chen,Dedong Zhang,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: The paper introduces SAGOnline, a lightweight framework for real-time 3D segmentation in Gaussian scenes, solving current limitations with computational efficiency and multi-object tracking.


<details>
  <summary>Details</summary>
Motivation: The main motivation stems from the limitations of existing 3D segmentation methods in terms of computational demands, inefficient spatial reasoning, and the inability to track multiple objects, especially in AR/VR and robotics contexts.

Method: The approach leverages two key innovations: (1) a decoupled strategy using video foundation models for view-consistent 2D mask propagation, and (2) a GPU-accelerated 3D mask generation with Gaussian-level instance labeling for efficient multi-object tracking.

Result: SAGOnline achieves superior performance on benchmarks like NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU), beating prior methods in inference speed by up to 1500x while producing robust segmentation and tracking results.

Conclusion: The study delivers a scalable, zero-shot 3D segmentation tool that enhances real-time scene understanding and offers transformative implications for AR/VR and robotics applications.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit
3D scene representation, yet achieving efficient and consistent 3D segmentation
remains challenging. Current methods suffer from prohibitive computational
costs, limited 3D spatial reasoning, and an inability to track multiple objects
simultaneously. We present Segment Any Gaussians Online (SAGOnline), a
lightweight and zero-shot framework for real-time 3D segmentation in Gaussian
scenes that addresses these limitations through two key innovations: (1) a
decoupled strategy that integrates video foundation models (e.g., SAM2) for
view-consistent 2D mask propagation across synthesized views; and (2) a
GPU-accelerated 3D mask generation and Gaussian-level instance labeling
algorithm that assigns unique identifiers to 3D primitives, enabling lossless
multi-object tracking and segmentation across views. SAGOnline achieves
state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)
benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times
in inference speed (27 ms/frame). Qualitative results demonstrate robust
multi-object segmentation and tracking in complex scenes. Our contributions
include: (i) a lightweight and zero-shot framework for 3D segmentation in
Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling
simultaneous segmentation and tracking, and (iii) the effective adaptation of
2D video foundation models to the 3D domain. This work allows real-time
rendering and 3D scene understanding, paving the way for practical AR/VR and
robotic applications.

</details>


### [384] [Learning User Preferences for Image Generation Model](https://arxiv.org/abs/2508.08220)
*Wenyi Mo,Ying Ba,Tianyu Zhang,Yalong Bai,Biye Li*

Main category: cs.CV

TL;DR: This paper proposes an improved approach for user preference prediction using Multimodal Large Language Models with techniques like contrastive preference loss and preference tokens.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with dynamic and multifaceted personal tastes, relying on static profiles or general human preferences.

Method: They employ contrastive preference loss to discern likes and dislikes and introduce learnable preference tokens for capturing shared interests among users.

Result: The model showed superior performance in preference prediction accuracy and identifying users with similar aesthetic preferences.

Conclusion: The approach enhances precision in understanding user preferences and generating personalized content.

Abstract: User preference prediction requires a comprehensive and accurate
understanding of individual tastes. This includes both surface-level
attributes, such as color and style, and deeper content-related aspects, such
as themes and composition. However, existing methods typically rely on general
human preferences or assume static user profiles, often neglecting individual
variability and the dynamic, multifaceted nature of personal taste. To address
these limitations, we propose an approach built upon Multimodal Large Language
Models, introducing contrastive preference loss and preference tokens to learn
personalized user preferences from historical interactions. The contrastive
preference loss is designed to effectively distinguish between user ''likes''
and ''dislikes'', while the learnable preference tokens capture shared interest
representations among existing users, enabling the model to activate
group-specific preferences and enhance consistency across similar users.
Extensive experiments demonstrate our model outperforms other methods in
preference prediction accuracy, effectively identifying users with similar
aesthetic inclinations and providing more precise guidance for generating
images that align with individual tastes. The project page is
\texttt{https://learn-user-pref.github.io/}.

</details>


### [385] [OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.08227)
*Zhiqiang Wu,Zhaomang Sun,Tong Zhou,Bingtao Fu,Ji Cong,Yitong Dong,Huaqi Zhang,Xuan Tang,Mingsong Chen,Xian Wei*

Main category: cs.CV

TL;DR: This paper introduces OMGSR, a framework for Real-World Image Super-Resolution (Real-ISR) using DDPM/FM-based generative models, effectively addressing the gap between low-quality (LQ) image distributions and generative priors.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between the low-quality (LQ) image latent distribution and the Gaussian noisy latent distribution that limits effective use of generative priors in Real-ISR models.

Method: The authors propose OMGSR, which injects LQ image latent distributions at mid-timesteps of DDPM/FM during image generation and introduces a Latent Distribution Refinement loss and Overlap-Chunked LPIPS/GAN loss to address latent distribution gaps and eliminate checkerboard artifacts.

Result: OMGSR demonstrates strong quantitative and qualitative performance for Real-ISR at 512 and 1k resolutions, with OMGSR-F achieving dominance in all evaluated metrics and producing high-quality image details.

Conclusion: OMGSR offers a universal and effective framework for Real-ISR, outperforming existing methods, and scaling robustly to generate high-resolution images using DDPM/FM-based generative models.

Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)
generative models show promising potential for one-step Real-World Image
Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a
Low-Quality (LQ) image latent distribution at the initial timestep. However, a
fundamental gap exists between the LQ image latent distribution and the
Gaussian noisy latent distribution, limiting the effective utilization of
generative priors. We observe that the noisy latent distribution at DDPM/FM
mid-timesteps aligns more closely with the LQ image latent distribution. Based
on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a
universal framework applicable to DDPM/FM-based generative models. OMGSR
injects the LQ image latent distribution at a pre-computed mid-timestep,
incorporating the proposed Latent Distribution Refinement loss to alleviate the
latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to
eliminate checkerboard artifacts in image generation. Within this framework, we
instantiate OMGSR for DDPM/FM-based generative models with two variants:
OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate
that OMGSR-S/F achieves balanced/excellent performance across quantitative and
qualitative metrics at 512-resolution. Notably, OMGSR-F establishes
overwhelming dominance in all reference metrics. We further train a
1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which
yields excellent results, especially in the details of the image generation. We
also generate 2k-resolution images by the 1k-resolution OMGSR-F using our
two-stage Tiled VAE & Diffusion.

</details>


### [386] [Cut2Next: Generating Next Shot via In-Context Tuning](https://arxiv.org/abs/2508.08244)
*Jingwen He,Hongbo Liu,Jiajun Li,Ziqi Huang,Yu Qiao,Wanli Ouyang,Ziwei Liu*

Main category: cs.CV

TL;DR: This paper introduces a novel framework, Cut2Next, for generating high-quality next shots in cinematic sequences, focusing on narrative continuity and professional editing patterns.


<details>
  <summary>Details</summary>
Motivation: Current multi-shot generation methods lack sophisticated narrative editing patterns, resulting in outputs that, while visually coherent, fail to achieve cinematic storytelling integrity.

Method: The authors propose Next Shot Generation (NSG) via Cut2Next, leveraging a Diffusion Transformer (DiT) and a Hierarchical Multi-Prompting strategy to guide cinematic content creation.

Result: Through experiments using RawCuts and CuratedCuts datasets and evaluation on CutBench, Cut2Next demonstrated strong performance in visual consistency, text fidelity, and user preference due to adherence to editing patterns.

Conclusion: Cut2Next is validated as a breakthrough framework for generating narratively expressive and cinematically coherent shots, advancing the state of multi-shot generation technologies.

Abstract: Effective multi-shot generation demands purposeful, film-like transitions and
strict cinematic continuity. Current methods, however, often prioritize basic
visual consistency, neglecting crucial editing patterns (e.g., shot/reverse
shot, cutaways) that drive narrative flow for compelling storytelling. This
yields outputs that may be visually coherent but lack narrative sophistication
and true cinematic integrity. To bridge this, we introduce Next Shot Generation
(NSG): synthesizing a subsequent, high-quality shot that critically conforms to
professional editing patterns while upholding rigorous cinematic continuity.
Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs
in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This
strategy uses Relational Prompts to define overall context and inter-shot
editing styles. Individual Prompts then specify per-shot content and
cinematographic attributes. Together, these guide Cut2Next to generate
cinematically appropriate next shots. Architectural innovations, Context-Aware
Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further
integrate these diverse signals without introducing new parameters. We
construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with
hierarchical prompts, and introduce CutBench for evaluation. Experiments show
Cut2Next excels in visual consistency and text fidelity. Crucially, user
studies reveal a strong preference for Cut2Next, particularly for its adherence
to intended editing patterns and overall cinematic continuity, validating its
ability to generate high-quality, narratively expressive, and cinematically
coherent subsequent shots.

</details>


### [387] [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2508.08248)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAvatar introduces a diffusion transformer capable of generating high-quality, infinite-length avatar videos synchronized to audio, overcoming limitations in existing models through innovative training and inference strategies.


<details>
  <summary>Details</summary>
Motivation: Existing audio-driven avatar video generation models struggle with long video synthesis that maintains audio synchronization and visual identity consistency.

Method: StableAvatar employs a Time-step-aware Audio Adapter for preventing error accumulation, an Audio Native Guidance Mechanism for dynamic audio synchronization, and a Dynamic Weighted Sliding-window Strategy for smoother infinite-length videos.

Result: Experiments on benchmarks demonstrate that StableAvatar produces superior results both qualitatively and quantitatively compared to prior models.

Conclusion: StableAvatar represents a significant advancement in audio-driven avatar video generation, offering a robust solution for synthesizing high-quality, infinite-length videos without external post-processing.

Abstract: Current diffusion models for audio-driven avatar video generation struggle to
synthesize long videos with natural audio synchronization and identity
consistency. This paper presents StableAvatar, the first end-to-end video
diffusion transformer that synthesizes infinite-length high-quality videos
without post-processing. Conditioned on a reference image and audio,
StableAvatar integrates tailored training and inference modules to enable
infinite-length video generation. We observe that the main reason preventing
existing models from generating long videos lies in their audio modeling. They
typically rely on third-party off-the-shelf extractors to obtain audio
embeddings, which are then directly injected into the diffusion model via
cross-attention. Since current diffusion backbones lack any audio-related
priors, this approach causes severe latent distribution error accumulation
across video clips, leading the latent distribution of subsequent segments to
drift away from the optimal distribution gradually. To address this,
StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents
error accumulation via time-step-aware modulation. During inference, we propose
a novel Audio Native Guidance Mechanism to further enhance the audio
synchronization by leveraging the diffusion's own evolving joint audio-latent
prediction as a dynamic guidance signal. To enhance the smoothness of the
infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy
that fuses latent over time. Experiments on benchmarks show the effectiveness
of StableAvatar both qualitatively and quantitatively.

</details>


### [388] [ReferSplat: Referring Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2508.08252)
*Shuting He,Guangquan Jie,Changshuo Wang,Yun Zhou,Shuming Hu,Guanbin Li,Henghui Ding*

Main category: cs.CV

TL;DR: The paper introduces R3DGS, a task and framework (ReferSplat) for segmenting 3D objects using natural language in Gaussian scenes, overcoming spatial modeling and occlusion challenges.


<details>
  <summary>Details</summary>
Motivation: To advance embodied AI by enabling models to understand 3D multi-modal data and spatial relationships via natural language descriptions.

Method: Developed a dataset (Ref-LERF) and proposed ReferSplat, a spatially aware framework connecting 3D Gaussian points to natural language expressions.

Result: ReferSplat achieves state-of-the-art performance on R3DGS task and 3D open-vocabulary segmentation benchmarks.

Conclusion: The paper highlights the importance of 3D multi-modal understanding and spatial relationships, offering a novel framework and dataset for further research in this domain.

Abstract: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task
that aims to segment target objects in a 3D Gaussian scene based on natural
language descriptions, which often contain spatial relationships or object
attributes. This task requires the model to identify newly described objects
that may be occluded or not directly visible in a novel view, posing a
significant challenge for 3D multi-modal understanding. Developing this
capability is crucial for advancing embodied AI. To support research in this
area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that
3D multi-modal understanding and spatial relationship modeling are key
challenges for R3DGS. To address these challenges, we propose ReferSplat, a
framework that explicitly models 3D Gaussian points with natural language
expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art
performance on both the newly proposed R3DGS task and 3D open-vocabulary
segmentation benchmarks. Dataset and code are available at
https://github.com/heshuting555/ReferSplat.

</details>


### [389] [Learning an Implicit Physics Model for Image-based Fluid Simulation](https://arxiv.org/abs/2508.08254)
*Emily Yue-Ting Jia,Jiageng Mao,Zhiyuan Gao,Yajie Zhao,Yue Wang*

Main category: cs.CV

TL;DR: The paper introduces a physics-informed neural network to generate realistic 4D scenes, focused on fluid imagery, from single still images, overcoming limitations of existing simplistic approaches.


<details>
  <summary>Details</summary>
Motivation: Humans can imagine realistic, motion-filled 4D scenes from static images using intuition and physics understanding; replicating this ability in neural networks for fluid imagery is the goal.

Method: The authors propose a physics-informed neural network guided by loss terms based on physical principles, e.g., Navier-Stokes equations, to predict motion on surfaces, alongside a feature-based 3D Gaussian prediction for rendering realistic animations.

Result: The experimental evaluations demonstrate that the proposed method produces physics-consistent animations, outperforming existing methods in terms of realism and fidelity.

Conclusion: The new approach significantly enhances animation realism in fluid imagery, offering physics-consistent scenes that align better with physical principles compared to prevailing methods.

Abstract: Humans possess an exceptional ability to imagine 4D scenes, encompassing both
motion and 3D geometry, from a single still image. This ability is rooted in
our accumulated observations of similar scenes and an intuitive understanding
of physics. In this paper, we aim to replicate this capacity in neural
networks, specifically focusing on natural fluid imagery. Existing methods for
this task typically employ simplistic 2D motion estimators to animate the
image, leading to motion predictions that often defy physical principles,
resulting in unrealistic animations. Our approach introduces a novel method for
generating 4D scenes with physics-consistent animation from a single image. We
propose the use of a physics-informed neural network that predicts motion for
each surface point, guided by a loss term derived from fundamental physical
principles, including the Navier-Stokes equations. To capture appearance, we
predict feature-based 3D Gaussians from the input image and its estimated
depth, which are then animated using the predicted motions and rendered from
any desired camera perspective. Experimental results highlight the
effectiveness of our method in producing physically plausible animations,
showcasing significant performance improvements over existing methods. Our
project page is https://physfluid.github.io/ .

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [390] [Taming Cold Starts: Proactive Serverless Scheduling with Model Predictive Control](https://arxiv.org/abs/2508.07640)
*Chanh Nguyen,Monowar Bhuyan,Erik Elmroth*

Main category: cs.DC

TL;DR: This paper addresses the cold start problem in serverless computing by presenting a predictive scheduling framework that improves latency and resource efficiency using Model Predictive Control.


<details>
  <summary>Details</summary>
Motivation: Serverless computing is attractive for dynamic workloads but suffers from the cold start problem, which causes significant delays when provisioning new containers.

Method: The paper uses Model Predictive Control to forecast future function invocations and optimizes container prewarming and request dispatching to mitigate cold starts.

Result: The proposed framework, tested on Apache OpenWhisk with real-world and synthetic workloads, achieved up to 85% lower tail latency and a 34% reduction in resource usage compared to baseline methods.

Conclusion: The predictive scheduling approach effectively addresses the cold start issue, improving both latency and resource utilization in serverless computing environments.

Abstract: Serverless computing has transformed cloud application deployment by
introducing a fine-grained, event-driven execution model that abstracts away
infrastructure management. Its on-demand nature makes it especially appealing
for latency-sensitive and bursty workloads. However, the cold start problem,
i.e., where the platform incurs significant delay when provisioning new
containers, remains the Achilles' heel of such platforms.
  This paper presents a predictive serverless scheduling framework based on
Model Predictive Control to proactively mitigate cold starts, thereby improving
end-to-end response time. By forecasting future invocations, the controller
jointly optimizes container prewarming and request dispatching, improving
latency while minimizing resource overhead.
  We implement our approach on Apache OpenWhisk, deployed on a Kubernetes-based
testbed. Experimental results using real-world function traces and synthetic
workloads demonstrate that our method significantly outperforms
state-of-the-art baselines, achieving up to 85% lower tail latency and a 34%
reduction in resource usage.

</details>


### [391] [PiKV: KV Cache Management System for Mixture of Experts](https://arxiv.org/abs/2508.06526)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu,Xuhong Wang*

Main category: cs.DC

TL;DR: The paper introduces PiKV, a framework for reducing memory and communication overhead in KV cache storage for MoE-based language models by partitioning, routing, scheduling, and compression techniques.


<details>
  <summary>Details</summary>
Motivation: Large-scale language models using MoE architectures face inefficiencies due to globally synchronized dense KV caches, which lead to significant memory and communication bottlenecks.

Method: PiKV introduces expert-sharded KV storage, adaptive entry scheduling, token-to-KV access optimization, and compression modules for efficient cache management.

Result: PiKV shows notable improvements in memory usage and acceleration when combined with Nvidia kvpress, with experimental results shared publicly.

Conclusion: PiKV is an open-source framework aimed at optimizing KV cache storage for MoE architectures, reducing overhead and enhancing scalability.

Abstract: As large language models continue to scale up in both size and context
length, the memory and communication cost of key-value (KV) cache storage has
become a major bottleneck in multi-GPU and multi-node inference. While
MoE-based architectures sparsify computation across experts, the corresponding
KV caches remain dense and globally synchronized, resulting in significant
overhead.
  We introduce \textbf{PiKV}, a parallel and distributed KV cache serving
framework tailored for MoE architecture. PiKV leverages \textit{expert-sharded
KV storage} to partition caches across GPUs, \textit{PiKV routing} to reduce
token-to-KV access, and a \textit{PiKV Scheduling} to adaptively retain
query-relevant entries. To further reduce memory usage, PiKV integrates
\textit{PiKV Compression} modules the caching pipeline for acceleration.
  PiKV is recently publicly available as an open-source software library:
\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.
Experiments details is recorded at:
\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\_Results}.
We also have PiKV integrated with Nvidia kvpress for acceleration, details see
\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.
PiKV is still a living project, aiming to become a comprehesive KV Cache
management system for MoE Architectures.

</details>


### [392] [Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive Loads in the Public Cloud](https://arxiv.org/abs/2508.06948)
*Jinyuan Chen,Jiuchen Shi,Quan Chen,Minyi Guo*

Main category: cs.DC

TL;DR: Kairos is a multi-agent orchestration system designed to reduce end-to-end latency for multi-agent applications utilizing shared LLMs by improving request scheduling and resource management.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent applications with shared LLMs underperform due to excessive load and inadequate request scheduling, ignoring latency and resource differences.

Method: Kairos introduces an orchestrator that analyzes workflows, a priority scheduler for latency optimization, and a dispatcher for GPU resource management, streamlining the task execution process.

Result: Experiments demonstrate that Kairos achieves a 17.8% to 28.4% reduction in end-to-end latency compared to state-of-the-art methods.

Conclusion: Kairos significantly enhances the serving performance for multi-agent applications with shared LLMs by addressing latency and resource allocation issues.

Abstract: Multi-agent applications utilize the advanced capabilities of large language
models (LLMs) for intricate task completion through agent collaboration in a
workflow. Under this situation, requests from different agents usually access
the same shared LLM to perform different kinds of tasks, forcing the shared LLM
to suffer excessive loads. However, existing works have low serving performance
for these multi-agent applications, mainly due to the ignorance of inter-agent
latency and resource differences for request scheduling. We therefore propose
Kairos, a multi-agent orchestration system that optimizes end-to-end latency
for multi-agent applications. Kairos consists of a workflow orchestrator, a
workflow-aware priority scheduler, and a memory-aware dispatcher. The
orchestrator collects agent-specific information for online workflow analysis.
The scheduler decides the serving priority of the requests based on their
latency characteristics to reduce the overall queuing. The dispatcher
dispatches the requests to different LLM instances based on their memory
demands to avoid GPU overloading. Experimental results show that Kairos reduces
end-to-end latency by 17.8% to 28.4% compared to state-of-the-art works.

</details>


### [393] [Convergence Sans Synchronization](https://arxiv.org/abs/2508.06949)
*Arya Tanmay Gupta*

Main category: cs.DC

TL;DR: The paper proposes a theory for asynchronous multiprocessor algorithms, reducing the need for costly synchronization mechanisms and simplifying convergence proofs, validated with experimental improvements in execution time.


<details>
  <summary>Details</summary>
Motivation: To address the computational and time inefficiencies of synchronization in parallel algorithms, and to enable algorithms to utilize hardware more efficiently by executing asynchronously.

Method: Developing a theory and criteria that prove convergence of algorithms in asynchrony using local state transition properties instead of global state graph analysis. Creating and analyzing new and existing algorithms under these criteria.

Result: The proposed asynchronous algorithms demonstrate faster convergence times in experiments compared to traditional synchronized algorithms, with similar success in both scheduled and asynchronous execution environments.

Conclusion: The theory drastically reduces the complexity of proving algorithmic convergence in asynchronous execution, enabling the development of efficient algorithms that outperform synchronized counterparts.

Abstract: We currently see a steady rise in the usage and size of multiprocessor
systems, and so the community is evermore interested in developing fast
parallel processing algorithms. However, most algorithms require a
synchronization mechanism, which is costly in terms of computational resources
and time. If an algorithm can be executed in asynchrony, then it can use all
the available computation power, and the nodes can execute without being
scheduled or locked. However, to show that an algorithm guarantees convergence
in asynchrony, we need to generate the entire global state transition graph and
check for the absence of cycles. This takes time exponential in the size of the
global state space. In this dissertation, we present a theory that explains the
necessary and sufficient properties of a multiprocessor algorithm that
guarantees convergence even without synchronization. We develop algorithms for
various problems that do not require synchronization. Additionally, we show for
several existing algorithms that they can be executed without any
synchronization mechanism. A significant theoretical benefit of our work is in
proving that an algorithm can converge even in asynchrony. Our theory implies
that we can make such conclusions about an algorithm, by only showing that the
local state transition graph of a computing node forms a partial order, rather
than generating the entire global state space and determining the absence of
cycles in it. Thus, the complexity of rendering such proofs, formal or social,
is phenomenally reduced. Experiments show a significant reduction in time taken
to converge, when we compare the execution time of algorithms in the literature
versus the algorithms that we design. We get similar results when we run an
algorithm, that guarantees convergence in asynchrony, under a scheduler versus
in asynchrony.

</details>


### [394] [The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries](https://arxiv.org/abs/2508.07071)
*Oscar Amoros,Albert Andaluz,Johnny Nunez,Antonio J. Pena*

Main category: cs.DC

TL;DR: The paper presents a methodology for automatic and on-demand kernel fusion (HF and VF) for GPU libraries, significantly improving performance without requiring manual development of fused kernels.


<details>
  <summary>Details</summary>
Motivation: Current GPU libraries often fail to maximize the parallel resources and on-chip memory due to manual kernel fusion processes, which lead to limited optimization and increased development costs.

Method: The methodology involves defining reusable, fusionable components and leveraging C++17 metaprogramming to generate tailored and optimized fused kernels at compile time, eliminating the need for pre-compilation or custom kernels.

Result: The implementation demonstrates speedups ranging from 2x to over 1000x in benchmarks compared to traditional libraries, showing significant improvements in GPU performance.

Conclusion: The proposed approach effectively abstract low-level GPU operations, maximizes resource usage, maintains intermediate data in SRAM, and enhances programmability while achieving substantial performance gains.

Abstract: Existing GPU libraries often struggle to fully exploit the parallel resources
and on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as
individual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion
(HF) and Vertical Fusion (VF) can mitigate this, current library
implementations often require library developers to manually create fused
kernels. Hence, library users rely on limited sets of pre-compiled or
template-based fused kernels. This limits the use cases that can benefit from
HF and VF and increases development costs. In order to solve these issues, we
present a novel methodology for building GPU libraries that enables automatic
on-demand HF and VF for arbitrary combinations of GPU library functions. Our
methodology defines reusable, fusionable components that users combine via
high-level programming interfaces. Leveraging C++17 metaprogramming features
available in compilers like nvcc, our methodology generates a single and
optimized fused kernel tailored to the user's specific sequence of operations
at compile time, without needing a custom compiler or manual development and
pre-compilation of kernel combinations. This approach abstracts low-level GPU
complexities while maximizing GPU resource utilization and keeping intermediate
data in SRAM. We provide an open-source implementation demonstrating
significant speedups compared to traditional libraries in various benchmarks,
validating the effectiveness of this methodology for improving GPU performance
in the range of 2x to more than 1000x, while preserving high-level
programmability.

</details>


### [395] [AerialDB: A Federated Peer-to-Peer Spatio-temporal Edge Datastore for Drone Fleets](https://arxiv.org/abs/2508.07124)
*Shashwat Jaiswal,Suman Raj,Subhajit Sidhanta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: The paper introduces AerialDB, a lightweight and decentralized system for storing and querying large spatio-temporal datasets collected by a fleet of UAVs in disaster regions.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of efficiently offloading and processing large datasets, like videos and images, collected by UAVs in highly dynamic disaster environments.

Method: Developed AerialDB, a decentralized, locality-aware system optimized for spatial and temporal query processing using containerized deployment, replica placement, and indexing techniques.

Result: AerialDB demonstrated scalable real-time performance in setups involving up to 400 drones and 80 edge servers, significantly outperforming state-of-the-art baselines in both insertion and query workloads.

Conclusion: AerialDB provides efficient, scalable, and robust performance for disaster management applications, surpassing existing solutions in handling spatio-temporal data.

Abstract: Recent years have seen an unprecedented growth in research that leverages the
newest computing paradigm of Internet of Drones, comprising a fleet of
connected Unmanned Aerial Vehicles (UAVs) used for a wide range of tasks such
as monitoring and analytics in highly mobile and changing environments
characteristic of disaster regions. Given that the typical data (i.e., videos
and images) collected by the fleet of UAVs deployed in such scenarios can be
considerably larger than what the onboard computers can process, the UAVs need
to offload their data in real-time to the edge and the cloud for further
processing. To that end, we present the design of AerialDB - a lightweight
decentralized data storage and query system that can store and process time
series data on a multi-UAV system comprising: A) a fleet of hundreds of UAVs
fitted with onboard computers, and B) ground-based edge servers connected
through a cellular link. Leveraging lightweight techniques for content-based
replica placement and indexing of shards, AerialDB has been optimized for
efficient processing of different possible combinations of typical spatial and
temporal queries performed by real-world disaster management applications.
Using containerized deployment spanning up to 400 drones and 80 edges, we
demonstrate that AerialDB is able to scale efficiently while providing near
real-time performance with different realistic workloads. Further, AerialDB
comprises a decentralized and locality-aware distributed execution engine which
provides graceful degradation of performance upon edge failures with relatively
low latency while processing large spatio-temporal data. AerialDB exhibits
comparable insertion performance and 100 times improvement in query performance
against state-of-the-art baseline. Moreover, it exhibits a 10 times and 100
times improvement with insertion and query workloads respectively over the
cloud baseline.

</details>


### [396] [FlashMP: Fast Discrete Transform-Based Solver for Preconditioning Maxwell's Equations on GPUs](https://arxiv.org/abs/2508.07193)
*Haoyuan Zhang,Yaqian Gao,Xinxin Zhang,Jialin Li,Runfeng Jin,Yidong Chen,Feng Zhang,Wu Yuan,Wenpeng Ma,Shan Liang,Jian Zhang,Zhonghua Lu*

Main category: cs.DC

TL;DR: FlashMP is a GPU-based preconditioning system that accelerates the solution of large-scale linear systems in electromagnetic simulations, demonstrating up to 16x reduction in iteration counts and 4.9x speedup.


<details>
  <summary>Details</summary>
Motivation: Current solvers for the CN-FDTD method in electromagnetic simulations face slow convergence issues due to ill-conditioned operators, and existing preconditioners and direct solvers are either ineffective or memory-intensive.

Method: FlashMP introduces a subdomain exact solver built on discrete transforms and utilizes a GPU-based implementation with multi-GPU domain decomposition for efficient scalability.

Result: FlashMP significantly improves convergence, reduces iteration counts by up to 16x, and achieves 2.5x to 4.9x speedups over Hypre library baselines, with parallel efficiencies of 84.1% in weak scalability tests on large GPU clusters.

Conclusion: FlashMP offers a practical and high-performance solution for solving large-scale sparse linear systems in electromagnetic simulations, especially in multi-GPU cluster environments.

Abstract: Efficiently solving large-scale linear systems is a critical challenge in
electromagnetic simulations, particularly when using the Crank-Nicolson
Finite-Difference Time-Domain (CN-FDTD) method. Existing iterative solvers are
commonly employed to handle the resulting sparse systems but suffer from slow
convergence due to the ill-conditioned nature of the double-curl operator.
Approximate preconditioners, like Successive Over-Relaxation (SOR) and
Incomplete LU decomposition (ILU), provide insufficient convergence, while
direct solvers are impractical due to excessive memory requirements. To address
this, we propose FlashMP, a novel preconditioning system that designs a
subdomain exact solver based on discrete transforms. FlashMP provides an
efficient GPU implementation that achieves multi-GPU scalability through domain
decomposition. Evaluations on AMD MI60 GPU clusters (up to 1000 GPUs) show that
FlashMP reduces iteration counts by up to 16x and achieves speedups of 2.5x to
4.9x compared to baseline implementations in state-of-the-art libraries such as
Hypre. Weak scalability tests show parallel efficiencies up to 84.1%.

</details>


### [397] [An Experimental Exploration of In-Memory Computing for Multi-Layer Perceptrons](https://arxiv.org/abs/2508.07317)
*Pedro Carrinho,Hamid Moghadaspour,Oscar Ferraz,João Dinis Ferreira,Yann Falevoz,Vitor Silva,Gabriel Falcao*

Main category: cs.DC

TL;DR: The paper studies the potential of processing-in-memory (PiM) architectures to accelerate neural networks, focusing on the UPMEM PiM system; results show significant performance improvements compared to CPUs and competitive speeds relative to low-power GPUs.


<details>
  <summary>Details</summary>
Motivation: Modern memory-bound workloads suffer from the data movement bottleneck when transferring large-scale data between memory and CPU, and processing-in-memory aims to address this inefficiency.

Method: The study evaluates the UPMEM PiM system by benchmarking multilayer perceptron (MLP) implementations against CPUs and low-power GPUs, focusing on both high-batch-size inference and smaller MLPs using scratchpad memory.

Result: UPMEM PiM achieves up to 259× performance improvement for large-batch inference compared to CPUs, and using WRAM results in kernel execution times under 3 ms, comparable to low-power GPUs.

Conclusion: Processing-in-memory, specifically UPMEM, shows promising potential for accelerating memory-bound workloads such as neural network inference, with substantial efficiency gains and competitive results to GPUs.

Abstract: In modern computer architectures, the performance of many memory-bound
workloads (e.g., machine learning, graph processing, databases) is limited by
the data movement bottleneck that emerges when transferring large amounts of
data between the main memory and the central processing unit (CPU).
Processing-in-memory is an emerging computing paradigm that aims to alleviate
this data movement bottleneck by performing computation close to or within the
memory units, where data resides. One example of a prevalent workload whose
performance is bound by the data movement bottleneck is the training and
inference process of artificial neural networks. In this work, we analyze the
potential of modern general-purpose PiM architectures to accelerate neural
networks. To this end, we selected the UPMEM PiM system, the first commercially
available real-world general-purpose PiM architecture. We compared the
implementation of multilayer perceptrons (MLPs) in PiM with a sequential
baseline running on an Intel Xeon CPU. The UPMEM implementation achieves up to
$259\times$ better performance for inference of large batch sizes when compared
against the CPU that exploits the size of the available PiM memory.
Additionally, two smaller MLPs were implemented using UPMEM's working SRAM
(WRAM), a scratchpad memory, to evaluate their performance against a low-power
Nvidia Jetson graphics processing unit (GPU), providing further insights into
the efficiency of UPMEM's PiM for neural network inference. Results show that
using WRAM achieves kernel execution times for MLP inference of under $3$ ms,
which is within the same order of magnitude as low-power GPUs.

</details>


### [398] [On the Efficiency of Dynamic Transaction Scheduling in Blockchain Sharding](https://arxiv.org/abs/2508.07472)
*Ramesh Adhikari,Costas Busch,Miroslav Popovic*

Main category: cs.DC

TL;DR: This paper proposes dynamic scheduling algorithms for blockchain sharding systems to tackle transactions arriving online with unknown access distances.


<details>
  <summary>Details</summary>
Motivation: The study aims to address latencies and inefficiencies in blockchain transaction processing by designing efficient shard-based scheduling systems for online transactions.

Method: Two models of shard graph scheduling (stateless and stateful) were explored, with competitive ratio bounds established for each and analysis on NP-hardness of approximating optimal schedules.

Result: For stateless scheduling, an $O(d \log^2 s \cdot \min\{k, \sqrt{s}\})$ competitive ratio for latency was achieved, while for stateful scheduling, the ratio was improved to $O(\log s\cdot \min\{k, \sqrt{s}\}+\log^2 s)$. It approximates optimal scheduling within poly-log factors.

Conclusion: The findings contribute novel dynamic scheduling algorithms with provable efficiency for blockchain sharding systems, substantiating their feasibility and computational limits.

Abstract: Sharding is a technique to speed up transaction processing in blockchains,
where the $n$ processing nodes in the blockchain are divided into $s$ disjoint
groups (shards) that can process transactions in parallel. We study dynamic
scheduling problems on a shard graph $G_s$ where transactions arrive online
over time and are not known in advance. Each transaction may access at most $k$
shards, and we denote by $d$ the worst distance between a transaction and its
accessing (destination) shards (the parameter $d$ is unknown to the shards). To
handle different values of $d$, we assume a locality sensitive decomposition of
$G_s$ into clusters of shards, where every cluster has a leader shard that
schedules transactions for the cluster. We first examine the simpler case of
the stateless model, where leaders are not aware of the current state of the
transaction accounts, and we prove a $O(d \log^2 s \cdot \min\{k, \sqrt{s}\})$
competitive ratio for latency. We then consider the stateful model, where
leader shards gather the current state of accounts, and we prove a $O(\log
s\cdot \min\{k, \sqrt{s}\}+\log^2 s)$ competitive ratio for latency. Each
leader calculates the schedule in polynomial time for each transaction that it
processes. We show that for any $\epsilon > 0$, approximating the optimal
schedule within a $(\min\{k, \sqrt{s}\})^{1 -\epsilon}$ factor is NP-hard.
Hence, our bound for the stateful model is within a poly-log factor from the
best possibly achievable. To the best of our knowledge, this is the first work
to establish provably efficient dynamic scheduling algorithms for blockchain
sharding systems.

</details>


### [399] [Coordinated Power Management on Heterogeneous Systems](https://arxiv.org/abs/2508.07605)
*Zhong Zheng,Michael E. Papka,Zhiling Lan*

Main category: cs.DC

TL;DR: OPEN integrates offline and online profiling phases for lightweight performance prediction in heterogeneous computing systems, achieving up to 98.29% accuracy and reducing profiling costs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reducing profiling costs in performance modeling for heterogeneous systems, while maintaining prediction accuracy.

Method: OPEN employs an offline phase to construct a performance predictor and dense matrix, followed by an online phase utilizing collaborative filtering for predictions based on lightweight profiling.

Result: OPEN was tested on various HPC environments, achieving prediction accuracy of up to 98.29%, proving its efficacy in cost reduction without compromising accuracy.

Conclusion: The framework effectively balances cost-efficiency and accuracy, making it suitable for power and performance-aware runtime decisions in modern HPC systems.

Abstract: Performance prediction is essential for energy-efficient computing in
heterogeneous computing systems that integrate CPUs and GPUs. However,
traditional performance modeling methods often rely on exhaustive offline
profiling, which becomes impractical due to the large setting space and the
high cost of profiling large-scale applications. In this paper, we present
OPEN, a framework consists of offline and online phases. The offline phase
involves building a performance predictor and constructing an initial dense
matrix. In the online phase, OPEN performs lightweight online profiling, and
leverages the performance predictor with collaborative filtering to make
performance prediction. We evaluate OPEN on multiple heterogeneous systems,
including those equipped with A100 and A30 GPUs. Results show that OPEN
achieves prediction accuracy up to 98.29\%. This demonstrates that OPEN
effectively reduces profiling cost while maintaining high accuracy, making it
practical for power-aware performance modeling in modern HPC environments.
Overall, OPEN provides a lightweight solution for performance prediction under
power constraints, enabling better runtime decisions in power-aware computing
environments.

</details>


### [400] [Perpetual exploration in anonymous synchronous networks with a Byzantine black hole](https://arxiv.org/abs/2508.07703)
*Adri Bhattacharya,Pritam Goswami,Evangelos Bampas,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: This paper investigates how mobile agents can perpetually explore an unknown graph with one adversarial Byzantine black hole node that may destroy agents unpredictably. It focuses on defining and solving two variants of the exploration problem.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of perpetual exploration in graphs when an adversarial node (a Byzantine black hole) intermittently behaves maliciously and the network's structure is unknown.

Method: The methodology involves defining two problems—perpetual exploration of at least one safe graph component and exploration of the component containing a specific 'home' node—using a synchronous scheduler and face-to-face communication. The minimum required number of agents is analyzed.

Result: For acyclic graphs, algorithms are presented requiring 4 agents for general exploration and 6 agents for home-based exploration. For general graphs, lower and upper bounds on the required agents are determined.

Conclusion: This work provides both lower bounds and optimal algorithms for perpetual graph exploration in the presence of a Byzantine black hole, and it is the first study addressing such problems without prior topological knowledge of the network.

Abstract: In this paper, we investigate: ``How can a group of initially co-located
mobile agents perpetually explore an unknown graph, when one stationary node
occasionally behaves maliciously, under an adversary's control?'' We call this
node a ``Byzantine black hole (BBH)'' and at any given round it may choose to
destroy all visiting agents, or none. This subtle power can drastically
undermine classical exploration strategies designed for an always active black
hole. We study this perpetual exploration problem in the presence of at most
one BBH, without initial knowledge of the network size. Since the underlying
graph may be 1-connected, perpetual exploration of the entire graph may be
infeasible. We thus define two variants: \pbmPerpExpl\ and \pbmPerpExplHome. In
the former, the agents are tasked to perform perpetual exploration of at least
one component, obtained after the exclusion of the BBH. In the latter, the
agents are tasked to perform perpetual exploration of the component which
contains the \emph{home} node, where agents are initially co-located.
Naturally, \pbmPerpExplHome\ is a special case of \pbmPerpExpl. Agents operate
under a synchronous scheduler and communicate in a face-to-face model. Our goal
is to determine the minimum number of agents necessary and sufficient to solve
these problems. In acyclic networks, we obtain optimal algorithms that solve
\pbmPerpExpl\ with $4$ agents, and \pbmPerpExplHome\ with $6$ agents in trees.
The lower bounds hold even in path graphs. In general graphs, we give a
non-trivial lower bound of $2\Delta-1$ agents for \pbmPerpExpl, and an upper
bound of $3\Delta+3$ agents for \pbmPerpExplHome. To our knowledge, this is the
first study of a black-hole variant in arbitrary networks without initial
topological knowledge.

</details>


### [401] [Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure](https://arxiv.org/abs/2508.07744)
*Ingo Friese,Jochen Klaffer,Mandy Galkow-Schneider,Sergiy Melnyk,Qiuheng Zhou,Hans Dieter Schotten*

Main category: cs.DC

TL;DR: The paper discusses a 6G network architecture promoting resource-sharing via split computing and a dynamic processing node framework, emphasizing an over-the-top broker for simplified resource allocation.


<details>
  <summary>Details</summary>
Motivation: To address the need for seamless resource access and uniformity across diverse infrastructure in future 6G networks, supporting collaborative ecosystems and performance-guaranteed service delivery.

Method: The study explores the introduction of an over-the-top broker that abstracts infrastructure complexity, tests its applicability in two split computing scenarios, and examines an early proof-of-concept implementation.

Result: The broker successfully streamlines resource allocation and proves its utility across various infrastructures like cloud and networks.

Conclusion: The over-the-top broker is a promising solution to simplify and optimize resource management in 6G networks, enabling collaborative and flexible service deployment.

Abstract: 6G network architectures will usher in a wave of innovative services and
capabilities, introducing concepts like split computing and dynamic processing
nodes. This implicates a paradigm where accessing resources seamlessly aligns
with diverse processing node characteristics, ensuring a uniform interface. In
this landscape, the identity of the operator becomes inconsequential, paving
the way for a collaborative ecosystem where multiple providers contribute to a
shared pool of resources. At the core of this vision is the guarantee of
specific performance parameters, precisely tailored to the location and service
requirements. A consistent layer, as the abstraction of the complexities of
different infrastructure providers, is needed to simplify service deployment.
One promising approach is the introduction of an over-the-top broker for
resource allocation, which streamlines the integration of these services into
the network and cloud infrastructure of the future. This paper explores the
role of the broker in two split computing scenarios. By abstracting the
complexities of various infrastructures, the broker proves to be a versatile
solution applicable not only to cloud environments but also to networks and
beyond. Additionally, a detailed discussion of a proof-of-concept
implementation provides insights into the broker's actual architectural
framework.

</details>


### [402] [Towards Lock Modularization for Heterogeneous Environments](https://arxiv.org/abs/2508.07756)
*Hanze Zhang,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: The paper proposes a novel method for modularizing locks across hardware components to improve resource utilization in heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: Modern hardware heterogeneity creates resource bottlenecks for lock operations, hindering application performance. Current solutions fail to optimize across diverse hardware components.

Method: Lock modularization involves decomposing locks into independent modules, matching resource requirements with hardware attributes to leverage strengths and reduce drawbacks.

Result: The approach systematically assigns lock modules to appropriate hardware components to enhance resource utilization and performance.

Conclusion: Lock modularization offers an effective strategy for addressing resource bottlenecks in heterogeneous environments, unlocking better application efficiency.

Abstract: Modern hardware environments are becoming increasingly heterogeneous, leading
to the emergence of applications specifically designed to exploit this
heterogeneity. Efficiently adopting locks in these applications poses distinct
challenges. The uneven distribution of resources in such environments can
create bottlenecks for lock operations, severely hindering application
performance. Existing solutions are often tailored to specific types of
hardware, which underutilizes resources on other components within
heterogeneous environments.
  This paper introduces a new design principle: decomposing locks across
hardware components to fully utilize unevenly distributed resources in
heterogeneous environments. Following this principle, we propose lock
modularization, a systematic approach that decomposes a lock into independent
modules and assigns them to appropriate hardware components. This approach
aligns the resource requirements of lock modules with the attributes of
specific hardware components, maximizing strengths while minimizing weaknesses.

</details>


### [403] [Performance Evaluation of Brokerless Messaging Libraries](https://arxiv.org/abs/2508.07934)
*Lorenzo La Corte,Syed Aftab Rashid,Andrei-Marian Dan*

Main category: cs.DC

TL;DR: This paper focuses on brokerless messaging systems, evaluating their performance using ZeroMQ, NanoMsg, and NNG through a systematic benchmarking approach.


<details>
  <summary>Details</summary>
Motivation: Performance evaluations for brokerless messaging systems are scarce, despite their growing prominence in addressing single points of failure and enhancing performance.

Method: The authors conduct qualitative analysis to choose messaging libraries and then design an open-source benchmark suite for evaluating these libraries under various metrics and workloads.

Result: Key insights into the limitations and suitability of ZeroMQ, NanoMsg, and NNG for different use cases are provided, assisting in decision-making.

Conclusion: This study aids practitioners in identifying brokerless messaging systems that best align with their needs and highlights the trade-offs in performance and reliability.

Abstract: Messaging systems are essential for efficiently transferring large volumes of
data, ensuring rapid response times and high-throughput communication. The
state-of-the-art on messaging systems mainly focuses on the performance
evaluation of brokered messaging systems, which use an intermediate broker to
guarantee reliability and quality of service. However, over the past decade,
brokerless messaging systems have emerged, eliminating the single point of
failure and trading off reliability guarantees for higher performance. Still,
the state-of-the-art on evaluating the performance of brokerless systems is
scarce. In this work, we solely focus on brokerless messaging systems. First,
we perform a qualitative analysis of several possible candidates, to find the
most promising ones. We then design and implement an extensive open-source
benchmarking suite to systematically and fairly evaluate the performance of the
chosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).
We evaluate these libraries considering different metrics and workload
conditions, and provide useful insights into their limitations. Our analysis
enables practitioners to select the most suitable library for their
requirements.

</details>


### [404] [Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids](https://arxiv.org/abs/2508.08022)
*Roopkatha Banerjee,Sampath Koti,Gyanendra Singh,Anirban Chakraborty,Gurunath Gurrala,Bhushan Jagyasi,Yogesh Simmhan*

Main category: cs.DC

TL;DR: The paper explores using Federated Learning (FL) to enable privacy-sensitive, scalable, and efficient time-series demand forecasting in micro-grids and cities.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for real-time power consumption monitoring while addressing privacy concerns and scalability challenges in transferring data to the cloud.

Method: The authors optimize FL-based DNN training for demand forecasting by addressing non-IID data and computational cost issues, including the use of exponentially weighted loss.

Result: Experiments on 1000+ clients using OpenEIA data confirm that the proposed FL approach improves prediction accuracy and training efficiency compared to baselines like ARIMA and non-scalable DNNs.

Conclusion: The proposed strategies succeed in enhancing FL for scalable, real-world applications in power demand forecasting, demonstrating scalable accuracy improvements and reduced training costs.

Abstract: Real-time monitoring of power consumption in cities and micro-grids through
the Internet of Things (IoT) can help forecast future demand and optimize grid
operations. But moving all consumer-level usage data to the cloud for
predictions and analysis at fine time scales can expose activity patterns.
Federated Learning~(FL) is a privacy-sensitive collaborative DNN training
approach that retains data on edge devices, trains the models on private data
locally, and aggregates the local models in the cloud. But key challenges
exist: (i) clients can have non-independently identically distributed~(non-IID)
data, and (ii) the learning should be computationally cheap while scaling to
1000s of (unseen) clients. In this paper, we develop and evaluate several
optimizations to FL training across edge and cloud for time-series demand
forecasting in micro-grids and city-scale utilities using DNNs to achieve a
high prediction accuracy while minimizing the training cost. We showcase the
benefit of using exponentially weighted loss while training and show that it
further improves the prediction of the final model. Finally, we evaluate these
strategies by validating over 1000s of clients for three states in the US from
the OpenEIA corpus, and performing FL both in a pseudo-distributed setting and
a Pi edge cluster. The results highlight the benefits of the proposed methods
over baselines like ARIMA and DNNs trained for individual consumers, which are
not scalable.

</details>


### [405] [On the Operational Resilience of CBDC: Threats and Prospects of Formal Validation for Offline Payments](https://arxiv.org/abs/2508.08064)
*Marco Bernardo,Federico Calandra,Andrea Esposito,Francesco Fabris*

Main category: cs.DC

TL;DR: The paper discusses the use of formal methods to ensure software reliability in digital financial systems, particularly CBDCs, focusing on offline payments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the potential catastrophic risks caused by software bugs in digital monetary systems like CBDCs.

Method: Advocating for the application of formal methods, which can provide assertions of system correctness, to validate critical software resilience.

Result: The result is a proposed framework or recommendation for applying formal methods to evaluate the operational resilience of CBDC infrastructures, especially offline transactions.

Conclusion: The study concludes that formal methods can play a crucial role in mitigating software risks in CBDCs, ensuring stability and resilience.

Abstract: Information and communication technologies are by now employed in most
activities, including economics and finance. Despite the extraordinary power of
modern computers and the vast amount of memory, some results of theoretical
computer science imply the impossibility of certifying software quality in
general. With the exception of safety-critical systems, this has primarily
concerned the information processed by confined systems, with limited
socio-economic consequences. In the emerging era of technologies for exchanging
digital money and tokenized assets over the Internet - such as central bank
digital currencies (CBDCs) - even a minor bug could trigger a financial
collapse. Although the aforementioned impossibility results cannot be overcome
in an absolute sense, there exist formal methods that can provide assertions of
computing systems correctness. We advocate their use to validate the
operational resilience of software infrastructures enabling CBDCs, with special
emphasis on offline payments as they constitute a very critical issue.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [406] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: The paper proposes a generalized scaling law for large language models addressing limitations in scalability and efficiency for both dense and sparse architectures.


<details>
  <summary>Details</summary>
Motivation: The growing computational cost and size of language models have made efficient resource allocation and training optimization a crucial challenge, which existing scaling laws address only within specific architectures.

Method: Revisiting current scaling laws, the authors propose a new universal scaling framework applicable to both dense and sparse language models.

Result: The generalized scaling law outperforms or complements existing architecture-specific laws, demonstrating broader effectiveness in resource efficiency and prediction of model size.

Conclusion: This work introduces a versatile scaling law framework suitable for diverse large language model architectures, potentially aiding efficient computational planning and resource management.

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [407] [Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems](https://arxiv.org/abs/2508.06539)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: This paper introduces a new theory called Self-Organizing Survival Manifolds (SOSM), which models survival as a geometric property arising from biological dynamics, rather than relying on labeled data.


<details>
  <summary>Details</summary>
Motivation: To move away from traditional survival modeling that depends on externally annotated labels and fixed covariates, and instead model survival as an emergent, unlabeled biological phenomenon rooted in physical laws.

Method: The paper develops a framework based on geodesic curvature minimization and builds a survival energy functional. It formulates the problem both discretely and continuously, leveraging connections to thermodynamic and geometric principles.

Result: The study demonstrates how survival-related dynamics naturally emerge and align with geometric flow stability under biologically plausible conditions. This provides a label-free modeling foundation.

Conclusion: This work redefines survival as a manifestation of geometric phase transitions in biological manifolds, offering a unifying perspective across machine learning, biophysics, and geometry.

Abstract: Survival is traditionally modeled as a supervised learning task, reliant on
curated outcome labels and fixed covariates. This work rejects that premise. It
proposes that survival is not an externally annotated target but a geometric
consequence: an emergent property of the curvature and flow inherent in
biological state space. We develop a theory of Self-Organizing Survival
Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature
geodesic flows on latent manifolds shaped by internal biological constraints. A
survival energy functional based on geodesic curvature minimization is
introduced and shown to induce structures where prognosis aligns with geometric
flow stability. We derive discrete and continuous formulations of the objective
and prove theoretical results demonstrating the emergence and convergence of
survival-aligned trajectories under biologically plausible conditions. The
framework draws connections to thermodynamic efficiency, entropy flow, Ricci
curvature, and optimal transport, grounding survival modeling in physical law.
Health, disease, aging, and death are reframed as geometric phase transitions
in the manifold's structure. This theory offers a universal, label-free
foundation for modeling survival as a property of form, not annotation-bridging
machine learning, biophysics, and the geometry of life itself.

</details>


### [408] [Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering](https://arxiv.org/abs/2508.06574)
*Fatemeh Moradi,Mehran Tarif,Mohammadhossein Homaei*

Main category: cs.LG

TL;DR: This paper introduces a two-phase fraud detection method for modern supply chains, achieving strong results with limited supervision and addressing class imbalance.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges like class imbalance, complexity, and limited labeled data in supply chain fraud detection systems.

Method: Proposed a two-phase framework: unsupervised anomaly detection using Isolation Forest, followed by semi-supervised learning via a self-trained SVM using labeled and pseudo-labeled samples.

Result: Achieved an F1-score of 0.817 and a false positive rate below 3.0% on the DataCo Smart Supply Chain Dataset.

Conclusion: The method is effective and efficient under real-world conditions but faces limitations such as concept drift and the absence of deep learning baseline comparisons.

Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the
complexity of global networks and the scarcity of labeled data. Traditional
detection methods often struggle with class imbalance and limited supervision,
reducing their effectiveness in real-world applications. This paper proposes a
novel two-phase learning framework to address these challenges. In the first
phase, the Isolation Forest algorithm performs unsupervised anomaly detection
to identify potential fraud cases and reduce the volume of data requiring
further analysis. In the second phase, a self-training Support Vector Machine
(SVM) refines the predictions using both labeled and high-confidence
pseudo-labeled samples, enabling robust semi-supervised learning. The proposed
method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive
real-world supply chain dataset with fraud indicators. It achieves an F1-score
of 0.817 while maintaining a false positive rate below 3.0%. These results
demonstrate the effectiveness and efficiency of combining unsupervised
pre-filtering with semi-supervised refinement for supply chain fraud detection
under real-world constraints, though we acknowledge limitations regarding
concept drift and the need for comparison with deep learning approaches.

</details>


### [409] [GFlowNets for Learning Better Drug-Drug Interaction Representations](https://arxiv.org/abs/2508.06576)
*Azmine Toushik Wasi*

Main category: cs.LG

TL;DR: The paper proposes a model combining Generative Flow Networks and Variational Graph Autoencoders to address class imbalance in Drug-Drug Interaction (DDI) prediction, with a focus on improving performance for rare interactions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve model performance and clinical reliability in DDI predictions by addressing the significant class imbalance, where rare but critical interactions are underrepresented.

Method: The method involves using Generative Flow Networks and Variational Graph Autoencoders to generate synthetic samples for underrepresented, rare classes of DDIs.

Result: The result is enhanced predictive performance across various class types of DDIs, including rare and critical interactions.

Conclusion: The study concludes that the proposed framework improves prediction reliability in clinical applications by balancing performance across interaction types.

Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.

</details>


### [410] [Hypergraph Neural Network with State Space Models for Node Classification](https://arxiv.org/abs/2508.06587)
*A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: The paper introduces HGMN, a hypergraph neural network blending role-aware and adjacency-based features using a state space model to enhance node classification.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs inadequately capture role-based characteristics in graph-structured data, leading to suboptimal performance in downstream tasks.

Method: HGMN leverages hypergraph construction techniques, a learnable transformer mechanism, and hypergraph convolution layers to enrich role-aware representations, reducing over-smoothing using residual networks.

Result: HGMN demonstrates superior node classification performance across one new dataset and four benchmarks compared to state-of-the-art GNNs.

Conclusion: HGMN effectively integrates role-based and adjacency-based features, enhancing representational power for versatile graph learning applications.

Abstract: In recent years, graph neural networks (GNNs) have gained significant
attention for node classification tasks on graph-structured data. However,
traditional GNNs primarily focus on adjacency relationships between nodes,
often overlooking the rich role-based characteristics that are crucial for
learning more expressive node representations. Existing methods for capturing
role-based features are largely unsupervised and fail to achieve optimal
performance in downstream tasks. To address these limitations, we propose a
novel hypergraph neural network with state space model (HGMN) that effectively
integrates role-aware representations into GNNs and the state space model. HGMN
utilizes hypergraph construction techniques to model higher-order relationships
and combines role-based and adjacency-based representations through a learnable
mamba transformer mechanism. By leveraging two distinct hypergraph construction
methods-based on node degree and neighborhood levels, it strengthens the
connections among nodes with similar roles, enhancing the model's
representational power. Additionally, the inclusion of hypergraph convolution
layers enables the model to capture complex dependencies within hypergraph
structures. To mitigate the over-smoothing problem inherent in deep GNNs, we
incorporate a residual network, ensuring improved stability and better feature
propagation across layers. Extensive experiments conducted on one newly
introduced dataset and four benchmark datasets demonstrate the superiority of
HGMN. The model achieves significant performance improvements on node
classification tasks compared to state-of-the-art GNN methods. These results
highlight HGMN's ability to provide enriched node representations by
effectively embedding role-based features alongside adjacency information,
making it a versatile and powerful tool for a variety of graph-based learning
applications.

</details>


### [411] [Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning](https://arxiv.org/abs/2508.06588)
*Zian Zhai,Fan Li,Xingyu Tan,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.LG

TL;DR: The paper addresses codebook collapse in Vector Quantization (VQ) applied to graph data and proposes a novel framework, RGVQ, to enhance token diversity and codebook utilization for improved performance.


<details>
  <summary>Details</summary>
Motivation: The motivation was to tackle the underexplored issue of codebook collapse in graph-structured data when applying Vector Quantization, as existing mitigation strategies from other domains proved ineffective.

Method: RGVQ introduces soft assignments using Gumbel-Softmax reparameterization to update all codewords during training, alongside structure-aware contrastive regularization to penalize token co-assignments among similar nodes.

Result: Extensive experiments confirmed that RGVQ significantly improves codebook utilization and boosts performance in graph VQ backbones across several downstream tasks.

Conclusion: RGVQ enhances the representation of graph tokens by addressing codebook collapse, promoting diversity and generalization, and enabling more effective application of VQ to graph data.

Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for
learning discrete representations of graph-structured data. However, a
fundamental challenge, i.e., codebook collapse, remains underexplored in the
graph domain, significantly limiting the expressiveness and generalization of
graph tokens.In this paper, we present the first empirical study showing that
codebook collapse consistently occurs when applying VQ to graph data, even with
mitigation strategies proposed in vision or language domains. To understand why
graph VQ is particularly vulnerable to collapse, we provide a theoretical
analysis and identify two key factors: early assignment imbalances caused by
redundancy in graph features and structural patterns, and self-reinforcing
optimization loops in deterministic VQ. To address these issues, we propose
RGVQ, a novel framework that integrates graph topology and feature similarity
as explicit regularization signals to enhance codebook utilization and promote
token diversity. RGVQ introduces soft assignments via Gumbel-Softmax
reparameterization, ensuring that all codewords receive gradient updates. In
addition, RGVQ incorporates a structure-aware contrastive regularization to
penalize the token co-assignments among similar node pairs. Extensive
experiments demonstrate that RGVQ substantially improves codebook utilization
and consistently boosts the performance of state-of-the-art graph VQ backbones
across multiple downstream tasks, enabling more expressive and transferable
graph token representations.

</details>


### [412] [A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis](https://arxiv.org/abs/2508.06589)
*Xinglin Zhao,Yanwen Wang,Xiaobo Liu,Yanrong Hao,Rui Cao,Xin Wen*

Main category: cs.LG

TL;DR: The paper introduces a federated learning framework with dynamic navigation and meta-integration modules to improve neuroimaging CAD systems for diagnosing neurological and psychiatric disorders, achieving enhanced accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Challenges in neuroimaging CAD systems include low reproducibility in small-sample studies and heterogeneity in large-scale datasets due to subtype confounding.

Method: The proposed framework features a dynamic navigation module to route samples based on latent subtypes and a meta-integration module to unify diagnostic predictions from local models.

Result: The framework demonstrated significant improvements in diagnostic accuracy, achieving 74.06% average accuracy across study sites, outperforming traditional methods.

Conclusion: The framework effectively handles data heterogeneity and subtype confounding, improving neuroimaging CAD system reliability and reproducibility, with potential for personalized medicine in neurology and psychiatry.

Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing
neuroimaging data for neurological and psychiatric disorders. However,
small-sample studies suffer from low reproducibility, while large-scale
datasets introduce confounding heterogeneity due to multiple disease subtypes
being labeled under a single category. To address these challenges, we propose
a novel federated learning framework tailored for neuroimaging CAD systems. Our
approach includes a dynamic navigation module that routes samples to the most
suitable local models based on latent subtype representations, and a
meta-integration module that combines predictions from heterogeneous local
models into a unified diagnostic output. We evaluated our framework using a
comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100
healthy controls across multiple study cohorts. Experimental results
demonstrate significant improvements in diagnostic accuracy and robustness
compared to traditional methods. Specifically, our framework achieved an
average accuracy of 74.06\% across all tested sites, showcasing its
effectiveness in handling subtype heterogeneity and enhancing model
generalizability. Ablation studies further confirmed the importance of both the
dynamic navigation and meta-integration modules in improving performance. By
addressing data heterogeneity and subtype confounding, our framework advances
reliable and reproducible neuroimaging CAD systems, offering significant
potential for personalized medicine and clinical decision-making in neurology
and psychiatry.

</details>


### [413] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: The paper introduces an innovative AI framework combining generative models and interdisciplinary literature to design bioinspired materials, validated through experimental results.


<details>
  <summary>Details</summary>
Motivation: The research aims to overcome the limited application of large language models in experimental sciences, especially in interdisciplinary domains like materials science.

Method: The study employs AI tools, including a fine-tuned model (BioinspiredLLM), Retrieval-Augmented Generation (RAG), agent-based systems, and Hierarchical Sampling. These tools are used to extract insights and build hypotheses for designing bioinspired materials.

Result: The framework successfully generated experimentally validated outputs, like a novel pollen-based adhesive with tunable morphology and high shear strength.

Conclusion: AI-assisted ideation is shown to be effective for materials design, establishing new pathways for integrating human-AI collaboration in interdisciplinary experimental science.

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [414] [PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets](https://arxiv.org/abs/2508.07253)
*Bartlomiej Chybowski,Shima Abdullateef,Hollan Haule,Alfredo Gonzalez-Sulser,Javier Escudero*

Main category: cs.LG

TL;DR: This paper introduces an open-source machine-learning framework for robust and generalizable seizure detection across clinical EEG datasets, showcasing high within-dataset performance and strong cross-dataset transferability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current seizure detection methods that rely on dataset-specific optimizations, hindering real-world applicability and reproducibility.

Method: The framework employs automated pre-processing for data standardization, a majority voting mechanism for model assessment, and evaluates its performance on two public EEG datasets for within-dataset and cross-dataset scenarios.

Result: Within-dataset AUC is 0.904 on CHB-MIT and 0.864 on TUSZ, with cross-dataset performance of 0.615 (CHB-MIT to TUSZ) and 0.762 (TUSZ to CHB-MIT). Post-processing yielded slight improvements in AUC.

Conclusion: The framework demonstrates potential for clinically viable seizure detection in diverse settings, complementing expert interpretation and advancing reproducibility for widespread adoption.

Abstract: Reliable seizure detection is critical for diagnosing and managing epilepsy,
yet clinical workflows remain dependent on time-consuming manual EEG
interpretation. While machine learning has shown promise, existing approaches
often rely on dataset-specific optimisations, limiting their real-world
applicability and reproducibility. Here, we introduce an innovative,
open-source machine-learning framework that enables robust and generalisable
seizure detection across varied clinical datasets. We evaluate our approach on
two publicly available EEG datasets that differ in patient populations and
electrode configurations. To enhance robustness, the framework incorporates an
automated pre-processing pipeline to standardise data and a majority voting
mechanism, in which multiple models independently assess each second of EEG
before reaching a final decision. We train, tune, and evaluate models within
each dataset, assessing their cross-dataset transferability. Our models achieve
high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and
0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets
despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models
trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)
without any post-processing. Furthermore, a mild post-processing improved the
within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset
results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the
potential of, and essential considerations for, deploying our framework in
diverse clinical settings. By making our methodology fully reproducible, we
provide a foundation for advancing clinically viable, dataset-agnostic seizure
detection systems. This approach has the potential for widespread adoption,
complementing rather than replacing expert interpretation, and accelerating
clinical integration.

</details>


### [415] [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)
*Kyle O'Brien,Stephen Casper,Quentin Anthony,Tomek Korbak,Robert Kirk,Xander Davies,Ishan Mishra,Geoffrey Irving,Yarin Gal,Stella Biderman*

Main category: cs.LG

TL;DR: This paper investigates the use of pretraining data curation to defend open-weight AI systems against tampering attacks, showing significant resistance to adversarial fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to address the vulnerability of open-weight AI systems to tampering attacks and the lack of a robust science for managing such risks.

Method: The authors propose a multi-stage pipeline for filtering dual-use topics from training data and evaluate its effectiveness by pretraining several models from scratch to assess resilience against attacks.

Result: The filtered models demonstrate substantial resistance to adversarial fine-tuning, outperforming existing methods by over an order of magnitude, while retaining unrelated capabilities.

Conclusion: Pretraining data curation emerges as a promising defense layer for open-weight AI, though complementary measures are necessary due to limitations when dangerous information is contextually provided.

Abstract: Open-weight AI systems offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to tampering attacks which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of adversarial fine-tuning. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable
data filtering and show that it offers a tractable and effective method for
minimizing biothreat proxy knowledge in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a defense-in-depth approach. Overall, these findings help to establish
pretraining data curation as a promising layer of defense for open-weight AI
systems.

</details>


### [416] [Local Diffusion Models and Phases of Data Distributions](https://arxiv.org/abs/2508.06614)
*Fangjun Hu,Guangkuo Liu,Yifan Zhang,Xun Gao*

Main category: cs.LG

TL;DR: The paper rethinks diffusion models, proposing local denoisers to reduce computational costs by analyzing phase transitions in data distributions.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in diffusion models caused by their reliance on computationally expensive global score functions that overlook spatially local data structures.

Method: The authors redefine data phases using spatially local operations and analyze denoising process phases, introducing conditional mutual information as a diagnostic tool to identify phase transitions.

Result: Local denoisers can be effectively utilized in early and late phases, with global networks being required only near phase transitions, as verified through theoretical analysis and real-world dataset experiments.

Conclusion: This study suggests the development of hybrid architectures where local and global networks are leveraged at different stages, improving the efficiency of diffusion models and guiding their design with insights from data distribution phases and physics.

Abstract: As a class of generative artificial intelligence frameworks inspired by
statistical physics, diffusion models have shown extraordinary performance in
synthesizing complicated data distributions through a denoising process
gradually guided by score functions. Real-life data, like images, is often
spatially structured in low-dimensional spaces. However, ordinary diffusion
models ignore this local structure and learn spatially global score functions,
which are often computationally expensive. In this work, we introduce a new
perspective on the phases of data distributions, which provides insight into
constructing local denoisers with reduced computational costs. We define two
distributions as belonging to the same data distribution phase if they can be
mutually connected via spatially local operations such as local denoisers.
Then, we show that the reverse denoising process consists of an early trivial
phase and a late data phase, sandwiching a rapid phase transition where local
denoisers must fail. To diagnose such phase transitions, we prove an
information-theoretic bound on the fidelity of local denoisers based on
conditional mutual information, and conduct numerical experiments in a
real-world dataset. This work suggests simpler and more efficient architectures
of diffusion models: far from the phase transition point, we can use small
local neural networks to compute the score function; global neural networks are
only necessary around the narrow time interval of phase transitions. This
result also opens up new directions for studying phases of data distributions,
the broader science of generative artificial intelligence, and guiding the
design of neural networks inspired by physics concepts.

</details>


### [417] [ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring](https://arxiv.org/abs/2508.08073)
*Dimitris Tsaras,Xing Li,Lei Chen,Zhiyao Xie,Mingxuan Yuan*

Main category: cs.LG

TL;DR: The paper proposes a method to speed up logic optimization in electronic design automation by using a classifier to eliminate unproductive resynthesis operations, achieving an average speedup of 3.9x.


<details>
  <summary>Details</summary>
Motivation: Logic optimization in electronic design automation involves high computational demands, with conventional methods failing in a significant number of cases.

Method: The authors leverage a classifier to preemptively prune unsuccessful iterated cuts during refactor operations, avoiding redundant computations.

Result: Experimental results show that using the proposed classifier to refine the refactor operator achieves a 3.9x speed improvement on average, tested on EPFL benchmarks and industrial designs.

Conclusion: Integrating machine learning classifiers into logic optimization workflows can significantly reduce computation costs while maintaining performance, presenting a promising direction for future enhancements.

Abstract: In electronic design automation, logic optimization operators play a crucial
role in minimizing the gate count of logic circuits. However, their computation
demands are high. Operators such as refactor conventionally form iterative cuts
for each node, striving for a more compact representation - a task which often
fails 98% on average. Prior research has sought to mitigate computational cost
through parallelization. In contrast, our approach leverages a classifier to
prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis
operations. Experiments on the refactor operator using the EPFL benchmark suite
and 10 large industrial designs demonstrate that this technique can speedup
logic optimization by 3.9x on average compared with the state-of-the-art ABC
implementation.

</details>


### [418] [Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels](https://arxiv.org/abs/2508.06622)
*Jeremiah Birrell,Reza Ebrahimi*

Main category: cs.LG

TL;DR: ANTIDOTE introduces objectives for learning under noisy labels through a relaxed information-divergence approach, reformulated into adversarial training. It outperforms existing methods while retaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training models in the presence of noisy or adversarially perturbed labels, a major issue in real-world applications.

Method: ANTIDOTE employs objectives based on an information-divergence neighborhood, reformulating noisy label learning into adversarial training with comparable computational costs to cross-entropy loss.

Result: ANTIDOTE adaptively minimizes the effect of noisy samples, outperforming other loss functions in empirical tests across various types of label noise.

Conclusion: The approach is robust to label noise, has practical adaptability, and maintains efficiency, making it suitable for real-world applications affected by imperfect labels.

Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy
labels which are defined in terms of a relaxation over an
information-divergence neighborhood. Using convex duality, we provide a
reformulation as an adversarial training method that has similar computational
cost to training with standard cross-entropy loss. We show that our approach
adaptively reduces the influence of the samples with noisy labels during
learning, exhibiting a behavior that is analogous to forgetting those samples.
ANTIDOTE is effective in practical environments where label noise is inherent
in the training data or where an adversary can alter the training labels.
Extensive empirical evaluations on different levels of symmetric, asymmetric,
human annotation, and real-world label noise show that ANTIDOTE outperforms
leading comparable losses in the field and enjoys a time complexity that is
very close to that of the standard cross entropy loss.

</details>


### [419] [Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles](https://arxiv.org/abs/2508.08080)
*Cas Oude Hoekstra,Floris den Hengst*

Main category: cs.LG

TL;DR: This study proposes Symbolic Quantile Regression (SQR) for predicting conditional quantiles and examines its efficacy in comparison with existing transparent and black-box models.


<details>
  <summary>Details</summary>
Motivation: To address the gap in using Symbolic Regression for estimating relationships between variables beyond the average, focusing on conditional quantiles, which are crucial in safety-critical applications.

Method: The authors introduce Symbolic Quantile Regression (SQR) and evaluate its performance extensively, including a case study on airline fuel usage.

Result: SQR outperforms transparent models and holds comparable performance to black-box baselines while maintaining transparency.

Conclusion: Symbolic Quantile Regression is effective for predicting conditional quantiles and understanding feature influences across varying quantiles.

Abstract: Symbolic Regression (SR) is a well-established framework for generating
interpretable or white-box predictive models. Although SR has been successfully
applied to create interpretable estimates of the average of the outcome, it is
currently not well understood how it can be used to estimate the relationship
between variables at other points in the distribution of the target variable.
Such estimates of e.g. the median or an extreme value provide a fuller picture
of how predictive variables affect the outcome and are necessary in
high-stakes, safety-critical application domains. This study introduces
Symbolic Quantile Regression (SQR), an approach to predict conditional
quantiles with SR. In an extensive evaluation, we find that SQR outperforms
transparent models and performs comparably to a strong black-box baseline
without compromising transparency. We also show how SQR can be used to explain
differences in the target distribution by comparing models that predict extreme
and central outcomes in an airline fuel usage case study. We conclude that SQR
is suitable for predicting conditional quantiles and understanding interesting
feature influences at varying quantiles.

</details>


### [420] [Using Imperfect Synthetic Data in Downstream Inference Tasks](https://arxiv.org/abs/2508.06635)
*Yewon Byun,Shantanu Gupta,Zachary C. Lipton,Rachel Leah Childers,Bryan Wilder*

Main category: cs.LG

TL;DR: This paper introduces a new generalized method of moments estimator for combining synthetic data generated by large language models with real data, ensuring statistically valid conclusions.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in using large language models to generate synthetic samples to aid computational social science research, but it is unclear how to combine such data with real data for valid statistical analysis.

Method: The authors propose a hyperparameter-free estimator based on generalized method of moments, with strong theoretical guarantees.

Result: The proposed estimator shows that interactions between residuals of synthetic and real data can enhance parameter estimation accuracy.

Conclusion: The method achieves strong experimental performance in regression tasks in computational social science, demonstrating its practical utility and effectiveness.

Abstract: Predictions and generations from large language models are increasingly being
explored as an aid to computational social science and human subject research
in limited data regimes. While previous technical work has explored the
potential to use model-predicted labels for unlabeled data in a principled
manner, there is increasing interest in using large language models to generate
entirely new synthetic samples (also termed as synthetic simulations), such as
in responses to surveys. However, it is not immediately clear by what means
practitioners can combine such data with real data and yet produce
statistically valid conclusions upon them. In this work, we introduce a new
estimator based on generalized method of moments, providing a
hyperparameter-free solution with strong theoretical guarantees to address the
challenge at hand. Surprisingly, we find that interactions between the moment
residuals of synthetic data and those of real data can improve estimates of the
target parameter. We empirically validate the finite-sample performance of our
estimator across different regression tasks in computational social science
applications, demonstrating large empirical gains.

</details>


### [421] [Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift](https://arxiv.org/abs/2508.06776)
*Amit Pandey*

Main category: cs.LG

TL;DR: This paper introduces Zero-Direction Probing (ZDP), a theory-driven method to detect transformer model drift without labels or output analysis, offering mathematical guarantees and metrics to monitor representational change.


<details>
  <summary>Details</summary>
Motivation: Understanding and monitoring model drift in transformers during deployment is crucial, but current methods often rely on task-specific data or outputs, which limits their generalizability.

Method: ZDP uses theoretical tools like null space analysis of layer activations, the Fisher geometry, and metrics like Spectral Null-Leakage to detect drift without requiring labeled data or task outputs.

Result: The authors provide mathematical proofs (e.g., Variance-Leak Theorem, Fisher Null-Conservation) and derive metrics with non-asymptotic bounds to reliably detect representational shifts.

Conclusion: Monitoring null spaces and their geometry offers a robust way to capture model drift in transformers, providing transparent, theory-grounded guarantees and eliminating dependence on task labels.

Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for
detecting model drift from null directions of transformer activations without
task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the
Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound
for low-rank updates, and (iv) a logarithmic-regret guarantee for online
null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with
non-asymptotic tail bounds and a concentration inequality, yielding a-priori
thresholds for drift under a Gaussian null model. These results show that
monitoring right/left null spaces of layer activations and their Fisher
geometry provides concrete, testable guarantees on representational change.

</details>


### [422] [Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record](https://arxiv.org/abs/2508.06627)
*Mosbah Aouad,Anirudh Choudhary,Awais Farooq,Steven Nevers,Lusine Demirkhanyan,Bhrandon Harris,Suguna Pappu,Christopher Gondi,Ravishankar Iyer*

Main category: cs.LG

TL;DR: The paper presents a novel multimodal method to detect pancreatic ductal adenocarcinoma (PDAC) up to a year earlier using electronic health records.


<details>
  <summary>Details</summary>
Motivation: The study addresses the urgent need for early detection of PDAC, one of the deadliest cancers, due to a lack of specific symptoms and reliable biomarkers.

Method: The proposed method integrates longitudinal diagnosis code histories and laboratory measurements, utilizing neural controlled differential equations, pretrained language models, recurrent networks, and cross-attention mechanisms.

Result: The method demonstrates an AUC improvement of 6.5% to 15.5% over current state-of-the-art methods using a dataset of 4,700 patients. It also identifies significant biomarkers linked to PDAC risk.

Conclusion: The approach offers a promising advancement in early detection of PDAC and provides insights into associated diagnosis codes and lab panels.

Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and
early detection remains a major clinical challenge due to the absence of
specific symptoms and reliable biomarkers. In this work, we propose a new
multimodal approach that integrates longitudinal diagnosis code histories and
routinely collected laboratory measurements from electronic health records to
detect PDAC up to one year prior to clinical diagnosis. Our method combines
neural controlled differential equations to model irregular lab time series,
pretrained language models and recurrent networks to learn diagnosis code
trajectory representations, and cross-attention mechanisms to capture
interactions between the two modalities. We develop and evaluate our approach
on a real-world dataset of nearly 4,700 patients and achieve significant
improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.
Furthermore, our model identifies diagnosis codes and laboratory panels
associated with elevated PDAC risk, including both established and new
biomarkers. Our code is available at
https://github.com/MosbahAouad/EarlyPDAC-MML.

</details>


### [423] [Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series](https://arxiv.org/abs/2508.06638)
*Muyan Anna Li,Aditi Gautam*

Main category: cs.LG

TL;DR: This paper proposes two adaptive thresholding frameworks (SCS and MACS) for improved anomaly detection in nonstationary time series data.


<details>
  <summary>Details</summary>
Motivation: There is a need for anomaly detection methods that can handle dynamic statistical properties in time series data across various domains.

Method: The authors developed SCS and MACS frameworks using statistical online learning and segmentation techniques to adapt thresholds locally while controlling false alarm rates.

Result: Experiments on Wafer Manufacturing benchmark datasets showed significant F1-score improvements compared to traditional methods.

Conclusion: Adaptive thresholds based on robust statistical principles enable better anomaly detection in evolving real-world conditions.

Abstract: As time series data become increasingly prevalent in domains such as
manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt
to nonstationary environments where statistical properties shift over time.
Traditional static thresholds are easily rendered obsolete by regime shifts,
concept drift, or multi-scale changes. To address these challenges, we
introduce and empirically evaluate two novel adaptive thresholding frameworks:
Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence
Segments (MACS). Both leverage statistical online learning and segmentation
principles for local, contextually sensitive adaptation, maintaining guarantees
on false alarm rates even under evolving distributions. Our experiments across
Wafer Manufacturing benchmark datasets show significant F1-score improvement
compared to traditional percentile and rolling quantile approaches. This work
demonstrates that robust, statistically principled adaptive thresholds enable
reliable, interpretable, and timely detection of diverse real-world anomalies.

</details>


### [424] [Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems](https://arxiv.org/abs/2508.07392)
*Nikita Puchkin,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: The paper proposes a new method using Schrödinger bridges and Ornstein-Uhlenbeck processes for generative modeling and image-to-image translation, deriving bounds on generalization ability and providing experimental results.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in generative modeling and unpaired image-to-image translation by developing an optimal method to transform initial to target distributions, suitable for cases where only i.i.d. samples are accessible.

Method: The authors utilize a stochastic optimal control framework with an Ornstein-Uhlenbeck process as the reference, estimate Schrödinger potentials, and derive generalization bounds using the Kullback-Leibler divergence.

Result: Tight bounds on generalization ability are derived, demonstrating nearly fast convergence rates under favorable conditions, and the method's performance is validated through numerical experiments.

Conclusion: The approach effectively combines theoretical tightness with practical performance, showcasing potential in generative tasks and unpaired image transformations.

Abstract: Modern methods of generative modelling and unpaired image-to-image
translation based on Schr\"odinger bridges and stochastic optimal control
theory aim to transform an initial density to a target one in an optimal way.
In the present paper, we assume that we only have access to i.i.d. samples from
initial and final distributions. This makes our setup suitable for both
generative modelling and unpaired image-to-image translation. Relying on the
stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as
the reference one and estimate the corresponding Schr\"odinger potential.
Introducing a risk function as the Kullback-Leibler divergence between
couplings, we derive tight bounds on generalization ability of an empirical
risk minimizer in a class of Schr\"odinger potentials including Gaussian
mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we
almost achieve fast rates of convergence up to some logarithmic factors in
favourable scenarios. We also illustrate performance of the suggested approach
with numerical experiments.

</details>


### [425] [Fractal Language Modelling by Universal Sequence Maps (USM)](https://arxiv.org/abs/2508.06641)
*Jonas S Almeida,Daniel E Russ,Susana Vinga,Ines Duarte,Lee Mason,Praphulla Bhawsar,Aaron Ge,Arlindo Oliveira,Jeya Balaji Balasubramanian*

Main category: cs.LG

TL;DR: The paper explores Universal Sequence Maps (USM), a fractal, bijective encoding technique for symbolic sequences, resolving issues and providing new insights for sequence embeddings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to design encoding mechanisms that retain contextual information from symbolic sequences to improve their numerical representations for downstream nonlinear modeling like neural networks.

Method: The method involves the use of Universal Sequence Maps (USM), composed of forward and backward Chaos Game Representations (CGR), leveraging frequency domain projections to encode sequences onto numeric spaces while calculating Chebyshev distance metrics and k-mer frequencies efficiently.

Result: The paper resolves biases in USM's iterated processes, achieving accurate numeric mappings for sequences and revealing its nature as a process converging to steady-state embeddings. Examples are demonstrated using genomic sequences.

Conclusion: Universal Sequence Maps are not only capable of resolving earlier inconsistencies but also offer a robust, scalable method for encoding symbolic data for modeling, relevant for diverse alphabets and domains.

Abstract: Motivation: With the advent of Language Models using Transformers,
popularized by ChatGPT, there is a renewed interest in exploring encoding
procedures that numerically represent symbolic sequences at multiple scales and
embedding dimensions. The challenge that encoding addresses is the need for
mechanisms that uniquely retain contextual information about the succession of
individual symbols, which can then be modeled by nonlinear formulations such as
neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively
encode symbolic sequences onto embedded numerical spaces. USM is composed of
two Chaos Game Representations (CGR), iterated forwardly and backwardly, that
can be projected into the frequency domain (FCGR). The corresponding USM
coordinates can be used to compute a Chebyshev distance metric as well as k-mer
frequencies, without having to recompute the embedded numeric coordinates, and,
paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal
Sequence Maps (USM) by resolving seeding biases affecting the iterated process.
The resolution had two results, the first expected, the second an intriguing
outcome: 1) full reconciliation of numeric positioning with sequence identity;
and 2) uncovering the nature of USM as an efficient numeric process converging
towards a steady state sequence embedding solution. We illustrate these results
for genomic sequences because of the convenience of a planar representation
defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,
the application to alphabet of arbitrary cardinality was found to be
straightforward.

</details>


### [426] [MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification](https://arxiv.org/abs/2508.07465)
*Tiantian Yang,Zhiqian Chen*

Main category: cs.LG

TL;DR: MOTGNN is a novel framework integrating multi-omics data using GNNs to enhance disease classification accuracy and interpretability compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in multi-omics disease modeling like high dimensionality and complex interactions, while improving predictive accuracy and interpretability.

Method: The method involves using XGBoost for supervised graph construction, modality-specific GNNs for hierarchical representation learning, and a deep feedforward network for cross-omics data integration.

Result: MOTGNN achieves 5-10% better performance in accuracy, ROC-AUC, and F1-score on disease datasets, is robust to class imbalance, and provides interpretability and computational efficiency.

Conclusion: MOTGNN shows promise in advancing multi-omics disease modeling, offering both improved predictive accuracy and actionable insights into biomarkers and omics contributions.

Abstract: Integrating multi-omics data, such as DNA methylation, mRNA expression, and
microRNA (miRNA) expression, offers a comprehensive view of the biological
mechanisms underlying disease. However, the high dimensionality and complex
interactions among omics layers present major challenges for predictive
modeling. We propose Multi-Omics integration with Tree-generated Graph Neural
Network (MOTGNN), a novel and interpretable framework for binary disease
classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform
omics-specific supervised graph construction, followed by modality-specific
Graph Neural Networks (GNNs) for hierarchical representation learning, and a
deep feedforward network for cross-omics integration. On three real-world
disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in
accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance
(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains
computational efficiency through sparse graphs (2.1-2.8 edges per node) and
provides built-in interpretability, revealing both top-ranked biomarkers and
the relative contributions of each omics modality. These results highlight
MOTGNN's potential to improve both predictive accuracy and interpretability in
multi-omics disease modeling.

</details>


### [427] [Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN](https://arxiv.org/abs/2508.06647)
*Andrey Sidorenko,Paul Tiwald*

Main category: cs.LG

TL;DR: This paper introduces the Tabular Auto-Regressive Generative Network (TabularARGN) for generating high-quality synthetic tabular data with a focus on privacy, utility, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a need for securely sharing sensitive datasets, but traditional anonymization techniques fail to ensure adequate privacy preservation.

Method: The authors propose TabularARGN, a neural network architecture that uses a discretization-based auto-regressive approach to generate synthetic tabular data. They also evaluate its performance using multiple metrics, including privacy and utility tests.

Result: The method delivers competitive results in terms of statistical similarity, machine learning utility, and robustness against detection in comparison with existing methods. It also demonstrates a balanced privacy-utility tradeoff under privacy evaluations.

Conclusion: TabularARGN is an effective and computationally efficient solution for generating synthetic tabular data that maintains high data fidelity and robust privacy protections.

Abstract: Synthetic data generation has become essential for securely sharing and
analyzing sensitive data sets. Traditional anonymization techniques, however,
often fail to adequately preserve privacy. We introduce the Tabular
Auto-Regressive Generative Network (TabularARGN), a neural network architecture
specifically designed for generating high-quality synthetic tabular data. Using
a discretization-based auto-regressive approach, TabularARGN achieves high data
fidelity while remaining computationally efficient. We evaluate TabularARGN
against existing synthetic data generation methods, showing competitive results
in statistical similarity, machine learning utility, and detection robustness.
We further perform an in-depth privacy evaluation using systematic
membership-inference attacks, highlighting the robustness and effective
privacy-utility balance of our approach.

</details>


### [428] [Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications](https://arxiv.org/abs/2508.07473)
*Zijian Liu*

Main category: cs.LG

TL;DR: The paper investigates Online Convex Optimization (OCO) with heavy-tailed gradient noise, providing optimal regret bounds for classical algorithms without modifications.


<details>
  <summary>Details</summary>
Motivation: Limited results exist for OCO when stochastic gradients exhibit heavy tails, i.e., only a finite $\mathsf{p}$-th central moment is guaranteed.

Method: The authors analyze existing OCO algorithms such as Online Gradient Descent in heavy-tailed settings and establish optimal regret bounds under bounded domain assumptions without modifying the algorithms.

Result: Classical OCO algorithms achieve optimal regret bounds, even in the presence of heavy-tailed gradient noise, and this extends to nonsmooth nonconvex optimization and optimistic algorithms.

Conclusion: OCO can be effectively addressed under heavy-tailed gradient noise without additional operations like gradient clipping, enabling broader applications.

Abstract: In Online Convex Optimization (OCO), when the stochastic gradient has a
finite variance, many algorithms provably work and guarantee a sublinear
regret. However, limited results are known if the gradient estimate has a heavy
tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th
central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this
work examines different old algorithms for OCO (e.g., Online Gradient Descent)
in the more challenging heavy-tailed setting. Under the standard bounded domain
assumption, we establish new regrets for these classical methods without any
algorithmic modification. Remarkably, these regret bounds are fully optimal in
all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting
that OCO with heavy tails can be solved effectively without any extra operation
(e.g., gradient clipping). Our new results have several applications. A
particularly interesting one is the first provable convergence result for
nonsmooth nonconvex optimization under heavy-tailed noise without gradient
clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and
extend our ideas to optimistic algorithms to handle different cases
simultaneously.

</details>


### [429] [In-Context Reinforcement Learning via Communicative World Models](https://arxiv.org/abs/2508.06659)
*Fernando Martinez-Lopez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: The paper introduces CORAL, a framework to improve reinforcement learning (RL) agents' ability to generalize and adapt to new tasks using emergent communication and pre-trained world models.


<details>
  <summary>Details</summary>
Motivation: RL agents struggle to generalize to new contexts because their learned representations and policies are overfitted to specific training environments.

Method: The authors propose CORAL, which decouples latent representation learning from control tasks. It uses an Information Agent (IA) pre-trained as a world model to communicate concise contextual messages to a Control Agent (CA). A novel Causal Influence Loss is introduced to shape the emergent communication protocol.

Result: CORAL demonstrated significant sample efficiency and zero-shot adaptation in unseen and sparse-reward environments.

Conclusion: The work validates the approach of using transferable communicative representations to boost RL agents' adaptability to unseen tasks and environments.

Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks
and contexts without updating their parameters, mainly because their learned
representations and policies are overfit to the specifics of their training
environments. To boost agents' in-context RL (ICRL) ability, this work
formulates ICRL as a two-agent emergent communication problem and introduces
CORAL (Communicative Representation for Adaptive RL), a framework that learns a
transferable communicative context by decoupling latent representation learning
from control. In CORAL, an Information Agent (IA) is pre-trained as a world
model on a diverse distribution of tasks. Its objective is not to maximize task
reward, but to build a world model and distill its understanding into concise
messages. The emergent communication protocol is shaped by a novel Causal
Influence Loss, which measures the effect that the message has on the next
action. During deployment, the previously trained IA serves as a fixed
contextualizer for a new Control Agent (CA), which learns to solve tasks by
interpreting the provided communicative context. Our experiments demonstrate
that this approach enables the CA to achieve significant gains in sample
efficiency and successfully perform zero-shot adaptation with the help of
pre-trained IA in entirely unseen sparse-reward environments, validating the
efficacy of learning a transferable communicative representation.

</details>


### [430] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: The paper introduces PANAMA, an algorithm that enhances multi-agent pathfinding using reinforcement learning, focusing on scalable and robust data sharing for Digital Twins.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient data-sharing frameworks and robust algorithms in scaling robotics and automated systems within Digital Twin ecosystems.

Method: Introduced PANAMA, leveraging Priority Asymmetry within a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures.

Result: PANAMA achieves superior pathfinding performance in terms of accuracy, speed, and scalability compared to existing methods.

Conclusion: PANAMA establishes a foundation for better network-aware decision-making and robust coordination among multiple agents, enhancing the interplay between Digital Twins, networks, and AI-driven automation.

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [431] [N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting](https://arxiv.org/abs/2508.07490)
*Ricardo Matos,Luis Roque,Vitor Cerqueira*

Main category: cs.LG

TL;DR: The paper introduces N-BEATS-MOE, a deep learning model for time series forecasting, incorporating a Mixture-of-Experts layer to enhance performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: N-BEATS, while effective, can benefit from a more adaptive and interpretable approach for handling diverse time series characteristics.

Method: The model integrates a Mixture-of-Experts layer with N-BEATS, using a gating network for dynamic block weighting, to adapt to varying series traits.

Result: N-BEATS-MOE delivers consistent improvements in forecasting accuracy across 12 benchmark datasets, particularly those with heterogeneous time series.

Conclusion: The proposed method enhances adaptivity and interpretability, setting a new standard on several time series forecasting tasks.

Abstract: Deep learning approaches are increasingly relevant for time series
forecasting tasks. Methods such as N-BEATS, which is built on stacks of
multilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on
benchmark datasets and competitions. N-BEATS is also more interpretable
relative to other deep learning approaches, as it decomposes forecasts into
different time series components, such as trend and seasonality. In this work,
we present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts
(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a
gating network which allows the model to better adapt to the characteristics of
each time series. We also hypothesize that the gating mechanism provides
additional interpretability by identifying which expert is most relevant for
each series. We evaluate our method across 12 benchmark datasets against
several approaches, achieving consistent improvements on several datasets,
especially those composed of heterogeneous time series.

</details>


### [432] [Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.06663)
*Yuan-Hung Chao,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: The paper integrates Kolmogorov-Arnold Networks (KANs) with traditional Graph Neural Networks (GNNs) to create new models, enhancing performance and enabling graph-independent inference.


<details>
  <summary>Details</summary>
Motivation: Current GNNs are limited in scalability and efficiency due to their dependence on graph connectivity. KANs offer a promising alternative with strong expressiveness and efficiency.

Method: KANs are integrated into GNN architectures (GAT, SGC, APPNP) to create KGAT, KSGC, and KAPPNP. Additionally, a multi-teacher knowledge amalgamation framework is used to train a graph-independent KAN student model.

Result: The new models demonstrate improved node classification accuracy, and the knowledge amalgamation method significantly enhances the performance of the graph-free student model.

Conclusion: Integrating KANs into GNNs enhances their expressiveness and enables efficient inference without relying on graph connectivity, showing significant potential for real-world applications.

Abstract: Graph Neural Networks (GNNs) have shown strong performance on
graph-structured data, but their reliance on graph connectivity often limits
scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent
architecture with learnable univariate functions, offer strong nonlinear
expressiveness and efficient inference. In this work, we integrate KANs into
three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new
models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge
amalgamation framework, where knowledge from multiple KAN-based GNNs is
distilled into a graph-independent KAN student model. Experiments on benchmark
datasets show that the proposed models improve node classification accuracy,
and the knowledge amalgamation approach significantly boosts student model
performance. Our findings highlight the potential of KANs for enhancing GNN
expressiveness and for enabling efficient, graph-free inference.

</details>


### [433] [Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation](https://arxiv.org/abs/2508.06676)
*Chia-Hsun Lu,Guan-Jhih Wu,Ya-Chi Ho,Chih-Ya Shen*

Main category: cs.LG

TL;DR: This paper introduces Discrete Cosine Transform-based Activation Watermarking (DCT-AW), tailored to Kolmogorov-Arnold Networks (KAN), for robust watermarking in machine learning.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to ensure robust protection of advanced machine learning models like KAN, which are increasingly important for network-structured data applications but pose new challenges for traditional watermarking techniques.

Method: The proposed DCT-AW method embeds watermarks into KAN's learnable activation functions by perturbing their outputs using discrete cosine transform, ensuring task independence and wide compatibility.

Result: Experimental evaluations show that DCT-AW minimally impacts model performance while offering superior robustness against various watermark removal attacks, including fine-tuning, pruning, and retraining.

Conclusion: DCT-AW proves to be a suitable and robust watermarking method for KAN, ensuring both effective intellectual property protection and minimal compromise in task performance.

Abstract: With the increasing importance of protecting intellectual property in machine
learning, watermarking techniques have gained significant attention. As
advanced models are increasingly deployed in domains such as social network
analysis, the need for robust model protection becomes even more critical.
While existing watermarking methods have demonstrated effectiveness for
conventional deep neural networks, they often fail to adapt to the novel
architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable
activation functions. KAN holds strong potential for modeling complex
relationships in network-structured data. However, their unique design also
introduces new challenges for watermarking. Therefore, we propose a novel
watermarking method, Discrete Cosine Transform-based Activation Watermarking
(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of
KAN, our method embeds watermarks by perturbing activation outputs using
discrete cosine transform, ensuring compatibility with diverse tasks and
achieving task independence. Experimental results demonstrate that DCT-AW has a
small impact on model performance and provides superior robustness against
various watermark removal attacks, including fine-tuning, pruning, and
retraining after pruning.

</details>


### [434] [FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction](https://arxiv.org/abs/2508.07518)
*Sichen Zhao,Wei Shao,Jeffrey Chan,Ziqi Xu,Flora Salim*

Main category: cs.LG

TL;DR: The paper introduces FairDRL-ST, a framework addressing fairness issues in spatio-temporal predictions, particularly in urban mobility demand forecasting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address fairness concerns arising from biased predictions that may disadvantage certain demographic or geographic groups in urban computing applications.

Method: The framework uses disentangled representation learning and adversarial learning to separate sensitive information and achieve fairness unsupervisedly, minimizing performance loss.

Result: FairDRL-ST reduces fairness gaps and maintains competitive predictive accuracy when applied to real-world urban mobility datasets.

Conclusion: The proposed framework offers a fair and effective approach to spatio-temporal prediction, with applications in ethically deploying AI for public services.

Abstract: As deep spatio-temporal neural networks are increasingly utilised in urban
computing contexts, the deployment of such methods can have a direct impact on
users of critical urban infrastructure, such as public transport, emergency
services, and traffic management systems. While many spatio-temporal methods
focus on improving accuracy, fairness has recently gained attention due to
growing evidence that biased predictions in spatio-temporal applications can
disproportionately disadvantage certain demographic or geographic groups,
thereby reinforcing existing socioeconomic inequalities and undermining the
ethical deployment of AI in public services. In this paper, we propose a novel
framework, FairDRL-ST, based on disentangled representation learning, to
address fairness concerns in spatio-temporal prediction, with a particular
focus on mobility demand forecasting. By leveraging adversarial learning and
disentangled representation learning, our framework learns to separate
attributes that contain sensitive information. Unlike existing methods that
enforce fairness through supervised learning, which may lead to
overcompensation and degraded performance, our framework achieves fairness in
an unsupervised manner with minimal performance loss. We apply our framework to
real-world urban mobility datasets and demonstrate its ability to close
fairness gaps while delivering competitive predictive performance compared to
state-of-the-art fairness-aware methods.

</details>


### [435] [Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select](https://arxiv.org/abs/2508.06692)
*Md. Akmol Masud,Md Abrar Jahin,Mahmud Hasan*

Main category: cs.LG

TL;DR: HeteRo-Select improves Federated Learning stability and accuracy in diverse client settings compared to existing methods, using a smart client selection framework.


<details>
  <summary>Details</summary>
Motivation: Address training instability in Federated Learning caused by client data heterogeneity.

Method: Developed HeteRo-Select framework with theoretical analysis and a scoring system focusing on usefulness, fairness, update speed, and data variety.

Result: Outperformed existing methods in training stability, peak accuracy, and final accuracy using CIFAR-10 dataset experiments.

Conclusion: HeteRo-Select proves effective and reliable for handling heterogeneous data in Federated Learning contexts.

Abstract: Federated Learning (FL) is a machine learning technique that often suffers
from training instability due to the diverse nature of client data. Although
utility-based client selection methods like Oort are used to converge by
prioritizing high-loss clients, they frequently experience significant drops in
accuracy during later stages of training. We propose a theoretical
HeteRo-Select framework designed to maintain high performance and ensure
long-term training stability. We provide a theoretical analysis showing that
when client data is very different (high heterogeneity), choosing a smart
subset of client participation can reduce communication more effectively
compared to full participation. Our HeteRo-Select method uses a clear,
step-by-step scoring system that considers client usefulness, fairness, update
speed, and data variety. It also shows convergence guarantees under strong
regularization. Our experimental results on the CIFAR-10 dataset under
significant label skew ($\alpha=0.1$) support the theoretical findings. The
HeteRo-Select method performs better than existing approaches in terms of peak
accuracy, final accuracy, and training stability. Specifically, HeteRo-Select
achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a
minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak
accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability
drop of $2.73\%$. The theoretical foundations and empirical performance in our
study make HeteRo-Select a reliable solution for real-world heterogeneous FL
problems.

</details>


### [436] [FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks](https://arxiv.org/abs/2508.08151)
*Moses Openja,Paolo Arcaini,Foutse Khomh,Fuyuki Ishikawa*

Main category: cs.LG

TL;DR: FairFLRep is proposed to address biases in DNNs by focusing on neuron weights tied to sensitive attributes like race or gender, ensuring fairness without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: DNNs are increasingly used in critical decision-making tasks, but they often exhibit biases stemming from training data, such as unequal misclassification rates within demographic sub-groups.

Method: FairFLRep involves fairness-driven fault localization and neuron adjustment using input-output analyses to target discrepancies in decision fairness.

Result: FairFLRep consistently improved fairness across datasets and models while maintaining prediction accuracy and outperformed existing methods in efficiency and effectiveness.

Conclusion: FairFLRep is a promising tool for mitigating bias in DNN classifiers, demonstrating its robustness, effectiveness, and efficiency in both fairness and accuracy improvements.

Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our
daily lives, including high-stakes decision-making applications that impact
individuals. However, these systems reflect and amplify bias from the data used
during training and testing, potentially resulting in biased behavior and
inaccurate decisions. For instance, having different misclassification rates
between white and black sub-populations. However, effectively and efficiently
identifying and correcting biased behavior in DNNs is a challenge. This paper
introduces FairFLRep, an automated fairness-aware fault localization and repair
technique that identifies and corrects potentially bias-inducing neurons in DNN
classifiers. FairFLRep focuses on adjusting neuron weights associated with
sensitive attributes, such as race or gender, that contribute to unfair
decisions. By analyzing the input-output relationships within the network,
FairFLRep corrects neurons responsible for disparities in predictive quality
parity. We evaluate FairFLRep on four image classification datasets using two
DNN classifiers, and four tabular datasets with a DNN model. The results show
that FairFLRep consistently outperforms existing methods in improving fairness
while preserving accuracy. An ablation study confirms the importance of
considering fairness during both fault localization and repair stages. Our
findings also show that FairFLRep is more efficient than the baseline
approaches in repairing the network.

</details>


### [437] [Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning](https://arxiv.org/abs/2508.07556)
*Stephan Rabanser*

Main category: cs.LG

TL;DR: The paper focuses on enhancing machine learning (ML) reliability through uncertainty estimation, mainly selective prediction, proposing new methods to improve performance, compatibility with differential privacy, clarify error sources, and defend against adversarial uncertainty manipulations.


<details>
  <summary>Details</summary>
Motivation: ML models are increasingly used in domains requiring high reliability, yet they face challenges in accurately estimating uncertainty, which is crucial for safety and trustworthiness.

Method: The paper introduces a lightweight method leveraging training trajectory uncertainty signals for selective prediction, compatible with differential privacy. It also provides a decomposition of selective classification gaps and explores defenses against adversarial manipulation.

Result: The proposed post-hoc abstention method achieves state-of-the-art performance in selective prediction, remains robust under differential privacy, and identifies specific error sources affecting oracle accuracy-coverage curves.

Conclusion: The findings enhance reliable ML by improving uncertainty estimation methods, understanding error mechanisms, and addressing vulnerabilities, enabling ML models to know when to abstain from predictions.

Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes
domains where reliability is paramount. This thesis investigates how
uncertainty estimation can enhance the safety and trustworthiness of ML,
focusing on selective prediction -- where models abstain when confidence is
low.
  We first show that a model's training trajectory contains rich uncertainty
signals that can be exploited without altering its architecture or loss. By
ensembling predictions from intermediate checkpoints, we propose a lightweight,
post-hoc abstention method that works across tasks, avoids the cost of deep
ensembles, and achieves state-of-the-art selective prediction performance.
Crucially, this approach is fully compatible with differential privacy (DP),
allowing us to study how privacy noise affects uncertainty quality. We find
that while many methods degrade under DP, our trajectory-based approach remains
robust, and we introduce a framework for isolating the privacy-uncertainty
trade-off. Next, we then develop a finite-sample decomposition of the selective
classification gap -- the deviation from the oracle accuracy-coverage curve --
identifying five interpretable error sources and clarifying which interventions
can close the gap. This explains why calibration alone cannot fix ranking
errors, motivating methods that improve uncertainty ordering. Finally, we show
that uncertainty signals can be adversarially manipulated to hide errors or
deny service while maintaining high accuracy, and we design defenses combining
calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating,
and safeguarding uncertainty estimation, enabling models that not only make
accurate predictions -- but also know when to say "I do not know".

</details>


### [438] [CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations](https://arxiv.org/abs/2508.06704)
*Hager Radi Abdelwahed,Mélisande Teng,Robin Zbinden,Laura Pollock,Hugo Larochelle,Devis Tuia,David Rolnick*

Main category: cs.LG

TL;DR: The paper introduces CISO, a deep learning method for species distribution modeling that incorporates incomplete biotic data and achieves superior predictive performance.


<details>
  <summary>Details</summary>
Motivation: Many species distribution models overlook the impact of biotic interactions and face challenges due to sparse or inconsistent species observation data.

Method: The authors propose CISO, a deep learning approach that integrates incomplete species observations with environmental variables to predict species distributions.

Result: CISO demonstrates improved predictive performance across three datasets (plants, birds, butterflies) and benefits from combining data from multiple datasets.

Conclusion: CISO offers a promising new approach for ecological research, effectively incorporating incomplete biotic data to improve species distribution predictions and detect interspecies interactions.

Abstract: Species distribution models (SDMs) are widely used to predict species'
geographic distributions, serving as critical tools for ecological research and
conservation planning. Typically, SDMs relate species occurrences to
environmental variables representing abiotic factors, such as temperature,
precipitation, and soil properties. However, species distributions are also
strongly influenced by biotic interactions with other species, which are often
overlooked. While some methods partially address this limitation by
incorporating biotic interactions, they often assume symmetrical pairwise
relationships between species and require consistent co-occurrence data. In
practice, species observations are sparse, and the availability of information
about the presence or absence of other species varies significantly across
locations. To address these challenges, we propose CISO, a deep learning-based
method for species distribution modeling Conditioned on Incomplete Species
Observations. CISO enables predictions to be conditioned on a flexible number
of species observations alongside environmental variables, accommodating the
variability and incompleteness of available biotic data. We demonstrate our
approach using three datasets representing different species groups: sPlotOpen
for plants, SatBird for birds, and a new dataset, SatButterfly, for
butterflies. Our results show that including partial biotic information
improves predictive performance on spatially separate test sets. When
conditioned on a subset of species within the same dataset, CISO outperforms
alternative methods in predicting the distribution of the remaining species.
Furthermore, we show that combining observations from multiple datasets can
improve performance. CISO is a promising ecological tool, capable of
incorporating incomplete biotic information and identifying potential
interactions between species from disparate taxa.

</details>


### [439] [Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach](https://arxiv.org/abs/2508.07505)
*Yueyang Quan,Chang Wang,Shengjie Zhai,Minghong Fang,Zhuqing Liu*

Main category: cs.LG

TL;DR: The paper introduces DPMixSGD, a privacy-preserving algorithm designed for non-convex decentralized min-max optimization, addressing challenges posed by differential privacy.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns arise in decentralized min-max optimization due to the risk of model updates leaking sensitive data, prompting the need for robust privacy-preserving methods.

Method: The authors propose DPMixSGD, a novel algorithm leveraging differential privacy and building on the STORM-based algorithm, ensuring minimal impact on convergence through added noise.

Result: Theoretical guarantees and empirical experiments demonstrate that DPMixSGD effectively preserves privacy while maintaining convergence performance across diverse tasks and models.

Conclusion: DPMixSGD successfully addresses privacy risks in decentralized min-max optimization while maintaining strong performance, offering a practical solution for sensitive multi-agent systems.

Abstract: Decentralized min-max optimization allows multi-agent systems to
collaboratively solve global min-max optimization problems by facilitating the
exchange of model updates among neighboring agents, eliminating the need for a
central server. However, sharing model updates in such systems carry a risk of
exposing sensitive data to inference attacks, raising significant privacy
concerns. To mitigate these privacy risks, differential privacy (DP) has become
a widely adopted technique for safeguarding individual data. Despite its
advantages, implementing DP in decentralized min-max optimization poses
challenges, as the added noise can hinder convergence, particularly in
non-convex scenarios with complex agent interactions in min-max optimization
problems. In this work, we propose an algorithm called DPMixSGD (Differential
Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving
algorithm specifically designed for non-convex decentralized min-max
optimization. Our method builds on the state-of-the-art STORM-based algorithm,
one of the fastest decentralized min-max solutions. We rigorously prove that
the noise added to local gradients does not significantly compromise
convergence performance, and we provide theoretical bounds to ensure privacy
guarantees. To validate our theoretical findings, we conduct extensive
experiments across various tasks and models, demonstrating the effectiveness of
our approach.

</details>


### [440] [Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information](https://arxiv.org/abs/2508.07713)
*Jinghan Yang,Jiayu Weng*

Main category: cs.LG

TL;DR: This paper introduces a mutual information-based approach to filter noisy samples in datasets affected by both label and input noise, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Corrupted labels and input noise can severely hamper deep neural networks' performance, highlighting the need for effective methods to filter noisy data.

Method: The method quantifies statistical dependencies between inputs and labels using mutual information, assessing each sample's pointwise contribution to identify noisy or mislabeled instances.

Result: Experiments on MNIST show that training only on high mutual information samples boosts accuracy by up to 15% compared to random sampling, while preserving valid data and filtering corrupted ones.

Conclusion: The proposed framework offers a robust and effective data selection method for dealing with hybrid noise scenarios, improving classification performance.

Abstract: Deep neural networks can memorize corrupted labels, making data quality
critical for model performance, yet real-world datasets are frequently
compromised by both label noise and input noise. This paper proposes a mutual
information-based framework for data selection under hybrid noise scenarios
that quantifies statistical dependencies between inputs and labels. We compute
each sample's pointwise contribution to the overall mutual information and find
that lower contributions indicate noisy or mislabeled instances. Empirical
validation on MNIST with different synthetic noise settings demonstrates that
the method effectively filters low-quality samples. Under label corruption,
training on high-MI samples improves classification accuracy by up to 15\%
compared to random sampling. Furthermore, the method exhibits robustness to
benign input modifications, preserving semantically valid data while filtering
truly corrupted samples.

</details>


### [441] [Analysis of Schedule-Free Nonconvex Optimization](https://arxiv.org/abs/2508.06743)
*Connor Brown*

Main category: cs.LG

TL;DR: The paper improves the understanding of the Schedule-Free (SF) method for nonconvex optimization with robust analysis and new rate bounds.


<details>
  <summary>Details</summary>
Motivation: Classical first-order methods require prior knowledge of total horizon $T$ for step-size scheduling, which is impractical. This paper seeks to extend the performance guarantees of the Schedule-Free (SF) method in nonconvex settings without relying on strong global assumptions.

Method: The authors propose a robust Lyapunov framework to simplify the analysis of the SF method in nonconvex optimization. They derive descent inequalities under $L$-smoothness and lower-boundedness without dependence on $T$.

Result: The paper achieves horizon-agnostic bounds: $O(1/\log T)$ with constant step sizes, $O(\log T/T)$ with linearly growing steps, and $O(T^{-(1-\alpha)})$ for polynomial averaging. Numerical PEP experiments validate these rates and suggest further optimization potential.

Conclusion: The work brings new insights into SF's applicability to smooth nonconvex optimization, providing theoretical and experimental validation. It paves the way for refined nonconvex optimization techniques with optimal rates.

Abstract: First-order methods underpin most large-scale learning algorithms, yet their
classical convergence guarantees hinge on carefully scheduled step-sizes that
depend on the total horizon $T$, which is rarely known in advance. The
Schedule-Free (SF) method promises optimal performance with hyperparameters
that are independent of $T$ by interpolating between Polyak--Ruppert averaging
and momentum, but nonconvex analysis of SF has been limited or reliant on
strong global assumptions. We introduce a robust Lyapunov framework that, under
only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step
descent inequality. This yields horizon-agnostic bounds in the nonconvex
setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a
linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for
polynomial averaging. We complement these proofs with Performance Estimation
Problem (PEP) experiments that numerically validate our rates and suggest that
our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to
$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex
optimization and charts future directions for optimal nonconvex rates.

</details>


### [442] [Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks](https://arxiv.org/abs/2508.07676)
*Chenchen Lin,Xuehe Wang*

Main category: cs.LG

TL;DR: Federated learning with embedded privacy concerns from social connections introduces privacy challenges. The paper proposes a novel socially-aware mechanism using game theory to enhance client privacy and utility.


<details>
  <summary>Details</summary>
Motivation: To address privacy externalities in federated learning caused by social connections that impact not only a client's privacy strategy but also other connected clients.

Method: The authors model the server-client interaction using a two-stage Stackelberg game and introduce a mean-field estimator to quantify privacy risks in federated learning.

Result: The proposed mechanism improves utility for clients, reduces server costs, and outperforms existing methods in privacy preservation and model performance.

Conclusion: The socially-aware federated learning mechanism is effective in enhancing privacy, achieving near-optimal social welfare, and balancing client incentives against server objectives.

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, thereby enhancing privacy and
facilitating collaboration among clients connected via social networks.
However, these social connections introduce privacy externalities: a client's
privacy loss depends not only on its privacy protection strategy but also on
the privacy decisions of others, propagated through the network via multi-hop
interactions. In this work, we propose a socially-aware privacy-preserving FL
mechanism that systematically quantifies indirect privacy leakage through a
multi-hop propagation model. We formulate the server-client interaction as a
two-stage Stackelberg game, where the server, as the leader, optimizes
incentive policies, and clients, as followers, strategically select their
privacy budgets, which determine their privacy-preserving levels by controlling
the magnitude of added noise. To mitigate information asymmetry in networked
privacy estimation, we introduce a mean-field estimator to approximate the
average external privacy risk. We theoretically prove the existence and
convergence of the fixed point of the mean-field estimator and derive
closed-form expressions for the Stackelberg Nash Equilibrium. Despite being
designed from a client-centric incentive perspective, our mechanism achieves
approximately-optimal social welfare, as revealed by Price of Anarchy (PoA)
analysis. Experiments on diverse datasets demonstrate that our approach
significantly improves client utilities and reduces server costs while
maintaining model performance, outperforming both Social-Agnostic (SA)
baselines and methods that account for social externalities.

</details>


### [443] [A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory](https://arxiv.org/abs/2508.07746)
*Fengdi Che*

Main category: cs.LG

TL;DR: The paper surveys theoretical insights into offline reinforcement learning (RL), bridging challenges and conditions necessary for algorithm success.


<details>
  <summary>Details</summary>
Motivation: To enhance the practical design of offline RL algorithms by leveraging theoretical knowledge and understanding inherent challenges.

Method: The paper presents a detailed exploration of theoretical conditions like function representation and data coverage, counterexamples to offline RL solvability, and sufficient conditions for success.

Result: Insights into the limitations of offline RL algorithms are highlighted, along with the sufficiency conditions necessary for their effectiveness.

Conclusion: By understanding theoretical progress alongside practical barriers, the work motivates novel solutions to achieve better offline RL outcomes.

Abstract: Offline reinforcement learning (RL) aims to optimize the return given a fixed
dataset of agent trajectories without additional interactions with the
environment. While algorithm development has progressed rapidly, significant
theoretical advances have also been made in understanding the fundamental
challenges of offline RL. However, bridging these theoretical insights with
practical algorithm design remains an ongoing challenge. In this survey, we
explore key intuitions derived from theoretical work and their implications for
offline RL algorithms.
  We begin by listing the conditions needed for the proofs, including function
representation and data coverage assumptions. Function representation
conditions tell us what to expect for generalization, and data coverage
assumptions describe the quality requirement of the data. We then examine
counterexamples, where offline RL is not solvable without an impractically
large amount of data. These cases highlight what cannot be achieved for all
algorithms and the inherent hardness of offline RL. Building on techniques to
mitigate these challenges, we discuss the conditions that are sufficient for
offline RL. These conditions are not merely assumptions for theoretical proofs,
but they also reveal the limitations of these algorithms and remind us to
search for novel solutions when the conditions cannot be satisfied.

</details>


### [444] [Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning](https://arxiv.org/abs/2508.06765)
*Xingke Yang,Liang Li,Sicong Li,Liwei Guan,Hao Wang,Xiaoqi Qi,Jiang Liu,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: The paper introduces Fed MobiLLM, a lightweight federated fine-tuning approach for large language models (LLMs) targeting heterogeneous mobile devices. It drastically reduces computational and communication costs while accelerating convergence, making LLM adaptation feasible for mobile scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges in federated fine-tuning of large language models for personalized intelligence on mobile devices, such as heavy computational and memory demands and synchronization bottlenecks for devices with varying capabilities.

Method: Fed MobiLLM employs a server-assisted federated side-tuning paradigm where mobile devices perform lightweight computations using frozen backbone LLMs and upload selected activations. The server trains a shared side-network asynchronously, complemented by adaptive layer-wise feature alignment to ensure model consistency across devices.

Result: Fed MobiLLM achieves significant efficiency improvements with at least a 95.2% reduction in computation overhead, 93.2% reduction in communication costs, and 5.1x faster convergence compared to traditional methods, alongside robust fine-tuning performance.

Conclusion: The study concludes that Fed MobiLLM is highly effective for federated fine-tuning of LLMs on heterogeneous mobile devices, offering a scalable and practical solution to enable personalized intelligence with minimal resource consumption.

Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated LLM FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across
mobile devices with diverse computing/communication speeds and local model
architectures. In particular, Fed MobiLLM implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone LLMs, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in communication costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
LLM adaptation over heterogeneous mobile devices.

</details>


### [445] [Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)
*Tong Yang,Yu Huang,Yingbin Liang,Yuejie Chi*

Main category: cs.LG

TL;DR: This paper investigates how one-layer transformers learn to perform symbolic multi-step reasoning tasks, specifically analyzing their ability to solve path-finding problems in trees through theoretical insights into training dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms by which transformers acquire multi-step reasoning abilities during training, particularly in symbolic reasoning tasks, which lack sufficient theoretical explanations.

Method: The authors focus on two tasks: a backward reasoning task (goal-to-root pathfinding) and a forward reasoning task (root-to-goal pathfinding done in two stages). They analyze these tasks through a theoretical lens of gradient descent dynamics, studying how one-layer transformers utilize attention mechanisms to solve them.

Result: Theoretical analysis demonstrates that trained one-layer transformers can provably solve both tasks, generalizing to unseen trees. The study reveals that attention heads independently specialize and coordinate during multi-phase training dynamics.

Conclusion: This research provides a mechanistic explanation for how transformers perform sequential algorithms. It highlights the potential of shallow multi-head transformers to solve structured reasoning problems when tasks include intermediate chain-of-thought steps, reducing reliance on deeper architectures.

Abstract: Transformers have demonstrated remarkable capabilities in multi-step
reasoning tasks. However, understandings of the underlying mechanisms by which
they acquire these abilities through training remain limited, particularly from
a theoretical standpoint. This work investigates how transformers learn to
solve symbolic multi-step reasoning problems through chain-of-thought
processes, focusing on path-finding in trees. We analyze two intertwined tasks:
a backward reasoning task, where the model outputs a path from a goal node to
the root, and a more complex forward reasoning task, where the model implements
two-stage reasoning by first identifying the goal-to-root path and then
reversing it to produce the root-to-goal path. Our theoretical analysis,
grounded in the dynamics of gradient descent, shows that trained one-layer
transformers can provably solve both tasks with generalization guarantees to
unseen trees. In particular, our multi-phase training dynamics for forward
reasoning elucidate how different attention heads learn to specialize and
coordinate autonomously to solve the two subtasks in a single autoregressive
path. These results provide a mechanistic explanation of how trained
transformers can implement sequential algorithmic procedures. Moreover, they
offer insights into the emergence of reasoning abilities, suggesting that when
tasks are structured to take intermediate chain-of-thought steps, even shallow
multi-head transformers can effectively solve problems that would otherwise
require deeper architectures.

</details>


### [446] [PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)
*Noel Teku,Fengwei Tian,Payel Bhattacharjee,Souradip Chakraborty,Amrit Singh Bedi,Ravi Tandon*

Main category: cs.LG

TL;DR: This paper addresses privacy concerns in aligning Large Language Models (LLMs) by proposing PROPS, a new framework that preserves human preference privacy while improving model utility.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address privacy concerns arising from the use of human feedback in aligning LLMs, as such feedback may reveal personal values, beliefs, and personality traits of the labelers.

Method: The proposed method, PROPS, is a multi-stage framework that progressively maintains privacy during alignment by using previously trained private models as supplemental labelers for new training. It also provides theoretical guarantees and is validated on multiple models and datasets.

Result: PROPS outperforms existing methods, achieving up to 3 times higher win-rates than DP-SGD and 2.5 times higher win-rates than Randomized Response at the same privacy budget.

Conclusion: PROPS is an effective framework that balances privacy preservation and model utility during LLM alignment, offering significant improvements over existing methods.

Abstract: Alignment is a key step in developing Large Language Models (LLMs) using
human feedback to ensure adherence to human values and societal norms.
Dependence on human feedback raises privacy concerns about how much a labeler's
preferences may reveal about their personal values, beliefs, and personality
traits. Existing approaches, such as Differentially Private SGD (DP-SGD),
provide rigorous privacy guarantees by privatizing gradients during fine-tuning
and alignment but can provide more privacy than necessary as human preferences
are tied only to labels of (prompt, response) pairs and can degrade model
utility. This work focuses on LLM alignment with preference-level privacy,
which preserves the privacy of preference labels provided by humans. We propose
PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving
alignment framework where privately aligned models in previous stages can serve
as labelers for supplementing training data in the subsequent stages of
alignment. We present theoretical guarantees for PROPS as well as comprehensive
validation using multiple models (Pythia and GPT) and datasets (AlpacaEval,
Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over
existing methods while still providing high privacy. For the same privacy
budget, alignment via PROPS can achieve up to 3x higher win-rates compared to
DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based
alignment.

</details>


### [447] [Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning](https://arxiv.org/abs/2508.06784)
*Junjing Zheng,Chengliang Song,Weidong Jiang,Xinyu Zhang*

Main category: cs.LG

TL;DR: The paper presents MA-NTAE, a novel autoencoder designed for high-order tensor data that combines tensor decomposition with non-linear encoding for improved compression and clustering.


<details>
  <summary>Details</summary>
Motivation: Current methods like classical autoencoders face computational inefficiencies due to high dimensionality, while tensor networks lack the ability to learn complex non-linear relationships.

Method: The MA-NTAE employs a non-linear Tucker decomposition framework, using a Pick-and-Unfold strategy for flexible, per-mode encoding of tensors.

Result: MA-NTAE achieves superior performance in compression and clustering tasks compared to existing methods, particularly in high-order tensor scenarios.

Conclusion: MA-NTAE successfully integrates tensor structural priors non-linearly, offering scalable and effective solutions for high-dimensional data challenges.

Abstract: High-dimensional data, particularly in the form of high-order tensors,
presents a major challenge in self-supervised learning. While MLP-based
autoencoders (AE) are commonly employed, their dependence on flattening
operations exacerbates the curse of dimensionality, leading to excessively
large model sizes, high computational overhead, and challenging optimization
for deep structural feature capture. Although existing tensor networks
alleviate computational burdens through tensor decomposition techniques, most
exhibit limited capability in learning non-linear relationships. To overcome
these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder
(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear
framework and employs a Pick-and-Unfold strategy, facilitating flexible
per-mode encoding of high-order tensors via recursive unfold-encode-fold
operations, effectively integrating tensor structural priors. Notably, MA-NTAE
exhibits linear growth in computational complexity with tensor order and
proportional growth with mode dimensions. Extensive experiments demonstrate
MA-NTAE's performance advantages over standard AE and current tensor networks
in compression and clustering tasks, which become increasingly pronounced for
higher-order, higher-dimensional tensors.

</details>


### [448] [Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities](https://arxiv.org/abs/2508.06800)
*Rui Liu,Haolin Zuo,Zheng Lian,Hongyu Yuan,Qi Fan*

Main category: cs.LG

TL;DR: The paper introduces HARDY-MER, a framework for multimodal emotion recognition under missing modalities. It focuses on adapting to sample hardness during training to improve the handling of challenging instances.


<details>
  <summary>Details</summary>
Motivation: Traditional MER methods fall short in addressing the varying difficulty levels of reconstructing missing modalities during training. This limitation impacts model efficacy, particularly on hard samples.

Method: The proposed HARDY-MER framework uses a Multi-view Hardness Evaluation mechanism to assess reconstruction difficulty based on Direct and Indirect Hardness. It also employs a Retrieval-based Dynamic Curriculum Learning strategy to refine training focus, balancing between easy and hard samples.

Result: Experimental results show that HARDY-MER consistently achieves superior performance compared to existing methods on benchmark datasets.

Conclusion: HARDY-MER enhances model robustness by incorporating hardness-aware strategies, demonstrating significant improvement in MER tasks with missing modalities.

Abstract: Missing modalities have recently emerged as a critical research direction in
multimodal emotion recognition (MER). Conventional approaches typically address
this issue through missing modality reconstruction. However, these methods fail
to account for variations in reconstruction difficulty across different
samples, consequently limiting the model's ability to handle hard samples
effectively. To overcome this limitation, we propose a novel Hardness-Aware
Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates
in two key stages: first, it estimates the hardness level of each sample, and
second, it strategically emphasizes hard samples during training to enhance
model performance on these challenging instances. Specifically, we first
introduce a Multi-view Hardness Evaluation mechanism that quantifies
reconstruction difficulty by considering both Direct Hardness (modality
reconstruction errors) and Indirect Hardness (cross-modal mutual information).
Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy
that dynamically adjusts the training curriculum by retrieving samples with
similar semantic information and balancing the learning focus between easy and
hard instances. Extensive experiments on benchmark datasets demonstrate that
HARDY-MER consistently outperforms existing methods in missing-modality
scenarios. Our code will be made publicly available at
https://github.com/HARDY-MER/HARDY-MER.

</details>


### [449] [Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation](https://arxiv.org/abs/2508.06806)
*Xiao Huang,Xu Liu,Enze Zhang,Tong Yu,Shuai Li*

Main category: cs.LG

TL;DR: The paper introduces CFDG, a method for enhancing data generation quality in Offline-to-online Reinforcement Learning (O2O RL), achieving significant performance gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the performance gap caused by mismatches between generated and online data distributions in O2O RL fine-tuning.

Method: The proposed CFDG uses classifier-free guidance diffusion and reweighting to improve alignment and generation quality of different data distributions.

Result: CFDG achieves a 15% average performance improvement on popular O2O RL algorithms (e.g., IQL, PEX, APL) in benchmarks like MuJoCo and AntMaze.

Conclusion: CFDG enhances data augmentation for O2O RL, improving algorithm performance and stability without additional classifier training.

Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online
fine-tuning on an offline pre-trained policy to minimize costly online
interactions. Existing work used offline datasets to generate data that conform
to the online data distribution for data augmentation. However, generated data
still exhibits a gap with the online data, limiting overall performance. To
address this, we propose a new data augmentation approach, Classifier-Free
Diffusion Generation (CFDG). Without introducing additional classifier training
overhead, CFDG leverages classifier-free guidance diffusion to significantly
enhance the generation quality of offline and online data with different
distributions. Additionally, it employs a reweighting method to enable more
generated data to align with the online data, enhancing performance while
maintaining the agent's stability. Experimental results show that CFDG
outperforms replaying the two data types or using a standard diffusion model to
generate new data. Our method is versatile and can be integrated with existing
offline-to-online RL algorithms. By implementing CFDG to popular methods IQL,
PEX and APL, we achieve a notable 15% average improvement in empirical
performance on the D4RL benchmark such as MuJoCo and AntMaze.

</details>


### [450] [Technical Report: Full-Stack Fine-Tuning for the Q Programming Language](https://arxiv.org/abs/2508.06813)
*Brendan R. Hogan,Will Brown,Adel Boyarsky,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: The paper focuses on adapting large language models (LLMs) to the niche Q programming language in quantitative finance by creating an evaluation dataset, training models of various sizes, and significantly outperforming existing models like GPT-4.1 and Claude Opus-4.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of using LLMs in specialized fields with scarce online data, particularly in the Q programming language used in quantitative finance.

Method: The authors create a custom evaluation dataset, benchmark existing models, and train reasoning and non-reasoning models using pretraining, supervised fine-tuning, and reinforcement learning.

Result: Their best model achieved a pass@1 accuracy of 59% on the Q benchmark, surpassing Claude Opus-4 by 29.5% and outperforming GPT-4.1 across all model sizes.

Conclusion: The approach provides an effective blueprint for adapting LLMs to specialized domains and niche programming languages, with methods that can generalize to other tasks with subjective or soft evaluation metrics.

Abstract: Even though large language models are becoming increasingly capable, it is
still unreasonable to expect them to excel at tasks that are under-represented
on the Internet. Leveraging LLMs for specialized applications, particularly in
niche programming languages and private domains, remains challenging and
largely unsolved. In this work, we address this gap by presenting a
comprehensive, open-source approach for adapting LLMs to the Q programming
language, a popular tool in quantitative finance that is much less present on
the Internet compared to Python, C, Java, and other ``mainstream" languages and
is therefore not a strong suit of general-purpose AI models. We introduce a new
Leetcode style evaluation dataset for Q, benchmark major frontier models on the
dataset, then do pretraining, supervised fine tuning, and reinforcement
learning to train a suite of reasoning and non-reasoning models based on the
Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our
best model achieves a pass@1 accuracy of 59 percent on our Q benchmark,
surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.
Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.
In addition to releasing models, code, and data, we provide a detailed
blueprint for dataset construction, model pretraining, supervised fine-tuning,
and reinforcement learning. Our methodology is broadly applicable, and we
discuss how these techniques can be extended to other tasks, including those
where evaluation may rely on soft or subjective signals.

</details>


### [451] [Who's the Evil Twin? Differential Auditing for Undesired Behavior](https://arxiv.org/abs/2508.06827)
*Ishwar Balappanawar,Venkata Hasith Vattikuti,Greta Kintzley,Ronan Azimi-Mancel,Satvik Golechha*

Main category: cs.LG

TL;DR: This paper studies how to detect hidden harmful behaviors in neural networks by framing it as an adversarial game between two teams, finding that adversarial-attack-based methods work well, especially when hints are available.


<details>
  <summary>Details</summary>
Motivation: Understanding and auditing hidden behaviors in neural networks is crucial, as these behaviors can be adversarially obfuscated, posing safety and security risks.

Method: The authors propose an adversarial game setup where a 'red team' creates a compromised model with hidden harmful behavior and a benign model, while a 'blue team' attempts to detect the compromised model using various strategies.

Result: Adversarial-attack-based methods showed 100% accuracy in prediction when hints are provided, while other techniques varied in effectiveness. For large language models (LLMs), effective auditing required distribution-specific hints.

Conclusion: Providing specific guidance or hints enhances auditing methods' success, especially when dealing with complex models. The findings aim to facilitate the development of robust neural network auditing tools.

Abstract: Detecting hidden behaviors in neural networks poses a significant challenge
due to minimal prior knowledge and potential adversarial obfuscation. We
explore this problem by framing detection as an adversarial game between two
teams: the red team trains two similar models, one trained solely on benign
data and the other trained on data containing hidden harmful behavior, with the
performance of both being nearly indistinguishable on the benign dataset. The
blue team, with limited to no information about the harmful behaviour, tries to
identify the compromised model. We experiment using CNNs and try various blue
team strategies, including Gaussian noise analysis, model diffing, integrated
gradients, and adversarial attacks under different levels of hints provided by
the red team. Results show high accuracy for adversarial-attack-based methods
(100\% correct prediction, using hints), which is very promising, whilst the
other techniques yield more varied performance. During our LLM-focused rounds,
we find that there are not many parallel methods that we could apply from our
study with CNNs. Instead, we find that effective LLM auditing methods require
some hints about the undesired distribution, which can then used in standard
black-box and open-weight methods to probe the models further and reveal their
misalignment. We open-source our auditing games (with the model and data) and
hope that our findings contribute to designing better audits.

</details>


### [452] [Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.06871)
*Aleksandar Todorov,Juan Cardenas-Cartagena,Rafael F. Cunha,Marco Zullich,Matthia Sabatelli*

Main category: cs.LG

TL;DR: The paper studies plasticity loss in multi-task reinforcement learning (MTRL) and shows that sparsification methods like Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET) counter plasticity degradation, leading to improved multi-task performance.


<details>
  <summary>Details</summary>
Motivation: To address plasticity loss in MTRL, a problem where the ability of the model to adapt diminishes as training progresses, especially in handling diverse and conflicting tasks.

Method: The authors evaluate sparsification methods (GMP, SET) across various MTRL architectures and benchmarks, comparing against dense models and other regularization approaches.

Result: The sparsification methods mitigated plasticity degradation indicators like neuron dormancy and representational collapse, boosting multi-task performance. Sparse agents often outperformed dense models and were competitive with other plasticity-inducing methods.

Conclusion: Sparsification is a promising strategy to enhance plasticity in MTRL systems, offering adaptability but requiring careful context-sensitive application.

Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a
critical challenge in deep reinforcement learning. We examine this issue in
multi-task reinforcement learning (MTRL), where higher representational
flexibility is crucial for managing diverse and potentially conflicting task
demands. We systematically explore how sparsification methods, particularly
Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance
plasticity and consequently improve performance in MTRL agents. We evaluate
these approaches across distinct MTRL architectures (shared backbone, Mixture
of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,
comparing against dense baselines, and a comprehensive range of alternative
plasticity-inducing or regularization methods. Our results demonstrate that
both GMP and SET effectively mitigate key indicators of plasticity degradation,
such as neuron dormancy and representational collapse. These plasticity
improvements often correlate with enhanced multi-task performance, with sparse
agents frequently outperforming dense counterparts and achieving competitive
results against explicit plasticity interventions. Our findings offer insights
into the interplay between plasticity, network sparsity, and MTRL designs,
highlighting dynamic sparsification as a robust but context-sensitive tool for
developing more adaptable MTRL systems.

</details>


### [453] [Conformal Prediction and Trustworthy AI](https://arxiv.org/abs/2508.06885)
*Anthony Bellotti,Xindi Zhao*

Main category: cs.LG

TL;DR: Conformal predictors, a machine learning methodology, provide reliable predictions with guaranteed confidence levels, aiding in trustworthy AI development. This review explores their applications beyond validity, including bias mitigation and governance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the potential of conformal predictors in contributing to trustworthy AI, an increasingly important topic in machine learning and society.

Method: The paper conducts a review of conformal predictors, evaluates their role in addressing issues like generalization risk and AI governance, and provides examples and experiments to validate their utility.

Result: The experiments and examples demonstrate that conformal predictors not only offer well-calibrated predictions but also assist in identifying and mitigating bias, enhancing their contributions to trustworthy AI.

Conclusion: Conformal prediction is a promising tool for advancing trustworthy AI, contributing to areas like uncertainty quantification, bias mitigation, and AI governance.

Abstract: Conformal predictors are machine learning algorithms developed in the 1990's
by Gammerman, Vovk, and their research team, to provide set predictions with
guaranteed confidence level. Over recent years, they have grown in popularity
and have become a mainstream methodology for uncertainty quantification in the
machine learning community. From its beginning, there was an understanding that
they enable reliable machine learning with well-calibrated uncertainty
quantification. This makes them extremely beneficial for developing trustworthy
AI, a topic that has also risen in interest over the past few years, in both
the AI community and society more widely. In this article, we review the
potential for conformal prediction to contribute to trustworthy AI beyond its
marginal validity property, addressing problems such as generalization risk and
AI governance. Experiments and examples are also provided to demonstrate its
use as a well-calibrated predictor and for bias identification and mitigation.

</details>


### [454] [QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting](https://arxiv.org/abs/2508.06915)
*Shichao Ma,Zhengyang Zhou,Qihe Huang,Binwu Wang,Kuo Yang,Huan Li,Yang Wang*

Main category: cs.LG

TL;DR: The paper introduces QuiZSF, a zero-shot time series forecasting framework that integrates retrieval-augmented generation methods with time-series pre-trained models, achieving high prediction accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Zero-shot time-series forecasting is vital for scenarios with limited data but remains challenging for traditional models. Time-series pre-trained models show potential for ZSF, but their adaptability with external knowledge is limited. This paper aims to bridge that gap by integrating retrieval-augmented generation techniques.

Method: The framework, QuiZSF, employs a hierarchical tree-structured ChronoRAG Base for efficient retrieval and storage, a Multi-grained Series Interaction Learner for relational feature extraction, and a dual-branch Model Cooperation Coherer for aligning retrieved knowledge with various pre-trained models.

Result: QuiZSF demonstrated superior performance over baselines, ranking first in 75% and 87.5% of prediction settings for Non-LLM and LLM-based models, respectively. It also maintains high efficiency in memory usage and inference time.

Conclusion: QuiZSF effectively combines retrieval-augmented generation with pre-trained time-series models, offering a modular and efficient solution for zero-shot forecasting with state-of-the-art performance.

Abstract: Time series forecasting has become increasingly important to empower diverse
applications with streaming data. Zero-shot time-series forecasting (ZSF),
particularly valuable in data-scarce scenarios, such as domain transfer or
forecasting under extreme conditions, is difficult for traditional models to
deal with. While time series pre-trained models (TSPMs) have demonstrated
strong performance in ZSF, they often lack mechanisms to dynamically
incorporate external knowledge. Fortunately, emerging retrieval-augmented
generation (RAG) offers a promising path for injecting such knowledge on
demand, yet they are rarely integrated with TSPMs. To leverage the strengths of
both worlds, we introduce RAG into TSPMs to enhance zero-shot time series
forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series
Forecaster), a lightweight and modular framework that couples efficient
retrieval with representation learning and model adaptation for ZSF.
Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)
for scalable time-series storage and domain-aware retrieval, introduce a
Multi-grained Series Interaction Learner (MSIL) to extract fine- and
coarse-grained relational features, and develop a dual-branch Model Cooperation
Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM
based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM
based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and
87.5% of prediction settings, while maintaining high efficiency in memory and
inference time.

</details>


### [455] [Class Unbiasing for Generalization in Medical Diagnosis](https://arxiv.org/abs/2508.06943)
*Lishi Zuo,Man-Wai Mak,Lu Yi,Youzhi Tu*

Main category: cs.LG

TL;DR: This paper identifies the issue of class-feature bias in medical diagnosis models and proposes a method to train unbiased models that address this bias and class imbalance simultaneously.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce performance bias in medical diagnosis models caused by reliance on features strongly correlated with only a subset of classes, aiming to improve generalization across all classes.

Method: The method involves proposing a class-wise inequality loss that ensures equal contributions from positive and negative classes, alongside a class-wise group distributionally robust optimization objective that upweights underperforming classes.

Result: Empirical analysis on synthetic and real-world datasets confirms that class-feature bias can harm model performance, while the proposed approach mitigates bias and imbalance to enhance generalization.

Conclusion: The proposed method successfully addresses class-feature bias and class imbalance, leading to improved diagnostic model robustness and generalizability.

Abstract: Medical diagnosis might fail due to bias. In this work, we identified
class-feature bias, which refers to models' potential reliance on features that
are strongly correlated with only a subset of classes, leading to biased
performance and poor generalization on other classes. We aim to train a
class-unbiased model (Cls-unbias) that mitigates both class imbalance and
class-feature bias simultaneously. Specifically, we propose a class-wise
inequality loss which promotes equal contributions of classification loss from
positive-class and negative-class samples. We propose to optimize a class-wise
group distributionally robust optimization objective-a class-weighted training
objective that upweights underperforming classes-to enhance the effectiveness
of the inequality loss under class imbalance. Through synthetic and real-world
datasets, we empirically demonstrate that class-feature bias can negatively
impact model performance. Our proposed method effectively mitigates both
class-feature bias and class imbalance, thereby improving the model's
generalization ability.

</details>


### [456] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: This paper presents Adaptive Meta Fine-Tuning (AMFT), a novel single-stage training algorithm for large language models that optimally balances supervised fine-tuning (SFT) and reinforcement learning (RL) using a meta-learning strategy.


<details>
  <summary>Details</summary>
Motivation: Current two-stage training pipelines for large language models experience issues like catastrophic forgetting and suboptimal balance between imitation learning (SFT) and exploration (RL), which calls for a more unified and dynamic training approach.

Method: AMFT introduces a meta-gradient-based adaptive weight controller to dynamically adjust the balance between SFT's path-based reward and RL's outcome-based reward, incorporating policy entropy regularization for increased stability.

Result: AMFT achieves state-of-the-art performance on benchmarks in mathematical reasoning, abstract visual reasoning, and vision-language navigation tasks. Additionally, it shows superior generalization capabilities on out-of-distribution tasks.

Conclusion: AMFT offers a more effective, stable, and principled framework for fine-tuning large language models, addressing the limitations of traditional two-stage training pipelines through a unified, adaptive curriculum approach.

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [457] [BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity](https://arxiv.org/abs/2508.06953)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: This paper introduces a novel method, BoRA, to improve the rank of LoRA weights in fine-tuning large language models without requiring a significant increase in trainable parameters, using block matrix multiplication and block-wise diagonal matrices.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the trade-off between increasing the rank of LoRA weights to improve fine-tuning performance and the accompanying rise in the number of trainable parameters.

Method: The paper proposes BoRA, which partitions the LoRA weight matrices into blocks and uses block-wise diagonal matrices to increase rank efficiently. This achieves a multiplication structure that increases rank without a large parameter cost.

Result: BoRA achieves higher rank improvements with only $b^2r$ additional parameters and demonstrates superior performance across diverse datasets and models as validated through extensive experimentation.

Conclusion: BoRA is an efficient and scalable method for improving LoRA's fine-tuning performance by enhancing weight rank with minimal additional parameters, as evidenced by both experimental results and ablation studies.

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). It approximates the update of a
pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two
low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and
$B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$
can raise the rank of LoRA weights (i.e., $BA$), which typically improves
fine-tuning performance but also significantly increases the number of
trainable parameters. In this paper, we propose Block Diversified Low-Rank
Adaptation (BoRA), which improves the rank of LoRA weights with a small number
of additional parameters. Specifically, BoRA treats the product $BA$ as a block
matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along
the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and
$B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the
concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the
diversity of different block products, BoRA introduces a unique diagonal matrix
$\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication,
resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal
matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only
requiring $b^2r$ additional parameters. Extensive experiments across multiple
datasets and models demonstrate the superiority of BoRA, and ablation studies
further validate its scalability.

</details>


### [458] [Can Multitask Learning Enhance Model Explainability?](https://arxiv.org/abs/2508.06966)
*Hiba Najjar,Bushra Alshbib,Andreas Dengel*

Main category: cs.LG

TL;DR: This paper proposes a multitask learning framework where certain satellite data modalities are used as auxiliary targets for improved interpretability, performance, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between performance and interpretability in complex multimodal remote sensing networks by leveraging modalities without additional inputs.

Method: Adopts a multitask learning approach where additional modalities are predicted as auxiliary tasks, keeping main modality inputs intact during deployment.

Result: Demonstrates comparable or superior performance compared to multimodal baselines, avoids the need for additional input modalities during deployment, and provides intrinsic explanations for main task errors.

Conclusion: Multitask learning can enhance interpretability, performance, and efficiency for satellite data applications without relying on complex multimodal networks.

Abstract: Remote sensing provides satellite data in diverse types and formats. The
usage of multimodal learning networks exploits this diversity to improve model
performance, except that the complexity of such networks comes at the expense
of their interpretability. In this study, we explore how modalities can be
leveraged through multitask learning to intrinsically explain model behavior.
In particular, instead of additional inputs, we use certain modalities as
additional targets to be predicted along with the main task. The success of
this approach relies on the rich information content of satellite data, which
remains as input modalities. We show how this modeling context provides
numerous benefits: (1) in case of data scarcity, the additional modalities do
not need to be collected for model inference at deployment, (2) the model
performance remains comparable to the multimodal baseline performance, and in
some cases achieves better scores, (3) prediction errors in the main task can
be explained via the model behavior in the auxiliary task(s). We demonstrate
the efficiency of our approach on three datasets, including segmentation,
classification, and regression tasks. Code available at
git.opendfki.de/hiba.najjar/mtl_explainability/.

</details>


### [459] [Structure-Preserving Digital Twins via Conditional Neural Whitney Forms](https://arxiv.org/abs/2508.06981)
*Brooks Kinch,Benjamin Shaffer,Elizabeth Armstrong,Michael Meehan,John Hewson,Nathaniel Trask*

Main category: cs.LG

TL;DR: The paper introduces a real-time digital twin framework using structure-preserving reduced finite element models and conditional attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of creating digital twins that maintain numerical well-posedness, exact conservation laws, and perform real-time calibration to sensor data and parametric variables.

Method: Utilizes conditional attention to learn reduced finite element bases and nonlinear conservation laws within finite element exterior calculus (FEEC), ensuring numerical accuracy and conservation. Interfaces non-invasively with conventional finite element tools.

Result: Demonstrates application across challenging problems (e.g., turbulence, thermal runaway) with sparse data, achieving a speedup of 3.1x10^8 relative to Large Eddy Simulations (LES) and real-time inference at ~0.1s.

Conclusion: The proposed framework is numerically robust, supports complex geometries, provides real-time predictions, and integrates seamlessly with existing finite element approaches. Furthermore, an open-source implementation is made available.

Abstract: We present a framework for constructing real-time digital twins based on
structure-preserving reduced finite element models conditioned on a latent
variable Z. The approach uses conditional attention mechanisms to learn both a
reduced finite element basis and a nonlinear conservation law within the
framework of finite element exterior calculus (FEEC). This guarantees numerical
well-posedness and exact preservation of conserved quantities, regardless of
data sparsity or optimization error. The conditioning mechanism supports
real-time calibration to parametric variables, allowing the construction of
digital twins which support closed loop inference and calibration to sensor
data. The framework interfaces with conventional finite element machinery in a
non-invasive manner, allowing treatment of complex geometries and integration
of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,
and a complex battery thermal runaway problem. The method achieves accurate
predictions on complex geometries with sparse data (25 LES simulations),
including capturing the transition to turbulence and achieving real-time
inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source
implementation is available on GitHub.

</details>


### [460] [Discovery Learning accelerates battery design evaluation](https://arxiv.org/abs/2508.06985)
*Jiawei Zhang,Yifei Zhang,Baozhao Yi,Yao Ren,Qi Jiao,Hanyu Bai,Weiran Jiang,Ziyou Song*

Main category: cs.LG

TL;DR: The paper introduces a method called Discovery Learning (DL) for efficient battery design validation, reducing prototyping needs using historical data and machine learning.


<details>
  <summary>Details</summary>
Motivation: Battery R&D faces bottlenecks in prototyping due to high time and energy costs, necessitating a quicker alternative for design validation.

Method: The paper integrates active learning, physics-guided learning, and zero-shot learning as part of Discovery Learning to assess battery designs without additional labeled data.

Result: Discovery Learning achieved a 7.2% test error, greatly reducing time (98%) and energy (95%) usage compared to traditional practices.

Conclusion: Discovery Learning accelerates scientific discovery and engineering innovation in battery technologies by reusing historical data and minimizing dependency on expensive prototyping.

Abstract: Fast and reliable validation of novel designs in complex physical systems
such as batteries is critical to accelerating technological innovation.
However, battery research and development remain bottlenecked by the
prohibitively high time and energy costs required to evaluate numerous new
design candidates, particularly in battery prototyping and life testing.
Despite recent progress in data-driven battery lifetime prediction, existing
methods require labeled data of target designs to improve accuracy and cannot
make reliable predictions until after prototyping, thus falling far short of
the efficiency needed to enable rapid feedback for battery design. Here, we
introduce Discovery Learning (DL), a scientific machine-learning paradigm that
integrates active learning, physics-guided learning, and zero-shot learning
into a human-like reasoning loop, drawing inspiration from learning theories in
educational psychology. DL can learn from historical battery designs and
actively reduce the need for prototyping, thus enabling rapid lifetime
evaluation for unobserved material-design combinations without requiring
additional data labeling. To test DL, we present 123 industrial-grade
large-format lithium-ion pouch cells, spanning eight material-design
combinations and diverse cycling protocols. Trained solely on public datasets
of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting
the average cycle life under unknown device variability. This results in
savings of 98% in time and 95% in energy compared to industrial practices. This
work highlights the potential of uncovering insights from historical designs to
inform and accelerate the development of next-generation battery technologies.
DL represents a key advance toward efficient data-driven modeling and helps
realize the promise of machine learning for accelerating scientific discovery
and engineering innovation.

</details>


### [461] [UniMove: A Unified Model for Multi-city Human Mobility Prediction](https://arxiv.org/abs/2508.06986)
*Chonghua Han,Yuan Yuan,Yukun Liu,Jingtao Ding,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: The paper introduces UniMove, a unified model for multi-city human mobility prediction, addressing challenges of randomness, non-uniform time intervals, and heterogeneous patterns in human mobility.


<details>
  <summary>Details</summary>
Motivation: Human mobility is complex and varies significantly across cities, making it challenging to achieve accurate predictions without training separate models for each city.

Method: UniMove uses a dual-tower architecture (location and trajectory towers) and MoE Transformer blocks to enable universal spatial encoding, sequential modeling, and adaptive handling of diverse mobility patterns.

Result: Extensive experimental results show UniMove outperforms existing models, improving mobility prediction accuracy by over 10.2% through joint training on multi-city datasets.

Conclusion: UniMove presents a significant step forward in creating a unified model for human mobility prediction and lays the foundation for further advancements in this field.

Abstract: Human mobility prediction is vital for urban planning, transportation
optimization, and personalized services. However, the inherent randomness,
non-uniform time intervals, and complex patterns of human mobility, compounded
by the heterogeneity introduced by varying city structures, infrastructure, and
population densities, present significant challenges in modeling. Existing
solutions often require training separate models for each city due to distinct
spatial representations and geographic coverage. In this paper, we propose
UniMove, a unified model for multi-city human mobility prediction, addressing
two challenges: (1) constructing universal spatial representations for
effective token sharing across cities, and (2) modeling heterogeneous mobility
patterns from varying city characteristics. We propose a trajectory-location
dual-tower architecture, with a location tower for universal spatial encoding
and a trajectory tower for sequential mobility modeling. We also design MoE
Transformer blocks to adaptively select experts to handle diverse movement
patterns. Extensive experiments across multiple datasets from diverse cities
demonstrate that UniMove truly embodies the essence of a unified model. By
enabling joint training on multi-city data with mutual data enhancement, it
significantly improves mobility prediction accuracy by over 10.2\%. UniMove
represents a key advancement toward realizing a true foundational model with a
unified architecture for human mobility. We release the implementation at
https://github.com/tsinghua-fib-lab/UniMove/.

</details>


### [462] [A Comparative Study of Feature Selection in Tsetlin Machines](https://arxiv.org/abs/2508.06991)
*Vojtech Halenka,Ole-Christoffer Granmo,Lei Jiao,Per-Arne Andersen*

Main category: cs.LG

TL;DR: The paper explores feature selection (FS) techniques for Tsetlin machines (TM) to improve interpretability, complexity, and accuracy, proposing novel TM-specific methods and comparing them to existing techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of established tools for estimating feature importance in TMs, despite their interpretability-oriented design.

Method: The study evaluates classical FS methods, post-hoc techniques (like SHAP and LIME), and novel embedded scorers based on TM clause weights and states, using protocols like ROAR and ROAD.

Result: TM-internal scorers perform competitively and leverage clause-based interpretability to identify interacting features. TM-specific scorers provide similar accuracy retention with reduced computational costs.

Conclusion: This paper establishes baseline methods for FS in TMs and facilitates the development of specialized interpretability techniques for these models.

Abstract: Feature Selection (FS) is crucial for improving model interpretability,
reducing complexity, and sometimes for enhancing accuracy. The recently
introduced Tsetlin machine (TM) offers interpretable clause-based learning, but
lacks established tools for estimating feature importance. In this paper, we
adapt and evaluate a range of FS techniques for TMs, including classical filter
and embedded methods as well as post-hoc explanation methods originally
developed for neural networks (e.g., SHAP and LIME) and a novel family of
embedded scorers derived from TM clause weights and Tsetlin automaton (TA)
states. We benchmark all methods across 12 datasets, using evaluation
protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias
(ROAD), to assess causal impact. Our results show that TM-internal scorers not
only perform competitively but also exploit the interpretability of clauses to
reveal interacting feature patterns. Simpler TM-specific scorers achieve
similar accuracy retention at a fraction of the computational cost. This study
establishes the first comprehensive baseline for FS in TM and paves the way for
developing specialized TM-specific interpretability techniques.

</details>


### [463] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: The paper explores improving decision support systems by selecting optimal subsets of human experts using a greedy algorithm, achieving better classification with conformal prediction sets.


<details>
  <summary>Details</summary>
Motivation: Enhancing human-AI collaboration in multi-expert classification tasks to improve predictive accuracy using conformal prediction sets.

Method: A greedy algorithm is proposed for subset selection of instance-specific human experts, leveraging conformal prediction sets.

Result: The greedy algorithm achieves near-optimal subsets of experts, improving classification performance in simulations on CIFAR-10H and ImageNet-16H dataset predictions.

Conclusion: Selecting optimal subsets of experts using conformal prediction sets optimizes human-AI decision-making processes in classification tasks, yielding enhanced accuracy.

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


### [464] [TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations](https://arxiv.org/abs/2508.07016)
*Jianfei Wu,Wenmian Yang,Bingning Liu,Weijia Jia*

Main category: cs.LG

TL;DR: Time series forecasting is improved by considering time-lagged cross-correlations using the TLCCSP framework, which reduces forecasting errors significantly across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance time series forecasting accuracy by addressing the overlooked time-lagged cross-correlations in predictive models.

Method: Utilizes the SSDTW algorithm for capturing lagged correlations and employs a contrastive learning-based encoder for efficient computation and approximation.

Result: SSDTW and CLE show significant MSE reduction across weather, finance, and real estate datasets, with CLE also reducing computational time by 99%.

Conclusion: The TLCCSP framework effectively integrates time-lagged cross-correlations, improves accuracy, and ensures computational efficiency, making it suitable for real-time applications.

Abstract: Time series forecasting is critical across various domains, such as weather,
finance and real estate forecasting, as accurate forecasts support informed
decision-making and risk mitigation. While recent deep learning models have
improved predictive capabilities, they often overlook time-lagged
cross-correlations between related sequences, which are crucial for capturing
complex temporal relationships. To address this, we propose the Time-Lagged
Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances
forecasting accuracy by effectively integrating time-lagged cross-correlated
sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)
algorithm to capture lagged correlations and a contrastive learning-based
encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets
demonstrate the effectiveness of our framework. On the weather dataset, SSDTW
reduces mean squared error (MSE) by 16.01% compared with single-sequence
methods, while the contrastive learning encoder (CLE) further decreases MSE by
17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE
reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by
21.29% and 8.62%, respectively. Additionally, the contrastive learning approach
decreases SSDTW computational time by approximately 99%, ensuring scalability
and real-time applicability across multiple time series forecasting tasks.

</details>


### [465] [From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving](https://arxiv.org/abs/2508.07029)
*Antonio Guillen-Perez*

Main category: cs.LG

TL;DR: This paper explores improving imitation learning using Behavioral Cloning (BC) and compares it to an Offline Reinforcement Learning approach (Conservative Q-Learning or CQL) for robust driving policies.


<details>
  <summary>Details</summary>
Motivation: To address the brittleness and compounding errors of policies trained with Behavioral Cloning (BC) during closed-loop execution, especially for autonomous driving.

Method: Developed multiple BC baselines, including a Transformer-based model, and compared them with an Offline Reinforcement Learning approach (CQL) using structured, entity-centric state representation and a reward-engineered value function.

Result: The CQL agent showed substantial improvements: a 3.2x higher success rate and a 7.4x lower collision rate on 1,000 unseen scenarios from the Waymo Open Motion Dataset compared to the strongest BC baseline.

Conclusion: Offline Reinforcement Learning (CQL) significantly outperforms BC in creating robust, long-horizon autonomous driving policies from static datasets.

Abstract: Learning robust driving policies from large-scale, real-world datasets is a
central challenge in autonomous driving, as online data collection is often
unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward
approach to imitation learning, policies trained with BC are notoriously
brittle and suffer from compounding errors in closed-loop execution. This work
presents a comprehensive pipeline and a comparative study to address this
limitation. We first develop a series of increasingly sophisticated BC
baselines, culminating in a Transformer-based model that operates on a
structured, entity-centric state representation. While this model achieves low
imitation loss, we show that it still fails in long-horizon simulations. We
then demonstrate that by applying a state-of-the-art Offline Reinforcement
Learning algorithm, Conservative Q-Learning (CQL), to the same data and
architecture, we can learn a significantly more robust policy. Using a
carefully engineered reward function, the CQL agent learns a conservative value
function that enables it to recover from minor errors and avoid
out-of-distribution states. In a large-scale evaluation on 1,000 unseen
scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a
3.2x higher success rate and a 7.4x lower collision rate than the strongest BC
baseline, proving that an offline RL approach is critical for learning robust,
long-horizon driving policies from static expert data.

</details>


### [466] [A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling](https://arxiv.org/abs/2508.07032)
*Tiantian He,Keyue Jiang,An Zhao,Anna Schroder,Elinor Thompson,Sonja Soskic,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.LG

TL;DR: This paper introduces a stage-aware Mixture of Experts (MoE) model combined with an Inhomogeneous Graph Neural Diffusion (IGND) framework to address challenges in modeling neurodegenerative disease progression, offering flexibility and stage-specific clinical insights.


<details>
  <summary>Details</summary>
Motivation: Modeling neurodegenerative disease progression is difficult due to irregular longitudinal data and complex, stage-dependent pathological mechanisms that traditional models fail to account for.

Method: The paper proposes the IGND-MoE framework, combining inhomogeneous graph neural diffusion for spatial modeling with a localized neural reaction module, and uses a dual optimization method to estimate temporal positions of observations.

Result: The model dynamically integrates disease mechanisms over time, revealing stage-specific patterns, such as the dominance of graph-related processes in early stages and other processes later in the disease.

Conclusion: The IGND-MoE model provides a novel approach to understanding stage-specific pathological mechanisms in neurodegenerative diseases and offers clinical insights aligned with existing literature.

Abstract: The long-term progression of neurodegenerative diseases is commonly
conceptualized as a spatiotemporal diffusion process that consists of a graph
diffusion process across the structural brain connectome and a localized
reaction process within brain regions. However, modeling this progression
remains challenging due to 1) the scarcity of longitudinal data obtained
through irregular and infrequent subject visits and 2) the complex interplay of
pathological mechanisms across brain regions and disease stages, where
traditional models assume fixed mechanisms throughout disease progression. To
address these limitations, we propose a novel stage-aware Mixture of Experts
(MoE) framework that explicitly models how different contributing mechanisms
dominate at different disease stages through time-dependent expert
weighting.Data-wise, we utilize an iterative dual optimization method to
properly estimate the temporal position of individual observations,
constructing a co hort-level progression trajectory from irregular snapshots.
Model-wise, we enhance the spatial component with an inhomogeneous graph neural
diffusion model (IGND) that allows diffusivity to vary based on node states and
time, providing more flexible representations of brain networks. We also
introduce a localized neural reaction module to capture complex dynamics beyond
standard processes.The resulting IGND-MoE model dynamically integrates these
components across temporal states, offering a principled way to understand how
stage-specific pathological mechanisms contribute to progression. The
stage-wise weights yield novel clinical insights that align with literature,
suggesting that graph-related processes are more influential at early stages,
while other unknown physical processes become dominant later on.

</details>


### [467] [Differentiable Adaptive Kalman Filtering via Optimal Transport](https://arxiv.org/abs/2508.07037)
*Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin*

Main category: cs.LG

TL;DR: OTAKNet is a new online method for addressing noise-statistics drift in learning-based adaptive Kalman filtering, resolving challenges like changing environmental conditions without requiring retraining or ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: Noise-statistics drift, caused by factors like wind changes or electromagnetic interference, degrades learning-based filtering methods, necessitating a solution for real-world dynamical systems.

Method: OTAKNet introduces an online approach using one-step predictive measurement likelihood and optimal transport to handle noise-statistics drift, avoiding the need for offline fine-tuning or retraining.

Result: Evaluation on synthetic and real-world datasets demonstrates OTAKNet's superior performance compared to classical model-based adaptive Kalman filtering and offline learning-based filtering, even with limited training data.

Conclusion: OTAKNet offers a novel, geometry-aware, and fully online alternative to adaptive Kalman filtering, overcoming significant practical challenges in dynamic environments.

Abstract: Learning-based filtering has demonstrated strong performance in non-linear
dynamical systems, particularly when the statistics of noise are unknown.
However, in real-world deployments, environmental factors, such as changing
wind conditions or electromagnetic interference, can induce unobserved
noise-statistics drift, leading to substantial degradation of learning-based
methods. To address this challenge, we propose OTAKNet, the first online
solution to noise-statistics drift within learning-based adaptive Kalman
filtering. Unlike existing learning-based methods that perform offline
fine-tuning using batch pointwise matching over entire trajectories, OTAKNet
establishes a connection between the state estimate and the drift via one-step
predictive measurement likelihood, and addresses it using optimal transport.
This leverages OT's geometry - aware cost and stable gradients to enable fully
online adaptation without ground truth labels or retraining. We compare OTAKNet
against classical model-based adaptive Kalman filtering and offline
learning-based filtering. The performance is demonstrated on both synthetic and
real-world NCLT datasets, particularly under limited training data.

</details>


### [468] [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054)
*Ziqi Zhang,Ali Shahin Shamsabadi,Hanxiao Lu,Yifeng Cai,Hamed Haddadi*

Main category: cs.LG

TL;DR: The paper evaluates membership and memorization privacy risks in six knowledge distillation (KD) approaches for training smaller language models from larger ones, diagnosing privacy concerns originating from the teacher model.


<details>
  <summary>Details</summary>
Motivation: To assess whether knowledge distillation techniques inadvertently transfer privacy risks, such as membership and memorization, from large teacher models to smaller student models trained on them.

Method: The study systematically analyzes privacy risks in six LLM KD techniques across seven NLP tasks, three teacher model families (GPT-2, LLAMA-2, OPT), and varying student model sizes. It also identifies the influence of KD components on privacy risks and evaluates risks at a per-block level.

Result: Findings reveal that all existing LLM KD methods carry some degree of privacy risk from teachers to students. However, risks differ across KD techniques. A misalignment is observed between memorization and membership privacy risks, and significant variability in privacy risks is noted across model blocks.

Conclusion: Privacy risks are an inherent issue in LLM knowledge distillation, but their magnitude varies by technique and model component. Understanding key components and improving KD designs could potentially mitigate these issues.

Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high
computational demands of Large Language Models (LLMs) by transferring knowledge
from a large ''teacher'' to a smaller ''student'' model. However, students may
inherit the teacher's privacy when the teacher is trained on private data. In
this work, we systematically characterize and investigate membership and
memorization privacy risks inherent in six LLM KD techniques. Using
instruction-tuning settings that span seven NLP tasks, together with three
teacher model families (GPT-2, LLAMA-2, and OPT), and various size student
models, we demonstrate that all existing LLM KD approaches carry membership and
memorization privacy risks from the teacher to its students. However, the
extent of privacy risks varies across different KD techniques. We
systematically analyse how key LLM KD components (KD objective functions,
student training data and NLP tasks) impact such privacy risks. We also
demonstrate a significant disagreement between memorization and membership
privacy risks of LLM KD techniques. Finally, we characterize per-block privacy
risk and demonstrate that the privacy risk varies across different blocks by a
large margin.

</details>


### [469] [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
*Stanley Ngugi*

Main category: cs.LG

TL;DR: This paper proposes a two-stage knowledge editing strategy for Large Language Models (LLMs) using the Infused Adapter ($IA^3$) technique, achieving high accuracy for updates and mitigated forgetting of unrelated knowledge.


<details>
  <summary>Details</summary>
Motivation: To solve the issues LLMs face in updating knowledge when new information conflicts with embedded facts, including resistance to updates and catastrophic forgetting of unrelated knowledge.

Method: The paper introduces a two-stage 'unlearn-then-learn' strategy leveraging PEFT and $IA^3$, combined with a preliminary circuit localization to target components responsible for conflicting facts.

Result: Experiments on microsoft/Phi-3-mini-4k-instruct showed 98.50% accuracy for new facts, 96.00% suppression of old conflicting facts, and better catastrophic forgetting control (72.00% F_control accuracy vs ~20% in direct fine-tuning).

Conclusion: This approach enhances accuracy, safety, and control in editing LLM knowledge by enabling precise and localized updates while mitigating unwanted knowledge loss.

Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates,
especially when new information conflicts with deeply embedded facts. Such
conflicting factual edits often lead to two critical issues: resistance to
adopting the new fact and severe catastrophic forgetting of unrelated
knowledge. This paper introduces and evaluates a novel "unlearn-then-learn"
strategy for precise knowledge editing in LLMs, leveraging the
parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting
and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach
is powered by an initial circuit localization phase that identifies and targets
the specific internal components responsible for encoding the conflicting fact.
Through a rigorous experimental methodology on
microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically
informed two-stage approach achieves near-perfect accuracy (98.50%) for the
new, modulated fact while simultaneously effectively suppressing the original
conflicting fact (96.00% forget rate). Critically, our strategy exhibits
unprecedented localization (72.00% F_control accuracy), dramatically mitigating
catastrophic forgetting observed in direct fine-tuning approaches (which showed
as low as ~20% F_control accuracy), a direct benefit of our targeted
interpretability-guided intervention. Furthermore, qualitative analysis reveals
a nuanced mechanism of "soft forgetting," where original knowledge is
suppressed from default retrieval but remains latent and conditionally
accessible, enhancing model safety and control. These findings represent a
significant advancement towards precise, localized, and safe knowledge
management in compact LLMs.

</details>


### [470] [Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework](https://arxiv.org/abs/2508.07085)
*N Harshit,K Mounvik*

Main category: cs.LG

TL;DR: The study introduces a hybrid drift detection framework integrating Transformers and Autoencoders to address the challenge of concept drift in applied machine learning, offering enhanced sensitivity and interpretability in detection.


<details>
  <summary>Details</summary>
Motivation: Concept drift can lead to significant reductions in model performance due to changes in data distribution. Existing detection methods often lack sensitivity for early detection.

Method: A hybrid framework combining Transformers and Autoencoders was developed alongside a Trust Score methodology that incorporates statistical drift metrics, prediction uncertainty, rule violations, and classifier errors for online drift detection.

Result: The framework demonstrated superior drift detection sensitivity and interpretability compared to baseline methods, particularly using a synthetic drift dataset. It detected drift earlier and handled error rates and logical violations better.

Conclusion: The proposed framework effectively and reliably monitors concept drift in real-time applications, outperforming traditional autoencoders in applied machine learning scenarios.

Abstract: In applied machine learning, concept drift, which is either gradual or abrupt
changes in data distribution, can significantly reduce model performance.
Typical detection methods,such as statistical tests or reconstruction-based
models,are generally reactive and not very sensitive to early detection. Our
study proposes a hybrid framework consisting of Transformers and Autoencoders
to model complex temporal dynamics and provide online drift detection. We
create a distinct Trust Score methodology, which includes signals on (1)
statistical and reconstruction-based drift metrics, more specifically, PSI,
JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,
and (4) trend of classifier error aligned with the combined metrics defined by
the Trust Score. Using a time sequenced airline passenger data set with
synthetic drift, our proposed model allows for a better detection of drift
using as a whole and at different detection thresholds for both sensitivity and
interpretability compared to baseline methods and provides a strong pipeline
for drift detection in real time for applied machine learning. We evaluated
performance using a time-sequenced airline passenger dataset having the
gradually injected stimulus of drift in expectations,e.g. permuted ticket
prices in later batches, broken into 10 time segments [1].In the data, our
results support that the Transformation-Autoencoder detected drift earlier and
with more sensitivity than the autoencoders commonly used in the literature,
and provided improved modeling over more error rates and logical violations.
Therefore, a robust framework was developed to reliably monitor concept drift.

</details>


### [471] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: Second-Order MeanFlow extends MeanFlow generative modeling by introducing average acceleration fields, enhancing expressivity and enabling efficient implementation through theoretical foundations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance generative modeling by exploring Second-Order MeanFlow, improving sampling efficiency and dynamic representation.

Method: The paper integrates average acceleration fields into MeanFlow, verifies feasibility through consistency conditions, analyzes expressivity using circuit complexity, and derives efficient implementation criteria using approximate attention computations.

Result: Second-Order MeanFlow satisfies generalized consistency conditions, demonstrates implementability within the $\mathsf{TC}^0$ class, and achieves scalable operations with $1/\mathrm{poly}(n)$ error approximations in time $n^{2+o(1)}$.

Conclusion: The theoretical framework supports the development of high-order flow matching generative models that combine complex dynamics with practical efficiency.

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [472] [BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation](https://arxiv.org/abs/2508.07106)
*Yiran Huang,Amirhossein Nouranizadeh,Christine Ahrends,Mengjia Xu*

Main category: cs.LG

TL;DR: This paper proposes BrainATCL, an unsupervised framework for adaptive temporal brain connectivity learning based on resting-state fMRI data, achieving superior performance in tasks such as functional link prediction and age estimation.


<details>
  <summary>Details</summary>
Motivation: To improve the modeling of dynamic fMRI connectivity, focusing on transient neural states and behaviors which are linked to neuropsychiatric diseases.

Method: Developed BrainATCL, which dynamically adjusts the lookback window and uses GINE-Mamba2 backbone to encode spatial-temporal representations. Added biologically-informed edge attributes for better spatial modeling.

Result: BrainATCL outperformed conventional methods in functional link prediction and age estimation tasks, demonstrating strong generalization across sessions.

Conclusion: BrainATCL can effectively capture biologically meaningful patterns in dynamic fMRI data and contribute to understanding brain connectivity and behaviors.

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely
used to study human brain activity. fMRI signals in areas across the brain
transiently synchronise and desynchronise their activity in a highly structured
manner, even when an individual is at rest. These functional connectivity
dynamics may be related to behaviour and neuropsychiatric disease. To model
these dynamics, temporal brain connectivity representations are essential, as
they reflect evolving interactions between brain regions and provide insight
into transient neural states and network reconfigurations. However,
conventional graph neural networks (GNNs) often struggle to capture long-range
temporal dependencies in dynamic fMRI data. To address this challenge, we
propose BrainATCL, an unsupervised, nonparametric framework for adaptive
temporal brain connectivity learning, enabling functional link prediction and
age estimation. Our method dynamically adjusts the lookback window for each
snapshot based on the rate of newly added edges. Graph sequences are
subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal
representations of dynamic functional connectivity in resting-state fMRI data
of 1,000 participants from the Human Connectome Project. To further improve
spatial modeling, we incorporate brain structure and function-informed edge
attributes, i.e., the left/right hemispheric identity and subnetwork membership
of brain regions, enabling the model to capture biologically meaningful
topological patterns. We evaluate our BrainATCL on two tasks: functional link
prediction and age estimation. The experimental results demonstrate superior
performance and strong generalization, including in cross-session prediction
scenarios.

</details>


### [473] [Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning](https://arxiv.org/abs/2508.07114)
*Atakan Azakli,Bernd Stelzer*

Main category: cs.LG

TL;DR: The paper presents a novel machine learning method, utilizing Multiple Instance Learning (MIL), to improve predictions in challenging hypothesis testing scenarios.


<details>
  <summary>Details</summary>
Motivation: Enhance predictions for hypothesis testing problems where current classifiers face significant challenges.

Method: Employs Multiple Instance Learning (MIL) with theoretical and scaling behavior analysis, applied to constraint-setting in particle physics.

Result: Shows MIL's potential for leveraging maximum Fisher Information from datasets.

Conclusion: MIL improves discriminative power and prediction accuracy in complex scenarios, exemplified in particle physics applications.

Abstract: In this work, we propose a new machine learning (ML) methodology to obtain
more precise predictions for some parameters of interest in a given hypotheses
testing problem. Our proposed method also allows ML models to have more
discriminative power in cases where it is extremely challenging for
state-of-the-art classifiers to have any level of accurate predictions. This
method can also allow us to systematically decrease the error from ML models in
their predictions. In this paper, we provide a mathematical motivation why
Multiple Instance Learning (MIL) would have more predictive power over their
single-instance counterparts. We support our theoretical claims by analyzing
the behavior of the MIL models through their scaling behaviors with respect to
the number of instances on which the model makes predictions. As a concrete
application, we constrain Wilson coefficients of the Standard Model Effective
Field Theory (SMEFT) using kinematic information from subatomic particle
collision events at the Large Hadron Collider (LHC). We show that under certain
circumstances, it might be possible to extract the theoretical maximum Fisher
Information latent in a dataset.

</details>


### [474] [From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context](https://arxiv.org/abs/2508.07117)
*Peyman Baghershahi,Gregoire Fournier,Pranav Nyati,Sourav Medya*

Main category: cs.LG

TL;DR: LOGIC is a post-hoc framework using large language models to generate interpretable explanations for Graph Neural Networks (GNNs) working on text-attributed graphs.


<details>
  <summary>Details</summary>
Motivation: Existing explanation methods for GNNs often fail to generate interpretable and detailed rationales for structured data with rich natural language attributes.

Method: LOGIC leverages large language models to align GNN embeddings with human reasoning, combining soft prompts with textual inputs from graphs.

Result: LOGIC improves fidelity and sparsity in explanations, and significantly enhances human-centric metrics like insightfulness across real-world datasets.

Conclusion: LOGIC demonstrates the potential of using LLM-based techniques as effective tools for improving interpretability and explainability in graph learning applications.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (LLMs) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
LLM embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the LLM to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and sparsity, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for LLM-based
explainability in graph learning by aligning GNN internals with human
reasoning.

</details>


### [475] [Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2508.07122)
*Zhihao Xue,Yun Zi,Nia Qi,Ming Gong,Yujun Zou*

Main category: cs.LG

TL;DR: The paper introduces a spatiotemporal graph neural network for forecasting performance in distributed backend systems, showing better prediction accuracy and robustness compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Performance fluctuation prediction in distributed backend systems with complex service call structures is challenging, necessitating advanced modeling approaches.

Method: The model integrates graph convolutional networks to extract service topology dependencies and gated recurrent networks for capturing temporal dynamics, combined with a time encoding mechanism for non-stationary temporal sequences.

Result: Experiments on a large-scale public cluster dataset demonstrate superior predictive accuracy and stability under varying conditions compared to other methods.

Conclusion: The proposed model is highly effective and robust for backend service performance management, showcasing significant practical utility.

Abstract: This paper proposes a spatiotemporal graph neural network-based performance
prediction algorithm to address the challenge of forecasting performance
fluctuations in distributed backend systems with multi-level service call
structures. The method abstracts system states at different time slices into a
sequence of graph structures. It integrates the runtime features of service
nodes with the invocation relationships among services to construct a unified
spatiotemporal modeling framework. The model first applies a graph
convolutional network to extract high-order dependency information from the
service topology. Then it uses a gated recurrent network to capture the dynamic
evolution of performance metrics over time. A time encoding mechanism is also
introduced to enhance the model's ability to represent non-stationary temporal
sequences. The architecture is trained in an end-to-end manner, optimizing the
multi-layer nested structure to achieve high-precision regression of future
service performance metrics. To validate the effectiveness of the proposed
method, a large-scale public cluster dataset is used. A series of
multi-dimensional experiments are designed, including variations in time
windows and concurrent load levels. These experiments comprehensively evaluate
the model's predictive performance and stability. The experimental results show
that the proposed model outperforms existing representative methods across key
metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying
load intensities and structural complexities. These results demonstrate the
model's practical potential for backend service performance management tasks.

</details>


### [476] [Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning](https://arxiv.org/abs/2508.07126)
*Zhengran Ji,Boyuan Chen*

Main category: cs.LG

TL;DR: Pref-GUIDE transforms noisy real-time scalar feedback into preference-based feedback to improve reward models in reinforcement learning, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for reinforcement learning agents capable of adapting in online scenarios, given that traditional dense reward functions fail to specify hard-to-define task objectives.

Method: Pref-GUIDE transforms scalar feedback into preferences, tackles temporal inconsistency, and aggregates feedback across users to achieve robust reward models.

Result: Across three environments, Pref-GUIDE outperformed scalar-feedback baselines and even expert-designed dense rewards.

Conclusion: Pref-GUIDE is a scalable method for utilizing human feedback effectively in online reinforcement learning, improving feedback reliability and reward model quality.

Abstract: Training reinforcement learning agents with human feedback is crucial when
task objectives are difficult to specify through dense reward functions. While
prior methods rely on offline trajectory comparisons to elicit human
preferences, such data is unavailable in online learning scenarios where agents
must adapt on the fly. Recent approaches address this by collecting real-time
scalar feedback to guide agent behavior and train reward models for continued
learning after human feedback becomes unavailable. However, scalar feedback is
often noisy and inconsistent, limiting the accuracy and generalization of
learned rewards. We propose Pref-GUIDE, a framework that transforms real-time
scalar feedback into preference-based data to improve reward model learning for
continual policy training. Pref-GUIDE Individual mitigates temporal
inconsistency by comparing agent behaviors within short windows and filtering
ambiguous feedback. Pref-GUIDE Voting further enhances robustness by
aggregating reward models across a population of users to form consensus
preferences. Across three challenging environments, Pref-GUIDE significantly
outperforms scalar-feedback baselines, with the voting variant exceeding even
expert-designed dense rewards. By reframing scalar feedback as structured
preferences with population feedback, Pref-GUIDE offers a scalable and
principled approach for harnessing human input in online reinforcement
learning.

</details>


### [477] [How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?](https://arxiv.org/abs/2508.07127)
*Niranjana Arun Menon,Iqra Farooq,Yulong Li,Sara Ahmed,Yutong Xie,Muhammad Awais,Imran Razzak*

Main category: cs.LG

TL;DR: This study investigates fine-tuned large language models (LLMs) for predicting cardiac diseases and genetic variations contributing to cardiovascular disease (CVD) risk, leveraging genomic data and Chain of Thought reasoning.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular diseases pose a global health challenge due to their complex nature and high morbidity and mortality rates. Extracting meaningful insights from high-dimensional genomic data remains difficult, signaling the need for innovative predictive approaches.

Method: The researchers fine-tuned large language models (LLMs) to predict cardiac conditions and SNPs linked to CVD risk by analyzing genetic markers obtained through high-throughput genomic profiling. They employed Chain of Thought (CoT) reasoning to prompt models to generate disease labels and deduce clinical insights.

Result: The study's findings show that LLMs can effectively extract latent biological relationships from genomic data, enabling predictions of disease risks and clinical associations across diverse phenotypes.

Conclusion: Fine-tuned LLMs hold significant potential in advancing personalized medicine for cardiac care by enabling early detection, precise risk assessment, and clinical decision support for cardiovascular diseases.

Abstract: Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned LLMs to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how LLMs can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of LLMs in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.

</details>


### [478] [A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs](https://arxiv.org/abs/2508.07134)
*Lu Chenggang*

Main category: cs.LG

TL;DR: This paper proposes a globally optimal, non-iterative Semi-NMF algorithm using orthogonal decomposition, offering theoretical guarantees and superior reconstruction accuracy compared to NMF and previous semi-NMF methods.


<details>
  <summary>Details</summary>
Motivation: Current semi-NMF algorithms are iterative, non-convex, and prone to local minima, which limits their effectiveness in decomposing data with mixed signs.

Method: The authors developed a novel non-iterative method based on orthogonal decomposition derived from the scatter matrix of input data to achieve a global minimum under the Frobenius norm.

Result: The proposed method achieves lower reconstruction errors than standard NMF and existing semi-NMF methods in experiments with synthetic data and the UCI Wine dataset.

Conclusion: The globally optimal and non-iterative solution provides theoretical rigor and better empirical performance, offering fresh insights into matrix factorization approaches for data analysis and optimization.

Abstract: Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical
Nonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain
both positive and negative entries, making it suitable for decomposing data
with mixed signs. However, most existing semi-NMF algorithms are iterative,
non-convex, and prone to local minima. In this paper, we propose a novel method
that yields a globally optimal solution to the semi-NMF problem under the
Frobenius norm, through an orthogonal decomposition derived from the scatter
matrix of the input data. We rigorously prove that our solution attains the
global minimum of the reconstruction error. Furthermore, we demonstrate that
when the input matrix is nonnegative, our method often achieves lower
reconstruction error than standard NMF algorithms, although unfortunately the
basis matrix may not satisfy nonnegativity. In particular, in low-rank cases
such as rank 1 or 2, our solution reduces exactly to a nonnegative
factorization, recovering the NMF structure. We validate our approach through
experiments on both synthetic data and the UCI Wine dataset, showing that our
method consistently outperforms existing NMF and semi-NMF methods in terms of
reconstruction accuracy. These results confirm that our globally optimal,
non-iterative formulation offers both theoretical guarantees and empirical
advantages, providing a new perspective on matrix factorization in optimization
and data analysis.

</details>


### [479] [A Stable and Principled Loss Function for Direct Language Model Alignment](https://arxiv.org/abs/2508.07137)
*Yuandong Tan*

Main category: cs.LG

TL;DR: A novel loss function addressing issues in Direct Preference Optimization (DPO) enables stable alignment of large language models with human feedback by targeting finite logits difference instead of indefinite maximization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address theoretical and practical misalignments in the DPO loss function used for aligning large language models, including training instability and reward hacking.

Method: A new loss function is proposed, derived directly from the RLHF optimality condition, aiming for a stable, finite logits difference. The method is supported by theoretical analysis, including gradient comparison.

Result: Experiments show that fine-tuning a Qwen2.5-7B model using the proposed method improves win rates over standard DPO baselines and performs competitively with larger models.

Conclusion: The proposed method enhances stability, prevents reward hacking, and improves alignment effectiveness in language models, offering a more robust alternative to DPO.

Abstract: The alignment of large language models (LLMs) with human preferences is
commonly achieved through Reinforcement Learning from Human Feedback (RLHF).
Direct Preference Optimization (DPO) simplified this paradigm by establishing a
direct mapping between the optimal policy and a reward function, eliminating
the need for an explicit reward model. However, we argue that the DPO loss
function is theoretically misaligned with its own derivation, as it promotes
the indefinite maximization of a logits difference, which can lead to training
instability and reward hacking. In this paper, we propose a novel loss function
derived directly from the RLHF optimality condition. Our proposed loss targets
a specific, finite value for the logits difference, which is dictated by the
underlying reward, rather than its maximization. We provide a theoretical
analysis, including a gradient-based comparison, to demonstrate that our method
avoids the large gradients that plague DPO when the probability of dispreferred
responses approaches zero. This inherent stability prevents reward hacking and
leads to more effective alignment. We validate our approach by fine-tuning a
Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO
baseline and achieving competitive performance against larger models like
Llama-3.1-8B.

</details>


### [480] [Strategic Incentivization for Locally Differentially Private Federated Learning](https://arxiv.org/abs/2508.07138)
*Yashwant Krishna Pagoti,Arunesh Sinha,Shamik Sural*

Main category: cs.LG

TL;DR: The paper discusses a game-theoretical approach to address the privacy-accuracy trade-off in Federated Learning (FL) using a token-based incentivization mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the privacy-accuracy trade-off in Federated Learning, where clients wish to preserve their privacy by adding noise to gradients, yet noise addition affects the overall model accuracy.

Method: The paper models the trade-off as a game, introduces a token-based incentivization mechanism, and identifies the players, actions, and payoffs to encourage lower noise perturbation by clients while strategically maintaining privacy and accuracy.

Result: Extensive experiments were conducted, showing how different parameters influenced the model performance and strategy under the incentivization mechanism.

Conclusion: The token-based incentive mechanism offers a strategic balance in the privacy-accuracy trade-off in Federated Learning, with the flexibility to study parameter impacts.

Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning
model by sharing gradient information, instead of raw data, with a server over
multiple rounds. To address the possibility of information leakage in spite of
sharing only the gradients, Local Differential Privacy (LDP) is often used. In
LDP, clients add a selective amount of noise to the gradients before sending
the same to the server. Although such noise addition protects the privacy of
clients, it leads to a degradation in global model accuracy. In this paper, we
model this privacy-accuracy trade-off as a game, where the sever incentivizes
the clients to add a lower degree of noise for achieving higher accuracy, while
the clients attempt to preserve their privacy at the cost of a potential loss
in accuracy. A token based incentivization mechanism is introduced in which the
quantum of tokens credited to a client in an FL round is a function of the
degree of perturbation of its gradients. The client can later access a newly
updated global model only after acquiring enough tokens, which are to be
deducted from its balance. We identify the players, their actions and payoff,
and perform a strategic analysis of the game. Extensive experiments were
carried out to study the impact of different parameters.

</details>


### [481] [SGD Convergence under Stepsize Shrinkage in Low-Precision Training](https://arxiv.org/abs/2508.07142)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: This paper investigates how low-precision training affects the convergence of stochastic gradient descent (SGD), introducing gradient shrinkage and noise.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and mitigate the computational and memory challenges in large-scale deep learning by quantizing gradients, despite its impact on SGD convergence.

Method: A theoretical analysis is conducted, modeling gradient shrinkage where gradients are scaled and perturbed by quantization noise, and studying its impact on SGD stepsizes and convergence.

Result: The analysis shows that low-precision SGD still converges but at a slower rate due to gradient shrinkage and with higher asymptotic error caused by quantization noise.

Conclusion: While low-precision training reduces computational costs, it introduces trade-offs, slowing convergence and increasing error, which must be considered in gradient-based optimization.

Abstract: Low-precision training has become essential for reducing the computational
and memory costs of large-scale deep learning. However, quantization of
gradients introduces both magnitude shrinkage and additive noise, which can
alter the convergence behavior of stochastic gradient descent (SGD). In this
work, we study the convergence of SGD under a gradient shrinkage model, where
each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by
zero-mean quantization noise. We show that this shrinkage is equivalent to
replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$,
which slows convergence when $q_{\min} < 1$. Under standard smoothness and
bounded-variance assumptions, we prove that low-precision SGD still converges,
but at a reduced rate determined by $q_{\min}$, and with an increased
asymptotic error floor due to quantization noise. We theoretically analyze how
reduced numerical precision slows down training by modeling it as gradient
shrinkage in the standard SGD convergence framework.

</details>


### [482] [What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains](https://arxiv.org/abs/2508.07208)
*Chanakya Ekbote,Marco Bondaschi,Nived Rajaraman,Jason D. Lee,Michael Gastpar,Ashok Vardhan Makkuva,Paul Pu Liang*

Main category: cs.LG

TL;DR: This paper shows that two-layer single-head transformers can represent any kth-order Markov process, providing new insights into in-context learning (ICL).


<details>
  <summary>Details</summary>
Motivation: To investigate the theoretical capacity of shallow transformer models, particularly two-layer architectures, in representing complex sequence tasks like higher-order Markov processes.

Method: The authors theoretically analyze and construct a two-layer transformer with one head per layer to demonstrate its ability to model conditional k-grams, while focusing on its learning dynamics in simpler cases like first-order Markov chains.

Result: The analysis reveals that two-layer single-head transformers are sufficient to represent any kth-order Markov process, filling a gap in prior work's understanding of model depth and expressiveness for ICL.

Conclusion: The study provides evidence that shallow transformer architectures can exhibit strong in-context learning abilities, advancing our theoretical understanding of how these models represent structured sequence data.

Abstract: In-context learning (ICL) is a hallmark capability of transformers, through
which trained models learn to adapt to new tasks by leveraging information from
the input context. Prior work has shown that ICL emerges in transformers due to
the presence of special circuits called induction heads. Given the equivalence
between induction heads and conditional k-grams, a recent line of work modeling
sequential inputs as Markov processes has revealed the fundamental impact of
model depth on its ICL capabilities: while a two-layer transformer can
efficiently represent a conditional 1-gram model, its single-layer counterpart
cannot solve the task unless it is exponentially large. However, for higher
order Markov sources, the best known constructions require at least three
layers (each with a single attention head) - leaving open the question: can a
two-layer single-head transformer represent any kth-order Markov process? In
this paper, we precisely address this and theoretically show that a two-layer
transformer with one head per layer can indeed represent any conditional
k-gram. Thus, our result provides the tightest known characterization of the
interplay between transformer depth and Markov order for ICL. Building on this,
we further analyze the learning dynamics of our two-layer construction,
focusing on a simplified variant for first-order Markov chains, illustrating
how effective in-context representations emerge during training. Together,
these results deepen our current understanding of transformer-based ICL and
illustrate how even shallow architectures can surprisingly exhibit strong ICL
capabilities on structured sequence modeling tasks.

</details>


### [483] [Neural Bridge Processes](https://arxiv.org/abs/2508.07220)
*Jian Xu,Yican Liu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: The paper introduces Neural Bridge Processes (NBPs), a novel approach to model stochastic functions that addresses limitations of Gaussian Processes and Neural Processes by ensuring endpoint coherence and capturing complex distributions.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the scalability, Gaussianity, and flexibility limitations of traditional Gaussian Processes and the inability of Neural Processes to model complex, multi-modal target distributions.

Method: NBPs reformulate the forward kernel to depend explicitly on inputs, creating a constrained diffusion trajectory terminating at the supervised target. This ensures coherent endpoints and stronger gradient signals.

Result: NBPs demonstrate substantial improvements across synthetic data, EEG signal regression, and image regression tasks compared to existing models.

Conclusion: NBPs prove the effectiveness of a DDPM-style bridge sampling approach in enhancing not just performance but also theoretical consistency for tasks involving structured prediction.

Abstract: Learning stochastic functions from partially observed context-target pairs is
a fundamental problem in probabilistic modeling. Traditional models like
Gaussian Processes (GPs) face scalability issues with large datasets and assume
Gaussianity, limiting their applicability. While Neural Processes (NPs) offer
more flexibility, they struggle with capturing complex, multi-modal target
distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a
learned diffusion process but rely solely on conditional signals in the
denoising network, resulting in weak input coupling from an unconditional
forward process and semantic mismatch at the diffusion endpoint. In this work,
we propose Neural Bridge Processes (NBPs), a novel method for modeling
stochastic functions where inputs x act as dynamic anchors for the entire
diffusion trajectory. By reformulating the forward kernel to explicitly depend
on x, NBP enforces a constrained path that strictly terminates at the
supervised target. This approach not only provides stronger gradient signals
but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG
signal regression and image regression tasks, achieving substantial
improvements over baselines. These results underscore the effectiveness of
DDPM-style bridge sampling in enhancing both performance and theoretical
consistency for structured prediction tasks.

</details>


### [484] [LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference](https://arxiv.org/abs/2508.07221)
*Po-Han Lee,Yu-Cheng Lin,Chan-Tung Ku,Chan Hsu,Pei-Cing Huang,Ping-Hsun Wu,Yihuang Kang*

Main category: cs.LG

TL;DR: The paper introduces an approach using Large Language Models (LLMs) for automated confounder discovery and subgroup analysis in causal Machine Learning, aiming to reduce human dependency and improve robustness in treatment effect estimation.


<details>
  <summary>Details</summary>
Motivation: Current causal ML methods, like causal trees and doubly robust estimators, face challenges in handling unmeasured confounding and structural biases, especially in complex real-world data. Additionally, reliance on domain experts increases costs and scalability issues.

Method: The authors propose integrating Large Language Model-based agents into the causal ML pipeline to simulate domain expertise. This framework leverages LLMs for reasoning, enabling automated confounder discovery and subgroup analysis while maintaining interpretability.

Result: Experiments on real-world medical datasets demonstrate that the proposed approach improves the robustness of treatment effect estimation, narrows confidence intervals, and uncovers previously unrecognized confounding biases.

Conclusion: The use of LLM-based agents represents a scalable and trustworthy solution for causal inference by automating confounder identification and subgroup analysis, reducing the need for human intervention and ensuring interpretability.

Abstract: Estimating individualized treatment effects from observational data presents
a persistent challenge due to unmeasured confounding and structural bias.
Causal Machine Learning (causal ML) methods, such as causal trees and doubly
robust estimators, provide tools for estimating conditional average treatment
effects. These methods have limited effectiveness in complex real-world
environments due to the presence of latent confounders or those described in
unstructured formats. Moreover, reliance on domain experts for confounder
identification and rule interpretation introduces high annotation cost and
scalability concerns. In this work, we proposed Large Language Model-based
agents for automated confounder discovery and subgroup analysis that integrate
agents into the causal ML pipeline to simulate domain expertise. Our framework
systematically performs subgroup identification and confounding structure
discovery by leveraging the reasoning capabilities of LLM-based agents, which
reduces human dependency while preserving interpretability. Experiments on
real-world medical datasets show that our proposed approach enhances treatment
effect estimation robustness by narrowing confidence intervals and uncovering
unrecognized confounding biases. Our findings suggest that LLM-based agents
offer a promising path toward scalable, trustworthy, and semantically aware
causal inference.

</details>


### [485] [EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning](https://arxiv.org/abs/2508.07224)
*Ananda Prakash Verma*

Main category: cs.LG

TL;DR: This paper introduces EDGE, an adaptive learning framework that integrates psychometrics, cognitive diagnostics, and scheduling for customized educational support.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptive learning systems that can diagnose misconceptions, generate effective learning materials, and optimize retrieval schedules.

Method: The framework involves four stages: evaluate learner abilities, diagnose misconceptions, generate counterfactual items, and schedule practice using a restless bandit approximation.

Result: The theoretical framework ensures effectiveness through formalized readiness metrics, EdgeScore, and efficient approaches for reducing misconceptions.

Conclusion: EDGE paves the way for a robust, tailored learning system, combining advanced psychometric theories with implementable strategies for future empirical validation.

Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning
framework composed of four stages: Evaluate (ability and state estimation),
Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual
item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies
psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics
(misconception discovery from distractor patterns and response latencies),
contrastive item generation (minimal perturbations that invalidate learner
shortcuts while pre-serving psychometric validity), and principled scheduling
(a restless bandit approximation to spaced retrieval). We formalize a composite
readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,
and derive an index policy that is near-optimal under mild assumptions on
forgetting and learning gains. We further establish conditions under which
counterfactual items provably reduce the posterior probability of a targeted
misconception faster than standard practice. The paper focuses on theory and
implementable pseudocode; empirical study is left to future work.

</details>


### [486] [Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation](https://arxiv.org/abs/2508.07243)
*Chu Zhao,Eneng Yang,Yizhou Dang,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.LG

TL;DR: This paper identifies false hard negatives (FHNS) as an issue in heuristic negative sampling for recommendation systems due to confounders like biases. It proposes CNSDiff, which generates bias-free negatives via a diffusion process and uses causal regularization to improve robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve recommendation systems' generalization by addressing the issue of false hard negative samples, which are caused by biases in predefined candidate pools.

Method: The authors propose CNSDiff, a method involving conditional diffusion to synthesize negative samples in latent space, avoiding predefined pool biases. Additionally, it adds causal regularization to mitigate environmental confounder effects.

Result: CNSDiff delivers a 13.96% average improvement in performance metrics over state-of-the-art baselines across four distribution shift scenarios, demonstrating effectiveness in out-of-distribution generalization.

Conclusion: CNSDiff effectively mitigates false hard negatives, improves generalization under distribution shifts, and provides a robust solution for recommendation systems leveraging causal reasoning and diffusion processes.

Abstract: Heuristic negative sampling enhances recommendation performance by selecting
negative samples of varying hardness levels from predefined candidate pools to
guide the model toward learning more accurate decision boundaries. However, our
empirical and theoretical analyses reveal that unobserved environmental
confounders (e.g., exposure or popularity biases) in candidate pools may cause
heuristic sampling methods to introduce false hard negatives (FHNS). These
misleading samples can encourage the model to learn spurious correlations
induced by such confounders, ultimately compromising its generalization ability
under distribution shifts. To address this issue, we propose a novel method
named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing
negative samples in the latent space via a conditional diffusion process,
CNSDiff avoids the bias introduced by predefined candidate pools and thus
reduces the likelihood of generating FHNS. Moreover, it incorporates a causal
regularization term to explicitly mitigate the influence of environmental
confounders during the negative sampling process, leading to robust negatives
that promote out-of-distribution (OOD) generalization. Comprehensive
experiments under four representative distribution shift scenarios demonstrate
that CNSDiff achieves an average improvement of 13.96% across all evaluation
metrics compared to state-of-the-art baselines, verifying its effectiveness and
robustness in OOD recommendation tasks.

</details>


### [487] [Policy Newton methods for Distortion Riskmetrics](https://arxiv.org/abs/2508.07249)
*Soumen Pachal,Mizhaan Prajit Maniyar,Prashanth L. A*

Main category: cs.LG

TL;DR: The paper develops methods for optimization in reinforcement learning focused on risk-sensitive objectives, using specific mathematical tools and algorithms to achieve theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: The need to optimize decision-making under uncertainty, while accounting for risk sensitivity, inspired the study of policies maximizing distortion risk metrics in reinforcement learning.

Method: The authors derive a policy Hessian theorem for distortion risk metrics using the likelihood ratio method, propose a DRM Hessian estimator, and craft a cubic-regularized policy Newton algorithm for on-policy reinforcement learning.

Result: The algorithm achieves convergence to an $
\epsilon$-second-order stationary point ($\epsilon$-SOSP) of the DRM objective, avoiding saddle points, with a sample complexity of $
\mathcal{O}(\epsilon^{-3.5})$.

Conclusion: The work is pioneering in achieving second-order stationary point convergence for risk-sensitive RL objectives, advancing both theory and practical understanding of risk-sensitive optimization.

Abstract: We consider the problem of risk-sensitive control in a reinforcement learning
(RL) framework. In particular, we aim to find a risk-optimal policy by
maximizing the distortion riskmetric (DRM) of the discounted reward in a finite
horizon Markov decision process (MDP). DRMs are a rich class of risk measures
that include several well-known risk measures as special cases. We derive a
policy Hessian theorem for the DRM objective using the likelihood ratio method.
Using this result, we propose a natural DRM Hessian estimator from sample
trajectories of the underlying MDP. Next, we present a cubic-regularized policy
Newton algorithm for solving this problem in an on-policy RL setting using
estimates of the DRM gradient and Hessian. Our proposed algorithm is shown to
converge to an $\epsilon$-second-order stationary point ($\epsilon$-SOSP) of
the DRM objective, and this guarantee ensures the escaping of saddle points.
The sample complexity of our algorithms to find an $ \epsilon$-SOSP is
$\mathcal{O}(\epsilon^{-3.5})$. Our experiments validate the theoretical
findings. To the best of our knowledge, our is the first work to present
convergence to an $\epsilon$-SOSP of a risk-sensitive objective, while existing
works in the literature have either shown convergence to a first-order
stationary point of a risk-sensitive objective, or a SOSP of a risk-neutral
one.

</details>


### [488] [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.LG

TL;DR: The paper explores the use of influence functions in deep learning for tracking predictions to their training data. It reviews their theoretical foundations, efficiency improvements, and applications while identifying challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability, debugging, and accountability in machine learning models by understanding the influence of individual training samples on predictions.

Method: Influence functions are used to estimate the impact of adjusting or removing individual data points on model parameters and predictions. The method includes advancements in efficient inverse-Hessian-vector product estimation.

Result: The paper reviews the effectiveness of influence functions for data attribution and mislabel detection, demonstrating their potential utility in deep learning.

Conclusion: The study highlights the advantages and current limitations of influence functions and encourages further research to maximize their applications in large-scale scenarios.

Abstract: The goal of data attribution is to trace the model's predictions through the
learning algorithm and back to its training data. thereby identifying the most
influential training samples and understanding how the model's behavior leads
to particular predictions. Understanding how individual training examples
influence a model's predictions is fundamental for machine learning
interpretability, data debugging, and model accountability. Influence
functions, originating from robust statistics, offer an efficient, first-order
approximation to estimate the impact of marginally upweighting or removing a
data point on a model's learned parameters and its subsequent predictions,
without the need for expensive retraining. This paper comprehensively reviews
the data attribution capability of influence functions in deep learning. We
discuss their theoretical foundations, recent algorithmic advances for
efficient inverse-Hessian-vector product estimation, and evaluate their
effectiveness for data attribution and mislabel detection. Finally,
highlighting current challenges and promising directions for unleashing the
huge potential of influence functions in large-scale, real-world deep learning
scenarios.

</details>


### [489] [When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective](https://arxiv.org/abs/2508.07299)
*Lin-Han Jia,Si-Yu Han,Wen-Chao Hu,Jie-Jing Shao,Wen-Da Wei,Zhi Zhou,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: This paper develops a unified theoretical framework for neuro-symbolic learning (Nesy) with unreliable knowledge and semi/self-supervised learning (SSL). It proposes metrics for predicting the impact of pretext tasks on target performance, validated experimentally.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of theoretical grounding in the selection of unsupervised pretext tasks, which are currently based on heuristics. They seek to unify theories of Nesy and SSL and provide a structured method for task evaluation.

Method: The paper extends Nesy theory to cover unreliable knowledge, analyzes the impact of pretext tasks on target performance based on knowledge learnability, reliability, and completeness, and proposes operationalization methods for these factors.

Result: The proposed theory and evaluation method demonstrate high predictive correlation between minimal-data performance estimations and actual performance in semi/self-supervised learning experiments.

Conclusion: This work establishes a theory-based framework for selecting and evaluating unsupervised pretext tasks, moving beyond current heuristic methods and confirming its practical effectiveness through experimental validation.

Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models
by enabling them to satisfy knowledge, while semi/self-supervised learning
(SSL) improves the target task performance by designing unsupervised pretext
tasks for unlabeled data to make models satisfy corresponding assumptions. We
extend the Nesy theory based on reliable knowledge to the scenario of
unreliable knowledge (i.e., assumptions), thereby unifying the theoretical
frameworks of SSL and Nesy. Through rigorous theoretical analysis, we
demonstrate that, in theory, the impact of pretext tasks on target performance
hinges on three factors: knowledge learnability with respect to the model,
knowledge reliability with respect to the data, and knowledge completeness with
respect to the target. We further propose schemes to operationalize these
theoretical metrics, and thereby develop a method that can predict the
effectiveness of pretext tasks in advance. This will change the current status
quo in practical applications, where the selections of unsupervised tasks are
heuristic-based rather than theory-based, and it is difficult to evaluate the
rationality of unsupervised pretext task selection before testing the model on
the target task. In experiments, we verify a high correlation between the
predicted performance-estimated using minimal data-and the actual performance
achieved after large-scale semi-supervised or self-supervised learning, thus
confirming the validity of the theory and the effectiveness of the evaluation
method.

</details>


### [490] [Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative](https://arxiv.org/abs/2508.07329)
*Tuo Zhang,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.LG

TL;DR: This paper introduces a method to efficiently deploy Mixture of Experts (MoE) large language models on resource-scarce edge devices through Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) excel in multiple tasks but face deployment challenges on resource-limited edge devices, particularly with the Mixture of Experts (MoE) model. The key challenges include quantization inaccuracy caused by outliers and balancing memory, latency, and throughput during inference.

Method: The authors propose (1) using a smoothed Hessian matrix for joint 8-bit quantization of activations and weights, mitigating outlier issues while ensuring compatibility with mainstream hardware, and (2) developing an expert-level collaborative offloading mechanism between CPU and GPU to optimize memory use and inference latency.

Result: The proposed method achieves near-full precision accuracy in low-bit quantization on datasets like Wikitext2 and C4 using mainstream large models such as OPT and Mixtral 8*7B. Significant resource savings include a 60% reduction in GPU memory usage and improved inference latency.

Conclusion: The study demonstrates that Hessian-Aware Quantization and expert-level collaborative inference can efficiently deploy MoE models on edge devices, maintaining high accuracy and reducing resource requirements.

Abstract: With the breakthrough progress of large language models (LLMs) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through sparse
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.

</details>


### [491] [Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants](https://arxiv.org/abs/2508.07333)
*Yuhao Liu,Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: The paper analyzes stochastic interpolants and provides finite-time error bounds for ODE numerical schemes, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: While stochastic interpolants hold promise for transforming various data distributions and enabling generative modeling, there is a lack of finite-time convergence guarantees for practical numerical implementations.

Method: The authors derive finite-time error bounds for the forward Euler and Heun's methods, conduct an iteration complexity analysis, and propose optimized computational schedules.

Result: They establish novel error bounds in total variation distance and optimize iteration complexities for specific stochastic interpolants, supported by numerical experiments.

Conclusion: The findings improve understanding of stochastic interpolants for generative modeling, offering practical, optimized numerical techniques with validated error bounds and computational efficiencies.

Abstract: Stochastic interpolants offer a robust framework for continuously
transforming samples between arbitrary data distributions, holding significant
promise for generative modeling. Despite their potential, rigorous finite-time
convergence guarantees for practical numerical schemes remain largely
unexplored. In this work, we address the finite-time convergence analysis of
numerical implementations for ordinary differential equations (ODEs) derived
from stochastic interpolants. Specifically, we establish novel finite-time
error bounds in total variation distance for two widely used numerical
integrators: the first-order forward Euler method and the second-order Heun's
method. Furthermore, our analysis on the iteration complexity of specific
stochastic interpolant constructions provides optimized schedules to enhance
computational efficiency. Our theoretical findings are corroborated by
numerical experiments, which validate the derived error bounds and complexity
analyses.

</details>


### [492] [ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis](https://arxiv.org/abs/2508.07345)
*Samiha Afaf Neha,Abir Ahammed Bhuiyan,Md. Ishrak Khan*

Main category: cs.LG

TL;DR: ProteoKnight offers a novel image-based encoding and achieves 90.8% accuracy in identifying Phage Virion Proteins (PVPs), along with uncertainty analysis for prediction confidence.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of Phage Virion Proteins, crucial structural elements in bacteriophages, faces challenges in computational annotation due to spatial limitations in current encoding methods.

Method: ProteoKnight adapts the DNA-Walk algorithm to encode protein sequences as images, utilizing pixel colors and sequence characteristics for classification via pre-trained convolutional neural networks with uncertainty analysis using Monte Carlo Dropout.

Result: Binary classification of PVPs achieved 90.8% accuracy, comparable to state-of-the-art methods, while revealing variability in prediction confidence based on protein class and sequence length.

Conclusion: ProteoKnight overcomes spatial information loss in prior methods, presenting a robust encoding and classification system with enhanced predictive accuracy and reliability.

Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is
essential for genomic studies due to their crucial role as structural elements
in bacteriophages. Computational tools, particularly machine learning, have
emerged for annotating phage protein sequences from high-throughput sequencing.
However, effective annotation requires specialized sequence encodings. Our
paper introduces ProteoKnight, a new image-based encoding method that addresses
spatial constraints in existing techniques, yielding competitive performance in
PVP classification using pre-trained convolutional neural networks.
Additionally, our study evaluates prediction uncertainty in binary PVP
classification through Monte Carlo Dropout (MCD). \textbf{Methods:}
ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,
incorporating pixel colors and adjusting walk distances to capture intricate
protein features. Encoded sequences were classified using multiple pre-trained
CNNs. Variance and entropy measures assessed prediction uncertainty across
proteins of various classes and lengths. \textbf{Results:} Our experiments
achieved 90.8% accuracy in binary classification, comparable to
state-of-the-art methods. Multi-class classification accuracy remains
suboptimal. Our uncertainty analysis unveils variability in prediction
confidence influenced by protein class and sequence length.
\textbf{Conclusions:} Our study surpasses frequency chaos game representation
(FCGR) by introducing novel image encoding that mitigates spatial information
loss limitations. Our classification technique yields accurate and robust PVP
predictions while identifying low-confidence predictions.

</details>


### [493] [Intrinsic training dynamics of deep neural networks](https://arxiv.org/abs/2508.07370)
*Sibylle Marcotte,Gabriel Peyré,Rémi Gribonval*

Main category: cs.LG

TL;DR: The paper investigates the conditions under which gradient flow in high-dimensional parameter spaces can be simplified into a lower-dimensional intrinsic gradient flow and applies this theory to ReLU networks and linear networks.


<details>
  <summary>Details</summary>
Motivation: To understand whether gradient-based training in deep learning can be simplified into lower-dimensional intrinsic structures, potentially unveiling new insights into implicit bias in high-dimensional models.

Method: The study introduces intrinsic dynamic properties, conservation laws, and an inclusion-based criterion to analyze when high-dimensional gradient flow reduces to lower dimensions. The theory is applied to ReLU and linear networks under various initializations.

Result: The authors illustrate that ReLU networks always allow intrinsic dynamics reduction when using path-lifting functions. For linear networks, they generalize the conditions under which such dimensionality reduction is possible, introducing the concept of relaxed balanced initializations.

Conclusion: Gradient flows in deep learning can be reduced to lower-dimensional dynamics under specific conditions, providing a foundation for understanding implicit bias and simplifying the study of neural network architectures.

Abstract: A fundamental challenge in the theory of deep learning is to understand
whether gradient-based training in high-dimensional parameter spaces can be
captured by simpler, lower-dimensional structures, leading to so-called
implicit bias. As a stepping stone, we study when a gradient flow on a
high-dimensional variable $\theta$ implies an intrinsic gradient flow on a
lower-dimensional variable $z = \phi(\theta)$, for an architecture-related
function $\phi$. We express a so-called intrinsic dynamic property and show how
it is related to the study of conservation laws associated with the
factorization $\phi$. This leads to a simple criterion based on the inclusion
of kernels of linear maps which yields a necessary condition for this property
to hold. We then apply our theory to general ReLU networks of arbitrary depth
and show that, for any initialization, it is possible to rewrite the flow as an
intrinsic dynamic in a lower dimension that depends only on $z$ and the
initialization, when $\phi$ is the so-called path-lifting. In the case of
linear networks with $\phi$ the product of weight matrices, so-called balanced
initializations are also known to enable such a dimensionality reduction; we
generalize this result to a broader class of {\em relaxed balanced}
initializations, showing that, in certain configurations, these are the
\emph{only} initializations that ensure the intrinsic dynamic property.
Finally, for the linear neural ODE associated with the limit of infinitely deep
linear networks, with relaxed balanced initialization, we explicitly express
the corresponding intrinsic dynamics.

</details>


### [494] [Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs](https://arxiv.org/abs/2508.07395)
*Behnoush Khavari,Mehran Shakerinava,Jayesh Khullar,Jerry Huang,François Rivest,Siamak Ravanbakhsh,Sarath Chandar*

Main category: cs.LG

TL;DR: Recent work reveals limitations in LRNN models for state-tracking tasks due to issues in transition matrices. This paper studies combining input-independent and non-negative SSMs and concludes they fail for parity tasks, emphasizing the need for input-dependence and negative eigenvalues.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve LRNN models for solving challenging state-tracking tasks like parity, which are not addressed due to constraints in existing transition matrices.

Method: The paper investigates the impact of combining input-independent and non-negative SSM layers using diagonal transition matrices and performs experimental analyses on models combining S4D and Mamba layers.

Result: The research demonstrates that such combinations still fail to solve simple state-tracking tasks like parity, confirming specific requirements for recurrence layers.

Conclusion: State-tracking tasks demand recurrence layers with both input-dependent matrices and negative eigenvalues, as confirmed by theoretical analysis and experiments.

Abstract: Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack
state-tracking capability due to either time-invariant transition matrices or
restricted eigenvalue ranges. To address this, input-dependent transition
matrices, particularly those that are complex or non-triangular, have been
proposed to enhance SSM performance on such tasks. While existing theorems
demonstrate that both input-independent and non-negative SSMs are incapable of
solving simple state-tracking tasks, such as parity, regardless of depth, they
do not explore whether combining these two types in a multilayer SSM could
help. We investigate this question for efficient SSMs with diagonal transition
matrices and show that such combinations still fail to solve parity. This
implies that a recurrence layer must both be input-dependent and include
negative eigenvalues. Our experiments support this conclusion by analyzing an
SSM model that combines S4D and Mamba layers.

</details>


### [495] [Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors](https://arxiv.org/abs/2508.07400)
*Mohamad Louai Shehab,Alperen Tercan,Necmiye Ozay*

Main category: cs.LG

TL;DR: The paper focuses on recovering time-varying reward functions using priors for sparsity or minimal feature representation, offering efficient algorithms for practical applications.


<details>
  <summary>Details</summary>
Motivation: The study addresses the ill-posed problem of recovering rewards from optimal policies or demonstrations in reinforcement learning, motivated by practical scenarios where rewards are parsimonious and prior knowledge is available.

Method: Two priors are considered: sparsity in reward changes and representation by minimal feature combinations. The authors solve the sparsity problem with a polynomial-time algorithm and tackle feature representation with rank minimization and convex optimization.

Result: Efficient optimization-based algorithms were derived for both reward identification approaches. The demonstrations confirm the accuracy and generalizability of the recovered rewards.

Conclusion: By leveraging priors on rewards, the paper offers practical methods for solving the ill-posed problem of reward identification in reinforcement learning, improving applicability in real-world scenarios.

Abstract: In this paper, we consider the problem of recovering time-varying reward
functions from either optimal policies or demonstrations coming from a max
entropy reinforcement learning problem. This problem is highly ill-posed
without additional assumptions on the underlying rewards. However, in many
applications, the rewards are indeed parsimonious, and some prior information
is available. We consider two such priors on the rewards: 1) rewards are mostly
constant and they change infrequently, 2) rewards can be represented by a
linear combination of a small number of feature functions. We first show that
the reward identification problem with the former prior can be recast as a
sparsification problem subject to linear constraints. Moreover, we give a
polynomial-time algorithm that solves this sparsification problem exactly.
Then, we show that identifying rewards representable with the minimum number of
features can be recast as a rank minimization problem subject to linear
constraints, for which convex relaxations of rank can be invoked. In both
cases, these observations lead to efficient optimization-based reward
identification algorithms. Several examples are given to demonstrate the
accuracy of the recovered rewards as well as their generalizability.

</details>


### [496] [Lightning Prediction under Uncertainty: DeepLight with Hazy Loss](https://arxiv.org/abs/2508.07428)
*Md Sultanul Arifin,Abu Nowshed Sakib,Yeasir Rayhan,Tanzima Hashem*

Main category: cs.LG

TL;DR: DeepLight is a deep learning model for predicting lightning occurrences with greater accuracy, addressing shortcomings in existing methods like computational expense and underutilized data.


<details>
  <summary>Details</summary>
Motivation: Lightning poses risks to human safety and economic stability, which are amplified by climate change. Improved prediction methods are needed to enable preventative measures.

Method: DeepLight utilizes a dual-encoder architecture with multi-source meteorological data and employs a novel Hazy Loss function to tackle spatio-temporal uncertainties.

Result: Extensive testing shows DeepLight outperforms state-of-the-art methods in lightning prediction, improving Equitable Threat Score (ETS) by 18%-30%.

Conclusion: DeepLight is effective for addressing the challenges of lightning prediction, making it a reliable tool for proactive meteorological risk management.

Abstract: Lightning, a common feature of severe meteorological conditions, poses
significant risks, from direct human injuries to substantial economic losses.
These risks are further exacerbated by climate change. Early and accurate
prediction of lightning would enable preventive measures to safeguard people,
protect property, and minimize economic losses. In this paper, we present
DeepLight, a novel deep learning architecture for predicting lightning
occurrences. Existing prediction models face several critical limitations: they
often struggle to capture the dynamic spatial context and inherent uncertainty
of lightning events, underutilize key observational data, such as radar
reflectivity and cloud properties, and rely heavily on Numerical Weather
Prediction (NWP) systems, which are both computationally expensive and highly
sensitive to parameter settings. To overcome these challenges, DeepLight
leverages multi-source meteorological data, including radar reflectivity, cloud
properties, and historical lightning occurrences through a dual-encoder
architecture. By employing multi-branch convolution techniques, it dynamically
captures spatial correlations across varying extents. Furthermore, its novel
Hazy Loss function explicitly addresses the spatio-temporal uncertainty of
lightning by penalizing deviations based on proximity to true events, enabling
the model to better learn patterns amidst randomness. Extensive experiments
show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over
state-of-the-art methods, establishing it as a robust solution for lightning
prediction.

</details>


### [497] [Unsupervised operator learning approach for dissipative equations via Onsager principle](https://arxiv.org/abs/2508.07440)
*Zhipeng Chang,Zhenye Wen,Xiaofei Zhao*

Main category: cs.LG

TL;DR: The proposed Deep Onsager Operator Learning (DOOL) method uses an unsupervised approach rooted in the Onsager variational principle to efficiently solve dissipative equations without labeled data.


<details>
  <summary>Details</summary>
Motivation: Current operator learning methods are computationally expensive as they require supervised training with high-fidelity labeled simulation data.

Method: DOOL uses the Onsager variational principle to train a deep operator network unsupervisedly by minimizing the Rayleighian functional and employs a spatiotemporal decoupling strategy for better efficiency.

Result: Numerical experiments verify DOOL's effectiveness and show it outperforms supervised methods like DeepONet and MIONet. Extensions allow it to address second-order dissipative wave models.

Conclusion: DOOL offers a more efficient and accurate approach for solving dissipative equations through unsupervised learning, bypassing the need for labeled training data.

Abstract: Existing operator learning methods rely on supervised training with
high-fidelity simulation data, introducing significant computational cost. In
this work, we propose the deep Onsager operator learning (DOOL) method, a novel
unsupervised framework for solving dissipative equations. Rooted in the Onsager
variational principle (OVP), DOOL trains a deep operator network by directly
minimizing the OVP-defined Rayleighian functional, requiring no labeled data,
and then proceeds in time explicitly through conservation/change laws for the
solution. Another key innovation here lies in the spatiotemporal decoupling
strategy: the operator's trunk network processes spatial coordinates
exclusively, thereby enhancing training efficiency, while integrated external
time stepping enables temporal extrapolation. Numerical experiments on typical
dissipative equations validate the effectiveness of the DOOL method, and
systematic comparisons with supervised DeepONet and MIONet demonstrate its
enhanced performance. Extensions are made to cover the second-order wave models
with dissipation that do not directly follow OVP.

</details>


### [498] [Stackelberg Coupling of Online Representation Learning and Reinforcement Learning](https://arxiv.org/abs/2508.07452)
*Fernando Martinez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: The paper introduces SCORER, a framework that applies game-theoretic dynamics to improve deep reinforcement learning by optimizing the interaction between perception and control networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning effective features from sparse reward signals in deep reinforcement learning without relying on complex auxiliary objectives or fully decoupling processes.

Method: The proposed SCORER framework models the interaction between perception and control networks as a Stackelberg game, where the perception network acts as the leader and the control network as the follower. A practical two-timescale algorithm approximates the game's equilibrium.

Result: SCORER improves sample efficiency and final performance when applied to standard DQN variants on benchmark tasks.

Conclusion: Principled game-theoretic design of perception-control dynamics can significantly enhance deep reinforcement learning performance without introducing additional complexity.

Abstract: Integrated, end-to-end learning of representations and policies remains a
cornerstone of deep reinforcement learning (RL). However, to address the
challenge of learning effective features from a sparse reward signal, recent
trends have shifted towards adding complex auxiliary objectives or fully
decoupling the two processes, often at the cost of increased design complexity.
This work proposes an alternative to both decoupling and naive end-to-end
learning, arguing that performance can be significantly improved by structuring
the interaction between distinct perception and control networks with a
principled, game-theoretic dynamic. We formalize this dynamic by introducing
the Stackelberg Coupled Representation and Reinforcement Learning (SCORER)
framework, which models the interaction between perception and control as a
Stackelberg game. The perception network (leader) strategically learns features
to benefit the control network (follower), whose own objective is to minimize
its Bellman error. We approximate the game's equilibrium with a practical
two-timescale algorithm. Applied to standard DQN variants on benchmark tasks,
SCORER improves sample efficiency and final performance. Our results show that
performance gains can be achieved through principled algorithmic design of the
perception-control dynamic, without requiring complex auxiliary objectives or
architectures.

</details>


### [499] [Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten](https://arxiv.org/abs/2508.07458)
*Wei Qian,Chenxu Zhao,Yangyi Li,Wenqian Ye,Mengdi Huai*

Main category: cs.LG

TL;DR: The paper introduces malicious unlearning attacks targeting prediction uncertainties in machine learning, showing their effectiveness against existing defenses.


<details>
  <summary>Details</summary>
Motivation: To explore vulnerabilities in predictive uncertainties of machine learning models amid growing use of uncertainty quantification and machine unlearning methods.

Method: The authors propose a novel class of malicious unlearning attacks designed to manipulate predictive uncertainty outputs and develop optimization frameworks to execute these attacks.

Result: Extensive experiments, including black-box scenarios, show that these attacks effectively manipulate predictive uncertainties and outperform traditional attacks focused on misclassification.

Conclusion: The study identifies a critical vulnerability in machine learning and highlights the ineffectiveness of existing defenses against these malicious unlearning attacks.

Abstract: Currently, various uncertainty quantification methods have been proposed to
provide certainty and probability estimates for deep learning models' label
predictions. Meanwhile, with the growing demand for the right to be forgotten,
machine unlearning has been extensively studied as a means to remove the impact
of requested sensitive data from a pre-trained model without retraining the
model from scratch. However, the vulnerabilities of such generated predictive
uncertainties with regard to dedicated malicious unlearning attacks remain
unexplored. To bridge this gap, for the first time, we propose a new class of
malicious unlearning attacks against predictive uncertainties, where the
adversary aims to cause the desired manipulations of specific predictive
uncertainty results. We also design novel optimization frameworks for our
attacks and conduct extensive experiments, including black-box scenarios.
Notably, our extensive experiments show that our attacks are more effective in
manipulating predictive uncertainties than traditional attacks that focus on
label misclassifications, and existing defenses against conventional attacks
are ineffective against our attacks.

</details>


### [500] [Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning](https://arxiv.org/abs/2508.07536)
*Tasfiq E. Alam,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.LG

TL;DR: The paper presents a physics-informed multimodal CNN integrating domain knowledge and transfer learning strategies to improve bearing fault classification under variable operating conditions.


<details>
  <summary>Details</summary>
Motivation: Ensure reliable rotating machinery operation by minimizing degraded model performance under variable conditions and domain shifts.

Method: The study introduces a multimodal CNN with late fusion of vibration and motor current signals, a physics-based feature extraction branch, and a novel physics-informed loss function. It evaluates three transfer learning strategies for generalizing performance.

Result: The physics-informed approach outperforms the baseline with higher accuracy, reduced false classifications, improved robustness, and up to 98% accuracy validated on another dataset. LAS strategy yielded the best generalization.

Conclusion: Integrating domain knowledge with data-driven methods provides robust, generalizable, and interpretable fault diagnosis applicable in real-world industrial scenarios.

Abstract: Accurate and interpretable bearing fault classification is critical for
ensuring the reliability of rotating machinery, particularly under variable
operating conditions where domain shifts can significantly degrade model
performance. This study proposes a physics-informed multimodal convolutional
neural network (CNN) with a late fusion architecture, integrating vibration and
motor current signals alongside a dedicated physics-based feature extraction
branch. The model incorporates a novel physics-informed loss function that
penalizes physically implausible predictions based on characteristic bearing
fault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency
Inner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive
experiments on the Paderborn University dataset demonstrate that the proposed
physics-informed approach consistently outperforms a non-physics-informed
baseline, achieving higher accuracy, reduced false classifications, and
improved robustness across multiple data splits. To address performance
degradation under unseen operating conditions, three transfer learning (TL)
strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy
(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS
yields the best generalization, with additional performance gains when combined
with physics-informed modeling. Validation on the KAIST bearing dataset
confirms the framework's cross-dataset applicability, achieving up to 98
percent accuracy. Statistical hypothesis testing further verifies significant
improvements (p < 0.01) in classification performance. The proposed framework
demonstrates the potential of integrating domain knowledge with data-driven
learning to achieve robust, interpretable, and generalizable fault diagnosis
for real-world industrial applications.

</details>


### [501] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: This paper addresses a scheduling problem for a multimodal machine learning system using remote sensors, proposing an index-based threshold policy to minimize inference error caused by the age of information (AoI).


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of dynamic, real-time inference in multimodal machine learning systems that rely on fresh but limited network-delivered data from remote sensors.

Method: The authors develop and prove the optimality of an index-based threshold policy for modality scheduling, which efficiently determines when to switch modalities based on the AoI functions and a shared threshold.

Result: The proposed policy reduces inference error by up to 55% compared to traditional round-robin and random scheduling methods, addressing general AoI functions and heterogeneous transmission times effectively.

Conclusion: The proposed scheduling policy optimizes accuracy in remote inference tasks by minimizing AoI-based errors, demonstrating significant potential to enhance task-specific inference accuracy under network resource constraints.

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [502] [Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression](https://arxiv.org/abs/2508.07571)
*Xingwu Chen,Miao Lu,Beining Wu,Difan Zou*

Main category: cs.LG

TL;DR: This paper explores the impact of randomness and sampling during language model inference, focusing on in-context linear regression.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between practical language model inference and theoretical analysis using randomness and sampling.

Method: A theoretical framework using noise injection and binary coefficient sampling for in-context linear regression.

Result: Detailed analysis of inference techniques and empirical results revealing insights into real-world inference behaviors.

Conclusion: The study fosters a deeper understanding of inference behaviors in language models through theoretical and empirical approaches.

Abstract: Using more test-time computation during language model inference, such as
generating more intermediate thoughts or sampling multiple candidate answers,
has proven effective in significantly improving model performance. This paper
takes an initial step toward bridging the gap between practical language model
inference and theoretical transformer analysis by incorporating randomness and
sampling. We focus on in-context linear regression with continuous/binary
coefficients, where our framework simulates language model decoding through
noise injection and binary coefficient sampling. Through this framework, we
provide detailed analyses of widely adopted inference techniques. Supported by
empirical results, our theoretical framework and analysis demonstrate the
potential for offering new insights into understanding inference behaviors in
real-world language models.

</details>


### [503] [When and how can inexact generative models still sample from the data manifold?](https://arxiv.org/abs/2508.07581)
*Nisha Chandramoorthy,Adriaan de Clercq*

Main category: cs.LG

TL;DR: This paper investigates why generative models with learning errors still produce samples that stay on the data's support. It identifies alignment of sensitive perturbation directions with data manifold boundaries as a key factor.


<details>
  <summary>Details</summary>
Motivation: To understand and theoretically explain the robustness in the support of data distribution observed in generative models despite having learning errors.

Method: The authors perform perturbation analysis on probability flows, explore Lyapunov vector alignment with tangent spaces, and derive a sufficient condition for robustness of generative models.

Result: The study finds that alignment of top Lyapunov vectors with tangent spaces at the data manifold's boundaries ensures robustness and accurate tangent bundle estimates in generative processes.

Conclusion: Robustness in generative models stems from specific dynamic alignments, and the findings provide theoretical insights to improve model reliability across various generative processes.

Abstract: A curious phenomenon observed in some dynamical generative models is the
following: despite learning errors in the score function or the drift vector
field, the generated samples appear to shift \emph{along} the support of the
data distribution but not \emph{away} from it. In this work, we investigate
this phenomenon of \emph{robustness of the support} by taking a dynamical
systems approach on the generating stochastic/deterministic process. Our
perturbation analysis of the probability flow reveals that infinitesimal
learning errors cause the predicted density to be different from the target
density only on the data manifold for a wide class of generative models.
Further, what is the dynamical mechanism that leads to the robustness of the
support? We show that the alignment of the top Lyapunov vectors (most sensitive
infinitesimal perturbation directions) with the tangent spaces along the
boundary of the data manifold leads to robustness and prove a sufficient
condition on the dynamics of the generating process to achieve this alignment.
Moreover, the alignment condition is efficient to compute and, in practice, for
robust generative models, automatically leads to accurate estimates of the
tangent bundle of the data manifold. Using a finite-time linear perturbation
analysis on samples paths as well as probability flows, our work complements
and extends existing works on obtaining theoretical guarantees for generative
models from a stochastic analysis, statistical learning and uncertainty
quantification points of view. Our results apply across different dynamical
generative models, such as conditional flow-matching and score-based generative
models, and for different target distributions that may or may not satisfy the
manifold hypothesis.

</details>


### [504] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: Klear-Reasoner is a reasoning model with advanced problem-solving capabilities and a detailed methodology, achieving high performance in reasoning tasks across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing inference models face reproducibility challenges due to incomplete disclosure of training details. This study aims to address these shortcomings with an in-depth examination of reasoning model workflows.

Method: The paper details a workflow involving long Chain-of-Thought supervised fine-tuning (long CoT SFT) and a newly proposed Gradient-Preserving clipping Policy Optimization (GPPO) to address challenges in reinforcement learning clipping mechanisms.

Result: Klear-Reasoner achieves exceptional scores in reasoning benchmarks, such as 90.5% on AIME 2024 and strong results on LiveCodeBench V5 and V6.

Conclusion: High-quality, focused data and innovative RL mechanisms like GPPO significantly improve model performance. Klear-Reasoner's methodology offers a pathway for reproducibility and advancement in reasoning models.

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [505] [Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo](https://arxiv.org/abs/2508.07631)
*Advait Parulekar,Litu Rout,Karthikeyan Shanmugam,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: This paper explores efficient posterior sampling for score-based generative models, introducing a method that approximates the posterior distribution under minimal assumptions.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the intractability of posterior sampling in tasks like image reconstruction and stylization, even though they exhibit empirical success.

Method: The authors propose a 'tilting' approach to bias a distribution towards a measurement, achieving closeness to the posterior in KL divergence and Fisher divergence under minimal assumptions.

Result: They demonstrate that their method can sample distributions consistently aligned with both the prior and the measurement, providing a tractable solution to approximate posterior sampling.

Conclusion: This work presents, for the first time, a polynomial-time formal method for approximate posterior sampling, balancing theoretical rigor and practical applicability.

Abstract: We study the problem of posterior sampling in the context of score based
generative models. We have a trained score network for a prior $p(x)$, a
measurement model $p(y|x)$, and are tasked with sampling from the posterior
$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)
under well-accepted computational hardness assumptions. Despite this, popular
algorithms for tasks such as image super-resolution, stylization, and
reconstruction enjoy empirical success. Rather than establishing distributional
assumptions or restricted settings under which exact posterior sampling is
tractable, we view this as a more general "tilting" problem of biasing a
distribution towards a measurement. Under minimal assumptions, we show that one
can tractably sample from a distribution that is simultaneously close to the
posterior of a noised prior in KL divergence and the true posterior in Fisher
divergence. Intuitively, this combination ensures that the resulting sample is
consistent with both the measurement and the prior. To the best of our
knowledge these are the first formal results for (approximate) posterior
sampling in polynomial time.

</details>


### [506] [Attribution Explanations for Deep Neural Networks: A Theoretical Perspective](https://arxiv.org/abs/2508.07636)
*Huiqi Deng,Hongbin Pei,Quanshi Zhang,Mengnan Du*

Main category: cs.LG

TL;DR: The paper addresses the challenges of ensuring faithfulness in attribution explanations for deep neural networks and highlights recent theoretical advances to overcome them.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve concerns regarding whether attribution methods accurately reflect the contribution of input variables to DNNs' decision-making, which affects their reliability.

Method: The paper reviews and categorizes recent theoretical developments into three areas: unification of attribution methods, clarification of foundational rationale, and rigorous faithfulness evaluation.

Result: The review highlights progress in understanding and improving attribution explanations and identifies insights to guide method selection and inspire further developments.

Conclusion: The paper summarizes the advancements and outlines open challenges to deepen understanding and address faithfulness in attribution methods for DNNs.

Abstract: Attribution explanation is a typical approach for explaining deep neural
networks (DNNs), inferring an importance or contribution score for each input
variable to the final output. In recent years, numerous attribution methods
have been developed to explain DNNs. However, a persistent concern remains
unresolved, i.e., whether and which attribution methods faithfully reflect the
actual contribution of input variables to the decision-making process. The
faithfulness issue undermines the reliability and practical utility of
attribution explanations. We argue that these concerns stem from three core
challenges. First, difficulties arise in comparing attribution methods due to
their unstructured heterogeneity, differences in heuristics, formulations, and
implementations that lack a unified organization. Second, most methods lack
solid theoretical underpinnings, with their rationales remaining absent,
ambiguous, or unverified. Third, empirically evaluating faithfulness is
challenging without ground truth. Recent theoretical advances provide a
promising way to tackle these challenges, attracting increasing attention. We
summarize these developments, with emphasis on three key directions: (i)
Theoretical unification, which uncovers commonalities and differences among
methods, enabling systematic comparisons; (ii) Theoretical rationale,
clarifying the foundations of existing methods; (iii) Theoretical evaluation,
rigorously proving whether methods satisfy faithfulness principles. Beyond a
comprehensive review, we provide insights into how these studies help deepen
theoretical understanding, inform method selection, and inspire new attribution
methods. We conclude with a discussion of promising open problems for further
work.

</details>


### [507] [Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs](https://arxiv.org/abs/2508.07637)
*Guanqun Ma,David Lenz,Hanqi Guo,Tom Peterka,Bei Wang*

Main category: cs.LG

TL;DR: The paper introduces a method to extract topological features from continuous implicit models, specifically multivariate functional approximations (MFA), without converting them into discrete data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance scientific data storage, transfer, and analysis by using continuous implicit models instead of traditional discrete data representations, allowing for smooth, high-order, and differentiable surrogates.

Method: The method involves extracting complex topological features such as contours, Jacobi sets, and ridge-valley graphs directly from MFA models, bypassing discrete data representations. The framework relies on querying function values and high-order derivatives.

Result: The proposed framework successfully extracts complex topological features directly from continuous implicit models, establishing a method applicable to models supporting high-order queries.

Conclusion: The paper lays the groundwork for performing topological data analysis and visualization on continuous implicit models, showcasing their potential for advanced scientific data applications.

Abstract: Implicit continuous models, such as functional models and implicit neural
networks, are an increasingly popular method for replacing discrete data
representations with continuous, high-order, and differentiable surrogates.
These models offer new perspectives on the storage, transfer, and analysis of
scientific data. In this paper, we introduce the first framework to directly
extract complex topological features -- contours, Jacobi sets, and ridge-valley
graphs -- from a type of continuous implicit model known as multivariate
functional approximation (MFA). MFA replaces discrete data with continuous
piecewise smooth functions. Given an MFA model as the input, our approach
enables direct extraction of complex topological features from the model,
without reverting to a discrete representation of the model. Our work is easily
generalizable to any continuous implicit model that supports the queries of
function values and high-order derivatives. Our work establishes the building
blocks for performing topological data analysis and visualization on implicit
continuous models.

</details>


### [508] [Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals](https://arxiv.org/abs/2508.07638)
*Jia Zhang,Yao Liu,Chen-Xi Zhang,Yi Liu,Yi-Xuan Jin,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: The paper introduces a robust and efficient data selection principle to fine-tune large language models based on fine-grained preferences, leveraging a novel metric called Preference Divergence.


<details>
  <summary>Details</summary>
Motivation: The need to align Large Language Models (LLMs) with diverse human values in a reliable and scalable manner, addressing challenges of noise and conflicts in preferences.

Method: The authors derive the Direct Multi-Preference Optimization (DMPO) objective and propose using the Preference Divergence (PD) metric for data selection, rather than direct optimization.

Result: Evaluations show that their method achieves over 10% relative improvement in performance compared to baseline strategies, while enhancing training efficiency.

Conclusion: The proposed PD selection method unlocks robust LLM alignment with fine-grained preferences, avoiding challenges like intractable holistic preference annotating and improving overall model efficiency.

Abstract: Aligning Large Language Models (LLMs) with diverse human values requires
moving beyond a single holistic "better-than" preference criterion. While
collecting fine-grained, aspect-specific preference data is more reliable and
scalable, existing methods like Direct Preference Optimization (DPO) struggle
with the severe noise and conflicts inherent in such aggregated datasets. In
this paper, we tackle this challenge from a data-centric perspective. We first
derive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a
key Preference Divergence (PD) term that quantifies inter-aspect preference
conflicts. Instead of using this term for direct optimization, we leverage it
to formulate a novel, theoretically-grounded data selection principle. Our
principle advocates for selecting a subset of high-consensus data-identified by
the most negative PD values-for efficient DPO training. We prove the optimality
of this strategy by analyzing the loss bounds of the DMPO objective in the
selection problem. To operationalize our approach, we introduce practical
methods of PD term estimation and length bias mitigation, thereby proposing our
PD selection method. Evaluation on the UltraFeedback dataset with three varying
conflict levels shows that our simple yet effective strategy achieves over 10%
relative improvement against both the standard holistic preference and a
stronger oracle using aggregated preference signals, all while boosting
training efficiency and obviating the need for intractable holistic preference
annotating, unlocking the potential of robust LLM alignment via fine-grained
preference signals.

</details>


### [509] [Multi-Turn Jailbreaks Are Simpler Than They Seem](https://arxiv.org/abs/2508.07646)
*Xiaoxue Yang,Jaeha Lee,Anna-Katharina Dick,Jasper Timm,Fei Xie,Diogo Cruz*

Main category: cs.LG

TL;DR: Multi-turn jailbreak attacks on large language models remain a significant vulnerability, often succeeding even against models optimized for single-turn protection.


<details>
  <summary>Details</summary>
Motivation: Address the persistent vulnerability of multi-turn jailbreak attacks in large language models that surpass single-turn defenses.

Method: Empirical analysis using the StrongREJECT benchmark to evaluate multi-turn jailbreak attacks on models like GPT-4, Claude, and Gemini variants.

Result: Multi-turn attacks are nearly as effective as repeated single-turn attempts, success correlates among similar models, and reasoning effort increases attack success rates.

Conclusion: Improved AI safety evaluation and the design of systems resistant to jailbreaks are crucial, especially with the noted vulnerabilities in multi-turn systems.

Abstract: While defenses against single-turn jailbreak attacks on Large Language Models
(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent
vulnerability, often achieving success rates exceeding 70% against models
optimized for single-turn protection. This work presents an empirical analysis
of automated multi-turn jailbreak attacks across state-of-the-art models
including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.
Our findings challenge the perceived sophistication of multi-turn attacks: when
accounting for the attacker's ability to learn from how models refuse harmful
requests, multi-turn jailbreaking approaches are approximately equivalent to
simply resampling single-turn attacks multiple times. Moreover, attack success
is correlated among similar models, making it easier to jailbreak newly
released ones. Additionally, for reasoning models, we find surprisingly that
higher reasoning effort often leads to higher attack success rates. Our results
have important implications for AI safety evaluation and the design of
jailbreak-resistant systems. We release the source code at
https://github.com/diogo-cruz/multi_turn_simpler

</details>


### [510] [Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning](https://arxiv.org/abs/2508.07659)
*Hyeon-Ju Jeon,Jeon-Ho Kang,In-Hyuk Kwon,O-Joun Lee*

Main category: cs.LG

TL;DR: The paper explores the use of spatiotemporal graph neural networks (STGNNs) with improved structure learning to enhance atmospheric forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance global atmospheric state estimation by uncovering spatial correlations between Earth observations and atmospheric states, areas that conventional numerical weather prediction systems struggle to address efficiently.

Method: They employ STGNNs with adaptive edge sampling, regulating node degrees and considering spatial distances to address issues such as structural information loss and over-smoothing caused by excessive edges.

Result: The proposed method demonstrated superior performance, especially in areas of high atmospheric variability, using real-world atmospheric and observation data from East Asia.

Conclusion: The adaptive structure learning approach in STGNNs effectively improves atmospheric forecasting accuracy, outperforming existing models even under challenging conditions.

Abstract: This study aims to discover spatial correlations between Earth observations
and atmospheric states to improve the forecasting accuracy of global
atmospheric state estimation, which are usually conducted using conventional
numerical weather prediction (NWP) systems and is the beginning of weather
forecasting. NWP systems predict future atmospheric states at fixed locations,
which are called NWP grid points, by analyzing previous atmospheric states and
newly acquired Earth observations without fixed locations. Thus, surrounding
meteorological context and the changing locations of the observations make
spatial correlations between atmospheric states and observations over time. To
handle complicated spatial correlations, which change dynamically, we employ
spatiotemporal graph neural networks (STGNNs) with structure learning. However,
structure learning has an inherent limitation that this can cause structural
information loss and over-smoothing problem by generating excessive edges. To
solve this problem, we regulate edge sampling by adaptively determining node
degrees and considering the spatial distances between NWP grid points and
observations. We validated the effectiveness of the proposed method by using
real-world atmospheric state and observation data from East Asia. Even in areas
with high atmospheric variability, the proposed method outperformed existing
STGNN models with and without structure learning.

</details>


### [511] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: GLiClass is introduced as a novel method for improving classification in AI applications, addressing challenges like efficiency, accuracy, and zero-shot/few-shot learning.


<details>
  <summary>Details</summary>
Motivation: To enhance classification efficiency and accuracy in AI applications, addressing shortcomings of generative LLMs, cross-encoders, and embedding-based approaches.

Method: Adaptation of the GLiNER architecture for sequence classification tasks and incorporation of proximal policy optimization (PPO) for multi-label classification.

Result: GLiClass achieves strong accuracy and efficiency, comparable to embedding-based methods, and maintains flexibility for zero-shot and few-shot learning scenarios.

Conclusion: GLiClass effectively solves key classification challenges in AI, advancing performance in data-sparse conditions and offering robust adaptability.

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [512] [AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting](https://arxiv.org/abs/2508.07668)
*Hyobin Park,Jinwook Jung,Minseok Seo,Hyunsoo Choi,Deukjae Cho,Sekil Park,Dong-Geol Choi*

Main category: cs.LG

TL;DR: AIS-LLM is a new framework integrating time-series AIS data with large language models to handle multiple maritime tasks (trajectory prediction, anomaly detection, collision risk assessment) simultaneously and effectively.


<details>
  <summary>Details</summary>
Motivation: Increased maritime traffic and mandatory AIS implementation demand efficient methods to monitor and analyze complex maritime scenarios, which existing task-specific approaches fail to address effectively.

Method: AIS-LLM introduces a framework comprising a Time-Series Encoder for AIS data, an LLM-based Prompt Encoder, a Cross-Modality Alignment Module for semantic alignment, and an LLM-based Multi-Task Decoder to simultaneously execute multiple maritime traffic tasks.

Result: AIS-LLM outperforms existing individual-task methods, demonstrating its superiority through experimental validation.

Conclusion: AIS-LLM advances maritime traffic management by enabling multi-task analysis and situational summaries within a unified system, demonstrating promise for smarter and more efficient operations.

Abstract: With the increase in maritime traffic and the mandatory implementation of the
Automatic Identification System (AIS), the importance and diversity of maritime
traffic analysis tasks based on AIS data, such as vessel trajectory prediction,
anomaly detection, and collision risk assessment, is rapidly growing. However,
existing approaches tend to address these tasks individually, making it
difficult to holistically consider complex maritime situations. To address this
limitation, we propose a novel framework, AIS-LLM, which integrates time-series
AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series
Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a
Cross-Modality Alignment Module for semantic alignment between time-series data
and textual prompts, and an LLM-based Multi-Task Decoder. This architecture
enables the simultaneous execution of three key tasks: trajectory prediction,
anomaly detection, and risk assessment of vessel collisions within a single
end-to-end system. Experimental results demonstrate that AIS-LLM outperforms
existing methods across individual tasks, validating its effectiveness.
Furthermore, by integratively analyzing task outputs to generate situation
summaries and briefings, AIS-LLM presents the potential for more intelligent
and efficient maritime traffic management.

</details>


### [513] [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/abs/2508.07675)
*Xutong Liu,Baran Atalar,Xiangxiang Dai,Jinhang Zuo,Siwei Wang,John C. S. Lui,Wei Chen,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: The paper proposes a learning-based framework to improve semantic caching in Large Language Model systems, addressing challenges like semantic mismatch costs and unknown query distributions.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have high inference costs and scalability challenges, necessitating efficient caching solutions beyond traditional methods that ignore semantic similarity.

Method: A principled framework is developed, encompassing offline optimization and online learning, with provably efficient algorithms to manage semantic cache eviction.

Result: Algorithms exhibit comparable or superior performance against baseline methods in experiments conducted on synthetic datasets.

Conclusion: The paper offers a theoretically grounded and adaptable solution for semantic caching, tackling uncertainties in practical query and cost distributions.

Abstract: Large Language Models (LLMs) are revolutionizing how users interact with
information systems, yet their high inference cost poses serious scalability
and sustainability challenges. Caching inference responses, allowing them to be
retrieved without another forward pass through the LLM, has emerged as one
possible solution. Traditional exact-match caching, however, overlooks the
semantic similarity between queries, leading to unnecessary recomputation.
Semantic caching addresses this by retrieving responses based on semantic
similarity, but introduces a fundamentally different cache eviction problem:
one must account for mismatch costs between incoming queries and cached
responses. Moreover, key system parameters, such as query arrival probabilities
and serving costs, are often unknown and must be learned over time. Existing
semantic caching methods are largely ad-hoc, lacking theoretical foundations
and unable to adapt to real-world uncertainty. In this paper, we present a
principled, learning-based framework for semantic cache eviction under unknown
query and cost distributions. We formulate both offline optimization and online
learning variants of the problem, and develop provably efficient algorithms
with state-of-the-art guarantees. We also evaluate our framework on a synthetic
dataset, showing that our proposed algorithms perform matching or superior
performance compared with baselines.

</details>


### [514] [MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation](https://arxiv.org/abs/2508.07681)
*Yooseok Lim,ByoungJun Jeon,Seong-A Park,Jisoo Lee,Sae Won Choi,Chang Wook Jeong,Ho-Geol Ryu,Hongyeol Lee,Hyun-Lim Yang*

Main category: cs.LG

TL;DR: The paper introduces the MORE-CLEAR framework, which applies multimodal offline reinforcement learning (RL) integrated with pre-trained large-scale language models (LLMs) for improved sepsis management in intensive care units.


<details>
  <summary>Details</summary>
Motivation: Sepsis management is highly challenging due to the complexity of interpreting clinical data and ensuring early detection. Existing RL approaches to sepsis rely mainly on structured data, lacking a comprehensive understanding of patient states, which this paper seeks to address.

Method: The paper proposes the MORE-CLEAR framework, combining pre-trained LLMs to extract semantic representations from clinical notes and multimodal data integration through gated fusion and cross-modal attention for better patient state representation.

Result: Using the MIMIC-III, MIMIC-IV, and a private dataset, the MORE-CLEAR framework demonstrated significant improvements in estimated survival rates and RL policy performance compared to single-modal RL approaches.

Conclusion: MORE-CLEAR is the first framework to effectively utilize LLMs within multimodal offline RL for medical applications, significantly enhancing patient state representation and enabling better sepsis treatment actions. This approach has the potential to improve treatment outcomes and management efficiency in sepsis.

Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ
dysfunction, making early detection and optimal management critical. Previous
reinforcement learning (RL) approaches to sepsis management rely primarily on
structured data, such as lab results or vital signs, and on a dearth of a
comprehensive understanding of the patient's condition. In this work, we
propose a Multimodal Offline REinforcement learning for Clinical notes
Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis
control in intensive care units. MORE-CLEAR employs pre-trained large-scale
language models (LLMs) to facilitate the extraction of rich semantic
representations from clinical notes, preserving clinical context and improving
patient state representation. Gated fusion and cross-modal attention allow
dynamic weight adjustment in the context of time and the effective integration
of multimodal data. Extensive cross-validation using two public (MIMIC-III and
MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly
improves estimated survival rate and policy performance compared to
single-modal RL approaches. To our knowledge, this is the first to leverage LLM
capabilities within a multimodal offline RL for better state representation in
medical applications. This approach can potentially expedite the treatment and
management of sepsis by enabling reinforcement learning models to propose
enhanced actions based on a more comprehensive understanding of patient
conditions.

</details>


### [515] [Semantic-Enhanced Time-Series Forecasting via Large Language Models](https://arxiv.org/abs/2508.07697)
*Hao Liu,Chun Yang,Zhang xiaoxing,Xiaobin Zhu*

Main category: cs.LG

TL;DR: This paper proposes a Semantic-Enhanced LLM (SE-LLM) to improve time series forecasting by bridging linguistic and time-series modality gaps and optimizing both long- and short-term dependency modeling.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for time series forecasting using large language models (LLMs) primarily focus on token-level alignment and overlook the intrinsic modality gap between language structures and time series data, limiting semantic representation.

Method: The proposed method introduces SE-LLM, which captures periodicity and anomalies in time-series data and embeds them into the semantic space. Additionally, a plugin module is incorporated into the self-attention mechanism to model long- and short-term dependencies, while freezing the LLM and reducing sequence dimensionality to lower computational costs.

Result: The SE-LLM demonstrates superior performance in time series forecasting, outperforming state-of-the-art (SOTA) methods in experiments.

Conclusion: SE-LLM effectively bridges the gap between linguistic modality and time series data patterns, optimizing both interpretability and computational efficiency for time-series analysis with LLMs.

Abstract: Time series forecasting plays a significant role in finance, energy,
meteorology, and IoT applications. Recent studies have leveraged the
generalization capabilities of large language models (LLMs) to adapt to time
series forecasting, achieving promising performance. However, existing studies
focus on token-level modal alignment, instead of bridging the intrinsic
modality gap between linguistic knowledge structures and time series data
patterns, greatly limiting the semantic representation. To address this issue,
we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent
periodicity and anomalous characteristics of time series to embed into the
semantic space to enhance the token embedding. This process enhances the
interpretability of tokens for LLMs, thereby activating the potential of LLMs
for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel
at capturing long-range dependencies but are weak at modeling short-term
anomalies in time-series data. Hence, we propose a plugin module embedded
within self-attention that models long-term and short-term dependencies to
effectively adapt LLMs to time-series analysis. Our approach freezes the LLM
and reduces the sequence dimensionality of tokens, greatly reducing
computational consumption. Experiments demonstrate the superiority performance
of our SE-LLM against the state-of-the-art (SOTA) methods.

</details>


### [516] [Energy Consumption in Parallel Neural Network Training](https://arxiv.org/abs/2508.07706)
*Philipp Huber,David Li,Juan Pedro Gutiérrez Hermosillo Muriedas,Deifilia Kieckhefen,Markus Götz,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: The paper investigates the energy consumption implications of data-parallel neural network training, showing that energy usage scales linearly with GPU hours but varies across models and hardware settings.


<details>
  <summary>Details</summary>
Motivation: To address the underestimated impact of energy consumption growth in neural network training due to increasing computational demands and parallelization.

Method: Conducted scaling experiments on ResNet50 and FourCastNet using data-parallel training, analyzed effects of GPU count, global and local batch sizes on performance, training time, and energy use.

Result: Energy consumption scales linearly with GPU resource utilization but is influenced by distinct factors like model type, hardware, and batch sample handling per GPU hour.

Conclusion: The findings advance understanding of neural network training energy dynamics, providing insights for sustainable AI research development.

Abstract: The increasing demand for computational resources of training neural networks
leads to a concerning growth in energy consumption. While parallelization has
enabled upscaling model and dataset sizes and accelerated training, its impact
on energy consumption is often overlooked. To close this research gap, we
conducted scaling experiments for data-parallel training of two models,
ResNet50 and FourCastNet, and evaluated the impact of parallelization
parameters, i.e., GPU count, global batch size, and local batch size, on
predictive performance, training time, and energy consumption. We show that
energy consumption scales approximately linearly with the consumed resources,
i.e., GPU hours; however, the respective scaling factor differs substantially
between distinct model trainings and hardware, and is systematically influenced
by the number of samples and gradient updates per GPU hour. Our results shed
light on the complex interplay of scaling up neural network training and can
inform future developments towards more sustainable AI research.

</details>


### [517] [Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer](https://arxiv.org/abs/2508.07710)
*Jingya Wang,Xin Deng,Wenjie Wei,Dehao Zhang,Shuai Wang,Qian Sun,Jieyuan Zhang,Hanwen Liu,Ning Xie,Malu Zhang*

Main category: cs.LG

TL;DR: The paper introduces a training-free framework to convert ANNs to Spiking Neural Networks for Transformer architectures, leveraging a novel MBE neuron design to handle nonlinear operations and minimize latency, achieving near-lossless conversion accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing ANN-to-SNN conversion methods for Transformers are costly, ineffective at handling nonlinear operations, and require additional fine-tuning, thus necessitating better solutions to enhance energy efficiency.

Method: The authors developed a Multi-basis Exponential Decay (MBE) neuron that employs exponential decay and multi-basis encoding for approximating nonlinear operations, eliminating the need for modifying pre-trained ANN weights.

Result: Extensive experiments show the proposed method works across CV, NLU, NLG tasks with popular Transformer models like ViT, RoBERTa, and GPT-2, achieving high conversion accuracy and lower latency.

Conclusion: The framework enables energy-efficient and scalable deployment of Spiking Transformers in practical applications, unlocking potential for real-world usage without compromising model performance.

Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a
promising approach for constructing energy-efficient Transformer architectures.
Compared to directly trained Spiking Transformers, ANN-to-SNN conversion
methods bypass the high training costs. However, existing methods still suffer
from notable limitations, failing to effectively handle nonlinear operations in
Transformer architectures and requiring additional fine-tuning processes for
pre-trained ANNs. To address these issues, we propose a high-performance and
training-free ANN-to-SNN conversion framework tailored for Transformer
architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)
neuron, which employs an exponential decay strategy and multi-basis encoding
method to efficiently approximate various nonlinear operations. It removes the
requirement for weight modifications in pre-trained ANNs. Extensive experiments
across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures
(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless
conversion accuracy with significantly lower latency. This provides a promising
pathway for the efficient and scalable deployment of Spiking Transformers in
real-world applications.

</details>


### [518] [Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations](https://arxiv.org/abs/2508.07722)
*Pietro Talli,Federico Mason,Federico Chiariotti,Andrea Zanella*

Main category: cs.LG

TL;DR: A novel architecture, HR3L, is presented to improve Reinforcement Learning over non-ideal wireless networks by encoding and decoding environmental states without gradient exchanges, leading to faster, more efficient training.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning struggles when agents operate with partial and intermittent information caused by lossy or delayed wireless communication channels.

Method: The HR3L architecture introduces a transmitter-receiver setup. The transmitter encodes meaningful state representations, and the receiver decodes messages to make decisions, without requiring gradient exchanges during training.

Result: HR3L demonstrated significant improvements in sample efficiency and the ability to adapt to various communication challenges such as packet loss, delays, and bandwidth limits.

Conclusion: HR3L provides an effective, efficient framework for RL applications in non-ideal wireless networks, outperforming existing methods while reducing communication overhead.

Abstract: In this work, we address the problem of training Reinforcement Learning (RL)
agents over communication networks. The RL paradigm requires the agent to
instantaneously perceive the state evolution to infer the effects of its
actions on the environment. This is impossible if the agent receives state
updates over lossy or delayed wireless systems and thus operates with partial
and intermittent information. In recent years, numerous frameworks have been
proposed to manage RL with imperfect feedback; however, they often offer
specific solutions with a substantial computational burden. To address these
limits, we propose a novel architecture, named Homomorphic Robust Remote
Reinforcement Learning (HR3L), that enables the training of remote RL agents
exchanging observations across a non-ideal wireless channel. HR3L considers two
units: the transmitter, which encodes meaningful representations of the
environment, and the receiver, which decodes these messages and performs
actions to maximize a reward signal. Importantly, HR3L does not require the
exchange of gradient information across the wireless channel, allowing for
quicker training and a lower communication overhead than state-of-the-art
solutions. Experimental results demonstrate that HR3L significantly outperforms
baseline methods in terms of sample efficiency and adapts to different
communication scenarios, including packet losses, delayed transmissions, and
capacity limitations.

</details>


### [519] [Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning](https://arxiv.org/abs/2508.07738)
*Jialu Zhou,Dianxi Shi,Shaowu Yang,Xinyu Wei,Mingyue Yang,Leqian Li,Mengzhu Wang,Chunping Qiu*

Main category: cs.LG

TL;DR: This paper introduces a novel method called Two-Level Routing Grouped Mixture-of-Experts (TRGE) to address challenges in Multi-Domain Continual Learning (MDCL) such as catastrophic forgetting and forward forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing Parameter-Efficient Fine-Tuning (PEFT) methods fail to effectively address catastrophic and forward forgetting in Multi-Domain Continual Learning, necessitating a better solution for handling the dual heterogeneity of shifting class sets and distributions.

Method: The proposed TRGE method dynamically expands a pre-trained CLIP model with expert groups to mitigate forgetting. It uses intra-group routing to limit routing complexity, inter-group policies for meaningful inter-task collaboration, and multimodal large language models to identify tasks accurately. It also fuses pre-trained and learned knowledge to dynamically handle unseen data.

Result: Extensive experiments show that TRGE achieves superior performance compared to other advanced methods while maintaining fewer trainable parameters.

Conclusion: The TRGE method effectively tackles catastrophic and forward forgetting in MDCL, and demonstrates the potential of expert group dynamics combined with multimodal LLMs for continual learning.

Abstract: Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential
tasks with shifting class sets and distribution. Despite the
Parameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual
heterogeneity, they still suffer from catastrophic forgetting and forward
forgetting. To address these challenges, we propose a Two-Level Routing Grouped
Mixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the
pre-trained CLIP model, assigning specific expert group for each task to
mitigate catastrophic forgetting. With the number of experts continually grows
in this process, TRGE maintains the static experts count within the group and
introduces the intra-group router to alleviate routing overfitting caused by
the increasing routing complexity. Meanwhile, we design an inter-group routing
policy based on task identifiers and task prototype distance, which dynamically
selects relevant expert groups and combines their outputs to enhance inter-task
collaboration. Secondly, to get the correct task identifiers, we leverage
Multimodal Large Language Models (MLLMs) which own powerful multimodal
comprehension capabilities to generate semantic task descriptions and recognize
the correct task identifier. Finally, to mitigate forward forgetting, we
dynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE
adapter based on training progress, leveraging both pre-trained and learned
knowledge. Through extensive experiments across various settings, our method
outperforms other advanced methods with fewer trainable parameters.

</details>


### [520] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: This paper introduces the GRAO (Group Relative Alignment Optimization) framework, which integrates strengths of supervised fine-tuning (SFT) and reinforcement learning (RL) to address challenges in language model alignment.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome two main challenges in current language model alignment approaches: inefficiency of supervised fine-tuning due to offline policy constraints, and low sample efficiency plus dependency on strong base models in reinforcement learning.

Method: The proposed GRAO method combines SFT and RL using three innovations: multi-sample generation for reward feedback, Group Direct Alignment Loss for intra-group relative weighting, and reference-aware parameter updates based on pairwise preferences.

Result: GRAO outperforms existing methods, yielding significant relative performance improvements of 57.70%, 17.65%, 7.95%, and 5.18% over SFT, DPO, PPO, and GRPO baselines, respectively, in alignment tasks.

Conclusion: This study provides a robust framework for language model alignment, offering both theoretical foundations and empirical validations for efficiently advancing language model capabilities.

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [521] [Sparse Probabilistic Graph Circuits](https://arxiv.org/abs/2508.07763)
*Martin Rektoris,Milan Papež,Václav Šmídl,Tomáš Pevný*

Main category: cs.LG

TL;DR: The paper introduces Sparse Probabilistic Graph Circuits (SPGCs), a scalable and efficient framework for tractable generative graph models.


<details>
  <summary>Details</summary>
Motivation: Current deep generative models for graphs are powerful but intractable for probabilistic inference, while existing tractable approaches like Probabilistic Graph Circuits are computationally expensive.

Method: The authors introduce SPGCs, which utilize sparse graph representations to reduce complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n + m)$, making them more efficient for large graphs.

Result: SPGCs maintain tractable inference, improve memory and speed efficiency, and perform comparably to intractable models in de novo drug design tasks.

Conclusion: SPGCs offer a promising solution for scalable and tractable graph generative modeling, addressing limitations in scalability and efficiency of prior approaches.

Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive
power thanks to very efficient and scalable neural networks. However, these
networks contain non-linearities that prevent analytical computation of many
standard probabilistic inference queries, i.e., these DGMs are considered
\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)
address this issue by enabling \emph{tractable} probabilistic inference, they
operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for
graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue,
we introduce Sparse PGCs, a new class of tractable generative models that
operate directly on sparse graph representation, reducing the complexity to
$\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the
context of de novo drug design, we empirically demonstrate that SPGCs retain
exact inference capabilities, improve memory efficiency and inference speed,
and match the performance of intractable DGMs in key metrics.

</details>


### [522] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: The paper introduces PAMA, an efficient algorithm for Multi-Objective Alignment in Large Language Models, addressing limitations in current methods like RLHF and overcoming computational challenges.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations of existing methods, like RLHF, which align LLMs to single reward objectives, hindering their ability to adapt to complex human preferences.

Method: The paper offers Pareto Multi-Objective Alignment (PAMA), leveraging convex optimization with a closed-form solution that scales computational complexity from O(n^2*d) to O(n).

Result: The research demonstrates that PAMA converges to Pareto stationary points effectively, through rigorous theoretical analysis and experiments on language models ranging from 125M to 7B parameters.

Conclusion: PAMA provides a scalable, practical solution for aligning LLMs with diverse objectives, surpassing challenges associated with previous methods and enabling more adaptable real-world deployments.

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [523] [Topological Feature Compression for Molecular Graph Neural Networks](https://arxiv.org/abs/2508.07807)
*Rahul Khorana*

Main category: cs.LG

TL;DR: The paper introduces a novel Graph Neural Network (GNN) that effectively balances predictive accuracy, interpretability, and computational efficiency by combining compressed higher-order topological signals with standard molecular features.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of extracting general chemical insights while balancing predictive accuracy, interpretability, and computational efficiency in molecular representation learning.

Method: A novel GNN architecture integrates higher-order topological signals with conventional molecular features to capture global geometric information while maintaining computational tractability and interpretability.

Result: The proposed model demonstrates superior performance, achieving high accuracy and robustness across various benchmarks, from small molecules to complex materials, using a parameter-efficient architecture.

Conclusion: The GNN approach successfully combines advanced topological signals with standard features, achieving state-of-the-art results in molecular representations, and the code has been made available for open-source use.

Abstract: Recent advances in molecular representation learning have produced highly
effective encodings of molecules for numerous cheminformatics and
bioinformatics tasks. However, extracting general chemical insight while
balancing predictive accuracy, interpretability, and computational efficiency
remains a major challenge. In this work, we introduce a novel Graph Neural
Network (GNN) architecture that combines compressed higher-order topological
signals with standard molecular features. Our approach captures global
geometric information while preserving computational tractability and
human-interpretable structure. We evaluate our model across a range of
benchmarks, from small-molecule datasets to complex material datasets, and
demonstrate superior performance using a parameter-efficient architecture. We
achieve the best performing results in both accuracy and robustness across
almost all benchmarks. We open source all code \footnote{All code and results
can be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.

</details>


### [524] [EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning](https://arxiv.org/abs/2508.07809)
*Huanyu Liu,Jia Li,Chang Yu,Taozhi Chen,Yihong Dong,Lecheng Wang,Hu XiaoLong,Ge Li*

Main category: cs.LG

TL;DR: The paper introduces EvoCoT, a curriculum learning framework for improving reasoning capabilities in large language models (LLMs) by creatively addressing sparse reward challenges in reinforcement learning-based optimization.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations in reinforcement learning-based enhancement of LLMs' reasoning capabilities, where sparse rewards on hard problems hinder learning efficiency and exploration.

Method: The method, named EvoCoT, employs a two-stage chain-of-thought reasoning optimization approach. It self-generates and verifies reasoning trajectories, then shortens them incrementally to control exploration space and enable learning on previously unsolvable problems.

Result: Experiments with various LLM families (Qwen, DeepSeek, Llama) demonstrate that EvoCoT improves reasoning accuracy, allows LLMs to solve harder problems, and works seamlessly with different RL fine-tuning methods.

Conclusion: EvoCoT proves effective in enhancing reasoning abilities of LLMs by addressing sparse rewards through a novel curriculum learning strategy, offering compatibility with multiple reinforcement learning methods and potential for broader research advancements.

Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (LLMs) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes sparse, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger LLMs for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
LLMs to stably learn from initially unsolved hard problems under sparse
rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables LLMs to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.

</details>


### [525] [Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow](https://arxiv.org/abs/2508.07841)
*Carlo Cena,Mauro Martini,Marcello Chiaberge*

Main category: cs.LG

TL;DR: The study integrates Physics-Informed Neural Networks (PINNs) into spacecraft attitude dynamics modeling and shows that such models enhance control accuracy, robustness, and stability, outperforming purely data-driven approaches.


<details>
  <summary>Details</summary>
Motivation: To improve upon the limitations of purely data-driven models, such as poor generalization and stability, when applied in spacecraft attitude control by leveraging physics-informed approaches.

Method: The study compares purely data-driven models and PINN-based models trained on simulated spacecraft data using the Real NVP neural network architecture with self-attention. The models were evaluated in Model Predictive Control (MPC) frameworks.

Result: The inclusion of physics-informed information improved model performance, reducing mean relative error by 27.08%, and provided up to 42.86% better control stability and robustness-to-noise compared to data-driven models.

Conclusion: Physics-informed learning methodologies significantly outperform purely data-driven techniques in spacecraft attitude control, enhancing robustness, stability, and accuracy of control frameworks.

Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model
Predictive Control (MPC) has emerged as a powerful strategy for these tasks,
relying on accurate models of the system dynamics to optimize control actions
over a prediction horizon. In scenarios where physics models are incomplete,
difficult to derive, or computationally expensive, machine learning offers a
flexible alternative by learning the system behavior directly from data.
However, purely data-driven models often struggle with generalization and
stability, especially when applied to inputs outside their training domain. To
address these limitations, we investigate the benefits of incorporating
Physics-Informed Neural Networks (PINNs) into the learning of spacecraft
attitude dynamics, comparing their performance with that of purely data-driven
approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network
architecture with a self-attention mechanism, we trained several models on
simulated data generated with the Basilisk simulator. Two training strategies
were considered: a purely data-driven baseline and a physics-informed variant
to improve robustness and stability. Our results demonstrate that the inclusion
of physics-based information significantly enhances the performance in terms of
the mean relative error of the best architectures found by 27.08%. These
advantages are particularly evident when the learned models are integrated into
an MPC framework, where PINN-based models consistently outperform their purely
data-driven counterparts in terms of control accuracy and robustness, yielding
improvements of up to 42.86% in performance stability error and increased
robustness-to-noise.

</details>


### [526] [Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant](https://arxiv.org/abs/2508.07887)
*Sabrina Namazova,Alessandra Brondetta,Younes Strittmatter,Matthew Nassar,Sebastian Musslick*

Main category: cs.LG

TL;DR: The paper discusses Centaur, an LLM fine-tuned on human experimental data, evaluating its potential as a participant simulator for generating human-like behavior in cognitive science experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to achieve a participant simulator in behavioral sciences that can reliably reproduce human-like behavior across tasks, enabling efficient experimental designs similar to the impact simulators have had in natural sciences.

Method: The authors review Centaur, a large language model fine-tuned on data from 160 human experiments, and assess its generative behavior and predictive accuracy against the criteria of a participant simulator.

Result: Centaur demonstrates strong predictive accuracy; however, its generative behavior deviates systematically from human-like behavior, making it unsuitable as a complete participant simulator.

Conclusion: While Centaur is a substantial progress in modeling human behavior, it currently falls short as a reliable participant simulator or a fully accurate cognitive model.

Abstract: Simulators have revolutionized scientific practice across the natural
sciences. By generating data that reliably approximate real-world phenomena,
they enable scientists to accelerate hypothesis testing and optimize
experimental designs. This is perhaps best illustrated by AlphaFold, a
Nobel-prize winning simulator in chemistry that predicts protein structures
from amino acid sequences, enabling rapid prototyping of molecular
interactions, drug targets, and protein functions. In the behavioral sciences,
a reliable participant simulator - a system capable of producing human-like
behavior across cognitive tasks - would represent a similarly transformative
advance. Recently, Binz et al. introduced Centaur, a large language model (LLM)
fine-tuned on human data from 160 experiments, proposing its use not only as a
model of cognition but also as a participant simulator for "in silico
prototyping of experimental studies", e.g., to advance automated cognitive
science. Here, we review the core criteria for a participant simulator and
assess how well Centaur meets them. Although Centaur demonstrates strong
predictive accuracy, its generative behavior - a critical criterion for a
participant simulator - systematically diverges from human data. This suggests
that, while Centaur is a significant step toward predicting human behavior, it
does not yet meet the standards of a reliable participant simulator or an
accurate model of cognition.

</details>


### [527] [Score Augmentation for Diffusion Models](https://arxiv.org/abs/2508.07926)
*Liang Hou,Yuan Gao,Boyuan Jiang,Xin Tao,Qi Yan,Renjie Liao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.LG

TL;DR: The study introduces a novel data augmentation framework, ScoreAug, tailored to diffusion models to counteract the challenges of overfitting in training, especially when data is limited.


<details>
  <summary>Details</summary>
Motivation: To tackle the problem of overfitting in diffusion models, specifically in scenarios with limited data availability, by designing an augmentation approach that aligns with the model's inherent mechanisms.

Method: The ScoreAug framework modifies noisy data during training rather than clean data, aligning with diffusion models' denoising mechanisms. It requires the denoiser to predict augmented versions of original targets using equivariant learning objectives.

Result: ScoreAug demonstrates significant performance improvements across benchmarks like CIFAR-10, FFHQ, AFHQv2, and ImageNet. It effectively mitigates overfitting while ensuring stable convergence, avoiding data leakage, and synergizing with existing augmentation techniques.

Conclusion: ScoreAug proves to be a robust solution for enhancing diffusion model training, offering scalability and adaptability under varying data conditions and improving results when combined with traditional augmentation techniques.

Abstract: Diffusion models have achieved remarkable success in generative modeling.
However, this study confirms the existence of overfitting in diffusion model
training, particularly in data-limited regimes. To address this challenge, we
propose Score Augmentation (ScoreAug), a novel data augmentation framework
specifically designed for diffusion models. Unlike conventional augmentation
approaches that operate on clean data, ScoreAug applies transformations to
noisy data, aligning with the inherent denoising mechanism of diffusion.
Crucially, ScoreAug further requires the denoiser to predict the augmentation
of the original target. This design establishes an equivariant learning
objective, enabling the denoiser to learn scores across varied denoising
spaces, thereby realizing what we term score augmentation. We also
theoretically analyze the relationship between scores in different spaces under
general transformations. In experiments, we extensively validate ScoreAug on
multiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with
results demonstrating significant performance improvements over baselines.
Notably, ScoreAug effectively mitigates overfitting across diverse scenarios,
such as varying data scales and model capacities, while exhibiting stable
convergence properties. Another advantage of ScoreAug over standard data
augmentation lies in its ability to circumvent data leakage issues under
certain conditions. Furthermore, we show that ScoreAug can be synergistically
combined with traditional data augmentation techniques to achieve additional
performance gains.

</details>


### [528] [Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting](https://arxiv.org/abs/2508.07927)
*Amal Saadallah,Abdulaziz Al-Ademi*

Main category: cs.LG

TL;DR: The paper addresses challenges in forecasting non-stationary time series by introducing a framework that adapts and selects deep neural networks (DNNs) based on evolving patterns.


<details>
  <summary>Details</summary>
Motivation: Time series often exhibit non-stationary behaviors where patterns evolve over time, presenting forecasting challenges. The paper aims to address these issues by improving DNN adaptability.

Method: The framework involves training a base DNN, segmenting patterns into distinct clusters, fine-tuning specialized versions for each cluster, matching input patterns during inference, and using concept drift detection to adapt to emerging changes.

Result: Significant performance improvements in forecasting were observed across various DNN architectures, including those in the GluonTS library.

Conclusion: The framework demonstrates generalizability and effectiveness in forecasting dynamic time series using specialized model adaptation and pattern tracking.

Abstract: Time series forecasting poses significant challenges in non-stationary
environments where underlying patterns evolve over time. In this work, we
propose a novel framework that enhances deep neural network (DNN) performance
by leveraging specialized model adaptation and selection. Initially, a base DNN
is trained offline on historical time series data. A reserved validation subset
is then segmented to extract and cluster the most dominant patterns within the
series, thereby identifying distinct regimes. For each identified cluster, the
base DNN is fine-tuned to produce a specialized version that captures unique
pattern characteristics. At inference, the most recent input is matched against
the cluster centroids, and the corresponding fine-tuned version is deployed
based on the closest similarity measure. Additionally, our approach integrates
a concept drift detection mechanism to identify and adapt to emerging patterns
caused by non-stationary behavior. The proposed framework is generalizable
across various DNN architectures and has demonstrated significant performance
gains on both traditional DNNs and recent advanced architectures implemented in
the GluonTS library.

</details>


### [529] [Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters](https://arxiv.org/abs/2508.07952)
*Richard J. Fawley,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: SHARK (Shapley Reweighted k-means) is a feature-weighted clustering algorithm using Shapley values to quantify feature relevance, outperforming existing methods, especially in noisy settings.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of traditional clustering algorithms that equally weight all features, which is ineffective in high-dimensional or noisy data scenarios.

Method: Proposed SHARK, a clustering algorithm using Shapley values from game theory to iteratively re-weight features based on their contribution, requiring no additional parameters beyond traditional k-means.

Result: Experiments demonstrated SHARK's robustness and accuracy in clustering tasks, particularly excelling under noisy conditions, outperforming other methods.

Conclusion: SHARK provides an effective and parameter-free approach to feature-weighted clustering, offering accuracy and robustness while addressing feature relevance efficiently.

Abstract: Clustering algorithms often assume all features contribute equally to the
data structure, an assumption that usually fails in high-dimensional or noisy
settings. Feature weighting methods can address this, but most require
additional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a
feature-weighted clustering algorithm motivated by the use of Shapley values
from cooperative game theory to quantify feature relevance, which requires no
additional parameters beyond those in $k$-means. We prove that the $k$-means
objective can be decomposed into a sum of per-feature Shapley values, providing
an axiomatic foundation for unsupervised feature relevance and reducing Shapley
computation from exponential to polynomial time. SHARK iteratively re-weights
features by the inverse of their Shapley contribution, emphasising informative
dimensions and down-weighting irrelevant ones. Experiments on synthetic and
real-world data sets show that SHARK consistently matches or outperforms
existing methods, achieving superior robustness and accuracy, particularly in
scenarios where noise may be present. Software:
https://github.com/rickfawley/shark.

</details>


### [530] [WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2508.07970)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Tingfeng Xian,Haoqiang Hong,Boqi Chen,Haotao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: The paper introduces WeChat-YATT, a scalable training framework addressing limitations in reinforcement learning from human feedback (RLHF) by enabling efficient workflows and resource management.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address scalability and efficiency issues in existing RLHF frameworks, particularly in managing complex multimodal systems and adapting to dynamic workloads.

Method: WeChat-YATT employs a parallel controller programming model for workflow orchestration and proposes a dynamic placement schema to optimize resource allocation and GPU utilization.

Result: Experimental scenarios demonstrate significant throughput improvements over state-of-the-art RLHF frameworks. Additionally, the system has been effectively deployed for large-scale WeChat applications.

Conclusion: WeChat-YATT is an effective, scalable, and efficient RLHF training framework, showing robustness in both experimental and real-world large-scale applications.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent
paradigm for training large language models and multimodal systems. Despite
notable advances enabled by existing RLHF training frameworks, significant
challenges remain in scaling to complex multimodal workflows and adapting to
dynamic workloads. In particular, current systems often encounter limitations
related to controller scalability when managing large models, as well as
inefficiencies in orchestrating intricate RLHF pipelines, especially in
scenarios that require dynamic sampling and resource allocation. In this paper,
we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,
scalable, and balanced RLHF training framework specifically designed to address
these challenges. WeChat-YATT features a parallel controller programming model
that enables flexible and efficient orchestration of complex RLHF workflows,
effectively mitigating the bottlenecks associated with centralized controller
architectures and facilitating scalability in large-scale data scenarios. In
addition, we propose a dynamic placement schema that adaptively partitions
computational resources and schedules workloads, thereby significantly reducing
hardware idle time and improving GPU utilization under variable training
conditions. We evaluate WeChat-YATT across a range of experimental scenarios,
demonstrating that it achieves substantial improvements in throughput compared
to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been
successfully deployed to train models supporting WeChat product features for a
large-scale user base, underscoring its effectiveness and robustness in
real-world applications.

</details>


### [531] [A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation](https://arxiv.org/abs/2508.08002)
*Hongxin Yu,Yibing Wang,Fengyue Jin,Meng Zhang,Anni Chen*

Main category: cs.LG

TL;DR: This paper introduces a novel approach for traffic state estimation (TSE) using a physics-informed deep operator network (PI-DeepONet) that outperforms existing methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance real-time traffic state estimation by bridging the gap between model-driven and data-driven approaches, uniting the strengths of both in a physics-informed neural network framework.

Method: The authors extend the PI-DeepONet architecture to incorporate features like 2D data input for CNN computations, nonlinear expansion layers, attention mechanisms, MIMO structures, and adaptive identification of traffic flow model parameters. Evaluations were conducted on urban traffic datasets.

Result: The proposed TSE method delivered superior performance compared to four baseline methods, achieving high precision in estimating traffic flow and mean speed across diverse traffic scenarios.

Conclusion: The paper concludes that the extended PI-DeepONet architecture offers a powerful and accurate tool for real-time traffic state estimation, effectively combining physical models and neural network-based approaches.

Abstract: Traffic state estimation (TSE) falls methodologically into three categories:
model-driven, data-driven, and model-data dual-driven. Model-driven TSE relies
on macroscopic traffic flow models originated from hydrodynamics. Data-driven
TSE leverages historical sensing data and employs statistical models or machine
learning methods to infer traffic state. Model-data dual-driven traffic state
estimation attempts to harness the strengths of both aspects to achieve more
accurate TSE. From the perspective of mathematical operator theory, TSE can be
viewed as a type of operator that maps available measurements of inerested
traffic state into unmeasured traffic state variables in real time. For the
first time this paper proposes to study real-time freeway TSE in the idea of
physics-informed deep operator network (PI-DeepONet), which is an
operator-oriented architecture embedding traffic flow models based on deep
neural networks. The paper has developed an extended architecture from the
original PI-DeepONet. The extended architecture is featured with: (1) the
acceptance of 2-D data input so as to support CNN-based computations; (2) the
introduction of a nonlinear expansion layer, an attention mechanism, and a MIMO
mechanism; (3) dedicated neural network design for adaptive identification of
traffic flow model parameters. A traffic state estimator built on the basis of
this extended PI-DeepONet architecture was evaluated with respect to a short
freeway stretch of NGSIM and a large-scale urban expressway in China, along
with other four baseline TSE methods. The evaluation results demonstrated that
this novel TSE method outperformed the baseline methods with high-precision
estimation results of flow and mean speed.

</details>


### [532] [Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP](https://arxiv.org/abs/2508.08005)
*Xiang Li,Shanshan Wang,Chenglong Xiao*

Main category: cs.LG

TL;DR: This paper proposes a dual-channel learning-based framework, combining traditional machine learning and Graph Neural Networks (GNNs), to better select algorithms for the Maximum Clique Problem (MCP).


<details>
  <summary>Details</summary>
Motivation: Despite previous studies on the MCP, no single algorithm consistently performs best across all instances, and little research has been done on algorithm selection based on instance features specifically tailored for the MCP.

Method: The authors construct a labeled dataset using four MCP algorithms and extract graph features. They evaluate traditional classifiers (e.g., RF, SVM, DT, KNN) and propose a dual-channel GAT-MLP model combining Graph Attention Networks (GAT) for local structure encoding with Multilayer Perceptron (MLP) for global feature modeling.

Result: Random Forest (RF) emerges as a reliable baseline among traditional classifiers. The developed GAT-MLP model demonstrates strong performance across all evaluation metrics, showcasing the utility of dual-channel architectures.

Conclusion: The paper establishes that dual-channel architectures combining GNNs and global features offer a promising approach for algorithm selection in the MCP, highlighting the potential of graph neural networks in this domain.

Abstract: Extensive experiments and prior studies show that no single maximum clique
algorithm consistently performs best across all instances, highlighting the
importance of selecting suitable algorithms based on instance features. Through
an extensive analysis of relevant studies, it is found that there is a lack of
research work concerning algorithm selection oriented toward the Maximum Clique
Problem (MCP). In this work, we propose a learning-based framework that
integrates both traditional machine learning and graph neural networks to
address this gap. We construct a labeled dataset by running four exact MCP
algorithms on a diverse collection of graph instances, accompanied by
structural and global statistical features extracted from each graph. We first
evaluate four conventional classifiers: Support Vector Machine (SVM), Random
Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple
dataset variants. Experimental results show that RF consistently shows strong
performance across metrics and dataset variants, making it a reliable baseline.
In addition, feature importance analysis indicates that connectivity and
topological structure are strong predictors of algorithm performance. Building
on these findings, we develop a dual-channel model named GAT-MLP, which
combines a Graph Attention Network (GAT) for local structural encoding with a
Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model
shows strong and consistent performance across all metrics. Our results
highlight the effectiveness of dual-channel architectures and the promise of
graph neural networks in combinatorial algorithm selection.

</details>


### [533] [Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks](https://arxiv.org/abs/2508.08013)
*Mohamad Assaad,Zeinab Nehme,Merouane Debbah*

Main category: cs.LG

TL;DR: The paper proposes two methods to make Federated Learning (FL) more communication-efficient by leveraging channel information and handling asynchronous devices.


<details>
  <summary>Details</summary>
Motivation: To address the significant communication overhead issue in Federated Learning, which limits its practicality in wireless systems.

Method: Two communication-efficient strategies are proposed: (1) using a zero-order optimization method with a two-point gradient estimator and (2) employing a first-order gradient computation. Both leverage channel information to reduce resource demands and address device asynchrony.

Result: The paper provides rigorous analytical frameworks for the proposed methods, deriving convergence guarantees and performance bounds.

Conclusion: The proposed methods improve efficiency in FL by leveraging wireless channel information and eliminating the need for additional resources while maintaining robust performance.

Abstract: Federated Learning (FL) is an emerging learning framework that enables edge
devices to collaboratively train ML models without sharing their local data. FL
faces, however, a significant challenge due to the high amount of information
that must be exchanged between the devices and the aggregator in the training
phase, which can exceed the limited capacity of wireless systems. In this
paper, two communication-efficient FL methods are considered where
communication overhead is reduced by communicating scalar values instead of
long vectors and by allowing high number of users to send information
simultaneously. The first approach employs a zero-order optimization technique
with two-point gradient estimator, while the second involves a first-order
gradient computation strategy. The novelty lies in leveraging channel
information in the learning algorithms, eliminating hence the need for
additional resources to acquire channel state information (CSI) and to remove
its impact, as well as in considering asynchronous devices. We provide a
rigorous analytical framework for the two methods, deriving convergence
guarantees and establishing appropriate performance bounds.

</details>


### [534] [Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles](https://arxiv.org/abs/2508.08034)
*Roksana Yahyaabadi,Ghazal Farhani,Taufiq Rahman,Soodeh Nikan,Abdullah Jirjees,Fadi Araji*

Main category: cs.LG

TL;DR: This study presents a scalable, data-driven method to accurately predict power consumption across ICE, EV, and HEV platforms using machine learning techniques, achieving high accuracy and low error rates.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for predicting power consumption are impractical for large-scale deployment due to reliance on specialized instruments or rigid physical models. A more scalable and efficient approach is needed.

Method: The study employs powertrain dynamic feature sets alongside machine learning (e.g., transformers and LSTMs) to estimate instantaneous and cumulative power consumption in various types of vehicles.

Result: Highly accurate models were developed, with errors in ICE models on the order of $10^{-3}$. Cumulative errors for EVs and HEVs were below 4.1% and 2.1%, respectively, demonstrating the effectiveness of the approach.

Conclusion: This data-driven approach is effective across different vehicle platforms and highlights the need for robust models, especially for advanced powertrain systems like EVs and HEVs due to their complex power management.

Abstract: Accurate power consumption prediction is crucial for improving efficiency and
reducing environmental impact, yet traditional methods relying on specialized
instruments or rigid physical models are impractical for large-scale,
real-world deployment. This study introduces a scalable data-driven method
using powertrain dynamic feature sets and both traditional machine learning and
deep neural networks to estimate instantaneous and cumulative power consumption
in internal combustion engine (ICE), electric vehicle (EV), and hybrid electric
vehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with
mean absolute error and root mean squared error on the order of $10^{-3}$, and
cumulative errors under 3%. Transformer and long short-term memory models
performed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,
respectively. Results confirm the approach's effectiveness across vehicles and
models. Uncertainty analysis revealed greater variability in EV and HEV
datasets than ICE, due to complex power management, emphasizing the need for
robust models for advanced powertrains.

</details>


### [535] [BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models](https://arxiv.org/abs/2508.08040)
*Maozhen Zhang,Mengnan Zhao,Bo Wang*

Main category: cs.LG

TL;DR: This paper introduces BadPromptFL, the first backdoor attack aimed at prompt-based federated learning in multimodal contrastive models, achieving high attack success rates.


<details>
  <summary>Details</summary>
Motivation: The security vulnerabilities of prompt-based federated learning models, specifically surrounding prompt aggregation under data privacy constraints, have remained unexamined.

Method: The authors propose BadPromptFL, where compromised clients inject poisoned prompt embeddings into the federated learning aggregation process, leveraging the characteristics of CLIP-style architectures.

Result: Experiments demonstrate that BadPromptFL achieves over 90% attack success rates with low visibility and minimal client participation in various datasets and protocols.

Conclusion: BadPromptFL highlights potential vulnerabilities in the security of prompt-based federated learning, emphasizing the need for robust defenses in real-world applications.

Abstract: Prompt-based tuning has emerged as a lightweight alternative to full
fine-tuning in large vision-language models, enabling efficient adaptation via
learned contextual prompts. This paradigm has recently been extended to
federated learning settings (e.g., PromptFL), where clients collaboratively
train prompts under data privacy constraints. However, the security
implications of prompt-based aggregation in federated multimodal learning
remain largely unexplored, leaving a critical attack surface unaddressed. In
this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack
targeting prompt-based federated learning in multimodal contrastive models. In
BadPromptFL, compromised clients jointly optimize local backdoor triggers and
prompt embeddings, injecting poisoned prompts into the global aggregation
process. These prompts are then propagated to benign clients, enabling
universal backdoor activation at inference without modifying model parameters.
Leveraging the contextual learning behavior of CLIP-style architectures,
BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal
visibility and limited client participation. Extensive experiments across
multiple datasets and aggregation protocols validate the effectiveness,
stealth, and generalizability of our attack, raising critical concerns about
the robustness of prompt-based federated learning in real-world deployments.

</details>


### [536] [On Understanding of the Dynamics of Model Capacity in Continual Learning](https://arxiv.org/abs/2508.08052)
*Supriyo Chakraborty,Krishnan Raghavan*

Main category: cs.LG

TL;DR: This paper introduces the concept of Effective Model Capacity (CLEMC) to address challenges in stability-plasticity for continual learning, providing both theoretical insights and experimental validation.


<details>
  <summary>Details</summary>
Motivation: The stability-plasticity dilemma in continual learning fundamentally affects a neural network’s ability to represent tasks effectively.

Method: The authors develop a difference equation to model interactions between the NN, task data, and optimization procedures, utilizing CLEMC to analyze and predict stability-plasticity dynamics.

Result: They demonstrate that effective capacity is non-stationary, diminishing when task distributions differ, regardless of NN architecture or optimization.

Conclusion: Neural networks face intrinsic limitations in task representation over evolving distributions, and understanding CLEMC provides insights into improving continual learning approaches.

Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN)
capacity-its ability to represent tasks-is a fundamental challenge in continual
learning (CL). Within this context, we introduce CL's effective model capacity
(CLEMC) that characterizes the dynamic behavior of the stability-plasticity
balance point. We develop a difference equation to model the evolution of the
interplay between the NN, task data, and optimization procedure. We then
leverage CLEMC to demonstrate that the effective capacity-and, by extension,
the stability-plasticity balance point is inherently non-stationary. We show
that regardless of the NN architecture or optimization method, a NN's ability
to represent new tasks diminishes when incoming task distributions differ from
previous ones. We conduct extensive experiments to support our theoretical
findings, spanning a range of architectures-from small feedforward network and
convolutional networks to medium-sized graph neural networks and
transformer-based large language models with millions of parameters.

</details>


### [537] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: The paper introduces a transfer learning-based predictive process monitoring (PPM) technique to enable predictions without requiring large amounts of data, useful for organizations with limited resources.


<details>
  <summary>Details</summary>
Motivation: Existing PPM approaches often require significant amounts of event data or other resources, which many organizations lack, precluding them from benefiting from PPM methods.

Method: The authors proposed a transfer-learning technique and validated its effectiveness by conducting numerical experiments using event logs from real-world IT service management processes in both intra- and inter-organizational contexts.

Result: Results reveal that knowledge from one business process could effectively transfer to another, enabling impactful PPM even in contexts with limited relevant resources.

Conclusion: Organizations can adopt the proposed technique to implement transfer learning, enabling predictive process monitoring even without extensive data, and utilize resources across organizational boundaries.

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [538] [C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction](https://arxiv.org/abs/2508.08071)
*Yunqing Li,Zixiang Tang,Jiaying Zhuang,Zhenyu Yang,Farhad Ameri,Jianbang Zhang*

Main category: cs.LG

TL;DR: The paper introduces PMGraph, a public benchmark of supply-chain graphs, and a novel Cascade Multimodal Attributed Graph (C-MAG) method for better linking of products with manufacturers.


<details>
  <summary>Details</summary>
Motivation: Managing resilient and efficient supply chains requires better methods to link products and manufacturers, overcoming traditional approaches that fail to handle complex attributes, certifications, and multimodal data.

Method: The paper introduces PMGraph, a dataset of supply-chain graphs, and proposes C-MAG, a two-stage architecture that aligns multimodal data and uses message passing for improved link prediction.

Result: C-MAG improves link prediction accuracy and offers modality-aware fusion approaches suitable for noisy, real-world supply-chain data.

Conclusion: PMGraph and C-MAG collectively advance capabilities in linking manufacturers and products within global supply chains, addressing gaps in complexity and multimodal data handling.

Abstract: Connecting an ever-expanding catalogue of products with suitable
manufacturers and suppliers is critical for resilient, efficient global supply
chains, yet traditional methods struggle to capture complex capabilities,
certifications, geographic constraints, and rich multimodal data of real-world
manufacturer profiles. To address these gaps, we introduce PMGraph, a public
benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking
8,888 manufacturers, over 70k products, more than 110k manufacturer-product
edges, and over 29k product images. Building on this benchmark, we propose the
Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first
aligns and aggregates textual and visual attributes into intermediate group
embeddings, then propagates them through a manufacturer-product hetero-graph
via multiscale message passing to enhance link prediction accuracy. C-MAG also
provides practical guidelines for modality-aware fusion, preserving predictive
performance in noisy, real-world settings.

</details>


### [539] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: The paper addresses challenges in applying reinforcement learning (RL) to reasoning tasks for large language models (LLMs), provides a systematic review and standardized framework, and proposes simplified guidance for practitioners.


<details>
  <summary>Details</summary>
Motivation: To resolve challenges in RL techniques for LLM reasoning, particularly the absence of standardized guidelines, diverging experimental setups, and unclear best practices.

Method: The authors systematically review RL techniques using rigorous reproductions and isolated evaluations within a unified framework, supported by fine-grained experiments across varying factors like datasets, model sizes, and architectures.

Result: The paper uncovers insights into RL technique mechanisms and effectiveness, while proposing a minimalist combination of two techniques that optimizes critic-free policies with vanilla PPO loss for consistent performance improvements.

Conclusion: Clear guidelines and a reliable roadmap are provided for practitioners, coupled with an effective minimalist solution that simplifies and advances RL applications in LLM reasoning.

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


### [540] [Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation](https://arxiv.org/abs/2508.08087)
*Amir Ali Panahi,Daniel Luder,Billy Wu,Gregory Offer,Dirk Uwe Sauer,Weihan Li*

Main category: cs.LG

TL;DR: The paper compares operator-learning surrogates of Single Particle Model (SPM) for lithium-ion batteries, proposing a novel PE-FNO model offering speed and accuracy benefits for digital twins.


<details>
  <summary>Details</summary>
Motivation: Achieve high physical fidelity and sub-millisecond speed for lithium-ion battery digital twins to meet real-time management and large-scale inference needs.

Method: Benchmark three operator-learning models: DeepONets, FNOs, and the proposed PE-FNO, using simulated trajectories across various current families and SOC ranges.

Result: DeepONets struggle with dynamic loads; FNO achieves high accuracy with <1% errors; PE-FNO generalizes well to parameters and runs 200x faster than traditional SPM solvers.

Conclusion: PE-FNO is a promising option for high-fidelity, real-time battery digital twins with superior speed and accuracy, enabling advanced battery management and experimentation.

Abstract: Reliable digital twins of lithium-ion batteries must achieve high physical
fidelity with sub-millisecond speed. In this work, we benchmark three
operator-learning surrogates for the Single Particle Model (SPM): Deep Operator
Networks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed
parameter-embedded Fourier Neural Operator (PE-FNO), which conditions each
spectral layer on particle radius and solid-phase diffusivity. Models are
trained on simulated trajectories spanning four current families (constant,
triangular, pulse-train, and Gaussian-random-field) and a full range of
State-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates
constant-current behaviour but struggles with more dynamic loads. The basic FNO
maintains mesh invariance and keeps concentration errors below 1 %, with
voltage mean-absolute errors under 1.7 mV across all load types. Introducing
parameter embedding marginally increases error, but enables generalisation to
varying radii and diffusivities. PE-FNO executes approximately 200 times faster
than a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse
tasks are explored in a parameter estimation task with Bayesian optimisation,
recovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute
percentage error, respectively, and 0.5918 percentage points higher error in
comparison with classical methods. These results pave the way for neural
operators to meet the accuracy, speed and parametric flexibility demands of
real-time battery management, design-of-experiments and large-scale inference.
PE-FNO outperforms conventional neural surrogates, offering a practical path
towards high-speed and high-fidelity electrochemical digital twins.

</details>


### [541] [Grid2Guide: A* Enabled Small Language Model for Indoor Navigation](https://arxiv.org/abs/2508.08100)
*Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: Grid2Guide combines the A* algorithm and a Small Language Model (SLM) to generate human-friendly indoor navigation instructions without requiring external signals or special infrastructure.


<details>
  <summary>Details</summary>
Motivation: To address challenges in indoor navigation where external signals and dedicated infrastructures are unavailable.

Method: The framework creates a binary occupancy matrix from an indoor map, uses the A* algorithm for optimal path computation, then translates the output into natural language instructions using an SLM.

Result: The proposed method was experimentally validated in various indoor scenarios, showing effectiveness, accuracy, and timely navigation guidance.

Conclusion: Grid2Guide provides a lightweight, infrastructure-free, and reliable solution for real-time indoor navigation using hybrid algorithmic and natural language processing techniques.

Abstract: Reliable indoor navigation remains a significant challenge in complex
environments, particularly where external positioning signals and dedicated
infrastructures are unavailable. This research presents Grid2Guide, a hybrid
navigation framework that combines the A* search algorithm with a Small
Language Model (SLM) to generate clear, human-readable route instructions. The
framework first conducts a binary occupancy matrix from a given indoor map.
Using this matrix, the A* algorithm computes the optimal path between origin
and destination, producing concise textual navigation steps. These steps are
then transformed into natural language instructions by the SLM, enhancing
interpretability for end users. Experimental evaluations across various indoor
scenarios demonstrate the method's effectiveness in producing accurate and
timely navigation guidance. The results validate the proposed approach as a
lightweight, infrastructure-free solution for real-time indoor navigation
support.

</details>


### [542] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: A novel integration of vision-based localization and large language model (LLM)-driven navigation successfully achieves indoor navigation accuracy without relying on GPS signals.


<details>
  <summary>Details</summary>
Motivation: Indoor navigation remains a significant challenge due to unreliable GPS signals and architectural complexities in enclosed environments.

Method: The study employs a ResNet-50 CNN for vision-based localization using smartphone camera input, combined with LLM-generated step-by-step navigation instructions based on floor plan interpretation.

Result: Localization achieved 96% accuracy under constrained viewing conditions, while navigation instructions from ChatGPT had an average accuracy of 75%, highlighting some limitations in zero-shot reasoning.

Conclusion: The research proves the viability of scalable indoor navigation using commonly available tools like cameras and floor maps, offering solutions for resource-limited environments.

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


### [543] [MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing](https://arxiv.org/abs/2508.08122)
*Mingrong Lin,Ke Deng,Zhengyang Wu,Zetao Zheng,Jie Li*

Main category: cs.LG

TL;DR: This paper introduces memoryKT, a novel knowledge tracing model leveraging temporal variational autoencoder to simulate real-world memory dynamics, including personalized forgetting patterns.


<details>
  <summary>Details</summary>
Motivation: Existing KT approaches overlook memory processes and personalized forgetting patterns, limiting performance and interpretability.

Method: The memoryKT model simulates memory dynamics using a three-stage process: distribution learning, feedback reconstruction, and personalized forgetting modulation.

Result: Experiments on four public datasets show memoryKT significantly outperforms state-of-the-art methods.

Conclusion: Incorporating memory processes and personalized forgetting patterns enhances KT models' performance and ability to capture individual differences in students' memory states.

Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery
from their historical interactions. Simulating students' memory states is a
promising approach to enhance both the performance and interpretability of
knowledge tracing models. Memory consists of three fundamental processes:
encoding, storage, and retrieval. Although forgetting primarily manifests
during the storage stage, most existing studies rely on a single,
undifferentiated forgetting mechanism, overlooking other memory processes as
well as personalized forgetting patterns. To address this, this paper proposes
memoryKT, a knowledge tracing model based on a novel temporal variational
autoencoder. The model simulates memory dynamics through a three-stage process:
(i) Learning the distribution of students' knowledge memory features, (ii)
Reconstructing their exercise feedback, while (iii) Embedding a personalized
forgetting module within the temporal workflow to dynamically modulate memory
storage strength. This jointly models the complete encoding-storage-retrieval
cycle, significantly enhancing the model's perception capability for individual
differences. Extensive experiments on four public datasets demonstrate that our
proposed approach significantly outperforms state-of-the-art baselines.

</details>


### [544] [NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection](https://arxiv.org/abs/2508.08124)
*Guanghao Jin,Yuan Liang,Yihan Ma,Jingpei Wu,Guoyang Liu*

Main category: cs.LG

TL;DR: NeuroDx-LM is a large-scale EEG-based model that leverages novel mechanisms to improve neurological disorder detection, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in deploying EEG-based large-scale models, including limited labeled data and suboptimal performance in clinical settings.

Method: The proposed model includes a Selective Temporal-Frequency Embedding mechanism for learning complex EEG patterns and a Progressive Feature-Aware Training strategy that refines features in two stages.

Result: NeuroDx-LM achieved state-of-the-art performance on the CHB-MIT and Schizophrenia datasets for seizure and schizophrenia detection.

Conclusion: The study highlights NeuroDx-LM's potential to significantly advance the clinical applicability of EEG-based large-scale models.

Abstract: Large-scale models pre-trained on Electroencephalography (EEG) have shown
promise in clinical applications such as neurological disorder detection.
However, the practical deployment of EEG-based large-scale models faces
critical challenges such as limited labeled EEG data and suboptimal performance
in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel
large-scale model specifically designed for detecting EEG-based neurological
disorders. Our key contributions include (i) a Selective Temporal-Frequency
Embedding mechanism that adaptively captures complex temporal and spectral
patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy
that refines feature representation in a two-stage process. In the first stage,
our model learns the fundamental discriminative features of EEG activities; in
the second stage, the model further extracts more specialized fine-grained
features for accurate diagnostic performance. We evaluated NeuroDx-LM on the
CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in
EEG-based seizure and schizophrenia detection, respectively. These results
demonstrate the great potential of EEG-based large-scale models to advance
clinical applicability. Our code is available at
https://github.com/LetItBe12345/NeuroDx-LM.

</details>


### [545] [OFAL: An Oracle-Free Active Learning Framework](https://arxiv.org/abs/2508.08126)
*Hadi Khorsand,Vahid Pourahmadi*

Main category: cs.LG

TL;DR: The paper proposes an oracle-free active learning approach called OFAL, leveraging neural network uncertainty to replace reliance on external labeling oracles.


<details>
  <summary>Details</summary>
Motivation: To reduce the cost and complexity of labeling data in active learning by eliminating the need for oracle involvement while achieving comparable or better results.

Method: OFAL separates and quantifies uncertainty using Monte Carlo Dropout for Bayesian approximations. It employs a variational autoencoder to generate new uncertain samples by navigating the latent space, starting with confident samples.

Result: The proposed method generates informative samples that contribute to active learning, enhancing the model's accuracy effectively.

Conclusion: OFAL demonstrates a promising oracle-free alternative to traditional active learning, integrating with existing sampling techniques to improve model performance.

Abstract: In the active learning paradigm, using an oracle to label data has always
been a complex and expensive task, and with the emersion of large unlabeled
data pools, it would be highly beneficial If we could achieve better results
without relying on an oracle. This research introduces OFAL, an oracle-free
active learning scheme that utilizes neural network uncertainty. OFAL uses the
model's own uncertainty to transform highly confident unlabeled samples into
informative uncertain samples. First, we start with separating and quantifying
different parts of uncertainty and introduce Monte Carlo Dropouts as an
approximation of the Bayesian Neural Network model. Secondly, by adding a
variational autoencoder, we go on to generate new uncertain samples by stepping
toward the uncertain part of latent space starting from a confidence seed
sample. By generating these new informative samples, we can perform active
learning and enhance the model's accuracy. Lastly, we try to compare and
integrate our method with other widely used active learning sampling methods.

</details>


### [546] [MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08137)
*Pravallika Abbineni,Saoud Aldowaish,Colin Liechty,Soroosh Noorzad,Ali Ghazizadeh,Morteza Fayazi*

Main category: cs.LG

TL;DR: The paper introduces MuaLLM, a multimodal Large Language Model developed for circuit design assistance, leveraging a novel framework for efficient literature retrieval and reasoning.


<details>
  <summary>Details</summary>
Motivation: The research seeks to address challenges in circuit design, including rapidly growing research, inconsistent data, and the complexity of optimization tasks, by creating a system for improved literature review and problem-solving.

Method: MuaLLM integrates a hybrid Retrieval-Augmented Generation (RAG) framework, a Reason + Act iterative workflow, and multimodal data processing to assist in circuit design by dynamically adapting its techniques with real-time updates.

Result: MuaLLM demonstrates strong performance with 90.1% recall on RAG-250 (retrieval/citation focus) and 86.8% accuracy on Reas-100 (multistep reasoning), while being up to 10x less costly and 1.6x faster than standard approaches.

Conclusion: MuaLLM provides scalable, efficient, and accurate solutions for circuit design support, overcoming limitations of traditional methods and enabling high-quality literature analysis and reasoning.

Abstract: Conducting a comprehensive literature review is crucial for advancing circuit
design methodologies. However, the rapid influx of state-of-the-art research,
inconsistent data representation, and the complexity of optimizing circuit
design objectives make this task significantly challenging. In this paper, we
propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for
circuit design assistance that integrates a hybrid Retrieval-Augmented
Generation (RAG) framework with an adaptive vector database of circuit design
research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +
Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step
information retrieval. It functions as a question-answering design assistant,
capable of interpreting complex queries and providing reasoned responses
grounded in circuit literature. Its multimodal capabilities enable processing
of both textual and visual data, facilitating more efficient and comprehensive
analysis. The system dynamically adapts using intelligent search tools,
automated document retrieval from the internet, and real-time database updates.
Unlike conventional approaches constrained by model context limits, MuaLLM
decouples retrieval from inference, enabling scalable reasoning over
arbitrarily large corpora. At the maximum context length supported by standard
LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining
the same accuracy. This allows rapid, no-human-in-the-loop database generation,
overcoming the bottleneck of simulation-based dataset creation for circuits. To
evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval
and citation performance, and Reasoning-100 (Reas-100), focused on multistep
reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%
accuracy on Reas-100.

</details>


### [547] [Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets](https://arxiv.org/abs/2508.08159)
*Cem Ata Baykara,Saurav Raj Pandey,Ali Burak Ünal,Harlin Lee,Mete Akgün*

Main category: cs.LG

TL;DR: This paper addresses seizure prediction challenges using EEG data from multiple locations through a balanced Federated Learning (FL) approach, promoting both accuracy and data privacy.


<details>
  <summary>Details</summary>
Motivation: To create accurate seizure prediction models using EEG data from multiple clinical sites while addressing patient privacy regulations and data heterogeneity.

Method: Implemented FL using a single EEG channel with privacy-preserving global normalization and proposed a Random Subset Aggregation strategy for balanced data contributions during training.

Result: The proposed method outperformed standard FL methods, especially for under-represented datasets, achieving a macro-average accuracy of 77.1% and pooled accuracy of 80.0%.

Conclusion: Balanced FL approaches like Random Subset Aggregation can effectively improve seizure prediction across diverse datasets while maintaining privacy, making them suitable for multi-hospital environments.

Abstract: Developing accurate and generalizable epileptic seizure prediction models
from electroencephalography (EEG) data across multiple clinical sites is
hindered by patient privacy regulations and significant data heterogeneity
(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving
framework for collaborative training, but standard aggregation methods like
Federated Averaging (FedAvg) can be biased by dominant datasets in
heterogeneous settings. This paper investigates FL for seizure prediction using
a single EEG channel across four diverse public datasets (Siena, CHB-MIT,
Helsinki, NCH), representing distinct patient populations (adult, pediatric,
neonate) and recording conditions. We implement privacy-preserving global
normalization and propose a Random Subset Aggregation strategy, where each
client trains on a fixed-size random subset of its data per round, ensuring
equal contribution during aggregation. Our results show that locally trained
models fail to generalize across sites, and standard weighted FedAvg yields
highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on
Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation
significantly improves performance on under-represented clients (accuracy
increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior
macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,
demonstrating a more robust and fair global model. This work highlights the
potential of balanced FL approaches for building effective and generalizable
seizure prediction systems in realistic, heterogeneous multi-hospital
environments while respecting data privacy.

</details>


### [548] [Neural Logic Networks for Interpretable Classification](https://arxiv.org/abs/2508.08172)
*Vincent Perreault,Katsumi Inoue,Richard Labib,Alain Hertz*

Main category: cs.LG

TL;DR: The paper generalizes Neural Logic Networks with NOT operations, introduces a factorized IF-THEN rule structure, and shows improved performance in Boolean networks and interpretable tabular classification.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitation of traditional neural networks in interpretability while enhancing the logical reasoning capability of Neural Logic Networks through a more comprehensive logical framework.

Method: The authors extended Neural Logic Networks with NOT operations, biases to consider unobserved data, and introduced a factorized IF-THEN rule structure paired with a modified learning algorithm.

Result: The proposed method advances the state-of-the-art in Boolean networks discovery and demonstrates the ability to learn interpretable rules, particularly in use cases such as medical tabular classification.

Conclusion: The proposed enhancements provide a significant step forward in creating interpretable and effective neural logic models, demonstrating practical impact in domains requiring clarity, like medical analysis.

Abstract: Traditional neural networks have an impressive classification performance,
but what they learn cannot be inspected, verified or extracted. Neural Logic
Networks on the other hand have an interpretable structure that enables them to
learn a logical mechanism relating the inputs and outputs with AND and OR
operations. We generalize these networks with NOT operations and biases that
take into account unobserved data and develop a rigorous logical and
probabilistic modeling in terms of concept combinations to motivate their use.
We also propose a novel factorized IF-THEN rule structure for the model as well
as a modified learning algorithm. Our method improves the state-of-the-art in
Boolean networks discovery and is able to learn relevant, interpretable rules
in tabular classification, notably on an example from the medical field where
interpretability has tangible value.

</details>


### [549] [Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion](https://arxiv.org/abs/2508.08216)
*Nicole Lai-Tan,Xiao Gu,Marios G. Philiastides,Fani Deligianni*

Main category: cs.LG

TL;DR: The paper introduces a novel method, Individual Tangent Space Alignment (ITSA), to enhance the generalizability of Brain-Computer Interfaces (BCIs) in music-based motor rehabilitation.


<details>
  <summary>Details</summary>
Motivation: Improve the adaptability and generalization of BCIs for personalized music-based motor rehabilitation, addressing challenges like inter-subject variability, movement artifacts, and calibration time.

Method: The ITSA pre-alignment strategy combines subject-specific recentering, distribution matching, and supervised rotational alignment alongside a hybrid architecture fusing Regularised Common Spatial Patterns (RCSP) and Riemannian geometry. Performance is validated using leave-one-subject-out cross-validation.

Result: ITSA significantly enhances cross-subject generalization in EEG signals, with the parallel fusion approach outperforming the sequential approach while maintaining robustness across varying conditions.

Conclusion: The ITSA framework effectively overcomes inter-subject differences in BCIs, enabling improved adaptability in music-based motor rehabilitation. The method and findings will be shared publicly.

Abstract: Personalised music-based interventions offer a powerful means of supporting
motor rehabilitation by dynamically tailoring auditory stimuli to provide
external timekeeping cues, modulate affective states, and stabilise gait
patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for
adapting these interventions across individuals. However, inter-subject
variability in EEG signals, further compounded by movement-induced artefacts
and motor planning differences, hinders the generalisability of BCIs and
results in lengthy calibration processes. We propose Individual Tangent Space
Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific
recentering, distribution matching, and supervised rotational alignment to
enhance cross-subject generalisation. Our hybrid architecture fuses Regularised
Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and
sequential configurations, improving class separability while maintaining the
geometric structure of covariance matrices for robust statistical computation.
Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant
performance improvements across subjects and conditions. The parallel fusion
approach shows the greatest enhancement over its sequential counterpart, with
robust performance maintained across varying data conditions and electrode
configurations. The code will be made publicly available at the time of
publication.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [550] [Reservoir computing with large valid prediction time for the Lorenz system](https://arxiv.org/abs/2508.06730)
*Lauren A Hurley,Sean E Shaheen*

Main category: cs.NE

TL;DR: The study examines how hyperparameters affect the Valid Prediction Time (VPT) in Reservoir Computers (RCs) and establishes relationships for optimal prediction performance, emphasizing solver choices.


<details>
  <summary>Details</summary>
Motivation: Understanding how to optimize prediction performance in dynamical systems using Reservoir Computers, especially for noiseless systems, underlines the importance of setting appropriate hyperparameters.

Method: Researchers analyzed hyperparameters like regularization coefficient, reservoir size, and spectral radius to correlate them with VPT, alongside evaluations using Lyapunov exponents and numerical solvers.

Result: Under ideal conditions, RCs achieved VPT values over 30 Lyapunov times with high prediction accuracy, particularly significant for noiseless datasets.

Conclusion: Performance in predicting noiseless systems can be optimized using proper hyperparameters and solver definitions, but caution must be exercised when extending results to noisy, real-world applications.

Abstract: We study the dependence of the Valid Prediction Time (VPT) of Reservoir
Computers (RCs) on hyperparameters including the regularization coefficient,
reservoir size, and spectral radius. Under carefully chosen conditions, the RC
can achieve approximately 70% of a benchmark performance, based on the output
of a single prediction step used as initial conditions for the Lorenz
equations. We report high VPT values (>30 Lyapunov times), as we are predicting
a noiseless system where overfitting can be beneficial. While these conditions
may not hold for noisy systems, they could still be useful for real-world
applications with limited noise. Furthermore, utilizing knowledge of the
Lyapunov exponent, we find that the VPT can be predicted by the error in the
first few prediction steps, offering a computationally efficient evaluation
method. We emphasize the importance of the numerical solver used to generate
the Lorenz dataset and define a Valid Ground Truth Time (VGTT), during which
the outputs of several common solvers agree. A VPT exceeding the VGTT is not
meaningful, as a different solver could produce a different result. Lastly, we
identify two spectral radius regimes that achieve large VPT: a small radius
near zero, resulting in simple but stable operation, and a larger radius
operating at the "edge of chaos."

</details>


### [551] [Geometry-Aware Spiking Graph Neural Network](https://arxiv.org/abs/2508.06793)
*Bowen Zhang,Genan Dai,Hu Huang,Long Lan*

Main category: cs.NE

TL;DR: This paper introduces a novel Geometry-Aware Spiking Graph Neural Network (GSG) that integrates spiking neural dynamics with adaptive representation learning on Riemannian manifolds for improved modeling of non-Euclidean graph structures.


<details>
  <summary>Details</summary>
Motivation: Existing spiking graph neural networks primarily operate in Euclidean space, limiting their ability to model complex graph structures such as hierarchies and cycles. The authors aim to overcome these geometric limitations.

Method: The proposed Geometry-Aware Spiking Graph Neural Network (GSG) consists of three main components: a Riemannian Embedding Layer for manifold projection, a Manifold Spiking Layer for geometry-consistent aggregation, and a Manifold Learning Objective involving jointly optimized classification and link prediction using geodesic distances.

Result: Experiments demonstrate that GSG achieves superior accuracy, robustness, and energy efficiency compared to state-of-the-art Euclidean Spiking Neural Networks and manifold-based Graph Neural Networks.

Conclusion: GSG establishes a new paradigm for curvature-aware and energy-efficient graph learning, unifying spike-based dynamics with non-Euclidean representation learning.

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive capabilities in
modeling graph-structured data, while Spiking Neural Networks (SNNs) offer high
energy efficiency through sparse, event-driven computation. However, existing
spiking GNNs predominantly operate in Euclidean space and rely on fixed
geometric assumptions, limiting their capacity to model complex graph
structures such as hierarchies and cycles. To overcome these limitations, we
propose \method{}, a novel Geometry-Aware Spiking Graph Neural Network that
unifies spike-based neural dynamics with adaptive representation learning on
Riemannian manifolds. \method{} features three key components: a Riemannian
Embedding Layer that projects node features into a pool of constant-curvature
manifolds, capturing non-Euclidean structures; a Manifold Spiking Layer that
models membrane potential evolution and spiking behavior in curved spaces via
geometry-consistent neighbor aggregation and curvature-based attention; and a
Manifold Learning Objective that enables instance-wise geometry adaptation
through jointly optimized classification and link prediction losses defined
over geodesic distances. All modules are trained using Riemannian SGD,
eliminating the need for backpropagation through time. Extensive experiments on
multiple benchmarks show that GSG achieves superior accuracy, robustness, and
energy efficiency compared to both Euclidean SNNs and manifold-based GNNs,
establishing a new paradigm for curvature-aware, energy-efficient graph
learning.

</details>


### [552] [Memory Enhanced Fractional-Order Dung Beetle Optimization for Photovoltaic Parameter Identification](https://arxiv.org/abs/2508.06841)
*Yiwei Li,Zhihua Allen-Zhao,Yuncheng Xu,Sanyang Liu*

Main category: cs.NE

TL;DR: The paper proposes an improved Memory Enhanced Fractional-Order Dung Beetle Optimization (MFO-DBO) algorithm to more accurately identify parameters in photovoltaic models, addressing challenges in convergence and solution quality.


<details>
  <summary>Details</summary>
Motivation: Accurate parameter identification in photovoltaic models is critical for their performance evaluation and poses algorithmic challenges due to their nonlinear, multimodal, and high-dimensional characteristics.

Method: The proposed MFO-DBO algorithm integrates fractional-order calculus for memory and improved convergence, fractional-order logistic chaotic map to enhance population diversity, and chaotic perturbation mechanisms to escape local optima.

Result: The MFO-DBO outperforms several state-of-the-art variants, competition winners, FO-based optimizers, and recent metaheuristics in terms of accuracy, robustness, convergence speed, and exploration-exploitation balance.

Conclusion: The MFO-DBO algorithm significantly improves parameter identification in photovoltaic models and offers a robust solution by addressing issues with standard DBO algorithms, enhancing performance evaluation.

Abstract: Accurate parameter identification in photovoltaic (PV) models is crucial for
performance evaluation but remains challenging due to their nonlinear,
multimodal, and high-dimensional nature. Although the Dung Beetle Optimization
(DBO) algorithm has shown potential in addressing such problems, it often
suffers from premature convergence. To overcome these issues, this paper
proposes a Memory Enhanced Fractional-Order Dung Beetle Optimization (MFO-DBO)
algorithm that integrates three coordinated strategies. Firstly,
fractional-order (FO) calculus introduces memory into the search process,
enhancing convergence stability and solution quality. Secondly, a
fractional-order logistic chaotic map improves population diversity during
initialization. Thirdly, a chaotic perturbation mechanism helps elite solutions
escape local optima. Numerical results on the CEC2017 benchmark suite and the
PV parameter identification problem demonstrate that MFO-DBO consistently
outperforms advanced DBO variants, CEC competition winners, FO-based
optimizers, enhanced classical algorithms, and recent metaheuristics in terms
of accuracy, robustness, convergence speed, while also maintaining an excellent
balance between exploration and exploitation compared to the standard DBO
algorithm.

</details>


### [553] [Enhancing Decision Space Diversity in Multi-Objective Evolutionary Optimization for the Diet Problem](https://arxiv.org/abs/2508.07077)
*Gustavo V. Nascimento,Ivan R. Meneghini,Valéria Santos,Eduardo Luz,Gladston Moreira*

Main category: cs.NE

TL;DR: The paper integrates a Hamming distance-based measure into MOEAs to enhance decision space diversity in optimization tasks like the diet problem.


<details>
  <summary>Details</summary>
Motivation: Balancing conflicting objectives in complex optimization problems often neglects decision space diversity, which is critical for offering diverse choices to decision-makers.

Method: A Hamming distance-based uniformity measure is directly incorporated into the selection mechanism of a MOEA to prioritize decision space diversity.

Result: The approach significantly improves decision space diversity in comparison with NSGA-II, while maintaining comparable performance in objective space.

Conclusion: The proposed method enhances decision space diversity in MOEAs and provides a generalizable strategy for decision space integration in optimization problems.

Abstract: Multi-objective evolutionary algorithms (MOEAs) are essential for solving
complex optimization problems, such as the diet problem, where balancing
conflicting objectives, like cost and nutritional content, is crucial. However,
most MOEAs focus on optimizing solutions in the objective space, often
neglecting the diversity of solutions in the decision space, which is critical
for providing decision-makers with a wide range of choices. This paper
introduces an approach that directly integrates a Hamming distance-based
measure of uniformity into the selection mechanism of a MOEA to enhance
decision space diversity. Experiments on a multi-objective formulation of the
diet problem demonstrate that our approach significantly improves decision
space diversity compared to NSGA-II, while maintaining comparable objective
space performance. The proposed method offers a generalizable strategy for
integrating decision space awareness into MOEAs.

</details>


### [554] [Evolutionary Optimization of Deep Learning Agents for Sparrow Mahjong](https://arxiv.org/abs/2508.07522)
*Jim O'Connor,Derin Gezgin,Gary B. Parker*

Main category: cs.NE

TL;DR: The paper introduces Evo-Sparrow, a deep learning-based evolutionary AI agent for strategic decision-making in Sparrow Mahjong. It combines LSTM with CMA-ES and outperforms random and rule-based agents while matching a PPO baseline.


<details>
  <summary>Details</summary>
Motivation: Develop a novel AI decision-making model for complex, stochastic, and partially observable games like Sparrow Mahjong that uses a computationally efficient alternative to traditional RL and gradient-based methods.

Method: The method leverages Long Short-Term Memory (LSTM) networks trained with Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for policy optimization and board state evaluation within a probabilistic game environment.

Result: Evo-Sparrow outperforms random and rule-based agents and achieves performance comparable to Proximal Policy Optimization (PPO), showcasing strong decision quality and strategic depth.

Conclusion: The proposed combination of deep learning and evolutionary strategies is a viable solution for complex game environments, with potential applications beyond Sparrow Mahjong, offering robust and computationally efficient AI decision-making capabilities.

Abstract: We present Evo-Sparrow, a deep learning-based agent for AI decision-making in
Sparrow Mahjong, trained by optimizing Long Short-Term Memory (LSTM) networks
using Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Our model
evaluates board states and optimizes decision policies in a non-deterministic,
partially observable game environment. Empirical analysis conducted over a
significant number of simulations demonstrates that our model outperforms both
random and rule-based agents, and achieves performance comparable to a Proximal
Policy Optimization (PPO) baseline, indicating strong strategic play and robust
policy quality. By combining deep learning with evolutionary optimization, our
approach provides a computationally effective alternative to traditional
reinforcement learning and gradient-based optimization methods. This research
contributes to the broader field of AI game playing, demonstrating the
viability of hybrid learning strategies for complex stochastic games. These
findings also offer potential applications in adaptive decision-making and
strategic AI development beyond Sparrow Mahjong.

</details>


### [555] [Energy and Quality of Surrogate-Assisted Search Algorithms: a First Analysis](https://arxiv.org/abs/2508.07691)
*Tomohiro Harada,Enrique Alba,Gabriel Luque*

Main category: cs.NE

TL;DR: This paper explores the energy efficiency and accuracy of surrogate-assisted metaheuristic algorithms, focusing on particle swarm optimization with surrogate models, including neural networks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limited understanding of how surrogates impact the energy profile, efficiency, and accuracy of metaheuristics used in real-world problem-solving.

Method: The authors analyze various versions of particle swarm optimization, incorporating pre-trained and retrained neural network-based surrogates, and examine their processor and memory energy profiles as well as surrogate accuracy.

Result: The analysis highlights the energy usage and performance characteristics of surrogate-assisted metaheuristics, advancing knowledge on how surrogates influence optimization and learning strategies.

Conclusion: This research introduces a new perspective for evaluating surrogate-assisted algorithms, emphasizing energy considerations alongside traditional metrics like efficiency and accuracy, paving the way for holistic assessments in algorithm development.

Abstract: Solving complex real problems often demands advanced algorithms, and then
continuous improvements in the internal operations of a search technique are
needed. Hybrid algorithms, parallel techniques, theoretical advances, and much
more are needed to transform a general search algorithm into an efficient,
useful one in practice. In this paper, we study how surrogates are helping
metaheuristics from an important and understudied point of view: their energy
profile. Even if surrogates are a great idea for substituting a time-demanding
complex fitness function, the energy profile, general efficiency, and accuracy
of the resulting surrogate-assisted metaheuristic still need considerable
research. In this work, we make a first step in analyzing particle swarm
optimization in different versions (including pre-trained and retrained neural
networks as surrogates) for its energy profile (for both processor and memory),
plus a further study on the surrogate accuracy to properly drive the search
towards an acceptable solution. Our conclusions shed new light on this topic
and could be understood as the first step towards a methodology for assessing
surrogate-assisted algorithms not only accounting for time or numerical
efficiency but also for energy and surrogate accuracy for a better, more
holistic characterization of optimization and learning techniques.

</details>


### [556] [Growing Reservoirs with Developmental Graph Cellular Automata](https://arxiv.org/abs/2508.08091)
*Matias Barandiaran,James Stovold*

Main category: cs.NE

TL;DR: Developmental Graph Cellular Automata (DGCA) model grow directed graphs. They outperform standard reservoirs in benchmark tasks, enabling adaptive morphogenesis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore methods to create adaptive, functional systems by growing directed graphs using DGCA to outperform existing benchmarks and model morphogenesis.

Method: The authors use DGCA trained with task-driven and task-independent targets to produce specialized structures capable of solving benchmark tasks.

Result: DGCAs effectively grow reservoirs that statistically outperform typical reservoirs in solving benchmark tasks, demonstrating life-like specialization.

Conclusion: DGCA systems show potential for developing adaptive, plastic reservoirs and providing computational models for functional morphogenesis.

Abstract: Developmental Graph Cellular Automata (DGCA) are a novel model for
morphogenesis, capable of growing directed graphs from single-node seeds. In
this paper, we show that DGCAs can be trained to grow reservoirs. Reservoirs
are grown with two types of targets: task-driven (using the NARMA family of
tasks) and task-independent (using reservoir metrics).
  Results show that DGCAs are able to grow into a variety of specialized,
life-like structures capable of effectively solving benchmark tasks,
statistically outperforming `typical' reservoirs on the same task. Overall,
these lay the foundation for the development of DGCA systems that produce
plastic reservoirs and for modeling functional, adaptive morphogenesis.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [557] [Checking Consistency of Event-driven Traces](https://arxiv.org/abs/2508.07855)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,R. Govind,Samuel Grahn,Ramanathan S. Thinniyam*

Main category: cs.PL

TL;DR: This paper proposes and analyzes a formal method to address the consistency problem in event-driven programming, highlighting computational complexity and providing a polynomial-time solution under specific conditions.


<details>
  <summary>Details</summary>
Motivation: Event-driven programming often faces the challenge of verifying if executions comply with its semantics, especially due to the interleaving of handlers and shared messaging among threads.

Method: The authors introduce an axiomatic semantics approach based on execution traces and prove its equivalence with operational semantics, enabling a reformulation of the consistency problem for better computational analysis.

Result: The general consistency problem for event-driven programs is NP-complete, but the problem becomes solvable in polynomial time when nested message postings are absent.

Conclusion: This work not only defines and analyzes the consistency problem in event-driven systems but also offers a practical solution for specific cases, validated by a prototype tool and benchmark tests.

Abstract: Event-driven programming is a popular paradigm where the flow of execution is
controlled by two features: (1) shared memory and (2) sending and receiving of
messages between multiple handler threads (just called handler). Each handler
has a mailbox (modelled as a queue) for receiving messages, with the constraint
that the handler processes its messages sequentially. Executions of messages by
different handlers may be interleaved. A central problem in this setting is
checking whether a candidate execution is consistent with the semantics of
event-driven programs. In this paper, we propose an axiomatic semantics for
eventdriven programs based on the standard notion of traces (also known as
execution graphs). We prove the equivalence of axiomatic and operational
semantics. This allows us to rephrase the consistency problem axiomatically,
resulting in the event-driven consistency problem: checking whether a given
trace is consistent. We analyze the computational complexity of this problem
and show that it is NP-complete, even when the number of handler threads is
bounded. We then identify a tractable fragment: in the absence of nested
posting, where handlers do not post new messages while processing a message,
consistency checking can be performed in polynomial time. Finally, we implement
our approach in a prototype tool and report on experimental results on a wide
range of benchmarks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [558] [Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing](https://arxiv.org/abs/2508.06518)
*Ray Wai Man Kong*

Main category: cs.RO

TL;DR: This paper introduces an automated folding and sewing machine for pleated pants, addressing inefficiencies of manual processes.


<details>
  <summary>Details</summary>
Motivation: The apparel industry faces challenges with labour-intensive, inconsistent manual pleating, necessitating automation.

Method: The paper designs a precision folding mechanism integrated with automated sewing, featuring real-time monitoring.

Result: The automated system reduced labour time by 93%, machinery time by 73%, and increased output by 72%.

Conclusion: The automation improves efficiency, reduces costs and waste, and aligns with sustainability trends in garment production.

Abstract: The applied research is the design and development of an automated folding
and sewing machine for pleated pants. It represents a significant advancement
in addressing the challenges associated with manual sewing processes.
Traditional methods for creating pleats are labour-intensive, prone to
inconsistencies, and require high levels of skill, making automation a critical
need in the apparel industry. This research explores the technical feasibility
and operational benefits of integrating advanced technologies into garment
production, focusing on the creation of an automated machine capable of precise
folding and sewing operations and eliminating the marking operation.
  The proposed machine incorporates key features such as a precision folding
mechanism integrated into the automated sewing unit with real-time monitoring
capabilities. The results demonstrate remarkable improvements: the standard
labour time has been reduced by 93%, dropping from 117 seconds per piece to
just 8 seconds with the automated system. Similarly, machinery time improved by
73%, and the total output rate increased by 72%. These enhancements translate
into a cycle time reduction from 117 seconds per piece to an impressive 33
seconds, enabling manufacturers to meet customer demand more swiftly. By
eliminating manual marking processes, the machine not only reduces labour costs
but also minimizes waste through consistent pleat formation. This automation
aligns with industry trends toward sustainability and efficiency, potentially
reducing environmental impact by decreasing material waste and energy
consumption.

</details>


### [559] [Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator](https://arxiv.org/abs/2508.06520)
*Liwei Chen,Tong Qin,Zhenhua Huangfu,Li Li,Wei Wei*

Main category: cs.RO

TL;DR: This paper introduces a novel optimization framework for reusable spacecraft landing trajectories using neural networks and differentiable solvers.


<details>
  <summary>Details</summary>
Motivation: Designing optimal landing trajectories for reusable spacecraft is challenging due to aerodynamic complexities and the need to meet physical constraints.

Method: A deep neural network surrogate and differentiable rigid-body dynamics solver are used for end-to-end gradient-based trajectory optimization.

Result: The framework successfully optimizes complex spacecraft maneuvers, handling nonlinearities and physical constraints.

Conclusion: The proposed framework establishes a basis for advanced trajectory optimization, enabling further explorations in unsteady aerodynamics and intelligent guidance.

Abstract: We propose a differentiable optimization framework for flip-and-landing
trajectory design of reusable spacecraft, exemplified by the Starship vehicle.
A deep neural network surrogate, trained on high-fidelity CFD data, predicts
aerodynamic forces and moments, and is tightly coupled with a differentiable
rigid-body dynamics solver. This enables end-to-end gradient-based trajectory
optimization without linearization or convex relaxation. The framework handles
actuator limits and terminal landing constraints, producing physically
consistent, optimized control sequences. Both standard automatic
differentiation and Neural ODEs are applied to support long-horizon rollouts.
Results demonstrate the framework's effectiveness in modeling and optimizing
complex maneuvers with high nonlinearities. This work lays the groundwork for
future extensions involving unsteady aerodynamics, plume interactions, and
intelligent guidance design.

</details>


### [560] [Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments](https://arxiv.org/abs/2508.06521)
*H. Liu,L. S. Moreu,T. S. Andersen,V. V. Puche,M. Fumagalli*

Main category: cs.RO

TL;DR: The paper introduces the Stinger Robot, a compact robotic platform designed for autonomous drilling in abandoned, infrastructure-less underground mines.


<details>
  <summary>Details</summary>
Motivation: There's a growing need for critical raw materials, which necessitates revisiting abandoned mines that traditional drilling machinery cannot access.

Method: The robot employs a tri-leg bracing mechanism for stability on irregular surfaces paired with a force-aware, closed-loop control strategy implemented in ROS 2.

Result: Simulation and hardware tests show that the Stinger Robot successfully drills in inaccessible underground conditions.

Conclusion: The work is the first validated system integrating force-bracing and autonomous drilling for underground mining, paving the way for modular robotic mining operations.

Abstract: The increasing demand for critical raw materials has revitalized interest in
abandoned underground mines, which pose extreme challenges for conventional
drilling machinery due to confined, unstructured, and infrastructure-less
environments. This paper presents the Stinger Robot, a novel compact robotic
platform specifically designed for autonomous high-force drilling in such
settings. The robot features a mechanically self-locking tri-leg bracing
mechanism that enables stable anchoring to irregular tunnel surfaces. A key
innovation lies in its force-aware, closed-loop control strategy, which enables
force interaction with unstructured environments during bracing and drilling.
Implemented as a finite-state machine in ROS 2, the control policy dynamically
adapts leg deployment based on real-time contact feedback and load thresholds,
ensuring stability without external supports. We demonstrate, through
simulation and preliminary hardware tests, that the Stinger Robot can
autonomously stabilize and drill in conditions previously inaccessible to
nowadays mining machines. This work constitutes the first validated robotic
architecture to integrate distributed force-bracing and autonomous drilling in
underground environments, laying the groundwork for future collaborative mining
operations using modular robot systems.

</details>


### [561] [MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving](https://arxiv.org/abs/2508.06534)
*Aishan Liu,Jiakai Wang,Tianyuan Zhang,Hainan Li,Jiangfan Liu,Siyuan Liang,Yilong Ren,Xianglong Liu,Dacheng Tao*

Main category: cs.RO

TL;DR: MetAdv is an innovative platform for evaluating the adversarial robustness of autonomous driving (AD) systems by integrating virtual simulations with physical vehicle feedback for a comprehensive and dynamic analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical and unresolved challenge of evaluating and ensuring the robustness of autonomous driving systems in adversarial conditions.

Method: MetAdv uses a hybrid virtual-physical testing sandbox with a three-layer closed-loop testing environment. This facilitates adversarial evaluation at different levels of the AD system, including adversarial generation, simulation-based interaction, and execution on physical vehicles. It also features human-in-the-loop capabilities for real-time physiological and behavioral feedback.

Result: MetAdv provides a comprehensive platform that supports diverse AD tasks, modular and end-to-end learning algorithms, and flexible transitions between virtual and physical environments. It is also compatible with commercial platforms like Apollo and Tesla.

Conclusion: MetAdv offers a unified, scalable framework for assessing the adversarial robustness of AD systems, contributing to safer and more reliable autonomous driving technologies.

Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD)
systems is a critical and unresolved challenge. This paper introduces MetAdv, a
novel adversarial testing platform that enables realistic, dynamic, and
interactive evaluation by tightly integrating virtual simulation with physical
vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical
sandbox, within which we design a three-layer closed-loop testing environment
with dynamic adversarial test evolution. This architecture facilitates
end-to-end adversarial evaluation, ranging from high-level unified adversarial
generation, through mid-level simulation-based interaction, to low-level
execution on physical vehicles. Additionally, MetAdv supports a broad spectrum
of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,
end-to-end learning, vision-language models). It supports flexible 3D vehicle
modeling and seamless transitions between simulated and physical environments,
with built-in compatibility for commercial platforms such as Apollo and Tesla.
A key feature of MetAdv is its human-in-the-loop capability: besides flexible
environmental configuration for more customized evaluation, it enables
real-time capture of physiological signals and behavioral feedback from
drivers, offering new insights into human-machine trust under adversarial
conditions. We believe MetAdv can offer a scalable and unified framework for
adversarial assessment, paving the way for safer AD.

</details>


### [562] [Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots](https://arxiv.org/abs/2508.06538)
*Gioele Buriani,Jingyue Liu,Maximilian Stölzle,Cosimo Della Santina,Jiatao Ding*

Main category: cs.RO

TL;DR: The paper proposes a novel method for creating reduced-order models for quadruped robots' jumping dynamics using a combined approach of SINDy and physical priors.


<details>
  <summary>Details</summary>
Motivation: Motion planning and control in quadruped robots require simplified models to navigate their complex dynamics effectively, particularly for jumping behaviors.

Method: The approach uses Sparse Identification of Nonlinear Dynamics (SINDy) integrated with physical structural priors to derive interpretable dynamic models for jumping quadruped robots.

Result: The proposed method outperforms traditional models (e.g., aSLIP model) in accuracy and is validated through both simulation and hardware experiments for various jumping strategies.

Conclusion: The methodology provides an effective, accurate, and interpretable reduced-order model for quadruped jumping dynamics, advancing motion control strategies.

Abstract: Reduced-order models are essential for motion planning and control of
quadruped robots, as they simplify complex dynamics while preserving critical
behaviors. This paper introduces a novel methodology for deriving such
interpretable dynamic models, specifically for jumping. We capture the
high-dimensional, nonlinear jumping dynamics in a low-dimensional latent space
by proposing a learning architecture combining Sparse Identification of
Nonlinear Dynamics (SINDy) with physical structural priors on the jump
dynamics. Our approach demonstrates superior accuracy to the traditional
actuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through
simulation and hardware experiments across different jumping strategies.

</details>


### [563] [A tutorial note on collecting simulated data for vision-language-action models](https://arxiv.org/abs/2508.06547)
*Heran Wu,Zirun Zhou,Jingfeng Zhang*

Main category: cs.RO

TL;DR: The paper discusses Vision-Language-Action models that integrate vision, language understanding, and motor control in a unified framework, and reviews systems for dataset generation, benchmarking, and scaling for robotic applications.


<details>
  <summary>Details</summary>
Motivation: Motivated by the limitations of traditional modular robotic systems and the need for unified processing of visual, language, and action data.

Method: The paper reviews and demonstrates three systems: PyBullet for simulation, LIBERO for benchmarking, and RT-X for large-scale multi-robot dataset acquisition.

Result: Demonstrated dataset generation with PyBullet and customized data collection with LIBERO. Provided insights into the RT-X dataset capabilities.

Conclusion: Unified neural models like VLA depend on tailored datasets. Systematic tools like PyBullet, LIBERO, and RT-X play key roles in advancing such frameworks.

Abstract: Traditional robotic systems typically decompose intelligence into independent
modules for computer vision, natural language processing, and motion control.
Vision-Language-Action (VLA) models fundamentally transform this approach by
employing a single neural network that can simultaneously process visual
observations, understand human instructions, and directly output robot actions
-- all within a unified framework. However, these systems are highly dependent
on high-quality training datasets that can capture the complex relationships
between visual observations, language instructions, and robotic actions. This
tutorial reviews three representative systems: the PyBullet simulation
framework for flexible customized data generation, the LIBERO benchmark suite
for standardized task definition and evaluation, and the RT-X dataset
collection for large-scale multi-robot data acquisition. We demonstrated
dataset generation approaches in PyBullet simulation and customized data
collection within LIBERO, and provide an overview of the characteristics and
roles of the RT-X dataset for large-scale multi-robot data acquisition.

</details>


### [564] [AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance](https://arxiv.org/abs/2508.06554)
*Abdelhaleem Saad,Waseem Akram,Irfan Hussain*

Main category: cs.RO

TL;DR: The paper introduces AquaChat++, a multi-ROV inspection system that uses Large Language Models (LLMs) for adaptive and robust aquaculture net pen inspections.


<details>
  <summary>Details</summary>
Motivation: Traditional aquaculture inspection methods are constrained by energy inefficiencies, hardware faults, and dynamic underwater conditions, necessitating more adaptive and robust solutions.

Method: A two-layered multi-ROV system is designed. The high-level layer uses LLMs, like ChatGPT-4, to translate user commands into multi-agent plans, while the low-level layer manages trajectory tracking and compensation for hardware faults.

Result: Simulated experiments in an aquaculture setting showed improved inspection coverage, energy efficiency, and resilience to actuator failures.

Conclusion: LLM-driven frameworks like AquaChat++ can significantly enhance the scalability, intelligence, and autonomy of underwater robotic systems for aquaculture.

Abstract: Inspection of aquaculture net pens is essential for ensuring the structural
integrity and sustainable operation of offshore fish farming systems.
Traditional methods, typically based on manually operated or single-ROV
systems, offer limited adaptability to real-time constraints such as energy
consumption, hardware faults, and dynamic underwater conditions. This paper
introduces AquaChat++, a novel multi-ROV inspection framework that uses Large
Language Models (LLMs) to enable adaptive mission planning, coordinated task
execution, and fault-tolerant control in complex aquaculture environments. The
proposed system consists of a two-layered architecture. The high-level plan
generation layer employs an LLM, such as ChatGPT-4, to translate natural
language user commands into symbolic, multi-agent inspection plans. A task
manager dynamically allocates and schedules actions among ROVs based on their
real-time status and operational constraints, including thruster faults and
battery levels. The low-level control layer ensures accurate trajectory
tracking and integrates thruster fault detection and compensation mechanisms.
By incorporating real-time feedback and event-triggered replanning, AquaChat++
enhances system robustness and operational efficiency. Simulated experiments in
a physics-based aquaculture environment demonstrate improved inspection
coverage, energy-efficient behavior, and resilience to actuator failures. These
findings highlight the potential of LLM-driven frameworks to support scalable,
intelligent, and autonomous underwater robotic operations within the
aquaculture sector.

</details>


### [565] [Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control](https://arxiv.org/abs/2508.06568)
*Amin Yazdanshenas,Reza Faieghi*

Main category: cs.RO

TL;DR: The paper introduces an advanced adaptive sliding mode control (SMC) framework for quadrotors, ensuring robust, agile, and computationally efficient flight.


<details>
  <summary>Details</summary>
Motivation: Address limitations in prior SMC methods, such as stability issues, oversimplified dynamics, quaternion unwinding, and gain overgrowth.

Method: Utilizes nonsmooth stability analysis for global stability proofs and deploys an innovative SMC scheme with resource-efficient implementation on nano quadrotors.

Result: Achieved superior trajectory tracking, robustness, and high-performance maneuvers in over 130 flight trials, outperforming existing methods.

Conclusion: The controller shows promise for robust, high-performance applications in scenarios with disturbances and computational limits.

Abstract: This paper presents a new adaptive sliding mode control (SMC) framework for
quadrotors that achieves robust and agile flight under tight computational
constraints. The proposed controller addresses key limitations of prior SMC
formulations, including (i) the slow convergence and almost-global stability of
$\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational
dynamics in Euler-based controllers, (iii) the unwinding phenomenon in
quaternion-based formulations, and (iv) the gain overgrowth problem in adaptive
SMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous
global stability proofs for both the nonsmooth attitude sliding dynamics
defined on $\mathbb{S}^3$ and the position sliding dynamics. Our controller is
computationally efficient and runs reliably on a resource-constrained nano
quadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude
control, respectively. In an extensive set of hardware experiments with over
130 flight trials, the proposed controller consistently outperforms three
benchmark methods, demonstrating superior trajectory tracking accuracy and
robustness with relatively low control effort. The controller enables
aggressive maneuvers such as dynamic throw launches, flip maneuvers, and
accelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.
These results highlight promising potential for real-world applications,
particularly in scenarios requiring robust, high-performance flight control
under significant external disturbances and tight computational constraints.

</details>


### [566] [Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios](https://arxiv.org/abs/2508.06575)
*Rui Zhou*

Main category: cs.RO

TL;DR: This paper presents an efficient testing method for ensuring autonomous vehicles' safety in critical scenarios, using an adaptive algorithm that strongly outperforms previous methods.


<details>
  <summary>Details</summary>
Motivation: To ensure the reliable deployment of autonomous vehicles, new methods are needed to validate their behavior in safety-critical scenarios.

Method: The study integrates real-world crash data and uses the Baidu Apollo driving system to control an AV. It applies a novel adaptive algorithm (ALVNS-SA) for accelerated testing.

Result: The proposed algorithm achieves high scenario coverage: 84.00% overall safety-critical, 96.83% crash, and 92.07% near-crash, outperforming genetic algorithms and randomness-based testing.

Conclusion: ALVNS-SA significantly improves efficiency and coverage in testing autonomous vehicles' safety, proving its potential over existing algorithms.

Abstract: Ensuring the safety of autonomous vehicles (AVs) is paramount in their
development and deployment. Safety-critical scenarios pose more severe
challenges, necessitating efficient testing methods to validate AVs safety.
This study focuses on designing an accelerated testing algorithm for AVs in
safety-critical scenarios, enabling swift recognition of their driving
capabilities. First, typical logical scenarios were extracted from real-world
crashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)
database, obtaining pre-crash features through reconstruction. Second, Baidu
Apollo, an advanced black-box automated driving system (ADS) is integrated to
control the behavior of the ego vehicle. Third, we proposed an adaptive
large-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to
expedite the testing process. Experimental results demonstrate a significant
enhancement in testing efficiency when utilizing ALVNS-SA. It achieves an
84.00% coverage of safety-critical scenarios, with crash scenario coverage of
96.83% and near-crash scenario coverage of 92.07%. Compared to genetic
algorithm (GA), adaptive large neighborhood-simulated annealing algorithm
(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage
in safety-critical scenarios.

</details>


### [567] [Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey](https://arxiv.org/abs/2508.07163)
*Kamal Acharya,Iman Sharifi,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.RO

TL;DR: This survey examines Neurosymbolic AI applications in Advanced Air Mobility (AAM) to address challenges like demand forecasting and air traffic management, highlighting both potential and limitations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the potential of Neurosymbolic AI in addressing complex challenges in AAM, such as scalability, robustness, and regulatory compliance.

Method: The authors review current literature, classify methodologies, showcase case studies, and highlight Neurosymbolic Reinforcement Learning for dynamic optimization.

Result: The study identifies a fragmented research landscape, noting potential but significant hurdles, such as compliance with aviation standards.

Conclusion: The paper proposes a roadmap for integrating Neurosymbolic AI into reliable and transparent systems to advance next-generation air mobility solutions.

Abstract: Neurosymbolic AI combines neural network adaptability with symbolic
reasoning, promising an approach to address the complex regulatory,
operational, and safety challenges in Advanced Air Mobility (AAM). This survey
reviews its applications across key AAM domains such as demand forecasting,
aircraft design, and real-time air traffic management. Our analysis reveals a
fragmented research landscape where methodologies, including Neurosymbolic
Reinforcement Learning, have shown potential for dynamic optimization but still
face hurdles in scalability, robustness, and compliance with aviation
standards. We classify current advancements, present relevant case studies, and
outline future research directions aimed at integrating these approaches into
reliable, transparent AAM systems. By linking advanced AI techniques with AAM's
operational demands, this work provides a concise roadmap for researchers and
practitioners developing next-generation air mobility solutions.

</details>


### [568] [Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation](https://arxiv.org/abs/2508.06687)
*Sreeja Roy-Singh,Vinay Ravindra,Richard Levinson,Mahta Moghaddam,Jan Mandel,Adam Kochanski,Angel Farguell Caus,Kurtis Nelson,Samira Alkaee Taleghan,Archana Kannan,Amer Melebari*

Main category: cs.RO

TL;DR: This paper introduces a new operational framework integrating optimal planning with machine learning to enhance wildfire monitoring and decision-making using NASA's CYGNSS satellite mission.


<details>
  <summary>Details</summary>
Motivation: Current wildfire monitoring tools face challenges in collecting high-resolution, timely data, and integrating it efficiently into decision-support models.

Method: The study combines Mixed Integer Programming (MIP) for optimal satellite scheduling and machine learning for enhanced fire prediction and processing of CYGNSS satellite data.

Result: The framework achieved near-optimal data collection (98-100%), a 40% improvement in fire prediction correlation, a 13% boost in burn prediction accuracy, and a 15% recall improvement.

Conclusion: This proposed workflow is scalable and efficient, delivering significantly improved and timely results compared to existing methods, with a latency of only 6-30 hours.

Abstract: We propose a novel concept of operations using optimal planning methods and
machine learning (ML) to collect spaceborne data that is unprecedented for
monitoring wildfires, process it to create new or enhanced products in the
context of wildfire danger or spread monitoring, and assimilate them to improve
existing, wildfire decision support tools delivered to firefighters within
latency appropriate for time-critical applications. The concept is studied with
respect to NASA's CYGNSS Mission, a constellation of passive microwave
receivers that measure specular GNSS-R reflections despite clouds and smoke.
Our planner uses a Mixed Integer Program formulation to schedule joint
observation data collection and downlink for all satellites. Optimal solutions
are found quickly that collect 98-100% of available observation opportunities.
ML-based fire predictions that drive the planner objective are greater than 40%
more correlated with ground truth than existing state-of-art. The presented
case study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025
represents the first high-resolution data collected by CYGNSS of active fires.
Creation of Burnt Area Maps (BAM) using ML applied to the data during active
fires and BAM assimilation into NASA's Weather Research and Forecasting Model
using ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained
soil moisture are integrated for the first time into USGS fire danger maps.
Inclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,
and inclusion of high-resolution data boosts ML recall by another 15%. The
proposed workflow has an expected latency of 6-30h, improving on the current
delivery time of multiple days. All components in the proposed concept are
shown to be computationally scalable and globally generalizable, with
sustainability considerations such as edge efficiency and low latency on small
devices.

</details>


### [569] [Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC](https://arxiv.org/abs/2508.06722)
*Justin London*

Main category: cs.RO

TL;DR: This paper introduces ORCA-FL—a fuzzy logic-enhanced method for obstacle avoidance that outperforms Optimal Reciprocal Collision Avoidance (ORCA) in multi-agent environments.


<details>
  <summary>Details</summary>
Motivation: Existing collision avoidance methods can be suboptimal due to fixed weights, high computational costs, or limited adaptability in dynamic, multi-agent scenarios.

Method: ORCA-FL utilizes fuzzy logic controllers (FLCs) for handling uncertainty in obstacle avoidance. A fuzzy Q reinforcement learning (FQL) extension is also proposed to refine FLCs.

Result: ORCA-FL reduces the number of collisions in multi-agent environments, particularly for agents with higher velocities.

Conclusion: ORCA-FL improves upon ORCA by leveraging fuzzy logic and reinforcement learning to better navigate dynamic and uncertain environments.

Abstract: Obstacle avoidance enables autonomous agents and robots to operate safely and
efficiently in dynamic and complex environments, reducing the risk of
collisions and damage. For a robot or autonomous system to successfully
navigate through obstacles, it must be able to detect such obstacles. While
numerous collision avoidance algorithms like the dynamic window approach (DWA),
timed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been
proposed, they may lead to suboptimal paths due to fixed weights, be
computationally expensive, or have limited adaptability to dynamic obstacles in
multi-agent environments. Optimal reciprocal collision avoidance (ORCA), which
improves on RVO, provides smoother trajectories and stronger collision
avoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy
logic controllers (FLCs) to better handle uncertainty and imprecision for
obstacle avoidance in path planning. Numerous multi-agent experiments are
conducted and it is shown that ORCA-FL can outperform ORCA in reducing the
number of collision if the agent has a velocity that exceeds a certain
threshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy
Q reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.

</details>


### [570] [Learning Causal Structure Distributions for Robust Planning](https://arxiv.org/abs/2508.06742)
*Alejandro Murillo-Gonzalez,Junhong Xu,Lantao Liu*

Main category: cs.RO

TL;DR: This paper proposes a method for learning robotic dynamics models using structural causal models, which enhance robustness and efficiency in downstream planning.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current model-learning methods for robotics that fail to leverage causal structures and sparsity of interactions.

Method: The method involves estimating a causal structure distribution, sampling causal graphs, and integrating them into an encoder-multidecoder probabilistic model for dynamics learning and planning.

Result: The approach improves robustness, adaptability, and computational efficiency in robotic planning, validated on manipulators and mobile robots in simulations and real-world tests.

Conclusion: Using causal structures enhances dynamics modeling for robots, with substantial benefits such as adaptability to corrupted inputs and environmental changes, making it suitable for realistic robotics applications.

Abstract: Structural causal models describe how the components of a robotic system
interact. They provide both structural and functional information about the
relationships that are present in the system. The structural information
outlines the variables among which there is interaction. The functional
information describes how such interactions work, via equations or learned
models. In this paper we find that learning the functional relationships while
accounting for the uncertainty about the structural information leads to more
robust dynamics models which improves downstream planning, while using
significantly lower computational resources. This in contrast with common
model-learning methods that ignore the causal structure and fail to leverage
the sparsity of interactions in robotic systems. We achieve this by estimating
a causal structure distribution that is used to sample causal graphs that
inform the latent-space representations in an encoder-multidecoder
probabilistic model. We show that our model can be used to learn the dynamics
of a robot, which together with a sampling-based planner can be used to perform
new tasks in novel environments, provided an objective function for the new
requirement is available. We validate our method using manipulators and mobile
robots in both simulation and the real-world. Additionally, we validate the
learned dynamics' adaptability and increased robustness to corrupted inputs and
changes in the environment, which is highly desirable in challenging real-world
robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.

</details>


### [571] [Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery](https://arxiv.org/abs/2508.06744)
*Yunke Ao,Manish Prajapat,Yarden As,Yassine Taoudi-Benchekroun,Fabio Carrillo,Hooman Esfandiari,Benjamin F. Grewe,Andreas Krause,Philipp Fürnstahl*

Main category: cs.RO

TL;DR: This paper introduces a method for ensuring safety in high-dimensional sensory feedback control systems, applying it to robotic spinal surgery.


<details>
  <summary>Details</summary>
Motivation: Safety-critical systems using high-dimensional sensory data struggle with estimation errors, which are difficult to model and provide safety guarantees for.

Method: The authors propose a sub-Gaussian noise characterization with bounded mean and develop a Model Predictive Control framework to ensure safety through uncertainty propagation and noise handling for linear systems.

Result: The approach was evaluated in a simulated robotic spinal surgery environment, showing that it solves complex tasks while ensuring safety.

Conclusion: The proposed framework demonstrates promise for improving safety in high-dimensional control systems, notably in robotic surgery applications.

Abstract: Safety-critical control using high-dimensional sensory feedback from optical
data (e.g., images, point clouds) poses significant challenges in domains like
autonomous driving and robotic surgery. Control can rely on low-dimensional
states estimated from high-dimensional data. However, the estimation errors
often follow complex, unknown distributions that standard probabilistic models
fail to capture, making formal safety guarantees challenging. In this work, we
introduce a novel characterization of these general estimation errors using
sub-Gaussian noise with bounded mean. We develop a new technique for
uncertainty propagation of proposed noise characterization in linear systems,
which combines robust set-based methods with the propagation of sub-Gaussian
variance proxies. We further develop a Model Predictive Control (MPC) framework
that provides closed-loop safety guarantees for linear systems under the
proposed noise assumption. We apply this MPC approach in an
ultrasound-image-guided robotic spinal surgery pipeline, which contains
deep-learning-based semantic segmentation, image-based registration, high-level
optimization-based planning, and low-level robotic control. To validate the
pipeline, we developed a realistic simulation environment integrating real
human anatomy, robot dynamics, efficient ultrasound simulation, as well as
in-vivo data of breathing motion and drilling force. Evaluation results in
simulation demonstrate the potential of our approach for solving complex
image-guided robotic surgery task while ensuring safety.

</details>


### [572] [Learning a Vision-Based Footstep Planner for Hierarchical Walking Control](https://arxiv.org/abs/2508.06779)
*Minku Kim,Brian Acosta,Pratik Chaudhari,Michael Posa*

Main category: cs.RO

TL;DR: The paper proposes a vision-based hierarchical control framework for bipedal robots, integrating reinforcement learning for footstep planning and a low-level controller to navigate complex terrains.


<details>
  <summary>Details</summary>
Motivation: Current methods for bipedal robot navigation rely mainly on proprioception or fragile visual pipelines, making footstep planning in unstructured terrains challenging.

Method: The framework combines a reinforcement learning-based footstep planner with a low-level Operational Space Controller, using a simplified low-dimensional state model for dynamic encoding.

Result: The approach was tested on the underactuated bipedal robot Cassie across diverse terrains, showing its capabilities and presenting challenges in simulation and hardware experiments.

Conclusion: The proposed framework improves real-time footstep planning and terrain navigation for bipedal robots, highlighting potential for future refinement and application in real-world scenarios.

Abstract: Bipedal robots demonstrate potential in navigating challenging terrains
through dynamic ground contact. However, current frameworks often depend solely
on proprioception or use manually designed visual pipelines, which are fragile
in real-world settings and complicate real-time footstep planning in
unstructured environments. To address this problem, we present a vision-based
hierarchical control framework that integrates a reinforcement learning
high-level footstep planner, which generates footstep commands based on a local
elevation map, with a low-level Operational Space Controller that tracks the
generated trajectories. We utilize the Angular Momentum Linear Inverted
Pendulum model to construct a low-dimensional state representation to capture
an informative encoding of the dynamics while reducing complexity. We evaluate
our method across different terrain conditions using the underactuated bipedal
robot Cassie and investigate the capabilities and challenges of our approach
through simulation and hardware experiments.

</details>


### [573] [D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning](https://arxiv.org/abs/2508.06804)
*Shu-Ang Yu,Feng Gao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: D3P improves robotic visuomotor tasks by adaptively allocating denoising steps for each action, achieving significant speed-ups without compromising success.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies face challenges in real-time robotic tasks due to their slow iterative denoising process, especially when all actions are treated equally.

Method: D3P employs a dynamic approach by introducing a state-aware adaptor that allocates the optimal number of denoising steps per action, jointly optimized with reinforcement learning.

Result: D3P delivers a 2.2× inference speed-up in simulations and 1.9× on physical robots compared to baselines.

Conclusion: D3P is a more efficient diffusion-based policy, maintaining task success while significantly improving speed during robotic tasks.

Abstract: Diffusion policies excel at learning complex action distributions for robotic
visuomotor tasks, yet their iterative denoising process poses a major
bottleneck for real-time deployment. Existing acceleration methods apply a
fixed number of denoising steps per action, implicitly treating all actions as
equally important. However, our experiments reveal that robotic tasks often
contain a mix of \emph{crucial} and \emph{routine} actions, which differ in
their impact on task success. Motivated by this finding, we propose
\textbf{D}ynamic \textbf{D}enoising \textbf{D}iffusion \textbf{P}olicy
\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising
steps across actions at test time. D3P uses a lightweight, state-aware adaptor
to allocate the optimal number of denoising steps for each action. We jointly
optimize the adaptor and base diffusion policy via reinforcement learning to
balance task performance and inference efficiency. On simulated tasks, D3P
achieves an averaged 2.2$\times$ inference speed-up over baselines without
degrading success. Furthermore, we demonstrate D3P's effectiveness on a
physical robot, achieving a 1.9$\times$ acceleration over the baseline.

</details>


### [574] [Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound](https://arxiv.org/abs/2508.06921)
*Zhongyu Chen,Chenyang Li,Xuesong Li,Dianye Huang,Zhongliang Jiang,Stefanie Speidel,Xiangyu Chu,K. W. Samuel Au*

Main category: cs.RO

TL;DR: The paper introduces a method for restoring needle alignment in robotic ultrasound-guided procedures by using a vibration-based system, rather than depending on needle visibility.


<details>
  <summary>Details</summary>
Motivation: Conventional needle alignment methods face challenges due to factors like speckle noise and reduced visibility in ultrasound images.

Method: The needle is periodically vibrated using a mechanical system, and a vibration-based energy metric is developed to guide probe repositioning in translation and rotation.

Result: Experiments on porcine tissue samples showed that the method achieved a translational error of 0.41±0.27 mm and a rotational error of 0.51±0.19 degrees, proving its efficacy.

Conclusion: The vibration-based approach overcomes limitations of traditional visibility-dependent methods, enabling more robust needle alignment in ultrasound-guided procedures.

Abstract: Precise needle alignment is essential for percutaneous needle insertion in
robotic ultrasound-guided procedures. However, inherent challenges such as
speckle noise, needle-like artifacts, and low image resolution make robust
needle detection difficult, particularly when visibility is reduced or lost. In
this paper, we propose a method to restore needle alignment when the ultrasound
imaging plane and the needle insertion plane are misaligned. Unlike many
existing approaches that rely heavily on needle visibility in ultrasound
images, our method uses a more robust feature by periodically vibrating the
needle using a mechanical system. Specifically, we propose a vibration-based
energy metric that remains effective even when the needle is fully out of
plane. Using this metric, we develop a control strategy to reposition the
ultrasound probe in response to misalignments between the imaging plane and the
needle insertion plane in both translation and rotation. Experiments conducted
on ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided
needle insertion system demonstrate the effectiveness of the proposed approach.
The experimental results show the translational error of 0.41$\pm$0.27 mm and
the rotational error of 0.51$\pm$0.19 degrees.

</details>


### [575] [Manipulator for people with limited abilities](https://arxiv.org/abs/2508.06969)
*Bingkun Huang,Evgeniy Kotov,Arkady Yuschenko*

Main category: cs.RO

TL;DR: This paper focuses on creating a robotic hand with four degrees of freedom to assist people with disabilities, integrating mechanical design, control system, and ROS-based software.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to develop robotic systems that improve the quality of life for people with disabilities through advanced assistive technologies.

Method: The authors design and manufacture a robotic hand with four degrees of freedom, employing a combination of mechanical, control system development, and integration with a vision system and ROS-based software.

Result: The study presents a functional four-degree-of-freedom robotic hand suitable for practical manipulation, meeting the specified objectives.

Conclusion: The developed robotic hand and its control system represent a significant contribution to assistive technology, addressing both scientific and practical challenges.

Abstract: The topic of this final qualification work was chosen due to the importance
of developing robotic systems designed to assist people with disabilities.
Advances in robotics and automation technologies have opened up new prospects
for creating devices that can significantly improve the quality of life for
these people. In this context, designing a robotic hand with a control system
adapted to the needs of people with disabilities is a major scientific and
practical challenge. This work addresses the problem of developing and
manufacturing a four-degree-of-freedom robotic hand suitable for practical
manipulation. Addressing this issue requires a comprehensive approach,
encompassing the design of the hand's mechanical structure, the development of
its control system, and its integration with a technical vision system and
software based on the Robot Operating System (ROS).

</details>


### [576] [Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation](https://arxiv.org/abs/2508.06990)
*Yue Hu,Junzhe Wu,Ruihan Xu,Hang Liu,Avery Xi,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: This paper introduces SGImagineNav, a framework for semantic navigation that leverages symbolic world modeling and imaginative strategies to improve navigation efficiency and target success.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance semantic navigation by enabling agents to predict future scenes, thereby navigating unseen environments more effectively and efficiently.

Method: The method integrates symbolic world modeling with hierarchical scene graphs and large language models. The framework proactively builds global environmental representations to predict future scenes and employs adaptive navigation strategies based on semantic shortcuts and exploratory actions.

Result: SGImagineNav achieves improved success rates of 65.4 on HM3D and 66.8 on HSSD benchmarks. It also demonstrates strong generalizability through cross-floor and cross-room navigation in real-world environments.

Conclusion: The proposed SGImagineNav framework outperforms existing methods in both simulated and real-world tests, proving its effectiveness in semantic navigation tasks and its ability to generalize across diverse scenarios.

Abstract: Semantic navigation requires an agent to navigate toward a specified target
in an unseen environment. Employing an imaginative navigation strategy that
predicts future scenes before taking action, can empower the agent to find
target faster. Inspired by this idea, we propose SGImagineNav, a novel
imaginative navigation framework that leverages symbolic world modeling to
proactively build a global environmental representation. SGImagineNav maintains
an evolving hierarchical scene graphs and uses large language models to predict
and explore unseen parts of the environment. While existing methods solely
relying on past observations, this imaginative scene graph provides richer
semantic context, enabling the agent to proactively estimate target locations.
Building upon this, SGImagineNav adopts an adaptive navigation strategy that
exploits semantic shortcuts when promising and explores unknown areas otherwise
to gather additional context. This strategy continuously expands the known
environment and accumulates valuable semantic contexts, ultimately guiding the
agent toward the target. SGImagineNav is evaluated in both real-world scenarios
and simulation benchmarks. SGImagineNav consistently outperforms previous
methods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and
demonstrating cross-floor and cross-room navigation in real-world environments,
underscoring its effectiveness and generalizability.

</details>


### [577] [EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events](https://arxiv.org/abs/2508.07003)
*Siyu Chen,Shenghai Yuan,Thien-Minh Nguyen,Zhuyu Huang,Chenyang Shi,Jin Jing,Lihua Xie*

Main category: cs.RO

TL;DR: EGS-SLAM integrates event data and RGB-D inputs to overcome motion blur challenges and enhance GS-SLAM's tracking and 3D reconstruction capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional GS-SLAM systems underperform in cases of severe motion blur, which affects tracking accuracy and 3D reconstruction quality.

Method: EGS-SLAM fuses event data with RGB-D inputs and explicitly models a continuous camera trajectory. It also introduces a learnable camera response function and a no-event loss to handle discrepancies and suppress artifacts.

Result: EGS-SLAM achieves superior trajectory accuracy and photorealistic 3D reconstruction compared to existing methods, as validated on both synthetic and real-world datasets with motion blur.

Conclusion: Fusing event data with RGB-D can significantly improve SLAM performance in challenging scenarios, and EGS-SLAM is a robust solution for achieving photorealistic 3D reconstruction in the presence of motion blur.

Abstract: Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over
traditional SLAM methods, enabling photorealistic 3D reconstruction that
conventional approaches often struggle to achieve. However, existing GS-SLAM
systems perform poorly under persistent and severe motion blur commonly
encountered in real-world scenarios, leading to significantly degraded tracking
accuracy and compromised 3D reconstruction quality. To address this limitation,
we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D
inputs to simultaneously reduce motion blur in images and compensate for the
sparse and discrete nature of event streams, enabling robust tracking and
high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system
explicitly models the camera's continuous trajectory during exposure,
supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian
Splatting scene. Furthermore, we introduce a learnable camera response function
to align the dynamic ranges of events and images, along with a no-event loss to
suppress ringing artifacts during reconstruction. We validate our approach on a
new dataset comprising synthetic and real-world sequences with significant
motion blur. Extensive experimental results demonstrate that EGS-SLAM
consistently outperforms existing GS-SLAM systems in both trajectory accuracy
and photorealistic 3D Gaussian Splatting reconstruction. The source code will
be available at https://github.com/Chensiyu00/EGS-SLAM.

</details>


### [578] [$\mathcal{P}^3$: Toward Versatile Embodied Agents](https://arxiv.org/abs/2508.07033)
*Shengli Zhou,Xiangchen Wang,Jinrui Zhang,Ruozai Tian,Rongtao Xu,Feng Zheng*

Main category: cs.RO

TL;DR: The paper introduces a unified framework, $
mathcal{P}^3$, for real-time perception and dynamic scheduling, addressing challenges in embodied agents related to perception, tool usage, and multi-task execution.


<details>
  <summary>Details</summary>
Motivation: Embodied agents face critical challenges such as dynamic environment perception, open-ended tool usage, and complex multi-task planning, limiting their adaptability and generalization across diverse environments.

Method: The proposed framework, $
mathcal{P}^3$, actively perceives task information, plugs tools without feedback requirements, and dynamically schedules multi-task execution based on dependencies and urgency.

Result: Real-world experiments demonstrate that $
mathcal{P}^3$ bridges the gap between benchmarks and practical deployment, enabling highly transferable and general-purpose embodied agents.

Conclusion: $
mathcal{P}^3$ represents a significant advancement for embodied agents by integrating real-time perception, tool flexibility, and dynamic task planning, paving the way for better adaptability in real-world applications.

Abstract: Embodied agents have shown promising generalization capabilities across
diverse physical environments, making them essential for a wide range of
real-world applications. However, building versatile embodied agents poses
critical challenges due to three key issues: dynamic environment perception,
open-ended tool usage, and complex multi-task planning. Most previous works
rely solely on feedback from tool agents to perceive environmental changes and
task status, which limits adaptability to real-time dynamics, causes error
accumulation, and restricts tool flexibility. Furthermore, multi-task
scheduling has received limited attention, primarily due to the inherent
complexity of managing task dependencies and balancing competing priorities in
dynamic and complex environments. To overcome these challenges, we introduce
$\mathcal{P}^3$, a unified framework that integrates real-time perception and
dynamic scheduling. Specifically, $\mathcal{P}^3$ enables 1) \textbf Perceive
relevant task information actively from the environment, 2) \textbf Plug and
utilize any tool without feedback requirement, and 3) \textbf Plan multi-task
execution based on prioritizing urgent tasks and dynamically adjusting task
order based on dependencies. Extensive real-world experiments show that our
approach bridges the gap between benchmarks and practical deployment,
delivering highly transferable, general-purpose embodied agents. Code and data
will be released soon.

</details>


### [579] [From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline](https://arxiv.org/abs/2508.07045)
*Dennis Benders,Johannes Köhler,Robert Babuška,Javier Alonso-Mora,Laura Ferranti*

Main category: cs.RO

TL;DR: The paper introduces a robust Model Predictive Control (MPC) pipeline for autonomous robot navigation, leveraging data-driven methods to handle disturbances and noisy measurements.


<details>
  <summary>Details</summary>
Motivation: Ensure safety in real-world autonomous robot navigation where disturbances and measurement noise pose challenges that are inadequately addressed by current methods.

Method: A modular and iterative pipeline is developed, using closed-loop experimental data to estimate disturbance bounds and synthesize a robust output-feedback MPC. The pipeline is implemented in deterministic, reproducible code.

Result: The proposed approach demonstrates robust constraint satisfaction and recursive feasibility in quadrotor simulations using Gazebo.

Conclusion: The presented robust MPC pipeline effectively addresses real-world challenges, offering a systematic and practical solution for safe robot navigation.

Abstract: Model predictive control (MPC) is a powerful strategy for planning and
control in autonomous mobile robot navigation. However, ensuring safety in
real-world deployments remains challenging due to the presence of disturbances
and measurement noise. Existing approaches often rely on idealized assumptions,
neglect the impact of noisy measurements, and simply heuristically guess
unrealistic bounds. In this work, we present an efficient and modular robust
MPC design pipeline that systematically addresses these limitations. The
pipeline consists of an iterative procedure that leverages closed-loop
experimental data to estimate disturbance bounds and synthesize a robust
output-feedback MPC scheme. We provide the pipeline in the form of
deterministic and reproducible code to synthesize the robust output-feedback
MPC from data. We empirically demonstrate robust constraint satisfaction and
recursive feasibility in quadrotor simulations using Gazebo.

</details>


### [580] [Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction](https://arxiv.org/abs/2508.07079)
*Mohamed Parvez Aslam,Bojan Derajic,Mohamed-Khalil Bouzidi,Sebastian Bernhard,Jan Oliver Ringert*

Main category: cs.RO

TL;DR: This paper integrates Social-Implicit (SI) pedestrian prediction with Model Predictive Control (MPC) for safer robot navigation, showing improved prediction accuracy and performance under varied pedestrian densities.


<details>
  <summary>Details</summary>
Motivation: To address challenges in autonomous robot navigation in environments populated by pedestrians, focusing on safety and adaptability.

Method: Integration of a deep learning-based Social-Implicit (SI) predictor within an MPC framework, tested on the Continental Corriere robot under various pedestrian densities.

Result: The SI model improves trajectory prediction by up to 76% in low-density environments. It also enhances motion smoothness and safety, though discrepancies between metrics and real-world performance were noted.

Conclusion: The SI-MPC framework offers promising advancements in safe and adaptive navigation in human-rich environments, with system-level evaluations proving critical for its development.

Abstract: Safe navigation in pedestrian-rich environments remains a key challenge for
autonomous robots. This work evaluates the integration of a deep learning-based
Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive
Control (MPC) framework on the physical Continental Corriere robot. Tested
across varied pedestrian densities, the SI-MPC system is compared to a
traditional Constant Velocity (CV) model in both open-loop prediction and
closed-loop navigation. Results show that SI improves trajectory prediction -
reducing errors by up to 76% in low-density settings - and enhances safety and
motion smoothness in crowded scenes. Moreover, real-world deployment reveals
discrepancies between open-loop metrics and closed-loop performance, as the SI
model yields broader, more cautious predictions. These findings emphasize the
importance of system-level evaluation and highlight the SI-MPC framework's
promise for safer, more adaptive navigation in dynamic, human-populated
environments.

</details>


### [581] [An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving](https://arxiv.org/abs/2508.07080)
*Haolin Liu,Zijun Guo,Yanbo Chen,Jiaqi Chen,Huilong Yu,Junqiang Xi*

Main category: cs.RO

TL;DR: The paper discusses an evolutionary game-theoretic framework for improving autonomous vehicle decision-making during highway on-ramp merging, focusing on efficiency, safety, and comfort by considering human-like driving preferences.


<details>
  <summary>Details</summary>
Motivation: The challenge of safely and efficiently merging autonomous vehicles onto highways amidst dynamic and socially complex scenarios motivates the need for better decision-making algorithms.

Method: An evolutionary game-theoretic framework is developed that formulates the merging scenario as a multi-objective payoff problem. The approach incorporates an online adaptation mechanism for driving styles and utilizes replication dynamics to reach stable strategies.

Result: Empirical studies revealed that the proposed method outperformed traditional approaches in terms of safety, comfort, and efficiency across multiple metrics.

Conclusion: The evolutionary game-theoretic method provides significant improvements in merging decisions, highlighting its potential for integrating human-like strategies in autonomous vehicle interactions.

Abstract: Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),
since they have to proactively interact with surrounding vehicles to enter the
main road safely within limited time. However, existing decision-making
algorithms fail to adequately address dynamic complexities and social
acceptance of AVs, leading to suboptimal or unsafe merging decisions. To
address this, we propose an evolutionary game-theoretic (EGT) merging
decision-making framework, grounded in the bounded rationality of human
drivers, which dynamically balances the benefits of both AVs and main-road
vehicles (MVs). We formulate the cut-in decision-making process as an EGT
problem with a multi-objective payoff function that reflects human-like driving
preferences. By solving the replicator dynamic equation for the evolutionarily
stable strategy (ESS), the optimal cut-in timing is derived, balancing
efficiency, comfort, and safety for both AVs and MVs. A real-time driving style
estimation algorithm is proposed to adjust the game payoff function online by
observing the immediate reactions of MVs. Empirical results demonstrate that we
improve the efficiency, comfort and safety of both AVs and MVs compared with
existing game-theoretic and traditional planning approaches across multi-object
metrics.

</details>


### [582] [DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit](https://arxiv.org/abs/2508.07118)
*Aiden Swann,Alex Qiu,Matthew Strong,Angelina Zhang,Samuel Morstein,Kai Rayle,Monroe Kennedy III*

Main category: cs.RO

TL;DR: DexFruit is a robotic manipulation framework that gently and autonomously handles fragile fruits, reducing damage, and achieves high success rates using optical tactile sensing and 3D damage representation techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of manually harvesting fragile fruits prone to bruising, and to develop an autonomous system that minimizes fruit damage while enabling precise damage evaluation.

Method: The framework uses optical tactile sensing to guide autonomous fruit manipulation through tactile-informed diffusion policies. It integrates a novel 3D representation technique called FruitSplat, leveraging 3D Gaussian Splatting to quantify and analyze damage.

Result: The system shows a 92% grasp policy success rate, up to a 20% reduction in visual bruising, and a 31% improvement in grasping success across strawberries, tomatoes, and blackberries, as evaluated across 630 trials.

Conclusion: DexFruit demonstrates that combining tactile sensing and advanced 3D damage representations can significantly improve the autonomous handling of fragile fruits, marking a step forward in agricultural robotics.

Abstract: DexFruit is a robotic manipulation framework that enables gentle, autonomous
handling of fragile fruit and precise evaluation of damage. Many fruits are
fragile and prone to bruising, thus requiring humans to manually harvest them
with care. In this work, we demonstrate by using optical tactile sensing,
autonomous manipulation of fruit with minimal damage can be achieved. We show
that our tactile informed diffusion policies outperform baselines in both
reduced bruising and pick-and-place success rate across three fruits:
strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,
a novel technique to represent and quantify visual damage in high-resolution 3D
representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring
damage lack quantitative rigor or require expensive equipment. With FruitSplat,
we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into
the 3DGS representation. Furthermore, this representation is modular and
general, compatible with any relevant 2D model. Overall, we demonstrate a 92%
grasping policy success rate, up to a 20% reduction in visual bruising, and up
to an 31% improvement in grasp success rate on challenging fruit compared to
our baselines across our three tested fruits. We rigorously evaluate this
result with over 630 trials. Please checkout our website at
https://dex-fruit.github.io .

</details>


### [583] [3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07182)
*Xuesong Li,Lars Petersson,Vivien Rolland*

Main category: cs.RO

TL;DR: This paper proposes combining 3D Gaussian Splatting with a motion trajectory field to improve novel-view synthesis and motion reconstruction for dynamic scenes using monocular video.


<details>
  <summary>Details</summary>
Motivation: Current methods like NeRF and 3DGS perform well for static scenes but face challenges in effectively reconstructing dynamic scenes.

Method: The approach introduces 3DGS combined with a motion trajectory field that decouples dynamic objects from the static background and uses time-invariant motion coefficients and shared motion trajectory bases.

Result: Extensive experiments show state-of-the-art performance in dynamic scene reconstruction for both novel-view synthesis and motion recovery.

Conclusion: The proposed method significantly improves the handling of complex object motions and enables accurate and efficient reconstruction of dynamic scenes from monocular video.

Abstract: This paper addresses the challenge of novel-view synthesis and motion
reconstruction of dynamic scenes from monocular video, which is critical for
many robotic applications. Although Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering
static scenes, extending them to reconstruct dynamic scenes remains
challenging. In this work, we introduce a novel approach that combines 3DGS
with a motion trajectory field, enabling precise handling of complex object
motions and achieving physically plausible motion trajectories. By decoupling
dynamic objects from static background, our method compactly optimizes the
motion trajectory field. The approach incorporates time-invariant motion
coefficients and shared motion trajectory bases to capture intricate motion
patterns while minimizing optimization complexity. Extensive experiments
demonstrate that our approach achieves state-of-the-art results in both
novel-view synthesis and motion trajectory recovery from monocular video,
advancing the capabilities of dynamic scene reconstruction.

</details>


### [584] [Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks](https://arxiv.org/abs/2508.07244)
*Ayesha Jena,Stefan Reitmann,Elin Anna Topp*

Main category: cs.RO

TL;DR: This study investigates head-gaze-based robot control and foveated visual augmentation for simulated search-and-rescue tasks, showing performance improvements, lower cognitive load, and reduced task time.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of head-gaze-based control and foveated visual augmentation in enhancing efficiency and reducing cognitive strain in critical scenarios like search-and-rescue.

Method: A user study was conducted on head-gaze-based robot control and foveated visual augmentation during simulated search-and-rescue tasks. Attention patterns were analyzed over task durations.

Result: Foveated augmentation improves task performance, reduces cognitive load by 38%, and shortens task execution time by over 60%. Gaze analysis revealed the importance of capturing near and far attention for understanding user intentions.

Conclusion: The study demonstrates foveation's capability to enhance task efficiency, alleviate cognitive load, and highlights the need for further research on leveraging gaze measures in critical scenarios.

Abstract: We present a user study analyzing head-gaze-based robot control and foveated
visual augmentation in a simulated search-and-rescue task. Results show that
foveated augmentation significantly improves task performance, reduces
cognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns
analysed over both the entire task duration and shorter time segments show that
near and far attention capture is essential to better understand user intention
in critical scenarios. Our findings highlight the potential of foveation as an
augmentation technique and the need to further study gaze measures to leverage
them during critical tasks.

</details>


### [585] [Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics](https://arxiv.org/abs/2508.07267)
*Daria de Tinguy,Tim Verbelen,Emilio Gamba,Bart Dhoedt*

Main category: cs.RO

TL;DR: The paper presents a bio-inspired autonomous navigation agent based on Active Inference Framework, integrating real-time mapping, localization, and decision-making without pre-training.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenges of autonomous navigation in dynamic or unknown environments, addressing the limitations of existing rule-based and data-intensive AI methods.

Method: Development of a bio-inspired agent that employs the Active Inference Framework, building and updating topological maps in real-time for adaptive, goal-directed navigation.

Result: The proposed agent demonstrated successful exploration and adaptability in simulated and real-world environments, performing comparably to methods like Gbplanner, FAEL, and Frontiers.

Conclusion: This bio-inspired approach provides a scalable, interpretable, and adaptable solution for autonomous navigation in complex, unstructured environments without requiring pre-training.

Abstract: Achieving fully autonomous exploration and navigation remains a critical
challenge in robotics, requiring integrated solutions for localisation,
mapping, decision-making and motion planning. Existing approaches either rely
on strict navigation rules lacking adaptability or on pre-training, which
requires large datasets. These AI methods are often computationally intensive
or based on static assumptions, limiting their adaptability in dynamic or
unknown environments. This paper introduces a bio-inspired agent based on the
Active Inference Framework (AIF), which unifies mapping, localisation, and
adaptive decision-making for autonomous navigation, including exploration and
goal-reaching. Our model creates and updates a topological map of the
environment in real-time, planning goal-directed trajectories to explore or
reach objectives without requiring pre-training. Key contributions include a
probabilistic reasoning framework for interpretable navigation, robust
adaptability to dynamic changes, and a modular ROS2 architecture compatible
with existing navigation systems. Our method was tested in simulated and
real-world environments. The agent successfully explores large-scale simulated
environments and adapts to dynamic obstacles and drift, proving to be
comparable to other exploration strategies such as Gbplanner, FAEL and
Frontiers. This approach offers a scalable and transparent approach for
navigating complex, unstructured environments.

</details>


### [586] [Navigation and Exploration with Active Inference: from Biology to Industry](https://arxiv.org/abs/2508.07269)
*Daria de Tinguy,Tim Verbelen,Bart Dhoedt*

Main category: cs.RO

TL;DR: The paper proposes a real-time robotic navigation system inspired by animal cognitive maps, grounded in the Active Inference Framework (AIF), capable of adaptable navigation in 2D and 3D environments.


<details>
  <summary>Details</summary>
Motivation: To develop a biologically inspired robotic navigation system that can navigate dynamic environments using cognitive mapping strategies without relying on extensive prior training.

Method: The system uses the Active Inference Framework (AIF) to build topological maps, infer locations, and plan actions through uncertainty minimization and perceptual goal fulfillment. It is integrated into the ROS2 ecosystem and tested in various environments.

Result: The proposed system demonstrates high adaptability and efficiency, performing on par with or better than traditional and state-of-the-art exploration approaches in 2D and 3D environments.

Conclusion: The biologically inspired navigation system provides a competitive, efficient, and adaptable solution to robotic navigation challenges, leveraging the principles of active inference.

Abstract: By building and updating internal cognitive maps, animals exhibit
extraordinary navigation abilities in complex, dynamic environments. Inspired
by these biological mechanisms, we present a real time robotic navigation
system grounded in the Active Inference Framework (AIF). Our model
incrementally constructs a topological map, infers the agent's location, and
plans actions by minimising expected uncertainty and fulfilling perceptual
goals without any prior training. Integrated into the ROS2 ecosystem, we
validate its adaptability and efficiency across both 2D and 3D environments
(simulated and real world), demonstrating competitive performance with
traditional and state of the art exploration approaches while offering a
biologically inspired navigation approach.

</details>


### [587] [Multimodal Spiking Neural Network for Space Robotic Manipulation](https://arxiv.org/abs/2508.07287)
*Liwen Zhang,Dong Zhou,Shibo Shao,Zihao Su,Guanghui Sun*

Main category: cs.RO

TL;DR: The paper introduces a spiking neural network-based multimodal control framework for robotic arms in space stations, designed for efficient resource use and robust autonomous operations.


<details>
  <summary>Details</summary>
Motivation: Address operational and resource constraints while enabling autonomous manipulation tasks for robotic arms in space environments.

Method: A multimodal SNN framework combining geometric states, tactile, and semantic information with a three-stage curriculum reinforcement learning scheme.

Result: The framework excelled in tasks like target approach and object grasping, surpassing baseline methods in task success rate and energy efficiency.

Conclusion: The approach proves effective and suitable for aerospace applications, presenting reliable control strategies for robotic arms under space constraints.

Abstract: This paper presents a multimodal control framework based on spiking neural
networks (SNNs) for robotic arms aboard space stations. It is designed to cope
with the constraints of limited onboard resources while enabling autonomous
manipulation and material transfer in space operations. By combining geometric
states with tactile and semantic information, the framework strengthens
environmental awareness and contributes to more robust control strategies. To
guide the learning process progressively, a dual-channel, three-stage
curriculum reinforcement learning (CRL) scheme is further integrated into the
system. The framework was tested across a range of tasks including target
approach, object grasping, and stable lifting with wall-mounted robotic arms,
demonstrating reliable performance throughout. Experimental evaluations
demonstrate that the proposed method consistently outperforms baseline
approaches in both task success rate and energy efficiency. These findings
highlight its suitability for real-world aerospace applications.

</details>


### [588] [A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks](https://arxiv.org/abs/2508.07319)
*Yanzhao Yu,Haotian Yang,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: The paper tackles the challenge of controlling deformable linear objects (DLOs) by proposing a hybrid force-position control strategy, integrating state trajectory planning, model predictive control (MPC), and advanced dynamics modeling.


<details>
  <summary>Details</summary>
Motivation: The manipulation of DLOs, such as wires and cables, is essential in fields like electronics and medical surgeries but faces significant challenges due to their infinite degrees of freedom, nonlinear dynamics, and underactuated systems.

Method: The method combines force and position representations of DLOs, utilizes state trajectory planning in the force space, MPC in the position space, and incorporates a dynamics model with an action encoder, property extractor, and graph processor based on Graph Attention Networks.

Result: Both simulations and real-world experiments show the proposed approach effectively achieves efficient and stable shape control of DLOs.

Conclusion: The hybrid force-position strategy significantly enhances control over DLOs, paving the way for practical applications in demanding scenarios. Implementation resources are publicly shared.

Abstract: Manipulating deformable linear objects (DLOs) such as wires and cables is
crucial in various applications like electronics assembly and medical
surgeries. However, it faces challenges due to DLOs' infinite degrees of
freedom, complex nonlinear dynamics, and the underactuated nature of the
system. To address these issues, this paper proposes a hybrid force-position
strategy for DLO shape control. The framework, combining both force and
position representations of DLO, integrates state trajectory planning in the
force space and Model Predictive Control (MPC) in the position space. We
present a dynamics model with an explicit action encoder, a property extractor
and a graph processor based on Graph Attention Networks. The model is used in
the MPC to enhance prediction accuracy. Results from both simulations and
real-world experiments demonstrate the effectiveness of our approach in
achieving efficient and stable shape control of DLOs. Codes and videos are
available at https://sites.google.com/view/dlom.

</details>


### [589] [Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)](https://arxiv.org/abs/2508.07323)
*Adeetya Uppal,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: The paper proposes an enhanced Artificial Potential Field (E-APF) framework combined with a hybrid trajectory optimization method to overcome limitations in robotic path planning, such as local minima and oscillatory motion.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of robotic trajectory planning in dynamic and cluttered environments, particularly ensuring time-efficient and smooth motion under actuation constraints.

Method: Introduces an Energy-based Artificial Potential Field (E-APF) framework that incorporates position and velocity-dependent potential functions, integrated with a hybrid trajectory optimizer for minimizing jerk and execution time under constraints.

Result: Simulation validation using a 7-degree-of-freedom Kinova Gen3 shows collision-free, smooth, time-efficient, and oscillation-free trajectories in the presence of obstacles.

Conclusion: The proposed framework demonstrates robust collision avoidance and trajectory optimization, paving the way for real-world implementation and integration with reactive control strategies.

Abstract: Robotic trajectory planning in dynamic and cluttered environments remains a
critical challenge, particularly when striving for both time efficiency and
motion smoothness under actuation constraints. Traditional path planner, such
as Artificial Potential Field (APF), offer computational efficiency but suffer
from local minima issue due to position-based potential field functions and
oscillatory motion near the obstacles due to Newtonian mechanics. To address
this limitation, an Energy-based Artificial Potential Field (APF) framework is
proposed in this paper that integrates position and velocity-dependent
potential functions. E-APF ensures dynamic adaptability and mitigates local
minima, enabling uninterrupted progression toward the goal. The proposed
framework integrates E-APF with a hybrid trajectory optimizer that jointly
minimizes jerk and execution time under velocity and acceleration constraints,
ensuring geometric smoothness and time efficiency. The entire framework is
validated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic
manipulator. The results demonstrate collision-free, smooth, time-efficient,
and oscillation-free trajectory in the presence of obstacles, highlighting the
efficacy of the combined trajectory optimization and real-time obstacle
avoidance approach. This work lays the foundation for future integration with
reactive control strategies and physical hardware deployment in real-world
manipulation tasks.

</details>


### [590] [MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control](https://arxiv.org/abs/2508.07387)
*Basant Sharma,Prajyot Jadhav,Pranjal Paul,K. Madhava Krishna,Arun Kumar Singh*

Main category: cs.RO

TL;DR: The paper proposes a method for navigating cluttered environments using a single RGB camera without relying on depth estimation for collision-checking.


<details>
  <summary>Details</summary>
Motivation: Current methods for collision-checking in navigation rely on depth estimation, but depth estimates from vision models are often unreliable in cluttered environments.

Method: The authors introduce a collision model that uses noisy depth estimates as context input to predict obstacle clearance. This is integrated with a risk-aware model predictive control planner.

Result: In real-world tests, the proposed method achieved 9x and 7x success rate improvements over NoMaD and the ROS stack, respectively. Ablation studies confirmed design effectiveness.

Conclusion: Using noisy depth as a context input to train a collision model and co-training risk metrics enhances navigation performance in challenging environments.

Abstract: Navigating unknown environments with a single RGB camera is challenging, as
the lack of depth information prevents reliable collision-checking. While some
methods use estimated depth to build collision maps, we found that depth
estimates from vision foundation models are too noisy for zero-shot navigation
in cluttered environments.
  We propose an alternative approach: instead of using noisy estimated depth
for direct collision-checking, we use it as a rich context input to a learned
collision model. This model predicts the distribution of minimum obstacle
clearance that the robot can expect for a given control sequence. At inference,
these predictions inform a risk-aware MPC planner that minimizes estimated
collision risk. Our joint learning pipeline co-trains the collision model and
risk metric using both safe and unsafe trajectories. Crucially, our
joint-training ensures optimal variance in our collision model that improves
navigation in highly cluttered environments. Consequently, real-world
experiments show 9x and 7x improvements in success rates over NoMaD and the ROS
stack, respectively. Ablation studies further validate the effectiveness of our
design choices.

</details>


### [591] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: This paper introduces the Agriculture to Agriculture (A2A) benchmark for improving navigation in agricultural robots and proposes AgriVLN, a Vision-and-Language Navigation approach tailored for agricultural settings.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of mobility and adaptability in agricultural robots due to their reliance on manual operation or fixed railway movement, aiming to leverage Vision-and-Language Navigation (VLN) for agricultural applications.

Method: The A2A benchmark provides realistic agricultural scenes data captured via RGB videos from a quadruped robot, alongside the AgriVLN baseline employing Vision-Language Models (VLM) with template-based prompts. An STL module is introduced for instruction decomposition.

Result: AgriVLN improves navigation success rate in agricultural environments, particularly excelling with short instructions. Integration of the STL module boosts the Success Rate from 0.33 to 0.47, outperforming existing VLN methods.

Conclusion: The paper successfully presents a novel benchmark and method tailored for agricultural robots, advancing VLN performance and adaptability in real-world agricultural scenarios.

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks,
nevertheless, still heavily rely on manual operation or untransportable railway
for movement, resulting in limited mobility and poor adaptability.
Vision-and-Language Navigation (VLN) enables robots to navigate to the target
destinations following natural language instructions, demonstrating strong
performance on several domains. However, none of the existing benchmarks or
methods is specifically designed for agricultural scenes. To bridge this gap,
we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560
episodes across six diverse agricultural scenes, in which all realistic RGB
videos are captured by front-facing camera on a quadruped robot at a height of
0.38 meters, aligning with the practical deployment conditions. Meanwhile, we
propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)
baseline based on Vision-Language Model (VLM) prompted with carefully crafted
templates, which can understand both given instructions and agricultural
environments to generate appropriate low-level actions for robot control. When
evaluated on A2A, AgriVLN performs well on short instructions but struggles
with long instructions, because it often fails to track which part of the
instruction is currently being executed. To address this, we further propose
Subtask List (STL) instruction decomposition module and integrate it into
AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare
AgriVLN with several existing VLN methods, demonstrating the state-of-the-art
performance in the agricultural domain.

</details>


### [592] [Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics](https://arxiv.org/abs/2508.07421)
*Zixi Jia,Hongbin Gao,Fashe Li,Jiqiang Liu,Hexiao Li,Qinghua Liu*

Main category: cs.RO

TL;DR: The paper introduces a Triple-S framework utilizing multiple LLMs for effective robot policy coding in long-horizon tasks, achieving improved robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To address API parameter, comments, and sequencing errors in long-horizon tasks when using LLMs for generating robot policy code.

Method: Introduced a collaborative Triple-S framework with multiple LLMs playing specific roles in a closed-loop Simplification-Solution-Summary process, coupled with a demonstration library update mechanism.

Result: The framework achieved a success rate of 89% in task execution within the LDIP dataset and was validated in both simulated and real-world experiments.

Conclusion: The Triple-S framework significantly enhances success rates and robustness for long-horizon robotic tasks by leveraging collaboration among LLMs and incremental learning from task successes.

Abstract: Leveraging Large Language Models (LLMs) to write policy code for controlling
robots has gained significant attention. However, in long-horizon implicative
tasks, this approach often results in API parameter, comments and sequencing
errors, leading to task failure. To address this problem, we propose a
collaborative Triple-S framework that involves multiple LLMs. Through
In-Context Learning, different LLMs assume specific roles in a closed-loop
Simplification-Solution-Summary process, effectively improving success rates
and robustness in long-horizon implicative tasks. Additionally, a novel
demonstration library update mechanism which learned from success allows it to
generalize to previously failed tasks. We validate the framework in the
Long-horizon Desktop Implicative Placement (LDIP) dataset across various
baseline models, where Triple-S successfully executes 89% of tasks in both
observable and partially observable scenarios. Experiments in both simulation
and real-world robot settings further validated the effectiveness of Triple-S.
Our code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.

</details>


### [593] [A Learning-Based Framework for Collision-Free Motion Planning](https://arxiv.org/abs/2508.07502)
*Mateus Salomão,Tianyü Ren,Alexander König*

Main category: cs.RO

TL;DR: This paper proposes a learning-based method to optimize Circular Field motion planning parameters, enhancing trajectory generation in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of manual tuning in CF-based motion planners, particularly for effective trajectory generation in complex settings.

Method: A deep neural network predicts optimal planner gains from depth images, integrated with a CUDA-accelerated perception module, predictive planning, and Bayesian optimization-generated datasets.

Result: The framework allows real-time, manual tuning-free planning with improved task success and generalization, validated in simulation and on a Franka Emika Panda robot.

Conclusion: This approach enhances CF motion planning with learning-based parameter optimization, achieving robust and efficient real-time trajectory generation.

Abstract: This paper presents a learning-based extension to a Circular Field (CF)-based
motion planner for efficient, collision-free trajectory generation in cluttered
environments. The proposed approach overcomes the limitations of hand-tuned
force field parameters by employing a deep neural network trained to infer
optimal planner gains from a single depth image of the scene. The pipeline
incorporates a CUDA-accelerated perception module, a predictive agent-based
planning strategy, and a dataset generated through Bayesian optimization in
simulation. The resulting framework enables real-time planning without manual
parameter tuning and is validated both in simulation and on a Franka Emika
Panda robot. Experimental results demonstrate successful task completion and
improved generalization compared to classical planners.

</details>


### [594] [Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2508.07560)
*Yan Gong,Naibang Wang,Jianli Lu,Xinyu Zhang,Yongsheng Gao,Jie Zhao,Zifan Huang,Haozhi Bai,Nanxin Zeng,Nayu Su,Lei Yang,Ziying Song,Xiaoxi Hu,Xinmin Jiang,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.RO

TL;DR: This paper surveys Bird's-Eye-View (BEV) perception in autonomous driving, emphasizing safety-critical aspects and open-world challenges.


<details>
  <summary>Details</summary>
Motivation: To address challenges in ensuring safety and reliability of BEV perception in complex real-world autonomous driving scenarios.

Method: The paper reviews state-of-the-art BEV frameworks and strategies across single-modality, multimodal, and multi-agent setups, evaluates datasets, and identifies open-world challenges.

Result: The survey presents a systematic analysis of BEV perception with an emphasis on its robustness in different scenarios, and identifies gaps and opportunities for improvement.

Conclusion: Progress in BEV perception must address challenges like open-set recognition, sensor issues, and communication latency, and focus on integration with modern AI systems for safer autonomous driving.

Abstract: Bird's-Eye-View (BEV) perception has become a foundational paradigm in
autonomous driving, enabling unified spatial representations that support
robust multi-sensor fusion and multi-agent collaboration. As autonomous
vehicles transition from controlled environments to real-world deployment,
ensuring the safety and reliability of BEV perception in complex scenarios -
such as occlusions, adverse weather, and dynamic traffic - remains a critical
challenge. This survey provides the first comprehensive review of BEV
perception from a safety-critical perspective, systematically analyzing
state-of-the-art frameworks and implementation strategies across three
progressive stages: single-modality vehicle-side, multimodal vehicle-side, and
multi-agent collaborative perception. Furthermore, we examine public datasets
encompassing vehicle-side, roadside, and collaborative settings, evaluating
their relevance to safety and robustness. We also identify key open-world
challenges - including open-set recognition, large-scale unlabeled data, sensor
degradation, and inter-agent communication latency - and outline future
research directions, such as integration with end-to-end autonomous driving
systems, embodied intelligence, and large language models.

</details>


### [595] [Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer](https://arxiv.org/abs/2508.07566)
*Conor K. Trygstad,Cody R. Longwell,Francisco M. F. R. Gonçalves,Elijah K. Blankenship,Néstor O. Pérez-Arancibia*

Main category: cs.RO

TL;DR: This paper presents an advanced version of FRISSHBot, a biologically-inspired aquatic robot, powered by a new SMA-based actuator, enabling controllable 2D movement and improved performance metrics.


<details>
  <summary>Details</summary>
Motivation: To demonstrate feedback-controlled trajectory tracking for a subgram-scale aquatic robot, addressing limitations of controllability and swimming efficiency in previous designs.

Method: The robot's design was improved with a physics-informed approach, involving an enlarged head, shortened tail, and integration of a new SMA-based bimorph actuator for enhanced controllability and performance.

Result: The evolved FRISSHBot achieved significant improvements, including forward swimming speeds of 13.6 mm/s, RMS tracking errors as low as 2.6 mm, turning rates up to 13.1°/s, and turning radii of 10 mm.

Conclusion: The enhanced FRISSHBot represents a major advancement in small-scale aquatic robots by achieving efficient and precise 2D maneuverability, paving the way for further applications in miniature robotics.

Abstract: We present an evolved steerable version of the single-tail
Fish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg
biologically inspired swimmer, which is driven by a new shape-memory alloy
(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the
two-dimensional (2D) space, which enabled the first demonstration of
feedback-controlled trajectory tracking of a single-tail aquatic robot with
onboard actuation at the subgram scale. These new capabilities are the result
of a physics-informed design with an enlarged head and shortened tail relative
to those of the original platform. Enhanced by its design, this new platform
achieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over
four times that of the original platform. Furthermore, when following 2D
references in closed loop, the tested FRISSHBot prototype attains forward
swimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as
low as 2.6 mm, turning rates of up to 13.1 {\deg}/s, and turning radii as small
as 10 mm.

</details>


### [596] [In-situ Value-aligned Human-Robot Interactions with Physical Constraints](https://arxiv.org/abs/2508.07606)
*Hongtao Li,Ziyuan Jiao,Xiaofeng Liu,Hangxin Liu,Zilong Zheng*

Main category: cs.RO

TL;DR: The paper proposes a framework for cognitive robots to incorporate human preferences and physical constraints while completing tasks using In-Context Learning from Human Feedback (ICLHF), validated through extensive experiments.


<details>
  <summary>Details</summary>
Motivation: Cognitive robots need to evolve beyond just completing tasks; they should be capable of learning and adjusting to human preferences for future adaptability.

Method: The framework involves creating a benchmark of everyday activities and introducing ICLHF, where robots receive human feedback through direct instructions and daily-life adjustments.

Result: Experiments show that the ICLHF framework efficiently plans tasks while balancing physical constraints and human preferences.

Conclusion: Integrating human preferences and physical constraints enables cognitive robots to better adapt to real-world tasks and scenarios.

Abstract: Equipped with Large Language Models (LLMs), human-centered robots are now
capable of performing a wide range of tasks that were previously deemed
challenging or unattainable. However, merely completing tasks is insufficient
for cognitive robots, who should learn and apply human preferences to future
scenarios. In this work, we propose a framework that combines human preferences
with physical constraints, requiring robots to complete tasks while considering
both. Firstly, we developed a benchmark of everyday household activities, which
are often evaluated based on specific preferences. We then introduced
In-Context Learning from Human Feedback (ICLHF), where human feedback comes
from direct instructions and adjustments made intentionally or unintentionally
in daily life. Extensive sets of experiments, testing the ICLHF to generate
task plans and balance physical constraints with preferences, have demonstrated
the efficiency of our approach.

</details>


### [597] [End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy](https://arxiv.org/abs/2508.07611)
*Zifan Wang,Xun Yang,Jianzhuang Zhao,Jiaming Zhou,Teli Ma,Ziyao Gao,Arash Ajoudani,Junwei Liang*

Main category: cs.RO

TL;DR: The work introduces an end-to-end locomotion policy utilizing LiDAR data for humanoid robots, ensuring safe and socially-aware navigation in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of current reinforcement learning methods for humanoid robots' navigation, such as blind controllers and inadequate perception of 3D obstacles.

Method: The authors propose a CMDP-based framework combining Control Barrier Function principles with P3O training and comfort-oriented rewards for smooth, predictable motions.

Result: The proposed framework successfully transfers from simulation to real-world, enabling a humanoid robot to perform safe and agile navigation in cluttered, dynamic environments.

Conclusion: The approach effectively blends robust navigation, safety, and social awareness for humanoid robots, demonstrating its feasibility in real-world settings.

Abstract: The deployment of humanoid robots in unstructured, human-centric environments
requires navigation capabilities that extend beyond simple locomotion to
include robust perception, provable safety, and socially aware behavior.
Current reinforcement learning approaches are often limited by blind
controllers that lack environmental awareness or by vision-based systems that
fail to perceive complex 3D obstacles. In this work, we present an end-to-end
locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to
motor commands, enabling robust navigation in cluttered dynamic scenes. We
formulate the control problem as a Constrained Markov Decision Process (CMDP)
to formally separate safety from task objectives. Our key contribution is a
novel methodology that translates the principles of Control Barrier Functions
(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal
Policy Optimization (P3O) to enforce safety constraints during training.
Furthermore, we introduce a set of comfort-oriented rewards, grounded in
human-robot interaction research, to promote motions that are smooth,
predictable, and less intrusive. We demonstrate the efficacy of our framework
through a successful sim-to-real transfer to a physical humanoid robot, which
exhibits agile and safe navigation around both static and dynamic 3D obstacles.

</details>


### [598] [Grasp-HGN: Grasping the Unexpected](https://arxiv.org/abs/2508.07648)
*Mehrshad Zandigohar,Mallesham Dasari,Gunar Schirner*

Main category: cs.RO

TL;DR: This paper addresses challenges in robotic prosthetic hand control for transradial amputees by introducing novel approaches for unseen object grasp estimation.


<details>
  <summary>Details</summary>
Motivation: Current grasp models fail to effectively generalize to unseen objects, which negatively impacts the independence and quality of life for users relying on robotic prosthetic hands.

Method: The paper proposes a Vision Language Model called Grasp-LLaVA for improved grasp estimation accuracy and introduces the Hybrid Grasp Network (HGN) for optimizing performance between edge and cloud deployment.

Result: Grasp-LLaVA improves unseen object accuracy significantly (50.2% vs. 36.7%), while HGN enhances semantic projection accuracy by 5.6% with 3.5x speedup, achieving 86% accuracy in real-world scenarios.

Conclusion: Integrating Grasp-LLaVA with HGN provides a robust and efficient solution for enhancing grasp estimation on real-world objects, advancing the functionality of prosthetic hands.

Abstract: For transradial amputees, robotic prosthetic hands promise to regain the
capability to perform daily living activities. To advance next-generation
prosthetic hand control design, it is crucial to address current shortcomings
in robustness to out of lab artifacts, and generalizability to new
environments. Due to the fixed number of object to interact with in existing
datasets, contrasted with the virtually infinite variety of objects encountered
in the real world, current grasp models perform poorly on unseen objects,
negatively affecting users' independence and quality of life.
  To address this: (i) we define semantic projection, the ability of a model to
generalize to unseen object types and show that conventional models like YOLO,
despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose
Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to
infer the suitable grasp type estimate based on the object's physical
characteristics resulting in a significant 50.2% accuracy over unseen object
types compared to 36.7% accuracy of an SOTA grasp estimation model.
  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp
Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp
estimation on edge and accurate cloud inference as a fail-safe, effectively
expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)
enables dynamic switching between edge and cloud models, improving semantic
projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object
types. Over a real-world sample mix, it reaches 86% average accuracy (12.2%
gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.

</details>


### [599] [GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions](https://arxiv.org/abs/2508.07650)
*Helong Huang,Min Cen,Kai Tan,Xingyue Quan,Guowei Huang,Hong Zhang*

Main category: cs.RO

TL;DR: This paper introduces GraphCoT-VLA, a vision-language-action (VLA) model that addresses ambiguities in language instructions and incorporates 3D spatial reasoning for robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Current vision-language-action models struggle to handle ambiguous language instructions and dynamic, unknown environmental states, with limited 3D spatial understanding.

Method: The proposed model includes a Chain-of-Thought reasoning module for high-level and low-level task interpretation and introduces a real-time 3D Pose-Object graph to better model 3D environments. It also employs a dropout hybrid reasoning strategy for efficient control outputs.

Result: GraphCoT-VLA achieves improved task success rates, faster response times, and enhanced generalization and robustness in open and uncertain settings, as demonstrated in various real-world robotic tasks.

Conclusion: GraphCoT-VLA substantially improves upon existing methods, offering a more capable framework for robotic manipulation in complex, real-world scenarios.

Abstract: Vision-language-action models have emerged as a crucial paradigm in robotic
manipulation. However, existing VLA models exhibit notable limitations in
handling ambiguous language instructions and unknown environmental states.
Furthermore, their perception is largely constrained to static two-dimensional
observations, lacking the capability to model three-dimensional interactions
between the robot and its environment. To address these challenges, this paper
proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's
ability to interpret ambiguous instructions and improve task planning, we
design a structured Chain-of-Thought reasoning module that integrates
high-level task understanding and planning, failed task feedback, and low-level
imaginative reasoning about future object positions and robot actions.
Additionally, we construct a real-time updatable 3D Pose-Object graph, which
captures the spatial configuration of robot joints and the topological
relationships between objects in 3D space, enabling the model to better
understand and manipulate their interactions. We further integrates a dropout
hybrid reasoning strategy to achieve efficient control outputs. Experimental
results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA
significantly outperforms existing methods in terms of task success rate and
response speed, exhibiting strong generalization and robustness in open
environments and under uncertain instructions.

</details>


### [600] [MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication](https://arxiv.org/abs/2508.07657)
*Zhuoli Tian,Yuyang Zhang,Jinsheng Wei,Meng Guo*

Main category: cs.RO

TL;DR: The paper introduces MoRoCo, a framework for human-robot coordination in multi-robot teams operating under communication constraints. It adapts to three switching modes to enable efficient exploration and interaction.


<details>
  <summary>Details</summary>
Motivation: To address the coordination challenges in multi-operator, multi-robot systems under communication constraints by incorporating real-time human-robot interactions, especially for complex tasks like search-and-rescue missions.

Method: The authors propose MoRoCo, a framework that integrates distributed algorithms allowing robots to switch between three operational modes: spread, migrate, and chain, to manage exploration and maintain connectivity under limited communication.

Result: Through simulations and hardware experiments, MoRoCo was validated to handle human-robot interactions effectively, ensuring reliable coordination and task efficiency with restricted communication.

Conclusion: The framework significantly improves the reliability and effectiveness of human-robot collaboration under constrained communication, advancing human-in-the-loop multi-robot autonomy in adverse environments.

Abstract: Fleets of autonomous robots are increasingly deployed alongside multiple
human operators to explore unknown environments, identify salient features, and
perform complex tasks in scenarios such as subterranean exploration,
reconnaissance, and search-and-rescue missions. In these contexts,
communication is often severely limited to short-range exchanges via ad-hoc
networks, posing challenges to coordination. While recent studies have
addressed multi-robot exploration under communication constraints, they largely
overlook the essential role of human operators and their real-time interaction
with robotic teams. Operators may demand timely updates on the exploration
progress and robot status, reprioritize or cancel tasks dynamically, or request
live video feeds and control access. Conversely, robots may seek human
confirmation for anomalous events or require help recovering from motion or
planning failures. To enable such bilateral, context-aware interactions under
restricted communication, this work proposes MoRoCo, a unified framework for
online coordination and exploration in multi-operator, multi-robot systems.
MoRoCo enables the team to adaptively switch among three coordination modes:
spread mode for parallelized exploration with intermittent data sharing,
migrate mode for coordinated relocation, and chain mode for maintaining
high-bandwidth connectivity through multi-hop links. These transitions are
managed through distributed algorithms via only local communication. Extensive
large-scale human-in-the-loop simulations and hardware experiments validate the
necessity of incorporating human robot interactions and demonstrate that MoRoCo
enables efficient, reliable coordination under limited communication, marking a
significant step toward robust human-in-the-loop multi-robot autonomy in
challenging environments.

</details>


### [601] [Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning](https://arxiv.org/abs/2508.07686)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jiaqi Ma,Jia Hu*

Main category: cs.RO

TL;DR: The paper introduces Risk Map as Middleware (RiskMM), an interpretable cooperative end-to-end driving framework that improves autonomous driving by addressing occlusion, perception range limitations, and system interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end autonomous driving systems suffer from occlusion, limited perception range, and lack of interpretability, leading to hazardous driving and unreliable systems.

Method: RiskMM employs a Transformer-based multi-agent spatiotemporal risk map representation and a learning-based Model Predictive Control (MPC) module for trajectory planning. The MPC module links learned parameters to explicit physical constraints for interpretability.

Result: RiskMM demonstrates superior and robust risk-aware trajectory planning performance on the real-world V2XPnP-Seq dataset, improving interpretability and cooperative driving.

Conclusion: The study positions RiskMM as a significant advancement in autonomous driving research that tackles key limitations in end-to-end frameworks. The planned release of the codebase aims to drive future research in cooperative interpretable driving systems.

Abstract: End-to-end paradigm has emerged as a promising approach to autonomous
driving. However, existing single-agent end-to-end pipelines are often
constrained by occlusion and limited perception range, resulting in hazardous
driving. Furthermore, their black-box nature prevents the interpretability of
the driving behavior, leading to an untrustworthiness system. To address these
limitations, we introduce Risk Map as Middleware (RiskMM) and propose an
interpretable cooperative end-to-end driving framework. The risk map learns
directly from the driving data and provides an interpretable spatiotemporal
representation of the scenario from the upstream perception and the
interactions between the ego vehicle and the surrounding environment for
downstream planning. RiskMM first constructs a multi-agent spatiotemporal
representation with unified Transformer-based architecture, then derives
risk-aware representations by modeling interactions among surrounding
environments with attention. These representations are subsequently fed into a
learning-based Model Predictive Control (MPC) module. The MPC planner
inherently accommodates physical constraints and different vehicle types and
can provide interpretation by aligning learned parameters with explicit MPC
elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm
that RiskMM achieves superior and robust performance in risk-aware trajectory
planning, significantly enhancing the interpretability of the cooperative
end-to-end driving framework. The codebase will be released to facilitate
future research in this field.

</details>


### [602] [LAURON VI: A Six-Legged Robot for Dynamic Walking](https://arxiv.org/abs/2508.07689)
*Christian Eichmann,Sabine Bellmann,Nicolas Hügel,Louis-Elias Enslin,Carsten Plasberg,Georg Heppner,Arne Roennau,Ruediger Dillmann*

Main category: cs.RO

TL;DR: This paper introduces LAURON VI, a six-legged robot designed for dynamic locomotion using three control approaches, tested extensively on varied terrains.


<details>
  <summary>Details</summary>
Motivation: Six-legged robots excel in stability and flexibility for difficult terrains but lack fast walking gaits for easier surfaces, limiting real-world application potential.

Method: The study designed and implemented three control approaches (kinematic-based, model-predictive, and reinforcement-learning) for high-frequency control on LAURON VI's 18 series elastic joint actuators.

Result: The control approaches were tested in both lab environments and challenging field missions, demonstrating suitability for faster and more dynamic locomotion.

Conclusion: Enhancing six-legged robots with fast locomotion strategies makes them more adaptable for diverse real-world environments.

Abstract: Legged locomotion enables robotic systems to traverse extremely challenging
terrains. In many real-world scenarios, the terrain is not that difficult and
these mixed terrain types introduce the need for flexible use of different
walking strategies to achieve mission goals in a fast, reliable, and
energy-efficient way. Six-legged robots have a high degree of flexibility and
inherent stability that aids them in traversing even some of the most difficult
terrains, such as collapsed buildings. However, their lack of fast walking
gaits for easier surfaces is one reason why they are not commonly applied in
these scenarios.
  This work presents LAURON VI, a six-legged robot platform for research on
dynamic walking gaits as well as on autonomy for complex field missions. The
robot's 18 series elastic joint actuators offer high-frequency interfaces for
Cartesian impedance and pure torque control. We have designed, implemented, and
compared three control approaches: kinematic-based, model-predictive, and
reinforcement-learned controllers. The robot hardware and the different control
approaches were extensively tested in a lab environment as well as on a Mars
analog mission. The introduction of fast locomotion strategies for LAURON VI
makes six-legged robots vastly more suitable for a wide range of real-world
applications.

</details>


### [603] [Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation](https://arxiv.org/abs/2508.07758)
*Antonio Rosales,Alaa Abderrahim,Markku Suomalainen,Mikael Haag,Tapio Heikkilä*

Main category: cs.RO

TL;DR: This paper proposes a collaborative robot-crane scheme for more efficient and safer payload manipulation using admittance control.


<details>
  <summary>Details</summary>
Motivation: The current industrial practice of manually guiding payloads lifted by cranes is labor-intensive and risky.

Method: The approach combines crane lifting with robot end-effector guidance, utilizing two admittance transfer functions for smooth interaction and collaboration.

Result: The method enables collaborative movement of the robot and crane to accurately position payloads, validated through simulations and experiments.

Conclusion: The introduced scheme enhances payload manipulation efficiency and safety by enabling smooth robot-crane collaboration.

Abstract: This paper presents a scheme to enhance payload manipulation using a robot
collaborating with an overhead crane. In the current industrial practice, when
the crane's payload has to be accurately manipulated and located in a desired
position, the task becomes laborious and risky since the operators have to
guide the fine motions of the payload by hand. In the proposed collaborative
scheme, the crane lifts the payload while the robot's end-effector guides it
toward the desired position. The only link between the robot and the crane is
the interaction force produced during the guiding of the payload. Two
admittance transfer functions are considered to accomplish harmless and smooth
contact with the payload. The first is used in a position-based admittance
control integrated with the robot. The second one adds compliance to the crane
by processing the interaction force through the admittance transfer function to
generate a crane's velocity command that makes the crane follow the payload.
Then the robot's end-effector and the crane move collaboratively to guide the
payload to the desired location. A method is presented to design the admittance
controllers that accomplish a fluent robot-crane collaboration. Simulations and
experiments validating the scheme potential are shown.

</details>


### [604] [AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation](https://arxiv.org/abs/2508.07770)
*Yizheng Zhang,Zhenjun Yu,Jiaxin Lai,Cewu Lu,Lei Han*

Main category: cs.RO

TL;DR: The paper presents AgentWorld, a platform for developing household mobile manipulation skills using automated scene construction, teleoperation, and simulation-to-reality benchmarking.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between simulation-based robotic training and real-world deployment in complex home environments.

Method: AgentWorld combines automated scene construction, dual-mode teleoperation for data collection, and benchmarking of advanced imitation learning methods on diverse household tasks.

Result: The platform and dataset enable sim-to-real transfer of robotic skills, showing the effectiveness of various imitation learning methods on mobile manipulation.

Conclusion: AgentWorld provides a scalable solution for robotic skill development in realistic home settings, with openly available code and datasets.

Abstract: We introduce AgentWorld, an interactive simulation platform for developing
household mobile manipulation capabilities. Our platform combines automated
scene construction that encompasses layout generation, semantic asset
placement, visual material configuration, and physics simulation, with a
dual-mode teleoperation system supporting both wheeled bases and humanoid
locomotion policies for data collection. The resulting AgentWorld Dataset
captures diverse tasks ranging from primitive actions (pick-and-place,
push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)
across living rooms, bedrooms, and kitchens. Through extensive benchmarking of
imitation learning methods including behavior cloning, action chunking
transformers, diffusion policies, and vision-language-action models, we
demonstrate the dataset's effectiveness for sim-to-real transfer. The
integrated system provides a comprehensive solution for scalable robotic skill
acquisition in complex home environments, bridging the gap between
simulation-based training and real-world deployment. The code, datasets will be
available at https://yizhengzhang1.github.io/agent_world/

</details>


### [605] [SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing](https://arxiv.org/abs/2508.07814)
*Malaika Zafar,Roohan Ahmed Khan,Faryal Batool,Yasheerah Yaqoot,Ziang Guo,Mikhail Litvinov,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: The paper introduces SwarmVLM, a system combining UAVs and ground robots for efficient navigation using Vision Language Model (VLM) and Retrieval-Augmented Generation (RAG).


<details>
  <summary>Details</summary>
Motivation: The need for efficient logistics integrating UAVs and AGVs for navigation in dense environments while addressing UAV limitations.

Method: SwarmVLM employs VLM and RAG for environmental adaptation, with UAVs using Artificial Potential Field (APF) planning and ground robots relying on adaptive impedance control.

Result: Achieved a 92% success rate in real-world trials; 8% object detection accuracy under optimal lighting; safe navigation with occasional lateral deviations up to 50 cm.

Conclusion: SwarmVLM successfully enables semantic collaboration and robust navigation in cluttered environments, balancing obstacles and UAV limitations.

Abstract: With the growing demand for efficient logistics, unmanned aerial vehicles
(UAVs) are increasingly being paired with automated guided vehicles (AGVs).
While UAVs offer the ability to navigate through dense environments and varying
altitudes, they are limited by battery life, payload capacity, and flight
duration, necessitating coordinated ground support.
  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by
enabling semantic collaboration between UAVs and ground robots through
impedance control. The system leverages the Vision Language Model (VLM) and the
Retrieval-Augmented Generation (RAG) to adjust impedance control parameters in
response to environmental changes. In this framework, the UAV acts as a leader
using Artificial Potential Field (APF) planning for real-time navigation, while
the ground robot follows via virtual impedance links with adaptive link
topology to avoid collisions with short obstacles.
  The system demonstrated a 92% success rate across 12 real-world trials. Under
optimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in
object detection and selection of impedance parameters. The mobile robot
prioritized short obstacle avoidance, occasionally resulting in a lateral
deviation of up to 50 cm from the UAV path, which showcases safe navigation in
a cluttered setting.

</details>


### [606] [Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans](https://arxiv.org/abs/2508.07839)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.RO

TL;DR: The study developed a multimodal system that combines vibrations and audio cues to enhance emotional communication in robots, showing multisensory integration significantly improves decoding accuracy.


<details>
  <summary>Details</summary>
Motivation: Human communication integrates touch with other sensory inputs, such as auditory cues, for nuanced emotional expression. However, robots' ability to use touch for emotional communication is underexplored.

Method: The researchers built a system with 25 vibration motors synchronized with audio playback to deliver combined haptic-audio stimuli. They conducted an experiment with 32 Chinese participants to evaluate emotional responses to these stimuli.

Result: The findings indicate combined haptic-audio stimuli improve emotional decoding accuracy over single modalities. Individual channels like vibration or sound can effectively convey specific emotions but have limitations when used alone.

Conclusion: Multisensory integration, blending touch and sound, is crucial for effective emotional communication in human-robot interaction, offering complementary strengths between haptic and auditory channels.

Abstract: Affective tactile interaction constitutes a fundamental component of human
communication. In natural human-human encounters, touch is seldom experienced
in isolation; rather, it is inherently multisensory. Individuals not only
perceive the physical sensation of touch but also register the accompanying
auditory cues generated through contact. The integration of haptic and auditory
information forms a rich and nuanced channel for emotional expression. While
extensive research has examined how robots convey emotions through facial
expressions and speech, their capacity to communicate social gestures and
emotions via touch remains largely underexplored. To address this gap, we
developed a multimodal interaction system incorporating a 5*5 grid of 25
vibration motors synchronized with audio playback, enabling robots to deliver
combined haptic-audio stimuli. In an experiment involving 32 Chinese
participants, ten emotions and six social gestures were presented through
vibration, sound, or their combination. Participants rated each stimulus on
arousal and valence scales. The results revealed that (1) the combined
haptic-audio modality significantly enhanced decoding accuracy compared to
single modalities; (2) each individual channel-vibration or sound-effectively
supported certain emotions recognition, with distinct advantages depending on
the emotional expression; and (3) gestures alone were generally insufficient
for conveying clearly distinguishable emotions. These findings underscore the
importance of multisensory integration in affective human-robot interaction and
highlight the complementary roles of haptic and auditory cues in enhancing
emotional communication.

</details>


### [607] [DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts](https://arxiv.org/abs/2508.07842)
*Yutong Shen,Hangxu Liu,Penghui Liu,Ruizhe Xia,Tianyi Yao,Yitong Sun,Tongtong Feng*

Main category: cs.RO

TL;DR: DETACH is a framework for solving long-horizon tasks using biologically inspired dual-stream disentanglement to improve generalization and execution efficiency across domains.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for long-horizon tasks in human-scene interaction lack the ability to generalize to new combinations of environments and skills, as they rely on skill-chaining of pre-trained subtasks tightly coupled with environment observations.

Method: The paper proposes DETACH, a framework inspired by the brain's dual-stream mechanism, with two modules: an environment learning module for spatial understanding and scene semantics, and a skill learning module for task execution with independent motor pattern encoding.

Result: DETACH achieves an average 23% improvement in subtask success rate and 29% improvement in execution efficiency compared to existing methods when tested on various long-horizon tasks in human-scene interaction.

Conclusion: The biologically inspired dual-stream disentanglement framework enables DETACH to effectively solve complex long-horizon tasks, achieving generalization across domains and improving task execution success and efficiency.

Abstract: Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex
multi-step tasks that require continuous planning, sequential decision-making,
and extended execution across domains to achieve the final goal. However,
existing methods heavily rely on skill chaining by concatenating pre-trained
subtasks, with environment observations and self-state tightly coupled, lacking
the ability to generalize to new combinations of environments and skills,
failing to complete various LH tasks across domains. To solve this problem,
this paper presents DETACH, a cross-domain learning framework for LH tasks via
biologically inspired dual-stream disentanglement. Inspired by the brain's
"where-what" dual pathway mechanism, DETACH comprises two core modules: i) an
environment learning module for spatial understanding, which captures object
functions, spatial relationships, and scene semantics, achieving cross-domain
transfer through complete environment-self disentanglement; ii) a skill
learning module for task execution, which processes self-state information
including joint degrees of freedom and motor patterns, enabling cross-skill
transfer through independent motor pattern encoding. We conducted extensive
experiments on various LH tasks in HSI scenes. Compared with existing methods,
DETACH can achieve an average subtasks success rate improvement of 23% and
average execution efficiency improvement of 29%.

</details>


### [608] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: The paper presents a cloud-supported AI system for navigating autonomous quadcopters in indoor GPS-denied spaces, achieving robust object detection, depth estimation, and collision avoidance with a multithreaded architecture and sensor data integration.


<details>
  <summary>Details</summary>
Motivation: Develop a framework for autonomous quadcopters to navigate challenging GPS-denied indoor environments safely and efficiently.

Method: The system incorporates YOLOv11 for object detection, Depth Anything V2 for depth estimation, ToF sensors, IMU on a custom PCB, and cloud-based LLM for decision-making. It uses Kalman filtering for 3D spatial awareness and ensures collision avoidance via safety envelope measures.

Result: Experiments report strong performance: mAP50 of 0.6 for object detection, MAE of 7.2 cm in depth estimation, only 16 collision breaches in 42 trials, and low latency below 1 second.

Conclusion: The framework effectively complements existing drone autonomy systems, showcasing reliability and enhanced navigation capabilities in confined indoor settings leveraging cloud intelligence.

Abstract: This paper introduces an advanced AI-driven perception system for autonomous
quadcopter navigation in GPS-denied indoor environments. The proposed framework
leverages cloud computing to offload computationally intensive tasks and
incorporates a custom-designed printed circuit board (PCB) for efficient sensor
data acquisition, enabling robust navigation in confined spaces. The system
integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth
estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial
Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for
context-aware decision-making. A virtual safety envelope, enforced by
calibrated sensor offsets, ensures collision avoidance, while a multithreaded
architecture achieves low-latency processing. Enhanced spatial awareness is
facilitated by 3D bounding box estimation with Kalman filtering. Experimental
results in an indoor testbed demonstrate strong performance, with object
detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation
Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42
trials over approximately 11 minutes, and end-to-end system latency below 1
second. This cloud-supported, high-intelligence framework serves as an
auxiliary perception and navigation system, complementing state-of-the-art
drone autonomy for GPS-denied confined spaces.

</details>


### [609] [MolmoAct: Action Reasoning Models that can Reason in Space](https://arxiv.org/abs/2508.07917)
*Jason Lee,Jiafei Duan,Haoquan Fang,Yuquan Deng,Shuo Liu,Boyang Li,Bohan Fang,Jieyu Zhang,Yi Ru Wang,Sangho Lee,Winson Han,Wilbert Pumacay,Angelica Wu,Rose Hendrix,Karen Farley,Eli VanderBilt,Ali Farhadi,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: The paper introduces Action Reasoning Models (ARMs), specifically MolmoAct, which integrates perception, planning, and control for robotic reasoning. It outperforms existing models, offers high generalization, and introduces a significant open robot dataset.


<details>
  <summary>Details</summary>
Motivation: Current robotic foundation models struggle with adaptability and semantic grounding as they map perception directly to control. A structured approach integrating reasoning is needed to enhance performance and generalization.

Method: The authors propose MolmoAct, a vision-language-action model with a three-stage pipeline: encoding observations, generating mid-level spatial plans, and predicting low-level actions. They also introduce a new dataset with over 10,000 robot trajectories.

Result: MolmoAct-7B-D demonstrates strong performance in both simulations and real-world tasks, achieving high accuracy, generalization, and preference scores. It outperforms existing models like Pi-0 and ThinkAct in zero-shot, long-horizon, and out-of-distribution scenarios.

Conclusion: MolmoAct establishes a new standard for robotics foundation models by combining structured reasoning, high-quality datasets, and open-access resources, paving the way for advanced robotic action reasoning models.

Abstract: Reasoning is central to purposeful action, yet most robotic foundation models
map perception and instructions directly to control, which limits adaptability,
generalization, and semantic grounding. We introduce Action Reasoning Models
(ARMs), a class of vision-language-action models that integrate perception,
planning, and control through a structured three-stage pipeline. Our model,
MolmoAct, encodes observations and instructions into depth-aware perception
tokens, generates mid-level spatial plans as editable trajectory traces, and
predicts precise low-level actions, enabling explainable and steerable
behavior. MolmoAct-7B-D achieves strong performance across simulation and
real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching
tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on
LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;
and in real-world fine-tuning, an additional 10% (single-arm) and an additional
22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines
by an additional 23.3% on out-of-distribution generalization and achieves top
human-preference scores for open-ended instruction following and trajectory
steering. Furthermore, we release, for the first time, the MolmoAct Dataset --
a mid-training robot dataset comprising over 10,000 high quality robot
trajectories across diverse scenarios and tasks. Training with this dataset
yields an average 5.5% improvement in general performance over the base model.
We release all model weights, training code, our collected dataset, and our
action reasoning dataset, establishing MolmoAct as both a state-of-the-art
robotics foundation model and an open blueprint for building ARMs that
transform perception into purposeful action through structured reasoning.
Blogpost: https://allenai.org/blog/molmoact

</details>


### [610] [PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF](https://arxiv.org/abs/2508.07945)
*En Yen Puang,Federico Ceola,Giulia Pasquale,Lorenzo Natale*

Main category: cs.RO

TL;DR: PCHands provides a universal representation for dexterous manipulators across varying morphologies by using principal components for RL-based tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning a unified representation for robotic manipulation across devices with different structures and degrees of freedom.

Method: PCHands utilizes anchor positions for defining manipulator configurations and extracts principal components from latent variable representations.

Result: The approach outperformed baseline joint-space methods in RL tasks and worked robustly across different manipulators, including real-world experiments.

Conclusion: PCHands successfully provides a compact, universal representation that enhances learning efficiency and consistency in robotic dexterous manipulation.

Abstract: We consider the problem of learning a common representation for dexterous
manipulation across manipulators of different morphologies. To this end, we
propose PCHands, a novel approach for extracting hand postural synergies from a
large set of manipulators. We define a simplified and unified description
format based on anchor positions for manipulators ranging from 2-finger
grippers to 5-finger anthropomorphic hands. This enables learning a
variable-length latent representation of the manipulator configuration and the
alignment of the end-effector frame of all manipulators. We show that it is
possible to extract principal components from this latent representation that
is universal across manipulators of different structures and degrees of
freedom. To evaluate PCHands, we use this compact representation to encode
observation and action spaces of control policies for dexterous manipulation
tasks learned with RL. In terms of learning efficiency and consistency, the
proposed representation outperforms a baseline that learns the same tasks in
joint space. We additionally show that PCHands performs robustly in RL from
demonstration, when demonstrations are provided from a different manipulator.
We further support our results with real-world experiments that involve a
2-finger gripper and a 4-finger anthropomorphic hand. Code and additional
material are available at https://hsp-iit.github.io/PCHands/.

</details>


### [611] [Aerial Target Encirclement and Interception with Noisy Range Observations](https://arxiv.org/abs/2508.08046)
*Fen Liu,Shenghai Yuan,Thien-Minh Nguyen,Wei Meng,Lihua Xie*

Main category: cs.RO

TL;DR: The paper proposes methods for intercepting aerial targets using noisy data, incorporating state estimation and adaptive encirclement strategies, demonstrated with simulations and UAV experiments.


<details>
  <summary>Details</summary>
Motivation: Develop strategies to deal with non-cooperative aerial targets efficiently using noisy measurements, ensuring robust state estimation and system adaptability.

Method: The paper uses a 3D vibrating string trajectory and Kalman filter for target state estimation, alongside designing an anti-target controller for adaptive encirclement and interception, while adhering to input constraints.

Result: The stability and convergence of the proposed strategies are rigorously analyzed with both simulations and real-world UAV experiments validating effectiveness.

Conclusion: The proposed design enhances the capability to detect, estimate, and neutralize both cooperative and hostile aerial targets while managing noise and constraints effectively.

Abstract: This paper proposes a strategy to encircle and intercept a non-cooperative
aerial point-mass moving target by leveraging noisy range measurements for
state estimation. In this approach, the guardians actively ensure the
observability of the target by using an anti-synchronization (AS), 3D
``vibrating string" trajectory, which enables rapid position and velocity
estimation based on the Kalman filter. Additionally, a novel anti-target
controller is designed for the guardians to enable adaptive transitions from
encircling a protected target to encircling, intercepting, and neutralizing a
hostile target, taking into consideration the input constraints of the
guardians. Based on the guaranteed uniform observability, the exponentially
bounded stability of the state estimation error and the convergence of the
encirclement error are rigorously analyzed. Simulation results and real-world
UAV experiments are presented to further validate the effectiveness of the
system design.

</details>


### [612] [Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain](https://arxiv.org/abs/2508.08108)
*Wei Zhang,Yinchuan Wang,Wangtao Lu,Pengyu Zhang,Xiang Zhang,Yue Wang,Chaoqun Wang*

Main category: cs.RO

TL;DR: A capsizing-aware trajectory planner (CAP) is proposed for ground robots navigating uneven terrain, optimizing for safety and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of safe and efficient trajectory planning for ground robots in harsh environments with uneven terrain and obstacles.

Method: The method involves analyzing tip-over stability, defining a traversable orientation range, introducing a capsizing-safety constraint into trajectory optimization, and using a graph-based solver.

Result: Simulation and real-world experiments validate the method, showing improved performance over state-of-the-art approaches for uneven terrain navigation.

Conclusion: CAP ensures safer and more effective autonomous navigation on challenging terrains compared to existing methods.

Abstract: It is a challenging task for ground robots to autonomously navigate in harsh
environments due to the presence of non-trivial obstacles and uneven terrain.
This requires trajectory planning that balances safety and efficiency. The
primary challenge is to generate a feasible trajectory that prevents robot from
tip-over while ensuring effective navigation. In this paper, we propose a
capsizing-aware trajectory planner (CAP) to achieve trajectory planning on the
uneven terrain. The tip-over stability of the robot on rough terrain is
analyzed. Based on the tip-over stability, we define the traversable
orientation, which indicates the safe range of robot orientations. This
orientation is then incorporated into a capsizing-safety constraint for
trajectory optimization. We employ a graph-based solver to compute a robust and
feasible trajectory while adhering to the capsizing-safety constraint.
Extensive simulation and real-world experiments validate the effectiveness and
robustness of the proposed method. The results demonstrate that CAP outperforms
existing state-of-the-art approaches, providing enhanced navigation performance
on uneven terrains.

</details>


### [613] [AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies](https://arxiv.org/abs/2508.08113)
*Yinpei Dai,Jayjun Lee,Yichi Zhang,Ziqiao Ma,Jed Yang,Amir Zadeh,Chuan Li,Nima Fazeli,Joyce Chai*

Main category: cs.RO

TL;DR: AimBot enhances visuomotor policy learning in robotic manipulation by overlaying spatial cues onto RGB images to improve end-effector guidance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in robotic manipulation, this paper aims to enhance visuomotor policy learning through spatial visual cues that improve understanding of gripper-object relationships.

Method: AimBot uses multi-view RGB images augmented with overlays such as shooting lines and scope reticles derived from depth data, camera extrinsics, and end-effector pose. The augmented images replace original visuals without altering model architectures.

Result: Experiments validate that AimBot significantly enhances visuomotor policy performance in both simulations and real-world scenarios, with minimal computational overhead.

Conclusion: AimBot provides notable improvement in robotic manipulation tasks via minimal and effective spatial visual augmentation, demonstrating its practical benefits without complexity.

Abstract: In this paper, we propose AimBot, a lightweight visual augmentation technique
that provides explicit spatial cues to improve visuomotor policy learning in
robotic manipulation. AimBot overlays shooting lines and scope reticles onto
multi-view RGB images, offering auxiliary visual guidance that encodes the
end-effector's state. The overlays are computed from depth images, camera
extrinsics, and the current end-effector pose, explicitly conveying spatial
relationships between the gripper and objects in the scene. AimBot incurs
minimal computational overhead (less than 1 ms) and requires no changes to
model architectures, as it simply replaces original RGB images with augmented
counterparts. Despite its simplicity, our results show that AimBot consistently
improves the performance of various visuomotor policies in both simulation and
real-world settings, highlighting the benefits of spatially grounded visual
feedback.

</details>


### [614] [COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models](https://arxiv.org/abs/2508.08144)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.RO

TL;DR: The paper proposes a structured pruning methodology to compress neural network controllers while preserving stability, aiming to make them viable for resource-constrained mobile platforms.


<details>
  <summary>Details</summary>
Motivation: The demand for computational efficiency in neural network controllers is increasing due to the rise of mobile robots, wearables, and IoT devices, which operate with hardware limitations.

Method: The paper employs a component-aware structured pruning strategy to optimize compression while using Lyapunov stability criteria to ensure the stability of neural network controllers.

Result: Experimental validation shows the proposed method reduces model complexity while maintaining control performance and guarantees stability properties, establishing limits for safe compression ratios.

Conclusion: The study provides a framework for safe model compression of neural network controllers, enabling deployment in resource-constrained environments while ensuring stability.

Abstract: The rapid growth of resource-constrained mobile platforms, including mobile
robots, wearable systems, and Internet-of-Things devices, has increased the
demand for computationally efficient neural network controllers (NNCs) that can
operate within strict hardware limitations. While deep neural networks (DNNs)
demonstrate superior performance in control applications, their substantial
computational complexity and memory requirements present significant barriers
to practical deployment on edge devices. This paper introduces a comprehensive
model compression methodology that leverages component-aware structured pruning
to determine the optimal pruning magnitude for each pruning group, ensuring a
balance between compression and stability for NNC deployment. Our approach is
rigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),
a state-of-the-art model-based reinforcement learning algorithm, with a
systematic integration of mathematical stability guarantee properties,
specifically Lyapunov criteria. The key contribution of this work lies in
providing a principled framework for determining the theoretical limits of
model compression while preserving controller stability. Experimental
validation demonstrates that our methodology successfully reduces model
complexity while maintaining requisite control performance and stability
characteristics. Furthermore, our approach establishes a quantitative boundary
for safe compression ratios, enabling practitioners to systematically determine
the maximum permissible model reduction before violating critical stability
properties, thereby facilitating the confident deployment of compressed NNCs in
resource-limited environments.

</details>


### [615] [Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy](https://arxiv.org/abs/2508.08226)
*Haiyue Chen,Aniket Datar,Tong Xu,Francesco Cancelliere,Harsh Rangwala,Madhan Balaji Rao,Daeun Song,David Eichinger,Xuesu Xiao*

Main category: cs.RO

TL;DR: This paper introduces Verti-Arena, a reconfigurable indoor facility designed to standardize and support off-road robotics research.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of a controllable, standardized, and real-world testbed that limits progress in off-road mobile robot navigation for challenging and inaccessible environments.

Method: The authors designed Verti-Arena, a reconfigurable facility with diverse terrains, accurate ground truth measurements via onboard sensors and motion capture, and a web-based interface for global remote experimentation.

Result: Verti-Arena created a reproducible benchmark for experiments on difficult terrains and enabled consistent data collection and algorithm comparison for off-road robotics.

Conclusion: Verti-Arena fills the critical gap in off-road robotics research by offering a standardized testbed, fostering reproducible experiments and furthering the development of autonomy in challenging terrains.

Abstract: Off-road navigation is an important capability for mobile robots deployed in
environments that are inaccessible or dangerous to humans, such as disaster
response or planetary exploration. Progress is limited due to the lack of a
controllable and standardized real-world testbed for systematic data collection
and validation. To fill this gap, we introduce Verti-Arena, a reconfigurable
indoor facility designed specifically for off-road autonomy. By providing a
repeatable benchmark environment, Verti-Arena supports reproducible experiments
across a variety of vertically challenging terrains and provides precise ground
truth measurements through onboard sensors and a motion capture system.
Verti-Arena also supports consistent data collection and comparative evaluation
of algorithms in off-road autonomy research. We also develop a web-based
interface that enables research groups worldwide to remotely conduct
standardized off-road autonomy experiments on Verti-Arena.

</details>


### [616] [ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks](https://arxiv.org/abs/2508.08240)
*Kaijun Wang,Liqin Lu,Mingyu Liu,Jianuo Jiang,Zeju Li,Bolin Zhang,Wancai Zheng,Xinyi Yu,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: This paper introduces ODYSSEY, a comprehensive framework for agile quadruped robots, integrating task planning and low-level control for robust mobile manipulation across diverse terrains and tasks.


<details>
  <summary>Details</summary>
Motivation: The goal is to overcome limitations in current mobile manipulation systems, including weak adaptation to mobile platforms, poor generalization in open environments, and insufficient coordination for precise control and maneuverability in unstructured scenarios.

Method: The proposed system combines a vision-language model-based hierarchical planner for task decomposition and a novel whole-body control policy for enhanced coordination and maneuverability. It also establishes a benchmark to evaluate long-horizon mobile manipulation in varied settings.

Result: ODYSSEY demonstrates successful sim-to-real performance transfer in both indoor and outdoor environments, showcasing generalization, robustness, and adaptability.

Conclusion: The framework advances practical use of legged robotic assistants for dynamic, complex tasks in unstructured environments and highlights their robust capabilities in long-horizon mobile manipulation.

Abstract: Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/

</details>


### [617] [BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion](https://arxiv.org/abs/2508.08241)
*Takara E. Truong,Qiayuan Liao,Xiaoyu Huang,Guy Tevet,C. Karen Liu,Koushil Sreenath*

Main category: cs.RO

TL;DR: This paper introduces BeyondMimic, a framework for humanoid control that learns complex motion skills via guided diffusion techniques, achieving real-world deployment with high-quality motion tracking and adaptive task-solving ability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in whole-body humanoid control, specifically in creating frameworks for motion tracking and distillation of human motion skills that reliably enable generalizable and dynamic humanoid actions on real hardware.

Method: BeyondMimic utilizes guided diffusion for motion tracking to replicate and exceed human-like motion skills. It integrates a unified diffusion policy for zero-shot control using task-specific cost functions, achieving both mimicking of existing motions and synthesis of novel ones.

Result: BeyondMimic demonstrates robust humanoid control capable of performing highly dynamic motions such as sprinting and cartwheels, as well as solving tasks like waypoint navigation and obstacle avoidance on real hardware.

Conclusion: The study bridges gaps in sim-to-real motion tracking and flexible motion synthesis, setting a new standard for dynamic, versatile, and natural humanoid control systems.

Abstract: Learning skills from human motions offers a promising path toward
generalizable policies for whole-body humanoid control, yet two key
cornerstones are missing: (1) a high-quality motion tracking framework that
faithfully transforms large-scale kinematic references into robust and
extremely dynamic motions on real hardware, and (2) a distillation approach
that can effectively learn these motion primitives and compose them to solve
downstream tasks. We address these gaps with BeyondMimic, the first real-world
framework to learn from human motions for versatile and naturalistic humanoid
control via guided diffusion. Our framework provides a motion tracking pipeline
capable of challenging skills such as jumping spins, sprinting, and cartwheels
with state-of-the-art motion quality. Moving beyond mimicking existing motions
and synthesize novel ones, we further introduce a unified diffusion policy that
enables zero-shot task-specific control at test time using simple cost
functions. Deployed on hardware, BeyondMimic performs diverse tasks at test
time, including waypoint navigation, joystick teleoperation, and obstacle
avoidance, bridging sim-to-real motion tracking and flexible synthesis of human
motion primitives for whole-body control. https://beyondmimic.github.io/.

</details>


### [618] [UPP: Unified Path Planner with Adaptive Safety and Optimality](https://arxiv.org/abs/2505.23197)
*Jatin Kumar Arora,Shubhendu Bhasin*

Main category: cs.RO

TL;DR: Proposes a Unified Path Planner (UPP) for balancing safety and optimality in robotics path planning.


<details>
  <summary>Details</summary>
Motivation: Current path planning algorithms tend to focus exclusively on either safety or optimality, leaving a gap in solutions addressing both simultaneously.

Method: Introduces a Unified Path Planner (UPP) that employs modified heuristics and a dynamic safety cost function, with tunable parameters to trade off safety, optimality, and computational complexity.

Result: UPP performance is demonstrated in simulations and real-world scenarios, showing adaptable safety levels and successful execution on a TurtleBot.

Conclusion: UPP presents a flexible solution balancing safety and optimality, validated against traditional path planning methods in diverse environments.

Abstract: We are surrounded by robots helping us perform complex tasks. Robots have a
wide range of applications, from industrial automation to personalized
assistance. However, with great technological innovation come significant
challenges. One of the major challenges in robotics is path planning. Despite
advancements such as graph search, sampling, and potential field methods, most
path planning algorithms focus either on optimality or on safety. Very little
research addresses both simultaneously. We propose a Unified Path Planner (UPP)
that uses modified heuristics and a dynamic safety cost function to balance
safety and optimality. The level of safety can be adjusted via tunable
parameters, trading off against computational complexity. We demonstrate the
planner's performance in simulations, showing how parameter variation affects
results. UPP is compared with various traditional and safe-optimal planning
algorithms across different scenarios. We also validate it on a TurtleBot,
where the robot successfully finds safe and sub-optimal paths.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [619] [An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects](https://arxiv.org/abs/2508.07084)
*Kaveh Shahedi,Nana Gyambrah,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: The study analyzed the performance impact of 1,499 method-level changes from 739 commits across 15 open-source Java projects, finding that 32.7% had measurable impacts. Regressions were more frequent than improvements, regardless of change type, challenging traditional assumptions.


<details>
  <summary>Details</summary>
Motivation: To empirically understand how method-level code changes influence software performance, as intuitive developer beliefs about performance regressions lack validation.

Method: The study used Java Microbenchmark Harness (JMH) for exact performance measurement and bytecode instrumentation to analyze 739 commits with 1,499 method-level changes across four aspects: performance patterns, change correlations, developer factors, and domain-size interactions.

Result: 32.7% of changes impacted performance, regressions were 1.3 times more frequent, with no difference across change categories. Algorithmic shifts presented high improvement potential but high regression risk. Senior developers made more stable changes, and code complexity increased regression likelihood.

Conclusion: This study emphasizes the importance of automated performance testing in CI pipelines, challenging conventional assumptions about risk-stratified development strategies and highlighting complexity and developer experience as key factors in performance stability.

Abstract: Performance is a critical quality attribute in software development, yet the
impact of method-level code changes on performance evolution remains poorly
understood. While developers often make intuitive assumptions about which types
of modifications are likely to cause performance regressions or improvements,
these beliefs lack empirical validation at a fine-grained level. We conducted a
large-scale empirical study analyzing performance evolution in 15 mature
open-source Java projects hosted on GitHub. Our analysis encompassed 739
commits containing 1,499 method-level code changes, using Java Microbenchmark
Harness (JMH) for precise performance measurement and rigorous statistical
analysis to quantify both the significance and magnitude of performance
variations. We employed bytecode instrumentation to capture method-specific
execution metrics and systematically analyzed four key aspects: temporal
performance patterns, code change type correlations, developer and complexity
factors, and domain-size interactions. Our findings reveal that 32.7% of
method-level changes result in measurable performance impacts, with regressions
occurring 1.3 times more frequently than improvements. Contrary to conventional
wisdom, we found no significant differences in performance impact distributions
across code change categories, challenging risk-stratified development
strategies. Algorithmic changes demonstrate the highest improvement potential
but carry substantial regression risk. Senior developers produce more stable
changes with fewer extreme variations, while code complexity correlates with
increased regression likelihood. Domain-size interactions reveal significant
patterns, with web server + small projects exhibiting the highest performance
instability. Our study provides empirical evidence for integrating automated
performance testing into continuous integration pipelines.

</details>


### [620] [Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks](https://arxiv.org/abs/2508.06718)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: The paper studies challenges in integrating patches between divergent GitHub forks (variants) and introduces RePatch, a refactoring-aware integration tool for Java repositories.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of integrating bug-fix patches between long-lived, divergent repository variants caused by structural code differences.

Method: The authors conducted an empirical study of patch integration failures in 14 divergent variant pairs and developed RePatch, an extension of RefMerge, to enable refactoring-aware and asymmetric patch transfer.

Result: Git failed to integrate 64.4% of the analyzed bug-fix pull requests due to structural misalignments, but RePatch successfully integrated 52.8% of these previously failing cases.

Conclusion: The findings emphasize the need for semantic reasoning tools like RePatch to address limitations in syntax-based patch integration, facilitating better variant-aware development workflows.

Abstract: While most forks on platforms like GitHub are short-lived and used for social
collaboration, a smaller but impactful subset evolve into long-lived forks,
referred to here as variants, that maintain independent development
trajectories. Integrating bug-fix patches across such divergent variants poses
challenges due to structural drift, including refactorings that rename,
relocate, or reorganize code elements and obscure semantic correspondence. This
paper presents an empirical study of patch integration failures in 14 divergent
pair of variants and introduces RePatch, a refactoring-aware integration system
for Java repositories. RePatch extends the RefMerge framework, originally
designed for symmetric merges, by supporting asymmetric patch transfer. RePatch
inverts refactorings in both the source and target to realign the patch
context, applies the patch, and replays the transformations to preserve the
intent of the variant. In our evaluation of 478 bug-fix pull requests, Git
cherry-pick fails in 64.4% of cases due to structural misalignments, while
RePatch successfully integrates 52.8% of the previously failing patches. These
results highlight the limitations of syntax-based tools and the need for
semantic reasoning in variant-aware patch propagation.

</details>


### [621] [Quo Vadis, Code Review? Exploring the Future of Code Review](https://arxiv.org/abs/2508.06879)
*Michael Dorner,Andreas Bauer,Darja Šmite,Lukas Thode,Daniel Mendez,Ricardo Britto,Stephan Lukasczyk,Ehsan Zabardast,Michael Kormann*

Main category: cs.SE

TL;DR: The paper examines current practitioner viewpoints on code review and anticipates future changes and risks for its role in collaborative software engineering.


<details>
  <summary>Details</summary>
Motivation: To understand the reflective opinions of practitioners on code review and the potential evolution and associated risks of its practice.

Method: Exploration of practitioner perspectives, likely through qualitative analysis or survey-based research.

Result: Identified anticipated changes in code review practices and highlighted potential long-term risks associated with those changes.

Conclusion: The evolution of code review carries risks that need to be considered for maintaining its effectiveness in collaborative software engineering.

Abstract: Code review has long been a core practice in collaborative software
engineering. In this research, we explore how practitioners reflect on code
review today and what changes they anticipate in the near future. We then
discuss the potential long-term risks of these anticipated changes for the
evolution of code review and its role in collaborative software engineering.

</details>


### [622] [Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs](https://arxiv.org/abs/2508.06888)
*Fanyu Wang,Chetan Arora,Yonghui Liu,Kaicheng Huang,Chakkrit Tantithamthavorn,Aldeida Aleti,Dishan Sambathkumar,David Lo*

Main category: cs.SE

TL;DR: Acceptance criteria play a crucial role in software development but pose challenges in manual creation, especially for UI-heavy applications. This paper introduces RAGcceptance M2RE, a method utilizing multi-modal requirements data to automate acceptance criteria generation, enhancing relevance, correctness, and comprehensibility in an industrial software case study.


<details>
  <summary>Details</summary>
Motivation: Creating acceptance criteria manually is challenging due to the need for domain-specific knowledge and integration of textual and visual UI information, especially in user interface-intensive applications.

Method: The paper proposes the RAGcceptance M2RE approach, which uses Retrieval-Augmented Generation (RAG) to generate acceptance criteria by integrating textual and visual requirements data.

Result: The study found that utilizing multi-modal information significantly improves the relevance, correctness, and comprehensibility of acceptance criteria. Practitioner evaluations revealed reduced effort, better stakeholder intent capture, and valuable insights often overlooked by experts.

Conclusion: RAGcceptance M2RE proves effective in streamlining software validation and improving development processes, with significant potential for industry adoption. The authors also provide resources such as the implementation and dataset for further exploration.

Abstract: Acceptance criteria (ACs) play a critical role in software development by
clearly defining the conditions under which a software feature satisfies
stakeholder expectations. However, manually creating accurate, comprehensive,
and unambiguous acceptance criteria is challenging, particularly in user
interface-intensive applications, due to the reliance on domain-specific
knowledge and visual context that is not always captured by textual
requirements alone. To address these challenges, we propose RAGcceptance M2RE,
a novel approach that leverages Retrieval-Augmented Generation (RAG) to
generate acceptance criteria from multi-modal requirements data, including both
textual documentation and visual UI information. We systematically evaluated
our approach in an industrial case study involving an education-focused
software system used by approximately 100,000 users. The results indicate that
integrating multi-modal information significantly enhances the relevance,
correctness, and comprehensibility of the generated ACs. Moreover, practitioner
evaluations confirm that our approach effectively reduces manual effort,
captures nuanced stakeholder intent, and provides valuable criteria that domain
experts may overlook, demonstrating practical utility and significant potential
for industry adoption. This research underscores the potential of multi-modal
RAG techniques in streamlining software validation processes and improving
development efficiency. We also make our implementation and a dataset
available.

</details>


### [623] [Integrating Rules and Semantics for LLM-Based C-to-Rust Translation](https://arxiv.org/abs/2508.06926)
*Feng Luo,Kexing Ji,Cuiyun Gao,Shuzheng Gao,Jia Feng,Kui Liu,Xin Xia,Michael R. Lyu*

Main category: cs.SE

TL;DR: IRENE is a framework aimed at improving the translation from legacy C code to Rust using a rule-augmented retrieval module, structured summarization, and error-driven translation for enhancing Rust rule adherence and semantic understanding.


<details>
  <summary>Details</summary>
Motivation: To ensure memory safety and simplify migration from legacy C to Rust, overcoming limitations of static rule-based translation methods and addressing issues that emerge in LLM-based approaches.

Method: Propose IRENE, a framework comprising three modules: (1) rule-augmented retrieval for improved Rust rule coverage, (2) structured summarization for better semantic understanding of C code, and (3) error-driven translation to iteratively refine output using compiler diagnostics.

Result: Demonstrated effectiveness on two datasets (xCodeEval and HW-Bench) for improving translation accuracy and safety across eight LLMs.

Conclusion: IRENE successfully enhances the translation process by incorporating rule-integrated retrieval and semantic refinement, allowing more accurate and compliant translations from C to Rust.

Abstract: Automated translation of legacy C code into Rust aims to ensure memory safety
while reducing the burden of manual migration. Early approaches in code
translation rely on static rule-based methods, but they suffer from limited
coverage due to dependence on predefined rule patterns. Recent works regard the
task as a sequence-to-sequence problem by leveraging large language models
(LLMs). Although these LLM-based methods are capable of reducing unsafe code
blocks, the translated code often exhibits issues in following Rust rules and
maintaining semantic consistency. On one hand, existing methods adopt a direct
prompting strategy to translate the C code, which struggles to accommodate the
syntactic rules between C and Rust. On the other hand, this strategy makes it
difficult for LLMs to accurately capture the semantics of complex code. To
address these challenges, we propose IRENE, an LLM-based framework that
Integrates RulEs aNd sEmantics to enhance translation. IRENE consists of three
modules: 1) a rule-augmented retrieval module that selects relevant translation
examples based on rules generated from a static analyzer developed by us,
thereby improving the handling of Rust rules; 2) a structured summarization
module that produces a structured summary for guiding LLMs to enhance the
semantic understanding of C code; 3) an error-driven translation module that
leverages compiler diagnostics to iteratively refine translations. We evaluate
IRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial
dataset provided by Huawei) and eight LLMs, focusing on translation accuracy
and safety.

</details>


### [624] [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction](https://arxiv.org/abs/2508.06942)
*Zhenchang Xing,Yang Liu,Zhuo Cheng,Qing Huang,Dehai Zhao,Daniel Sun,Chenhua Liu*

Main category: cs.SE

TL;DR: The paper introduces Controlled Natural Language for Prompt (CNL-P), a structured prompt methodology, enhancing LLM interaction and output quality by blending prompt engineering and software engineering principles.


<details>
  <summary>Details</summary>
Motivation: Natural language prompts have inherent ambiguities, which can limit large language models' (LLMs) ability to interpret and produce consistent high-quality results. This paper aims to reduce such ambiguities while improving output quality.

Method: CNL-P incorporates best practices from prompt engineering and software engineering to define precise grammar and semantic norms for prompts. Additionally, an NL2CNL-P conversion tool and a linting tool are developed to facilitate adoption and improve CNL-P correctness.

Result: Experiments reveal that CNL-P enables better interpretation by LLMs and delivers more consistent and reliable outputs, demonstrating a seamless integration of prompt engineering and software engineering practices.

Conclusion: CNL-P bridges the gap between prompt engineering and traditional software engineering, paving the way for a novel programming paradigm focused on natural language-based programming.

Abstract: With the growing capabilities of large language models (LLMs), they are
increasingly applied in areas like intelligent customer service, code
generation, and knowledge management. Natural language (NL) prompts act as the
``APIs'' for human-LLM interaction. To improve prompt quality, best practices
for prompt engineering (PE) have been developed, including writing guidelines
and templates. Building on this, we propose Controlled NL for Prompt (CNL-P),
which not only incorporates PE best practices but also draws on key principles
from software engineering (SE). CNL-P introduces precise grammar structures and
strict semantic norms, further eliminating NL's ambiguity, allowing for a
declarative but structured and accurate expression of user intent. This helps
LLMs better interpret and execute the prompts, leading to more consistent and
higher-quality outputs. We also introduce an NL2CNL-P conversion tool based on
LLMs, enabling users to write prompts in NL, which are then transformed into
CNL-P format, thus lowering the learning curve of CNL-P. In particular, we
develop a linting tool that checks CNL-P prompts for syntactic and semantic
accuracy, applying static analysis techniques to NL for the first time.
Extensive experiments demonstrate that CNL-P enhances the quality of LLM
responses through the novel and organic synergy of PE and SE. We believe that
CNL-P can bridge the gap between emerging PE and traditional SE, laying the
foundation for a new programming paradigm centered around NL.

</details>


### [625] [From Noise to Knowledge: Interactive Summaries for Developer Alerts](https://arxiv.org/abs/2508.07169)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: CLARITY is an interactive tool that improves the analysis of bug-finding tool warnings by identifying related patterns using active learning for customizable grouping.


<details>
  <summary>Details</summary>
Motivation: Programmers reviewing bug-finding tool warnings face cognitive challenges in sensemaking, as warnings are often reviewed one by one without connections highlighted.

Method: The method involves deriving summary rules to group related warnings based on users' active feedback, emphasizing structural similarities such as containment, invoked methods, and expressions.

Result: A user study with 14 participants showed faster and more confident identification of root causes for uninteresting warnings using CLARITY. Simulation indicated fewer interactions required for rule alignment with active feedback.

Conclusion: CLARITY's approach supports enhanced and customizable sensemaking in interpreting bug-finding tool warnings through interactive inquiry and active learning-based summarization.

Abstract: Programmers using bug-finding tools often review their reported warnings one
by one. Based on the insight that identifying recurring themes and
relationships can enhance the cognitive process of sensemaking, we propose
CLARITY, which supports interpreting tool-generated warnings through
interactive inquiry. CLARITY derives summary rules for custom grouping of
related warnings with active feedback. As users mark warnings as interesting or
uninteresting, CLARITY's rule inference algorithm surfaces common symptoms,
highlighting structural similarities in containment, subtyping, invoked
methods, accessed fields, and expressions.
  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java
projects. In a within-subject user study with 14 participants, users
articulated root causes for similar uninteresting warnings faster and with more
confidence using CLARITY. We observed significant individual variation in
desired grouping, reinforcing the need for customizable sensemaking. Simulation
shows that with rule-level feedback, only 11.8 interactions are needed on
average to align all inferred rules with a simulated user's labels (vs. 17.8
without). Our evaluation suggests that CLARITY's active learning-based
summarization enhances interactive warning sensemaking.

</details>


### [626] [Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes](https://arxiv.org/abs/2508.07180)
*Zhe Zhang,Runlin Liu,Aishan Liu,Xingyu Liu,Xiang Gao,Hailong Sun*

Main category: cs.SE

TL;DR: CODE2BENCH is a pipeline designed to create robust benchmarks for testing large language models (LLMs) on real-world code tasks, addressing data contamination and rigor challenges. It introduces methods for contamination resistance, structured dependency analysis, and automated functional verification.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating large language models in code generation are often affected by data contamination and insufficiently rigorous testing protocols, limiting their ability to effectively highlight model shortcomings.

Method: CODE2BENCH uses three innovations: automated dynamism to minimize contamination, dependency analysis using Scope Graphs for structural task classification, and property-based testing to synthesize comprehensive test suites for code validation.

Result: The CODE2BENCH-2505 benchmark was developed using 880 recent Python GitHub repositories, covering 1,163 tasks and achieving full branch coverage in ground-truth implementations. Tests showed that LLMs perform better on Weakly Self-Contained tasks but struggle with Self-Contained tasks requiring complex reasoning.

Conclusion: CODE2BENCH offers a robust and contamination-resistant methodology for evaluating LLMs in real-world software development tasks, providing a foundation for assessing both language-agnostic and specific coding challenges.

Abstract: As large language models LLMs) become increasingly integrated into software
development workflows, rigorously evaluating their performance on complex,
real-world code generation tasks has become essential. However, existing
benchmarks often suffer from data contamination and limited test rigor,
constraining their ability to reveal model failures effectively. To address
these, we present CODE2BENCH, a end-to-end pipeline for dynamically
constructing robust and contamination-resistant benchmarks from real-world
GitHub repositories. Specifically, CODE2BENCH introduces three key innovations:
(1) Automated Dynamism, achieved through periodic ingestion of recent code to
minimize training data contamination; (2) Scope Graph-based dependency
analysis, which enables structured classification of functions into benchmark
instances with controlled dependency levels (distinguishing between
Self-Contained (SC) tasks for cross-language evaluation and Weakly
Self-Contained (WSC) tasks involving permitted library usage); and (3)
Property-Based Testing (PBT) for the automated synthesis of rigorous test
suites to enable thorough functional verification. Using this pipeline, we
construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python
projects spanning diverse domains, comprising 1,163 code generation tasks with
100% average branch coverage on ground-truth implementations. Extensive
evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently
struggle with SC tasks requiring complex, non-standard logic and cross-language
transfer, while showing relatively stronger performance on WSC tasks in Python.
Our work introduces a contamination-resistant, language-agnostic methodology
for dynamic benchmark construction, offering a principled foundation for the
comprehensive and realistic evaluation of LLMs on real-world software
development tasks.

</details>


### [627] [TraceLens: Question-Driven Debugging for Taint Flow Understanding](https://arxiv.org/abs/2508.07198)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: TraceLens introduces a question-answer style debugging interface for taint analysis that improves sensemaking by addressing why, why-not, and what-if queries about dataflows.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective end-user debugging capabilities in existing taint analysis tools, which limits understanding of suspicious or missing data flows.

Method: TraceLens integrates QA inquiry processes for end-users, enabling speculative analysis to evaluate the connectivity assumptions and overall impact using intuitive debugging tools.

Result: A user study demonstrated improved debugging accuracy (21% higher than CodeQL), reduced mental demand (45% lower), and increased confidence in identifying suspicious flows.

Conclusion: TraceLens enhances taint analysis with user-friendly debugging tools, facilitating sensemaking of complex dataflows and boosting accuracy and usability.

Abstract: Taint analysis is a security analysis technique used to track the flow of
potentially dangerous data through an application and its dependent libraries.
Investigating why certain unexpected flows appear and why expected flows are
missing is an important sensemaking process during end-user taint analysis.
Existing taint analysis tools often do not provide this end-user debugging
capability, where developers can ask why, why-not, and what-if questions about
dataflows and reason about the impact of configuring sources and sinks, and
models of 3rd-party libraries that abstract permissible and impermissible data
flows. Furthermore, a tree-view or a list-view used in existing
taint-analyzer's visualization makes it difficult to reason about the global
impact on connectivity between multiple sources and sinks.
  Inspired by the insight that sensemaking tool-generated results can be
significantly improved by a QA inquiry process, we propose TraceLens, a first
end-user question-answer style debugging interface for taint analysis. It
enables a user to ask why, why-not, and what-if questions to investigate the
existence of suspicious flows, the non-existence of expected flows, and the
global impact of third-party library models. TraceLens performs speculative
what-if analysis, to help a user in debugging how different connectivity
assumptions affect overall results. A user study with 12 participants shows
that participants using TraceLens achieved 21% higher accuracy on average,
compared to CodeQL. They also reported a 45% reduction in mental demand
(NASA-TLX) and rated higher confidence in identifying relevant flows using
TraceLens.

</details>


### [628] [AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation](https://arxiv.org/abs/2508.07371)
*Yi Zhong,Hongchao Liu,Di ZHao*

Main category: cs.SE

TL;DR: The paper introduces a Hardware Description Language (HDL)-based assertion generation method using a lightweight large language model and Unsloth platform to automate test case generation with reduced costs and robust results.


<details>
  <summary>Details</summary>
Motivation: The rapidly growing complexity of software systems has intensified the need for automated testing and maintenance tools.

Method: The proposed method combines a lightweight, adjustable large language model with the Unsloth platform to automatically generate test cases, with a focus on reducing training costs while ensuring accuracy.

Result: Empirical evaluation demonstrates the method’s ability to efficiently generate assertions that align closely with hardware logic.

Conclusion: The framework offers a robust, flexible solution for addressing modern challenges in software testing and maintenance.

Abstract: As the complexity of software systems continues to increase, the demand for
automated testing and maintenance tools is growing exponentially. To meet this
urgent need, we propose a new assertion generation method based on Hardware
Description Language (HDL). This method combines a lightweight,
parameter-adjustable large language model (LLM) with the Unsloth platform to
automatically generate test cases, thereby significantly reducing training
costs without sacrificing accuracy or generalization performance. Empirical
evaluation shows that our method can efficiently generate assertions that
strictly conform to the hardware logic. This framework provides a robust and
flexible solution to modern software testing and maintenance challenges.
https://github.com/liusu-orange/AutoAssert-1 and
https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.

</details>


### [629] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: This paper proposes Mo2oM, a new microservice extraction framework using soft clustering for better scalability and modularization.


<details>
  <summary>Details</summary>
Motivation: The shift from monolithic architectures to microservices requires better decomposition methods to reduce inter-service coupling and improve service cohesion, which current hard clustering methods fail to optimize.

Method: Mo2oM formulates microservice extraction as a soft clustering problem, combining deep semantic embeddings with structural dependencies captured through method-call graphs, and employs a graph neural network-based algorithm to identify overlapping microservices.

Result: When evaluated on four open-source benchmarks, Mo2oM outperforms eight state-of-the-art baselines, achieving significant improvements in structural modularity, communication overhead, modularity, decoupling, and service size balance.

Conclusion: Mo2oM shows strong potential as a superior alternative for microservice extraction, delivering better architectural quality through a soft clustering approach.

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


### [630] [Adopting Road-Weather Open Data in Route Recommendation Engine](https://arxiv.org/abs/2508.07881)
*Henna Tammia,Benjamin Kämä,Ella Peltonen*

Main category: cs.SE

TL;DR: The paper explores challenges in utilizing Finland's DigiTraffic road data API for practical applications, focusing on road weather attributes and a personalized road recommendation system.


<details>
  <summary>Details</summary>
Motivation: Efficiently leveraging DigiTraffic's road data API for practical applications requires a methodology that considers data qualities, preprocessing, and machine learning.

Method: The authors propose a methodology to process DigiTraffic data, focusing on road-weather-related attributes, and use the data to develop a personalized road recommendation engine validated through real-world testing.

Result: The solution efficiently identifies and recommends personalized routes for three driver profiles based on DigiTraffic data.

Conclusion: The paper highlights the feasibility of utilizing large-scale road data for building personalized solutions and provides actionable methodologies to handle such datasets effectively.

Abstract: Digitraffic, Finland's open road data interface, provides access to
nationwide road sensors with more than 2,300 real-time attributes from 1,814
stations. However, efficiently utilizing such a versatile data API for a
practical application requires a deeper understanding of the data qualities,
preprocessing phases, and machine learning tools. This paper discusses the
challenges of large-scale road weather and traffic data. We go through the
road-weather-related attributes from DigiTraffic as a practical example of
processes required to work with such a dataset. In addition, we provide a
methodology for efficient data utilization for the target application, a
personalized road recommendation engine based on a simple routing application.
We validate our solution based on real-world data, showing we can efficiently
identify and recommend personalized routes for three different driver profiles.

</details>


### [631] [SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows](https://arxiv.org/abs/2508.07935)
*Jingwen Zhou,Jieshan Chen,Qinghua Lu,Dehai Zhao,Liming Zhu*

Main category: cs.SE

TL;DR: SHIELDA is a framework for handling execution exceptions in LLM-driven workflows by linking them to their root causes and facilitating recovery.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in existing exception handling solutions for LLM-powered agent systems, which often fail to trace root causes or offer structured recovery.

Method: The authors propose a modular framework called SHIELDA, which uses an exception classifier, a pattern registry, and a handling executor to manage exceptions.

Result: SHIELDA effectively recovers from reasoning-induced exceptions in a case study with the AutoPR agent.

Conclusion: SHIELDA provides an innovative, structured approach for exception handling in LLM-driven systems, improving robustness and adaptability.

Abstract: Large Language Model (LLM) agentic systems are software systems powered by
LLMs that autonomously reason, plan, and execute multi-step workflows to
achieve human goals, rather than merely executing predefined steps. During
execution, these workflows frequently encounter exceptions. Existing exception
handling solutions often treat exceptions superficially, failing to trace
execution-phase exceptions to their reasoning-phase root causes. Furthermore,
their recovery logic is brittle, lacking structured escalation pathways when
initial attempts fail. To tackle these challenges, we first present a
comprehensive taxonomy of 36 exception types across 12 agent artifacts.
Building on this, we propose SHIELDA (Structured Handling of Exceptions in
LLM-Driven Agentic Workflows), a modular runtime exception handling framework
for LLM agentic workflows. SHIELDA uses an exception classifier to select a
predefined exception handling pattern from a handling pattern registry. These
patterns are then executed via a structured handling executor, comprising local
handling, flow control, and state recovery, to enable phase-aware recovery by
linking exceptions to their root causes and facilitating composable strategies.
We validate SHIELDA's effectiveness through a case study on the AutoPR agent,
demonstrating effective, cross-phase recovery from a reasoning-induced
exception.

</details>


### [632] [Exploring the Challenges and Opportunities of AI-assisted Codebase Generation](https://arxiv.org/abs/2508.07966)
*Philipp Eibl,Sadra Sabouri,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: This paper investigates developers' interactions with codebase AI assistants (CBAs), revealing low satisfaction (mean = 2.8) due to issues with functionality, code quality, and communication. Participants faced six challenges and five workflow barriers. The study also surveyed 21 CBAs to identify future design opportunities.


<details>
  <summary>Details</summary>
Motivation: To better understand how developers interact with codebase AI assistants (CBAs), identify the gaps in their effectiveness, and explore why these tools remain less adopted compared to snippet-level assistants.

Method: The research involved a counterbalanced user study and interviews with 16 participants, including students and developers, working on coding tasks with CBAs. The study analyzed their prompts and dissatisfaction points and surveyed 21 commercial CBAs to compare offerings with user needs.

Result: Participants provided varied prompts, yet overall satisfaction with CBAs was low. Major dissatisfaction factors included functionality (77%), poor code quality (42%), and communication challenges (25%). The paper identified six user challenges and five workflow barriers.

Conclusion: The findings highlight significant gaps in current CBA capabilities, alongside design opportunities to create more efficient and user-friendly tools. Addressing functionality, code quality, and communication could improve adoption and satisfaction.

Abstract: Recent AI code assistants have significantly improved their ability to
process more complex contexts and generate entire codebases based on a textual
description, compared to the popular snippet-level generation. These codebase
AI assistants (CBAs) can also extend or adapt codebases, allowing users to
focus on higher-level design and deployment decisions. While prior work has
extensively studied the impact of snippet-level code generation, this new class
of codebase generation models is relatively unexplored. Despite initial
anecdotal reports of excitement about these agents, they remain less frequently
adopted compared to snippet-level code assistants. To utilize CBAs better, we
need to understand how developers interact with CBAs, and how and why CBAs fall
short of developers' needs. In this paper, we explored these gaps through a
counterbalanced user study and interview with (n = 16) students and developers
working on coding tasks with CBAs. We found that participants varied the
information in their prompts, like problem description (48% of prompts),
required functionality (98% of prompts), code structure (48% of prompts), and
their prompt writing process. Despite various strategies, the overall
satisfaction score with generated codebases remained low (mean = 2.8, median =
3, on a scale of one to five). Participants mentioned functionality as the most
common factor for dissatisfaction (77% of instances), alongside poor code
quality (42% of instances) and communication issues (25% of instances). We
delve deeper into participants' dissatisfaction to identify six underlying
challenges that participants faced when using CBAs, and extracted five barriers
to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial
CBAs to compare their capabilities with participant challenges and present
design opportunities for more efficient and useful CBAs.

</details>


### [633] [PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C](https://arxiv.org/abs/2508.08171)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: PyVeritas proposes a novel framework for using Large Language Models (LLMs) to transpile Python code into C for formal verification and bug localization using existing model checkers.


<details>
  <summary>Details</summary>
Motivation: Python lacks robust tools for formal verification, unlike C, which has mature model checkers like CBMC.

Method: PyVeritas leverages LLMs for high-level Python to C transpilation, followed by bounded model checking and MaxSAT-based fault localization in the C code.

Result: Empirical evaluation on two Python benchmarks shows that LLM-based transpilation achieves 80–90% accuracy with assertion-based verification and interpretable fault diagnosis.

Conclusion: PyVeritas bridges the gap in formal verification for Python by utilizing LLMs, enabling effective verification and debugging for small Python programs.

Abstract: Python has become the dominant language for general-purpose programming, yet
it lacks robust tools for formal verification. In contrast, programmers working
in languages such as C benefit from mature model checkers, for example CBMC,
which enable exhaustive symbolic reasoning and fault localisation. The inherent
complexity of Python, coupled with the verbosity and low-level nature of
existing transpilers (e.g., Cython), have historically limited the
applicability of formal verification to Python programs.
  In this paper, we propose PyVeritas, a novel framework that leverages Large
Language Models (LLMs) for high-level transpilation from Python to C, followed
by bounded model checking and MaxSAT-based fault localisation in the generated
C code. PyVeritas enables verification and bug localisation for Python code
using existing model checking tools for C. Our empirical evaluation on two
Python benchmarks demonstrates that LLM-based transpilation can achieve a high
degree of accuracy, up to 80--90% for some LLMs, enabling effective development
environment that supports assertion-based verification and interpretable fault
diagnosis for small yet non-trivial Python programs.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [634] [Network-Specific Models for Multimodal Brain Response Prediction](https://arxiv.org/abs/2508.06499)
*Andrea Corsico,Giorgia Rigamonti,Simone Zini,Luigi Celona,Paolo Napoletano*

Main category: q-bio.NC

TL;DR: The paper introduces a method that uses clustered functional networks and multi-layer perceptrons (MLPs) for predicting brain responses to multimodal movies, significantly improving prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve brain response prediction to complex multimodal movies by leveraging functional organization of the brain.

Method: The authors grouped the seven Yeo functional networks into four clusters and trained cluster-specific, multi-subject MLP models to optimize temporal dynamics and modality weighting.

Result: The clustered strategy outperformed baseline methods, achieving high prediction accuracy and an eighth-place ranking in the Algonauts Project 2025 Challenge, with OOD correlation scores nearly doubling those of the baseline.

Conclusion: The clustered approach tailored to functional networks improves brain response predictions, emphasizing its effectiveness in network-specific modeling for complex stimuli like movies.

Abstract: In this work, we present a network-specific approach for predicting brain
responses to complex multimodal movies, leveraging the Yeo 7-network
parcellation of the Schaefer atlas. Rather than treating the brain as a
homogeneous system, we grouped the seven functional networks into four clusters
and trained separate multi-subject, multi-layer perceptron (MLP) models for
each. This architecture supports cluster-specific optimization and adaptive
memory modeling, allowing each model to adjust temporal dynamics and modality
weighting based on the functional role of its target network. Our results
demonstrate that this clustered strategy significantly enhances prediction
accuracy across the 1,000 cortical regions of the Schaefer atlas. The final
model achieved an eighth-place ranking in the Algonauts Project 2025 Challenge,
with out-of-distribution (OOD) correlation scores nearly double those of the
baseline model used in the selection phase. Code is available at
https://github.com/Corsi01/algo2025.

</details>


### [635] [Computing with Canonical Microcircuits](https://arxiv.org/abs/2508.06501)
*PK Douglas*

Main category: q-bio.NC

TL;DR: The paper presents a brain-inspired computational architecture, leveraging canonical microcircuits modeled as neural ODEs, achieving high accuracy and efficiency in image classification tasks while using fewer parameters than traditional models.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the desire to emulate the human brain's ability to perform robust learning and adaptive decision-making efficiently on minimal power, something current AI systems struggle to replicate.

Method: The method involves creating computational models based on canonical microcircuits (CMCs) of the brain, implemented as neural ODEs with biologically plausible recurrent connections. These circuits integrate various neuron types to form an 8-dimensional dynamical system.

Result: The model achieves 97.8% accuracy on the MNIST dataset with a single node and demonstrates improved performance on more complex datasets when using hierarchical configurations. It also uses significantly fewer parameters than conventional deep learning models.

Conclusion: Neuromorphic computing architectures based on brain-like computational principles offer an efficient and interpretable alternative to traditional AI systems, with potential to drive parameter-efficient model development.

Abstract: The human brain represents the only known example of general intelligence
that naturally aligns with human values. On a mere 20-watt power budget, the
brain achieves robust learning and adaptive decision-making in ways that
continue to elude advanced AI systems. Inspired by the brain, we present a
computational architecture based on canonical microcircuits (CMCs) -
stereotyped patterns of neurons found ubiquitously throughout the cortex. We
implement these circuits as neural ODEs comprising spiny stellate, inhibitory,
and pyramidal neurons, forming an 8-dimensional dynamical system with
biologically plausible recurrent connections. Our experiments show that even a
single CMC node achieves 97.8 percent accuracy on MNIST, while hierarchical
configurations - with learnable inter-regional connectivity and recurrent
connections - yield improved performance on more complex image benchmarks.
Notably, our approach achieves competitive results using substantially fewer
parameters than conventional deep learning models. Phase space analysis
revealed distinct dynamical trajectories for different input classes,
highlighting interpretable, emergent behaviors observed in biological systems.
These findings suggest that neuromorphic computing approaches can improve both
efficiency and interpretability in artificial neural networks, offering new
directions for parameter-efficient architectures grounded in the computational
principles of the human brain.

</details>


### [636] [Understanding Human Limits in Pattern Recognition: A Computational Model of Sequential Reasoning in Rock, Paper, Scissors](https://arxiv.org/abs/2508.06503)
*Logan Cross,Erik Brockbank,Tobias Gerstenberg,Judith E. Fan,Daniel L. K. Yamins,Nick Haber*

Main category: q-bio.NC

TL;DR: This paper investigates human prediction of behavior based on patterns, modeled through a rock-paper-scissors game, using a language model (Hypothetical Minds) that mirrors human strengths and limitations.


<details>
  <summary>Details</summary>
Motivation: Exploring the cognitive mechanisms and constraints that influence human ability to predict others' behavior from patterns, especially in the context of sequential-game strategies.

Method: The study used data from human rock-paper-scissors games against algorithmic opponents and applied the Hypothetical Minds (HM) language model to simulate human prediction and cognitive processing.

Result: The HM agent mimicked human success and failure in exploiting patterns, revealing that hypothesis generation is the primary cognitive bottleneck. Strategic interventions enhanced model performance, helping identify limitations in human reasoning.

Conclusion: Hypothetical Minds effectively mirrors human cognitive behavior, with its failures linked to hypothesis generation. Enhancing hypothesis generation experimentally contributes to understanding human learning constraints and predictive strategies.

Abstract: How do we predict others from patterns in their behavior and what are the
computational constraints that limit this ability? We investigate these
questions by modeling human behavior over repeated games of rock, paper,
scissors from Brockbank & Vul (2024). Against algorithmic opponents that varied
in strategic sophistication, people readily exploit simple transition patterns
(e.g., consistently playing rock after paper) but struggle to detect more
complex sequential dependencies. To understand the cognitive mechanisms
underlying these abilities and their limitations, we deploy Hypothetical Minds
(HM), a large language model-based agent that generates and tests hypotheses
about opponent strategies, as a cognitive model of this behavior (Cross et al.,
2024). We show that when applied to the same experimental conditions, HM
closely mirrors human performance patterns, succeeding and failing in similar
ways. To better understand the source of HM's failures and whether people might
face similar cognitive bottlenecks in this context, we performed a series of
ablations and augmentations targeting different components of the system. When
provided with natural language descriptions of the opponents' strategies, HM
successfully exploited 6/7 bot opponents with win rates >80% suggesting that
accurate hypothesis generation is the primary cognitive bottleneck in this
task. Further, by systematically manipulating the model's hypotheses through
pedagogically-inspired interventions, we find that the model substantially
updates its causal understanding of opponent behavior, revealing how
model-based analyses can produce testable hypotheses about human cognition.

</details>


### [637] [Synthetic Data Generation for Classifying Electrophysiological and Morpho-Electrophysiological Neurons from Mouse Visual Cortex](https://arxiv.org/abs/2508.06514)
*Xavier Vasques,Laura Cif*

Main category: q-bio.NC

TL;DR: This paper investigates the use of synthetic data augmentation methods, such as SMOTE, GANs, VAEs, Normalizing Flows, and DDPMs, to classify neuron types from mouse visual cortex data. It finds SMOTE produces the best classification results among the tested methods.


<details>
  <summary>Details</summary>
Motivation: Classifying neuronal cell types is vital for understanding brain function but is challenged by limited data and biological variability.

Method: The study benchmarks classical (e.g., SMOTE) and deep generative (e.g., GANs, VAEs) synthetic data augmentation methods using a curated dataset containing electrophysiological and morphological neuron features.

Result: SMOTE yielded the highest classification accuracy improvements for both electrophysiological and morpho-electrophysiological neuron types. GANs also performed well under specific conditions but were highly sensitive to settings.

Conclusion: SMOTE is an effective and outperforming synthetic data augmentation technique for classifying neuron types, with deep generative models showing potential under optimized conditions.

Abstract: The accurate classification of neuronal cell types is central to decoding
brain function, yet remains hindered by data scarcity and cellular
heterogeneity. Here, we benchmarked classical and deep generative synthetic
data augmentation strategies -- including SMOTE, GANs, VAEs, Normalizing Flows,
and DDPMs -- for supervised classification of both electrophysiological
(e-type) and morpho-electrophysiological (mee-type) neuron types from the mouse
visual cortex. Using a curated dataset annotated with 48 electrophysiological
and 24 morphological features, we established baseline classifiers and
introduced synthetic data generated by each method. Our results demonstrate
that SMOTE-based augmentation yields the highest classification accuracies
(absolute gains of 0.16 for e-types, 0.12 for mee-types), outperforming deep
generative models. GANs approached similar performance when hyperparameters and
sample sizes were optimized, but were more sensitive to model specification. In
addition, we benchmarked synthetic neuron fidelity by comparing mean absolute
errors between synthetic and real class profiles against the natural phenotypic
variability observed between real neuronal classes.

</details>


### [638] [Data-Efficient Neural Training with Dynamic Connectomes](https://arxiv.org/abs/2508.06817)
*Yutong Wu,Peilin He,Tananun Songdechakraiwut*

Main category: q-bio.NC

TL;DR: This paper introduces a method to analyze neural network training dynamics using functional connectome representations and proposes an early stopping criterion based on these dynamics.


<details>
  <summary>Details</summary>
Motivation: Understanding and controlling training dynamics in neural networks are challenging due to their complex hierarchical structures and high-dimensional parameter spaces.

Method: The authors represent evolving neural activations during training as dynamic functional connectomes, extract key signatures from these representations, and use them to monitor learning progress.

Result: Dynamic functional connectome signatures effectively capture key transitions in network organization and serve as reliable indicators for learning progress.

Conclusion: The framework provides robust learning progress indicators across benchmarks, insights into training dynamics, and introduces a principled early stopping criterion for neural networks.

Abstract: The study of dynamic functional connectomes has provided valuable insights
into how patterns of brain activity change over time. Neural networks process
information through artificial neurons, conceptually inspired by patterns of
activation in the brain. However, their hierarchical structure and
high-dimensional parameter space pose challenges for understanding and
controlling training dynamics. In this study, we introduce a novel approach to
characterize training dynamics in neural networks by representing evolving
neural activations as functional connectomes and extracting dynamic signatures
of activity throughout training. Our results show that these signatures
effectively capture key transitions in the functional organization of the
network. Building on this analysis, we propose the use of a time series of
functional connectomes as an intrinsic indicator of learning progress, enabling
a principled early stopping criterion. Our framework performs robustly across
benchmarks and provides new insights into neural network training dynamics.

</details>


### [639] [Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models](https://arxiv.org/abs/2508.07115)
*Antonino Greco,Marco D'Alessandro,Karl J. Friston,Giovanni Pezzulo,Markus Siegel*

Main category: q-bio.NC

TL;DR: Neuromorphic ConvRNNs with top-down feedback and dropout enhance robustness, stabilizing networks on low-dimensional manifolds.


<details>
  <summary>Details</summary>
Motivation: To explore the functional significance and computational role of top-down feedback in artificial vision systems inspired by biological vision.

Method: Convolutional recurrent neural networks (ConvRNNs) were trained on image classification tasks with and without top-down feedback pathways under conditions of stochastic variability simulated by dropout.

Result: Top-down feedback improved speed-accuracy trade-off, robustness to noise, adversarial attacks, and shaped representational geometry, especially when combined with dropout-induced variability.

Conclusion: Top-down feedback and neural stochasticity synergistically improve resilient sensory coding by stabilizing dynamics on low-dimensional manifolds and preventing co-adaptation.

Abstract: Biological systems leverage top-down feedback for visual processing, yet most
artificial vision models succeed in image classification using purely
feedforward or recurrent architectures, calling into question the functional
significance of descending cortical pathways. Here, we trained convolutional
recurrent neural networks (ConvRNN) on image classification in the presence or
absence of top-down feedback projections to elucidate the specific
computational contributions of those feedback pathways. We found that ConvRNNs
with top-down feedback exhibited remarkable speed-accuracy trade-off and
robustness to noise perturbations and adversarial attacks, but only when they
were trained with stochastic neural variability, simulated by randomly
silencing single units via dropout. By performing detailed analyses to identify
the reasons for such benefits, we observed that feedback information
substantially shaped the representational geometry of the post-integration
layer, combining the bottom-up and top-down streams, and this effect was
amplified by dropout. Moreover, feedback signals coupled with dropout optimally
constrained network activity onto a low-dimensional manifold and encoded object
information more efficiently in out-of-distribution regimes, with top-down
information stabilizing the representational dynamics at the population level.
Together, these findings uncover a dual mechanism for resilient sensory coding.
On the one hand, neural stochasticity prevents unit-level co-adaptation albeit
at the cost of more chaotic dynamics. On the other hand, top-down feedback
harnesses high-level information to stabilize network activity on compact
low-dimensional manifolds.

</details>


### [640] [Modeling bias in decision-making attractor networks](https://arxiv.org/abs/2508.07471)
*Safaan Sadiq*

Main category: q-bio.NC

TL;DR: This paper mathematically investigates bias shaping in decision-making in attractor neural networks with a focus on basins of attraction.


<details>
  <summary>Details</summary>
Motivation: Decision-making attractor networks blend choices and biases, but behavioral economics shows biases can shift independently. This study aims to explore how network parameters influence decision biases without altering choices.

Method: The study employs mathematical analysis on threshold linear networks, a computational neuroscience firing rate model, to understand parameter control of decision-making biases.

Result: The paper reveals how parameters independently adjust basins of attraction, affecting decision-making biases while attractors remain unchanged.

Conclusion: It is feasible to manipulate decision biases in attractor neural networks by altering basin shapes without disrupting the fundamental encoding of the choices.

Abstract: Attractor neural network models of cortical decision-making circuits
represent them as dynamical systems in the state space of neural firing rates
with the attractors of the network encoding possible decisions. While the
attractors of these models are well studied, far less attention is paid to the
basins of attraction even though their sizes can be said to encode the biases
towards the corresponding decisions. The parameters of an attractor network
control both the attractors and the basins of attraction. However, findings in
behavioral economics suggest that the framing of a decision-making task can
affect preferences even when the same choices are being offered. This suggests
that the circuit encodes both choices and biases separately, that preferences
can be changed without disrupting the encoding of the choices themselves. In
the context of attractor networks, this would mean that the parameters can be
adjusted to reshape the basins of attraction without changing the attractors
themselves. How can this be realized and how do the parameters shape
decision-making biases?
  In this PhD thesis we study this question mathematically in the context of
threshold linear networks, a common firing rate model used in computational
neuroscience. (Note: This is an abbreviated abstract. Please see the document
for the full abstract.)

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [641] [Federated Online Learning for Heterogeneous Multisource Streaming Data](https://arxiv.org/abs/2508.06652)
*Jingmao Li,Yuanxing Chen,Shuangge Ma,Kuangnan Fang*

Main category: stat.ML

TL;DR: The paper introduces a Federated Online Learning (FOL) method tailored for distributed, high-dimensional, streaming datasets, ensuring privacy and reduced storage needs while achieving optimal efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning methods are designed for static datasets, whereas real-world data often arrives continuously, leading to challenges in storage and algorithm design for streaming datasets, especially in high dimensions.

Method: The FOL method personalizes models for each data source, applies a ‘subgroup’ assumption to capture similarities, and uses penalized renewable estimation with proximal gradient descent for privacy-preserving, efficient updates.

Result: Theoretical proof of consistency in model estimation, variable selection, and subgroup recovery is provided. Simulations validate effectiveness, and practical applications on financial lending and web log data show superior predictive performance.

Conclusion: The method addresses federated online learning challenges efficiently and practically, offering both theoretical guarantees and real-world benefits like privacy preservation and insightful predictions.

Abstract: Federated learning has emerged as an essential paradigm for distributed
multi-source data analysis under privacy concerns. Most existing federated
learning methods focus on the ``static" datasets. However, in many real-world
applications, data arrive continuously over time, forming streaming datasets.
This introduces additional challenges for data storage and algorithm design,
particularly under high-dimensional settings. In this paper, we propose a
federated online learning (FOL) method for distributed multi-source streaming
data analysis. To account for heterogeneity, a personalized model is
constructed for each data source, and a novel ``subgroup" assumption is
employed to capture potential similarities, thereby enhancing model
performance. We adopt the penalized renewable estimation method and the
efficient proximal gradient descent for model training. The proposed method
aligns with both federated and online learning frameworks: raw data are not
exchanged among sources, ensuring data privacy, and only summary statistics of
previous data batches are required for model updates, significantly reducing
storage demands. Theoretically, we establish the consistency properties for
model estimation, variable selection, and subgroup structure recovery,
demonstrating optimal statistical efficiency. Simulations illustrate the
effectiveness of the proposed method. Furthermore, when applied to the
financial lending data and the web log data, the proposed method also exhibits
advantageous prediction performance. Results of the analysis also provide some
practical insights.

</details>


### [642] [MOCA-HESP: Meta High-dimensional Bayesian Optimization for Combinatorial and Mixed Spaces via Hyper-ellipsoid Partitioning](https://arxiv.org/abs/2508.06847)
*Lam Ngo,Huong Ha,Jeffrey Chan,Hongyu Zhang*

Main category: stat.ML

TL;DR: This paper introduces MOCA-HESP, a novel Bayesian Optimization method tailored for high-dimensional combinatorial and mixed domains. It improves performance by adaptively selecting optimal categorical encoders and integrating state-of-the-art optimizers.


<details>
  <summary>Details</summary>
Motivation: Current Bayesian Optimization methods struggle in high-dimensional combinatorial and mixed-variable domains, as these remain less explored compared to continuous domains.

Method: The paper proposes MOCA-HESP, which uses Hyper-Ellipsoid Space Partitioning alongside adaptive categorical encoder selection, facilitated by a multi-armed bandit technique. It acts as a meta-algorithm to enhance existing combinatorial and mixed BO optimizers.

Result: Three practical methods based on MOCA-HESP were tested on synthetic and real-world benchmarks. These methods consistently outperformed existing baseline BO approaches.

Conclusion: MOCA-HESP demonstrates significant advancement in optimizing high-dimensional combinatorial and mixed spaces. It is versatile, integrates well with existing optimizers, and improves state-of-the-art BO performance.

Abstract: High-dimensional Bayesian Optimization (BO) has attracted significant
attention in recent research. However, existing methods have mainly focused on
optimizing in continuous domains, while combinatorial (ordinal and categorical)
and mixed domains still remain challenging. In this paper, we first propose
MOCA-HESP, a novel high-dimensional BO method for combinatorial and mixed
variables. The key idea is to leverage the hyper-ellipsoid space partitioning
(HESP) technique with different categorical encoders to work with
high-dimensional, combinatorial and mixed spaces, while adaptively selecting
the optimal encoders for HESP using a multi-armed bandit technique. Our method,
MOCA-HESP, is designed as a \textit{meta-algorithm} such that it can
incorporate other combinatorial and mixed BO optimizers to further enhance the
optimizers' performance. Finally, we develop three practical BO methods by
integrating MOCA-HESP with state-of-the-art BO optimizers for combinatorial and
mixed variables: standard BO, CASMOPOLITAN, and Bounce. Our experimental
results on various synthetic and real-world benchmarks show that our methods
outperform existing baselines. Our code implementation can be found at
https://github.com/LamNgo1/moca-hesp

</details>


### [643] [Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation](https://arxiv.org/abs/2508.07049)
*Tran Tuan Kiet,Nguyen Thang Loi,Vo Nguyen Le Duy*

Main category: stat.ML

TL;DR: STAND-DA introduces a statistically rigorous framework for anomaly detection after domain adaptation, ensuring valid $p$-values and controlled false positive rates, with enhanced computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of degraded anomaly detection performance in target domains with limited data and the need for statistically valid outcomes in domain adaptation.

Method: STAND-DA combines Autoencoder-based anomaly detection with Representation Learning-based domain adaptation using a Selective Inference framework to compute valid $p$-values and control false positive rates. GPU-accelerated implementation is developed to improve scalability and runtime performance.

Result: The proposed framework demonstrates both theoretical soundness and computational efficiency in experiments on synthetic and real-world datasets.

Conclusion: STAND-DA makes Selective Inference practical for large-scale deep learning and offers a statistically rigorous solution for anomaly detection after domain adaptation.

Abstract: Anomaly detection (AD) plays a vital role across a wide range of domains, but
its performance might deteriorate when applied to target domains with limited
data. Domain Adaptation (DA) offers a solution by transferring knowledge from a
related source domain with abundant data. However, this adaptation process can
introduce additional uncertainty, making it difficult to draw statistically
valid conclusions from AD results. In this paper, we propose STAND-DA -- a
novel framework for statistically rigorous Autoencoder-based AD after
Representation Learning-based DA. Built on the Selective Inference (SI)
framework, STAND-DA computes valid $p$-values for detected anomalies and
rigorously controls the false positive rate below a pre-specified level
$\alpha$ (e.g., 0.05). To address the computational challenges of applying SI
to deep learning models, we develop the GPU-accelerated SI implementation,
significantly enhancing both scalability and runtime performance. This
advancement makes SI practically feasible for modern, large-scale deep
architectures. Extensive experiments on synthetic and real-world datasets
validate the theoretical results and computational efficiency of the proposed
STAND-DA method.

</details>


### [644] [Membership Inference Attacks with False Discovery Rate Control](https://arxiv.org/abs/2508.07066)
*Chenxu Zhao,Wei Qian,Aobo Chen,Mengdi Huai*

Main category: stat.ML

TL;DR: The paper proposes a new membership inference attack (MIA) method that offers guarantees for the false discovery rate (FDR) and can integrate with existing MIA techniques.


<details>
  <summary>Details</summary>
Motivation: The vulnerabilities of deep learning models to MIAs have gained attention, but existing methods lack guarantees on false discovery rates, which measure the proportion of false positives among identified positives.

Method: The authors designed a novel MIA method that wraps around existing approaches to ensure FDR guarantees and also provides marginal probability guarantees for identifying true non-member data as members.

Result: Theoretical analysis and extensive experiments, including black-box and lifelong learning settings, confirm the effectiveness and desirable performance of the proposed method.

Conclusion: This work advances the understanding and mitigation of MIAs by introducing an effective approach with FDR guarantees that can seamlessly complement existing techniques.

Abstract: Recent studies have shown that deep learning models are vulnerable to
membership inference attacks (MIAs), which aim to infer whether a data record
was used to train a target model or not. To analyze and study these
vulnerabilities, various MIA methods have been proposed. Despite the
significance and popularity of MIAs, existing works on MIAs are limited in
providing guarantees on the false discovery rate (FDR), which refers to the
expected proportion of false discoveries among the identified positive
discoveries. However, it is very challenging to ensure the false discovery rate
guarantees, because the underlying distribution is usually unknown, and the
estimated non-member probabilities often exhibit interdependence. To tackle the
above challenges, in this paper, we design a novel membership inference attack
method, which can provide the guarantees on the false discovery rate.
Additionally, we show that our method can also provide the marginal probability
guarantee on labeling true non-member data as member data. Notably, our method
can work as a wrapper that can be seamlessly integrated with existing MIA
methods in a post-hoc manner, while also providing the FDR control. We perform
the theoretical analysis for our method. Extensive experiments in various
settings (e.g., the black-box setting and the lifelong learning setting) are
also conducted to verify the desirable performance of our method.

</details>


### [645] [Stochastic dynamics learning with state-space systems](https://arxiv.org/abs/2508.07876)
*Juan-Pablo Ortega,Florian Rossmannek*

Main category: stat.ML

TL;DR: The paper unifies fading memory and echo state property (ESP) in reservoir computing, emphasizing their generic applicability across deterministic and stochastic settings and proposing novel theories for stochastic dynamics.


<details>
  <summary>Details</summary>
Motivation: To improve theoretical understanding of reservoir computing (RC), particularly why RC models perform well empirically despite lacking strict contractivity conditions.

Method: Examining state-space systems and analyzing fading memory and solution stability generically. Introducing distributional perspectives for stochastic echo states using attractor dynamics on probability distribution spaces.

Result: Demonstration that fading memory and solution stability are generic properties in RC models. Development of a coherent theory for stochastic echo states grounded in attractor dynamics.

Conclusion: The study provides a theoretical framework for RC models aiding reliable generative temporal data modeling in deterministic and stochastic settings.

Abstract: This work advances the theoretical foundations of reservoir computing (RC) by
providing a unified treatment of fading memory and the echo state property
(ESP) in both deterministic and stochastic settings. We investigate state-space
systems, a central model class in time series learning, and establish that
fading memory and solution stability hold generically -- even in the absence of
the ESP -- offering a robust explanation for the empirical success of RC models
without strict contractivity conditions. In the stochastic case, we critically
assess stochastic echo states, proposing a novel distributional perspective
rooted in attractor dynamics on the space of probability distributions, which
leads to a rich and coherent theory. Our results extend and generalize previous
work on non-autonomous dynamical systems, offering new insights into causality,
stability, and memory in RC models. This lays the groundwork for reliable
generative modeling of temporal data in both deterministic and stochastic
regimes.

</details>


### [646] [Meta Off-Policy Estimation](https://arxiv.org/abs/2508.07914)
*Olivier Jeunen*

Main category: stat.ML

TL;DR: This research introduces a method to improve off-policy estimation by combining multiple estimators and their confidence intervals using a correlated meta-analysis framework.


<details>
  <summary>Details</summary>
Motivation: While off-policy estimation methods are critical for unbiased evaluation in recommender systems, researchers face challenges when trying to leverage competing estimators effectively. This work aims to improve accuracy and statistical efficiency using a new combination strategy.

Method: The paper uses a correlated fixed-effects meta-analysis framework to combine multiple estimators. This approach accounts for dependencies among estimators due to shared offline data, creating a Best Linear Unbiased Estimate (BLUE) and offering improved confidence intervals.

Result: The method was validated on both simulated and real-world datasets, showing improved statistical efficiency compared to using individual estimators.

Conclusion: This approach provides a more accurate policy evaluation method for recommender systems, enhancing the reliability of off-policy estimation while maintaining conservative confidence intervals.

Abstract: Off-policy estimation (OPE) methods enable unbiased offline evaluation of
recommender systems, directly estimating the online reward some target policy
would have obtained, from offline data and with statistical guarantees. The
theoretical elegance of the framework combined with practical successes have
led to a surge of interest, with many competing estimators now available to
practitioners and researchers. Among these, Doubly Robust methods provide a
prominent strategy to combine value- and policy-based estimators.
  In this work, we take an alternative perspective to combine a set of OPE
estimators and their associated confidence intervals into a single, more
accurate estimate. Our approach leverages a correlated fixed-effects
meta-analysis framework, explicitly accounting for dependencies among
estimators that arise due to shared data. This yields a best linear unbiased
estimate (BLUE) of the target policy's value, along with an appropriately
conservative confidence interval that reflects inter-estimator correlation. We
validate our method on both simulated and real-world data, demonstrating
improved statistical efficiency over existing individual estimators.

</details>


### [647] [Gaussian Approximation for Two-Timescale Linear Stochastic Approximation](https://arxiv.org/abs/2508.07928)
*Bogdan Butyrin,Artemy Rubtsov,Alexey Naumov,Vladimir Ulyanov,Sergey Samsonov*

Main category: stat.ML

TL;DR: The paper derives non-asymptotic bounds for normal approximations in two-timescale stochastic approximation algorithms, focusing on the last iterate and Polyak-Ruppert averaging.


<details>
  <summary>Details</summary>
Motivation: To better understand the accuracy of normal approximation methods for TTSA algorithms, particularly how fast and slow timescales interact and influence the performance in different regimes.

Method: The authors analyze the convex distance between probability distributions and study non-asymptotic bounds for TTSA algorithms under martingale difference or Markov noise.

Result: The study found that the rate of normal approximation improves for the last iterate with increased timescale separation, but decreases in the Polyak-Ruppert averaging context. High-order moment bounds for TTSA errors are also provided.

Conclusion: The interaction between fast and slow timescales significantly affects normal approximation accuracy, shedding light on the trade-offs in TTSA algorithm performance and offering useful insights.

Abstract: In this paper, we establish non-asymptotic bounds for accuracy of normal
approximation for linear two-timescale stochastic approximation (TTSA)
algorithms driven by martingale difference or Markov noise. Focusing on both
the last iterate and Polyak-Ruppert averaging regimes, we derive bounds for
normal approximation in terms of the convex distance between probability
distributions. Our analysis reveals a non-trivial interaction between the fast
and slow timescales: the normal approximation rate for the last iterate
improves as the timescale separation increases, while it decreases in the
Polyak-Ruppert averaged setting. We also provide the high-order moment bounds
for the error of linear TTSA algorithm, which may be of independent interest.

</details>


### [648] [Likelihood Ratio Tests by Kernel Gaussian Embedding](https://arxiv.org/abs/2508.07982)
*Leonardo V. Santoro,Victor M. Panaretos*

Main category: stat.ML

TL;DR: The paper introduces a new kernel-based nonparametric two-sample test leveraging kernel mean and kernel covariance embeddings, demonstrating superior performance, especially in high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: Current two-sample test methods face challenges in high-dimensional or weak-signal environments, necessitating advancements for more robust and powerful tools.

Method: The authors propose a test leveraging combined embeddings that map probability measures to Gaussian measures in RKHS, utilizing a likelihood ratio test statistic tailored for singularity detection. A regularized version is implemented for finite samples, calibrated via permutation.

Result: The proposed method outperforms state-of-the-art methods in synthetic and real datasets, particularly excelling in high-dimensional and weak-signal contexts.

Conclusion: This test framework provides a consistent, powerful alternative for two-sample testing, improving upon and unifying existing methods like the spectrally regularized MMD.

Abstract: We propose a novel kernel-based nonparametric two-sample test, employing the
combined use of kernel mean and kernel covariance embedding. Our test builds on
recent results showing how such combined embeddings map distinct probability
measures to mutually singular Gaussian measures on the kernel's RKHS.
Leveraging this result, we construct a test statistic based on the relative
entropy between the Gaussian embeddings, i.e.\ the likelihood ratio. The
likelihood ratio is specifically tailored to detect equality versus singularity
of two Gaussians, and satisfies a ``$0/\infty$" law, in that it vanishes under
the null and diverges under the alternative. To implement the test in finite
samples, we introduce a regularised version, calibrated by way of permutation.
We prove consistency, establish uniform power guarantees under mild conditions,
and discuss how our framework unifies and extends prior approaches based on
spectrally regularized MMD. Empirical results on synthetic and real data
demonstrate remarkable gains in power compared to state-of-the-art methods,
particularly in high-dimensional and weak-signal regimes.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [649] [Rethinking Self-Replication: Detecting Distributed Selfhood in the Outlier Cellular Automaton](https://arxiv.org/abs/2508.08047)
*Arend Hintze,Clifford Bohm*

Main category: nlin.CG

TL;DR: This paper demonstrates that spontaneous self-replication in cellular automata can emerge unassisted, often in distributed forms, within a deterministic rule called the Outlier CA.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the possibility of spontaneous self-replication in cellular automata without requiring predesigned or artificial initialization.

Method: The study employs a data-driven framework to reconstruct causal ancestries of patterns in deterministic cellular automata, allowing for rigorous identification of self-replicators based on their causal lineages.

Result: The research establishes that self-replication in the Outlier cellular automaton is spontaneous, robust, and frequently involves coordinated multi-component systems.

Conclusion: This work challenges traditional definitions of individuality and replication in artificial life, showing that self-replication phenomena can occur naturally and complexly in cellular automata.

Abstract: Spontaneous self-replication in cellular automata has long been considered
rare, with most known examples requiring careful design or artificial
initialization. In this paper, we present formal, causal evidence that such
replication can emerge unassisted -- and that it can do so in a distributed,
multi-component form. Building on prior work identifying complex dynamics in
the Outlier rule, we introduce a data-driven framework that reconstructs the
full causal ancestry of patterns in a deterministic cellular automaton. This
allows us to rigorously identify self-replicating structures via explicit
causal lineages. Our results show definitively that self-replicators in the
Outlier CA are not only spontaneous and robust, but are also often composed of
multiple disjoint clusters working in coordination, raising questions about
some conventional notions of individuality and replication in artificial life
systems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [650] [Benchmarking Self-Driving Labs](https://arxiv.org/abs/2508.06642)
*Adedire D. Adesiji,Jiashuo Wang,Cheng-Shu Kuo,Keith A. Brown*

Main category: physics.comp-ph

TL;DR: The paper discusses self-driving labs (SDLs), which use machine learning and automation for improved materials discovery efficiency. Key metrics, acceleration factor (AF) and enhancement factor (EF), are introduced to quantify SDLs' effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to streamline and enhance materials discovery processes through self-driving labs. SDLs aim to conduct smarter and faster experiments, producing reliable results with rich metadata which conventional methods may lack.

Method: The authors introduce terms such as acceleration factor (AF) and enhancement factor (EF) to measure SDL performance. They review existing literature and run simulated Bayesian optimization campaigns for analysis.

Result: The study finds a wide range of acceleration factors (median AF = 6) and enhancement factor values peaking at 10-20 experiments per dimension. AF varies with complexity, while EF is influenced by parameter space properties.

Conclusion: SDLs show promising results in materials discovery by reducing experimental requirements and enhancing efficiency. The paper establishes standardized metrics and highlights SDLs' applicability across varied material parameter spaces.

Abstract: A key goal of modern materials science is accelerating the pace of materials
discovery. Self-driving labs, or systems that select experiments using machine
learning and then execute them using automation, are designed to fulfil this
promise by performing experiments faster, more intelligently, more reliably,
and with richer metadata than conventional means. This review summarizes
progress in understanding the degree to which SDLs accelerate learning by
quantifying how much they reduce the number of experiments required for a given
goal. The review begins by summarizing the theory underlying two key metrics,
namely acceleration factor AF and enhancement factor EF, which quantify how
much faster and better an algorithm is relative to a reference strategy. Next,
we provide a comprehensive review of the literature, which reveals a wide range
of AFs with a median of 6, and that tends to increase with the dimensionality
of the space, reflecting an interesting blessing of dimensionality. In
contrast, reported EF values vary by over two orders of magnitude, although
they consistently peak at 10-20 experiments per dimension. To understand these
results, we perform a series of simulated Bayesian optimization campaigns that
reveal how EF depends upon the statistical properties of the parameter space
while AF depends on its complexity. Collectively, these results reinforce the
motivation for using SDLs by revealing their value across a wide range of
material parameter spaces and provide a common language for quantifying and
understanding this acceleration.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [651] [CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs](https://arxiv.org/abs/2508.06917)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: q-bio.QM

TL;DR: The paper introduces the "CROss-view Prefixes" (CROP) method to enhance large language models (LLMs) in understanding complex molecular structures by integrating multiple structural views efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research stems from the limitations of large language models (LLMs) that rely solely on molecular sequences, which fail to capture crucial structural information. The aim is to leverage complementary structural views — topological (graph) and spatial (image) — to better represent and interpret molecular complexity.

Method: The proposed method, CROP, integrates multiple molecular structural views (graph and image) to enhance LLMs. It uses a SMILES Guided Resampler to resample structural views and a Structural Embedding Gate to encode them as efficient prefixes for LLMs, addressing challenges of context length and multi-view expansion.

Result: Experiments show that CROP achieves superior performance in tasks like molecule captioning, IUPAC name prediction, and molecule property prediction, demonstrating its effectiveness in multi-view molecular representation.

Conclusion: CROP successfully bridges the gap between LLMs and molecular structural complexity by efficiently and effectively integrating complementary views, paving the way for improved molecular understanding and applications.

Abstract: Recent advances in molecular science have been propelled significantly by
large language models (LLMs). However, their effectiveness is limited when
relying solely on molecular sequences, which fail to capture the complex
structures of molecules. Beyond sequence representation, molecules exhibit two
complementary structural views: the first focuses on the topological
relationships between atoms, as exemplified by the graph view; and the second
emphasizes the spatial configuration of molecules, as represented by the image
view. The two types of views provide unique insights into molecular structures.
To leverage these views collaboratively, we propose the CROss-view Prefixes
(CROP) to enhance LLMs' molecular understanding through efficient multi-view
integration. CROP possesses two advantages: (i) efficiency: by jointly
resampling multiple structural views into fixed-length prefixes, it avoids
excessive consumption of the LLM's limited context length and allows easy
expansion to more views; (ii) effectiveness: by utilizing the LLM's
self-encoded molecular sequences to guide the resampling process, it boosts the
quality of the generated prefixes. Specifically, our framework features a
carefully designed SMILES Guided Resampler for view resampling, and a
Structural Embedding Gate for converting the resulting embeddings into LLM's
prefixes. Extensive experiments demonstrate the superiority of CROP in tasks
including molecule captioning, IUPAC name prediction and molecule property
prediction.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [652] [Presburger Functional Synthesis: Complexity and Tractable Normal Forms](https://arxiv.org/abs/2508.07207)
*S. Akshay,A. R. Balasubramanian,Supratik Chakraborty,Georg Zetzsche*

Main category: cs.LO

TL;DR: The paper investigates the problem of functional synthesis for Presburger Arithmetic, showing EXPTIME complexity and introducing a specialized normal form (PSyNF) for efficient solvability.


<details>
  <summary>Details</summary>
Motivation: To explore functional synthesis for Presburger Arithmetic, as recent developments mostly focus on theories like Boolean or general first-order logic.

Method: Provide an EXPTIME algorithm for solving Presburger Functional Synthesis (PFnS), establish lower bounds, introduce PSyNF for efficient solvability, and analyze its properties and compilation conditions.

Result: PFnS is proven to be solvable in EXPTIME with a matching lower bound, showing its unique complexity compared to BFnS. PSyNF is proposed, allowing polynomial-time synthesis under certain conditions.

Conclusion: The research introduces computable strategies and forms (PSyNF) for tackling PFnS efficiently, making significant theoretical contributions to the field of logic-based functional synthesis.

Abstract: Given a relational specification between inputs and outputs as a logic
formula, the problem of functional synthesis is to automatically synthesize a
function from inputs to outputs satisfying the relation. Recently, a rich line
of work has emerged tackling this problem for specifications in different
theories, from Boolean to general first-order logic. In this paper, we launch
an investigation of this problem for the theory of Presburger Arithmetic, that
we call Presburger Functional Synthesis (PFnS). We show that PFnS can be solved
in EXPTIME and provide a matching exponential lower bound. This is unlike the
case for Boolean functional synthesis (BFnS), where only conditional
exponential lower bounds are known. Further, we show that PFnS for one input
and one output variable is as hard as BFnS in general. We then identify a
special normal form, called PSyNF, for the specification formula that
guarantees poly-time and poly-size solvability of PFnS. We prove several
properties of PSyNF, including how to check and compile to this form, and
conditions under which any other form that guarantees poly-time solvability of
PFnS can be compiled in poly-time to PSyNF. Finally, we identify a syntactic
normal form that is easier to check but is exponentially less succinct than
PSyNF.

</details>


### [653] [From Knowledge to Conjectures: A Modal Framework for Reasoning about Hypotheses](https://arxiv.org/abs/2508.07304)
*Fabio Vitali*

Main category: cs.LO

TL;DR: The paper introduces conjectural modal logics to formalize reasoning with hypothetical assumptions, avoiding modal collapse by utilizing a paracomplete semantic framework.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in formalizing conjectural reasoning while mitigating issues such as modal collapse in classical systems.

Method: The authors employ a logic framework grounded in Weak Kleene logic or Description Logic, avoiding Axiom T and utilizing paracomplete semantics for undefined propositions.

Result: Constructed new modal systems KC and KDC that are complete, decidable, and robust under partial knowledge alongside dynamic transition operation $
\mathsf{settle}(\varphi)$.

Conclusion: The research successfully avoids modal collapse, formalizes conjectural reasoning, and presents robust modal systems capable of reasoning in hypothetical frameworks.

Abstract: This paper introduces a new family of cognitive modal logics designed to
formalize conjectural reasoning: a modal system in which cognitive contexts
extend known facts with hypothetical assumptions to explore their consequences.
Unlike traditional doxastic and epistemic systems, conjectural logics rely on a
principle, called Axiom C ($\varphi \rightarrow \Box\varphi$), that ensures
that all established facts are preserved across hypothetical layers. While
Axiom C was dismissed in the past due to its association with modal collapse,
we show that the collapse only arises under classical and bivalent assumptions,
and specifically in the presence of Axiom T. Hence we avoid Axiom T and adopt a
paracomplete semantic framework, grounded in Weak Kleene logic or Description
Logic, where undefined propositions coexist with modal assertions. This
prevents the modal collapse and guarantees a layering to distinguish between
factual and conjectural statements. Under this framework we define new modal
systems, e.g., KC and KDC, and show that they are complete, decidable, and
robust under partial knowledge. Finally, we introduce a dynamic operation,
$\mathsf{settle}(\varphi)$, which formalizes the transition from conjecture to
accepted fact, capturing the event of the update of a world's cognitive state
through the resolution of uncertainty.

</details>


### [654] [A Rule-Based Approach to Specifying Preferences over Conflicting Facts and Querying Inconsistent Knowledge Bases](https://arxiv.org/abs/2508.07742)
*Meghyn Bienvenu,Camille Bourgaux,Katsumi Inoue,Robin Jean*

Main category: cs.LO

TL;DR: This paper proposes a rule-based framework to specify and compute priority relations between conflicting facts in inconsistent knowledge bases, addressing cycles that may arise in preferences.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods for specifying preferences over inconsistent knowledge bases when resolving conflicts using repair-based semantics.

Method: A declarative rule-based framework is developed to specify priority relations, with techniques to handle cycles in expressed preferences and Answer Set Programming for system implementation.

Result: The framework was experimentally evaluated to demonstrate query answering under prioritized-repair semantics and its cycle resolution techniques.

Conclusion: The paper presents a valuable contribution towards managing inconsistency in knowledge bases by introducing a preference specification framework with practical implementations.

Abstract: Repair-based semantics have been extensively studied as a means of obtaining
meaningful answers to queries posed over inconsistent knowledge bases (KBs).
While several works have considered how to exploit a priority relation between
facts to select optimal repairs, the question of how to specify such
preferences remains largely unaddressed. This motivates us to introduce a
declarative rule-based framework for specifying and computing a priority
relation between conflicting facts. As the expressed preferences may contain
undesirable cycles, we consider the problem of determining when a set of
preference rules always yields an acyclic relation, and we also explore a
pragmatic approach that extracts an acyclic relation by applying various cycle
removal techniques. Towards an end-to-end system for querying inconsistent KBs,
we present a preliminary implementation and experimental evaluation of the
framework, which employs answer set programming to evaluate the preference
rules, apply the desired cycle resolution techniques to obtain a priority
relation, and answer queries under prioritized-repair semantics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [655] [GPU-Accelerated Syndrome Decoding for Quantum LDPC Codes below the 63 $μ$s Latency Threshold](https://arxiv.org/abs/2508.07879)
*Oscar Ferraz,Bruno Coutinho,Gabriel Falcao,Marco Gomes,Francisco A. Monteiro,Vitor Silva*

Main category: quant-ph

TL;DR: The paper introduces a GPU-accelerated decoder for QLDPC codes achieving sub-63 μs latency, outperforming surface codes in scalability and meeting real-time requirements.


<details>
  <summary>Details</summary>
Motivation: Surface codes struggle with scalability as their encoding rates diminish with increasing code distances. QLDPC codes offer constant-rate encoding and asymptotic goodness but introduce higher decoding complexities.

Method: The authors develop a parallelized belief propagation decoder utilizing syndrome information and commodity GPU hardware to achieve high performance.

Result: Their decoder meets tight latency constraints (23.3-50 μs) for various QLDPC codes, demonstrating feasibility for real-time fault-tolerant quantum computation.

Conclusion: Real-time, scalable decoding of advanced quantum codes is achievable using common GPU hardware, pushing quantum computation forward beyond surface codes.

Abstract: This paper presents a GPU-accelerated decoder for quantum low-density
parity-check (QLDPC) codes that achieves sub-$63$ $\mu$s latency, below the
surface code decoder's real-time threshold demonstrated on Google's Willow
quantum processor. While surface codes have demonstrated below-threshold
performance, the encoding rates approach zero as code distances increase,
posing challenges for scalability. Recently proposed QLDPC codes, such as those
by Panteleev and Kalachev, offer constant-rate encoding and asymptotic goodness
but introduce higher decoding complexity. To address such limitation, this work
presents a parallelized belief propagation decoder leveraging syndrome
information on commodity GPU hardware. Parallelism was exploited to maximize
performance within the limits of target latency, allowing decoding latencies
under $50$ $\mu$s for [[$784$, $24$, $24$]] codes and as low as $23.3$ $\mu$s
for smaller codes, meeting the tight timing constraints of superconducting
qubit cycles. These results show that real-time, scalable decoding of
asymptotically good quantum codes is achievable using widely available
commodity hardware, advancing the feasibility of fault-tolerant quantum
computation beyond surface codes.

</details>


### [656] [QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search](https://arxiv.org/abs/2508.07104)
*Yaswitha Gujju,Romain Harang,Chao Li,Tetsuo Shibuya,Qibin Zhao*

Main category: quant-ph

TL;DR: The paper introduces an evolutionary training-free quantum architecture search framework to address challenges in quantum feature maps, offering enhanced evaluation efficiency, hardware robustness, and competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To solve the challenges of flat training landscapes and lengthy training processes in quantum feature maps for data encoding.

Method: An evolutionary training-free QAS framework using circuit-based heuristics considering trainability, robustness, generalization, expressivity, complexity, and kernel-target alignment.

Result: Competitive accuracy in classification tasks across datasets, with improved hardware robustness and up to a 2x speedup in architecture search runtime.

Conclusion: The proposed approach surpasses state-of-the-art methods in sampling efficiency and runtime, demonstrating its viability and effectiveness in quantum circuit design.

Abstract: The quest for effective quantum feature maps for data encoding presents
significant challenges, particularly due to the flat training landscapes and
lengthy training processes associated with parameterised quantum circuits. To
address these issues, we propose an evolutionary training-free quantum
architecture search (QAS) framework that employs circuit-based heuristics
focused on trainability, hardware robustness, generalisation ability,
expressivity, complexity, and kernel-target alignment. By ranking circuit
architectures with various proxies, we reduce evaluation costs and incorporate
hardware-aware circuits to enhance robustness against noise. We evaluate our
approach on classification tasks (using quantum support vector machine) across
diverse datasets using both artificial and quantum-generated datasets. Our
approach demonstrates competitive accuracy on both simulators and real quantum
hardware, surpassing state-of-the-art QAS methods in terms of sampling
efficiency and achieving up to a 2x speedup in architecture search runtime.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [657] [Neural Beam Field for Spatial Beam RSRP Prediction](https://arxiv.org/abs/2508.06956)
*Keqiang Guo,Yuheng Zhong,Xin Tong,Jiangbin Lyu,Rui Zhang*

Main category: cs.IT

TL;DR: This paper introduces Neural Beam Field (NBF), a hybrid neural-physical framework, to predict beam-level RSRP efficiently and accurately in dense wireless networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high measurement overhead and fast channel variations in RSRP prediction for beam management in dense wireless networks.

Method: The paper combines a Transformer-based deep neural network to learn a Multi-path Conditional Power Profile (MCPP) from sparse data, and a physics-inspired module for analytical RSRP inference. It also integrates a Pretrain-and-Calibrate (PaC) strategy to enhance model adaptability and convergence.

Result: Extensive simulations show that NBF significantly outperforms traditional methods and pure DNNs in prediction accuracy, training efficiency, and generalization with a compact model size.

Conclusion: NBF offers a scalable, efficient, and physically grounded framework for beam management in next-generation dense wireless networks.

Abstract: Accurately predicting beam-level reference signal received power (RSRP) is
essential for beam management in dense multi-user wireless networks, yet
challenging due to high measurement overhead and fast channel variations. This
paper proposes Neural Beam Field (NBF), a hybrid neural-physical framework for
efficient and interpretable spatial beam RSRP prediction. Central to our
approach is the introduction of the Multi-path Conditional Power Profile
(MCPP), which bridges site-specific multipath propagation with antenna/beam
configurations via closed-form analytical modeling. We adopt a decoupled
``blackbox-whitebox" design: a Transformer-based deep neural network (DNN)
learns the MCPP from sparse user measurements and positions, while a
physics-inspired module analytically infers beam RSRP statistics. To improve
convergence and adaptivity, we further introduce a Pretrain-and-Calibrate (PaC)
strategy that leverages ray-tracing priors and on-site calibration using RSRP
data. Extensive simulations results demonstrate that NBF significantly
outperforms conventional table-based channel knowledge maps (CKMs) and pure
blackbox DNNs in prediction accuracy, training efficiency, and generalization,
while maintaining a compact model size. The proposed framework offers a
scalable and physically grounded solution for intelligent beam management in
next-generation dense wireless networks.

</details>


### [658] [Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems](https://arxiv.org/abs/2508.07009)
*Xintong Chen,Zhenyu Jiang,Jiangbin Lyu,Liqun Fu*

Main category: cs.IT

TL;DR: This paper proposes a framework using neural networks and a scheduling algorithm to address major challenges in using Intelligent Reflecting Surfaces (IRSs) for wireless networks.


<details>
  <summary>Details</summary>
Motivation: Intelligent Reflecting Surfaces face challenges like double-pathloss and complex multi-user scheduling in dense wireless networks, particularly in multi-user multi-IRS setups.

Method: A neural Channel Knowledge Map (CKM) is introduced with Transformer-based DNNs to predict spectral efficiency. Two cascade networks (LPS-Net and SE-Net) are used, supported by the Stable Matching-Iterative Balancing (SM-IB) scheduling algorithm.

Result: The CKM significantly improves prediction accuracy and computational efficiency, while SM-IB scheduling achieves near-optimal throughput with reduced complexity.

Conclusion: This method balances high prediction performance and scheduling efficiency, marking an advancement for addressing IRS challenges in next-generation wireless networks.

Abstract: Intelligent Reflecting Surfaces (IRSs) have potential for significant
performance gains in next-generation wireless networks but face key challenges,
notably severe double-pathloss and complex multi-user scheduling due to
hardware constraints. Active IRSs partially address pathloss but still require
efficient scheduling in cell-level multi-IRS multi-user systems, whereby the
overhead/delay of channel state acquisition and the scheduling complexity both
rise dramatically as the user density and channel dimensions increase.
Motivated by these challenges, this paper proposes a novel scheduling framework
based on neural Channel Knowledge Map (CKM), designing Transformer-based deep
neural networks (DNNs) to predict ergodic spectral efficiency (SE) from
historical channel/throughput measurements tagged with user positions.
Specifically, two cascaded networks, LPS-Net and SE-Net, are designed to
predict link power statistics (LPS) and ergodic SE accurately. We further
propose a low-complexity Stable Matching-Iterative Balancing (SM-IB) scheduling
algorithm. Numerical evaluations verify that the proposed neural CKM
significantly enhances prediction accuracy and computational efficiency, while
the SM-IB algorithm effectively achieves near-optimal max-min throughput with
greatly reduced complexity.

</details>


### [659] [Communication-Learning Co-Design for Differentially Private Over-the-Air Federated Distillation](https://arxiv.org/abs/2508.06557)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: The paper addresses challenges in federated learning by introducing a differentially private, over-the-air federated distillation framework that improves communication efficiency and privacy while preserving learning quality.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and privacy concerns in traditional federated learning caused by ever-growing model sizes.

Method: A differentially private over-the-air federated distillation (FD) framework is proposed, leveraging noise-perturbed model outputs and multi-access channels, paired with an analytical approach to optimize transceiver and training decisions.

Result: The proposed framework achieves improved learning convergence rates, better learning-privacy trade-offs, and reduced communication overhead compared to conventional federated learning benchmarks.

Conclusion: Differentially private over-the-air FD offers a more communication-efficient and privacy-preserving approach to federated learning, showing potential for practical implementations.

Abstract: The ever-growing learning model size nowadays challenges the communication
efficiency and privacy preservation of the traditional federated learning (FL).
In this paper, we propose a novel differentially private (DP) over-the-air
federated distillation (FD) framework, where wireless devices (WDs)
periodically share noise-perturbed model outputs with the parameter server by
harnessing the superposition property of multi-access channels. Accordingly,
over-the-air FD enables the shared responsibility of the DP preservation on the
low-dimensional disclosed signals among WDs. We study the
communication-learning co-design problem in differentially private over-the-air
FD, aiming to maximize the learning convergence rate while meeting the transmit
power and DP requirements of WDs. The main challenge is rooted in the
intractable learning and privacy analysis in over-the-air FD, together with the
strong coupling among the decision variables spanning two timescales. To tackle
this problem, we first derive the analytical learning convergence rate and
privacy losses of WDs, based on which the optimal transceiver design per FD
round and long-term training rounds decision are obtained in the closed forms.
Numerical results demonstrate that the proposed differentially private
over-the-air FD approach achieves a better learning-privacy trade-off with
largely-reduced communication overhead than the conventional FL benchmarks.

</details>


### [660] [Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths](https://arxiv.org/abs/2508.07487)
*Vukan Ninkovic,Dejan Vukobratovic*

Main category: cs.IT

TL;DR: This paper proposes a structured autoencoder (AE)-based method for unequal error protection (UEP) coding, extending AE applicability to larger blocklengths while enabling adjustable reliability levels.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying AE-based methods to UEP coding, especially for intermediate blocklengths, and improve existing UEP schemes with more scalability and efficiency.

Method: The authors designed an architecture that organizes encoding and decoding into smaller AE subblocks for efficient training and flexible adjustment of UEP reliability levels.

Result: Numerical experiments demonstrate that the proposed approach outperforms the achievability bounds of traditional UEP schemes based on randomized superposition coding with successive interference cancellation (SIC) decoding.

Conclusion: The structured AE-based architecture offers a scalable, effective solution for advanced UEP coding, suitable for next-generation communication networks.

Abstract: Unequal error protection (UEP) coding that enables differentiated reliability
levels within a transmitted message is essential for modern communication
systems. Autoencoder (AE)-based code designs have shown promise in the context
of learned equal error protection (EEP) coding schemes. However, their
application to UEP remains largely unexplored, particularly at intermediate
blocklengths, due to the increasing complexity of AE-based models. Inspired by
the proven effectiveness of superposition coding and successive interference
cancellation (SIC) decoding in conventional UEP schemes, we propose a
structured AE-based architecture that extends AE-based UEP codes to
substantially larger blocklengths while maintaining efficient training. By
structuring encoding and decoding into smaller AE subblocks, our method
provides a flexible framework for fine-tuning UEP reliability levels while
adapting to diverse system parameters. Numerical results show that the proposed
approach improves over established achievability bounds of randomized
superposition coding-based UEP schemes with SIC decoding, making the proposed
structured AE-based UEP codes a scalable and efficient solution for
next-generation networks.

</details>


### [661] [Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer](https://arxiv.org/abs/2402.16868)
*Peigen Ye,Yaping Sun,Shumin Yao,Hao Chen,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: This paper presents a robust approach for codebook-assisted generative semantic communication for images, effectively mitigating channel noise and outperforming existing methods by leveraging a high-quality codebook.


<details>
  <summary>Details</summary>
Motivation: Addressing the vulnerability of codebook-based semantic communication systems to channel noise by improving system robustness and enhancing image generation quality.

Method: The method involves jointly constructing the semantic codec and codebook, designing a vector-to-index transformer guided by the codebook to counteract channel noise and enable robust image generation.

Result: Generated images using the proposed system have better visual perception compared to traditional methods like JPEG+LDPC and JSCC, supported by numerical results and empirical comparisons.

Conclusion: Integrating high-quality codebooks significantly improves generative semantic communication systems, offering enhanced robustness and image quality under channel noise conditions.

Abstract: Codebook-based generative semantic communication attracts increasing
attention, since only indices are required to be transmitted when the codebook
is shared between transmitter and receiver. However, due to the fact that the
semantic relations among code vectors are not necessarily related to the
distance of the corresponding code indices, the performance of the
codebook-enabled semantic communication system is susceptible to the channel
noise. Thus, how to improve the system robustness against the noise requires
careful design. This paper proposes a robust codebook-assisted image semantic
communication system, where semantic codec and codebook are first jointly
constructed, and then vector-to-index transformer is designed guided by the
codebook to eliminate the effects of channel noise, and achieve image
generation. Thanks to the assistance of the high-quality codebook to the
Transformer, the generated images at the receiver outperform those of the
compared methods in terms of visual perception. In the end, numerical results
and generated images demonstrate the advantages of the generative semantic
communication method over JPEG+LDPC and traditional joint source channel coding
(JSCC) methods.

</details>


### [662] [Adaptive Source-Channel Coding for Semantic Communications](https://arxiv.org/abs/2508.07958)
*Dongxu Li,Kai Yuan,Jianhao Huang,Chuan Huang,Xiaoqi Qin,Shuguang Cui,Ping Zhang*

Main category: cs.IT

TL;DR: The paper introduces an adaptive source-channel coding (ASCC) scheme that integrates deep neural network-based semantic source coding with conventional digital channel coding to optimize end-to-end distortion in semantic communications over parallel Gaussian channels.


<details>
  <summary>Details</summary>
Motivation: Current methods for semantic communications, such as joint source-channel coding (JSCC) and separate source-channel coding (SSCC), face compatibility and optimality issues in existing communication systems, particularly in finite blocklength scenarios.

Method: The authors propose an ASCC approach that combines semantic coding using DNNs with digital channel coding, adapting rates and power allocation via logistic regression modeling and solving optimization problems using successive convex approximation.

Result: Simulation results confirm that the ASCC scheme achieves superior performance compared to JSCC and SSCC, proving more effective in handling single and parallel-channel scenarios while retaining compatibility with digital systems.

Conclusion: The ASCC scheme provides a viable and superior alternative for semantic communications by addressing adaptability and compatibility issues, delivering efficient performance in optimizing end-to-end distortions.

Abstract: Semantic communications (SemComs) have emerged as a promising paradigm for
joint data and task-oriented transmissions, combining the demands for both the
bit-accurate delivery and end-to-end (E2E) distortion minimization. However,
current joint source-channel coding (JSCC) in SemComs is not compatible with
the existing communication systems and cannot adapt to the variations of the
sources or the channels, while separate source-channel coding (SSCC) is
suboptimal in the finite blocklength regime. To address these issues, we
propose an adaptive source-channel coding (ASCC) scheme for SemComs over
parallel Gaussian channels, where the deep neural network (DNN)-based semantic
source coding and conventional digital channel coding are separately deployed
and adaptively designed. To enable efficient adaptation between the source and
channel coding, we first approximate the E2E data and semantic distortions as
functions of source coding rate and bit error ratio (BER) via logistic
regression, where BER is further modeled as functions of signal-to-noise ratio
(SNR) and channel coding rate. Then, we formulate the weighted sum E2E
distortion minimization problem for joint source-channel coding rate and power
allocation over parallel channels, which is solved by the successive convex
approximation. Finally, simulation results demonstrate that the proposed ASCC
scheme outperforms typical deep JSCC and SSCC schemes for both the single- and
parallel-channel scenarios while maintaining full compatibility with practical
digital systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [663] [Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody](https://arxiv.org/abs/2508.06890)
*Jinsung Yoon,Wooyeol Jeong,Jio Gim,Young-Joo Suh*

Main category: cs.SD

TL;DR: This paper presents Maestro-EVC, an emotional voice conversion framework with enhanced controllability for transferring emotions and speaker identity independently.


<details>
  <summary>Details</summary>
Motivation: Current EVC methods face challenges in disentangling speaker identity and emotional style, and in modeling fine-grained emotional expressions such as temporal dynamics.

Method: The proposed method employs attribute disentanglement using separate references for content, speaker identity, and emotion. It introduces temporal emotion representation and explicit prosody modeling paired with prosody augmentation.

Result: Experiments demonstrate that Maestro-EVC generates speech with high quality, independent controllability of attributes, and expressive emotional transfers.

Conclusion: Maestro-EVC advances EVC capabilities by enabling fine-grained control of emotional style and speaker identity, successfully addressing prior limitations.

Abstract: Emotional voice conversion (EVC) aims to modify the emotional style of speech
while preserving its linguistic content. In practical EVC, controllability, the
ability to independently control speaker identity and emotional style using
distinct references, is crucial. However, existing methods often struggle to
fully disentangle these attributes and lack the ability to model fine-grained
emotional expressions such as temporal dynamics. We propose Maestro-EVC, a
controllable EVC framework that enables independent control of content, speaker
identity, and emotion by effectively disentangling each attribute from separate
references. We further introduce a temporal emotion representation and an
explicit prosody modeling with prosody augmentation to robustly capture and
transfer the temporal dynamics of the target emotion, even under
prosody-mismatched conditions. Experimental results confirm that Maestro-EVC
achieves high-quality, controllable, and emotionally expressive speech
synthesis.

</details>


### [664] [Joint Transcription of Acoustic Guitar Strumming Directions and Chords](https://arxiv.org/abs/2508.07973)
*Sebastian Murgul,Johannes Schimper,Michael Heizmann*

Main category: cs.SD

TL;DR: The paper addresses the challenging task of automatic guitar strumming transcription by introducing a novel dataset and a deep learning model, achieving significant improvements in accuracy for strumming detection and chord classification.


<details>
  <summary>Details</summary>
Motivation: Transcribing guitar strumming, including directions and chord progressions, is underexplored in Music Information Retrieval (MIR) and hindered by limited datasets.

Method: The authors introduce a multimodal approach utilizing a mix of real-world and synthetic datasets. A CRNN model is trained with 90 minutes of smartwatch motion sensor recordings and 4 hours of labeled synthetic audio data.

Result: Significant improvements are achieved in strumming detection and chord classification compared to baseline methods, especially using a hybrid dataset.

Conclusion: Deep learning shows promising potential for robust automatic transcription of guitar strumming and opens opportunities for further rhythm guitar analysis.

Abstract: Automatic transcription of guitar strumming is an underrepresented and
challenging task in Music Information Retrieval (MIR), particularly for
extracting both strumming directions and chord progressions from audio signals.
While existing methods show promise, their effectiveness is often hindered by
limited datasets. In this work, we extend a multimodal approach to guitar
strumming transcription by introducing a novel dataset and a deep
learning-based transcription model. We collect 90 min of real-world guitar
recordings using an ESP32 smartwatch motion sensor and a structured recording
protocol, complemented by a synthetic dataset of 4h of labeled strumming audio.
A Convolutional Recurrent Neural Network (CRNN) model is trained to detect
strumming events, classify their direction, and identify the corresponding
chords using only microphone audio. Our evaluation demonstrates significant
improvements over baseline onset detection algorithms, with a hybrid method
combining synthetic and real-world data achieving the highest accuracy for both
strumming action detection and chord classification. These results highlight
the potential of deep learning for robust guitar strumming transcription and
open new avenues for automatic rhythm guitar analysis.

</details>


### [665] [Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription](https://arxiv.org/abs/2508.07987)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: The paper explores using procedurally generated audio data to train guitar transcription models due to limited labeled datasets and legal constraints, showing that this approach improves transcription accuracy, especially with fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Transcribing acoustic guitar performances is difficult because of the lack of labeled data and legal limitations of using real recordings.

Method: The study generates synthetic training data using a pipeline comprising tablature composition, MIDI rendering, audio synthesis with an extended Karplus-Strong algorithm, and audio augmentation. A CRNN-based model is trained and evaluated on both synthetic and real datasets.

Result: Models trained on synthetic data demonstrated reasonable transcription accuracy, which further improved when fine-tuned with real audio data.

Conclusion: Procedurally generated audio offers a promising alternative for transcription tasks in data-scarce music information retrieval scenarios.

Abstract: Automatic transcription of acoustic guitar fingerpicking performances remains
a challenging task due to the scarcity of labeled training data and legal
constraints connected with musical recordings. This work investigates a
procedural data generation pipeline as an alternative to real audio recordings
for training transcription models. Our approach synthesizes training data
through four stages: knowledge-based fingerpicking tablature composition, MIDI
performance rendering, physical modeling using an extended Karplus-Strong
algorithm, and audio augmentation including reverb and distortion. We train and
evaluate a CRNN-based note-tracking model on both real and synthetic datasets,
demonstrating that procedural data can be used to achieve reasonable
note-tracking results. Finetuning with a small amount of real data further
enhances transcription accuracy, improving over models trained exclusively on
real recordings. These results highlight the potential of procedurally
generated audio for data-scarce music information retrieval tasks.

</details>


### [666] [Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning](https://arxiv.org/abs/2508.08039)
*Shu Wu,Chenxing Li,Wenfu Wang,Hao Zhang,Hualei Wang,Meng Yu,Dong Yu*

Main category: cs.SD

TL;DR: The paper introduces "Audio-Thinker," a reinforcement learning framework that improves the reasoning capabilities of Large Audio Language Models (LALMs), outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in LALMs, these models still underperform in auditory-language reasoning compared to human capabilities, necessitating better strategies for improving reasoning adaptability and quality.

Method: The proposed framework includes adaptive think accuracy rewards and external consistency evaluation. It utilizes reinforcement learning with think-based rewards to improve the model's reasoning strategies and monitor flawed reasoning.

Result: Experimental evidence demonstrates that the Audio-Thinker model significantly surpasses state-of-the-art LALMs in reasoning and generalization across benchmark tasks.

Conclusion: Audio-Thinker effectively addresses reasoning challenges in LALMs, providing a novel approach that enhances adaptability, consistency, and reasoning outcomes.

Abstract: Recent advancements in large language models, multimodal large language
models, and large audio language models (LALMs) have significantly improved
their reasoning capabilities through reinforcement learning with rule-based
rewards. However, the explicit reasoning process has yet to show significant
benefits for audio question answering, and effectively leveraging deep
reasoning remains an open challenge, with LALMs still falling short of
human-level auditory-language reasoning. To address these limitations, we
propose Audio-Thinker, a reinforcement learning framework designed to enhance
the reasoning capabilities of LALMs, with a focus on improving adaptability,
consistency, and effectiveness. Our approach introduces an adaptive think
accuracy reward, enabling the model to adjust its reasoning strategies based on
task complexity dynamically. Furthermore, we incorporate an external reward
model to evaluate the overall consistency and quality of the reasoning process,
complemented by think-based rewards that help the model distinguish between
valid and flawed reasoning paths during training. Experimental results
demonstrate that our Audio-Thinker model outperforms existing
reasoning-oriented LALMs across various benchmark tasks, exhibiting superior
reasoning and generalization capabilities.

</details>


### [667] [Whisfusion: Parallel ASR Decoding via a Diffusion Transformer](https://arxiv.org/abs/2508.07048)
*Taeyoun Kwon,Junhyuk Ahn,Taegeun Yun,Heeju Jwa,Yoonchae Choi,Siwon Park,Nam-Joon Kim,Jangchan Kim,Hyun Gon Ryu,Hyuk-Jae Lee*

Main category: cs.SD

TL;DR: The paper presents Whisfusion, an ASR framework combining a Whisper encoder and text diffusion decoder to provide fast, parallel, and efficient speech recognition, particularly for long utterances.


<details>
  <summary>Details</summary>
Motivation: Autoregressive ASR models suffer from latency due to sequential token generation, while non-autoregressive models struggle with context limitations, making real-time applications challenging.

Method: The paper introduces Whisfusion, which uses a Whisper encoder paired with a text diffusion decoder in a non-autoregressive architecture, supported by cross-attention adapters and a batch-parallel, multi-step decoding strategy.

Result: Whisfusion achieves better word error rates (8.3% vs. 9.7%) compared to Whisper-tiny and is up to 2.6x faster than autoregressive models on long utterances.

Conclusion: Whisfusion demonstrates a significant trade-off between speed and accuracy for both short and long audio inputs, offering a practical solution for real-time transcription tasks in ASR.

Abstract: Fast Automatic Speech Recognition (ASR) is critical for latency-sensitive
applications such as real-time captioning and meeting transcription. However,
truly parallel ASR decoding remains challenging due to the sequential nature of
autoregressive (AR) decoders and the context limitations of non-autoregressive
(NAR) methods. While modern ASR encoders can process up to 30 seconds of audio
at once, AR decoders still generate tokens sequentially, creating a latency
bottleneck. We propose Whisfusion, the first framework to fuse a pre-trained
Whisper encoder with a text diffusion decoder. This NAR architecture resolves
the AR latency bottleneck by processing the entire acoustic context in parallel
at every decoding step. A lightweight cross-attention adapter trained via
parameter-efficient fine-tuning (PEFT) bridges the two modalities. We also
introduce a batch-parallel, multi-step decoding strategy that improves accuracy
by increasing the number of candidates with minimal impact on speed. Fine-tuned
solely on LibriSpeech (960h), Whisfusion achieves a lower WER than Whisper-tiny
(8.3% vs. 9.7%), and offers comparable latency on short audio. For longer
utterances (>20s), it is up to 2.6x faster than the AR baseline, establishing a
new, efficient operating point for long-form ASR. The implementation and
training scripts are available at https://github.com/taeyoun811/Whisfusion.

</details>


### [668] [SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization](https://arxiv.org/abs/2508.07086)
*Beilong Tang,Xiaoxiao Miao,Xin Wang,Ming Li*

Main category: cs.SD

TL;DR: SEF-MK is a novel framework that anonymizes voice data using multiple k-means models for improved linguistic preservation and privacy dynamics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance voice anonymization methods that balance speaker privacy and retention of linguistic/emotional information.

Method: SEF-MK framework uses multiple k-means models, each trained on different speaker subsets, to anonymize self-supervised learning representations for each utterance.

Result: SEF-MK is effective in preserving linguistic and emotional content (user perspective) but also increases vulnerability to privacy attacks (attacker perspective).

Conclusion: The framework provides insights for designing voice anonymization systems, offering a trade-off between preserving content and minimizing privacy risks.

Abstract: Voice anonymization protects speaker privacy by concealing identity while
preserving linguistic and paralinguistic content. Self-supervised learning
(SSL) representations encode linguistic features but preserve speaker traits.
We propose a novel speaker-embedding-free framework called SEF-MK. Instead of
using a single k-means model trained on the entire dataset, SEF-MK anonymizes
SSL representations for each utterance by randomly selecting one of multiple
k-means models, each trained on a different subset of speakers. We explore this
approach from both attacker and user perspectives. Extensive experiments show
that, compared to a single k-means model, SEF-MK with multiple k-means models
better preserves linguistic and emotional content from the user's viewpoint.
However, from the attacker's perspective, utilizing multiple k-means models
boosts the effectiveness of privacy attacks. These insights can aid users in
designing voice anonymization systems to mitigate attacker threats.

</details>


### [669] [A Small-footprint Acoustic Echo Cancellation Solution for Mobile Full-Duplex Speech Interactions](https://arxiv.org/abs/2508.07561)
*Yiheng Jiang,Tian Biao*

Main category: cs.SD

TL;DR: This paper proposes a neural network-based Acoustic Echo Cancellation (AEC) method focusing on robustness, progressive learning, and tailored post-processing for speech enhancement, designed for mobile devices.


<details>
  <summary>Details</summary>
Motivation: To address challenges in mobile speech interaction systems caused by echo contamination, nonlinear distortions, and hardware variability.

Method: The approach includes diverse data augmentation, progressive learning, a novel post-processing strategy, and a small-footprint model for streaming inference.

Result: The method significantly improves echo reduction, speech quality, Voice Activity Detection (VAD), and Automatic Speech Recognition (ASR) performance.

Conclusion: The proposed solution is effective and practical for real-world applications on mobile devices with demonstrated empirical results.

Abstract: In full-duplex speech interaction systems, effective Acoustic Echo
Cancellation (AEC) is crucial for recovering echo-contaminated speech. This
paper presents a neural network-based AEC solution to address challenges in
mobile scenarios with varying hardware, nonlinear distortions and long latency.
We first incorporate diverse data augmentation strategies to enhance the
model's robustness across various environments. Moreover, progressive learning
is employed to incrementally improve AEC effectiveness, resulting in a
considerable improvement in speech quality. To further optimize AEC's
downstream applications, we introduce a novel post-processing strategy
employing tailored parameters designed specifically for tasks such as Voice
Activity Detection (VAD) and Automatic Speech Recognition (ASR), thus enhancing
their overall efficacy. Finally, our method employs a small-footprint model
with streaming inference, enabling seamless deployment on mobile devices.
Empirical results demonstrate effectiveness of the proposed method in Echo
Return Loss Enhancement and Perceptual Evaluation of Speech Quality, alongside
significant improvements in both VAD and ASR results.

</details>


### [670] [SCDF: A Speaker Characteristics DeepFake Speech Dataset for Bias Analysis](https://arxiv.org/abs/2508.07944)
*Vojtěch Staněk,Karel Srna,Anton Firc,Kamil Malinka*

Main category: cs.SD

TL;DR: The study introduces a dataset for analyzing demographic biases in deepfake speech detection, revealing disparities in detection performance based on speaker characteristics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on bias and fairness in deepfake speech detection methods.

Method: The authors created the SCDF dataset, containing over 237,000 balanced utterances across different sexes, languages, age groups, and evaluated deepfake detectors on these parameters.

Result: Analysis showed disparities in detection efficacy based on demographic factors like sex, language, age, and synthesizer type.

Conclusion: Bias-aware development is essential for creating fair and non-discriminatory deepfake detection systems adhering to ethical standards.

Abstract: Despite growing attention to deepfake speech detection, the aspects of bias
and fairness remain underexplored in the speech domain. To address this gap, we
introduce the Speaker Characteristics Deepfake (SCDF) dataset: a novel, richly
annotated resource enabling systematic evaluation of demographic biases in
deepfake speech detection. SCDF contains over 237,000 utterances in a balanced
representation of both male and female speakers spanning five languages and a
wide age range. We evaluate several state-of-the-art detectors and show that
speaker characteristics significantly influence detection performance,
revealing disparities across sex, language, age, and synthesizer type. These
findings highlight the need for bias-aware development and provide a foundation
for building non-discriminatory deepfake detection systems aligned with ethical
and regulatory standards.

</details>


### [671] [Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches](https://arxiv.org/abs/2508.08027)
*Ahmed Aboeitta,Ahmed Sharshar,Youssef Nafea,Shady Shehata*

Main category: cs.SD

TL;DR: The paper evaluates self-supervised ASR models for their effectiveness on dysarthric speech, integrating LLM-based decoding to enhance intelligibility and grammaticality, and offers an error analysis.


<details>
  <summary>Details</summary>
Motivation: Current ASR models struggle with dysarthric speech due to phoneme distortions and variability, prompting the need to explore their effectiveness and potential improvements.

Method: The paper systematically benchmarks multiple self-supervised ASR models with decoding strategies like CTC, seq2seq, and inclusion of LLMs like BART, GPT-2, and Vicuna for decoding.

Result: LLM-enhanced decoding demonstrated improved intelligibility and grammatical correctness for dysarthric speech by leveraging linguistic constraints.

Conclusion: Integrating LLM-based decoding into self-supervised ASR models can significantly enhance recognition of dysarthric speech, aiding in phoneme restoration and grammatical correction.

Abstract: Speech Recognition (ASR) due to phoneme distortions and high variability.
While self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown
promise, their effectiveness in dysarthric speech remains unclear. This study
systematically benchmarks these models with different decoding strategies,
including CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our
contributions include (1) benchmarking ASR architectures for dysarthric speech,
(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing
generalization across datasets, and (4) providing insights into recognition
errors across severity levels. Findings highlight that LLM-enhanced decoding
improves dysarthric ASR by leveraging linguistic constraints for phoneme
restoration and grammatical correction.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [672] [TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree](https://arxiv.org/abs/2508.07014)
*Andrei Andrusenko,Vladimir Bataev,Lilit Grigoryan,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: This paper introduces a universal context-biasing framework for ASR that is compatible with major ASR types and achieves high accuracy and speed using GPU-accelerated word boosting.


<details>
  <summary>Details</summary>
Motivation: Key phrase recognition in ASR often faces limitations due to the need for additional training, slow decoding processes, or system compatibility constraints.

Method: The authors implemented a GPU-accelerated word boosting tree, making the framework usable in shallow fusion mode for both greedy and beam search decoding without speed degradation.

Result: The proposed method demonstrated superior accuracy and decoding speed compared to open-source alternatives, even when dealing with up to 20K key phrases.

Conclusion: The framework delivers efficient, universal context biasing for ASR and is open-sourced in the NeMo toolkit.

Abstract: Recognizing specific key phrases is an essential task for contextualized
Automatic Speech Recognition (ASR). However, most existing context-biasing
approaches have limitations associated with the necessity of additional model
training, significantly slow down the decoding process, or constrain the choice
of the ASR system type. This paper proposes a universal ASR context-biasing
framework that supports all major types: CTC, Transducers, and Attention
Encoder-Decoder models. The framework is based on a GPU-accelerated word
boosting tree, which enables it to be used in shallow fusion mode for greedy
and beam search decoding without noticeable speed degradation, even with a vast
number of key phrases (up to 20K items). The obtained results showed high
efficiency of the proposed method, surpassing the considered open-source
context-biasing approaches in accuracy and decoding speed. Our context-biasing
framework is open-sourced as a part of the NeMo toolkit.

</details>


### [673] [FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities](https://arxiv.org/abs/2508.07315)
*Lilit Grigoryan,Vladimir Bataev,Nikolay Karpov,Andrei Andrusenko,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: The paper introduces FlexCTC, a Python and PyTorch-based toolkit enabling fast, efficient beam decoding fully on GPUs for CTC models, optimizing hardware utilization.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of slow, sequential, and CPU-bound beam decoding in speech recognition and maximize the potential of modern GPU hardware.

Method: FlexCTC toolkit implements fully GPU-based beam decoding using high-performance batching, CUDA Graphs to reduce overhead, and supports advanced language model fusion and contextualization techniques.

Result: FlexCTC achieves accurate, efficient GPU-powered beam decoding, eliminating CPU-GPU synchronization and enabling modern contextualization tools, improving usability for research and production.

Conclusion: FlexCTC provides a high-performance, user-friendly alternative to traditional decoders, making GPU-based decoding accessible and practical for speech recognition applications.

Abstract: While beam search improves speech recognition quality over greedy decoding,
standard implementations are slow, often sequential, and CPU-bound. To fully
leverage modern hardware capabilities, we present a novel open-source FlexCTC
toolkit for fully GPU-based beam decoding, designed for Connectionist Temporal
Classification (CTC) models. Developed entirely in Python and PyTorch, it
offers a fast, user-friendly, and extensible alternative to traditional C++,
CUDA, or WFST-based decoders. The toolkit features a high-performance, fully
batched GPU implementation with eliminated CPU-GPU synchronization and
minimized kernel launch overhead via CUDA Graphs. It also supports advanced
contextualization techniques, including GPU-powered N-gram language model
fusion and phrase-level boosting. These features enable accurate and efficient
decoding, making them suitable for both research and production use.

</details>


### [674] [G-IFT: A Gated Linear Unit adapter with Iterative Fine-Tuning for Low-Resource Children's Speaker Verification](https://arxiv.org/abs/2508.07836)
*Vishwas M. Shetty,Jiusi Zheng,Abeer Alwan*

Main category: eess.AS

TL;DR: The paper presents the G-IFT framework to improve speaker verification performance for children's speech using limited data.


<details>
  <summary>Details</summary>
Motivation: Existing SV systems trained on adult speech struggle with children's speech due to mismatch and insufficient child speech data.

Method: The G-IFT framework integrates a Gated Linear Unit adapter with iterative fine-tuning into pre-trained SV architectures.

Result: The framework reduces Equal Error Rates across multiple SV architectures when tested on OGI and MyST datasets.

Conclusion: G-IFT enhances knowledge transfer between adult and children's speech domains, improving performance for SV tasks involving children.

Abstract: Speaker Verification (SV) systems trained on adults speech often underperform
on children's SV due to the acoustic mismatch, and limited children speech data
makes fine-tuning not very effective. In this paper, we propose an innovative
framework, a Gated Linear Unit adapter with Iterative Fine-Tuning (G-IFT), to
enhance knowledge transfer efficiency between the high-resource adults speech
domain and the low-resource children's speech domain. In this framework, a
Gated Linear Unit adapter is first inserted between the pre-trained speaker
embedding model and the classifier. Then the classifier, adapter, and
pre-trained speaker embedding model are optimized sequentially in an iterative
way. This framework is agnostic to the type of the underlying architecture of
the SV system. Our experiments on ECAPA-TDNN, ResNet, and X-vector
architectures using the OGI and MyST datasets demonstrate that the G-IFT
framework yields consistent reductions in Equal Error Rates compared to
baseline methods.

</details>


### [675] [KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features](https://arxiv.org/abs/2508.07337)
*Ivan Kukanov,Jun Wah Ng*

Main category: eess.AS

TL;DR: This paper addresses the detection of audio-visual deepfakes using multimodal methods, emphasizing robustness, interpretability, and adaptability under unseen attack scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create robust, generalizable, and efficient methods to detect and localize increasingly advanced audio-visual deepfakes, as current detectors face limitations in generalization and high computational costs.

Method: The paper proposes a multimodal approach: the visual modality leverages hand-crafted features for interpretability and adaptability, while the audio modality employs a self-supervised learning backbone combined with graph attention networks for capturing rich representations.

Result: The approach achieves an AUC of 92.78% for deepfake classification and an IoU of 0.3536 for temporal localization when evaluated on the AV-Deepfake1M++ dataset using only the audio modality.

Conclusion: The proposed methods balance performance with deployment potential, improving detection resilience and interpretability, suggesting their suitability for real-world deepfake detection scenarios.

Abstract: The rapid development of audio-driven talking head generators and advanced
Text-To-Speech (TTS) models has led to more sophisticated temporal deepfakes.
These advances highlight the need for robust methods capable of detecting and
localizing deepfakes, even under novel, unseen attack scenarios. Current
state-of-the-art deepfake detectors, while accurate, are often computationally
expensive and struggle to generalize to novel manipulation techniques. To
address these challenges, we propose multimodal approaches for the
AV-Deepfake1M 2025 challenge. For the visual modality, we leverage handcrafted
features to improve interpretability and adaptability. For the audio modality,
we adapt a self-supervised learning (SSL) backbone coupled with graph attention
networks to capture rich audio representations, improving detection robustness.
Our approach strikes a balance between performance and real-world deployment,
focusing on resilience and potential interpretability. On the AV-Deepfake1M++
dataset, our multimodal system achieves AUC of 92.78% for deepfake
classification task and IoU of 0.3536 for temporal localization using only the
audio modality.

</details>


### [676] [Auditory Intelligence: Understanding the World Through Sound](https://arxiv.org/abs/2508.07829)
*Hyeonuk Nam*

Main category: eess.AS

TL;DR: The paper proposes expanding auditory intelligence to include perception, reasoning, and interaction, not just surface-level recognition.


<details>
  <summary>Details</summary>
Motivation: A need to go beyond just recognizing audio events to understanding their context, implications, and deeper meanings.

Method: Introduction of four task paradigms: ASPIRE, SODA, AUX, AUGMENT, targeting layered audio understanding including captioning, hierarchical description, causal explanation, and goal-driven interpretation.

Result: Developed paradigms provide a foundational roadmap for creating generalizable and human-aligned auditory intelligence systems.

Conclusion: Advances auditory intelligence toward machine systems capable of deeper, explainable sound understanding, driving broader discussions about machine audio comprehension.

Abstract: Recent progress in auditory intelligence has yielded high-performing systems
for sound event detection (SED), acoustic scene classification (ASC), automated
audio captioning (AAC), and audio question answering (AQA). Yet these tasks
remain largely constrained to surface-level recognition-capturing what happened
but not why, what it implies, or how it unfolds in context. I propose a
conceptual reframing of auditory intelligence as a layered, situated process
that encompasses perception, reasoning, and interaction. To instantiate this
view, I introduce four cognitively inspired task paradigms-ASPIRE, SODA, AUX,
and AUGMENT-those structure auditory understanding across time-frequency
pattern captioning, hierarchical event/scene description, causal explanation,
and goal-driven interpretation, respectively. Together, these paradigms provide
a roadmap toward more generalizable, explainable, and human-aligned auditory
intelligence, and are intended to catalyze a broader discussion of what it
means for machines to understand sound.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [677] [Noise-Aware Generative Microscopic Traffic Simulation](https://arxiv.org/abs/2508.07453)
*Vindula Jayawardana,Catherine Tang,Junyi Ji,Jonah Philion,Xue Bin Peng,Cathy Wu*

Main category: eess.SY

TL;DR: The paper presents the I-24 MOTION Scenario Dataset (I24-MSD), a noise-preserving dataset designed for generative traffic simulation models, and outlines how these models outperform traditional methods by accounting for data imperfections.


<details>
  <summary>Details</summary>
Motivation: To address the lack of realistic, standardized datasets for infrastructure-based sensing in traffic simulation, which often fail to account for real-world sensor imperfections.

Method: The researchers curated the I24-MSD dataset to capture sensor imperfections explicitly and applied noise-aware learning strategies to generative models for traffic simulation.

Result: The adapted generative models using noise-aware loss functions surpassed traditional baselines in realism and demonstrated improved handling of sensor imperfections.

Conclusion: Engaging with data imperfections rather than suppressing them can lead to more realistic and effective microscopic traffic simulation models, aligned with practical challenges in transportation systems.

Abstract: Accurately modeling individual vehicle behavior in microscopic traffic
simulation remains a key challenge in intelligent transportation systems, as it
requires vehicles to realistically generate and respond to complex traffic
phenomena such as phantom traffic jams. While traditional human driver
simulation models offer computational tractability, they do so by abstracting
away the very complexity that defines human driving. On the other hand, recent
advances in infrastructure-mounted camera-based roadway sensing have enabled
the extraction of vehicle trajectory data, presenting an opportunity to shift
toward generative, agent-based models. Yet, a major bottleneck remains: most
existing datasets are either overly sanitized or lack standardization, failing
to reflect the noisy, imperfect nature of real-world sensing. Unlike data from
vehicle-mounted sensors-which can mitigate sensing artifacts like occlusion
through overlapping fields of view and sensor fusion-infrastructure-based
sensors surface a messier, more practical view of challenges that traffic
engineers encounter. To this end, we present the I-24 MOTION Scenario Dataset
(I24-MSD)-a standardized, curated dataset designed to preserve a realistic
level of sensor imperfection, embracing these errors as part of the learning
problem rather than an obstacle to overcome purely from preprocessing. Drawing
from noise-aware learning strategies in computer vision, we further adapt
existing generative models in the autonomous driving community for I24-MSD with
noise-aware loss functions. Our results show that such models not only
outperform traditional baselines in realism but also benefit from explicitly
engaging with, rather than suppressing, data imperfection. We view I24-MSD as a
stepping stone toward a new generation of microscopic traffic simulation that
embraces the real-world challenges and is better aligned with practical needs.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [678] [Nonparametric Reaction Coordinate Optimization with Histories: A Framework for Rare Event Dynamics](https://arxiv.org/abs/2508.07326)
*Polina V. Banushkina,Sergei V. Krivov*

Main category: physics.chem-ph

TL;DR: This paper introduces a framework for optimizing reaction coordinates (RCs) to better capture the progress of rare events in complex systems, demonstrated using protein folding, ocean circulation models, and clinical datasets.


<details>
  <summary>Details</summary>
Motivation: Understanding rare events in systems like protein folding and climate phenomena requires optimal reaction coordinates (RCs) to analyze high-dimensional, stochastic dynamics. Current methods face challenges with irregular or incomplete data.

Method: The paper proposes a nonparametric RC optimization framework that incorporates trajectory histories, addressing the limitations of irregular or incomplete data through general application and validation tests.

Result: The framework accurately characterizes rare event dynamics, validated by protein folding analysis, which provided accurate committor estimates and high-resolution free energy profiles. It was also successfully applied to ocean circulation models and clinical datasets.

Conclusion: This general and flexible optimization framework enables robust characterization of rare event dynamics in high-dimensional systems without exhaustive sampling, opening new avenues for analyzing complex systems and datasets.

Abstract: Rare but critical events in complex systems, such as protein folding,
chemical reactions, disease progression, and extreme weather or climate
phenomena, are governed by complex, high-dimensional, stochastic dynamics.
Identifying an optimal reaction coordinate (RC) that accurately captures the
progress of these dynamics is crucial for understanding and simulating such
processes. This work introduces a nonparametric RC optimization framework that
incorporates trajectory histories, enabling robust analysis even for irregular
or incomplete data. The power of the method is demonstrated through
increasingly challenging analyses of protein folding dynamics, where it
provides accurate committor estimates that pass a stringent validation test and
yield high-resolution free energy profiles. Its generality is further
illustrated through applications to dynamics in phase space, a conceptual ocean
circulation model, and a longitudinal clinical dataset. These results
demonstrate that rare event dynamics can be accurately characterized without
exhaustive sampling of the configuration space, establishing a general,
flexible, and robust framework for analyzing complex dynamical systems and
longitudinal datasets.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [679] [Machines Learn Number Fields, But How? The Case of Galois Groups](https://arxiv.org/abs/2508.06670)
*Kyu-Hwan Lee,Seewoo Lee*

Main category: math.NT

TL;DR: The paper uses interpretable machine learning with decision trees to classify Galois groups using Dedekind zeta coefficients, proving new classification criteria.


<details>
  <summary>Details</summary>
Motivation: To explore whether interpretable machine learning tools can uncover mathematical insights, specifically to classify Galois groups using data characteristics from Dedekind zeta coefficients.

Method: Interpretable machine learning methods, particularly decision trees, are applied to analyze the dependence of Galois group classification on zeta coefficients.

Result: The machine learning analysis revealed patterns in zeta coefficients that are tied to specific Galois groups, enabling the derivation of new classification criteria.

Conclusion: This demonstrates how machine learning can serve as a paradigm for advancing mathematical research, providing both interpretability and the ability to discover new mathematical insights.

Abstract: By applying interpretable machine learning methods such as decision trees, we
study how simple models can classify the Galois groups of Galois extensions
over $\mathbb{Q}$ of degrees 4, 6, 8, 9, and 10, using Dedekind zeta
coefficients. Our interpretation of the machine learning results allows us to
understand how the distribution of zeta coefficients depends on the Galois
group, and to prove new criteria for classifying the Galois groups of these
extensions. Combined with previous results, this work provides another example
of a new paradigm in mathematical research driven by machine learning.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [680] [MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training](https://arxiv.org/abs/2508.07590)
*Xiongwei Xiao,Baoying Chen,Jishen Zeng,Jianquan Yang*

Main category: cs.MM

TL;DR: The paper introduces a lightweight network for face image quality assessment with a Multi-Stage Progressive Training (MSPT) strategy, achieving high performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional face image quality assessment methods struggle with face-specific characteristics, and learning-based methods, though effective, are computationally expensive.

Method: The paper proposes a lightweight network trained using a three-stage progressive strategy, involving diverse data samples and increasing image resolution.

Result: The MSPT achieved the second-highest score on the VQualA 2025 benchmark, demonstrating competitive performance with state-of-the-art methods while being efficient.

Conclusion: The proposed MSPT strategy allows lightweight networks to efficiently learn complex face quality features, offering a practical solution for face image quality assessment.

Abstract: Accurately assessing the perceptual quality of face images is crucial,
especially with the rapid progress in face restoration and generation.
Traditional quality assessment methods often struggle with the unique
characteristics of face images, limiting their generalizability. While
learning-based approaches demonstrate superior performance due to their strong
fitting capabilities, their high complexity typically incurs significant
computational and storage costs, hindering practical deployment. To address
this, we propose a lightweight face quality assessment network with Multi-Stage
Progressive Training (MSPT). Our network employs a three-stage progressive
training strategy that gradually introduces more diverse data samples and
increases input image resolution. This novel approach enables lightweight
networks to achieve high performance by effectively learning complex quality
features while significantly mitigating catastrophic forgetting. Our MSPT
achieved the second highest score on the VQualA 2025 face image quality
assessment benchmark dataset, demonstrating that MSPT achieves comparable or
better performance than state-of-the-art methods while maintaining efficient
inference.

</details>


### [681] [AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition](https://arxiv.org/abs/2508.07608)
*Junxiao Xue,Xiaozhen Liu,Xuecheng Wu,Xinyi Yin,Danlei Huang,Fei Yu*

Main category: cs.MM

TL;DR: The paper introduces AD-AVSR, a novel framework for audio-visual speech recognition (AVSR), using bidirectional modality enhancement to improve performance in noisy settings.


<details>
  <summary>Details</summary>
Motivation: Existing AVSR methods struggle with utilizing heterogeneous and complementary audio-visual data correlations, especially when handling asymmetric information between audio and visual inputs, leading to suboptimal recognition in challenging environments.

Method: AD-AVSR employs a dual-stream audio encoding strategy to enrich audio perspectives and incorporates two modules: an Audio-aware Visual Refinement Module to enhance visual data using audio cues, and a Cross-modal Noise Suppression Masking Module to refine audio using visual guidance. It also uses a threshold-based selection mechanism to discard weakly correlated audio-visual pairs.

Result: AD-AVSR outperforms state-of-the-art methods on the LRS2 and LRS3 datasets, showcasing superior recognition accuracy and noise robustness.

Conclusion: The AD-AVSR framework effectively captures and leverages bidirectional correlations in audio-visual data, providing improved performance and noise handling in AVSR tasks.

Abstract: Audio-visual speech recognition (AVSR) combines audio-visual modalities to
improve speech recognition, especially in noisy environments. However, most
existing methods deploy the unidirectional enhancement or symmetric fusion
manner, which limits their capability to capture heterogeneous and
complementary correlations of audio-visual data-especially under asymmetric
information conditions. To tackle these gaps, we introduce a new AVSR framework
termed AD-AVSR based on bidirectional modality enhancement. Specifically, we
first introduce the audio dual-stream encoding strategy to enrich audio
representations from multiple perspectives and intentionally establish
asymmetry to support subsequent cross-modal interactions. The enhancement
process involves two key components, Audio-aware Visual Refinement Module for
enhanced visual representations under audio guidance, and Cross-modal Noise
Suppression Masking Module which refines audio representations using visual
cues, collaboratively leading to the closed-loop and bidirectional information
flow. To further enhance correlation robustness, we adopt a threshold-based
selection mechanism to filter out irrelevant or weakly correlated audio-visual
pairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate
that our AD-AVSR consistently surpasses SOTA methods in both performance and
noise robustness, highlighting the effectiveness of our model design.

</details>


### [682] [VGGSounder: Audio-Visual Evaluations for Foundation Models](https://arxiv.org/abs/2508.08237)
*Daniil Zverev,Thaddäus Wiedemer,Ameya Prabhu,Matthias Bethge,Wieland Brendel,A. Sophia Koepke*

Main category: cs.MM

TL;DR: The paper critiques the limitations of the VGGSounder dataset in audio-visual model evaluation and introduces an improved version with better annotations and performance metrics.


<details>
  <summary>Details</summary>
Motivation: To address the deficiencies in the VGGSounder dataset, which include incomplete labels, overlapping classes, and misaligned modalities, thus ensuring a more reliable evaluation of audio-visual foundation models.

Method: The authors comprehensively re-annotated the VGGSound dataset to create a multi-label test set with detailed modality-specific annotations. They also introduced a new modality confusion metric for analyzing performance when additional modalities are introduced.

Result: They developed VGGSounder, a refined test set that resolves issues in the original VGGSounder dataset, and demonstrated its utility in evaluating multi-modal foundation models with improved precision.

Conclusion: The improved VGGSounder dataset and the new evaluation metrics offer a more reliable and nuanced framework for assessing the capabilities of audio-visual foundation models.

Abstract: The emergence of audio-visual foundation models underscores the importance of
reliably assessing their multi-modal understanding. The VGGSounder dataset is
commonly used as a benchmark for evaluation audio-visual classification.
However, our analysis identifies several limitations of VGGSounder, including
incomplete labelling, partially overlapping classes, and misaligned modalities.
These lead to distorted evaluations of auditory and visual capabilities. To
address these limitations, we introduce VGGSounder, a comprehensively
re-annotated, multi-label test set that extends VGGSound and is specifically
designed to evaluate audio-visual foundation models. VGGSounder features
detailed modality annotations, enabling precise analyses of modality-specific
performance. Furthermore, we reveal model limitations by analysing performance
degradation when adding another input modality with our new modality confusion
metric.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [683] [Leveraging GNN to Enhance MEF Method in Predicting ENSO](https://arxiv.org/abs/2508.07410)
*Saghar Ganji,Mohammad Naisipour*

Main category: physics.ao-ph

TL;DR: This paper proposes a graph-based framework to improve long-lead ENSO forecasting by optimizing ensemble predictions and enhancing coherence, yielding more robust outputs.


<details>
  <summary>Details</summary>
Motivation: Long-lead ENSO forecasting is challenging, and prior models, like MEF, did not optimize ensemble member utilization, making improvements necessary.

Method: The study uses graph-based analysis to model similarity among ensemble members, clusters high-performing predictions through community detection, and averages an optimized subset to improve forecast results.

Result: The graph-based method yields more stable and consistent predictions, especially in long-lead scenarios, and offers robust statistical insights into ensemble performance.

Conclusion: This approach demonstrates better forecast skill, increased stability, and model-agnostic applicability, enabling improvements for various large-scale ensemble forecasting models.

Abstract: Reliable long-lead forecasting of the El Nino Southern Oscillation (ENSO)
remains a long-standing challenge in climate science. The previously developed
Multimodal ENSO Forecast (MEF) model uses 80 ensemble predictions by two
independent deep learning modules: a 3D Convolutional Neural Network (3D-CNN)
and a time-series module. In their approach, outputs of the two modules are
combined using a weighting strategy wherein one is prioritized over the other
as a function of global performance. Separate weighting or testing of
individual ensemble members did not occur, however, which may have limited the
model to optimize the use of high-performing but spread-out forecasts. In this
study, we propose a better framework that employs graph-based analysis to
directly model similarity between all 80 members of the ensemble. By
constructing an undirected graph whose vertices are ensemble outputs and whose
weights on edges measure similarity (via RMSE and correlation), we identify and
cluster structurally similar and accurate predictions. From which we obtain an
optimized subset of 20 members using community detection methods. The final
prediction is then obtained by averaging this optimized subset. This method
improves the forecast skill through noise removal and emphasis on ensemble
coherence. Interestingly, our graph-based selection shows robust statistical
characteristics among top performers, offering new ensemble behavior insights.
In addition, we observe that while the GNN-based approach does not always
outperform the baseline MEF under every scenario, it produces more stable and
consistent outputs, particularly in compound long-lead situations. The approach
is model-agnostic too, suggesting that it can be applied directly to other
forecasting models with gargantuan ensemble outputs, such as statistical,
physical, or hybrid models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [684] [Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.06863)
*Hamidreza Asadian-Rad,Hossein Soleimani,Shahrokh Farahmand*

Main category: cs.MA

TL;DR: The paper outlines a fully decentralized approach using drones as edge servers in MEC, addressing challenges like UAV trajectory design and communication bottlenecks by integrating graph attention and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the need to address critical challenges in using UAVs as edge servers in MEC, including non-convex optimization, user mobility uncertainty, and communication bottlenecks in semi-centralized systems.

Method: The study proposes a fully decentralized framework where UAVs share limited information with neighbors. Each UAV uses local observations and executes a deep reinforcement learning (DRL) method incorporating graph attention layers (GAT) and experience-sharing proximal policy optimization (EPS-PPO).

Result: The proposed framework outperforms existing MADRL methods like MADDPG and local DRLs like IPPO in several performance metrics, all while relying solely on local communications.

Conclusion: Fully decentralized methods using DRL and GAT layers can improve UAVs' trajectory optimization in MEC systems, eliminating limitations of semi-centralized systems while maintaining or enhancing performance.

Abstract: Unmanned aerial vehicles (UAVs) have been recently utilized in multi-access
edge computing (MEC) as edge servers. It is desirable to design UAVs'
trajectories and user to UAV assignments to ensure satisfactory service to the
users and energy efficient operation simultaneously. The posed optimization
problem is challenging to solve because: (i) The formulated problem is
non-convex, (ii) Due to the mobility of ground users, their future positions
and channel gains are not known in advance, (iii) Local UAVs' observations
should be communicated to a central entity that solves the optimization
problem. The (semi-) centralized processing leads to communication overhead,
communication/processing bottlenecks, lack of flexibility and scalability, and
loss of robustness to system failures. To simultaneously address all these
limitations, we advocate a fully decentralized setup with no centralized
entity. Each UAV obtains its local observation and then communicates with its
immediate neighbors only. After sharing information with neighbors, each UAV
determines its next position via a locally run deep reinforcement learning
(DRL) algorithm. None of the UAVs need to know the global communication graph.
Two main components of our proposed solution are (i) Graph attention layers
(GAT), and (ii) Experience and parameter sharing proximal policy optimization
(EPS-PPO). Our proposed approach eliminates all the limitations of
semi-centralized MADRL methods such as MAPPO and MA deep deterministic policy
gradient (MADDPG), while guaranteeing a better performance than independent
local DRLs such as in IPPO. Numerical results reveal notable performance gains
in several different criteria compared to the existing MADDPG algorithm,
demonstrating the potential for offering a better performance, while utilizing
local communications only.

</details>


### [685] [Retrieval-Augmented Multi-Agent System for Rapid Statement of Work Generation](https://arxiv.org/abs/2508.07569)
*Amulya Suravarjhula,Rashi Chandrashekhar Agrawal,Sakshi Jayesh Patel,Rahul Gupta*

Main category: cs.MA

TL;DR: The paper introduces an AI-driven system for drafting Statements of Work (SOW), significantly reducing time and improving accuracy compared to manual methods.


<details>
  <summary>Details</summary>
Motivation: The SOW drafting process is slow, complex, and prone to errors, necessitating a solution to enhance efficiency and accuracy.

Method: The system uses three specialized AI agents—one for drafting, one for legal compliance checks, and one for formatting—to automate and customize the SOW creation process.

Result: The AI system created full SOWs in under 3 minutes, demonstrating high accuracy and quality in real-world testing.

Conclusion: The solution showcases how AI can streamline legal and business workflows, reduce legal risks, save time, and support professionals in decision-making tasks.

Abstract: Drafting a Statement of Work (SOW) is a vital part of business and legal
projects. It outlines key details like deliverables, timelines,
responsibilities, and legal terms. However, creating these documents is often a
slow and complex process. It usually involves multiple people, takes several
days, and leaves room for errors or outdated content. This paper introduces a
new AI-driven automation system that makes the entire SOW drafting process
faster, easier, and more accurate. Instead of relying completely on humans, the
system uses three intelligent components or 'agents' that each handle a part of
the job. One agent writes the first draft, another checks if everything is
legally correct, and the third agent formats the document and ensures
everything is in order. Unlike basic online tools that just fill in templates,
this system understands the meaning behind the content and customizes the SOW
to match the needs of the project. It also checks legal compliance and
formatting so that users can trust the result. The system was tested using real
business examples. It was able to create a full SOW in under three minutes,
compared to several hours or days using manual methods. It also performed well
in accuracy and quality, showing that it can reduce legal risks and save a lot
of time. This solution shows how artificial intelligence can be used to support
legal and business professionals by taking care of routine work and helping
them focus on more important decisions. It's a step toward making legal
processes smarter, faster, and more reliable.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [686] [Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions](https://arxiv.org/abs/2508.07948)
*John D. Mayfield*

Main category: q-bio.OT

TL;DR: The paper proposes a mathematical and computational framework for analyzing neurodegenerative diseases using quantum-classical hybrid techniques, focusing on oscillatory patterns and precision medicine.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional time-domain analyses of complex, nonlinear trajectories in neurodegenerative diseases by exploring hidden oscillatory patterns for improved predictive accuracy.

Method: Transforming time-series data into frequency domain using Fourier and Laplace transforms, modeling neuronal dynamics via Hamiltonian formulations, and applying quantum-classical hybrid computing, including variational quantum eigensolvers (VQE) and quaternionic representations.

Result: Demonstrates feasibility of quantum-enhanced analyses and highlights potential clinical applications for identifying high-risk patients using s-domain biomarkers, supported by previous studies showing high accuracy in Alzheimer’s classification.

Conclusion: The framework offers a theoretical foundation for advancing precision medicine in neurodegenerative diseases by leveraging quantum computing for future empirical validations.

Abstract: Progressive neurodegenerative diseases, including Alzheimer's disease (AD),
multiple sclerosis (MS), Parkinson's disease (PD), and amyotrophic lateral
sclerosis (ALS), exhibit complex, nonlinear trajectories that challenge
deterministic modeling. Traditional time-domain analyses of multiomic and
neuroimaging data often fail to capture hidden oscillatory patterns, limiting
predictive accuracy. We propose a theoretical mathematical framework that
transforms time-series data into frequency or s-domain using Fourier and
Laplace transforms, models neuronal dynamics via Hamiltonian formulations, and
employs quantum-classical hybrid computing with variational quantum
eigensolvers (VQE) for enhanced pattern detection. This theoretical construct
serves as a foundation for future empirical works in quantum-enhanced analysis
of neurodegenerative diseases. We extend this to quaternionic representations
with three imaginary axes ($i, j, k$) to model multistate Hamiltonians in
multifaceted disorders, drawing from quantum neuromorphic computing to capture
entangled neural dynamics \citep{Pehle2020, Emani2019}. This approach leverages
quantum advantages in handling high-dimensional amplitude-phase data, enabling
outlier detection and frequency signature analysis. Potential clinical
applications include identifying high-risk patients with rapid progression or
therapy resistance using s-domain biomarkers, supported by quantum machine
learning (QML) precedents achieving up to 99.89% accuracy in Alzheimer's
classification \citep{Belay2024, Bhowmik2025}. This framework aims to lay the
groundwork for redefining precision medicine for neurodegenerative diseases
through future validations.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [687] [Statistical Theory of Multi-stage Newton Iteration Algorithm for Online Continual Learning](https://arxiv.org/abs/2508.07419)
*Xinjia Lu,Chuhan Wang,Qian Zhao,Lixing Zhu,Xuehu Zhu*

Main category: stat.ME

TL;DR: The paper addresses catastrophic forgetting in online continual learning with non-stationary data streams by introducing a statistical framework and an efficient algorithm.


<details>
  <summary>Details</summary>
Motivation: To tackle the issue of catastrophic forgetting in continual learning, especially when constrained storage capacity prevents retaining historical data.

Method: Proposes a statistical framework incorporating random effects across model parameters, scalable to infinite dimensions, and introduces a Multi-step Newton Iteration algorithm to optimize computational efficiency.

Result: The framework's theoretical soundness is validated through asymptotic normality derivation, and its effectiveness is demonstrated on synthetic and real datasets.

Conclusion: The proposed framework offers a practical and theoretically grounded approach to addressing catastrophic forgetting and improving performance in continual learning scenarios.

Abstract: We focus on the critical challenge of handling non-stationary data streams in
online continual learning environments, where constrained storage capacity
prevents complete retention of historical data, leading to catastrophic
forgetting during sequential task training. To more effectively analyze and
address the problem of catastrophic forgetting in continual learning, we
propose a novel continual learning framework from a statistical perspective.
Our approach incorporates random effects across all model parameters and allows
the dimension of parameters to diverge to infinity, offering a general
formulation for continual learning problems. To efficiently process streaming
data, we develop a Multi-step Newton Iteration algorithm that significantly
reduces computational costs in certain scenarios by alleviating the burden of
matrix inversion. Theoretically, we derive the asymptotic normality of the
estimator, enabling subsequent statistical inference. Comprehensive validation
through synthetic data experiments and two real datasets analyses demonstrates
the effectiveness of our proposed method.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [688] [Role of Large Language Models and Retrieval-Augmented Generation for Accelerating Crystalline Material Discovery: A Systematic Review](https://arxiv.org/abs/2508.06691)
*Agada Joseph Oche,Arpan Biswas*

Main category: cond-mat.mtrl-sci

TL;DR: This paper reviews the role of large language models (LLMs), particularly when combined with domain-specific knowledge via retrieval-augmented generation (RAG), in accelerating materials discovery, including crystal structure prediction, defect analysis, and other applications.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of time-intensive and costly experiments in materials science by exploring the potential of LLMs to streamline the discovery of novel materials for real-world applications.

Method: A systematic review was conducted to examine the recent advancements in applying LLMs and RAG to materials science problems, with a focus on their integration into various sub-domains.

Result: The paper highlights state-of-the-art developments showing that LLMs, when paired with external knowledge sources, introduce new capabilities in predicting structures, discovering materials, and mining literature with improved efficiencies.

Conclusion: LLMs and RAG have transformative potential in materials science by reducing experimental costs and timelines. Future directions include refining these methods for broader applications in technologies such as electronics, optics, biomedicine, and energy storage.

Abstract: Large language models (LLMs) have emerged as powerful tools for
knowledge-intensive tasks across domains. In materials science, to find novel
materials for various energy efficient devices for various real-world
applications, requires several time and cost expensive simulations and
experiments. In order to tune down the uncharted material search space,
minimizing the experimental cost, LLMs can play a bigger role to first provide
an accelerated search of promising known material candidates. Furthermore, the
integration of LLMs with domain-specific information via retrieval-augmented
generation (RAG) is poised to revolutionize how researchers predict materials
structures, analyze defects, discover novel compounds, and extract knowledge
from literature and databases. In motivation to the potentials of LLMs and RAG
in accelerating material discovery, this paper presents a broad and systematic
review to examine the recent advancements in applying LLMs and RAG to key
materials science problems. We survey state-of-the-art developments in crystal
structure prediction, defect analysis, materials discovery, literature mining,
database integration, and multi-modal retrieval, highlighting how combining
LLMs with external knowledge sources enables new capabilities. We discuss the
performance, limitations, and implications of these approaches, and outline
future directions for leveraging LLMs to accelerate materials research and
discovery for advancement in technologies in the area of electronics, optics,
biomedical, and energy storage.

</details>


### [689] [Explainable AI for Curie Temperature Prediction in Magnetic Materials](https://arxiv.org/abs/2508.06996)
*M. Adeel Ajaib,Fariha Nasir,Abdul Rehman*

Main category: cond-mat.mtrl-sci

TL;DR: This paper applies machine learning to predict Curie temperatures of magnetic materials with high accuracy, emphasizing interpretability using explainable AI techniques.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the prediction of Curie temperatures for magnetic materials by leveraging modern machine learning approaches and gaining interpretable insights into the underlying physical factors.

Method: They augmented the NEMAD database with composition-based and domain-aware descriptors, employed multiple machine learning models (with Extra Trees Regressor showing the best R^2 = 0.85 ± 0.01), used k-means clustering for material group insights, and performed SHAP analysis for identifying key physicochemical variables.

Result: The Extra Trees Regressor achieved the highest predictive performance with R^2 = 0.85 on a balanced dataset, and SHAP analysis highlighted drivers like average atomic number and magnetic moment.

Conclusion: Machine learning models, combined with explainable AI techniques, effectively predict Curie temperatures while providing insights into the physicochemical factors involved in magnetic behavior.

Abstract: We explore machine learning techniques for predicting Curie temperatures of
magnetic materials using the NEMAD database. By augmenting the dataset with
composition-based and domain-aware descriptors, we evaluate the performance of
several machine learning models. We find that the Extra Trees Regressor
delivers the best performance reaching an R^2 score of up to 0.85 $\pm$ 0.01
(cross-validated) for a balanced dataset. We employ the k-means clustering
algorithm to gain insights into the performance of chemically distinct material
groups. Furthermore, we perform the SHAP analysis to identify key
physicochemical drivers of Curie behavior, such as average atomic number and
magnetic moment. By employing explainable AI techniques, this analysis offers
insights into the model's predictive behavior, thereby advancing scientific
interpretability.

</details>


### [690] [Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys](https://arxiv.org/abs/2508.07798)
*Cheng Li,Pengfei Danga,Yuehui Xiana,Yumei Zhou,Bofeng Shi,Xiangdong Ding,Jun Suna,Dezhen Xue*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces a data-driven approach for designing Shape Memory Alloys (SMAs) using GAN inversion, achieving alloys with superior properties like high transformation temperatures and mechanical work output.


<details>
  <summary>Details</summary>
Motivation: The challenge in designing SMAs with both high transformation temperatures and strong mechanical work capabilities remains unresolved in materials engineering.

Method: The study employs generative adversarial network (GAN) inversion combined with a property prediction model to optimize alloy compositions and processing parameters based on user-defined property goals.

Result: The framework successfully designed and experimentally validated five NiTi-based SMAs. One alloy, Ni$_{49.8}$Ti$_{26.4}$Hf$_{18.6}$Zr$_{5.2}$, outperformed existing NiTi alloys with superior transformation temperature, mechanical work output, enthalpy, and thermal hysteresis.

Conclusion: GAN inversion is proven as an efficient and versatile method for discovering high-performance complex alloys tailored to specific property targets.

Abstract: The design of shape memory alloys (SMAs) with high transformation
temperatures and large mechanical work output remains a longstanding challenge
in functional materials engineering. Here, we introduce a data-driven framework
based on generative adversarial network (GAN) inversion for the inverse design
of high-performance SMAs. By coupling a pretrained GAN with a property
prediction model, we perform gradient-based latent space optimization to
directly generate candidate alloy compositions and processing parameters that
satisfy user-defined property targets. The framework is experimentally
validated through the synthesis and characterization of five NiTi-based SMAs.
Among them, the Ni$_{49.8}$Ti$_{26.4}$Hf$_{18.6}$Zr$_{5.2}$ alloy achieves a
high transformation temperature of 404 $^\circ$C, a large mechanical work
output of 9.9 J/cm$^3$, a transformation enthalpy of 43 J/g , and a thermal
hysteresis of 29 {\deg}C, outperforming existing NiTi alloys. The enhanced
performance is attributed to a pronounced transformation volume change and a
finely dispersed of Ti$_2$Ni-type precipitates, enabled by sluggish Zr and Hf
diffusion, and semi-coherent interfaces with localized strain fields. This
study demonstrates that GAN inversion offers an efficient and generalizable
route for the property-targeted discovery of complex alloys.

</details>


### [691] [Digital generation of the 3-D pore architecture of isotropic membranes using 2-D cross-sectional scanning electron microscopy images](https://arxiv.org/abs/2508.06664)
*Sima Zeinali Danalou,Hooman Chamani,Arash Rabbani,Patrick C. Lee,Jason Hattrick Simpers,Jay R Werber*

Main category: cond-mat.mtrl-sci

TL;DR: The study advances a method to reconstruct 3D pore networks of membranes from 2D SEM images, overcoming challenges in resolving intricate pore geometries.


<details>
  <summary>Details</summary>
Motivation: Traditional SEM imaging doesn't resolve 3D pore architecture, yet 3D pore interconnectivity is critical for membrane performance. Conventional tomography is inaccessible and costly, necessitating a more efficient alternative.

Method: The researchers developed an enhanced reconstruction algorithm that generates 3D models from 2D SEM images while preserving statistical properties and accurately reproducing complex pore morphologies.

Result: The method produced high-fidelity 3D reconstructions of microporous membranes with superior resolution compared to X-ray tomography, though further development is needed for anisotropic membranes.

Conclusion: The SEM-based approach offers a cost-effective and accessible means for 3D reconstruction of isotropic porous membrane structures, while demonstrating excellent validation against traditional tomography methods.

Abstract: A major limitation of two-dimensional scanning electron microscopy (SEM) in
imaging porous membranes is its inability to resolve three-dimensional pore
architecture and interconnectivity, which are critical factors governing
membrane performance. Although conventional tomographic 3-D reconstruction
techniques can address this limitation, they are often expensive, technically
challenging, and not widely accessible. We previously introduced a
proof-of-concept method for reconstructing a membrane's 3-D pore network from a
single 2-D SEM image, yielding statistically equivalent results to those
obtained from 3-D tomography. However, this initial approach struggled to
replicate the diverse pore geometries commonly observed in real membranes. In
this study, we advance the methodology by developing an enhanced reconstruction
algorithm that not only maintains essential statistical properties (e.g., pore
size distribution), but also accurately reproduces intricate pore morphologies.
Applying this technique to a commercial microfiltration membrane, we generated
a high-fidelity 3-D reconstruction and derived key membrane properties.
Validation with X-ray tomography data revealed excellent agreement in
structural metrics, with our SEM-based approach achieving superior resolution
in resolving fine pore features. The tool can be readily applied to isotropic
porous membrane structures of any pore size, as long as those pores can be
visualized by SEM. Further work is needed for 3-D structure generation of
anisotropic membranes.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [692] [Historical Prediction Attention Mechanism based Trajectory Forecasting for Proactive Work Zone Safety in a Digital Twin Environment](https://arxiv.org/abs/2508.06544)
*Minhaj Uddin Ahmad,Mizanur Rahman,Alican Sevim,David Bodoh,Sakib Khan,Li Zhao,Nathan Huynh,Eren Erman Ozguven*

Main category: cs.OH

TL;DR: This study presents a Digital Twin-enabled proactive work zone safety system using advanced trajectory prediction and simulation techniques to issue early warnings for vehicle collisions.


<details>
  <summary>Details</summary>
Motivation: To enhance road safety in work zones by leveraging predictive technologies to provide early warnings and reduce vehicle conflicts.

Method: The system integrates real-time multi-sensor data, High-Definition maps, and a historical prediction network (HPNet) in a co-simulation environment combining SUMO, CARLA, and Lanelet2. It uses metrics like ADE and FDE for trajectory prediction accuracy.

Result: On work-zone datasets, the HPNet model outperformed benchmarks, achieving a Joint FDE of 0.3228 meters and Joint ADE of 0.1327 meters, better than Argoverse and Interaction datasets.

Conclusion: The infrastructure-enabled proactive warning system effectively predicts vehicle trajectories and issues early alerts, offering significant safety improvements in freeway work zones.

Abstract: Proactive safety systems aim to mitigate risks by anticipating potential
conflicts between vehicles and enabling early intervention to prevent work
zone-related crashes. This study presents an infrastructure-enabled proactive
work zone safety warning system that leverages a Digital Twin environment,
integrating real-time multi-sensor data, detailed High-Definition (HD) maps,
and a historical prediction attention mechanism-based trajectory prediction
model. Using a co-simulation environment that combines Simulation of Urban
MObility (SUMO) and CAR Learning to Act (CARLA) simulators, along with Lanelet2
HD maps and the Historical Prediction Network (HPNet) model, we demonstrate
effective trajectory prediction and early warning generation for vehicle
interactions in freeway work zones. To evaluate the accuracy of predicted
trajectories, we use two standard metrics: Joint Average Displacement Error
(ADE) and Joint Final Displacement Error (FDE). Specifically, the
infrastructure-enabled HPNet model demonstrates superior performance on the
work-zone datasets generated from the co-simulation environment, achieving a
minimum Joint FDE of 0.3228 meters and a minimum Joint ADE of 0.1327 meters,
lower than the benchmarks on the Argoverse (minJointFDE: 1.0986 m, minJointADE:
0.7612 m) and Interaction (minJointFDE: 0.8231 m, minJointADE: 0.2548 m)
datasets. In addition, our proactive safety warning generation application,
utilizing vehicle bounding boxes and probabilistic conflict modeling,
demonstrates its capability to issue alerts for potential vehicle conflicts.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [693] [Forecasting Commodity Price Shocks Using Temporal and Semantic Fusion of Prices Signals and Agentic Generative AI Extracted Economic News](https://arxiv.org/abs/2508.06497)
*Mohammed-Khalil Ghali,Cecil Pang,Oscar Molina,Carlos Gershenson-Garcia,Daehan Won*

Main category: q-fin.CP

TL;DR: This paper introduces a hybrid AI model combining time-series data and semantically processed news for highly accurate commodity price forecasting, significantly outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for accurate forecasting of commodity price spikes, especially in countries with fragile economic systems, as price shocks can severely impact national budgets, critical imports, and overall economic stability.

Method: The paper utilizes a dual-stream LSTM network with attention mechanisms to integrate historical commodity price data and semantically embedded global news summaries obtained through a generative AI pipeline. The model assesses data from 1960 to 2023 for its evaluation.

Result: The proposed model achieved an impressive mean AUC of 0.94 and an accuracy of 0.91, outshining baseline models like logistic regression (AUC=0.34), random forest (AUC=0.57), and support vector machines (AUC=0.47). Ablation studies highlight its dependence on semantic news input for performance.

Conclusion: The findings show that combining generative AI with deep learning in this architecture offers a valuable solution for detecting commodity price shocks early. The model is a practical, cost-effective tool to support economic risk management and planning in volatile markets.

Abstract: Accurate forecasting of commodity price spikes is vital for countries with
limited economic buffers, where sudden increases can strain national budgets,
disrupt import-reliant sectors, and undermine food and energy security. This
paper introduces a hybrid forecasting framework that combines historical
commodity price data with semantic signals derived from global economic news,
using an agentic generative AI pipeline. The architecture integrates
dual-stream Long Short-Term Memory (LSTM) networks with attention mechanisms to
fuse structured time-series inputs with semantically embedded, fact-checked
news summaries collected from 1960 to 2023. The model is evaluated on a 64-year
dataset comprising normalized commodity price series and temporally aligned
news embeddings. Results show that the proposed approach achieves a mean AUC of
0.94 and an overall accuracy of 0.91 substantially outperforming traditional
baselines such as logistic regression (AUC = 0.34), random forest (AUC = 0.57),
and support vector machines (AUC = 0.47). Additional ablation studies reveal
that the removal of attention or dimensionality reduction leads to moderate
declines in performance, while eliminating the news component causes a steep
drop in AUC to 0.46, underscoring the critical value of incorporating
real-world context through unstructured text. These findings demonstrate that
integrating agentic generative AI with deep learning can meaningfully improve
early detection of commodity price shocks, offering a practical tool for
economic planning and risk mitigation in volatile market environments while
saving the very high costs of operating a full generative AI agents pipeline.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [694] [Taking the Garbage Out of Data-Driven Prediction Across Climate Timescales](https://arxiv.org/abs/2508.07062)
*Jason C. Furtado,Maria J. Molina,Marybeth C. Arcodia,Weston Anderson,Tom Beucler,John A. Callahan,Laura M. Ciasto,Vittorio A. Gensini,Michelle L'Heureux,Kathleen Pegion,Jhayron S. Pérez-Carrasquilla,Maike Sonnewald,Ken Takahashi,Baoqiang Xiang,Brian G. Zimmerman*

Main category: physics.data-an

TL;DR: This paper introduces protocols to improve data preprocessing for AI/ML climate prediction models, emphasizing better practices for data quality to enhance forecast reliability.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing role of AI/ML in climate prediction and the need for robust data preprocessing methods to ensure forecast accuracy and reliability.

Method: The authors establish recommended practices for data preprocessing, including handling anomalies, addressing non-stationarity, managing spatial-temporal correlations, and dealing with extreme values in climate data.

Result: Case studies show that different preprocessing techniques can lead to varied predictions from the same model, underscoring the importance of standardized preprocessing methods for reliable climate predictions.

Conclusion: Proper data preprocessing enhances the robustness and transparency of AI/ML models in climate prediction, ultimately leading to higher confidence and skill in forecasts.

Abstract: Artificial intelligence (AI) -- and specifically machine learning (ML) --
applications for climate prediction across timescales are proliferating
quickly. The emergence of these methods prompts a revisit to the impact of data
preprocessing, a topic familiar to the climate community, as more traditional
statistical models work with relatively small sample sizes. Indeed, the skill
and confidence in the forecasts produced by data-driven models are directly
influenced by the quality of the datasets and how they are treated during model
development, thus yielding the colloquialism "garbage in, garbage out." As
such, this article establishes protocols for the proper preprocessing of input
data for AI/ML models designed for climate prediction (i.e., subseasonal to
decadal and longer). The three aims are to: (1) educate researchers,
developers, and end users on the effects that preprocessing has on climate
predictions; (2) provide recommended practices for data preprocessing for such
applications; and (3) empower end users to decipher whether the models they are
using are properly designed for their objectives. Specific topics covered in
this article include the creation of (standardized) anomalies, dealing with
non-stationarity and the spatiotemporally correlated nature of climate data,
and handling of extreme values and variables with potentially complex
distributions. Case studies will illustrate how using different preprocessing
techniques can produce different predictions from the same model, which can
create confusion and decrease confidence in the overall process. Ultimately,
implementing the recommended practices set forth in this article will enhance
the robustness and transparency of AI/ML in climate prediction studies.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [695] [Sharper Perturbed-Kullback-Leibler Exponential Tail Bounds for Beta and Dirichlet Distributions](https://arxiv.org/abs/2508.07991)
*Pierre Perrault*

Main category: math.PR

TL;DR: The paper refines the exponential tail bound for Beta distributions by introducing a perturbation that shifts the mean closer to zero, tightening the bound, and extending these results to Dirichlet distributions and processes.


<details>
  <summary>Details</summary>
Motivation: To refine the existing exponential tail bounds for Beta distributions by improving their KL divergence interpretation and extending this understanding to Dirichlet distributions and processes.

Method: The proposed method involves interpreting the exponential tail bound as a regular KL divergence while introducing perturbation to shift the Beta distribution's mean closer to zero, and then generalizing this approach to Dirichlet distributions and processes.

Result: Improved tightness of the exponential tail bound for Beta distributions, achieved through optimizing a perturbation. The refined framework is successfully extended to Dirichlet distributions and Dirichlet processes.

Conclusion: This improvement offers a better theoretical understanding and tighter bounds for Beta distributions, with implications for Dirichlet distributions and processes.

Abstract: This paper presents an improved exponential tail bound for Beta
distributions, refining a result in [15]. This improvement is achieved by
interpreting their bound as a regular Kullback-Leibler (KL) divergence one,
while introducing a specific perturbation $\eta$ that shifts the mean of the
Beta distribution closer to zero within the KL bound. Our contribution is to
show that a larger perturbation can be chosen, thereby tightening the bound. We
then extend this result from the Beta distribution to Dirichlet distributions
and Dirichlet processes (DPs).

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [696] [A Spin Glass Characterization of Neural Networks](https://arxiv.org/abs/2508.07397)
*Jun Li*

Main category: cond-mat.dis-nn

TL;DR: The paper connects neural networks with spin-glass systems, offering a new structural descriptor for individual feedforward neural networks (FNNs) based on replica overlaps to understand properties like generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the phenomenon of replica symmetry breaking (RSB) in spin glasses and the need for deeper insights into the structural properties of neural networks beyond standard metrics like loss or accuracy.

Method: A Hopfield-type spin glass model is derived from feedforward neural networks. By using overlaps of replica samples as descriptors, the structural characteristics of FNNs are investigated empirically.

Result: The empirical investigation links spin-glass overlaps with key neural network properties such as data fitting, capacity, generalization, and robustness. The method reveals unique structural insights not captured by traditional metrics.

Conclusion: The method offers a novel computable descriptor for analyzing individual neural networks, with potential applications in model inspection, safety verification, and detecting vulnerabilities.

Abstract: This work presents a statistical mechanics characterization of neural
networks, motivated by the replica symmetry breaking (RSB) phenomenon in spin
glasses. A Hopfield-type spin glass model is constructed from a given
feedforward neural network (FNN). Overlaps between simulated replica samples
serve as a characteristic descriptor of the FNN. The connection between the
spin-glass description and commonly studied properties of the FNN -- such as
data fitting, capacity, generalization, and robustness -- has been investigated
and empirically demonstrated. Unlike prior analytical studies that focus on
model ensembles, this method provides a computable descriptor for individual
network instances, which reveals nontrivial structural properties that are not
captured by conventional metrics such as loss or accuracy. Preliminary results
suggests its potential for practical applications such as model inspection,
safety verification, and detection of hidden vulnerabilities.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [697] [A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions](https://arxiv.org/abs/2508.06834)
*Toan Huynh,Ruth Lopez Fajardo,Guannan Zhang,Lili Ju,Feng Bao*

Main category: stat.CO

TL;DR: The paper introduces a novel score-based diffusion framework for adaptive solutions of stochastic partial differential equations (SPDEs), addressing issues like error and uncertainty.


<details>
  <summary>Details</summary>
Motivation: SPDEs are critical for modeling uncertain physical systems but face errors due to incomplete knowledge, necessitating adaptive and efficient numerical solutions.

Method: The authors use score-based diffusion models with physics-driven score functions and add likelihood-based corrections for Bayesian inference, introducing a computationally efficient ensemble score filter.

Result: The proposed method demonstrates high accuracy and robustness in solving SPDEs under sparse and noisy observational conditions, validated through numerical experiments.

Conclusion: This framework effectively integrates data and physics to adaptively and efficiently solve SPDEs, improving real-time inference even in high-dimensional scenarios.

Abstract: We propose a novel framework for adaptively learning the time-evolving
solutions of stochastic partial differential equations (SPDEs) using
score-based diffusion models within a recursive Bayesian inference setting.
SPDEs play a central role in modeling complex physical systems under
uncertainty, but their numerical solutions often suffer from model errors and
reduced accuracy due to incomplete physical knowledge and environmental
variability. To address these challenges, we encode the governing physics into
the score function of a diffusion model using simulation data and incorporate
observational information via a likelihood-based correction in a reverse-time
stochastic differential equation. This enables adaptive learning through
iterative refinement of the solution as new data becomes available. To improve
computational efficiency in high-dimensional settings, we introduce the
ensemble score filter, a training-free approximation of the score function
designed for real-time inference. Numerical experiments on benchmark SPDEs
demonstrate the accuracy and robustness of the proposed method under sparse and
noisy observations.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [698] [Exploring Strategies for Personalized Radiation Therapy: Part III Identifying genetic determinants for Radiation Response with Meta Learning](https://arxiv.org/abs/2508.08030)
*Hao Peng,Yuanyuan Zhang,Steve Jiang,Robert Timmerman,John Minna*

Main category: physics.med-ph

TL;DR: The paper proposes a meta-learning framework to predict cancer radiosensitivity using gene expression data, aiming for personalized treatment.


<details>
  <summary>Details</summary>
Motivation: Current cancer radiation treatments do not account for tumor heterogeneity, leading to standard dose prescriptions that may not be optimal.

Method: A meta-learning framework was developed that adjusts the importance of genes per sample and predicts radiosensitivity using cell-line gene expression data.

Result: The method showed good generalization to unseen samples and performed effectively in tumor types with high variability in radiosensitivity.

Conclusion: The approach improves accuracy in predicting radiosensitivity and enables personalized treatment by uncovering sample-specific gene influence patterns.

Abstract: Radiation response in cancer is shaped by complex, patient specific biology,
yet current treatment strategies often rely on uniform dose prescriptions
without accounting for tumor heterogeneity. In this study, we introduce a meta
learning framework for one-shot prediction of radiosensitivity measured by SF2
using cell line level gene expression data. Unlike the widely used
Radiosensitivity Index RSI a rank-based linear model trained on a fixed 10-gene
signature, our proposed meta-learned model allows the importance of each gene
to vary by sample through fine tuning. This flexibility addresses key
limitations of static models like RSI, which assume uniform gene contributions
across tumor types and discard expression magnitude and gene gene interactions.
Our results show that meta learning offers robust generalization to unseen
samples and performs well in tumor subgroups with high radiosensitivity
variability, such as adenocarcinoma and large cell carcinoma. By learning
transferable structure across tasks while preserving sample specific
adaptability, our approach enables rapid adaptation to individual samples,
improving predictive accuracy across diverse tumor subtypes while uncovering
context dependent patterns of gene influence that may inform personalized
therapy.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [699] [Teaching Introduction to Programming in the times of AI: A case study of a course re-design](https://arxiv.org/abs/2508.06572)
*Nikolaos Avouris,Kyriakos Sgarbas,George Caridakis,Christos Sintoris*

Main category: cs.CY

TL;DR: The paper reviews current AI tools in programming education, focusing on challenges and proposing course redesign strategies.


<details>
  <summary>Details</summary>
Motivation: To address the integration of AI tools in programming education and the associated challenges.

Method: Provides a review of AI tools and discusses redesigning courses, assignments, and pedagogy.

Result: Offers a guideline for educators and institutions to effectively use AI in programming courses.

Conclusion: AI tools in education require thoughtful course redesign to maximize benefits and tackle challenges.

Abstract: The integration of AI tools into programming education has become
increasingly prevalent in recent years, transforming the way programming is
taught and learned. This paper provides a review of the state-of-the-art AI
tools available for teaching and learning programming, particularly in the
context of introductory courses. It highlights the challenges on course design,
learning objectives, course delivery and formative and summative assessment, as
well as the misuse of such tools by the students. We discuss ways of
re-designing an existing course, re-shaping assignments and pedagogy to address
the current AI technologies challenges. This example can serve as a guideline
for policies for institutions and teachers involved in teaching programming,
aiming to maximize the benefits of AI tools while addressing the associated
challenges and concerns.

</details>


### [700] [Leveraging LLMs for Privacy-Aware Predictions in Participatory Budgeting](https://arxiv.org/abs/2508.06577)
*Juan Zambrano,Clément Contet,Jairo Gudiño,Felipe Garrido-Lucero,Umberto Grandi,Cesar A Hidalgo*

Main category: cs.CY

TL;DR: The paper focuses on enhancing Participatory Budgeting (PB) by utilizing AI to predict the success of project proposals using anonymous data, aiming to improve transparency and participation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issues of low participation rates and lack of visibility in PB initiatives, which affects their democratic legitimacy.

Method: The authors introduce a privacy-preserving AI model that predicts the likelihood of funding PB proposals using textual descriptions and anonymous historical voting records, without using personally identifiable data.

Result: The study demonstrates that GPT-4 Turbo can successfully forecast proposal outcomes when combined with historical voting data, though its initial predictions need additional context to align with real-world behavior.

Conclusion: AI-driven tools like this can enhance PB processes by making them more transparent, efficient, and engaging for citizens.

Abstract: Participatory Budgeting (PB) empowers citizens to propose and vote on public
investment projects. Yet, despite its democratic potential, PB initiatives
often suffer from low participation rates, limiting their visibility and
perceived legitimacy. In this work, we aim to strengthen PB elections in two
key ways: by supporting project proposers in crafting better proposals, and by
helping PB organizers manage large volumes of submissions in a transparent
manner. We propose a privacy-preserving approach to predict which PB proposals
are likely to be funded, using only their textual descriptions and anonymous
historical voting records -- without relying on voter demographics or
personally identifiable information. We evaluate the performance of GPT 4 Turbo
in forecasting proposal outcomes across varying contextual scenarios, observing
that the LLM's prior knowledge needs to be complemented by past voting data to
obtain predictions reflecting real-world PB voting behavior. Our findings
highlight the potential of AI-driven tools to support PB processes by improving
transparency, planning efficiency, and civic engagement.

</details>


### [701] [Towards Integrated Alignment](https://arxiv.org/abs/2508.06592)
*Ben Y. Reis,William La Cava*

Main category: cs.CY

TL;DR: The paper proposes Integrated Alignment frameworks to address AI model alignment challenges by combining diverse approaches and emphasizing collaboration within AI alignment research.


<details>
  <summary>Details</summary>
Motivation: The AI alignment field faces challenges due to fragmentation between behavioral and representational approaches, resulting in models susceptible to deceptive misalignment.

Method: The authors draw lessons from immunology and cybersecurity to design principles for Integrated Alignment frameworks combining strength through deep integration and adaptive coevolution.

Result: Strategic diversity is emphasized to deploy orthogonal alignment and misalignment detection approaches, avoiding homogeneous pipelines. Recommendations are made for collaborative efforts in the AI alignment community.

Conclusion: Integrated frameworks and collective efforts can address alignment gaps in AI models, incorporating diverse approaches and fostering community collaboration.

Abstract: As AI adoption expands across human society, the problem of aligning AI
models to match human preferences remains a grand challenge. Currently, the AI
alignment field is deeply divided between behavioral and representational
approaches, resulting in narrowly aligned models that are more vulnerable to
increasingly deceptive misalignment threats. In the face of this fragmentation,
we propose an integrated vision for the future of the field. Drawing on related
lessons from immunology and cybersecurity, we lay out a set of design
principles for the development of Integrated Alignment frameworks that combine
the complementary strengths of diverse alignment approaches through deep
integration and adaptive coevolution. We highlight the importance of strategic
diversity - deploying orthogonal alignment and misalignment detection
approaches to avoid homogeneous pipelines that may be "doomed to success". We
also recommend steps for greater unification of the AI alignment research field
itself, through cross-collaboration, open model weights and shared community
resources.

</details>


### [702] [Towards Experience-Centered AI: A Framework for Integrating Lived Experience in Design and Development](https://arxiv.org/abs/2508.06849)
*Sanjana Gautam,Mohit Chandra,Ankolika De,Tatiana Chakravorti,Girik Malik,Munmun De Choudhury*

Main category: cs.CY

TL;DR: This paper introduces a framework to integrate lived experiences into AI system design and evaluation, focusing on education, healthcare, and cultural alignment, emphasizing empathy, context-awareness, and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: Current approaches in AI development fall short in systematically understanding and embedding lived human experiences into the design and evaluation process of AI systems.

Method: The authors synthesized interdisciplinary literature on lived experience, human-centered design, and human-AI interaction; developed a targeted taxonomy of lived experiences applicable to AI; and applied the framework to education, healthcare, and cultural alignment domains.

Result: The resulting framework demonstrates how lived experience shapes user goals, expectations, and ethical considerations, while highlighting challenges like responsibility allocation and mental model calibration in human-AI collaborations.

Conclusion: The study provides actionable strategies for creating experience-centered AI systems that align with human realities, encouraging further research to combine technical development with lived experiences.

Abstract: Lived experiences fundamentally shape how individuals interact with AI
systems, influencing perceptions of safety, trust, and usability. While prior
research has focused on developing techniques to emulate human preferences, and
proposed taxonomies to categorize risks (such as psychological harms and
algorithmic biases), these efforts have provided limited systematic
understanding of lived human experiences or actionable strategies for embedding
them meaningfully into the AI development lifecycle. This work proposes a
framework for meaningfully integrating lived experience into the design and
evaluation of AI systems. We synthesize interdisciplinary literature across
lived experience philosophy, human-centered design, and human-AI interaction,
arguing that centering lived experience can lead to models that more accurately
reflect the retrospective, emotional, and contextual dimensions of human
cognition. Drawing from a wide body of work across psychology, education,
healthcare, and social policy, we present a targeted taxonomy of lived
experiences with specific applicability to AI systems. To ground our framework,
we examine three application domains (i) education, (ii) healthcare, and (iii)
cultural alignment, illustrating how lived experience informs user goals,
system expectations, and ethical considerations in each context. We further
incorporate insights from AI system operators and human-AI partnerships to
highlight challenges in responsibility allocation, mental model calibration,
and long-term system adaptation. We conclude with actionable recommendations
for developing experience-centered AI systems that are not only technically
robust but also empathetic, context-aware, and aligned with human realities.
This work offers a foundation for future research that bridges technical
development with the lived experiences of those impacted by AI systems.

</details>


### [703] [Making Effective Decisions: Machine Learning and the Ecogame in 1970](https://arxiv.org/abs/2508.07027)
*Catherine Mason*

Main category: cs.CY

TL;DR: The paper analyzes "Ecogame," a 1970 innovative art project which utilized early machine learning and cybernetics to explore a participatory and democratized technological future.


<details>
  <summary>Details</summary>
Motivation: To explore how historical projects like Ecogame envisioned a participatory technological future through the fusion of cybernetics, art, and machine learning.

Method: The paper reviews the historical context, technological approach, and artistic vision applied in Ecogame, focusing on its use of cybernetic principles and early AI techniques.

Result: Ecogame successfully demonstrated how behaviour and adaptation could impact the broader system, providing historical insights into human-centered applications of AI in art.

Conclusion: Ecogame serves as a forerunner to contemporary discussions on AI-driven art, emphasizing the need for AI's integration in human-centered, participatory frameworks.

Abstract: This paper considers Ecogame, an innovative art project of 1970, whose
creators believed in a positive vision of a technological future; an
understanding, posited on cybernetics, of a future that could be participatory
via digital means, and therefore more democratised. Using simulation and early
machine learning techniques over a live network, Ecogame combined the power of
visual art with cybernetic concepts of adaptation, feedback, and control to
propose that behaviour had implications for the total system. It provides an
historical precedent for contemporary AI-driven art about using AI in a more
human-centred way.

</details>


### [704] ["Draw me a curator" Examining the visual stereotyping of a cultural services profession by generative AI](https://arxiv.org/abs/2508.07132)
*Dirk HR Spennemann*

Main category: cs.CY

TL;DR: The paper studies how generative AI (ChatGPT4o) visualizes museum curators and finds significant biases against real-world demographics.


<details>
  <summary>Details</summary>
Motivation: To investigate how generative AI represents museum curators and identify biases in AI-generated imagery compared to real-world demographics.

Method: Analyzed 230 visualizations created by ChatGPT4o for their depictions of museum curators, comparing them with real-world demographic data.

Result: AI-generated images of curators significantly underrepresent women and ethnic minorities, overrepresent young curators, and depict stereotypical features, suggesting biases in the AI dataset.

Conclusion: Generative AI's depictions of museum curators are biased and misaligned with real-world demographics, which could perpetuate inaccuracies if adopted uncritically.

Abstract: Based on 230 visualisations, this paper examines the depiction of museum
curators by the popular generative Artificial Intelligence (AI) model,
ChatGPT4o. While the AI-generated representations do not reiterate popular
stereotypes of curators as nerdy, conservative in dress and stuck in time
rummaging through collections, they contrast sharply with real-world
demographics. AI-generated imagery extremely underrepresents women (3.5% vs 49%
to 72% in reality) and disregards ethnic communities other than Caucasian (0%
vs 18% to 36%). It only over-represents young curators (79% vs approx. 27%) but
also renders curators to resemble yuppie professionals or people featuring in
fashion advertising. Stereotypical attributes are prevalent, with curators
widely depicted as wearing beards and holding clipboards or digital tablets.
The findings highlight biases in the generative AI image creation dataset,
which is poised to shape an inaccurate portrayal of museum professionals if the
images were to be taken uncritically at face value.

</details>


### [705] [Intersectoral Knowledge in AI and Urban Studies: A Framework for Transdisciplinary Research](https://arxiv.org/abs/2508.07507)
*Rashid Mushkani*

Main category: cs.CY

TL;DR: The paper presents a six-dimensional framework for validating and integrating transdisciplinary knowledge in AI and city studies, focusing on diverse epistemic and ontological perspectives.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of validating and integrating knowledge across diverse disciplines, particularly for societal challenges requiring transdisciplinary approaches.

Method: An extensive analysis of highly cited research from 2014 to 2024, classifying research orientations into six dimensions: ontological, epistemological, methodological, teleological, axiological, and valorization.

Result: The findings reveal dominant perspectives such as critical realism, positivism, analytical methods, consequentialism, epistemic values, and social/economic valorization, while also exploring less common stances like idealism and cultural valorization.

Conclusion: The framework assists early career researchers and transdisciplinary teams in reconciling disciplinary differences and fostering socially accountable research outcomes.

Abstract: Transdisciplinary approaches are increasingly essential for addressing grand
societal challenges, particularly in complex domains such as Artificial
Intelligence (AI), urban planning, and social sciences. However, effectively
validating and integrating knowledge across distinct epistemic and ontological
perspectives poses significant difficulties. This article proposes a
six-dimensional framework for assessing and strengthening transdisciplinary
knowledge validity in AI and city studies, based on an extensive analysis of
the most cited research (2014--2024). Specifically, the framework classifies
research orientations according to ontological, epistemological,
methodological, teleological, axiological, and valorization dimensions. Our
findings show a predominance of perspectives aligned with critical realism
(ontological), positivism (epistemological), analytical methods
(methodological), consequentialism (teleological), epistemic values
(axiological), and social/economic valorization. Less common stances, such as
idealism, mixed methods, and cultural valorization, are also examined for their
potential to enrich knowledge production. We highlight how early career
researchers and transdisciplinary teams can leverage this framework to
reconcile divergent disciplinary viewpoints and promote socially accountable
outcomes.

</details>


### [706] [Unequal Uncertainty: Rethinking Algorithmic Interventions for Mitigating Discrimination from AI](https://arxiv.org/abs/2508.07872)
*Holli Sargeant,Mackenzie Jorgensen,Arina Shah,Adrian Weller,Umang Bhatt*

Main category: cs.CY

TL;DR: The paper examines socio-technical and legal challenges associated with uncertainty in AI-assisted decision-making. It analyzes two interventions, selective abstention and selective friction, to mitigate discrimination risks and improve transparency.


<details>
  <summary>Details</summary>
Motivation: To address AI's uncertainty-driven ethical and legal challenges, particularly its potential discriminatory impacts in decision-making processes.

Method: Introducing and comparing two algorithmic interventions: selective abstention (withholding uncertain predictions) and selective friction (delivering warnings with uncertain predictions) in the context of two case studies: consumer credit decisions and content moderation.

Result: Selective abstention can unintentionally exacerbate disparities, but selective friction demonstrates potential for fairer, accountable decision-making.

Conclusion: Selective friction, by preserving transparency and encouraging cautious human judgment, is a promising guardrail for reducing discrimination and ethical risks in AI-assisted decisions.

Abstract: Uncertainty in artificial intelligence (AI) predictions poses urgent legal
and ethical challenges for AI-assisted decision-making. We examine two
algorithmic interventions that act as guardrails for human-AI collaboration:
selective abstention, which withholds high-uncertainty predictions from human
decision-makers, and selective friction, which delivers those predictions
together with salient warnings or disclosures that slow the decision process.
Research has shown that selective abstention based on uncertainty can
inadvertently exacerbate disparities and disadvantage under-represented groups
that disproportionately receive uncertain predictions. In this paper, we
provide the first integrated socio-technical and legal analysis of
uncertainty-based algorithmic interventions. Through two case studies,
AI-assisted consumer credit decisions and AI-assisted content moderation, we
demonstrate how the seemingly neutral use of uncertainty thresholds can trigger
discriminatory impacts. We argue that, although both interventions pose risks
of unlawful discrimination under UK law, selective frictions offer a promising
pathway toward fairer and more accountable AI-assisted decision-making by
preserving transparency and encouraging more cautious human judgment.

</details>


### [707] [Advancing Knowledge Tracing by Exploring Follow-up Performance Trends](https://arxiv.org/abs/2508.08019)
*Hengyu Liu,Yushuai Li,Minghe Yu,Tiancheng Zhang,Ge Yu,Torben Bach Pedersen,Kristian Torp,Christian S. Jensen,Tianyi Li*

Main category: cs.CY

TL;DR: This paper proposes a new method called Forward-Looking Knowledge Tracing (FINER) to enhance prediction accuracy of students' future performance in Intelligent Tutoring Systems (ITS) by addressing correlation conflicts in historical learning sequences.


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Tracing (KT) methods face difficulties in accurately analyzing the relationships between students' historical learning sequences and their future performance due to correlation conflicts.

Method: The proposed method, FINER, extracts Follow-up Performance Trends (FPTs) from historical learning data, uses a similarity-aware attention mechanism to aggregate FPTs by frequency and contextual similarity, and combines these trends with historical sequences for prediction. The method processes data efficiently in linear time.

Result: FINER significantly outperforms ten state-of-the-art KT methods across six real-world datasets, improving prediction accuracy by 8.74% to 84.85%.

Conclusion: FINER effectively addresses challenges in existing KT methods, enhancing the prediction of student performance and advancing ITS capabilities for monitoring and improving human learning processes.

Abstract: Intelligent Tutoring Systems (ITS), such as Massive Open Online Courses,
offer new opportunities for human learning. At the core of such systems,
knowledge tracing (KT) predicts students' future performance by analyzing their
historical learning activities, enabling an accurate evaluation of students'
knowledge states over time. We show that existing KT methods often encounter
correlation conflicts when analyzing the relationships between historical
learning sequences and future performance. To address such conflicts, we
propose to extract so-called Follow-up Performance Trends (FPTs) from
historical ITS data and to incorporate them into KT. We propose a method called
Forward-Looking Knowledge Tracing (FINER) that combines historical learning
sequences with FPTs to enhance student performance prediction accuracy. FINER
constructs learning patterns that facilitate the retrieval of FPTs from
historical ITS data in linear time; FINER includes a novel similarity-aware
attention mechanism that aggregates FPTs based on both frequency and contextual
similarity; and FINER offers means of combining FPTs and historical learning
sequences to enable more accurate prediction of student future performance.
Experiments on six real-world datasets show that FINER can outperform ten
state-of-the-art KT methods, increasing accuracy by 8.74% to 84.85%.

</details>


### [708] [Street-Level AI: Are Large Language Models Ready for Real-World Judgments?](https://arxiv.org/abs/2508.08193)
*Gaurab Pokharel,Shafkat Farabi,Patrick J. Fowler,Sanmay Das*

Main category: cs.CY

TL;DR: The paper examines the ethical implications of using large-scale AI models for decision-making in societal contexts, finding that current LLMs are inconsistent and not yet ready for high-stakes decisions.


<details>
  <summary>Details</summary>
Motivation: To explore how well large language models (LLMs) align with human and societal judgments in prioritizing resources like homelessness allocation.

Method: The study evaluated LLM judgments using real-world data while maintaining strict confidentiality. It compared LLM outputs internally, across different models, and against vulnerability scoring systems.

Result: LLM prioritizations were found to be inconsistent internally, across models, and when compared with scoring systems, although they showed qualitative alignment with lay human judgments in pairwise tests.

Conclusion: Current LLMs lack the reliability required for integration into high-stakes societal decision-making, particularly in resource allocation systems.

Abstract: A surge of recent work explores the ethical and societal implications of
large-scale AI models that make "moral" judgments. Much of this literature
focuses either on alignment with human judgments through various thought
experiments or on the group fairness implications of AI judgments. However, the
most immediate and likely use of AI is to help or fully replace the so-called
street-level bureaucrats, the individuals deciding to allocate scarce social
resources or approve benefits. There is a rich history underlying how
principles of local justice determine how society decides on prioritization
mechanisms in such domains. In this paper, we examine how well LLM judgments
align with human judgments, as well as with socially and politically determined
vulnerability scoring systems currently used in the domain of homelessness
resource allocation. Crucially, we use real data on those needing services
(maintaining strict confidentiality by only using local large models) to
perform our analyses. We find that LLM prioritizations are extremely
inconsistent in several ways: internally on different runs, between different
LLMs, and between LLMs and the vulnerability scoring systems. At the same time,
LLMs demonstrate qualitative consistency with lay human judgments in pairwise
testing. Findings call into question the readiness of current generation AI
systems for naive integration in high-stakes societal decision-making.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [709] [Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection](https://arxiv.org/abs/2508.07201)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: The paper introduces RAGCL, a graph contrastive learning method tailored for detecting rumors on social media. It leverages statistical insights showing that rumor propagation trees (RPTs) often have shallow, wide structures.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve rumor detection by addressing the gap in existing models, which presume that rumor propagation trees primarily have deep structures instead of their observed wide and shallow nature.

Method: They propose the RAGCL method with adaptive view augmentation guided by centrality measures. Key principles for augmentation include exempting root nodes, retaining deep reply nodes, and preserving lower-level nodes in deep sections while using techniques such as node dropping, attribute masking, and edge dropping.

Result: RAGCL consistently outperforms state-of-the-art models on four benchmark datasets, validating its ability to capture wide-structure characteristics of rumor propagation trees.

Conclusion: The study not only enhances rumor detection using graph contrastive learning but also introduces principles and techniques that could be applied to other tree-structured graph tasks.

Abstract: Rumor detection on social media has become increasingly important. Most
existing graph-based models presume rumor propagation trees (RPTs) have deep
structures and learn sequential stance features along branches. However,
through statistical analysis on real-world datasets, we find RPTs exhibit wide
structures, with most nodes being shallow 1-level replies. To focus learning on
intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning
(RAGCL) method with adaptive view augmentation guided by node centralities. We
summarize three principles for RPT augmentation: 1) exempt root nodes, 2)
retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We
employ node dropping, attribute masking and edge dropping with probabilities
from centrality-based importance scores to generate views. A graph contrastive
objective then learns robust rumor representations. Extensive experiments on
four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.
Our work reveals the wide-structure nature of RPTs and contributes an effective
graph contrastive learning approach tailored for rumor detection through
principled adaptive augmentation. The proposed principles and augmentation
techniques can potentially benefit other applications involving tree-structured
graphs.

</details>


### [710] [Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning](https://arxiv.org/abs/2508.07205)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: This paper introduces AD-GSCL, a new rumor detection method leveraging graph-supervised contrastive learning to handle imbalanced and limited labeled social media data.


<details>
  <summary>Details</summary>
Motivation: Imbalanced data distribution in social media, where rumors represent a minority, makes effective rumor detection challenging. Addressing data scarcity and imbalance can improve detection performance.

Method: The authors designed a framework called AD-GSCL, treating unlabeled data heuristically as non-rumors and employing graph contrastive learning. Two large-scale datasets from Weibo and Twitter were constructed and analyzed for this purpose.

Result: AD-GSCL showed superior performance across class-balanced, imbalanced, and few-shot scenarios during experiments.

Conclusion: The study suggests that leveraging anomaly detection and domain-specific insights can significantly enhance rumor detection in real-world imbalanced social media datasets.

Abstract: Current rumor detection methods based on propagation structure learning
predominately treat rumor detection as a class-balanced classification task on
limited labeled data. However, real-world social media data exhibits an
imbalanced distribution with a minority of rumors among massive regular posts.
To address the data scarcity and imbalance issues, we construct two large-scale
conversation datasets from Weibo and Twitter and analyze the domain
distributions. We find obvious differences between rumor and non-rumor
distributions, with non-rumors mostly in entertainment domains while rumors
concentrate in news, indicating the conformity of rumor detection to an anomaly
detection paradigm. Correspondingly, we propose the Anomaly Detection framework
with Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats
unlabeled data as non-rumors and adapts graph contrastive learning for rumor
detection. Extensive experiments demonstrate AD-GSCL's superiority under
class-balanced, imbalanced, and few-shot conditions. Our findings provide
valuable insights for real-world rumor detection featuring imbalanced data
distributions.

</details>


### [711] [Anatomy of a Machine Learning Ecosystem: 2 Million Models on Hugging Face](https://arxiv.org/abs/2508.06811)
*Benjamin Laufer,Hamidah Oderinwale,Jon Kleinberg*

Main category: cs.SI

TL;DR: The paper analyzes 1.86 million models on Hugging Face to study fine-tuning relationships using evolutionary concepts, finding patterns in model evolution and revealing directional trends in licensing, language scope, and model documentation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of empirical analysis regarding the structural patterns and interactions in fine-tuning generative ML and AI models, a key aspect of their development.

Method: The researchers analyzed 1.86 million models on Hugging Face, constructing and studying model family trees using model metadata and model cards through an evolutionary biology framework.

Result: The paper uncovers that fine-tuned models frequently resemble their family more than simple parent/child ties would suggest, with sibling models showing higher similarity. It also identifies trends such as license relaxation, a drift toward English-only models, and reduced model card complexity.

Conclusion: This work highlights how ecological and evolutionary frameworks can deepen the understanding of model fine-tuning dynamics, offering insights into the open ML ecosystem and its unique traits like license drift and documentation changes.

Abstract: Many have observed that the development and deployment of generative machine
learning (ML) and artificial intelligence (AI) models follow a distinctive
pattern in which pre-trained models are adapted and fine-tuned for specific
downstream tasks. However, there is limited empirical work that examines the
structure of these interactions. This paper analyzes 1.86 million models on
Hugging Face, a leading peer production platform for model development. Our
study of model family trees -- networks that connect fine-tuned models to their
base or parent -- reveals sprawling fine-tuning lineages that vary widely in
size and structure. Using an evolutionary biology lens to study ML models, we
use model metadata and model cards to measure the genetic similarity and
mutation of traits over model families. We find that models tend to exhibit a
family resemblance, meaning their genetic markers and traits exhibit more
overlap when they belong to the same model family. However, these similarities
depart in certain ways from standard models of asexual reproduction, because
mutations are fast and directed, such that two `sibling' models tend to exhibit
more similarity than parent/child pairs. Further analysis of the directional
drifts of these mutations reveals qualitative insights about the open machine
learning ecosystem: Licenses counter-intuitively drift from restrictive,
commercial licenses towards permissive or copyleft licenses, often in violation
of upstream license's terms; models evolve from multi-lingual compatibility
towards english-only compatibility; and model cards reduce in length and
standardize by turning, more often, to templates and automatically generated
text. Overall, this work takes a step toward an empirically grounded
understanding of model fine-tuning and suggests that ecological models and
methods can yield novel scientific insights.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [712] [Channel Charting in Smart Radio Environments](https://arxiv.org/abs/2508.07305)
*Mahdi Maleki,Reza Agahzadeh Ayoubi,Marouan Mizmizi,Umberto Spagnolini*

Main category: eess.SP

TL;DR: The paper proposes using static electromagnetic skins (EMSs) to improve device localization in urban environments, reducing localization errors significantly through optimized EMS configurations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of robust device localization under None-Line-of-Sight (NLoS) conditions in urban environments.

Method: The authors develop an optimization framework for designing EMS phase profiles, using a codebook-based approach to enhance channel dissimilarity and spatial fingerprinting, validated through 3D ray-traced city simulations.

Result: The optimized EMS configurations reduce the 90th-percentile localization error from over 60 m (no EMS) to less than 25 m, while also improving trustworthiness and continuity.

Conclusion: This work establishes the feasibility of using static EMS in Smart Radio Environments for significant improvements in channel charting and localization in NLoS scenarios.

Abstract: This paper introduces the use of static electromagnetic skins (EMSs) to
enable robust device localization via channel charting (CC) in realistic urban
environments. We develop a rigorous optimization framework that leverages EMS
to enhance channel dissimilarity and spatial fingerprinting, formulating EMS
phase profile design as a codebook-based problem targeting the upper quantiles
of key embedding metric, localization error, trustworthiness, and continuity.
Through 3D ray-traced simulations of a representative city scenario, we
demonstrate that optimized EMS configurations, in addition to significant
improvement of the average positioning error, reduce the 90th-percentile
localization error from over 60 m (no EMS) to less than 25 m, while drastically
improving trustworthiness and continuity. To the best of our knowledge, this is
the first work to exploit Smart Radio Environment (SRE) with static EMS for
enhancing CC, achieving substantial gains in localization performance under
challenging None-Line-of-Sight (NLoS) conditions.

</details>


### [713] [Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers](https://arxiv.org/abs/2508.08206)
*Amirhossein Taherpour,Abbas Taherpour,Tamer Khattab*

Main category: eess.SP

TL;DR: This paper proposes a joint learning framework combining Byzantine-resilient spectrum sensing with secure IRS-assisted communications under CSI uncertainty, offering strong empirical performance in adversarial and diverse network conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of securing opportunistic communication under Channel State Information (CSI) uncertainty and mitigating the impacts of Byzantine adversarial attacks.

Method: The authors introduce a joint framework combining logit-domain Bayesian updates with optimization strategies, offering methods for both partial/known and unknown CSI through augmented Lagrangian algorithms and constrained Bayesian optimization.

Result: Simulation results demonstrate improved detection probability, reduced mean-squared error for honest users, suppression of eavesdropper signal power, and fast convergence across diverse network conditions.

Conclusion: The framework successfully facilitates secure opportunistic communication adaptable to CSI availability, effectively coordinating sensing and transmission through integrated learning approaches.

Abstract: We propose a joint learning framework for Byzantine-resilient spectrum
sensing and secure intelligent reflecting surface (IRS)--assisted opportunistic
access under channel state information (CSI) uncertainty. The sensing stage
performs logit-domain Bayesian updates with trimmed aggregation and
attention-weighted consensus, and the base station (BS) fuses network beliefs
with a conservative minimum rule, preserving detection accuracy under a bounded
number of Byzantine users. Conditioned on the sensing outcome, we pose downlink
design as sum mean-squared error (MSE) minimization under transmit-power and
signal-leakage constraints and jointly optimize the BS precoder, IRS phase
shifts, and user equalizers. With partial (or known) CSI, we develop an
augmented-Lagrangian alternating algorithm with projected updates and provide
provable sublinear convergence, with accelerated rates under mild local
curvature. With unknown CSI, we perform constrained Bayesian optimization (BO)
in a geometry-aware low-dimensional latent space using Gaussian process (GP)
surrogates; we prove regret bounds for a constrained upper confidence bound
(UCB) variant of the BO module, and demonstrate strong empirical performance of
the implemented procedure. Simulations across diverse network conditions show
higher detection probability at fixed false-alarm rate under adversarial
attacks, large reductions in sum MSE for honest users, strong suppression of
eavesdropper signal power, and fast convergence. The framework offers a
practical path to secure opportunistic communication that adapts to CSI
availability while coherently coordinating sensing and transmission through
joint learning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [714] [Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification](https://arxiv.org/abs/2508.06535)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: This paper uses transfer learning with pretrained CNNs and efficient data augmentation to improve classification accuracy of Acute Lymphoblastic Leukemia (ALL) images, achieving impressive results using EfficientNet-B3.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve early diagnosis and treatment planning for Acute Lymphoblastic Leukemia through better classification methods using blood smear images.

Method: Transfer learning was applied using pretrained CNNs like ResNet and EfficientNet variants, coupled with data augmentation to balance the dataset before training.

Result: EfficientNet-B3 achieved the best performance among the evaluated models, with metrics including an F1-score of 94.30%, accuracy of 92.02%, and AUC of 94.79%.

Conclusion: Combining data augmentation with advanced transfer learning architectures, particularly EfficientNet-B3, offers an effective approach for developing diagnostic tools for hematologic malignancies.

Abstract: Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral
blood smear images is essential for early diagnosis and effective treatment
planning. This study investigates the use of transfer learning with pretrained
convolutional neural networks (CNNs) to improve diagnostic performance. To
address the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL
images, we applied extensive data augmentation techniques to create a balanced
training set of 10,000 images per class. We evaluated several models, including
ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3
achieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,
andAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.
Thesefindings demonstrate the effectiveness of combining data augmentation with
advanced transfer learning models, particularly EfficientNet-B3, in developing
accurate and robust diagnostic tools for hematologic malignancy detection.

</details>


### [715] [LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification](https://arxiv.org/abs/2508.06874)
*Shisheng Zhang,Ramtin Gharleghi,Sonit Singh,Daniel Moses,Dona Adikari,Arcot Sowmya,Susann Beier*

Main category: eess.IV

TL;DR: The paper proposes a lightweight method combining anatomical knowledge and rule-based topology constraints for automated coronary artery labelling, overcoming challenges of variability and computational demands.


<details>
  <summary>Details</summary>
Motivation: Coronary artery disease is the leading cause of death worldwide, and current methods in coronary artery analysis via CTCA are labor-intensive, time-consuming, and fail to fully utilize anatomical insights effectively.

Method: The authors introduce an efficient and lightweight technique integrating anatomical knowledge and rule-based topology constraints for coronary artery labelling.

Result: The method achieves state-of-the-art performance on benchmark datasets for automated coronary artery labelling.

Conclusion: This work provides a promising alternative to traditional knowledge-based and deep-learning methods by offering an automated, anatomically-informed solution with reduced computational demands.

Abstract: Coronary artery disease (CAD) remains the leading cause of death globally,
with computed tomography coronary angiography (CTCA) serving as a key
diagnostic tool. However, coronary arterial analysis using CTCA, such as
identifying artery-specific features from computational modelling, is
labour-intensive and time-consuming. Automated anatomical labelling of coronary
arteries offers a potential solution, yet the inherent anatomical variability
of coronary trees presents a significant challenge. Traditional knowledge-based
labelling methods fall short in leveraging data-driven insights, while recent
deep-learning approaches often demand substantial computational resources and
overlook critical clinical knowledge. To address these limitations, we propose
a lightweight method that integrates anatomical knowledge with rule-based
topology constraints for effective coronary artery labelling. Our approach
achieves state-of-the-art performance on benchmark datasets, providing a
promising alternative for automated coronary artery labelling.

</details>


### [716] [Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning](https://arxiv.org/abs/2508.06891)
*Melika Filvantorkaman,Mohsen Piri,Maral Filvan Torkaman,Ashkan Zabihi,Hamidreza Moradi*

Main category: eess.IV

TL;DR: The paper introduces an ensemble deep learning framework combining MobileNetV2 and DenseNet121 to classify brain tumors from MRI images with high accuracy (91.7%). It integrates interpretability via Grad-CAM++ and Clinical Decision Rule Overlays.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate, interpretable, and clinically trustable system for classifying brain tumors from MRI scans to aid in diagnosis and treatment planning.

Method: The paper employs an ensemble-based framework using MobileNetV2 and DenseNet121 CNNs, combined via soft voting. It includes an Explainable AI module (Grad-CAM++) for saliency visualization and Clinical Decision Rule Overlay for mapping predictions to radiological rules.

Result: The ensemble classifier achieved 91.7% accuracy with strong spatial alignment between model attention and expert annotations (Dice coefficient up to 0.88). Clinical rule activation further validated the model, and radiologists found explanations useful (Likert scale mean = 4.4).

Conclusion: The framework offers a robust and interpretable solution for automated brain tumor classification, enhancing the reliability and applicability of deep learning in clinical neurodiagnostics.

Abstract: Accurate and interpretable classification of brain tumors from magnetic
resonance imaging (MRI) is critical for effective diagnosis and treatment
planning. This study presents an ensemble-based deep learning framework that
combines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using
a soft voting strategy to classify three common brain tumor types: glioma,
meningioma, and pituitary adenoma. The models were trained and evaluated on the
Figshare dataset using a stratified 5-fold cross-validation protocol. To
enhance transparency and clinical trust, the framework integrates an
Explainable AI (XAI) module employing Grad-CAM++ for class-specific saliency
visualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that
maps predictions to established radiological heuristics. The ensemble
classifier achieved superior performance compared to individual CNNs, with an
accuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.
Grad-CAM++ visualizations revealed strong spatial alignment between model
attention and expert-annotated tumor regions, supported by Dice coefficients up
to 0.88 and IoU scores up to 0.78. Clinical rule activation further validated
model predictions in cases with distinct morphological features. A
human-centered interpretability assessment involving five board-certified
radiologists yielded high Likert-scale scores for both explanation usefulness
(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the
framework's clinical relevance. Overall, the proposed approach offers a robust,
interpretable, and generalizable solution for automated brain tumor
classification, advancing the integration of deep learning into clinical
neurodiagnostics.

</details>


### [717] [Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments](https://arxiv.org/abs/2508.07006)
*Gian Mario Favero,Ge Ya Luo,Nima Fathi,Justin Szeto,Douglas L. Arnold,Brennan Nichyporuk,Chris Pal,Tal Arbel*

Main category: eess.IV

TL;DR: The paper introduces a diffusion model capable of predicting lesion evolution for Multiple Sclerosis patients based on MRI and treatment data, showing high applicability for clinical tasks like lesion forecasting.


<details>
  <summary>Details</summary>
Motivation: To address the heterogeneous progression of diseases like MS and utilize advanced models to improve personalized medicine.

Method: Development of a spatio-temporal diffusion model using multi-modal MRI data and treatment information for lesion evolution prediction.

Result: Accurate NET2 lesion mask predictions demonstrated on data from 2131 patients across six treatments, showcasing potential for clinical applications.

Conclusion: The proposed model exemplifies the value of causal image-based generative tools in advancing prognostics for complex diseases like MS.

Abstract: Image-based personalized medicine has the potential to transform healthcare,
particularly for diseases that exhibit heterogeneous progression such as
Multiple Sclerosis (MS). In this work, we introduce the first treatment-aware
spatio-temporal diffusion model that is able to generate future masks
demonstrating lesion evolution in MS. Our voxel-space approach incorporates
multi-modal patient data, including MRI and treatment information, to forecast
new and enlarging T2 (NET2) lesion masks at a future time point. Extensive
experiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized
clinical trials for relapsing-remitting MS demonstrate that our generative
model is able to accurately predict NET2 lesion masks for patients across six
different treatments. Moreover, we demonstrate our model has the potential for
real-world clinical applications through downstream tasks such as future lesion
count and location estimation, binary lesion activity classification, and
generating counterfactual future NET2 masks for several treatments with
different efficacies. This work highlights the potential of causal, image-based
generative models as powerful tools for advancing data-driven prognostics in
MS.

</details>


### [718] [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](https://arxiv.org/abs/2508.07031)
*Anindya Bijoy Das,Shahnewaz Karim Sakib,Shibbir Ahmed*

Main category: eess.IV

TL;DR: The paper explores hallucinations in large language models (LLMs) applied to medical imaging tasks, focusing on errors in image interpretation and synthetic image generation.


<details>
  <summary>Details</summary>
Motivation: Understand and mitigate hallucinations in LLMs for improving clinical reliability in medical imaging tasks.

Method: Analyze hallucinations in image-to-text and text-to-image directions using expert-informed criteria across medical imaging modalities.

Result: Common patterns of hallucinations identified, along with contributing factors such as model architecture and training data.

Conclusion: This study offers insights to enhance the safety and trustworthiness of LLMs in medical imaging systems.

Abstract: Large Language Models (LLMs) are increasingly applied to medical imaging
tasks, including image interpretation and synthetic image generation. However,
these models often produce hallucinations, which are confident but incorrect
outputs that can mislead clinical decisions. This study examines hallucinations
in two directions: image to text, where LLMs generate reports from X-ray, CT,
or MRI scans, and text to image, where models create medical images from
clinical prompts. We analyze errors such as factual inconsistencies and
anatomical inaccuracies, evaluating outputs using expert informed criteria
across imaging modalities. Our findings reveal common patterns of hallucination
in both interpretive and generative tasks, with implications for clinical
reliability. We also discuss factors contributing to these failures, including
model architecture and training data. By systematically studying both image
understanding and generation, this work provides insights into improving the
safety and trustworthiness of LLM driven medical imaging systems.

</details>


### [719] [3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression](https://arxiv.org/abs/2508.07038)
*Yuke Xing,William Gordon,Qi Yang,Kaifa Yang,Jiarui Wang,Yiling Xu*

Main category: eess.IV

TL;DR: The paper introduces 3DGS-VBench, a large-scale Video Quality Assessment dataset and benchmark made to systematically evaluate distortions in compressed 3D Gaussian Splatting models.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting methods struggle with high storage demands despite their visual fidelity and lack comprehensive studies on distortions caused by compression.

Method: They developed 3DGS-VBench containing 660 models and videos from different scenes under 6 compression algorithms, annotated by 50 participants, and validated dataset reliability.

Result: Benchmarking was performed for 6 compression algorithms on storage and quality, alongside evaluation of 15 assessment metrics.

Conclusion: 3DGS-VBench advances research in compression and quality assessment by providing specialized datasets and fostering new VQA model developments.

Abstract: 3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high
visual fidelity, but its substantial storage requirements hinder practical
deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate
compression modules. However, these 3DGS generative compression techniques
introduce unique distortions lacking systematic quality assessment research. To
this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment
(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences
generated from 11 scenes across 6 SOTA 3DGS compression algorithms with
systematically designed parameter levels. With annotations from 50
participants, we obtained MOS scores with outlier removal and validated dataset
reliability. We benchmark 6 3DGS compression algorithms on storage efficiency
and visual quality, and evaluate 15 quality assessment metrics across multiple
paradigms. Our work enables specialized VQA model training for 3DGS, serving as
a catalyst for compression and quality assessment research. The dataset is
available at https://github.com/YukeXing/3DGS-VBench.

</details>


### [720] [SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging](https://arxiv.org/abs/2508.07041)
*Junkai Liu,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: eess.IV

TL;DR: This paper introduces SAGCNet, an advanced method for imputing missing slices in 3D volumetric MRI data.


<details>
  <summary>Details</summary>
Motivation: Missing or unusable slices in volumetric MRI data hinder accurate clinical diagnoses and demand improved synthesis methods.

Method: The SAGCNet model utilizes a graph-based approach and spatial adapters to capture inter-slice relationships and 3D spatial information.

Result: Experiments demonstrate SAGCNet outperforms state-of-the-art methods in synthesizing absent slices of cardiac MRI and shows robustness with limited data.

Conclusion: SAGCNet effectively addresses challenges in volumetric MRI synthesis, offering valuable improvements for clinical applications in imputation tasks.

Abstract: Magnetic resonance imaging (MRI) provides detailed soft-tissue
characteristics that assist in disease diagnosis and screening. However, the
accuracy of clinical practice is often hindered by missing or unusable slices
due to various factors. Volumetric MRI synthesis methods have been developed to
address this issue by imputing missing slices from available ones. The inherent
3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR),
poses significant challenges for missing slice imputation approaches, including
(1) the difficulty of modeling local inter-slice correlations and dependencies
of volumetric slices, and (2) the limited exploration of crucial 3D spatial
information and global context. In this study, to mitigate these issues, we
present Spatial-Aware Graph Completion Network (SAGCNet) to overcome the
dependency on complete volumetric data, featuring two main innovations: (1) a
volumetric slice graph completion module that incorporates the inter-slice
relationships into a graph structure, and (2) a volumetric spatial adapter
component that enables our model to effectively capture and utilize various
forms of 3D spatial context. Extensive experiments on cardiac MRI datasets
demonstrate that SAGCNet is capable of synthesizing absent CMR slices,
outperforming competitive state-of-the-art MRI synthesis methods both
quantitatively and qualitatively. Notably, our model maintains superior
performance even with limited slice data.

</details>


### [721] [Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications](https://arxiv.org/abs/2508.07165)
*Zelin Qiu,Xi Wang,Zhuoyao Xie,Juan Zhou,Yu Wang,Lingjie Yang,Xinrui Jiang,Juyoung Bae,Moo Hyun Son,Qiang Ye,Dexuan Chen,Rui Zhang,Tao Li,Neeraj Ramesh Mahboobani,Varut Vardhanabhuti,Xiaohui Duan,Yinghua Zhao,Hao Chen*

Main category: eess.IV

TL;DR: The paper introduces PRISM, a foundation model pre-trained on the largest multi-sequence MRI dataset to date, designed to address the challenges of generalization in MRI analysis.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization capability of deep learning models in multi-sequence MRI analysis, which is critical for robust clinical applications.

Method: The authors constructed a large-scale MRI pretraining corpus with 336,476 volumetric scans and proposed a pretraining paradigm to separate anatomically invariant features from sequence-specific variations while preserving semantic representations. PRISM was benchmarked on 44 tasks using diverse datasets.

Result: PRISM achieved first-rank results in 39 out of 44 downstream evaluations, consistently outperforming both non-pretrained and existing models.

Conclusion: PRISM demonstrates robust and generalizable performance for multi-sequence MRI analysis, enhancing its translational potential and clinical applicability.

Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable
versatility, enabling the distinct visualization of different tissue types.
Nevertheless, the inherent heterogeneity among MRI sequences poses significant
challenges to the generalization capability of deep learning models. These
challenges undermine model performance when faced with varying acquisition
parameters, thereby severely restricting their clinical utility. In this study,
we present PRISM, a foundation model PRe-trained with large-scale
multI-Sequence MRI. We collected a total of 64 datasets from both public and
private sources, encompassing a wide range of whole-body anatomical structures,
with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI
scans from 34 datasets (8 public and 26 private) were curated to construct the
largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a
novel pretraining paradigm that disentangles anatomically invariant features
from sequence-specific variations in MRI, while preserving high-level semantic
representations. We established a benchmark comprising 44 downstream tasks,
including disease diagnosis, image segmentation, registration, progression
prediction, and report generation. These tasks were evaluated on 32 public
datasets and 5 private cohorts. PRISM consistently outperformed both
non-pretrained models and existing foundation models, achieving first-rank
results in 39 out of 44 downstream benchmarks with statistical significance
improvements. These results underscore its ability to learn robust and
generalizable representations across unseen data acquired under diverse MRI
protocols. PRISM provides a scalable framework for multi-sequence MRI analysis,
thereby enhancing the translational potential of AI in radiology. It delivers
consistent performance across diverse imaging protocols, reinforcing its
clinical applicability.

</details>


### [722] [HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation](https://arxiv.org/abs/2508.07225)
*Xuepeng Liu,Zheng Jiang,Pinan Zhu,Hanyu Liu,Chao Li*

Main category: eess.IV

TL;DR: The paper introduces HaDM-ST, a framework for improving high-resolution spatial transcriptomics predictions using H&E images and low-resolution data, addressing key challenges in this field.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in spatial transcriptomics resolution and address challenges in isolating relevant features, precise multimodal alignment, and gene-specific variation modeling.

Method: The framework, HaDM-ST, involves three core components: a semantic distillation network for extracting predictive histology features, a spatial alignment module for achieving precise pixel-level alignment, and a channel-aware adversarial learner for gene-specific variation modeling.

Result: HaDM-ST outperforms previous methods in tests on 200 genes across diverse tissue samples and species, showing better spatial fidelity and gene-level coherence.

Conclusion: HaDM-ST effectively improves the resolution of spatial transcriptomics, offering a more precise and coherent high-resolution prediction method that addresses prior challenges.

Abstract: Spatial transcriptomics (ST) reveals spatial heterogeneity of gene
expression, yet its resolution is limited by current platforms. Recent methods
enhance resolution via H&E-stained histology, but three major challenges
persist: (1) isolating expression-relevant features from visually complex H&E
images; (2) achieving spatially precise multimodal alignment in diffusion-based
frameworks; and (3) modeling gene-specific variation across expression
channels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST
Generation), a high-resolution ST generation framework conditioned on H&E
images and low-resolution ST. HaDM-ST includes: (i) a semantic distillation
network to extract predictive cues from H&E; (ii) a spatial alignment module
enforcing pixel-wise correspondence with low-resolution ST; and (iii) a
channel-aware adversarial learner for fine-grained gene-level modeling.
Experiments on 200 genes across diverse tissues and species show HaDM-ST
consistently outperforms prior methods, enhancing spatial fidelity and
gene-level coherence in high-resolution ST predictions.

</details>


### [723] [DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework](https://arxiv.org/abs/2508.07682)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: eess.IV

TL;DR: The research proposes DiffVC-OSD, a One-Step Diffusion-based Neural Video Compression model optimized for perceptual quality and higher efficiency with reduced complexity.


<details>
  <summary>Details</summary>
Motivation: Current video compression methods employing multi-step diffusion are computationally intensive with room for enhanced perceptual quality.

Method: The approach integrates a One-Step Diffusion Model utilizing a Temporal Context Adapter for conditional inputs, along with End-to-End Finetuning for better performance.

Result: DiffVC-OSD achieves state-of-the-art perceptual compression, 20x faster decoding, and reduces bitrate by 86.92% compared to multi-step variants.

Conclusion: DiffVC-OSD offers a superior alternative for efficient and perceptually optimized video compression, reducing computational overhead and improving quality.

Abstract: In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based
Perceptual Neural Video Compression framework. Unlike conventional multi-step
diffusion-based methods, DiffVC-OSD feeds the reconstructed latent
representation directly into a One-Step Diffusion Model, enhancing perceptual
quality through a single diffusion step guided by both temporal context and the
latent itself. To better leverage temporal dependencies, we design a Temporal
Context Adapter that encodes conditional inputs into multi-level features,
offering more fine-grained guidance for the Denoising Unet. Additionally, we
employ an End-to-End Finetuning strategy to improve overall compression
performance. Extensive experiments demonstrate that DiffVC-OSD achieves
state-of-the-art perceptual compression performance, offers about 20$\times$
faster decoding and a 86.92\% bitrate reduction compared to the corresponding
multi-step diffusion-based variant.

</details>


### [724] [Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning](https://arxiv.org/abs/2508.07788)
*Runze Wang,Zeli Chen,Zhiyun Song,Wei Fang,Jiajin Zhang,Danyang Tu,Yuxing Tang,Minfeng Xu,Xianghua Ye,Le Lu,Dakai Jin*

Main category: eess.IV

TL;DR: ALDEN introduces an anatomy-aware approach to improve LDCT denoising by leveraging features from pretrained vision models alongside adversarial and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Previous LDCT denoising methods often neglect the anatomical semantics of human tissue, leading to suboptimal outcomes.

Method: ALDEN incorporates semantic features from pretrained vision models into its anatomy-aware discriminator and employs contrastive learning to ensure anatomical consistency.

Result: ALDEN outperformed existing methods in experiments on two LDCT datasets, reducing issues like over-smoothing and preserving anatomical detail. It also showed superior performance in a multi-organ segmentation task.

Conclusion: The proposed approach establishes a novel benchmark in LDCT denoising, offering both improved diagnostic efficacy and anatomical preservation.

Abstract: To reduce radiation exposure and improve the diagnostic efficacy of low-dose
computed tomography (LDCT), numerous deep learning-based denoising methods have
been developed to mitigate noise and artifacts. However, most of these
approaches ignore the anatomical semantics of human tissues, which may
potentially result in suboptimal denoising outcomes. To address this problem,
we propose ALDEN, an anatomy-aware LDCT denoising method that integrates
semantic features of pretrained vision models (PVMs) with adversarial and
contrastive learning. Specifically, we introduce an anatomy-aware discriminator
that dynamically fuses hierarchical semantic features from reference
normal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific
realism evaluation in the discriminator. In addition, we propose a
semantic-guided contrastive learning module that enforces anatomical
consistency by contrasting PVM-derived features from LDCT, denoised CT and
NDCT, preserving tissue-specific patterns through positive pairs and
suppressing artifacts via dual negative pairs. Extensive experiments conducted
on two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art
performance, offering superior anatomy preservation and substantially reducing
over-smoothing issue of previous work. Further validation on a downstream
multi-organ segmentation task (encompassing 117 anatomical structures) affirms
the model's ability to maintain anatomical awareness.

</details>


### [725] [Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images](https://arxiv.org/abs/2508.07875)
*Shuo Han,Ahmed Karam Eldaly,Solomon Sunday Oyelere*

Main category: eess.IV

TL;DR: This paper introduces a human-in-the-loop (HITL) deep learning system for improving the accuracy of Invasive Ductal Carcinoma (IDC) diagnosis using histopathology images, enhancing AI precision through iterative collaboration with medical experts.


<details>
  <summary>Details</summary>
Motivation: To improve early and accurate detection of IDC, leveraging both human expertise and artificial intelligence for better patient outcomes.

Method: A HITL system integrates a high-performing EfficientNetV2S model with feedback from medical experts. Experts correct misclassifications, updating the training dataset iteratively to refine the model's performance.

Result: The EfficientNetV2S model achieves a state-of-the-art accuracy of 93.65%, and incorporating the HITL approach further improves diagnostic performance using experimental groups.

Conclusion: The HITL approach effectively enhances AI diagnostic accuracy, providing a promising framework for future AI-assisted medical diagnostics and showing the value of human-AI collaboration.

Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,
and early, accurate diagnosis is critical to improving patient survival rates
by guiding treatment decisions. Combining medical expertise with artificial
intelligence (AI) holds significant promise for enhancing the precision and
efficiency of IDC detection. In this work, we propose a human-in-the-loop
(HITL) deep learning system designed to detect IDC in histopathology images.
The system begins with an initial diagnosis provided by a high-performance
EfficientNetV2S model, offering feedback from AI to the human expert. Medical
professionals then review the AI-generated results, correct any misclassified
images, and integrate the revised labels into the training dataset, forming a
feedback loop from the human back to the AI. This iterative process refines the
model's performance over time. The EfficientNetV2S model itself achieves
state-of-the-art performance compared to existing methods in the literature,
with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system
further improves the model's accuracy using four experimental groups with
misclassified images. These results demonstrate the potential of this
collaborative approach to enhance AI performance in diagnostic systems. This
work contributes to advancing automated, efficient, and highly accurate methods
for IDC detection through human-AI collaboration, offering a promising
direction for future AI-assisted medical diagnostics.

</details>


### [726] [Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models](https://arxiv.org/abs/2508.07903)
*Johanna P. Müller,Anika Knupfer,Pedro Blöss,Edoardo Berardi Vittur,Bernhard Kainz,Jana Hutter*

Main category: eess.IV

TL;DR: This paper introduces a novel framework for synthesizing high-quality uterine MRI images using diffusion models, addressing privacy and data scarcity challenges.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing diffusion models in generating anatomically precise pelvic images, particularly for gynaecological applications where privacy and data scarcity are critical.

Method: Developing a framework combining unconditional and conditioned DDPMs and LDMs in both 2D and 3D to produce anatomically coherent synthetic uterine MRI images.

Result: The synthesized images closely mimic real scans, improve diagnostic accuracy in classification tasks, and are validated by blinded expert evaluation for clinical realism.

Conclusion: The proposed methodology advances the creation of anatomically accurate synthetic pelvic images, which enhance diagnostic models and support equitable AI-driven research in gynaecology.

Abstract: Despite significant progress in generative modelling, existing diffusion
models often struggle to produce anatomically precise female pelvic images,
limiting their application in gynaecological imaging, where data scarcity and
patient privacy concerns are critical. To overcome these barriers, we introduce
a novel diffusion-based framework for uterine MRI synthesis, integrating both
unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)
and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates
anatomically coherent, high fidelity synthetic images that closely mimic real
scans and provide valuable resources for training robust diagnostic models. We
evaluate generative quality using advanced perceptual and distributional
metrics, benchmarking against standard reconstruction methods, and demonstrate
substantial gains in diagnostic accuracy on a key classification task. A
blinded expert evaluation further validates the clinical realism of our
synthetic images. We release our models with privacy safeguards and a
comprehensive synthetic uterine MRI dataset to support reproducible research
and advance equitable AI in gynaecology.

</details>


### [727] [A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images](https://arxiv.org/abs/2508.08123)
*Lingjing Chen,Chengxiu Zhang,Yinqiao Yi,Yida Wang,Yang Song,Xu Yan,Shengfang Xu,Dalin Zhu,Mengqiu Cao,Yan Zhou,Chenglong Wang,Guang Yang*

Main category: eess.IV

TL;DR: The paper presents a deep learning approach incorporating MRI physics to improve accuracy in synthesizing quantitative MRI maps.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and generalizability of quantitative MRI synthesis from clinical weighted MRI images.

Method: A physics-driven neural network that embeds MRI sequence parameters (TR, TE, TI) into the model via parameter embedding to synthesize T1, T2, and PD quantitative maps from traditional T1/ T2-weighted and T2-FLAIR images.

Result: The model surpasses conventional approaches with PSNR >34 dB and SSIM >0.92, demonstrating superior robustness and performance even on unseen pathological regions.

Conclusion: Embedding MRI sequence parameters into the model enhances quantitative MRI synthesis, offering potential for improved clinical application and accelerated qMRI.

Abstract: We propose a deep learning-based approach that integrates MRI sequence
parameters to improve the accuracy and generalizability of quantitative image
synthesis from clinical weighted MRI. Our physics-driven neural network embeds
MRI sequence parameters -- repetition time (TR), echo time (TE), and inversion
time (TI) -- directly into the model via parameter embedding, enabling the
network to learn the underlying physical principles of MRI signal formation.
The model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as
input and synthesizes T1, T2, and proton density (PD) quantitative maps.
Trained on healthy brain MR images, it was evaluated on both internal and
external test datasets. The proposed method achieved high performance with PSNR
values exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter
maps. It outperformed conventional deep learning models in accuracy and
robustness, including data with previously unseen brain structures and lesions.
Notably, our model accurately synthesized quantitative maps for these unseen
pathological regions, highlighting its superior generalization capability.
Incorporating MRI sequence parameters via parameter embedding allows the neural
network to better learn the physical characteristics of MR signals,
significantly enhancing the performance and reliability of quantitative MRI
synthesis. This method shows great potential for accelerating qMRI and
improving its clinical utility.

</details>


### [728] [RedDino: A foundation model for red blood cell analysis](https://arxiv.org/abs/2508.08180)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto,Carsten Marr*

Main category: eess.IV

TL;DR: RedDino is a self-supervised foundation model designed for analyzing red blood cell (RBC) morphology. Trained on a diverse dataset, it outperforms existing tools in classification tasks and aids in diagnosing hematological disorders.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of comprehensive AI solutions for red blood cell morphological analysis, which is critical for diagnosing hematological disorders.

Method: Developed RedDino using the DINOv2 self-supervised learning framework, adapted specifically for RBCs, and trained the model on a dataset of 1.25 million RBC images from various modalities.

Result: RedDino demonstrated superior performance over existing models in RBC shape classification and proved robust in feature representation and generalization through extensive evaluations.

Conclusion: RedDino advances computational hematology by accurately analyzing nuanced RBC morphology, supporting the development of reliable diagnostic tools. The source code and pretrained models have been made publicly available.

Abstract: Red blood cells (RBCs) are essential to human health, and their precise
morphological analysis is important for diagnosing hematological disorders.
Despite the promise of foundation models in medical diagnostics, comprehensive
AI solutions for RBC analysis remain scarce. We present RedDino, a
self-supervised foundation model designed for RBC image analysis. RedDino uses
an RBC-specific adaptation of the DINOv2 self-supervised learning framework and
is trained on a curated dataset of 1.25 million RBC images from diverse
acquisition modalities and sources. Extensive evaluations show that RedDino
outperforms existing state-of-the-art models on RBC shape classification.
Through assessments including linear probing and nearest neighbor
classification, we confirm its strong feature representations and
generalization ability. Our main contributions are: (1) a foundation model
tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations
for RBC modeling, and (3) a detailed evaluation of generalization performance.
RedDino addresses key challenges in computational hematology by capturing
nuanced morphological features, advancing the development of reliable
diagnostic tools. The source code and pretrained models for RedDino are
available at https://github.com/Snarci/RedDino, and the pretrained models can
be downloaded from our Hugging Face collection at
https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc

</details>


### [729] [PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography](https://arxiv.org/abs/2508.07773)
*Mohammed Salah,Numan Saeed,Davor Svetinovic,Stefano Sfarra,Mohammed Omar,Yusra Abdulrahman*

Main category: eess.IV

TL;DR: The paper proposes a PCA-guided autoencoding framework for structured dimensionality reduction in Active Infrared Thermography (AIRT) data, addressing limitations in existing methods by enforcing a structured latent space.


<details>
  <summary>Details</summary>
Motivation: Current AIRT approaches using non-linear autoencoders produce unstructured latent spaces, which limit their effectiveness in characterizing subsurface defects.

Method: The paper introduces a PCA-guided autoencoding framework with a novel PCA distillation loss to align latent representations with PCA-structured components while preserving non-linear patterns.

Result: The proposed PCA-guided AE framework performs better than existing dimensionality reduction methods on PVC, CFRP, and PLA samples, showing improvements in contrast, SNR, and neural network-based metrics.

Conclusion: The PCA-guided AE framework demonstrates its effectiveness in creating structured latent spaces for AIRT data, enhancing downstream defect characterization in industrial applications.

Abstract: Active Infrared thermography (AIRT) is a widely adopted non-destructive
testing (NDT) technique for detecting subsurface anomalies in industrial
components. Due to the high dimensionality of AIRT data, current approaches
employ non-linear autoencoders (AEs) for dimensionality reduction. However, the
latent space learned by AIRT AEs lacks structure, limiting their effectiveness
in downstream defect characterization tasks. To address this limitation, this
paper proposes a principal component analysis guided (PCA-guided) autoencoding
framework for structured dimensionality reduction to capture intricate,
non-linear features in thermographic signals while enforcing a structured
latent space. A novel loss function, PCA distillation loss, is introduced to
guide AIRT AEs to align the latent representation with structured PCA
components while capturing the intricate, non-linear patterns in thermographic
signals. To evaluate the utility of the learned, structured latent space, we
propose a neural network-based evaluation metric that assesses its suitability
for defect characterization. Experimental results show that the proposed
PCA-guided AE outperforms state-of-the-art dimensionality reduction methods on
PVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR),
and neural network-based metrics.

</details>


### [730] [MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer](https://arxiv.org/abs/2508.07817)
*Tao Tang,Chengxu Yang*

Main category: eess.IV

TL;DR: This paper proposes MI-ND, a medical image denoising model leveraging multi-scale convolutional and Transformer architecture to improve diagnostic accuracy through noise level estimation and adaptive attention.


<details>
  <summary>Details</summary>
Motivation: Address the issue of non-uniform noise interference in medical images caused by factors like low-dose scanning and equipment limitations, which hampers accurate disease diagnosis.

Method: Introduced the MI-ND model with multi-scale convolutional and Transformer architectures, a noise level estimator (NLE), and noise adaptive attention module (NAAB) for noise-driven channel-spatial attention regulation and cross-modal feature fusion.

Result: Experiments on public datasets showed MI-ND significantly outperformed other models on PSNR, SSIM, LPIPS, and diagnostic metrics like F1 score and ROC-AUC, demonstrating robust results.

Conclusion: The model effectively enhances medical images, improves diagnostic sensitivity and structural recovery, and shows strong potential for applications in AI-assisted diagnosis and treatment.

Abstract: The core role of medical images in disease diagnosis makes their quality
directly affect the accuracy of clinical judgment. However, due to factors such
as low-dose scanning, equipment limitations and imaging artifacts, medical
images are often accompanied by non-uniform noise interference, which seriously
affects structure recognition and lesion detection. This paper proposes a
medical image adaptive denoising model (MI-ND) that integrates multi-scale
convolutional and Transformer architecture, introduces a noise level estimator
(NLE) and a noise adaptive attention module (NAAB), and realizes
channel-spatial attention regulation and cross-modal feature fusion driven by
noise perception. Systematic testing is carried out on multimodal public
datasets. Experiments show that this method significantly outperforms the
comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,
and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing
strong prac-tical value and promotional potential. The model has outstanding
benefits in structural recovery, diagnostic sensitivity, and cross-modal
robustness, and provides an effective solution for medical image enhancement
and AI-assisted diagnosis and treatment.

</details>


### [731] [Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping](https://arxiv.org/abs/2508.07760)
*Maximilian Kromer,Panagiotis Agrafiotis,Begüm Demir*

Main category: eess.IV

TL;DR: The paper introduces Sea-Undistort, a synthetic dataset for improving image-based bathymetric mapping in shallow waters, addressing optical distortions through supervised learning.


<details>
  <summary>Details</summary>
Motivation: Bathymetric mapping in shallow waters is difficult due to complex optical distortions like waves, sun glint, and scattering, which hinder accurate seabed imaging.

Method: A synthetic dataset called Sea-Undistort was created with 1200 paired distortion-free and distorted images, along with metadata such as camera and sun parameters. This dataset was used to benchmark and enhance image restoration methods, including an improved diffusion-based framework.

Result: The enhanced diffusion model applied on real aerial data produced better Digital Surface Models by reducing bathymetric errors, suppressing distortions, and preserving fine seabed details.

Conclusion: Sea-Undistort enables supervised training for shallow water bathymetry, producing substantial improvements in seabed mapping accuracy. The dataset and tools are freely available for further research.

Abstract: Accurate image-based bathymetric mapping in shallow waters remains
challenging due to the complex optical distortions such as wave induced
patterns, scattering and sunglint, introduced by the dynamic water surface, the
water column properties, and solar illumination. In this work, we introduce
Sea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512
through-water scenes rendered in Blender. Each pair comprises a distortion-free
and a distorted view, featuring realistic water effects such as sun glint,
waves, and scattering over diverse seabeds. Accompanied by per-image metadata
such as camera parameters, sun position, and average depth, Sea-Undistort
enables supervised training that is otherwise infeasible in real environments.
We use Sea-Undistort to benchmark two state-of-the-art image restoration
methods alongside an enhanced lightweight diffusion-based framework with an
early-fusion sun-glint mask. When applied to real aerial data, the enhanced
diffusion model delivers more complete Digital Surface Models (DSMs) of the
seabed, especially in deeper areas, reduces bathymetric errors, suppresses
glint and scattering, and crisply restores fine seabed details. Dataset,
weights, and code are publicly available at
https://www.magicbathy.eu/Sea-Undistort.html.

</details>


### [732] [Learned Regularization for Microwave Tomography](https://arxiv.org/abs/2508.08114)
*Bowen Tong,Hao Chen,Shaorui Guo,Dong Liu*

Main category: eess.IV

TL;DR: The paper introduces "Single-Step Diffusion Regularization" (SSD-Reg) for microwave tomography (MWT), integrating diffusion models into reconstruction to handle nonlinearity and ill-posedness effectively.


<details>
  <summary>Details</summary>
Motivation: Current MWT reconstruction methods struggle with the nonlinear and ill-posed nature of the problem and lack generalization when utilizing deep learning approaches.

Method: The proposed physics-informed approach embeds diffusion models as regularization into a data-consistent variational scheme, operating without the need for paired training data.

Result: Extensive experiments show that SSD-Reg significantly improves reconstruction accuracy, robustness, and stability while maintaining adherence to both physics and structural priors.

Conclusion: SSD-Reg offers a novel, flexible, and reliable solution for overcoming the inherent challenges of MWT, advancing the accuracy of functional image reconstruction.

Abstract: Microwave Tomography (MWT) aims to reconstruct the dielectric properties of
tissues from measured scattered electromagnetic fields. This inverse problem is
highly nonlinear and ill-posed, posing significant challenges for conventional
optimization-based methods, which, despite being grounded in physical models,
often fail to recover fine structural details. Recent deep learning strategies,
including end-to-end and post-processing networks, have improved reconstruction
quality but typically require large paired training datasets and may struggle
to generalize. To overcome these limitations, we propose a physics-informed
hybrid framework that integrates diffusion models as learned regularization
within a data-consistency-driven variational scheme. Specifically, we introduce
Single-Step Diffusion Regularization (SSD-Reg), a novel approach that embeds
diffusion priors into the iterative reconstruction process, enabling the
recovery of complex anatomical structures without the need for paired data.
SSD-Reg maintains fidelity to both the governing physics and learned structural
distributions, improving accuracy, stability, and robustness. Extensive
experiments demonstrate that SSD-Reg, implemented as a Plug-and-Play (PnP)
module, provides a flexible and effective solution for tackling the
ill-posedness inherent in functional image reconstruction.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [733] [Can Smaller Large Language Models Evaluate Research Quality?](https://arxiv.org/abs/2508.07196)
*Mike Thelwall*

Main category: cs.DL

TL;DR: The article evaluates the capability of a smaller offline LLM (Gemma-3-27b-it) in estimating research quality scores and establishes its effectiveness, albeit weaker than larger models like ChatGPT 4o.


<details>
  <summary>Details</summary>
Motivation: To determine whether smaller, offline Large Language Models can effectively assess research quality, comparable to larger LLMs, and to explore the potential cost and security benefits.

Method: The study analyzes the correlation between Gemma-3-27b-it's scores and an expert proxy of research quality for over 104,000 articles across 34 fields, comparing its performance with larger LLMs.

Result: Gemma-3-27b-it shows positive correlations in research quality evaluation scores for all fields, albeit with lower strength compared to ChatGPT 4o and 4o-mini. Unlike larger LLMs, it does not benefit much from repetitions or stylistic improvements.

Conclusion: Smaller, offline LLMs like Gemma-3-27b-it can effectively estimate research quality scores, making them viable alternatives for cost-saving and offline secure processing needs, even though larger models are more powerful.

Abstract: Although both Google Gemini (1.5 Flash) and ChatGPT (4o and 4o-mini) give
research quality evaluation scores that correlate positively with expert scores
in nearly all fields, and more strongly that citations in most, it is not known
whether this is true for smaller Large Language Models (LLMs). In response,
this article assesses Google's Gemma-3-27b-it, a downloadable LLM (60Gb). The
results for 104,187 articles show that Gemma-3-27b-it scores correlate
positively with an expert research quality score proxy for all 34 Units of
Assessment (broad fields) from the UK Research Excellence Framework 2021. The
Gemma-3-27b-it correlations have 83.8% of the strength of ChatGPT 4o and 94.7%
of the strength of ChatGPT 4o-mini correlations. Differently from the two
larger LLMs, the Gemma-3-27b-it correlations do not increase substantially when
the scores are averaged across five repetitions, its scores tend to be lower,
and its reports are relatively uniform in style. Overall, the results show that
research quality score estimation can be conducted by offline LLMs, so this
capability is not an emergent property of the largest LLMs. Moreover, score
improvement through repetition is not a universal feature of LLMs. In
conclusion, although the largest LLMs still have the highest research
evaluation score estimation capability, smaller ones can also be used for this
task, and this can be helpful for cost saving or when secure offline processing
is needed.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [734] [A Portable Multi-GPU Solver for Collisional Plasmas with Coulombic Interactions](https://arxiv.org/abs/2508.06771)
*James Almgren-Bell,Nader Al Awar,Dilip S Geethakrishnan,Milos Gligoric,George Biros*

Main category: cs.CE

TL;DR: The paper addresses parallel Particle-in-Cell methods for low-temperature plasmas, focusing on GPU acceleration for velocity-space interactions and electron collisions. It compares GPU performance and scalability using PyKokkos.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to enhance simulation tools for kinetic models of low-temperature plasmas, specifically focusing on improving computational efficiency using GPU-based methods.

Method: The authors utilize parallel PIC methods combined with GPU acceleration techniques for velocity-space interactions and evaluate their performance using Python-based HPC tools, particularly PyKokkos.

Result: The MI250X GPU performed slightly faster for most kernels compared to the NVIDIA Volta V100 but exhibited increased sensitivity to register pressure. Distributed memory implementation was tested, scaling up to 16 MPI ranks.

Conclusion: The study demonstrates the viability of Python-based HPC tools like PyKokkos for rapid prototyping in GPU acceleration of PIC methods. Insights into hardware optimization and implementation scalability were provided.

Abstract: We study parallel particle-in-cell (PIC) methods for low-temperature plasmas
(LTPs), which discretize kinetic formulations that capture the time evolution
of the probability density function of particles as a function of position and
velocity. We use a kinetic description for electrons and a fluid approximation
for heavy species. In this paper, we focus on GPU acceleration of algorithms
for velocity-space interactions and in particular, collisions of electrons with
neutrals, ions, and electrons. Our work has two thrusts. The first is
algorithmic exploration and analysis. The second is examining the viability of
rapid-prototyping implementations using Python-based HPC tools, in particular
PyKokkos. We discuss several common PIC kernels and present performance results
on NVIDIA Volta V100 and AMD MI250X GPUs. Overall, the MI250X is slightly
faster for most kernels but shows more sensitivity to register pressure. We
also report scaling results for a distributed memory implementation on up to 16
MPI ranks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [735] [Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution](https://arxiv.org/abs/2508.06584)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.DB

TL;DR: Geospatial databases require efficient matching procedures for diverse geometries, which current neural methods oversimplify. This paper proposes Omni, a model embedding varied geometries, improving resolution accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: Existing geospatial entity resolution methods oversimplify complex geometries, which leads to a loss of spatial detail and accuracy in applications such as geospatial databases.

Method: Omni utilizes an omni-geometry encoder to embed diverse geometries and integrates a transformer-based pre-trained model for attribute-level affinity analysis in entity records. Both point-only datasets and new diverse-geometry datasets were analyzed.

Result: Omni achieves up to 12% improvement in F1 score compared to existing methods. Additionally, experiments show that Large Language Models (LLMs) can also perform competitively in this domain.

Conclusion: Omni effectively addresses the challenges of embedding diverse geometries, improving geospatial entity resolution accuracy. LLMs are promising alternatives for geospatial tasks and merit further exploration.

Abstract: The development, integration, and maintenance of geospatial databases rely
heavily on efficient and accurate matching procedures of Geospatial Entity
Resolution (ER). While resolution of points-of-interest (POIs) has been widely
addressed, resolution of entities with diverse geometries has been largely
overlooked. This is partly due to the lack of a uniform technique for embedding
heterogeneous geometries seamlessly into a neural network framework. Existing
neural approaches simplify complex geometries to a single point, resulting in
significant loss of spatial information. To address this limitation, we propose
Omni, a geospatial ER model featuring an omni-geometry encoder. This encoder is
capable of embedding point, line, polyline, polygon, and multi-polygon
geometries, enabling the model to capture the complex geospatial intricacies of
the places being compared. Furthermore, Omni leverages transformer-based
pre-trained language models over individual textual attributes of place records
in an Attribute Affinity mechanism. The model is rigorously tested on existing
point-only datasets and a new diverse-geometry geospatial ER dataset. Omni
produces up to 12% (F1) improvement over existing methods.
  Furthermore, we test the potential of Large Language Models (LLMs) to conduct
geospatial ER, experimenting with prompting strategies and learning scenarios,
comparing the results of pre-trained language model-based methods with LLMs.
Results indicate that LLMs show competitive results.

</details>


### [736] [SQL-Exchange: Transforming SQL Queries Across Domains](https://arxiv.org/abs/2508.07087)
*Mohammadreza Daviran,Brian Lin,Davood Rafiei*

Main category: cs.DB

TL;DR: The paper introduces SQL-Exchange, a framework for adapting SQL queries across different database schemas to improve downstream text-to-SQL systems' performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance text-to-SQL systems by enabling SQL query mappings across various database schemas, aligning with structural and domain-specific differences to maximize usability and learning performance.

Method: The methodology includes developing SQL-Exchange, which adapts queries structurally and semantically to target schemas, and evaluating its impact through comprehensive experiments involving structural alignment, execution validity, and semantic correctness measures.

Result: The evaluations demonstrate the efficiency and effectiveness of SQL-Exchange across diverse schemas and queries. Additionally, using the adapted queries as examples led to consistent improvements in text-to-SQL task performance.

Conclusion: SQL-Exchange shows potential as a versatile tool for schema mapping, significantly enhancing the performance of downstream text-to-SQL models by leveraging mapped queries for in-context learning.

Abstract: We introduce SQL-Exchange, a framework for mapping SQL queries across
different database schemas by preserving the source query structure while
adapting domain-specific elements to align with the target schema. We
investigate the conditions under which such mappings are feasible and
beneficial, and examine their impact on enhancing the in-context learning
performance of text-to-SQL systems as a downstream task. Our comprehensive
evaluation across multiple model families and benchmark datasets--assessing
structural alignment with source queries, execution validity on target
databases, and semantic correctness--demonstrates that SQL-Exchange is
effective across a wide range of schemas and query types. Our results further
show that using mapped queries as in-context examples consistently improves
text-to-SQL performance over using queries from the source schema.

</details>


### [737] [Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption](https://arxiv.org/abs/2508.07044)
*William Zerong Wang,Dongfang Zhao*

Main category: cs.DB

TL;DR: The paper proposes using Additive Homomorphic Encryption (AHE) to protect music vector embeddings for privacy-preserving similarity searches, improving efficiency over Fully Homomorphic Encryption (FHE).


<details>
  <summary>Details</summary>
Motivation: To address the challenges of protecting music embeddings, which are vulnerable to misuse and theft, particularly given the limitations of traditional safeguarding methods like copyright licensing and digital watermarking.

Method: The authors analyze specific threat models for music information retrieval systems, conduct a theoretical exploration, and propose an AHE-based solution for secure vector similarity search using inner products of music embeddings.

Result: The proposed method is both efficient and practical, as demonstrated through empirical evaluations on real-world MP3 files, showing improvements over FHE in terms of performance.

Conclusion: Additive Homomorphic Encryption is a viable and practical solution for privacy-preserving similarity searches in music data, balancing security and computational feasibility.

Abstract: In the era of generative AI, ensuring the privacy of music data presents
unique challenges: unlike static artworks such as images, music data is
inherently temporal and multimodal, and it is sampled, transformed, and remixed
at an unprecedented scale. These characteristics make its core vector
embeddings, i.e, the numerical representations of the music, highly susceptible
to being learned, misused, or even stolen by models without accessing the
original audio files. Traditional methods like copyright licensing and digital
watermarking offer limited protection for these abstract mathematical
representations, thus necessitating a stronger, e.g., cryptographic, approach
to safeguarding the embeddings themselves. Standard encryption schemes, such as
AES, render data unintelligible for computation, making such searches
impossible. While Fully Homomorphic Encryption (FHE) provides a plausible
solution by allowing arbitrary computations on ciphertexts, its substantial
performance overhead remains impractical for large-scale vector similarity
searches. Given this trade-off, we propose a more practical approach using
Additive Homomorphic Encryption (AHE) for vector similarity search. The primary
contributions of this paper are threefold: we analyze threat models unique to
music information retrieval systems; we provide a theoretical analysis and
propose an efficient AHE-based solution through inner products of music
embeddings to deliver privacy-preserving similarity search; and finally, we
demonstrate the efficiency and practicality of the proposed approach through
empirical evaluation and comparison to FHE schemes on real-world MP3 files.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [738] [Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading](https://arxiv.org/abs/2508.07408)
*Yueyi Wang,Qiyao Wei*

Main category: q-fin.ST

TL;DR: The paper demonstrates the use of large language models (LLMs) for financial semantic annotation and alpha signal discovery by labeling event categories in company-related tweets and analyzing their correlation with market returns.


<details>
  <summary>Details</summary>
Motivation: To explore the utility of LLMs for transforming unstructured social media text into structured event variables for financial forecasting.

Method: Company-related tweets are analyzed using an LLM to assign multi-label event categories, and their labeled sentiment signals are statistically aligned with market returns over 1-to-7-day horizons.

Result: Certain event labels consistently show negative alpha with statistically significant metrics (e.g., Sharpe ratios as low as -0.38 and information coefficients above 0.05).

Conclusion: The study validates that social media sentiment can be a valuable input for financial forecasting and emphasizes transparency, reproducibility, and the democratization of algorithmic trading research via open-source frameworks.

Abstract: In this study, we wish to showcase the unique utility of large language
models (LLMs) in financial semantic annotation and alpha signal discovery.
Leveraging a corpus of company-related tweets, we use an LLM to automatically
assign multi-label event categories to high-sentiment-intensity tweets. We
align these labeled sentiment signals with forward returns over 1-to-7-day
horizons to evaluate their statistical efficacy and market tradability. Our
experiments reveal that certain event labels consistently yield negative alpha,
with Sharpe ratios as low as -0.38 and information coefficients exceeding 0.05,
all statistically significant at the 95\% confidence level. This study
establishes the feasibility of transforming unstructured social media text into
structured, multi-label event variables. A key contribution of this work is its
commitment to transparency and reproducibility; all code and methodologies are
made publicly available. Our results provide compelling evidence that social
media sentiment is a valuable, albeit noisy, signal in financial forecasting
and underscore the potential of open-source frameworks to democratize
algorithmic trading research.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [739] [Do Streetscapes Still Matter for Customer Ratings of Eating and Drinking Establishments in Car-Dependent Cities?](https://arxiv.org/abs/2508.06513)
*Chaeyeon Han,Seung Jae Lieu,Uijeong Hwang,Subhrajit Guhathakurta*

Main category: physics.soc-ph

TL;DR: The study investigates how indoor and outdoor aesthetics influence customer satisfaction at eating and dining establishments, revealing context-dependent effects of streetscape quality on ratings.


<details>
  <summary>Details</summary>
Motivation: To understand how the aesthetic quality of indoor and outdoor spaces, along with neighborhood features, affects customer satisfaction across urban settings with varying levels of car dependency.

Method: The researchers utilized review photos and street view images, analyzed through computer vision models, to gauge perceived safety and visual appeal. They then applied ordinal logistic regression to assess the relationship between these factors and Yelp ratings.

Result: Both indoor and outdoor environments significantly influence EDE ratings, whereas streetscape quality's impact diminishes in car-dependent urban areas.

Conclusion: Urban planning should consider both indoor and outdoor factors to improve customer experiences, tailoring strategies to specific urban contexts.

Abstract: This study examines how indoor and outdoor aesthetics, streetscapes, and
neighborhood features shape customer satisfaction at eating and dining
establishments (EDEs) across different urban contexts, varying in car
dependency, in Washington, DC. Using review photos and street view images,
computer vision models quantified perceived safety and visual appeal. Ordinal
logistic regression analyzed their effects on Yelp ratings. Findings reveal
that both indoor and outdoor environments significantly impact EDE ratings,
while streetscape quality's influence diminishes in car-dependent areas. The
study highlights the need for context-sensitive planning that integrates indoor
and outdoor factors to enhance customer experiences in diverse settings.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [740] [Generative Bid Shading in Real-Time Bidding Advertising](https://arxiv.org/abs/2508.06550)
*Yinqiu Huang,Hao Ma,Wenshuai Chen,Shuli Wang,Yongqiang Zhang,Xue Wei,Yinhua Zhu,Haitao Wang,Xingxing Wang*

Main category: cs.GT

TL;DR: This paper introduces Generative Bid Shading (GBS), a method for optimizing bid shading in Real-Time Bidding (RTB) through a generative model and a reward preference alignment system.


<details>
  <summary>Details</summary>
Motivation: Existing bid shading methods in RTB face challenges due to unimodal assumptions, discretization issues, and sample selection bias, which limit error correction and adaptability.

Method: The paper proposes an end-to-end generative model for shading ratios using stepwise residuals, paired with a reward model featuring CHNet and group relative policy optimization (GRPO).

Result: Experiments show that GBS effectively captures complex value dependencies and optimizes surplus, with deployment demonstrating high performance on the Meituan DSP platform.

Conclusion: GBS is a well-validated and practical solution for improving bid shading, addressing shortcomings in current methods and achieving scalable real-world application.

Abstract: Bid shading plays a crucial role in Real-Time Bidding~(RTB) by adaptively
adjusting the bid to avoid advertisers overspending. Existing mainstream
two-stage methods, which first model bid landscapes and then optimize surplus
using operations research techniques, are constrained by unimodal assumptions
that fail to adapt for non-convex surplus curves and are vulnerable to
cascading errors in sequential workflows. Additionally, existing discretization
models of continuous values ignore the dependence between discrete intervals,
reducing the model's error correction ability, while sample selection bias in
bidding scenarios presents further challenges for prediction. To address these
issues, this paper introduces Generative Bid Shading~(GBS), which comprises two
primary components: (1) an end-to-end generative model that utilizes an
autoregressive approach to generate shading ratios by stepwise residuals,
capturing complex value dependencies without relying on predefined priors; and
(2) a reward preference alignment system, which incorporates a channel-aware
hierarchical dynamic network~(CHNet) as the reward model to extract
fine-grained features, along with modules for surplus optimization and
exploration utility reward alignment, ultimately optimizing both short-term and
long-term surplus using group relative policy optimization~(GRPO). Extensive
experiments on both offline and online A/B tests validate GBS's effectiveness.
Moreover, GBS has been deployed on the Meituan DSP platform, serving billions
of bid requests daily.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [741] [Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment](https://arxiv.org/abs/2508.07283)
*Bujar Raufi*

Main category: cs.HC

TL;DR: The study integrates EEG microstate features with LLM fine-tuning to better predict cognitive states like 'Rest' and 'Load.'


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of cognitive load state assessment by combining EEG data and machine learning techniques.

Method: The research involved four stages: EEG data collection and preprocessing, EEG microstate analysis, feature extraction with prompt engineering, and supervised LLM fine-tuning using EEG features.

Result: Fine-tuning LLMs with EEG-derived features improved their ability to discriminate cognitive load states accurately.

Conclusion: EEG-informed LLMs show promise in cognitive neuroscience and cognitive AI, advancing both brain dynamics understanding and machine learning techniques.

Abstract: This study explores the intersection of electroencephalography (EEG)
microstates and Large Language Models (LLMs) to enhance the assessment of
cognitive load states. By utilizing EEG microstate features, the research aims
to fine-tune LLMs for improved predictions of distinct cognitive states,
specifically 'Rest' and 'Load'. The experimental design is delineated in four
comprehensive stages: dataset collection and preprocessing, microstate
segmentation and EEG backfitting, feature extraction paired with prompt
engineering, and meticulous LLM model selection and refinement. Employing a
supervised learning paradigm, the LLM is trained to identify cognitive load
states based on EEG microstate features integrated into prompts, producing
accurate discrimination of cognitive load. A curated dataset, linking EEG
features to specified cognitive load conditions, underpins the experimental
framework. The results indicate a significant improvement in model performance
following the proposed fine-tuning, showcasing the potential of EEG-informed
LLMs in cognitive neuroscience and cognitive AI applications. This approach not
only contributes to the understanding of brain dynamics but also paves the way
for advancements in machine learning techniques applicable to cognitive load
and cognitive AI research.

</details>


### [742] [Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools](https://arxiv.org/abs/2508.07203)
*Prashant Sharma*

Main category: cs.HC

TL;DR: The paper introduces an open-source platform designed for technically skilled civil servants outside formal IT roles to develop, review, and deploy small-scale applications securely within government systems.


<details>
  <summary>Details</summary>
Motivation: Address the lack of secure, sanctioned workflows for technically skilled civil servants who can write code but are excluded from formal IT roles.

Method: Develop an open-source platform combining Jupyter Notebooks, preapproved libraries, and a sandboxed workflow, adhering to IT security and procurement rules.

Result: The platform allows development, peer review, and deployment of government applications while avoiding vendor lock-in and enhancing skills of civil servants.

Conclusion: The platform introduces a replicable model for skill retention, digital transformation, and resilience among public servants, filling a critical middle-ground gap in digital government ecosystems.

Abstract: Current digital government literature focuses on professional in-house IT
teams, specialized digital service teams, vendor-developed systems, or
proprietary low-code/no-code tools. Almost no scholarship addresses a growing
middle ground: technically skilled civil servants outside formal IT roles who
can write real code but lack a sanctioned, secure path to deploy their work.
This paper introduces a limits-aware, open-source and replicable platform that
enables such public servants to develop, peer review, and deploy small-scale,
domain-specific applications within government networks via a sandboxed,
auditable workflow. By combining Jupyter Notebooks, preapproved open-source
libraries, and lightweight governance, the platform works within institutional
constraints such as procurement rules and IT security policies while avoiding
vendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil
servants' programming skills, keeping them technically competitive with their
private-sector peers. This contribution fills a critical gap, offering a
replicable model for public-sector skill retention, resilience, and bottom-up
digital transformation.

</details>


### [743] [ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience](https://arxiv.org/abs/2508.08101)
*Yeana Lee Bond,Mungyeong Choe,Baker Kasim Hasan,Arsh Siddiqui,Myounghoon Jeon*

Main category: cs.HC

TL;DR: This study integrates a ChatGPT-based conversational agent into a driving simulator, finding it enhances driving performance and user interaction compared to pre-scripted agents.


<details>
  <summary>Details</summary>
Motivation: Traditional in-vehicle conversational agents using pre-scripted prompts restrict natural interactions between drivers and the agent.

Method: The researchers conducted an experiment with 40 drivers using a motion-based driving simulator, comparing three agent conditions: No agent, Pre-scripted agent, and ChatGPT-based agent.

Result: The ChatGPT-based agent improved driving stability across metrics (e.g., acceleration, lane deviation) and outperformed the Pre-scripted agent in user ratings, such as competence and trust.

Conclusion: LLM-powered in-vehicle agents, like ChatGPT, show promise in improving both driving safety and user experience by facilitating continuous, context-aware dialogues.

Abstract: Studies on in-vehicle conversational agents have traditionally relied on
pre-scripted prompts or limited voice commands, constraining natural
driver-agent interaction. To resolve this issue, the present study explored the
potential of a ChatGPT-based in-vehicle agent capable of carrying continuous,
multi-turn dialogues. Forty drivers participated in our experiment using a
motion-based driving simulator, comparing three conditions (No agent,
Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.
Results showed that the ChatGPT-based agent condition led to more stable
driving performance across multiple metrics. Participants demonstrated lower
variability in longitudinal acceleration, lateral acceleration, and lane
deviation compared to the other two conditions. In subjective evaluations, the
ChatGPT-based agent also received significantly higher ratings in competence,
animacy, affective trust, and preference compared to the Pre-scripted agent.
Our thematic analysis of driver-agent conversations revealed diverse
interaction patterns in topics, including driving assistance/questions,
entertainment requests, and anthropomorphic interactions. Our results highlight
the potential of LLM-powered in-vehicle conversational agents to enhance
driving safety and user experience through natural, context-rich interactions.

</details>


### [744] [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)
*Catherine Yeh,Tara Menon,Robin Singh Arya,Helen He,Moira Weigel,Fernanda Viégas,Martin Wattenberg*

Main category: cs.HC

TL;DR: This paper introduces "Story Ribbons," a visualization tool powered by large language models (LLMs) to analyze and map literary narratives, including character and theme interactions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in extracting structured information from unstructured story data and enhance storyline visualization techniques using advancements in LLMs.

Method: The authors developed an LLM-driven data parsing pipeline to automatically extract narrative information and applied it to build Story Ribbons, an interactive visualization system.

Result: The study evaluated the pipeline and Story Ribbons on 36 literary works, showcasing LLMs' ability to streamline narrative visualization and uncover insights while highlighting current limitations.

Conclusion: LLMs can effectively support literary analysis, but AI-based systems have limitations requiring thoughtful interaction designs to address them.

Abstract: Analyzing literature involves tracking interactions between characters,
locations, and themes. Visualization has the potential to facilitate the
mapping and analysis of these complex relationships, but capturing structured
information from unstructured story data remains a challenge. As large language
models (LLMs) continue to advance, we see an opportunity to use their text
processing and analysis capabilities to augment and reimagine existing
storyline visualization techniques. Toward this goal, we introduce an
LLM-driven data parsing pipeline that automatically extracts relevant narrative
information from novels and scripts. We then apply this pipeline to create
Story Ribbons, an interactive visualization system that helps novice and expert
literary analysts explore detailed character and theme trajectories at multiple
narrative levels. Through pipeline evaluations and user studies with Story
Ribbons on 36 literary works, we demonstrate the potential of LLMs to
streamline narrative visualization creation and reveal new insights about
familiar stories. We also describe current limitations of AI-based systems, and
interaction motifs designed to address these issues.

</details>


### [745] [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)
*Baihan Lin*

Main category: cs.HC

TL;DR: The paper introduces "Conversational DNA," a visualization tool for analyzing dialogue as structured systems, highlighting patterns missed by traditional methods.


<details>
  <summary>Details</summary>
Motivation: To explore deeper communication aspects underlying dialogues, especially with increasing interaction between humans and artificial intelligence.

Method: Developed a novel visualization methodology using biological metaphors to represent dialogue dynamics, analyzing therapeutic and historical human-AI conversations for insights.

Result: Showed how Conversational DNA reveals overlooked interaction patterns, enhancing understanding of dialogue structure and emotional flow.

Conclusion: Conversational DNA offers a creative framework unifying data visualization and dialogue analysis, aiding comprehension of meaningful interactions in human and AI communication.

Abstract: What if the patterns hidden within dialogue reveal more about communication
than the words themselves? We introduce Conversational DNA, a novel visual
language that treats any dialogue -- whether between humans, between human and
AI, or among groups -- as a living system with interpretable structure that can
be visualized, compared, and understood. Unlike traditional conversation
analysis that reduces rich interaction to statistical summaries, our approach
reveals the temporal architecture of dialogue through biological metaphors.
Linguistic complexity flows through strand thickness, emotional trajectories
cascade through color gradients, conversational relevance forms through
connecting elements, and topic coherence maintains structural integrity through
helical patterns. Through exploratory analysis of therapeutic conversations and
historically significant human-AI dialogues, we demonstrate how this
visualization approach reveals interaction patterns that traditional methods
miss. Our work contributes a new creative framework for understanding
communication that bridges data visualization, human-computer interaction, and
the fundamental question of what makes dialogue meaningful in an age where
humans increasingly converse with artificial minds.

</details>


### [746] [Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators](https://arxiv.org/abs/2508.06846)
*Hyo Jin Do,Rachel Ostrand,Werner Geyer,Keerthiram Murugesan,Dennis Wei,Justin Weisz*

Main category: cs.HC

TL;DR: The study explores methods to improve communication of factuality in large language models (LLMs) through design strategies, finding that color-coding phrases enhances user trust and ease of validation.


<details>
  <summary>Details</summary>
Motivation: Large language models generate inaccurate or false information (hallucinations), and there is limited research on how to effectively communicate these inaccuracies to users.

Method: The paper conducted two scenario-based experiments involving 208 participants to evaluate the effects of design strategies for communicating factuality scores, including trust, validation ease, and user preferences.

Result: Participants preferred and trusted a design where all phrases in a response were color-coded by their factuality scores. This approach also facilitated easier validation of LLM outputs compared to a baseline design.

Conclusion: The study provides practical design guidelines for LLM applications, focusing on improving user trust, aligning designs with user preferences, and making it easier for users to assess the accuracy of LLM responses.

Abstract: Large language models (LLMs) are susceptible to generating inaccurate or
false information, often referred to as "hallucinations" or "confabulations."
While several technical advancements have been made to detect hallucinated
content by assessing the factuality of the model's responses, there is still
limited research on how to effectively communicate this information to users.
To address this gap, we conducted two scenario-based experiments with a total
of 208 participants to systematically compare the effects of various design
strategies for communicating factuality scores by assessing participants'
ratings of trust, ease in validating response accuracy, and preference. Our
findings reveal that participants preferred and trusted a design in which all
phrases within a response were color-coded based on factuality scores.
Participants also found it easier to validate accuracy of the response in this
style compared to a baseline with no style applied. Our study offers practical
design guidelines for LLM application developers and designers, aimed at
calibrating user trust, aligning with user preferences, and enhancing users'
ability to scrutinize LLM outputs.

</details>


### [747] [Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust](https://arxiv.org/abs/2508.07095)
*Hyo Jin Do,Werner Geyer*

Main category: cs.HC

TL;DR: The study explores strategies to present factuality assessments in AI-generated outputs, revealing that opaque and ambiguity approaches improve user trust compared to transparent or attention strategies.


<details>
  <summary>Details</summary>
Motivation: To address the issue of factually incorrect outputs by large language models and examine how different presentations of factuality assessments influence user trust.

Method: The researchers tested four presentation strategies (transparent, attention, opaque, ambiguity) of factuality assessments in AI outputs using a question-answering format with 148 human participants.

Result: Opaque and ambiguity presentation strategies resulted in higher user trust and perceived answer quality compared to transparent or attention approaches, and a baseline response without factuality information.

Conclusion: Hiding less factual content strategically enhances end-user trust while maintaining perceived quality of AI-generated answers.

Abstract: Large language models are known to produce outputs that are plausible but
factually incorrect. To prevent people from making erroneous decisions by
blindly trusting AI, researchers have explored various ways of communicating
factuality estimates in AI-generated outputs to end-users. However, little is
known about whether revealing content estimated to be factually incorrect
influences users' trust when compared to hiding it altogether. We tested four
different ways of disclosing an AI-generated output with factuality
assessments: transparent (highlights less factual content), attention
(highlights factual content), opaque (removes less factual content), ambiguity
(makes less factual content vague), and compared them with a baseline response
without factuality information. We conducted a human subjects research (N =
148) using the strategies in question-answering scenarios. We found that the
opaque and ambiguity strategies led to higher trust while maintaining perceived
answer quality, compared to the other strategies. We discuss the efficacy of
hiding presumably less factual content to build end-user trust.

</details>


### [748] [Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers](https://arxiv.org/abs/2508.07129)
*Caroline M. Johnston,Olga Koumoundouros,Angel Hsing-Chi Hwang,Laura Onasch-Vera,Eric Rice,Phebe Vayanos*

Main category: cs.HC

TL;DR: This study explores the reception of AI-driven housing matching algorithms by practitioners in homeless services, identifying potential gains, drawbacks, and design considerations.


<details>
  <summary>Details</summary>
Motivation: To investigate whether AI algorithms can be effectively and ethically integrated into housing resource allocation processes for individuals experiencing homelessness.

Method: Semi-structured interviews were conducted with 13 policymakers in homeless services in Los Angeles, followed by qualitative analysis.

Result: Policymakers expressed openness to AI tools if designed thoughtfully, recognizing potential efficiency improvements and ethical constraints but seeing challenges in aligning with fairness and transparency.

Conclusion: Insights from policymakers highlight the need for responsible and collaborative AI designs, offering guidance for future AI system development in low-resource decision-making contexts.

Abstract: Artificial intelligence researchers have proposed various data-driven
algorithms to improve the processes that match individuals experiencing
homelessness to scarce housing resources. It remains unclear whether and how
these algorithms are received or adopted by practitioners and what their
corresponding consequences are. Through semi-structured interviews with 13
policymakers in homeless services in Los Angeles, we investigate whether such
change-makers are open to the idea of integrating AI into the housing resource
matching process, identifying where they see potential gains and drawbacks from
such a system in issues of efficiency, fairness, and transparency. Our
qualitative analysis indicates that, even when aware of various complicating
factors, policymakers welcome the idea of an AI matching tool if thoughtfully
designed and used in tandem with human decision-makers. Though there is no
consensus as to the exact design of such an AI system, insights from
policymakers raise open questions and design considerations that can be
enlightening for future researchers and practitioners who aim to build
responsible algorithmic systems to support decision-making in low-resource
scenarios.

</details>


### [749] [ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets](https://arxiv.org/abs/2508.06732)
*Yuya Kawakami,Daniel Cayan,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.HC

TL;DR: ClimateSOM is a workflow combining self-organizing maps (SOM) and large language models (LLMs) for analyzing climate ensemble datasets, demonstrated with precipitation projections in the U.S.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding variability among climate ensemble model runs and analyzing their patterns and magnitude effectively.

Method: The authors used a SOM to map ensemble datasets to a 2D space, capturing variability, and integrated LLMs to aid in interpreting the SOM-defined space. The workflow facilitates interactive exploration, clustering, and comparison of ensemble runs.

Result: They demonstrated ClimateSOM on precipitation projection datasets in California and the Northwestern U.S., and evaluated its effectiveness through expert analysis and LLM integration testing.

Conclusion: ClimateSOM supports climate scientists in exploring, understanding, and interpreting variabilities in ensemble model runs effectively with promising utility verified by expert feedback.

Abstract: Ensemble datasets are ever more prevalent in various scientific domains. In
climate science, ensemble datasets are used to capture variability in
projections under plausible future conditions including greenhouse and aerosol
emissions. Each ensemble model run produces projections that are fundamentally
similar yet meaningfully distinct. Understanding this variability among
ensemble model runs and analyzing its magnitude and patterns is a vital task
for climate scientists. In this paper, we present ClimateSOM, a visual analysis
workflow that leverages a self-organizing map (SOM) and Large Language Models
(LLMs) to support interactive exploration and interpretation of climate
ensemble datasets. The workflow abstracts climate ensemble model runs -
spatiotemporal time series - into a distribution over a 2D space that captures
the variability among the ensemble model runs using a SOM. LLMs are integrated
to assist in sensemaking of this SOM-defined 2D space, the basis for the visual
analysis tasks. In all, ClimateSOM enables users to explore the variability
among ensemble model runs, identify patterns, compare and cluster the ensemble
model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to
an ensemble dataset of precipitation projections over California and the
Northwestern United States. Furthermore, we conduct a short evaluation of our
LLM integration, and conduct an expert review of the visual workflow and the
insights from the case studies with six domain experts to evaluate our approach
and its utility.

</details>


### [750] [Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI](https://arxiv.org/abs/2508.07183)
*Ahmed M. Abuzuraiq,Philippe Pasquier*

Main category: cs.HC

TL;DR: The paper focuses on enabling artists to use large-scale generative models as creative tools through a craft-based explainability approach that emphasizes hands-on manipulation and understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance explainability in AI for creative contexts by empowering artists to interact with and modify large generative models, fostering agency and sustained artistic practice.

Method: The paper proposes a craft-based approach to explainability inspired by reflection-in-action principles. It introduces a model manipulation and inspection plugin integrated into the interface of ComfyUI.

Result: The study illustrates that through interactive manipulation of components of generative models, artists can gain intuition about how these parts affect outputs. This enhances their creative engagement.

Conclusion: The paper concludes that exposing and making generative models manipulable allows them to be treated as creative materials, extending artistic agency and enabling deeper understanding of the systems.

Abstract: Explainable AI (XAI) in creative contexts can go beyond transparency to
support artistic engagement, modifiability, and sustained practice. While
curated datasets and training human-scale models can offer artists greater
agency and control, large-scale generative models like text-to-image diffusion
systems often obscure these possibilities. We suggest that even large models
can be treated as creative materials if their internal structure is exposed and
manipulable. We propose a craft-based approach to explainability rooted in
long-term, hands-on engagement akin to Sch\"on's "reflection-in-action" and
demonstrate its application through a model-bending and inspection plugin
integrated into the node-based interface of ComfyUI. We demonstrate that by
interactively manipulating different parts of a generative model, artists can
develop an intuition about how each component influences the output.

</details>


### [751] [Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics](https://arxiv.org/abs/2508.07390)
*Gustavo Moreira,Leonardo Ferreira,Carolina Veiga,Maryam Hosseini,Fabio Miranda*

Main category: cs.HC

TL;DR: The paper introduces Urbanite, a framework leveraging large language models for human-AI collaboration in urban visual analytics, addressing alignment challenges between user intent and system behavior.


<details>
  <summary>Details</summary>
Motivation: The authors aim to make urban data analysis more accessible by addressing challenges posed by the increasing complexity of data and workflows, particularly for users lacking technical expertise.

Method: Urbanite uses a dataflow-based model to let users specify intent at various scopes, providing interactive alignment through features like explainability, multi-resolution task definition, and interaction provenance.

Result: The framework's effectiveness is demonstrated through real-world usage scenarios co-developed with urban experts.

Conclusion: Urbanite represents an innovative step toward lowering the barriers to urban analytics, facilitating human-AI collaboration and bridging gaps between user intent, system performance, and analytical outcomes.

Abstract: With the growing availability of urban data and the increasing complexity of
societal challenges, visual analytics has become essential for deriving
insights into pressing real-world problems. However, analyzing such data is
inherently complex and iterative, requiring expertise across multiple domains.
The need to manage diverse datasets, distill intricate workflows, and integrate
various analytical methods presents a high barrier to entry, especially for
researchers and urban experts who lack proficiency in data management, machine
learning, and visualization. Advancements in large language models offer a
promising solution to lower the barriers to the construction of analytics
systems by enabling users to specify intent rather than define precise
computational operations. However, this shift from explicit operations to
intent-based interaction introduces challenges in ensuring alignment throughout
the design and development process. Without proper mechanisms, gaps can emerge
between user intent, system behavior, and analytical outcomes. To address these
challenges, we propose Urbanite, a framework for human-AI collaboration in
urban visual analytics. Urbanite leverages a dataflow-based model that allows
users to specify intent at multiple scopes, enabling interactive alignment
across the specification, process, and evaluation stages of urban analytics.
Based on findings from a survey to uncover challenges, Urbanite incorporates
features to facilitate explainability, multi-resolution definition of tasks
across dataflows, nodes, and parameters, while supporting the provenance of
interactions. We demonstrate Urbanite's effectiveness through usage scenarios
created in collaboration with urban experts. Urbanite is available at
https://urbantk.org/urbanite.

</details>


### [752] [VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design](https://arxiv.org/abs/2508.07497)
*Leonardo Ferreira,Gustavo Moreira,Fabio Miranda*

Main category: cs.HC

TL;DR: The paper introduces VA-Blueprint, a methodology and knowledge base to systematically categorize components of urban visual analytics (VA) systems and automate this process using large language models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of structured knowledge bases for the design and development of visual analytics systems, particularly given the complexity of integrating data processing, analytics, and visualization.

Method: The authors developed VA-Blueprint by systematically reviewing and categorizing 20 papers' core components of urban VA systems, then scaled this process using a large language model to analyze an additional 81 papers, forming a corpus of 101 papers.

Result: The study organized the fundamental components into a multi-level structure, evaluated the scaling effectiveness by interviewing experts, and performing quantitative annotation analysis.

Conclusion: This work delivers a practical methodology and resource for building urban VA systems, making development more structured, reproducible, and efficient.

Abstract: Designing and building visual analytics (VA) systems is a complex, iterative
process that requires the seamless integration of data processing, analytics
capabilities, and visualization techniques. While prior research has
extensively examined the social and collaborative aspects of VA system
authoring, the practical challenges of developing these systems remain
underexplored. As a result, despite the growing number of VA systems, there are
only a few structured knowledge bases to guide their design and development. To
tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that
systematically reviews and categorizes the fundamental building blocks of urban
VA systems, a domain particularly rich and representative due to its intricate
data and unique problem sets. Applying this methodology to an initial set of 20
systems, we identify and organize their core components into a multi-level
structure, forming an initial knowledge base with a structured blueprint for VA
system development. To scale this effort, we leverage a large language model to
automate the extraction of these components for other 81 papers (completing a
corpus of 101 papers), assessing its effectiveness in scaling knowledge base
construction. We evaluate our method through interviews with experts and a
quantitative analysis of annotation metrics. Our contributions provide a deeper
understanding of VA systems' composition and establish a practical foundation
to support more structured, reproducible, and efficient system development.
VA-Blueprint is available at https://urbantk.org/va-blueprint.

</details>


### [753] [On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making](https://arxiv.org/abs/2508.07617)
*Sarah Jabbour,David Fouhey,Nikola Banovic,Stephanie D. Shepard,Ella Kazerooni,Michael W. Sjoding,Jenna Wiens*

Main category: cs.HC

TL;DR: AI can assist human decision-making but might lead to errors due to automation bias. Selective prediction, where the AI abstains in uncertain situations, helps mitigate this but introduces new patterns of mistakes.


<details>
  <summary>Details</summary>
Motivation: To address challenges in human reliance on AI predictions, especially in contexts such as clinical decision-making, and test whether selective prediction can improve decision accuracy.

Method: A user study with 259 clinicians assessing diagnostic and treatment accuracy, comparing baseline performance, AI-assisted decisions with inaccurate predictions, and selective prediction scenarios.

Result: Selective prediction reduced the negative effects of inaccurate AI predictions, recovering clinical decision accuracy but leading to an increase in underdiagnosis and undertreatment when AI abstains.

Conclusion: Selective prediction is a promising approach to safeguard decision accuracy against erroneous AI predictions but demands careful validation of associated behavioral impacts in human-AI systems.

Abstract: AI has the potential to augment human decision making. However, even
high-performing models can produce inaccurate predictions when deployed. These
inaccuracies, combined with automation bias, where humans overrely on AI
predictions, can result in worse decisions. Selective prediction, in which
potentially unreliable model predictions are hidden from users, has been
proposed as a solution. This approach assumes that when AI abstains and informs
the user so, humans make decisions as they would without AI involvement. To
test this assumption, we study the effects of selective prediction on human
decisions in a clinical context. We conducted a user study of 259 clinicians
tasked with diagnosing and treating hospitalized patients. We compared their
baseline performance without any AI involvement to their AI-assisted accuracy
with and without selective prediction. Our findings indicate that selective
prediction mitigates the negative effects of inaccurate AI in terms of decision
accuracy. Compared to no AI assistance, clinician accuracy declined when shown
inaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]),
but recovered under selective prediction (64% [95% CI: 54%-73%]). However,
while selective prediction nearly maintains overall accuracy, our results
suggest that it alters patterns of mistakes: when informed the AI abstains,
clinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35%
increase in missed treatments) compared to no AI input at all. Our findings
underscore the importance of empirically validating assumptions about how
humans engage with AI within human-AI systems.

</details>


### [754] [CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning](https://arxiv.org/abs/2508.07731)
*Abdul Basit,Maha Nawaz,Saim Rehman,Muhammad Shafique*

Main category: cs.HC

TL;DR: The paper introduces CognitiveArm, a real-time brain-computer interface for prosthetic control using EEG signals, optimized with efficient deep learning models for use on embedded hardware.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving real-time, accurate, and efficient prosthetic control through non-invasive brain-computer interfaces on resource-constrained edge AI hardware.

Method: The authors developed CognitiveArm, which integrates EEG data acquisition through BrainFlow and optimized deep learning models. They applied evolutionary search for hyperparameter tuning, model compression (pruning and quantization), and created an annotation pipeline for precise labeling of EEG signals. Voice commands were also incorporated for enhanced control.

Result: CognitiveArm achieved up to 90% accuracy in classifying three core actions (left, right, idle) while running on embedded hardware. Voice integration facilitated more complex actions, demonstrating effective real-world application.

Conclusion: CognitiveArm successfully balances accuracy, computational efficiency, and real-time responsiveness, showcasing its potential for advanced, non-invasive prosthetic control systems using embedded AI hardware.

Abstract: Efficient control of prosthetic limbs via non-invasive brain-computer
interfaces (BCIs) requires advanced EEG processing, including pre-filtering,
feature extraction, and action prediction, performed in real time on edge AI
hardware. Achieving this on resource-constrained devices presents challenges in
balancing model complexity, computational efficiency, and latency. We present
CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on
embedded AI hardware, achieving real-time operation without compromising
accuracy. The system integrates BrainFlow, an open-source library for EEG data
acquisition and streaming, with optimized deep learning (DL) models for precise
brain signal classification. Using evolutionary search, we identify
Pareto-optimal DL configurations through hyperparameter tuning, optimizer
analysis, and window selection, analyzed individually and in ensemble
configurations. We apply model compression techniques such as pruning and
quantization to optimize models for embedded deployment, balancing efficiency
and accuracy. We collected an EEG dataset and designed an annotation pipeline
enabling precise labeling of brain signals corresponding to specific intended
actions, forming the basis for training our optimized DL models. CognitiveArm
also supports voice commands for seamless mode switching, enabling control of
the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded
hardware, it ensures low latency and real-time responsiveness. A full-scale
prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset,
achieved up to 90% accuracy in classifying three core actions (left, right,
idle). Voice integration enables multiplexed, variable movement for everyday
tasks (e.g., handshake, cup picking), enhancing real-world performance and
demonstrating CognitiveArm's potential for advanced prosthetic control.

</details>


### [755] [Can AI Explanations Make You Change Your Mind?](https://arxiv.org/abs/2508.08158)
*Laura Spillner,Rachel Ringe,Robert Porzel,Rainer Malaka*

Main category: cs.HC

TL;DR: This paper examines how users engage with explanations in AI decision support systems (DSS) and their impact on trust and decision reconsideration.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address whether users sufficiently consider AI explanations to spot errors and biases, enabling better human oversight and trust in AI decision-making.

Method: An online study was conducted to investigate participants' engagement with explanations provided by an AI decision support system, followed by exploratory analysis.

Result: Findings reveal that participants often spend minimal time engaging with AI explanations, affecting their openness to reconsider decisions based on AI suggestions.

Conclusion: User engagement with AI explanations is crucial for fostering trust and ensuring effective oversight in AI-supported decision-making systems. Mechanisms to encourage deeper consideration may be needed.

Abstract: In the context of AI-based decision support systems, explanations can help
users to judge when to trust the AI's suggestion, and when to question it. In
this way, human oversight can prevent AI errors and biased decision-making.
However, this rests on the assumption that users will consider explanations in
enough detail to be able to catch such errors. We conducted an online study on
trust in explainable DSS, and were surprised to find that in many cases,
participants spent little time on the explanation and did not always consider
it in detail. We present an exploratory analysis of this data, investigating
what factors impact how carefully study participants consider AI explanations,
and how this in turn impacts whether they are open to changing their mind based
on what the AI suggests.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [756] [Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach](https://arxiv.org/abs/2508.06616)
*Md Arafat Habib,Medhat Elsayed,Yigit Ozcan,Pedro Enrique Iturria-Rivera,Majid Bavand,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: The paper discusses utilizing Large Language Models (LLMs) and Generative AI for Intent-Driven Networks (IDNs) to improve 6G network automation, proposing an advanced hierarchical framework and showcasing its effectiveness through a case study.


<details>
  <summary>Details</summary>
Motivation: To address the increasing complexity and heterogeneity of 6G networks needing advanced, intelligent automation beyond traditional means, leveraging LLMs and Generative AI for translating user intents into efficient network management.

Method: The authors survey LLM-based IDN architectures and propose a hierarchical GenAI-enabled IDN framework covering three core stages: intent processing, intent validation, and intent execution. A case study is conducted using the GenAI framework, Mamba.

Result: The proposed GenAI-driven IDN architecture outperforms traditional IDN architectures in managing and optimizing network performance within 6G environments.

Conclusion: Integrating GenAI, particularly LLMs, across multiple stages of IDN management enables a more intelligent, adaptive, and efficient approach to managing complex 6G networks.

Abstract: With the emergence of 6G, mobile networks are becoming increasingly
heterogeneous and dynamic, necessitating advanced automation for efficient
management. Intent-Driven Networks (IDNs) address this by translating
high-level intents into optimization policies. Large Language Models (LLMs) can
enhance this process by understanding complex human instructions to enable
adaptive, intelligent automation. Given the rapid advancements in Generative AI
(GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated
Radio Access Network (RAN) environments is both timely and critical. This
article provides such a survey, along with a case study on a hierarchical
learning-enabled IDN architecture that integrates GenAI across three key
stages: intent processing, intent validation, and intent execution. Unlike most
existing approaches that apply GenAI in the form of LLMs for intent processing
only, we propose a hierarchical framework that introduces GenAI across all
three stages of IDN. To demonstrate the effectiveness of the proposed IDN
management architecture, we present a case study based on the latest GenAI
architecture named Mamba. The case study shows how the proposed GenAI-driven
architecture enhances network performance through intelligent automation,
surpassing the performance of the conventional IDN architectures.

</details>


### [757] [Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization](https://arxiv.org/abs/2508.07001)
*Myeung Suk Oh,Zhiyao Zhang,FNU Hairi,Alvaro Velasquez,Jia Liu*

Main category: cs.NI

TL;DR: The paper introduces a decentralized multi-agent reinforcement learning (MARL) method for improving medium access control in wireless networks, using local rewards and consensus-based exchanges to minimize collisions and communication overhead.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of wireless devices forming smart networks demands efficient designs to manage unpredictable data traffic, which is challenging in RA-based MAC protocols due to issues like collisions and fairness.

Method: The study proposes a fully decentralized MARL framework built on an actor-critic network, where devices exchange local rewards and leverage consensus-based information to bypass the need for centralized training.

Result: Numerical experiments demonstrate that the proposed MARL algorithm significantly outperforms existing baselines in RA network performance.

Conclusion: The proposed decentralized MARL algorithm, with reduced communication overhead and theoretical proof of global convergence, provides a practical and effective solution for improving wireless medium access control.

Abstract: With wireless devices increasingly forming a unified smart network for
seamless, user-friendly operations, random access (RA) medium access control
(MAC) design is considered a key solution for handling unpredictable data
traffic from multiple terminals. However, it remains challenging to design an
effective RA-based MAC protocol to minimize collisions and ensure transmission
fairness across the devices. While existing multi-agent reinforcement learning
(MARL) approaches with centralized training and decentralized execution (CTDE)
have been proposed to optimize RA performance, their reliance on centralized
training and the significant overhead required for information collection can
make real-world applications unrealistic. In this work, we adopt a fully
decentralized MARL architecture, where policy learning does not rely on
centralized tasks but leverages consensus-based information exchanges across
devices. We design our MARL algorithm over an actor-critic (AC) network and
propose exchanging only local rewards to minimize communication overhead.
Furthermore, we provide a theoretical proof of global convergence for our
approach. Numerical experiments show that our proposed MARL algorithm can
significantly improve RA network performance compared to other baselines.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [758] [Near-Optimal Convergence of Accelerated Gradient Methods under Generalized and $(L_0, L_1)$-Smoothness](https://arxiv.org/abs/2508.06884)
*Alexander Tyurin*

Main category: math.OC

TL;DR: The paper addresses an open question in $
ell$-smoothness convex optimization, achieving an optimal oracle complexity of $O(\sqrt{\ell(0)} R / \sqrt{\varepsilon})$ without additional multiplicative penalties.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and theoretical bounds of first-order methods in convex optimization, especially under generalized $
ell$-smoothness conditions, where existing methods introduce undesirable computational factors.

Method: The researchers designed new algorithms and developed a novel Lyapunov function to achieve optimal oracle complexity for small-$\varepsilon$ specifically tailored to the $
ell$-smoothness framework.

Result: The proposed method achieves a complexity of $O(\sqrt{\ell(0)} R / \sqrt{\varepsilon})$ for virtually any $
ell$. This result is optimal for the small-$\varepsilon$ regime, especially for $(L_{0},L_{1})$-smoothness.

Conclusion: The resolution confirms that an AGD-type optimal rate is attainable under $
ell$-smoothness, eliminating prior multiplicative inefficiencies and expanding the boundary of accelerated gradient methods.

Abstract: We study first-order methods for convex optimization problems with functions
$f$ satisfying the recently proposed $\ell$-smoothness condition
$||\nabla^{2}f(x)|| \le \ell\left(||\nabla f(x)||\right),$ which generalizes
the $L$-smoothness and $(L_{0},L_{1})$-smoothness. While accelerated gradient
descent AGD is known to reach the optimal complexity $O(\sqrt{L} R /
\sqrt{\varepsilon})$ under $L$-smoothness, where $\varepsilon$ is an error
tolerance and $R$ is the distance between a starting and an optimal point,
existing extensions to $\ell$-smoothness either incur extra dependence on the
initial gradient, suffer exponential factors in $L_{1} R$, or require costly
auxiliary sub-routines, leaving open whether an AGD-type $O(\sqrt{\ell(0)} R /
\sqrt{\varepsilon})$ rate is possible for small-$\varepsilon$, even in the
$(L_{0},L_{1})$-smoothness case.
  We resolve this open question. Leveraging a new Lyapunov function and
designing new algorithms, we achieve $O(\sqrt{\ell(0)} R / \sqrt{\varepsilon})$
oracle complexity for small-$\varepsilon$ and virtually any $\ell$. For
instance, for $(L_{0},L_{1})$-smoothness, our bound $O(\sqrt{L_0} R /
\sqrt{\varepsilon})$ is provably optimal in the small-$\varepsilon$ regime and
removes all non-constant multiplicative factors present in prior accelerated
algorithms.

</details>


### [759] [Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization](https://arxiv.org/abs/2508.06906)
*Morteza Kimiaei,Vyacheslav Kungurtsev,Brian Olimba*

Main category: math.OC

TL;DR: Machine learning and reinforcement learning techniques are used to enhance integer and mixed-integer nonlinear programming optimization, focusing on branch-and-bound methods for solving logistics, energy, and scheduling problems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational challenges found in integer and mixed-integer nonlinear programming problems, which are important for industries such as logistics and energy, while retaining global optimality in solutions.

Method: It introduces a unified branch-and-bound framework that leverages machine learning models such as supervised, imitation, and reinforcement learning to improve aspects like branching strategies, node ordering, and parameter control.

Result: Machine learning models were successfully integrated into classical optimization techniques to accelerate the convergence rate, offering efficient solutions to real-world problems without sacrificing correctness.

Conclusion: A taxonomy of solver classes and learning paradigms is provided, alongside identifying challenges in generalization, hybridization, and scalability for intelligent solvers.

Abstract: Integer and mixed-integer nonlinear programming (INLP, MINLP) are central to
logistics, energy, and scheduling, but remain computationally challenging. This
survey examines how machine learning and reinforcement learning can enhance
exact optimization methods - particularly branch-and-bound (BB), without
compromising global optimality. We cover discrete, continuous, and
mixed-integer formulations, and highlight applications such as crew scheduling,
vehicle routing, and hydropower planning. We introduce a unified BB framework
that embeds learning-based strategies into branching, cut selection, node
ordering, and parameter control. Classical algorithms are augmented using
supervised, imitation, and reinforcement learning models to accelerate
convergence while maintaining correctness. We conclude with a taxonomy of
learning methods by solver class and learning paradigm, and outline open
challenges in generalization, hybridization, and scaling intelligent solvers.

</details>


### [760] [From Product Hilbert Spaces to the Generalized Koopman Operator and the Nonlinear Fundamental Lemma](https://arxiv.org/abs/2508.07494)
*Mircea Lazar*

Main category: math.OC

TL;DR: This paper extends the Koopman operator to systems with control input using a product Hilbert space and introduces methods for scalable data-driven computation.


<details>
  <summary>Details</summary>
Motivation: Data-driven control methods for nonlinear systems face challenges in constructing suitable basis functions and infinite-dimensional representations.

Method: The authors construct a product Hilbert space combining state and input observable functions, enabling the definition of a generalized Koopman operator and finite-dimensional approximations.

Result: A proof of the linearity of the generalized Koopman operator and its bilinear structure is provided, along with practical application to the Van der Pol oscillator.

Conclusion: The generalized Koopman framework offers scalable tools for nonlinear system analysis and control, bridging gaps in operator theory with practical applications.

Abstract: The generalization of the Koopman operator to systems with control input and
the derivation of a nonlinear fundamental lemma are two open problems that play
a key role in the development of data-driven control methods for nonlinear
systems. Both problems hinge on the construction of observable or basis
functions and their corresponding Hilbert space that enable an
infinite-dimensional, linear system representation. In this paper we derive a
novel solution to these problems based on orthonormal expansion in a product
Hilbert space constructed as the tensor product between the Hilbert spaces of
the state and input observable functions, respectively. We prove that there
exists an infinite-dimensional linear operator, i.e. the generalized Koopman
operator, from the constructed product Hilbert space to the Hilbert space
corresponding to the lifted state propagated forward in time. A scalable
data-driven method for computing finite-dimensional approximations of
generalized Koopman operators and several choices of observable functions are
also presented. Moreover, we derive a nonlinear fundamental lemma by exploiting
the bilinear structure of the infinite-dimensional generalized Koopman model.
The effectiveness of the developed generalized Koopman embedding is illustrated
on the Van der Pol oscillator.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [761] [Barron Space Representations for Elliptic PDEs with Homogeneous Boundary Conditions](https://arxiv.org/abs/2508.07559)
*Ziang Chen,Liqiang Huang*

Main category: math.NA

TL;DR: The paper investigates the approximation of high-dimensional elliptic PDEs using two-layer neural networks within Barron spaces, avoiding the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To understand how high-dimensional PDE solutions can be efficiently approximated without suffering from the curse of dimensionality.

Method: The study assumes the coefficients belong to Barron spaces and employs two-layer neural networks to approximate the solution of elliptic PDEs.

Result: The paper proves that high-dimensional solutions can be effectively approximated by shallow networks under specific structural conditions.

Conclusion: Two-layer neural networks are demonstrated to have significant expressive power in approximating high-dimensional elliptic PDEs within the Barron framework.

Abstract: We study the approximation complexity of high-dimensional second-order
elliptic PDEs with homogeneous boundary conditions on the unit hypercube,
within the framework of Barron spaces. Under the assumption that the
coefficients belong to suitably defined Barron spaces, we prove that the
solution can be efficiently approximated by two-layer neural networks,
circumventing the curse of dimensionality. Our results demonstrate the
expressive power of shallow networks in capturing high-dimensional PDE
solutions under appropriate structural assumptions.

</details>


### [762] [Prediction error certification for PINNs: Theory, computation, and application to Stokes flow](https://arxiv.org/abs/2508.07994)
*Birgit Hillebrecht,Benjamin Unger*

Main category: math.NA

TL;DR: The paper extends a semigroup-based error estimator for physics-informed neural networks (PINNs) to handle more complex problems.


<details>
  <summary>Details</summary>
Motivation: To enable rigorous error estimation in PINN predictions, which are increasingly used for solving partial differential equations.

Method: Modifies the existing error bound and introduces numerical strategies for stability parameter approximation in the PINN error estimation framework.

Result: The extended framework allows error certification for PINN predictions in realistic scenarios like Stokes flow around a cylinder.

Conclusion: The proposed approach broadens the applicability of PINN error estimation, making it viable for practical and complex scenarios.

Abstract: Rigorous error estimation is a fundamental topic in numerical analysis. With
the increasing use of physics-informed neural networks (PINNs) for solving
partial differential equations, several approaches have been developed to
quantify the associated prediction error. In this work, we build upon a
semigroup-based framework previously introduced by the authors for estimating
the PINN error. While this estimator has so far been limited to academic
examples - due to the need to compute quantities related to input-to-state
stability - we extend its applicability to a significantly broader class of
problems. This is accomplished by modifying the error bound and proposing
numerical strategies to approximate the required stability parameters. The
extended framework enables the certification of PINN predictions in more
realistic scenarios, as demonstrated by a numerical study of Stokes flow around
a cylinder.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [763] [Decomposing Global AUC into Cluster-Level Contributions for Localized Model Diagnostics](https://arxiv.org/abs/2508.07495)
*Agus Sudjianto,Alice J. Liu*

Main category: stat.AP

TL;DR: The paper introduces a method to decompose global AUC into intra- and inter-cluster components for analyzing classifier weaknesses within and across data subpopulations, and compares this approach with additive metrics.


<details>
  <summary>Details</summary>
Motivation: Detecting localized weaknesses in classifiers is critical in high-stakes fields like credit approval and fraud detection, as these shortcomings can result in significant risks.

Method: The paper proposes a formal decomposition of global AUC into intra- and inter-cluster components, alongside comparisons with decomposable additive metrics like the Brier score and log loss.

Result: The decomposition framework enables granular diagnostic evaluations of classifier performance and supports subgroup analysis.

Conclusion: The framework enhances model validation and development by providing deeper insights for identifying weaknesses and improving model risk management practices.

Abstract: The Area Under the ROC Curve (AUC) is a widely used performance metric for
binary classifiers. However, as a global ranking statistic, the AUC aggregates
model behavior over the entire dataset, masking localized weaknesses in
specific subpopulations. In high-stakes applications such as credit approval
and fraud detection, these weaknesses can lead to financial risk or operational
failures. In this paper, we introduce a formal decomposition of global AUC into
intra- and inter-cluster components. This allows practitioners to evaluate
classifier performance within and across clusters of data, enabling granular
diagnostics and subgroup analysis. We also compare the AUC with additive
performance metrics such as the Brier score and log loss, which support
decomposability and direct attribution. Our framework enhances model
development and validation practice by providing additional insights to detect
model weakness for model risk management.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [764] [Emergent morphogenesis via planar fabrication enabled by a reduced model of composites](https://arxiv.org/abs/2508.08198)
*Yupeng Zhang,Adam Alon,M. Khalid Jawed*

Main category: cs.GR

TL;DR: The paper introduces a framework for transforming planar sheets into precise 3D shapes using a bilayer system and programmable control.


<details>
  <summary>Details</summary>
Motivation: Emerging technologies like soft robotics and functional materials require precise and scalable methods to create 3D morphologies from planar sheets.

Method: The authors developed a reduced-order numerical and experimental framework using a bilayer system composed of a stimuli-responsive thermoplastic and kirigami-patterned inert plastic layer, leveraging heat-induced contraction and programmable strain mismatch.

Result: The framework successfully created 3D shapes like bowls, canoes, and flower petals, validated by both simulations and physical prototypes.

Conclusion: This technique significantly improves computational efficiency and scalability in designing programmable 3D forms, with potential applications in robotics and reconfigurable devices.

Abstract: The ability to engineer complex three-dimensional shapes from planar sheets
with precise, programmable control underpins emerging technologies in soft
robotics, reconfigurable devices, and functional materials. Here, we present a
reduced-order numerical and experimental framework for a bilayer system
consisting of a stimuli-responsive thermoplastic sheet (Shrinky Dink) bonded to
a kirigami-patterned, inert plastic layer. Upon uniform heating, the active
layer contracts while the patterned layer constrains in-plane stretch but
allows out-of-plane bending, yielding programmable 3D morphologies from simple
planar precursors. Our approach enables efficient computational design and
scalable manufacturing of 3D forms with a single-layer reduced model that
captures the coupled mechanics of stretching and bending. Unlike traditional
bilayer modeling, our framework collapses the multilayer composite into a
single layer of nodes and elements, reducing the degrees of freedom and
enabling simulation on a 2D geometry. This is achieved by introducing a novel
energy formulation that captures the coupling between in-plane stretch mismatch
and out-of-plane bending - extending beyond simple isotropic linear elastic
models. Experimentally, we establish a fully planar, repeatable fabrication
protocol using a stimuli-responsive thermoplastic and a laser-cut inert plastic
layer. The programmed strain mismatch drives an array of 3D morphologies, such
as bowls, canoes, and flower petals, all verified by both simulation and
physical prototypes.

</details>


### [765] [Vertex Features for Neural Global Illumination](https://arxiv.org/abs/2508.07852)
*Rui Su,Honghao Dong,Haojie Jin,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: The paper proposes storing learnable features at mesh vertices instead of using feature grids, reducing memory usage while maintaining rendering quality.


<details>
  <summary>Details</summary>
Motivation: Traditional feature grid representations in neural rendering suffer from high memory requirements, limiting their efficiency on modern hardware.

Method: The authors introduce neural vertex features, storing features directly at mesh vertices and leveraging geometric priors for structured representation.

Result: The method reduces memory usage to less than a fifth of existing methods, while keeping comparable rendering quality and reducing inference overhead.

Conclusion: Neural vertex features are an efficient, geometry-aligned representation for neural rendering, combining high memory efficiency with high-quality rendering.

Abstract: Recent research on learnable neural representations has been widely adopted
in the field of 3D scene reconstruction and neural rendering applications.
However, traditional feature grid representations often suffer from substantial
memory footprint, posing a significant bottleneck for modern parallel computing
hardware. In this paper, we present neural vertex features, a generalized
formulation of learnable representation for neural rendering tasks involving
explicit mesh surfaces. Instead of uniformly distributing neural features
throughout 3D space, our method stores learnable features directly at mesh
vertices, leveraging the underlying geometry as a compact and structured
representation for neural processing. This not only optimizes memory
efficiency, but also improves feature representation by aligning compactly with
the surface using task-specific geometric priors. We validate our neural
representation across diverse neural rendering tasks, with a specific emphasis
on neural radiosity. Experimental results demonstrate that our method reduces
memory consumption to only one-fifth (or even less) of grid-based
representations, while maintaining comparable rendering quality and lowering
inference overhead.

</details>


### [766] [LL3M: Large Language 3D Modelers](https://arxiv.org/abs/2508.08228)
*Sining Lu,Guan Chen,Nam Anh Dinh,Itai Lang,Ari Holtzman,Rana Hanocka*

Main category: cs.GR

TL;DR: LL3M introduces a novel system utilizing multi-agent large language models to generate interpretable Python code in Blender for 3D asset creation, emphasizing modularity, editability, and artist collaboration.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional 3D generative approaches that rely on pre-available collections of datasets, and to provide a modular and interpretable way to generate and refine 3D assets.

Method: It uses specialized LLM agents that collaborate to interpret text prompts, plan, retrieve, write, debug, and refine Blender scripts for geometry and material creation.

Result: LL3M demonstrated success across various categories, with streamlined user refinement, code interaction, and self-critiquing capabilities enhancing editable 3D shapes, styles, and scenes.

Conclusion: LL3M establishes Python code as an effective medium for creating interpretable, editable, and diverse 3D assets, paving new paths for co-creative workflows and experimentation with large language models.

Abstract: We present LL3M, a multi-agent system that leverages pretrained large
language models (LLMs) to generate 3D assets by writing interpretable Python
code in Blender. We break away from the typical generative approach that learns
from a collection of 3D data. Instead, we reformulate shape generation as a
code-writing task, enabling greater modularity, editability, and integration
with artist workflows. Given a text prompt, LL3M coordinates a team of
specialized LLM agents to plan, retrieve, write, debug, and refine Blender
scripts that generate and edit geometry and appearance. The generated code
works as a high-level, interpretable, human-readable, well-documented
representation of scenes and objects, making full use of sophisticated Blender
constructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,
unconstrained shapes, materials, and scenes. This code presents many avenues
for further agent and human editing and experimentation via code tweaks or
procedural parameters. This medium naturally enables a co-creative loop in our
system: agents can automatically self-critique using code and visuals, while
iterative user instructions provide an intuitive way to refine assets. A shared
code context across agents enables awareness of previous attempts, and a
retrieval-augmented generation knowledge base built from Blender API
documentation, BlenderRAG, equips agents with examples, types, and functions
empowering advanced modeling operations and code correctness. We demonstrate
the effectiveness of LL3M across diverse shape categories, style and material
edits, and user-driven refinements. Our experiments showcase the power of code
as a generative and interpretable medium for 3D asset creation. Our project
page is at https://threedle.github.io/ll3m.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [767] [Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation](https://arxiv.org/abs/2508.07745)
*Jiongchi Yu,Xiaofei Xie,Qiang Hu,Yuhan Ma,Ziming Zhao*

Main category: cs.CR

TL;DR: Insider threats pose major security risks but lack adequate realistic datasets for detection research. Chimera, a framework using large language models and multi-agent simulation, creates realistic datasets to improve detection methods.


<details>
  <summary>Details</summary>
Motivation: Insider threats are a critical security challenge, and existing public datasets for detection fail to capture realistic enterprise behavior, limiting research progress.

Method: Developed Chimera, a LLM-based multi-agent simulation framework that replicates both benign and malicious insider activities in diverse enterprise environments, capturing realistic behaviors and organizational dynamics.

Result: Produced the ChimeraLog dataset showcasing diverse, realistic, and explainable threat patterns. Existing detection methods had difficulty achieving high accuracy, revealing the dataset's utility for driving advancements.

Conclusion: Chimera enables the creation of sophisticated datasets essential for advancing machine learning-based insider threat detection research through realistic simulations.

Abstract: Insider threats, which can lead to severe losses, remain a major security
concern. While machine learning-based insider threat detection (ITD) methods
have shown promising results, their progress is hindered by the scarcity of
high-quality data. Enterprise data is sensitive and rarely accessible, while
publicly available datasets, when limited in scale due to cost, lack sufficient
real-world coverage; and when purely synthetic, they fail to capture rich
semantics and realistic user behavior. To address this, we propose Chimera, the
first large language model (LLM)-based multi-agent framework that automatically
simulates both benign and malicious insider activities and collects diverse
logs across diverse enterprise environments. Chimera models each employee with
agents that have role-specific behavior and integrates modules for group
meetings, pairwise interactions, and autonomous scheduling, capturing realistic
organizational dynamics. It incorporates 15 types of insider attacks (e.g., IP
theft, system sabotage) and has been deployed to simulate activities in three
sensitive domains: technology company, finance corporation, and medical
institution, producing a new dataset, ChimeraLog. We assess ChimeraLog via
human studies and quantitative analysis, confirming its diversity, realism, and
presence of explainable threat patterns. Evaluations of existing ITD methods
show an average F1-score of 0.83, which is significantly lower than 0.99 on the
CERT dataset, demonstrating ChimeraLog's higher difficulty and utility for
advancing ITD research.

</details>


### [768] [EFU: Enforcing Federated Unlearning via Functional Encryption](https://arxiv.org/abs/2508.07873)
*Samaneh Mohammadi,Vasileios Tsouvalas,Iraklis Symeonidis,Ali Balador,Tanir Ozcelebi,Francesco Flammini,Nirvana Meratnia*

Main category: cs.CR

TL;DR: The paper introduces EFU, a secure and privacy-preserving Federated Unlearning mechanism, which uses cryptography to ensure clients can enforce unlearning without the server knowing about it.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address privacy and autonomy concerns in Federated Unlearning, ensuring that clients can remove their data's influence on the model without revealing unlearning intent or compromising security.

Method: The authors propose EFU, which employs functional encryption to secure updates and uses auxiliary unlearning losses to mask changes. EFU ensures that servers cannot detect unlearning requests or bypass them.

Result: EFU achieves effective unlearning, comparable to full retraining on datasets and architectures, while keeping the unlearning intent and process hidden from the server.

Conclusion: EFU allows secure, verifiable, and function-hiding Federated Unlearning for any client-side mechanism, advancing privacy and autonomy in federated learning.

Abstract: Federated unlearning (FU) algorithms allow clients in federated settings to
exercise their ''right to be forgotten'' by removing the influence of their
data from a collaboratively trained model. Existing FU methods maintain data
privacy by performing unlearning locally on the client-side and sending
targeted updates to the server without exposing forgotten data; yet they often
rely on server-side cooperation, revealing the client's intent and identity
without enforcement guarantees - compromising autonomy and unlearning privacy.
In this work, we propose EFU (Enforced Federated Unlearning), a
cryptographically enforced FU framework that enables clients to initiate
unlearning while concealing its occurrence from the server. Specifically, EFU
leverages functional encryption to bind encrypted updates to specific
aggregation functions, ensuring the server can neither perform unauthorized
computations nor detect or skip unlearning requests. To further mask behavioral
and parameter shifts in the aggregated model, we incorporate auxiliary
unlearning losses based on adversarial examples and parameter importance
regularization. Extensive experiments show that EFU achieves near-random
accuracy on forgotten data while maintaining performance comparable to full
retraining across datasets and neural architectures - all while concealing
unlearning intent from the server. Furthermore, we demonstrate that EFU is
agnostic to the underlying unlearning algorithm, enabling secure,
function-hiding, and verifiable unlearning for any client-side FU mechanism
that issues targeted updates.

</details>


### [769] [Fully-Fluctuating Participation in Sleepy Consensus](https://arxiv.org/abs/2508.08068)
*Yuval Efron,Joachim Neu,Toniann Pitassi*

Main category: cs.CR

TL;DR: This paper introduces the 'external adversary' model to improve security and efficiency in proof-of-stake consensus protocols under fluctuating participation.


<details>
  <summary>Details</summary>
Motivation: Current proof-of-work protocols like Bitcoin are robust under fluctuating miner participation, but proof-of-stake protocols lack comparable security mechanisms under such conditions.

Method: The authors propose a new adversary model, the 'external adversary,' where corrupt nodes do not share key-related information, making it adaptable for scenarios of fluctuating participation.

Result: Their approach demonstrates that protocols in the sleepy model can achieve security under fully fluctuating participation without sacrificing efficiency or corruption resilience.

Conclusion: This new adversary model naturally captures malicious behavior and addresses limitations in existing models, pushing proof-of-stake protocols closer to the robustness of proof-of-work systems.

Abstract: Proof-of-work allows Bitcoin to boast security amidst arbitrary fluctuations
in participation of miners throughout time, so long as, at any point in time, a
majority of hash power is honest. In recent years, however, the pendulum has
shifted in favor of proof-of-stake-based consensus protocols. There, the sleepy
model is the most prominent model for handling fluctuating participation of
nodes. However, to date, no protocol in the sleepy model rivals Bitcoin in its
robustness to drastic fluctuations in participation levels, with
state-of-the-art protocols making various restrictive assumptions. In this
work, we present a new adversary model, called external adversary. Intuitively,
in our model, corrupt nodes do not divulge information about their secret keys.
In this model, we show that protocols in the sleepy model can meaningfully
claim to remain secure against fully fluctuating participation, without
compromising efficiency or corruption resilience. Our adversary model is quite
natural, and arguably naturally captures the process via which malicious
behavior arises in protocols, as opposed to traditional worst-case modeling. On
top of which, the model is also theoretically appealing, circumventing a
barrier established in a recent work of Malkhi, Momose, and Ren.

</details>


### [770] [AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers](https://arxiv.org/abs/2508.05691)
*Kai Yao,Marc Juarez*

Main category: cs.CR

TL;DR: The paper introduces a method for verifying the origin of outputs from generative models, even in adversarial settings.


<details>
  <summary>Details</summary>
Motivation: The increasing use of generative models in sensitive areas requires robust mechanisms for verifying the origin of model outputs, especially given the potential for adversarial interference.

Method: The authors propose using a trusted verifier to extract secret fingerprints from a model's output space, training a model to predict and verify these fingerprints without the provider's knowledge.

Result: The proposed methods reliably achieve near-zero false positive rates while maintaining robustness against adversarial attacks, as demonstrated through experiments on GAN and diffusion models.

Conclusion: This work introduces a novel, reliable method for fingerprinting generative models and ensures provenance verification even in adversarial conditions, contributing significantly to trust and security in AI systems.

Abstract: Generative models are increasingly adopted in high-stakes domains, yet
current deployments offer no mechanisms to verify the origin of model outputs.
We address this gap by extending model fingerprinting techniques beyond the
traditional collaborative setting to one where the model provider may act
adversarially. To our knowledge, this is the first work to evaluate
fingerprinting for provenance attribution under such a threat model. The
methods rely on a trusted verifier that extracts secret fingerprints from the
model's output space, unknown to the provider, and trains a model to predict
and verify them. Our empirical evaluation shows that our methods achieve
near-zero FPR@95%TPR for instances of GAN and diffusion models, even when
tested on small modifications to the original architecture and training data.
Moreover, the methods remain robust against adversarial attacks that actively
modify the outputs to bypass detection. Source codes are available at
https://github.com/PSMLab/authprint.

</details>


### [771] [A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection](https://arxiv.org/abs/2508.07139)
*Ivan Zhang*

Main category: cs.CR

TL;DR: The paper introduces a real-time, self-tuning moderator framework to protect language models from adversarial attacks, focusing on reducing training complexity while ensuring adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing defenses for large language models, which fail to adapt quickly to emerging attacks, compromise performance on regular queries, or lack scalability.

Method: The authors propose a Real-Time, Self-Tuning (RTST) moderator framework to provide lightweight and adaptive defense mechanisms, tested on Google's Gemini models.

Result: Experiments show RTST offers effective defense against modern adversarial jailbreaks and maintains model performance across benign prompts.

Conclusion: The adaptive RTST framework demonstrates superior effectiveness and scalability over traditional fine-tuning and classifier-based defensive approaches.

Abstract: Ensuring LLM alignment is critical to information security as AI models
become increasingly widespread and integrated in society. Unfortunately, many
defenses against adversarial attacks and jailbreaking on LLMs cannot adapt
quickly to new attacks, degrade model responses to benign prompts, or introduce
significant barriers to scalable implementation. To mitigate these challenges,
we introduce a real-time, self-tuning (RTST) moderator framework to defend
against adversarial attacks while maintaining a lightweight training footprint.
We empirically evaluate its effectiveness using Google's Gemini models against
modern, effective jailbreaks. Our results demonstrate the advantages of an
adaptive, minimally intrusive framework for jailbreak defense over traditional
fine-tuning or classifier models.

</details>


### [772] [Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings](https://arxiv.org/abs/2508.06734)
*Ngoc N. Tran,Anwar Said,Waseem Abbas,Tyler Derr,Xenofon D. Koutsoukos*

Main category: cs.CR

TL;DR: Existing graph-based malware classifiers, despite high accuracy, underperform against unseen variants due to poor semantic pattern capture. This paper presents a semantic enrichment framework using contextual features to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Analyze the shortcomings in current graph-based malware classifiers, especially their inability to generalize well in real-world scenarios with unseen malware variants.

Method: Introduced a semantic enrichment framework combining function call graphs with contextual features, such as function metadata and code embeddings, compatible even under inconsistent real-world feature availability.

Result: Demonstrated up to 8% improved classification performance under distribution shifts and consistent enhancement in robustness in evolving malware detection scenarios using new benchmarks.

Conclusion: The proposed framework offers a practical solution for resilient malware detection, enabling improved generalization under domain shifts and temporal evolution of threats.

Abstract: Graph-based malware classifiers can achieve over 94% accuracy on standard
Android datasets, yet we find they suffer accuracy drops of up to 45% when
evaluated on previously unseen malware variants from the same family - a
scenario where strong generalization would typically be expected. This
highlights a key limitation in existing approaches: both the model
architectures and their structure-only representations often fail to capture
deeper semantic patterns. In this work, we propose a robust semantic enrichment
framework that enhances function call graphs with contextual features,
including function-level metadata and, when available, code embeddings derived
from large language models. The framework is designed to operate under
real-world constraints where feature availability is inconsistent, and supports
flexible integration of semantic signals. To evaluate generalization under
realistic domain and temporal shifts, we introduce two new benchmarks:
MalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware family
partitioning to simulate cross-family generalization and evolving threat
behavior. Experiments across multiple graph neural network backbones show that
our method improves classification performance by up to 8% under distribution
shift and consistently enhances robustness when integrated with
adaptation-based methods. These results offer a practical path toward building
resilient malware detection systems in evolving threat environments.

</details>


### [773] [Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems](https://arxiv.org/abs/2508.07263)
*Qingyuan Zeng,Shu Jiang,Jiajing Lin,Zhenzhong Wang,Kay Chen Tan,Min Jiang*

Main category: cs.CR

TL;DR: The paper introduces GMEA, a black-box attack framework targeting digital watermarking in 3D Gaussian Splatting models, effectively removing watermarks while preserving visual fidelity.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to explore and address the uninvestigated robustness of watermarking systems used for copyright protection in 3D Gaussian Splatting models against possible attacks.

Method: The proposed Group-based Multi-objective Evolutionary Attack (GMEA) framework uses a large-scale multi-objective optimization method combining watermark removal and visual quality preservation. It employs an indirect objective function and group-based optimization to minimize feature map informativeness and partition the large search space.

Result: Experiments show that GMEA successfully removes both 1D and 2D watermarks from popular 3DGS methods while ensuring high visual quality.

Conclusion: This paper reveals vulnerabilities in current 3DGS copyright protection techniques and emphasizes the need for developing more robust watermarking approaches.

Abstract: With the rise of 3D Gaussian Splatting (3DGS), a variety of digital
watermarking techniques, embedding either 1D bitstreams or 2D images, are used
for copyright protection. However, the robustness of these watermarking
techniques against potential attacks remains underexplored. This paper
introduces the first universal black-box attack framework, the Group-based
Multi-objective Evolutionary Attack (GMEA), designed to challenge these
watermarking systems. We formulate the attack as a large-scale multi-objective
optimization problem, balancing watermark removal with visual quality. In a
black-box setting, we introduce an indirect objective function that blinds the
watermark detector by minimizing the standard deviation of features extracted
by a convolutional network, thus rendering the feature maps uninformative. To
manage the vast search space of 3DGS models, we employ a group-based
optimization strategy to partition the model into multiple, independent
sub-optimization problems. Experiments demonstrate that our framework
effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking
methods while maintaining high visual fidelity. This work reveals critical
vulnerabilities in existing 3DGS copyright protection schemes and calls for the
development of more robust watermarking systems.

</details>


### [774] [Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks](https://arxiv.org/abs/2508.08029)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: The paper explores the use of Large Language Models (LLMs) for anomaly detection in O-RAN, showing they are robust against adversarial Unicode manipulations but need accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address security vulnerabilities in O-RAN's Shared Data Layer caused by malicious xApps manipulating data with subtle Unicode changes (hypoglyphs), bypassing traditional ML-based anomaly detection methods.

Method: Large Language Models (LLMs) were tested as anomaly detection systems within O-RAN architecture, assessing their capabilities to handle adversarial Unicode alterations and operational performance.

Result: LLMs processed manipulated messages without crashing, demonstrated low latency (~0.07 seconds), and showcased robustness against hypoglyph attacks, albeit with room for accuracy improvements.

Conclusion: LLMs show promise for robust and efficient anomaly detection in O-RAN platforms, but their detection accuracy needs refinement, potentially through prompt engineering and further research.

Abstract: The introduction of 5G and the Open Radio Access Network (O-RAN) architecture
has enabled more flexible and intelligent network deployments. However, the
increased complexity and openness of these architectures also introduce novel
security challenges, such as data manipulation attacks on the semi-standardised
Shared Data Layer (SDL) within the O-RAN platform through malicious xApps. In
particular, malicious xApps can exploit this vulnerability by introducing
subtle Unicode-wise alterations (hypoglyphs) into the data that are being used
by traditional machine learning (ML)-based anomaly detection methods. These
Unicode-wise manipulations can potentially bypass detection and cause failures
in anomaly detection systems based on traditional ML, such as AutoEncoders,
which are unable to process hypoglyphed data without crashing. We investigate
the use of Large Language Models (LLMs) for anomaly detection within the O-RAN
architecture to address this challenge. We demonstrate that LLM-based xApps
maintain robust operational performance and are capable of processing
manipulated messages without crashing. While initial detection accuracy
requires further improvements, our results highlight the robustness of LLMs to
adversarial attacks such as hypoglyphs in input data. There is potential to use
their adaptability through prompt engineering to further improve the accuracy,
although this requires further research. Additionally, we show that LLMs
achieve low detection latency (under 0.07 seconds), making them suitable for
Near-Real-Time (Near-RT) RIC deployments.

</details>


### [775] [IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2508.08031)
*Jiayao Wang,Yang Song,Zhendong Zhao,Jiale Zhang,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: The paper proposes IPBA, an effective and imperceptible backdoor attack method tailored for Federated Self-Supervised Learning (FSSL), aiming to enhance stealth and practicality in real-world settings.


<details>
  <summary>Details</summary>
Motivation: FSSL is increasingly used for decentralized modeling and representation learning but remains vulnerable to backdoor attacks, particularly stealthy triggers suited for realistic environments.

Method: IPBA addresses existing issues like limited transferability and feature entanglement by decoupling feature distributions between backdoor and augmented samples and using Sliced-Wasserstein distance to optimize trigger generation.

Result: Experiments on various scenarios and datasets demonstrate that IPBA surpasses traditional backdoor attack methods both in effectiveness and robustness against defenses.

Conclusion: IPBA presents a sophisticated and superior approach to stealthy backdoor attacks in FSSL, overcoming traditional limitations while maintaining robustness and performance.

Abstract: Federated self-supervised learning (FSSL) combines the advantages of
decentralized modeling and unlabeled representation learning, serving as a
cutting-edge paradigm with strong potential for scalability and privacy
preservation. Although FSSL has garnered increasing attention, research
indicates that it remains vulnerable to backdoor attacks. Existing methods
generally rely on visually obvious triggers, which makes it difficult to meet
the requirements for stealth and practicality in real-world deployment. In this
paper, we propose an imperceptible and effective backdoor attack method against
FSSL, called IPBA. Our empirical study reveals that existing imperceptible
triggers face a series of challenges in FSSL, particularly limited
transferability, feature entanglement with augmented samples, and
out-of-distribution properties. These issues collectively undermine the
effectiveness and stealthiness of traditional backdoor attacks in FSSL. To
overcome these challenges, IPBA decouples the feature distributions of backdoor
and augmented samples, and introduces Sliced-Wasserstein distance to mitigate
the out-of-distribution properties of backdoor samples, thereby optimizing the
trigger generation process. Our experimental results on several FSSL scenarios
and datasets show that IPBA significantly outperforms existing backdoor attack
methods in performance and exhibits strong robustness under various defense
mechanisms.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [776] [BIGBOY1.2: Generating Realistic Synthetic Data for Disease Outbreak Modelling and Analytics](https://arxiv.org/abs/2508.07239)
*Raunak Narwal,Syed Abbas*

Main category: q-bio.PE

TL;DR: BIGBOY1.2 is a synthetic dataset generator designed for benchmarking disease outbreak modelling and forecasting, supporting SEIR/SIR models and modern machine learning techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in disease outbreak modelling due to incomplete data, noise, and lack of standardized datasets.

Method: BIGBOY1.2 generates configurable epidemic time series using compartmental logic, custom seasonality, and noise injection to simulate realistic reporting issues.

Result: BIGBOY1.2 produces diverse datasets suitable for evaluating traditional epidemiological models and modern machine learning methods.

Conclusion: BIGBOY1.2 is a useful tool for researchers to create synthetic datasets that improve comparison and evaluation of various disease modelling approaches.

Abstract: Modelling disease outbreak models remains challenging due to incomplete
surveillance data, noise, and limited access to standardized datasets. We have
created BIGBOY1.2, an open synthetic dataset generator that creates
configurable epidemic time series and population-level trajectories suitable
for benchmarking modelling, forecasting, and visualisation. The framework
supports SEIR and SIR-like compartmental logic, custom seasonality, and noise
injection to mimic real reporting artifacts. BIGBOY1.2 can produce datasets
with diverse characteristics, making it suitable for comparing traditional
epidemiological models (e.g., SIR, SEIR) with modern machine learning
approaches (e.g., SVM, neural networks).

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [777] [An effective potential for generative modelling with active matter](https://arxiv.org/abs/2508.08146)
*Adrian Baule*

Main category: cond-mat.stat-mech

TL;DR: The paper introduces a new method for implementing generative diffusion models using active particle processes, avoiding previous dependencies on velocity-based score functions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve generative diffusion models by introducing a new mechanism based on active particle dynamics with finite correlation time, simplifying and potentially enhancing the time-reversal process.

Method: The method involves using an effective time-dependent potential imposed on the position coordinates, avoiding direct reliance on velocity-based score functions. The force field is determined using the standard score function and its second-order derivatives.

Result: Numerical experiments demonstrate the validity of the proposed effective potential in representing artificial data distributions.

Conclusion: The paper provides a novel approach for implementing score-based diffusion models, showing that effective potential can simplify the implementation and maintain accuracy.

Abstract: Score-based diffusion models generate samples from a complex underlying data
distribution by time-reversal of a diffusion process and represent the
state-of-the-art in many generative AI applications such as artificial image
synthesis. Here, I show how a generative diffusion model can be implemented
based on an underlying active particle process with finite correlation time. In
contrast to previous approaches that use a score function acting on the
velocity coordinate of the active particle, time reversal is here achieved by
imposing an effective time-dependent potential on the position coordinate only.
The effective potential is valid to first order in the persistence time and
leads to a force field that is fully determined by the standard score function
and its derivatives up to 2nd order. Numerical experiments for artificial data
distributions confirm the validity of the effective potential.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [778] [LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning](https://arxiv.org/abs/2508.06799)
*Naiyi Li,Zihui Ma,Runlong Yu,Lingyao Li*

Main category: cs.ET

TL;DR: This paper introduces LSDTs, a framework combining Large Language Models (LLMs) and Digital Twins to extract, organize, and utilize unstructured planning knowledge for complex infrastructure systems.


<details>
  <summary>Details</summary>
Motivation: Digital Twins suffer from limited ability to integrate unstructured knowledge, and advances in LLMs offer potential solutions for extracting and organizing this type of data.

Method: The authors propose a framework, LSDTs, where LLMs extract information from unstructured documents and organize it into a formal ontology that serves as a semantic layer for Digital Twins.

Result: In a case study of offshore wind farm planning during Hurricane Sandy, LSDTs demonstrated effectiveness in regulation-aware layout optimization, high-fidelity simulations, and enhanced infrastructure planning adaptability.

Conclusion: Combining generative AI with Digital Twins empowers complex, knowledge-driven planning tasks, showcasing the value of integrating LLMs into infrastructure management frameworks.

Abstract: Digital Twins (DTs) offer powerful tools for managing complex infrastructure
systems, but their effectiveness is often limited by challenges in integrating
unstructured knowledge. Recent advances in Large Language Models (LLMs) bring
new potential to address this gap, with strong abilities in extracting and
organizing diverse textual information. We therefore propose LSDTs
(LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract
planning knowledge from unstructured documents like environmental regulations
and technical guidelines, and organize it into a formal ontology. This ontology
forms a semantic layer that powers a digital twin-a virtual model of the
physical system-allowing it to simulate realistic, regulation-aware planning
scenarios. We evaluate LSDTs through a case study of offshore wind farm
planning in Maryland, including its application during Hurricane Sandy. Results
demonstrate that LSDTs support interpretable, regulation-aware layout
optimization, enable high-fidelity simulation, and enhance adaptability in
infrastructure planning. This work shows the potential of combining generative
AI with digital twins to support complex, knowledge-driven planning tasks.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [779] [A Tight Lower Bound for the Approximation Guarantee of Higher-Order Singular Value Decomposition](https://arxiv.org/abs/2508.06693)
*Matthew Fahrbach,Mehrdad Ghadiri*

Main category: cs.DS

TL;DR: The paper proves that the approximation guarantees of HOSVD, ST-HOSVD, and HOOI algorithms for tensor decomposition are tight and cannot be improved.


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze the theoretical tightness of approximation guarantees for widely used tensor decomposition algorithms like HOSVD, ST-HOSVD, and HOOI.

Method: The authors construct specific tensors to empirically demonstrate and mathematically prove the worst-case approximation ratio for each algorithm.

Result: All three algorithms, HOSVD, ST-HOSVD, and HOOI, are shown to achieve their worst-case approximation ratios of $N / (1 + \varepsilon)$, matching the previously known upper bounds.

Conclusion: The approximation guarantees of these tensor decomposition methods are tight, confirming that their worst-case performance cannot be improved theoretically.

Abstract: We prove that the classic approximation guarantee for the higher-order
singular value decomposition (HOSVD) is tight by constructing a tensor for
which HOSVD achieves an approximation ratio of $N/(1+\varepsilon)$, for any
$\varepsilon > 0$. This matches the upper bound of De Lathauwer et al. (2000a)
and shows that the approximation ratio of HOSVD cannot be improved. Using a
more advanced construction, we also prove that the approximation guarantees for
the ST-HOSVD algorithm of Vannieuwenhoven et al. (2012) and higher-order
orthogonal iteration (HOOI) of De Lathauwer et al. (2000b) are tight by showing
that they can achieve their worst-case approximation ratio of $N / (1 +
\varepsilon)$, for any $\varepsilon > 0$.

</details>


### [780] [Optimizing Districting Plans to Maximize Majority-Minority Districts via IPs and Local Search](https://arxiv.org/abs/2508.07446)
*Daniel Brous,David Shmoys*

Main category: cs.DS

TL;DR: This paper introduces an integer programming-based method for generating districting plans with more majority-minority districts compared to existing algorithms.


<details>
  <summary>Details</summary>
Motivation: To enhance the effectiveness of Voting Rights Act enforcement by generating districting plans with a greater number of majority-minority districts.

Method: The authors propose an integer programming method built upon the stochastic hierarchical partitioning algorithm. They introduce a column generation algorithm and additional algorithms to improve compactness and local re-optimization of districts.

Result: Their method outperformed the existing 'short bursts' algorithm by generating more statewide plans with a higher count of majority-minority districts across multiple data sets.

Conclusion: The integer programming framework provides a more effective approach for generating majority-minority districting plans, advancing fairness and optimization in redistricting.

Abstract: In redistricting litigation, effective enforcement of the Voting Rights Act
has often involved providing the court with districting plans that display a
larger number of majority-minority districts than the current proposal (as was
true, for example, in what followed Allen v. Milligan concerning the
congressional districting plan for Alabama in 2023). Recent work by Cannon et
al. proposed a heuristic algorithm for generating plans to optimize
majority-minority districts, which they called short bursts; that algorithm
relies on a sophisticated random walk over the space of all plans,
transitioning in bursts, where the initial plan for each burst is the most
successful plan from the previous burst. We propose a method based on integer
programming, where we build upon another previous work, the stochastic
hierarchical partitioning algorithm, which heuristically generates a robust set
of potential districts (viewed as columns in a standard set partitioning
formulation); that approach was designed to optimize a different notion of
fairness across a statewide plan. We design a new column generation algorithm
to find plans via integer programming that outperforms short bursts on multiple
data sets in generating statewide plans with significantly more
majority-minority districts. These results also rely on a new local
re-optimization algorithm to iteratively improve on any baseline solution, as
well as an algorithm to increase the compactness of districts in plans
generated (without impacting the number of majority-minority districts).

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [781] [Reconstruction of Solar EUV Irradiance Using CaII K Images and SOHO/SEM Data with Bayesian Deep Learning and Uncertainty Quantification](https://arxiv.org/abs/2508.07065)
*Haodi Jiang,Qin Li,Jason T. L. Wang,Haimin Wang,Serena Criscuoli*

Main category: astro-ph.SR

TL;DR: This paper introduces SEMNet, a Bayesian deep learning model, to reconstruct missing solar EUV flux data using historical CaII K images, filling significant gaps in long-term data over multiple solar cycles.


<details>
  <summary>Details</summary>
Motivation: There is a lack of understanding and data on the long-term evolution of solar EUV flux, which significantly impacts Earth's atmosphere, as continuous measurements have only been available since 1995.

Method: The study proposes and validates a Bayesian deep learning model, SEMNet, which uses CaII K images as a proxy to reconstruct past solar EUV flux data.

Result: SEMNet accurately reconstructed SOHO/SEM EUV measurements from 1998–2014 and extended its utility to reconstruct data for 1950–1960 using transfer learning, providing reliable predictions with uncertainty bounds.

Conclusion: The study demonstrates the viability of historical CaII K images as a proxy for reconstructing long-term EUV flux, offering deeper insights into solar impacts on Earth's climate.

Abstract: Solar extreme ultraviolet (EUV) irradiance plays a crucial role in heating
the Earth's ionosphere, thermosphere, and mesosphere, affecting atmospheric
dynamics over varying time scales. Although significant effort has been spent
studying short-term EUV variations from solar transient events, there is little
work to explore the long-term evolution of the EUV flux over multiple solar
cycles. Continuous EUV flux measurements have only been available since 1995,
leaving significant gaps in earlier data. In this study, we propose a Bayesian
deep learning model, named SEMNet, to fill the gaps. We validate our approach
by applying SEMNet to construct SOHO/SEM EUV flux measurements in the period
between 1998 and 2014 using CaII K images from the Precision Solar Photometric
Telescope. We then extend SEMNet through transfer learning to reconstruct solar
EUV irradiance in the period between 1950 and 1960 using CaII K images from the
Kodaikanal Solar Observatory. Experimental results show that SEMNet provides
reliable predictions along with uncertainty bounds, demonstrating the
feasibility of CaII K images as a robust proxy for long-term EUV fluxes. These
findings contribute to a better understanding of solar influences on Earth's
climate over extended periods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [782] [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050)
*Wenhan Liu,Xinyu Ma,Weiwei Sun,Yutao Zhu,Yuchen Li,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: ReasonRank significantly improves listwise ranking performance by introducing reasoning-intensive training data synthesis and a novel two-stage post-training approach.


<details>
  <summary>Details</summary>
Motivation: Current rerankers struggle in complex ranking scenarios due to limited reasoning-intensive training data.

Method: Automated data synthesis is combined with a two-stage post-training approach: supervised fine-tuning for reasoning patterns and reinforcement learning with multi-view ranking rewards.

Result: ReasonRank outperforms baselines, achieves state-of-the-art performance of 40.6 on the BRIGHT leaderboard, and offers lower latency compared to pointwise reranker Rank1.

Conclusion: ReasonRank demonstrates strong reasoning-based ranking capabilities and sets new benchmarks in listwise ranking tasks.

Abstract: Large Language Model (LLM) based listwise ranking has shown superior
performance in many passage ranking tasks. With the development of Large
Reasoning Models, many studies have demonstrated that step-by-step reasoning
during test-time helps improve listwise ranking performance. However, due to
the scarcity of reasoning-intensive training data, existing rerankers perform
poorly in many complex ranking scenarios and the ranking ability of
reasoning-intensive rerankers remains largely underdeveloped. In this paper, we
first propose an automated reasoning-intensive training data synthesis
framework, which sources training queries and passages from diverse domains and
applies DeepSeek-R1 to generate high-quality training labels. A
self-consistency data filtering mechanism is designed to ensure the data
quality. To empower the listwise reranker with strong reasoning ability, we
further propose a two-stage post-training approach, which includes a cold-start
supervised fine-tuning (SFT) stage for reasoning pattern learning and a
reinforcement learning (RL) stage for further ranking ability enhancement.
During the RL stage, based on the nature of listwise ranking, we design a
multi-view ranking reward, which is more effective than a ranking metric-based
reward. Extensive experiments demonstrate that our trained reasoning-intensive
reranker \textbf{ReasonRank} outperforms existing baselines significantly and
also achieves much lower latency than pointwise reranker Rank1. \textbf{Through
further experiments, our ReasonRank has achieved state-of-the-art (SOTA)
performance 40.6 on the BRIGHT
leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are
available at https://github.com/8421BCD/ReasonRank.

</details>


### [783] [PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization](https://arxiv.org/abs/2508.07342)
*Kepu Zhang,Teng Shi,Weijie Yu,Jun Xu*

Main category: cs.IR

TL;DR: The paper introduces PrLM, a framework utilizing reinforcement learning for personalized text generation by enabling language models to reason over retrieved user profiles explicitly.


<details>
  <summary>Details</summary>
Motivation: Existing methods for personalized retrieval-augmented generation rely on improving retrieval and implicitly integrating user profiles, which can result in responses misaligned with user preferences.

Method: The proposed PrLM framework employs reinforcement learning with a personalization reward model trained via contrastive learning, allowing explicit reasoning over user profiles without requiring annotated reasoning paths.

Result: PrLM demonstrated improved performance in personalized text generation across three datasets, showing robustness in varying retrieval conditions.

Conclusion: PrLM effectively enhances the alignment of generated responses with user preferences by explicitly reasoning over user profiles, outperforming existing methods.

Abstract: Personalized retrieval-augmented generation (RAG) aims to produce
user-tailored responses by incorporating retrieved user profiles alongside the
input query. Existing methods primarily focus on improving retrieval and rely
on large language models (LLMs) to implicitly integrate the retrieved context
with the query. However, such models are often sensitive to retrieval quality
and may generate responses that are misaligned with user preferences. To
address this limitation, we propose PrLM, a reinforcement learning framework
that trains LLMs to explicitly reason over retrieved user profiles. Guided by a
contrastively trained personalization reward model, PrLM effectively learns
from user responses without requiring annotated reasoning paths. Experiments on
three personalized text generation datasets show that PrLM outperforms existing
methods and remains robust across varying numbers of retrieved profiles and
different retrievers.

</details>


### [784] [BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation](https://arxiv.org/abs/2508.06781)
*Christos Tsirigotis,Vaibhav Adlakha,Joao Monteiro,Aaron Courville,Perouz Taslakian*

Main category: cs.IR

TL;DR: BiXSE is a new method for dense retrieval using fine-grained graded relevance scores generated by large language models (LLMs) to improve retrieval performance with reduced costs.


<details>
  <summary>Details</summary>
Motivation: Real-world relevance is often on a continuum, requiring a shift from binary labels to graded relevance scores, which can be generated at scale using LLMs.

Method: BiXSE optimizes binary cross-entropy over LLM-generated graded relevance scores, interpreting the scores as probabilistic targets for training dense retrieval models efficiently.

Result: BiXSE outperformed softmax-based contrastive learning and matched or exceeded strong pairwise ranking baselines while reducing annotation and compute costs.

Conclusion: BiXSE is a scalable and effective alternative for dense retrieval model training as LLM-generated graded relevance labels become more accessible.

Abstract: Neural sentence embedding models for dense retrieval typically rely on binary
relevance labels, treating query-document pairs as either relevant or
irrelevant. However, real-world relevance often exists on a continuum, and
recent advances in large language models (LLMs) have made it feasible to scale
the generation of fine-grained graded relevance labels. In this work, we
propose BiXSE, a simple and effective pointwise training method that optimizes
binary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSE
interprets these scores as probabilistic targets, enabling granular supervision
from a single labeled query-document pair per query. Unlike pairwise or
listwise losses that require multiple annotated comparisons per query, BiXSE
achieves strong performance with reduced annotation and compute costs by
leveraging in-batch negatives. Extensive experiments across sentence embedding
(MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistently
outperforms softmax-based contrastive learning (InfoNCE), and matches or
exceeds strong pairwise ranking baselines when trained on LLM-supervised data.
BiXSE offers a robust, scalable alternative for training dense retrieval models
as graded relevance supervision becomes increasingly accessible.

</details>


### [785] [Improving Document Retrieval Coherence for Semantically Equivalent Queries](https://arxiv.org/abs/2508.07975)
*Stefano Campese,Alessandro Moschitti,Ivano Lauriola*

Main category: cs.IR

TL;DR: The paper proposes an enhanced loss function to improve dense retrieval model coherence and accuracy.


<details>
  <summary>Details</summary>
Motivation: Dense retrieval models often fail to maintain consistency when handling semantically equivalent queries.

Method: Introduced a modified Multi-Negative Ranking loss that penalizes incoherence in top-k retrieval for semantically similar queries.

Result: Experiments across several benchmarks (e.g., MS-MARCO, BEIR) demonstrate reduced sensitivity and improved accuracy in retrieval models.

Conclusion: Adapting the proposed loss function can enhance both the stability and performance of dense retrieval systems.

Abstract: Dense Retrieval (DR) models have proven to be effective for Document
Retrieval and Information Grounding tasks. Usually, these models are trained
and optimized for improving the relevance of top-ranked documents for a given
query. Previous work has shown that popular DR models are sensitive to the
query and document lexicon: small variations of it may lead to a significant
difference in the set of retrieved documents. In this paper, we propose a
variation of the Multi-Negative Ranking loss for training DR that improves the
coherence of models in retrieving the same documents with respect to
semantically similar queries. The loss penalizes discrepancies between the
top-k ranked documents retrieved for diverse but semantic equivalent queries.
We conducted extensive experiments on various datasets, MS-MARCO, Natural
Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes
by our loss are subject to lower sensitivity, and, (ii) interestingly, higher
accuracy.

</details>


### [786] [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches](https://arxiv.org/abs/2508.08088)
*Jiejun Tan,Zhicheng Dou,Yan Yu,Jiehan Cheng,Qiang Ju,Jian Xie,Ji-Rong Wen*

Main category: cs.IR

TL;DR: The paper introduces HierSearch, a hierarchical agentic framework for deep search using local and Web corpus, and showcases its effectiveness in multiple domains.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing deep search models confined to single knowledge sources and inefficiencies of flat RL in multi-modal deep search.

Method: HierSearch utilizes hierarchical RL with distinct low-level agents for local and Web searches, coordinated by a high-level planner, alongside a knowledge refiner to prevent error propagation.

Result: HierSearch performs better than flat RL and other baselines across six benchmarks in general, finance, and medical domains.

Conclusion: HierSearch enhances deep search capabilities for enterprises by combining hierarchical RL and a knowledge refiner strategy, achieving state-of-the-art performance in multi-source retrieval.

Abstract: Recently, large reasoning models have demonstrated strong mathematical and
coding abilities, and deep search leverages their reasoning capabilities in
challenging information retrieval tasks. Existing deep search works are
generally limited to a single knowledge source, either local or the Web.
However, enterprises often require private deep search systems that can
leverage search tools over both local and the Web corpus. Simply training an
agent equipped with multiple search tools using flat reinforcement learning
(RL) is a straightforward idea, but it has problems such as low training data
efficiency and poor mastery of complex tools. To address the above issue, we
propose a hierarchical agentic deep search framework, HierSearch, trained with
hierarchical RL. At the low level, a local deep search agent and a Web deep
search agent are trained to retrieve evidence from their corresponding domains.
At the high level, a planner agent coordinates low-level agents and provides
the final answer. Moreover, to prevent direct answer copying and error
propagation, we design a knowledge refiner that filters out hallucinations and
irrelevant evidence returned by low-level agents. Experiments show that
HierSearch achieves better performance compared to flat RL, and outperforms
various deep search and multi-source retrieval-augmented generation baselines
in six benchmarks across general, finance, and medical domains.

</details>


### [787] [CLAP: Coreference-Linked Augmentation for Passage Retrieval](https://arxiv.org/abs/2508.06941)
*Huanwei Xu,Lin Xu,Liang Yuan*

Main category: cs.IR

TL;DR: The paper introduces CLAP, a framework using large language models (LLMs) to enhance passage retrieval by resolving semantic drift and noise issues, yielding substantial improvements across domains and out-of-domain settings.


<details>
  <summary>Details</summary>
Motivation: Passage retrieval often suffers from semantic drift and misalignment when integrating LLM-based expansion with dense retrievers, as well as from noise introduced by irrelevant parts of the passage and continuity issues caused by chunking. The study seeks to address these challenges for consistent and domain-agnostic performance.

Method: The proposed method, CLAP, segments passages into coherent chunks, resolves coreference chains, and generates localized pseudo-queries that are semantically aligned with dense retrievers. It combines global topical signals and fine-grained subtopic signals for enhanced performance.

Result: CLAP shows consistent performance improvements, including up to a 20.68% absolute nDCG@10 gain. It enables dense retrievers to match or outpace second-stage rankers and demonstrates superior performance in out-of-domain retrieval tasks compared to traditional LLM-based methods.

Conclusion: CLAP offers a lightweight and logic-centric solution for robust passage retrieval by improving alignment with dense retrievers and reducing noise, delivering strong and generalizable performance across diverse retrieval settings.

Abstract: Large Language Model (LLM)-based passage expansion has shown promise for
enhancing first-stage retrieval, but often underperforms with dense retrievers
due to semantic drift and misalignment with their pretrained semantic space.
Beyond this, only a portion of a passage is typically relevant to a query,
while the rest introduces noise--an issue compounded by chunking techniques
that break coreference continuity. We propose Coreference-Linked Augmentation
for Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that
segments passages into coherent chunks, resolves coreference chains, and
generates localized pseudo-queries aligned with dense retriever
representations. A simple fusion of global topical signals and fine-grained
subtopic signals achieves robust performance across domains. CLAP yields
consistent gains even as retriever strength increases, enabling dense
retrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,
with up to 20.68% absolute nDCG@10 improvement. These improvements are
especially notable in out-of-domain settings, where conventional LLM-based
expansion methods relying on domain knowledge often falter. CLAP instead adopts
a logic-centric pipeline that enables robust, domain-agnostic generalization.

</details>


### [788] [Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation](https://arxiv.org/abs/2508.07223)
*Guanchen Wang,Mingming Ha,Tianbao Ma,Linxun Chen,Zhaojie Liu,Guorui Zhou,Kun Gai*

Main category: cs.IR

TL;DR: This paper introduces a Knowledge Selection & Exploitation Recommendation (KSER) framework to address issues of hallucination, redundancy, and homogenization in using large language models (LLMs) for recommendation tasks, achieving effective knowledge filtering, alignment, and adaptation.


<details>
  <summary>Details</summary>
Motivation: To leverage the reasoning and generalization capabilities of large language models (LLMs) to improve recommendation systems while addressing limitations like hallucination, redundancy, and homogenization of derived world knowledge.

Method: The authors propose the KSER framework, featuring a knowledge filtering module with an Embedding Selection Filter Network (ESFNet) for adaptive weighting of knowledge chunks and an attention-based alignment module to harmonize LLM embeddings with recommendation model feature spaces. Two training strategies, all-parameters training and extractor-only training, are included to adapt to various downstream tasks.

Result: The experimental results validate the importance of knowledge filtering and semantic alignment, demonstrating that the KSER framework enhances performance. The extractor-only training strategy is additionally showcased as efficient and effective for knowledge-augmented recommendation.

Conclusion: The KSER framework successfully addresses critical challenges in incorporating LLM-derived knowledge for recommendation systems, improving both knowledge quality and alignment while offering flexible training options for diverse applications.

Abstract: In recent years, there has been growing interest in leveraging the impressive
generalization capabilities and reasoning ability of large language models
(LLMs) to improve the performance of recommenders. With this operation,
recommenders can access and learn the additional world knowledge and reasoning
information via LLMs. However, in general, for different users and items, the
world knowledge derived from LLMs suffers from issues of hallucination, content
redundant, and information homogenization. Directly feeding the generated
response embeddings into the recommendation model can lead to unavoidable
performance deterioration. To address these challenges, we propose a Knowledge
Selection \& Exploitation Recommendation (KSER) framework, which effectively
select and extracts the high-quality knowledge from LLMs. The framework
consists of two key components: a knowledge filtering module and a embedding
spaces alignment module. In the knowledge filtering module, a Embedding
Selection Filter Network (ESFNet) is designed to assign adaptive weights to
different knowledge chunks in different knowledge fields. In the space
alignment module, an attention-based architecture is proposed to align the
semantic embeddings from LLMs with the feature space used to train the
recommendation models. In addition, two training
strategies--\textbf{all-parameters training} and \textbf{extractor-only
training}--are proposed to flexibly adapt to different downstream tasks and
application scenarios, where the extractor-only training strategy offers a
novel perspective on knowledge-augmented recommendation. Experimental results
validate the necessity and effectiveness of both the knowledge filtering and
alignment modules, and further demonstrate the efficiency and effectiveness of
the extractor-only training strategy.

</details>


### [789] [SocRipple: A Two-Stage Framework for Cold-Start Video Recommendations](https://arxiv.org/abs/2508.07241)
*Amit Jaspal,Kapil Dalwani,Ajantha Ramineni*

Main category: cs.IR

TL;DR: The paper introduces SocRipple, a two-stage retrieval framework to improve cold start item distribution on social graph platforms, with a 36% boost in distribution in experiments.


<details>
  <summary>Details</summary>
Motivation: Current recommender systems struggle with cold start challenges where new items lack interaction history, making personalized distribution difficult.

Method: The SocRipple framework utilizes a two-stage approach: Stage 1 incorporates creators' social connections for initial exposure, while Stage 2 uses early engagement signals and user embeddings for outward item distribution via KNN search.

Result: Experiments on a major video platform demonstrate that SocRipple improves cold start item distribution by 36% without compromising user engagement.

Conclusion: SocRipple provides an effective solution for balancing new item distribution with personalized recommendations, addressing cold start challenges in recommender systems.

Abstract: Most industry scale recommender systems face critical cold start challenges
new items lack interaction history, making it difficult to distribute them in a
personalized manner. Standard collaborative filtering models underperform due
to sparse engagement signals, while content only approaches lack user specific
relevance. We propose SocRipple, a novel two stage retrieval framework tailored
for coldstart item distribution in social graph based platforms. Stage 1
leverages the creators social connections for targeted initial exposure. Stage
2 builds on early engagement signals and stable user embeddings learned from
historical interactions to "ripple" outwards via K Nearest Neighbor (KNN)
search. Large scale experiments on a major video platform show that SocRipple
boosts cold start item distribution by +36% while maintaining user engagement
rate on cold start items, effectively balancing new item exposure with
personalized recommendations.

</details>


### [790] [Recommendation Is a Dish Better Served Warm](https://arxiv.org/abs/2508.07856)
*Danil Gusak,Nikita Sukhorukov,Evgeny Frolov*

Main category: cs.IR

TL;DR: The paper investigates the arbitrary and inconsistent use of cold-start thresholds in recommender systems and examines its impact on data and results' reliability.


<details>
  <summary>Details</summary>
Motivation: To address inconsistencies and arbitrariness in determining cold-start thresholds in recommender systems, which affect reliability and comparability of experiments.

Method: Conducting experiments by varying the number of interactions for items and the length of user interaction history on multiple datasets and baselines.

Result: Demonstrated that inconsistent thresholds could lead to either the loss of valuable data or misclassification, introducing noise.

Conclusion: Using arbitrary thresholds compromises data quality and system reliability, emphasizing the need for a more systematic approach to define cold-start boundaries.

Abstract: In modern recommender systems, experimental settings typically include
filtering out cold users and items based on a minimum interaction threshold.
However, these thresholds are often chosen arbitrarily and vary widely across
studies, leading to inconsistencies that can significantly affect the
comparability and reliability of evaluation results. In this paper, we
systematically explore the cold-start boundary by examining the criteria used
to determine whether a user or an item should be considered cold. Our
experiments incrementally vary the number of interactions for different items
during training, and gradually update the length of user interaction histories
during inference. We investigate the thresholds across several widely used
datasets, commonly represented in recent papers from top-tier conferences, and
on multiple established recommender baselines. Our findings show that
inconsistent selection of cold-start thresholds can either result in the
unnecessary removal of valuable data or lead to the misclassification of cold
instances as warm, introducing more noise into the system.

</details>


### [791] [DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval](https://arxiv.org/abs/2508.07995)
*Meixiu Long,Duolin Sun,Dan Yang,Junjie Wang,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu,Jiahai Wang*

Main category: cs.IR

TL;DR: DIVER is a retrieval pipeline targeting reasoning-intensive information queries, improving performance by leveraging document processing, LLM-driven query expansion, a custom retriever, and reranking.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current retrievers struggling with abstract reasoning, analogical thinking, and multi-step inference queries.

Method: Includes document processing, iterative LLM-driven query expansion, reasoning-enhanced retriever fine-tuned on synthetic data, and a pointwise reranker combining retrieval and helpfulness scores.

Result: Obtained state-of-the-art performance on BRIGHT benchmark with nDCG@10 scores of 41.6 and 28.9, outperforming baseline models.

Conclusion: Reasoning-aware retrieval strategies, such as DIVER, are effective for handling complex real-world tasks, improving retrieval model accuracy significantly.

Abstract: Retrieval-augmented generation has achieved strong performance on
knowledge-intensive tasks where query-document relevance can be identified
through direct lexical or semantic matches. However, many real-world queries
involve abstract reasoning, analogical thinking, or multi-step inference, which
existing retrievers often struggle to capture. To address this challenge, we
present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive
information retrieval. DIVER consists of four components: document processing
to improve input quality, LLM-driven query expansion via iterative document
interaction, a reasoning-enhanced retriever fine-tuned on synthetic
multi-domain data with hard negatives, and a pointwise reranker that combines
LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,
DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original
queries, consistently outperforming competitive reasoning-aware models. These
results demonstrate the effectiveness of reasoning-aware retrieval strategies
in complex real-world tasks. Our code and retrieval model will be released
soon.

</details>


### [792] [Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation](https://arxiv.org/abs/2508.08042)
*Van-Khang Nguyen,Duc-Hoang Pham,Huy-Son Nguyen,Cam-Van Thi Nguyen,Hoang-Quynh Le,Duc-Trong Le*

Main category: cs.IR

TL;DR: The paper introduces MAMEX, a new framework for multimodal cold-start recommendations that dynamically integrates latent representations from multiple modalities, outperforming existing methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of cold-start scenarios in recommendation systems by leveraging insights from multimodal data, which are often underutilized by simplistic integration approaches.

Method: The paper proposes a Mixture of Experts (MoE) framework called MAMEX, featuring modality-specific expert networks and a learnable gating mechanism to dynamically weight modality contributions based on content characteristics.

Result: Experiments on benchmark datasets indicate that MAMEX provides superior accuracy and adaptability compared to state-of-the-art methods in multimodal cold-start recommendation settings.

Conclusion: MAMEX effectively improves cold-start recommendation performance by emphasizing informative modalities and remaining robust to irrelevant or missing modalities, showcasing its potential for real-world applications.

Abstract: Recommendation systems have faced significant challenges in cold-start
scenarios, where new items with a limited history of interaction need to be
effectively recommended to users. Though multimodal data (e.g., images, text,
audio, etc.) offer rich information to address this issue, existing approaches
often employ simplistic integration methods such as concatenation, average
pooling, or fixed weighting schemes, which fail to capture the complex
relationships between modalities. Our study proposes a novel Mixture of Experts
(MoE) framework for multimodal cold-start recommendation, named MAMEX, which
dynamically leverages latent representation from different modalities. MAMEX
utilizes modality-specific expert networks and introduces a learnable gating
mechanism that adaptively weights the contribution of each modality based on
its content characteristics. This approach enables MAMEX to emphasize the most
informative modalities for each item while maintaining robustness when certain
modalities are less relevant or missing. Extensive experiments on benchmark
datasets show that MAMEX outperforms state-of-the-art methods in cold-start
scenarios, with superior accuracy and adaptability. For reproducibility, the
code has been made available on Github https://github.com/L2R-UET/MAMEX.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [793] [Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures](https://arxiv.org/abs/2508.07423)
*Fotis I. Giasemis*

Main category: hep-ex

TL;DR: The paper discusses implementing a graph neural network-based pipeline for particle track reconstruction at CERN's LHCb experiment, aiming for efficient real-time data processing and better computational performance.


<details>
  <summary>Details</summary>
Motivation: With increasing collision data at CERN's LHC experiments, there is a need for real-time data filtering to improve understandings in physics while addressing computational and energy challenges.

Method: This work utilizes a graph neural network-based pipeline for charged particle track reconstruction, implemented at LHCb's first-level trigger using GPUs, and compares its performance with traditional algorithms. The pipeline was also tested on FPGAs for energy and speed metrics.

Result: The pipeline successfully demonstrated the capacity to process high-frequency data, exhibiting faster speeds and lower energy consumption when implemented on modern hardware architectures like GPUs and FPGAs.

Conclusion: Graph neural networks and modern hardware architectures provide an effective approach to handling the stringent computational demands of real-time data processing in high-energy physics experiments like those at LHCb.

Abstract: As the particle physics community needs higher and higher precisions in order
to test our current model of the subatomic world, larger and larger datasets
are necessary. With upgrades scheduled for the detectors of colliding-beam
experiments around the world, and specifically at the Large Hadron Collider at
CERN, more collisions and more complex interactions are expected. This directly
implies an increase in data produced and consequently in the computational
resources needed to process them. At CERN, the amount of data produced is
gargantuan. This is why the data have to be heavily filtered and selected in
real time before being permanently stored. This data can then be used to
perform physics analyses, in order to expand our current understanding of the
universe and improve the Standard Model of physics. This real-time filtering,
known as triggering, involves complex processing happening often at frequencies
as high as 40 MHz. This thesis contributes to understanding how machine
learning models can be efficiently deployed in such environments, in order to
maximize throughput and minimize energy consumption. Inevitably, modern
hardware designed for such tasks and contemporary algorithms are needed in
order to meet the challenges posed by the stringent, high-frequency data rates.
In this work, I present our graph neural network-based pipeline, developed for
charged particle track reconstruction at the LHCb experiment at CERN. The
pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely
on GPUs. Its performance was compared against the classical tracking algorithms
currently in production at LHCb. The pipeline was also accelerated on the FPGA
architecture, and its performance in terms of power consumption and processing
speed was compared against the GPU implementation.

</details>
