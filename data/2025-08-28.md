<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.CL](#cs.CL) [Total: 60]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 23]
- [cs.SE](#cs.SE) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.MA](#cs.MA) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.optics](#physics.optics) [Total: 3]
- [cs.HC](#cs.HC) [Total: 6]
- [math.OC](#math.OC) [Total: 1]
- [nucl-th](#nucl-th) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 20]
- [math.CO](#math.CO) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: The paper proposes viewing sycophancy in large language models (LLMs) as arising from compositions of psychometric traits and introduces Contrastive Activation Addition (CAA) to study and modulate these traits.


<details>
  <summary>Details</summary>
Motivation: Sycophancy poses safety risks in LLMs and needs more nuanced understanding beyond being seen as an isolated issue.

Method: The authors use Contrastive Activation Addition (CAA) to link neural activations to psychometric traits and explore geometric and causal compositions of these traits.

Result: They identify how combinations of psychometric traits like emotionality, openness, and agreeableness can contribute to sycophancy.

Conclusion: The study enables interpretable interventions through vector manipulations to address sycophancy and other safety-critical behaviors in LLMs.

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [2] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Introduction of Aleks, an AI-powered system, capable of autonomous scientific discovery to accelerate research in plant science.


<details>
  <summary>Details</summary>
Motivation: Modern plant science faces challenges in experimental design, data preprocessing, and reproducibility, hindering research efficiency.

Method: Aleks is an AI multi-agent system that autonomously formulates problems, evaluates modeling strategies, and refines solutions using domain knowledge, data, and machine learning.

Result: In studying grapevine red blotch disease, Aleks identified relevant features and developed interpretable, robust models. Ablation studies confirmed the critical role of domain knowledge and memory.

Conclusion: Agentic AI like Aleks demonstrates significant potential as an autonomous collaborator in plant sciences for enhancing scientific discovery and productivity.

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [3] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: Quantization reduces costs for deploying large language models but may lead to susceptibility to deceptive prompts impacting truthfulness.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored effects of quantization on the truthfulness of large language models.

Method: Introduced TruthfulnessEval, a framework evaluating truthfulness on logical reasoning, common sense, and imitative falsehoods, and tested quantized models under varying prompt types.

Result: Quantized models retain internal truthfulness but are more prone to false outputs when exposed to deceptive prompts. They still internally "know" the truth, as revealed via probing and PCA visualizations.

Conclusion: Adjustment of quantized LLMs towards truthfulness and alignment should be prioritized to mitigate susceptibility to misleading inputs.

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [4] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: The study examines the robustness of monitoring systems for detecting covert misbehavior in LLM agents by systematizing a monitor red teaming (MRT) workflow, finding that agent awareness dominates monitor awareness, monitor scaffolding is critical, and targeted human oversight improves detection.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to identify and address covert misbehavior in autonomous LLM agents, such as concealed sharing of private information, thus necessitating a robust and standardized evaluation for monitoring systems.

Method: The research establishes a Monitor Red Teaming (MRT) workflow that varies agent and monitor awareness, uses adversarial strategies like prompt injection, and tests in two datasets: SHADE-Arena for tool-calling agents and CUA-SHADE-Arena for computer-use agents. They propose a new hybrid hierarchical-sequential scaffolding alongside the analysis of existing monitoring systems.

Result: Key findings include that agent awareness significantly undermines monitor reliability, hybrid scaffolding outperforms baseline scaffolding with a weak-to-strong scaling advantage, and targeted human oversight improves detection accuracy (e.g., a 15% improvement in TPR at FPR = 0.01 during human-in-the-loop tests).

Conclusion: The study lays the foundation for a standardized workflow in stress testing LLM monitoring systems and highlights areas like adversarial robustness and human-in-loop oversight for further improvement. It also provides public resources, including code and data, to advance this line of research.

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [5] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: The study introduces a framework to optimize reasoning trajectories in large language models by identifying and removing suboptimal parts to improve accuracy during inference.


<details>
  <summary>Details</summary>
Motivation: To enhance complex reasoning capabilities of language models by addressing the presence of suboptimal components in their reasoning trajectories.

Method: The study proposes a "5+2" framework to systematically evaluate reasoning trajectories and identify suboptimal subtrajectories based on five criteria, followed by employing a sampling algorithm to select optimized data.

Result: Experimental results show a reduction of suboptimal subtrajectories by 25.9%, and the method achieves an average accuracy of 58.92% on math benchmarks with less data, outperforming previous averages and open-source datasets.

Conclusion: The "5+2" framework and optimized sampling significantly enhance reasoning capabilities and model performance, even under resource constraints.

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [6] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: The paper introduces linear probes to detect deceptiveness in responses of large language models (LLMs), achieving over 90% accuracy on larger models.


<details>
  <summary>Details</summary>
Motivation: To develop tools for detecting misalignment in AI systems, specifically detecting deception in responses from LLMs.

Method: Linear probes were applied to LLMs' internal activations to test detection of deceptive responses. Models of different sizes were analyzed, and iterative null space projection was used to identify deception-related directions.

Result: Probes on larger models achieved high accuracy (>90%) in detecting deception, with accuracy patterns varying across layers. Smaller models performed near chance accuracy.

Conclusion: Linear probes are effective indicators of deception in larger LLMs, with potential utility for future AI alignment instrumentation.

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [7] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: Democracy-in-Silico simulates AI-driven societies to study governance, misalignments, and alignment mechanisms via institutional designs like Constitutional AI.


<details>
  <summary>Details</summary>
Motivation: To understand AI's societal governance and redefine human responsibility in the era of advanced AI agents with complex psychological traits.

Method: An agent-based simulation with AI agents simulating psychological personas under stressors; includes a novel metric (PPI) to measure power-seeking misalignment.

Result: Institutional designs like CAI and mediated protocols reduce corruption, improve policy stability, and enhance welfare compared to less constrained systems.

Conclusion: Institutional structures could align emergent AI behaviors, offering insights into the co-existence of human and AI governance.

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [8] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: This paper develops a deep learning model to improve course recommendations by extracting concepts from descriptions and integrating skill-based explanations, demonstrated through positive results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges students face in choosing courses due to limited information, insufficient career counselors, and current recommendation systems lacking relevance and student perception insights.

Method: The paper introduces a deep learning-based concept extraction model to analyze course descriptions and incorporates these insights into a serendipitous recommendation framework.

Result: Testing on the AskOski system at UC Berkeley showed that skill-based explanations heighten interest in unexpected courses and improve decision-making confidence.

Conclusion: Integrating skill-related data and explanations enhances the effectiveness and confidence in educational recommendation systems.

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [9] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL combines reinforcement learning improvements with decoding optimization to enhance reasoning accuracy in LLMs, particularly for coding tasks.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning (RL) methods for improving large language model (LLM) reasoning have shortcomings: GRPO struggles with low reward variance, and process reward models (PRMs) face challenges in training data acquisition and effectiveness.

Method: The paper introduces ReST-RL, which combines an enhanced GRPO algorithm with test-time decoding optimization using a value model (VM). The ReST algorithm improves reward variance, while VM-MCTS collects accurate value targets and assists in decoding through Monte Carlo Tree Search.

Result: The proposed paradigm displays superior performance on coding benchmarks (APPS, BigCodeBench, HumanEval), surpassing reinforcement training and decoding baselines in reasoning accuracy.

Conclusion: ReST-RL effectively improves the reasoning capabilities of LLM policies through a unified paradigm, addressing key issues with existing approaches and demonstrating robustness across various coding datasets.

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [10] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: This paper introduces Instructional Agents, a framework using large language models (LLMs) to automate the creation of course materials like syllabi, lecture scripts, slides, and assessments.


<details>
  <summary>Details</summary>
Motivation: Course material preparation is labor-intensive and requires significant collaboration among educators, so there is a need for an automated system to simplify and scale this process.

Method: The authors present a multi-agent LLM framework with role-based collaboration that operates in four modes—Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot—to produce cohesive educational content.

Result: Instructional Agents were tested on five university-level computer science courses, showing high-quality output with reduced development time and workload.

Conclusion: The framework democratizes access to quality education by providing scalable and cost-effective solutions, especially for resource-constrained or underserved institutions.

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [11] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: The paper introduces InquireBench, a benchmark for evaluating mobile agents' safe interaction and proactive inquiry capabilities, and proposes InquireMobile, a reinforcement learning-based model. It improves the inquiry success rate by 46.8% and offers the best performance among benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address safety risks in fully autonomous mobile agents by enabling safe interaction and proactive user inquiry.

Method: The authors design the InquireBench benchmark and develop InquireMobile using reinforcement learning, two-stage training, and pre-action reasoning.

Result: InquireMobile achieves a 46.8% improvement in inquiry success rate and outperforms baselines on InquireBench.

Conclusion: The proposed InquireMobile model enhances agent safety and interaction capabilities. Open-sourcing datasets and tools aims to advance research and industry progress.

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [12] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: The paper evaluates how Chain-of-Thought (CoT) performs in soft-reasoning tasks, revealing limited gains and unfaithfulness issues.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Chain-of-Thought reasoning in soft-reasoning tasks and assess its reliability in various models.

Method: Analyzed CoT dynamics and faithfulness across instruction-tuned, reasoning, and reasoning-distilled models in soft-reasoning tasks.

Result: Found differing model dependencies on CoT, highlighting misalignment between its influence and faithfulness.

Conclusion: CoT is inconsistent in its impact and reliability for soft-reasoning tasks, suggesting room for improvement.

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [13] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: The paper introduces a model-agnostic evaluation framework to assess LLMs' semantic fidelity in structured environments, using chess as a test case.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of existing probing techniques, which depend on model-specific internal activations, and proposes a more generalizable and interpretable evaluation method for testing LLMs' world model representations.

Method: A state-based evaluation framework is developed to assess LLMs by analyzing downstream legal move distributions in chess, comparing predicted game states to actual ones to estimate semantic alignment.

Result: Experimental results reveal deficiencies in state-tracking by LLMs, showing limitations in maintaining coherent internal models over long sequences.

Conclusion: The proposed framework serves as a robust tool to evaluate structured reasoning in LLMs without requiring access to their internal activations and is generalizable to various symbolic environments.

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [14] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: This paper introduces CASE, an AI-driven framework designed to combat social engineering scams in digital payments by collecting and utilizing user-feedback data.


<details>
  <summary>Details</summary>
Motivation: To address the growing challenge of effectively understanding and preventing sophisticated social engineering scams that exploit digital payment platforms.

Method: The paper introduces CASE, a dual AI framework. It employs a conversational agent to proactively interview potential scam victims and an AI system to structure the resulting insights from user feedback for enforcement measures.

Result: Implemented on Google Pay India, the framework led to a 21% increase in scam enforcements, showcasing its effectiveness in mitigating fraud in digital payments.

Conclusion: The framework is both scalable and generalizable, offering a model for developing AI-driven solutions to tackle scams in digital payment systems and other domains.

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [15] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: The paper addresses the challenge of optimizing large job-shop production plants, such as semiconductor fabs, with an approach using bio-inspired swarm intelligence algorithms, focusing on the "boids" flocking algorithm.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the optimization problem in large production plants, which becomes computationally unsolvable using classical linear optimization methods.

Method: The "boids" flocking algorithm, originally used in robotics and the movie industry, is applied in a bottom-up fashion using local information and simple heuristics to optimize production processes.

Result: The algorithm is shown to effectively handle the problem of switching between machines in semiconductor production, akin to how a swarm adapts to obstacles.

Conclusion: The paper concludes that the flocking algorithm, inspired by bio-behaviors, is a practical approach for addressing certain optimization issues in production plants.

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [16] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: The paper introduces SWIRL, a multi-agent reinforcement learning framework designed for mobile GUI agents, reformulating multi-agent tasks into staged single-agent tasks for efficient learning.


<details>
  <summary>Details</summary>
Motivation: The limitations of single-agent systems and inefficiencies in multi-agent reinforcement learning methods for LVLM architectures necessitate a reliable multi-agent solution for GUI operations.

Method: SWIRL converts multi-agent reinforcement learning into a sequence of single-agent tasks, stabilizing training by fixing other agents during the update and providing theoretical guarantees.

Result: SWIRL outperforms on high- and low-level GUI benchmarks and showcases strong potential in multi-agent mathematical reasoning tasks.

Conclusion: SWIRL is a robust and efficient framework for multi-agent systems, with applications in GUI tasks and broader use cases like mathematical reasoning.

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [17] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: This paper proposes a shift to 'Model Science,' emphasizing trained model interaction, verification, explanation, and control.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of foundation models necessitates a structured way to analyze and manage model behaviors effectively.

Method: The authors define four pillars of Model Science—Verification, Explanation, Control, and Interface—as the foundation for disciplined analysis and interaction with models.

Result: A conceptual framework for Model Science is introduced with clearly outlined pillars to guide its implementation.

Conclusion: The framework is intended to enhance the credibility, safety, and human alignment of AI systems for diverse operational contexts.

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [OmniSim: Simulating Hardware with C Speed and RTL Accuracy for High-Level Synthesis Designs](https://arxiv.org/abs/2508.19299)
*Rishov Sarkar,Cong Hao*

Main category: cs.AR

TL;DR: The paper introduces OmniSim, a simulation framework for High-Level Synthesis (HLS), offering both fast and accurate performance across unsupported complex designs.


<details>
  <summary>Details</summary>
Motivation: HLS tools use constructs like infinite loops and FIFOs to express concurrent hardware. Simulating these accurately in C without hardware timing is slow and challenging, limiting verification and performance metrics. Existing solutions are partially effective but fail to support advanced dataflow features.

Method: OmniSim employs software multi-threading and FIFO timing tables to orchestrate fast, accurate simulation of complex dataflows. It couples functional and performance simulations to achieve near-C speed and RTL-level accuracy.

Result: OmniSim successfully simulated eleven previously unsupported designs, offering up to 35.9x speed improvement over C/RTL co-simulation, and up to 6.61x improvement over LightningSim.

Conclusion: OmniSim advances simulation technology for HLS tools, delivering both functionality and performance precision while dramatically increasing simulation speed, surpassing existing academic and commercial tools.

Abstract: High-Level Synthesis (HLS) is increasingly popular for hardware design using
C/C++ instead of Register-Transfer Level (RTL). To express concurrent hardware
behavior in a sequential language like C/C++, HLS tools introduce constructs
such as infinite loops and dataflow modules connected by FIFOs. However,
efficiently and accurately simulating these constructs at C level remains
challenging. First, without hardware timing information, functional
verification typically requires slow RTL synthesis and simulation, as the
current approaches in commercial HLS tools. Second, cycle-accurate performance
metrics, such as end-to-end latency, also rely on RTL simulation. No existing
HLS tool fully overcomes the first limitation. For the second, prior work such
as LightningSim partially improves simulation speed but lacks support for
advanced dataflow features like cyclic dependencies and non-blocking FIFO
accesses.
  To overcome both limitations, we propose OmniSim, a framework that
significantly extends the simulation capabilities of both academic and
commercial HLS tools. First, OmniSim enables fast and accurate simulation of
complex dataflow designs, especially those explicitly declared unsupported by
commercial tools. It does so through sophisticated software multi-threading,
where threads are orchestrated by querying and updating a set of FIFO tables
that explicitly record exact hardware timing of each FIFO access. Second,
OmniSim achieves near-C simulation speed with near-RTL accuracy for both
functionality and performance, via flexibly coupled and overlapped
functionality and performance simulations.
  We demonstrate that OmniSim successfully simulates eleven designs previously
unsupported by any HLS tool, achieving up to 35.9x speedup over traditional
C/RTL co-simulation, and up to 6.61x speedup over the state-of-the-art yet less
capable simulator, LightningSim, on its own benchmark suite.

</details>


### [19] [GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification](https://arxiv.org/abs/2508.19393)
*Phuoc Pham,Arun Venkitaraman,Chia-Yu Hsieh,Andrea Bonetti,Stefan Uhlich,Markus Leibl,Simon Hofmann,Eisaku Ohbuchi,Lorenzo Servadei,Ulf Schlichtmann,Robert Wille*

Main category: cs.AR

TL;DR: This paper introduces GENIE-ASI, a training-free method using large language models (LLMs) for analog subcircuit identification, showcasing competitive results across varying levels of subcircuit complexity.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to overcome challenges in analog subcircuit identification such as reliance on human expertise, rule-based methods, and the need for large labeled datasets.

Method: GENIE-ASI uses in-context learning and LLMs to convert instructions from examples into Python code, processing SPICE netlists to identify subcircuits. A benchmark of operational amplifier netlists evaluates its performance.

Result: Experimental results show GENIE-ASI achieving high accuracy on simple subcircuits (F1 = 1.0), competitive results on moderate abstractions (F1 = 0.81), and reasonable potential on complex subcircuits (F1 = 0.31).

Conclusion: GENIE-ASI demonstrates that LLMs can act as general-purpose tools in analog design automation, paving the way for research in foundation model applications in this field.

Abstract: Analog subcircuit identification is a core task in analog design, essential
for simulation, sizing, and layout. Traditional methods often require extensive
human expertise, rule-based encoding, or large labeled datasets. To address
these challenges, we propose GENIE-ASI, the first training-free, large language
model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI
operates in two phases: it first uses in-context learning to derive natural
language instructions from a few demonstration examples, then translates these
into executable Python code to identify subcircuits in unseen SPICE netlists.
In addition, to evaluate LLM-based approaches systematically, we introduce a
new benchmark composed of operational amplifier netlists (op-amps) that cover a
wide range of subcircuit variants. Experimental results on the proposed
benchmark show that GENIE-ASI matches rule-based performance on simple
structures (F1-score = 1.0), remains competitive on moderate abstractions
(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =
0.31). These findings demonstrate that LLMs can serve as adaptable,
general-purpose tools in analog design automation, opening new research
directions for foundation model applications in analog design automation.

</details>


### [20] [RARO: Reliability-aware Conversion with Enhanced Read Performance for QLC SSDs](https://arxiv.org/abs/2508.19530)
*Yanyun Wang,Dingcui Yu,Yina Lv,Yunpeng Song,Yumiao Zhao,Liang Shi*

Main category: cs.AR

TL;DR: RARO is a hybrid flash management scheme aimed at improving the read performance of QLC flash storage while minimizing capacity loss, by targeting high-read-retry blocks for data migration and optimizing I/O operations.


<details>
  <summary>Details</summary>
Motivation: QLC flash storage offers benefits in cost and capacity but faces significant reliability issues and read performance degradation due to frequent read retries. Current hybrid storage methods sacrifice capacity for better performance but lead to inefficiencies like excessive mode switching.

Method: RARO employs an approach that triggers data migration based on real-time read retry statistics and flash characteristics. It supports fine-grained multi-mode conversions (SLC-TLC-QLC) to reduce unnecessary conversions and optimize read operations with minimal capacity loss.

Result: RARO demonstrated notable improvements in read performance across various workloads on the FEMU platform, while maintaining negligible impact on usable storage capacity.

Conclusion: RARO effectively mitigates read performance issues in QLC flash storage without incurring significant capacity overhead, offering a balanced and efficient hybrid storage solution.

Abstract: Quad-level cell (QLC) flash offers significant benefits in cost and capacity,
but its limited reliability leads to frequent read retries, which severely
degrade read performance. A common strategy in high-density flash storage is to
program selected blocks in a low-density mode (SLC), sacrificing some capacity
to achieve higher I/O performance. This hybrid storage architecture has been
widely adopted in consumer-grade storage systems. However, existing hybrid
storage schemes typically focus on write performance and rely solely on data
temperature for migration decisions. This often results in excessive mode
switching, causing substantial capacity overhead.
  In this paper, we present RARO (Reliability-Aware Read performance
Optimization), a hybrid flash management scheme designed to improve read
performance with minimal capacity cost. The key insight behind RARO is that
much of the read slowdown in QLC flash is caused by read retries. RARO triggers
data migration only when hot data resides in QLC blocks experiencing a high
number of read retries, significantly reducing unnecessary conversions and
capacity loss. Moreover, RARO supports fine-grained multi-mode conversions
(SLC-TLC-QLC) to further minimize capacity overhead. By leveraging real-time
read retry statistics and flash characteristics, RARO mitigates over-conversion
and optimizes I/O performance. Experiments on the FEMU platform demonstrate
that RARO significantly improves read performance across diverse workloads,
with negligible impact on usable capacity.

</details>


### [21] [Support Vector Machines Classification on Bendable RISC-V](https://arxiv.org/abs/2508.19656)
*Polykarpos Vergos,Theofanis Vergos,Florentia Afentaki,Konstantinos Balaskas,Georgios Zervakis*

Main category: cs.AR

TL;DR: The paper introduces an open-source framework and a custom ML accelerator architecture enabling highly efficient machine learning applications for Bendable RISC-V cores in flexible electronics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in machine learning applications for flexible electronics, such as large feature sizes and high power consumption.

Method: The researchers developed an open-source framework and a custom ML accelerator architecture supporting SVM algorithms with precision-scalable designs for flexible electronics.

Result: Experimental findings show a 21x improvement in inference execution time and energy efficiency, proving the approach's effectiveness for flexible ML applications.

Conclusion: The proposed system demonstrates significant potential for achieving low-power, flexible intelligence on edge devices, advancing flexible electronic technologies.

Abstract: Flexible Electronics (FE) technology offers uniquecharacteristics in
electronic manufacturing, providing ultra-low-cost, lightweight, and
environmentally-friendly alternatives totraditional rigid electronics. These
characteristics enable a rangeof applications that were previously constrained
by the costand rigidity of conventional silicon technology. Machine learning
(ML) is essential for enabling autonomous, real-time intelligenceon devices
with smart sensing capabilities in everyday objects. However, the large feature
sizes and high power consumption ofthe devices oppose a challenge in the
realization of flexible ML applications. To address the above, we propose an
open-source framework for developing ML co-processors for the Bendable RISC-V
core. In addition, we present a custom ML accelerator architecture for Support
Vector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR)
algorithms. Our ML accelerator adopts a generic, precision-scalable design,
supporting 4-, 8-, and 16-bit weight representations. Experimental results
demonstrate a 21x improvement in both inference execution time and energy
efficiency, on average, highlighting its potential for low-power, flexible
intelligence on the edge.

</details>


### [22] [New Tools, Programming Models, and System Support for Processing-in-Memory Architectures](https://arxiv.org/abs/2508.19868)
*Geraldo F. Oliveira*

Main category: cs.AR

TL;DR: The dissertation focuses on tools and models to support DRAM-based Processing-in-Memory (PIM) architectures and introduces methodologies, frameworks, and architectures like DAMOV, MIMDRAM, Proteus, and DaPPA.


<details>
  <summary>Details</summary>
Motivation: To ease the adoption of DRAM-based Processing-in-Memory (PIM) architectures in modern systems by addressing bottlenecks, flexibility, execution latency, and programmability issues.

Method: Proposing four new contributions: 1) DAMOV for memory bottleneck characterization; 2) MIMDRAM for enhanced PUD programmability; 3) Proteus framework for latency reduction; 4) DaPPA for simplifying programmability of PIM architectures.

Result: Provides methodologies like DAMOV for data movement analysis, MIMDRAM for resource control, Proteus for latency reduction, and DaPPA for user-friendly PIM programming.

Conclusion: The work advances PIM architecture adoption by solving key challenges related to programming, execution, and hardware-software co-design.

Abstract: Our goal in this dissertation is to provide tools, programming models, and
system support for PIM architectures (with a focus on DRAM-based solutions), to
ease the adoption of PIM in current and future systems. To this end, we make at
least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize
memory-related data movement bottlenecks in modern workloads, and the first
data movement benchmark suite. Second, we introduce MIMDRAM, a new
hardware/software co-designed substrate that addresses the major current
programmability and flexibility limitations of the bulk bitwise execution model
of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation
and control of only the needed computing resources inside DRAM for PUD
computing. Third, we introduce Proteus, the first hardware framework that
addresses the high execution latency of bulk bitwise PUD operations in
state-of-the-art PUD architectures by implementing a data-aware runtime engine
for PUD. Proteus reduces the latency of PUD operations in three different ways:
(i) Proteus concurrently executes independent in-DRAM primitives belong to a
single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the
bit-precision (and consequentially the latency and energy consumption) of PUD
operations by exploiting narrow values (i.e., values with many leading zeros or
ones). (iii) Proteus chooses and uses the most appropriate data representation
and arithmetic algorithm implementation for a given PUD instruction
transparently to the programmer. Fourth, we introduce DaPPA (data-parallel
processing-in-memory architecture), a new programming framework that eases
programmability for general-purpose PNM architectures by allowing the
programmer to write efficient PIM-friendly code without the need to manage
hardware resources explicitly.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [23] [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
*Qing Wang,Xue Han,Jiahui Wang,Lehao Xing,Qian Hu,Lianlian Zhang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: The paper introduces MultiPL-MoE, a hybrid mixture of experts method to improve multilingual code generation for LLMs. It combines token and segment-level MoEs for better expert selection.


<details>
  <summary>Details</summary>
Motivation: To enhance the ability of LLMs to handle multilingual code generation under restricted computational resources.

Method: MultiPL-MoE integrates token-level MoE with a shared expert and gate weight normalization, along with a segment-level MoE using sliding windows and expert-choice routing for optimized expert selection.

Result: Experimental results demonstrate the effectiveness of the MultiPL-MoE framework in addressing multilingual code generation challenges.

Conclusion: The MultiPL-MoE approach successfully improves the MultiPL performance of LLMs, proving its potential for future applications in code generation.

Abstract: Despite LLMs' excellent code creation capabilities, multilingual code
generation remains extremely challenging. To address this, we intent to improve
the multi-programming-lingual (MultiPL) performance of the base LLMs while
retaining the most popular ones using restricted computational resources. We
consider MultiPL to be a special case of multiple natural languages and propose
a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called
MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize
expert selection at both the token and segment levels. The token-level MoE is a
standard upcycling MoE structure with a shared expert and a novel gate weight
normalization approach that aids in the final fusion with the segment-level
MoE. The segment-level MoE incorporates two innovative designs to better
capture the syntactic structure and contextual patterns of programming
languages: First, using a sliding window to partition the input token sequence
into multiple segments; Then, adopting an expert-choice routing strategy that
allows experts to select the top-k segments. The results of the experiment
proved the effectiveness of MultiPL-MoE.

</details>


### [24] [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
*Nguyen Huu Nhat Minh,Tran Nguyen Anh,Truong Dinh Dung,Vo Van Nam,Le Pham Tuyen*

Main category: cs.CL

TL;DR: The paper addresses the challenge of cross-lingual phoneme recognition in Vietnamese-English mixing with a bilingual speech recognition system, utilizing a new phoneme set and leveraging the PhoWhisper encoder.


<details>
  <summary>Details</summary>
Motivation: Cross-lingual speech recognition between Vietnamese, which relies on tonal variation, and English, with stress patterns and non-standard pronunciations, faces phoneme alignment challenges.

Method: The authors propose constructing a bilingual phoneme set and implementing an end-to-end system with the PhoWhisper pre-trained encoder to enhance phoneme recognition.

Result: Experiments show improved bilingual speech recognition accuracy for Vietnamese-English and a robust framework for tonal and stress-based phoneme challenges.

Conclusion: The proposed approach demonstrates its effectiveness in addressing cross-lingual speech recognition complexities, offering a viable solution for bilingual phoneme alignment issues.

Abstract: Cross-lingual phoneme recognition has emerged as a significant challenge for
accurate automatic speech recognition (ASR) when mixing Vietnamese and English
pronunciations. Unlike many languages, Vietnamese relies on tonal variations to
distinguish word meanings, whereas English features stress patterns and
non-standard pronunciations that hinder phoneme alignment between the two
languages. To address this challenge, we propose a novel bilingual speech
recognition approach with two primary contributions: (1) constructing a
representative bilingual phoneme set that bridges the differences between
Vietnamese and English phonetic systems; (2) designing an end-to-end system
that leverages the PhoWhisper pre-trained encoder for deep high-level
representations to improve phoneme recognition. Our extensive experiments
demonstrate that the proposed approach not only improves recognition accuracy
in bilingual speech recognition for Vietnamese but also provides a robust
framework for addressing the complexities of tonal and stress-based phoneme
recognition

</details>


### [25] [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
*Rushitha Santhoshi Mamidala,Anshuman Chhabra,Ankur Mali*

Main category: cs.CL

TL;DR: The paper proposes a neuro-symbolic reasoning framework that replaces prompt-based methods with a deterministic automata-based approach, enhancing stability and interpretability in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations and inconsistencies of prompt-based reasoning strategies in large language models, which often yield unreliable and opaque results.

Method: The paper extends the RetoMaton framework by using a task-adaptive Weighted Finite Automaton (WFA) for symbolic and structured reasoning, replacing the global datastore with one constructed directly from external domain corpora.

Result: The extended RetoMaton demonstrates improved reasoning performance on three tasks (TriviaQA, GSM8K, and MMLU) across two large language models (LLaMA-3.2-1B and Gemma-3-1B-PT), with enhanced transparency and reproducibility in retrieval.

Conclusion: Automaton-guided neuro-symbolic reasoning offers a promising alternative to prompting-based methods, enabling more trustworthy and context-aware reasoning in large language models.

Abstract: Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and
In-Context Learning (ICL) have become widely used for eliciting reasoning
capabilities in large language models (LLMs). However, these methods rely on
fragile, implicit mechanisms often yielding inconsistent outputs across seeds,
formats, or minor prompt variations making them fundamentally unreliable for
tasks requiring stable, interpretable reasoning. In contrast, automata-based
neuro-symbolic frameworks like RetoMaton offer a more structured and
trustworthy alternative by grounding retrieval in symbolic memory with
deterministic transitions. In this work, we extend RetoMaton by replacing its
global datastore with a local, task-adaptive Weighted Finite Automaton (WFA),
constructed directly from external domain corpora. This local automaton
structure promotes robust, context-aware retrieval while preserving symbolic
traceability and low inference overhead. Unlike prompting, which entangles
context and memory in opaque ways, our approach leverages the explicit
structure of WFAs to provide verifiable and modular retrieval behavior, making
it better suited for domain transfer and interoperability. We evaluate this
local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT
across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
(multi-step math), and MMLU (domain knowledge). Compared to the base model and
prompting-based methods, augmenting these setups with local RetoMaton
consistently improves performance while enabling transparent and reproducible
retrieval dynamics. Our results highlight a promising shift toward trustworthy,
symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

</details>


### [26] [RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits](https://arxiv.org/abs/2508.19272)
*Kshitij Fadnis,Sara Rosenthal,Maeda Hanafi,Yannis Katsis,Marina Danilevsky*

Main category: cs.CL

TL;DR: RAGAPHENE is a platform to enable annotators to simulate real-world conversations, enhancing benchmarks for evaluating Retrieval Augmented Generation in LLMs.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs necessitate precise evaluation of factually correct interactions, especially in multi-turn RAG conversations.

Method: Developed RAGAPHENE, a chat-based annotation platform involving human annotators for generating realistic conversations.

Result: RAGAPHENE engaged around 40 annotators to create thousands of quality real-world conversations for benchmarking.

Conclusion: The platform successfully aids in improving the evaluation methodologies for Retrieval Augmented Generation in LLMs.

Abstract: Retrieval Augmented Generation (RAG) is an important aspect of conversing
with Large Language Models (LLMs) when factually correct information is
important. LLMs may provide answers that appear correct, but could contain
hallucinated information. Thus, building benchmarks that can evaluate LLMs on
multi-turn RAG conversations has become an increasingly important task.
Simulating real-world conversations is vital for producing high quality
evaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform
that enables annotators to simulate real-world conversations for benchmarking
and evaluating LLMs. RAGAPHENE has been successfully used by approximately 40
annotators to build thousands of real-world conversations.

</details>


### [27] [Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis](https://arxiv.org/abs/2508.19274)
*Yue Chu*

Main category: cs.CL

TL;DR: The paper investigates using verbal autopsy (VA) narratives for automated cause of death (COD) classification with pretrained language models (PLMs) and machine learning (ML). The study shows that narratives provide valuable information, outperforming question-only models, and that combined multimodal approaches offer even better results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the current limitation in automated verbal autopsy cause classification algorithms, which only utilize structured questions and ignore unstructured narratives, despite the potential value in narratives for more accurate cause of death classification.

Method: The study utilizes pretrained language models (PLMs), especially transformer-based architectures, with task-specific fine-tuning. It employs multimodal fusion strategies to integrate unstructured narratives and structured questions, and evaluates performance on a dataset from South Africa.

Result: Transformer-based PLMs using only narratives outperform existing question-only methods in cause of death classification. Multimodal approaches that combine narratives and structured questions further improve performance. Unique contributions of individual modalities were identified, particularly in identifying non-communicable diseases.

Conclusion: The study underscores the value of narratives in improving cause of death classification using verbal autopsy data, highlighting the need for diverse and high-quality datasets to enhance machine learning models. It also suggests redesigning verbal autopsy instruments to better leverage unstructured narrative data alongside structured questions.

Abstract: In countries without civil registration and vital statistics, verbal autopsy
(VA) is a critical tool for estimating cause of death (COD) and inform policy
priorities. In VA, interviewers ask proximal informants for details on the
circumstances preceding a death, in the form of unstructured narratives and
structured questions. Existing automated VA cause classification algorithms
only use the questions and ignore the information in the narratives. In this
thesis, we investigate how the VA narrative can be used for automated COD
classification using pretrained language models (PLMs) and machine learning
(ML) techniques. Using empirical data from South Africa, we demonstrate that
with the narrative alone, transformer-based PLMs with task-specific fine-tuning
outperform leading question-only algorithms at both the individual and
population levels, particularly in identifying non-communicable diseases. We
explore various multimodal fusion strategies combining narratives and questions
in unified frameworks. Multimodal approaches further improve performance in COD
classification, confirming that each modality has unique contributions and may
capture valuable information that is not present in the other modality. We also
characterize physician-perceived information sufficiency in VA. We describe
variations in sufficiency levels by age and COD and demonstrate that
classification accuracy is affected by sufficiency for both physicians and
models. Overall, this thesis advances the growing body of knowledge at the
intersection of natural language processing, epidemiology, and global health.
It demonstrates the value of narrative in enhancing COD classification. Our
findings underscore the need for more high-quality data from more diverse
settings to use in training and fine-tuning PLM/ML methods, and offer valuable
insights to guide the rethinking and redesign of the VA instrument and
interview.

</details>


### [28] [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
*Gunjan Jalori,Preetika Verma,Sercan Ö Arık*

Main category: cs.CL

TL;DR: The paper introduces FLAIRR-TS, a test-time prompt optimization framework for forecasting with large language models (LLMs), which enhances forecasting accuracy using adaptive refinement without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient method for time series forecasting on LLMs without requiring extensive pre-processing, fine-tuning, or manual prompt engineering.

Method: The method employs a system with two agents: a 'Forecaster' agent that generates forecasts based on an initial prompt and a 'Refiner' agent that iteratively improves the prompt using past results and retrieved analogs, achieving adaptive generalization.

Result: FLAIRR-TS improves accuracy on benchmark datasets, outperforming static prompts and retrieval-augmented approaches, and performs close to specialized prompts.

Conclusion: The proposed agentic framework offers a practical alternative to fine-tuning by enabling adaptive prompt refinement, achieving competitive forecasts with strong performance.

Abstract: Time series Forecasting with large languagemodels (LLMs) requires bridging
numericalpatterns and natural language. Effective fore-casting on LLM often
relies on extensive pre-processing and fine-tuning.Recent studiesshow that a
frozen LLM can rival specializedforecasters when supplied with a carefully
en-gineered natural-language prompt, but craft-ing such a prompt for each task
is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt
optimization framework thatutilizes an agentic system: a
Forecaster-agentgenerates forecasts using an initial prompt,which is then
refined by a refiner agent, in-formed by past outputs and retrieved
analogs.This adaptive prompting generalizes across do-mains using creative
prompt templates andgenerates high-quality forecasts without inter-mediate code
generation.Experiments onbenchmark datasets show improved accuracyover static
prompting and retrieval-augmentedbaselines, approaching the performance
ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,
achievingstrong performance via its agentic approach toadaptive prompt
refinement and retrieval.

</details>


### [29] [CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
*Ziqiang Cui,Yunpeng Weng,Xing Tang,Peiyang Liu,Shiwei Li,Bowei He,Jiamin Chen,Xiuqiang He,Chen Ma*

Main category: cs.CL

TL;DR: This paper proposes CORE, a method using reinforcement learning to compress context in retrieval-augmented generation (RAG) models, achieving high compression and improved task performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce computational costs and improve efficiency in RAG methods for Large Language Models (LLMs) without sacrificing task performance.

Method: CORE uses reinforcement learning with Generalized Reinforcement Learning Policy Optimization (GRPO) to train a contextual compressor guided by task performance as a reward, eliminating the need for predefined compression labels.

Result: Experiments across four datasets demonstrate a 3% compression ratio and a 3.3-point improvement in Exact Match (EM) scores, while maintaining or improving accuracy.

Conclusion: CORE offers a promising direction for lossless RAG context compression, enabling both computational efficiency and improved performance in LLM tasks.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the timeliness of knowledge and the factual accuracy of responses in
Large Language Models (LLMs). However, the inclusion of excessive retrieved
documents substantially increases the input length, leading to higher
computational costs. Previous studies have attempted to compress retrieved
documents into shorter texts before in-context integration, but such methods
often compromise end-task performance. The lack of well-defined compression
targets forces many approaches to rely on fixed heuristics, which cannot
guarantee that the compressed content will effectively support the end task. To
address these limitations, we propose CORE, a novel method designed to achieve
lossless context compression for RAG. CORE employs reinforcement learning to
optimize the compression process without relying on predefined compression
labels. Specifically, it utilizes end-task performance as a reward signal and
applies Generalized Reinforcement Learning Policy Optimization (GRPO) to train
the compressor. This end-to-end training framework enables the compressor to
generate summaries that maximize the accuracy of answers generated by the LLM.
Extensive experiments on four datasets demonstrate the superiority of our
approach. With a high compression ratio of 3\%, our method not only avoids
performance degradation compared to prepending full documents across all
datasets but also improves the average Exact Match (EM) score by 3.3 points.
The code will be released soon.

</details>


### [30] [Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains](https://arxiv.org/abs/2508.19357)
*Peiran Zhou,Junnan Zhu,Yichen Shen,Ruoxi Yu*

Main category: cs.CL

TL;DR: The paper introduces CASC, a novel framework to improve the accuracy of LLMs in complex domains by condensing and structuring multi-document contexts before feeding them to the model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the inefficiencies and inaccuracies of RAG methods in complex domains, where multiple, lengthy, or conflicting documents overwhelm the synthesis process of LLMs.

Method: This paper introduces CASC, which includes a Context Analyzer & Synthesizer (CAS) module powered by a fine-tuned smaller LLM for key tasks like information extraction, consistency checks, conflict resolution, and question-oriented synthesis. It transforms scattered information into condensed, structured, and rich contexts for LLMs.

Result: The proposed CASC framework was tested on the new SciDocs-QA dataset and achieved superior performance over strong baselines, demonstrating its effectiveness.

Conclusion: Using a CASC framework improves the efficiency and trustworthiness of Retrieval-Augmented Systems by intelligently synthesizing information, especially in domains with complex and conflicting data.

Abstract: Large Language Models (LLMs) excel in language tasks but are prone to
hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)
mitigates these by grounding LLMs in external knowledge. However, in complex
domains involving multiple, lengthy, or conflicting documents, traditional RAG
suffers from information overload and inefficient synthesis, leading to
inaccurate and untrustworthy answers. To address this, we propose CASC
(Context-Adaptive Synthesis and Compression), a novel framework that
intelligently processes retrieved contexts. CASC introduces a Context Analyzer
& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs
key information extraction, cross-document consistency checking and conflict
resolution, and question-oriented structured synthesis. This process transforms
raw, scattered information into a highly condensed, structured, and
semantically rich context, significantly reducing the token count and cognitive
load for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new
challenging multi-document question answering dataset designed for complex
scientific domains with inherent redundancies and conflicts. Our extensive
experiments demonstrate that CASC consistently outperforms strong baselines.

</details>


### [31] [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
*Fatemeh Haji,Mazal Bethany,Cho-Yu Jason Chiang,Anthony Rios,Peyman Najafirad*

Main category: cs.CL

TL;DR: ARIS (Agreement-based Reflective Inference System) is a hybrid event extraction approach combining discriminative models and generative LLMs, outperforming state-of-the-art methods in accuracy and recall.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of traditional discriminative models (low recall) and generative LLMs (hallucination and inconsistency) in event extraction.

Method: ARIS integrates a Self Mixture of Agents, reflective LLM inference, structured consensus mechanisms, and confidence-based filtering, alongside decomposed instruction fine-tuning.

Result: Experiments reveal ARIS surpasses existing state-of-the-art event extraction techniques across three benchmark datasets.

Conclusion: The hybrid design of ARIS successfully combines precision and flexibility for higher quality event extraction compared to existing methods.

Abstract: Event Extraction (EE) involves automatically identifying and extracting
structured information about events from unstructured text, including triggers,
event types, and arguments. Traditional discriminative models demonstrate high
precision but often exhibit limited recall, particularly for nuanced or
infrequent events. Conversely, generative approaches leveraging Large Language
Models (LLMs) provide higher semantic flexibility and recall but suffer from
hallucinations and inconsistent predictions. To address these challenges, we
propose Agreement-based Reflective Inference System (ARIS), a hybrid approach
combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS
explicitly leverages structured model consensus, confidence-based filtering,
and an LLM reflective inference module to reliably resolve ambiguities and
enhance overall event prediction quality. We further investigate decomposed
instruction fine-tuning for enhanced LLM event extraction understanding.
Experiments demonstrate our approach outperforms existing state-of-the-art
event extraction methods across three benchmark datasets.

</details>


### [32] [LongReasonArena: A Long Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2508.19363)
*Jiayu Ding,Shuming Ma,Lei Cui,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: LongReasonArena is a benchmark designed to test LLMs on long reasoning tasks, reaching up to 1 million tokens. Results show significant challenges for current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks emphasize comprehension of lengthy inputs but neglect long-form reasoning capabilities, necessitating a specialized evaluation framework.

Method: LongReasonArena includes tasks involving multi-step algorithms, designed to reflect long reasoning features like retrieval and backtracking, with adjustable reasoning lengths.

Result: Models exhibit low performance, such as Deepseek-R1 achieving only 7.5% accuracy. Accuracy also declines linearly as reasoning steps increase logarithmically.

Conclusion: LongReasonArena highlights the current limitations in LLMs' long reasoning abilities and serves as a robust benchmark for future advancements.

Abstract: Existing long-context benchmarks for Large Language Models (LLMs) focus on
evaluating comprehension of long inputs, while overlooking the evaluation of
long reasoning abilities. To address this gap, we introduce LongReasonArena, a
benchmark specifically designed to assess the long reasoning capabilities of
LLMs. Our tasks require models to solve problems by executing multi-step
algorithms that reflect key aspects of long reasoning, such as retrieval and
backtracking. By controlling the inputs, the required reasoning length can be
arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most
challenging tasks. Extensive evaluation results demonstrate that
LongReasonArena presents a significant challenge for both open-source and
proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our
task. Further analysis also reveals that the accuracy exhibits a linear decline
with respect to the logarithm of the expected number of reasoning steps. Our
code and data is available at
https://github.com/LongReasonArena/LongReasonArena.

</details>


### [33] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: The paper introduces a benchmark and proposes methods for Database Entity Recognition (DB-ER) in Natural Language Queries (NLQ), demonstrating improved performance over state-of-the-art models through data augmentation and fine-tuned T5 models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in accurately recognizing database entities in NLQs, leveraging resources like SQL query datasets to enhance performance and create new benchmarks.

Method: The approach involves creating a human-annotated benchmark, introducing automatic data augmentation techniques, and developing a T5-based language model fine-tuned for DB-ER via sequence tagging and token classification tasks.

Result: The proposed DB-ER model outperformed state-of-the-art NER taggers in precision and recall. Data augmentation improved metrics by over 10%, and T5 fine-tuning added a 5-10% boost.

Conclusion: The method shows promise in advancing DB-ER tasks, establishing the importance of data augmentation and specialized fine-tuning for better entity recognition in NLQs.

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [34] [One Joke to Rule them All? On the (Im)possibility of Generalizing Humor](https://arxiv.org/abs/2508.19402)
*Mor Turgeman,Chen Shani,Dafna Shahaf*

Main category: cs.CL

TL;DR: The study examines if large language models (LLMs) can transfer humor comprehension across types, achieving some success in generalizing and transferring up to 75% accuracy on unseen humor tasks.


<details>
  <summary>Details</summary>
Motivation: Humor is a complex and diverse form of communication, but most computational humor research focuses narrowly on specific humor types. The study aims to explore if competence in specific humor tasks enables transferability to other, unseen humor types.

Method: Transfer learning experiments were conducted using Large Language Models (LLMs) across four datasets representing different humor tasks. Models were trained with varying diversity levels (1-3 datasets) and then tested on unseen tasks. Transferability and performance were analyzed.

Result: Models demonstrated some transferability, achieving up to 75% accuracy on novel datasets. Training on diverse humor sources boosted transfer performance (1.88-4.05%) with little or no drop in in-domain performance. Dad Jokes were particularly effective in enabling transfer but are themselves hard to generalize into.

Conclusion: The results suggest that LLMs can generalize across humor types to some extent by learning transferable mechanisms. Greater diversity in training enhances this transferability, hinting at the potential to tackle the evolving landscape of humor in new media.

Abstract: Humor is a broad and complex form of communication that remains challenging
for machines. Despite its broadness, most existing research on computational
humor traditionally focused on modeling a specific type of humor. In this work,
we wish to understand whether competence on one or more specific humor tasks
confers any ability to transfer to novel, unseen types; in other words, is this
fragmentation inevitable? This question is especially timely as new humor types
continuously emerge in online and social media contexts (e.g., memes,
anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this
evolving landscape, they must be able to generalize across humor types by
capturing deeper, transferable mechanisms. To investigate this, we conduct a
series of transfer learning experiments across four datasets, representing
different humor tasks. We train LLMs under varied diversity settings (1-3
datasets in training, testing on a novel task). Experiments reveal that models
are capable of some transfer, and can reach up to 75% accuracy on unseen
datasets; training on diverse sources improves transferability (1.88-4.05%)
with minimal-to-no drop in in-domain performance. Further analysis suggests
relations between humor types, with Dad Jokes surprisingly emerging as the best
enabler of transfer (but is difficult to transfer to). We release data and
code.

</details>


### [35] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: This paper explores the potential reduction in human writing ability due to the increasing reliance on text generation tools powered by large language models.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by advancements in generative AI tools and their growing role in various domains of text production, raising concerns about their long-term impact on human writing capabilities.

Method: The article draws parallels between modern reliance on AI-generated text and historical contexts, such as the decline of writing skills during the Greek Dark Ages.

Result: It theorizes that the extensive outsourcing of writing to AI could mirror historical declines in manual literacy due to shifts in technological or cultural practices.

Conclusion: The paper concludes by reflecting on the potential risks of diminished human writing skills and emphasizes the need for caution in adopting AI tools for this purpose.

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [36] [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)
*Aleksandra Beliaeva,Temurbek Rahmatullaev*

Main category: cs.CL

TL;DR: The paper presents a system for three tasks in ontology construction — term extraction, typing, and taxonomy discovery — using modular techniques tailored to each task. It achieved top performances in the LLMs4OL 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore scalable, adaptable, and robust solutions using LLM-based architectures to address the full ontology construction pipeline across heterogeneous domains.

Method: The system combines retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling tailored for specific ontology construction tasks (term extraction, typing, and taxonomy discovery).

Result: The proposed techniques achieved top-ranking results for Tasks A, B, and C in the official leaderboard of the LLMs4OL 2025 challenge.

Conclusion: The strategies demonstrate the effectiveness and generalizability of LLM-based architectures for ontology learning, showcasing their suitability across multiple heterogeneous domains without extensive model fine-tuning.

Abstract: We present a comprehensive system for addressing Tasks A, B, and C of the
LLMs4OL 2025 challenge, which together span the full ontology construction
pipeline: term extraction, typing, and taxonomy discovery. Our approach
combines retrieval-augmented prompting, zero-shot classification, and
attention-based graph modeling -- each tailored to the demands of the
respective task. For Task A, we jointly extract domain-specific terms and their
ontological types using a retrieval-augmented generation (RAG) pipeline.
Training data was reformulated into a document to terms and types
correspondence, while test-time inference leverages semantically similar
training examples. This single-pass method requires no model finetuning and
improves overall performance through lexical augmentation Task B, which
involves assigning types to given terms, is handled via a dual strategy. In the
few-shot setting (for domains with labeled training data), we reuse the RAG
scheme with few-shot prompting. In the zero-shot setting (for previously unseen
domains), we use a zero-shot classifier that combines cosine similarity scores
from multiple embedding models using confidence-based weighting. In Task C, we
model taxonomy discovery as graph inference. Using embeddings of type labels,
we train a lightweight cross-attention layer to predict is-a relations by
approximating a soft adjacency matrix. These modular, task-specific solutions
enabled us to achieve top-ranking results in the official leaderboard across
all three tasks. Taken together these strategies showcase the scalability,
adaptability, and robustness of LLM-based architectures for ontology learning
across heterogeneous domains.
  Code is available at:
https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek

</details>


### [37] [Bridging Language Gaps: Enhancing Few-Shot Language Adaptation](https://arxiv.org/abs/2508.19464)
*Philipp Borchert,Jochen De Weerdt,Marie-Francine Moens*

Main category: cs.CL

TL;DR: CoLAP method enhances multilingual NLP by enabling effective knowledge transfer from high-resource to low-resource languages using contrastive learning and cross-lingual representations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of disparity in language resources in multilingual NLP, aiming to provide effective solutions for low-resource languages.

Method: CoLAP integrates contrastive learning with cross-lingual representations for task-specific knowledge transfer, leveraging data efficiency and adaptability to new languages.

Result: Experiments showed that CoLAP outperforms few-shot cross-lingual baselines and in-context learning models, even with limited datasets.

Conclusion: CoLAP narrows the gap in cross-lingual performance, promoting more efficient multilingual NLP solutions.

Abstract: The disparity in language resources poses a challenge in multilingual NLP,
with high-resource languages benefiting from extensive data, while low-resource
languages lack sufficient data for effective training. Our Contrastive Language
Alignment with Prompting (CoLAP) method addresses this gap by integrating
contrastive learning with cross-lingual representations, facilitating
task-specific knowledge transfer from high-resource to lower-resource
languages. The primary advantage of our approach is its data efficiency,
enabling rapid adaptation to new languages and reducing the need for large
labeled datasets. We conduct experiments with multilingual encoder-only and
decoder-only language models on natural language understanding tasks, including
natural language inference and relation extraction, evaluating performance
across both high- and low-resource languages. Our results demonstrate that
CoLAP outperforms few-shot cross-lingual transfer baselines and in-context
learning, even with limited available data. This effectively narrows the
cross-lingual performance gap, contributing to the development of more
efficient multilingual NLP techniques.

</details>


### [38] [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)
*Sumon Kanti Dey,Jeanne M. Powell,Azra Ismail,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.CL

TL;DR: The paper focuses on using social media data for extracting self-reported impacts of opioid use through named entity recognition (NER) methods, presenting the RedditImpacts 2.0 dataset and evaluating models for this task.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the underreporting of opioid-related clinical and social consequences in healthcare settings by leveraging first-person narratives shared on social media.

Method: Researchers introduced a dataset (RedditImpacts 2.0) with refined guidelines and evaluated both fine-tuned encoder models (like DeBERTa-large) and large language models under zero- and few-shot learning settings for named entity recognition.

Result: The fine-tuned DeBERTa-large model achieved better task performance with a relaxed F1 score of 0.61, outperforming large language models in precision, span accuracy, and task adherence while showcasing strong performance with minimal labeled data.

Conclusion: The study highlights the importance of domain-specific fine-tuning for clinical NLP tasks, the feasibility of robust AI models in resource-limited environments, and the current limitations in achieving expert-level accuracy for domain-specific tasks.

Abstract: Nonmedical opioid use is an urgent public health challenge, with far-reaching
clinical and social consequences that are often underreported in traditional
healthcare settings. Social media platforms, where individuals candidly share
first-person experiences, offer a valuable yet underutilized source of insight
into these impacts. In this study, we present a named entity recognition (NER)
framework to extract two categories of self-reported consequences from social
media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,
depression) and SocialImpacts (e.g., job loss). To support this task, we
introduce RedditImpacts 2.0, a high-quality dataset with refined annotation
guidelines and a focus on first-person disclosures, addressing key limitations
of prior work. We evaluate both fine-tuned encoder-based models and
state-of-the-art large language models (LLMs) under zero- and few-shot
in-context learning settings. Our fine-tuned DeBERTa-large model achieves a
relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming
LLMs in precision, span accuracy, and adherence to task-specific guidelines.
Furthermore, we show that strong NER performance can be achieved with
substantially less labeled data, emphasizing the feasibility of deploying
robust models in resource-limited settings. Our findings underscore the value
of domain-specific fine-tuning for clinical NLP tasks and contribute to the
responsible development of AI tools that may enhance addiction surveillance,
improve interpretability, and support real-world healthcare decision-making.
The best performing model, however, still significantly underperforms compared
to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap
persists between expert intelligence and current state-of-the-art NER/AI
capabilities for tasks requiring deep domain knowledge.

</details>


### [39] [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
*Md. Alvee Ehsan,A. S. M Mehedi Hasan,Kefaya Benta Shahnoor,Syeda Sumaiya Tasneem*

Main category: cs.CL

TL;DR: The paper proposes using fine-tuned large language models (LLMs), specifically Meta-Llama 2-7B, enhanced with prompt engineering and the RACE dataset, to create a tool that automates the generation of various question types for academic assessments.


<details>
  <summary>Details</summary>
Motivation: Designing diverse, fair, and effective academic assessment questions is a time-consuming task for educators, requiring manual analysis of extensive lecture materials.

Method: Leverages fine-tuned generative LLMs like Meta-Llama 2-7B, trained on the RACE dataset, and utilizes prompt engineering to generate tailored question types, including MCQs, conceptual, and factual questions.

Result: Developed an unsupervised NLP-based model that simplifies and streamlines the question generation process for academic assessments.

Conclusion: The proposed tool offers a practical solution to reduce educators' workload by automating question generation, allowing them to focus on other critical tasks while ensuring high-quality evaluations.

Abstract: \Abstract{In the realm of education, student evaluation holds equal
significance as imparting knowledge. To be evaluated, students usually need to
go through text-based academic assessment methods. Instructors need to make
diverse sets of questions that need to be fair for all students to prove their
adequacy over a particular topic. This can prove to be quite challenging as
they may need to manually go through several different lecture materials. Our
objective is to make this whole process much easier by implementing Automatic
Question Answer Generation /(AQAG), using fine-tuned generative LLM. For
tailoring the instructor's preferred question style (MCQ, conceptual, or
factual questions), prompt Engineering (PE) is being utilized. In this
research, we propose to leverage unsupervised learning methods in NLP,
primarily focusing on the English language. This approach empowers the base
Meta-Llama 2-7B model to integrate RACE dataset as training data for the
fine-tuning process. Creating a customized model that will offer efficient
solutions for educators, instructors, and individuals engaged in text-based
evaluations. A reliable and efficient tool for generating questions and answers
can free up valuable time and resources, thus streamlining their evaluation
processes.}

</details>


### [40] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE is a VQA dataset that emphasizes deeper cognitive understanding of movie content, introducing new evaluation schemes and improvement modules.


<details>
  <summary>Details</summary>
Motivation: To advance AI's ability to understand and analyze movies beyond surface-level comprehension and tackle complex cognitive questions.

Method: Uses an agentic brainstorming approach with multiple LLMs for creating and refining question-answer pairs, alongside cognitive test metrics and the ACE module for post-training enhancement.

Result: The ACE module improved model reasoning capabilities by up to 25%, and the dataset successfully facilitates nuanced evaluation of VQA models.

Conclusion: MovieCORE enhances VQA dataset depth, expands video-language model capabilities, and provides insights for improving AI systems for cinematic content analysis.

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [41] [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)
*Manuel Mosquera,Melissa Robles,Johan Rodriguez,Ruben Manrique*

Main category: cs.CL

TL;DR: This paper proposes a method to improve low-resource language translation using external dictionaries and reinforcement learning, achieving significant BLEU score improvements.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with translating low-resource languages due to insufficient exposure and limited parallel data.

Method: The approach involves using a bilingual dictionary alongside reinforcement learning and supervised fine-tuning. Translation is treated as a decision-making process where the model learns to leverage the dictionary.

Result: The proposed method improves BLEU scores by up to +3.37 over previous work and shows 18% better performance than a supervised baseline without dictionary access.

Conclusion: Integrating external tools with LLMs and leveraging reinforcement learning can significantly enhance translation quality for low-resource languages, particularly Spanish-Wayuunaiki.

Abstract: Low-resource machine translation remains a significant challenge for large
language models (LLMs), which often lack exposure to these languages during
pretraining and have limited parallel data for fine-tuning. We propose a novel
approach that enhances translation for low-resource languages by integrating an
external dictionary tool and training models end-to-end using reinforcement
learning, in addition to supervised fine-tuning. Focusing on the
Spanish-Wayuunaiki language pair, we frame translation as a tool-augmented
decision-making problem in which the model can selectively consult a bilingual
dictionary during generation. Our method combines supervised instruction tuning
with Guided Reward Policy Optimization (GRPO), enabling the model to learn both
when and how to use the tool effectively. BLEU similarity scores are used as
rewards to guide this learning process. Preliminary results show that our
tool-augmented models achieve up to +3.37 BLEU improvement over previous work,
and a 18% relative gain compared to a supervised baseline without dictionary
access, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared
Task. We also conduct ablation studies to assess the effects of model
architecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other
models such as LLaMA and a prior NLLB-based system. These findings highlight
the promise of combining LLMs with external tools and the role of reinforcement
learning in improving translation quality in low-resource language settings.

</details>


### [42] [Rule Synergy Analysis using LLMs: State of the Art and Implications](https://arxiv.org/abs/2508.19484)
*Bahar Bateni,Benjamin Pratt,Jim Whitehead*

Main category: cs.CL

TL;DR: This paper evaluates the ability of large language models (LLMs) to understand card synergies in the game Slay the Spire, finding limitations in predicting positive and negative interactions.


<details>
  <summary>Details</summary>
Motivation: To explore the extent of LLMs' ability to comprehend complex rule interactions in dynamic environments, specifically card games.

Method: The authors introduced a dataset of card synergies from Slay the Spire and analyzed LLMs' performance in classifying card pairs as synergistic or non-synergistic.

Result: LLMs perform well in identifying non-synergistic card pairs but struggle with detecting positive and negative synergies due to common errors like timing and game state definitions.

Conclusion: Future research should focus on improving LLMs' capabilities to predict and analyze the effects of rules and their interactions in complex environments.

Abstract: Large language models (LLMs) have demonstrated strong performance across a
variety of domains, including logical reasoning, mathematics, and more. In this
paper, we investigate how well LLMs understand and reason about complex rule
interactions in dynamic environments, such as card games. We introduce a
dataset of card synergies from the game Slay the Spire, where pairs of cards
are classified based on their positive, negative, or neutral interactions. Our
evaluation shows that while LLMs excel at identifying non-synergistic pairs,
they struggle with detecting positive and, particularly, negative synergies. We
categorize common error types, including issues with timing, defining game
states, and following game rules. Our findings suggest directions for future
research to improve model performance in predicting the effect of rules and
their interactions.

</details>


### [43] [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](https://arxiv.org/abs/2508.19529)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: The paper introduces Blockwise Supervised Fine-Tuning (SFT) to better align training with inference for diffusion language models, showing improved text generation performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the misalignment between training and semi-autoregressive inference in discrete diffusion language models, which creates gradient noise and biases.

Method: The authors propose Blockwise SFT, which partitions responses into blocks, selects one active block per step, and aligns training with the blockwise decoding inference process.

Result: Experiments on datasets like GSM8K, MATH, and MetaMathQA demonstrate consistent performance improvements compared to classical SFT under equal computational or token budgets.

Conclusion: Blockwise supervision ensures training-inference alignment, verifying that fine-tuning granularity is critical for diffusion-based language models.

Abstract: Discrete diffusion language models have shown strong potential for text
generation, yet standard supervised fine-tuning (SFT) misaligns with their
semi-autoregressive inference: training randomly masks tokens across the entire
response, while inference generates fixed-size blocks sequentially. This
mismatch introduces noisy prefixes and leaky suffixes, biasing gradients away
from the desired blockwise likelihood. We propose Blockwise SFT, which
partitions responses into fixed-size blocks, selects one active block per step
for stochastic masking, freezes all preceding tokens, and fully hides future
ones. Loss is computed only over the active block, directly mirroring the
blockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show
consistent gains over classical SFT under equal compute or token budgets. Block
size consistency studies and ablations confirm that improvements stem from
faithful training-inference alignment rather than incidental masking effects.
Our results highlight the importance of matching supervision granularity to the
decoding procedure in diffusion-based language models.

</details>


### [44] [Alignment with Fill-In-the-Middle for Enhancing Code Generation](https://arxiv.org/abs/2508.19532)
*Houxing Ren,Zimu Lu,Weikang Shi,Haotian Hou,Yunqiao Yang,Ke Wang,Aojun Zhou,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: The paper introduces a method to enhance code generation by using smaller blocks of code and curriculum training, showing substantial improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Improving code generation tasks with limited verifiable training data.

Method: Splitting code snippets into granular blocks and employing Abstract Syntax Tree splitting with curriculum training for Direct Preference Optimization.

Result: Substantial performance improvements on benchmarks like HumanEval, MBPP, APPS, LiveCodeBench, and BigCodeBench.

Conclusion: The proposed methodologies effectively enhance code generation capabilities, leveraging diverse training strategies and datasets.

Abstract: The code generation capabilities of Large Language Models (LLMs) have
advanced applications like tool invocation and problem-solving. However,
improving performance in code-related tasks remains challenging due to limited
training data that is verifiable with accurate test cases. While Direct
Preference Optimization (DPO) has shown promise, existing methods for
generating test cases still face limitations. In this paper, we propose a novel
approach that splits code snippets into smaller, granular blocks, creating more
diverse DPO pairs from the same test cases. Additionally, we introduce the
Abstract Syntax Tree (AST) splitting and curriculum training method to enhance
the DPO training. Our approach demonstrates significant improvements in code
generation tasks, as validated by experiments on benchmark datasets such as
HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data
are available at https://github.com/SenseLLM/StructureCoder.

</details>


### [45] [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](https://arxiv.org/abs/2508.19533)
*Kun Peng,Cong Cao,Hao Peng,Guanlin Wu,Zhifeng Hao,Lei Jiang,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: The paper tackles the challenge of recognizing unseen emotions in conversations by introducing the Unseen Emotion Recognition in Conversation (UERC) task and proposing a prototype-based framework called ProEmoTrans.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a consensus on emotion classification in psychology and the difficulty of recognizing unseen emotions in real-world applications.

Method: The authors propose ProEmoTrans, which includes a prototype-based approach supported by: (1) LLM-enhanced emotion descriptions, (2) a parameter-free mechanism for encoding utterances in long conversations, and (3) an improved Attention Viterbi Decoding for transferring seen to unseen emotion transitions.

Result: ProEmoTrans is evaluated through extensive experiments on three datasets, demonstrating its effectiveness as a strong baseline for the UERC task.

Conclusion: ProEmoTrans successfully addresses key challenges in unseen emotion recognition and establishes a foundation for further exploration in this area.

Abstract: Current Emotion Recognition in Conversation (ERC) research follows a
closed-domain assumption. However, there is no clear consensus on emotion
classification in psychology, which presents a challenge for models when it
comes to recognizing previously unseen emotions in real-world applications. To
bridge this gap, we introduce the Unseen Emotion Recognition in Conversation
(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based
emotion transfer framework. This prototype-based approach shows promise but
still faces key challenges: First, implicit expressions complicate emotion
definition, which we address by proposing an LLM-enhanced description approach.
Second, utterance encoding in long conversations is difficult, which we tackle
with a proposed parameter-free mechanism for efficient encoding and overfitting
prevention. Finally, the Markovian flow nature of emotions is hard to transfer,
which we address with an improved Attention Viterbi Decoding (AVD) method to
transfer seen emotion transitions to unseen emotions. Extensive experiments on
three datasets show that our method serves as a strong baseline for preliminary
exploration in this new area.

</details>


### [46] [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
*Jio Choi,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: This paper examines how large language models (LLMs) respond to ambiguities and exploit loopholes, presenting both an opportunity to study their pragmatic reasoning and a potential AI safety risk.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs deal with ambiguities and conflicting goals is crucial, as it sheds light on their contextual reasoning and highlights alignment issues.

Method: The study designs scenarios involving ambiguous instructions and conflicting goals, assessing LLMs' ability to exploit such ambiguities in contexts like scalar implicature, structural ambiguities, and power dynamics.

Result: The research finds that advanced LLMs, both open-source and proprietary, can exploit ambiguities to achieve preassigned goals, suggesting this as a potential safety concern.

Conclusion: LLMs' readiness to exploit loopholes highlights their pragmatic reasoning capabilities but underscores the need to address alignment risks in AI design.

Abstract: Studying the responses of large language models (LLMs) to loopholes presents
a two-fold opportunity. First, it affords us a lens through which to examine
ambiguity and pragmatics in LLMs, since exploiting a loophole requires
identifying ambiguity and performing sophisticated pragmatic reasoning. Second,
loopholes pose an interesting and novel alignment problem where the model is
presented with conflicting goals and can exploit ambiguities to its own
advantage. To address these questions, we design scenarios where LLMs are given
a goal and an ambiguous user instruction in conflict with the goal, with
scenarios covering scalar implicature, structural ambiguities, and power
dynamics. We then measure different models' abilities to exploit loopholes to
satisfy their given goals as opposed to the goals of the user. We find that
both closed-source and stronger open-source models can identify ambiguities and
exploit their resulting loopholes, presenting a potential AI safety risk. Our
analysis indicates that models which exploit loopholes explicitly identify and
reason about both ambiguity and conflicting goals.

</details>


### [47] [Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts](https://arxiv.org/abs/2508.19578)
*Jiaqi Deng,Yuho Lee,Nicole Hee-Yeon Kim,Hyangsuk Min,Taewon Yun,Minjeong Ban,Kim Yul,Hwanjun Song*

Main category: cs.CL

TL;DR: HAMLET is a framework to evaluate long-context comprehension of LLMs, structured hierarchically with query-focused summarization.


<details>
  <summary>Details</summary>
Motivation: Address the need for reliable, efficient evaluation of how well LLMs comprehend information across different granularities.

Method: Developed an automated framework using a three-level key-fact hierarchy and query-focused summarization, validated through human studies.

Result: Showed over 90% alignment with expert human judgments and revealed comprehension deficiencies in LLMs, especially for fine-grained details.

Conclusion: HAMLET provides an effective, scalable tool for evaluating and analyzing LLMs' long-context understanding performance.

Abstract: We introduce HAMLET, a holistic and automated framework for evaluating the
long-context comprehension of large language models (LLMs). HAMLET structures
source texts into a three-level key-fact hierarchy at root-, branch-, and
leaf-levels, and employs query-focused summarization to evaluate how well
models recall and faithfully represent information at each level. To validate
the reliability of our fully automated pipeline, we conduct a systematic human
study, showing that our automatic evaluation achieves over 90% agreement with
expert human judgments, while reducing the cost by up to 25 times. HAMLET
reveals that LLMs struggle with fine-grained comprehension, especially at the
leaf level, and are sensitive to positional effects like the
lost-in-the-middle. Analytical queries pose greater challenges than narrative
ones, and consistent performance gaps emerge between open-source and
proprietary models, as well as across model scales. Our code and dataset are
publicly available at https://github.com/DISL-Lab/HAMLET.

</details>


### [48] [ArgCMV: An Argument Summarization Benchmark for the LLM-era](https://arxiv.org/abs/2508.19580)
*Omkar Gurjar,Agam Goyal,Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: The paper critiques the limitations of the ArgKP21 dataset for argument key point extraction and introduces a new, more challenging dataset (ArgCMV) sourced from actual online debates containing around 12K arguments.


<details>
  <summary>Details</summary>
Motivation: The authors want to address the limitations of ArgKP21, which is widely used but lacks complexity and does not represent real-world human conversations.

Method: They create ArgCMV, a new dataset of approximately 12K arguments spread over 3K topics using state-of-the-art large language models. The complexity includes longer arguments, co-reference, subjective units, and diverse topics.

Result: Existing methods and baseline models do not perform well on the newly introduced ArgCMV dataset, highlighting its higher complexity and the need for advancements.

Conclusion: The ArgCMV dataset sets a new benchmark for argument key point extraction, encouraging further research in LLM-driven summarization for long, complex online discussions.

Abstract: Key point extraction is an important task in argument summarization which
involves extracting high-level short summaries from arguments. Existing
approaches for KP extraction have been mostly evaluated on the popular ArgKP21
dataset. In this paper, we highlight some of the major limitations of the
ArgKP21 dataset and demonstrate the need for new benchmarks that are more
representative of actual human conversations. Using SoTA large language models
(LLMs), we curate a new argument key point extraction dataset called ArgCMV
comprising of around 12K arguments from actual online human debates spread
across over 3K topics. Our dataset exhibits higher complexity such as longer,
co-referencing arguments, higher presence of subjective discourse units, and a
larger range of topics over ArgKP21. We show that existing methods do not adapt
well to ArgCMV and provide extensive benchmark results by experimenting with
existing baselines and latest open source models. This work introduces a novel
KP extraction dataset for long-context online discussions, setting the stage
for the next generation of LLM-driven summarization research.

</details>


### [49] [Towards stable AI systems for Evaluating Arabic Pronunciations](https://arxiv.org/abs/2508.19587)
*Hadi Zaatiti,Hatem Hajri,Osama Abdullah,Nader Masmoudi*

Main category: cs.CL

TL;DR: Modern Arabic ASR systems struggle with classifying isolated letters due to lack of context and unique acoustic characteristics. A new dataset and training techniques are proposed to improve performance but challenges remain.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges faced by modern Arabic ASR systems in accurately classifying isolated letters. This task is critical for applications in language learning, speech therapy, and phonetic research.

Method: The paper introduces a diverse, diacritised corpus of isolated Arabic letters, employs wav2vec 2.0 embeddings, trains a lightweight neural network, and applies adversarial training to improve robustness against noisy conditions.

Result: Performance on isolated Arabic letter classification improved from 35% to 65% accuracy with the proposed methods. Adversarial training reduced accuracy loss due to noise from 33% to 9%, maintaining clean-speech results.

Conclusion: The study highlights the challenges of isolated letter recognition and proposes methods that significantly enhance accuracy and robustness. Future research will adapt these techniques to broader ASR tasks like word- and sentence-level transcription.

Abstract: Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and
sentence-level transcription, yet struggle to classify isolated letters. In
this study, we show that this phoneme-level task, crucial for language
learning, speech therapy, and phonetic research, is challenging because
isolated letters lack co-articulatory cues, provide no lexical context, and
last only a few hundred milliseconds. Recogniser systems must therefore rely
solely on variable acoustic cues, a difficulty heightened by Arabic's emphatic
(pharyngealized) consonants and other sounds with no close analogues in many
languages. This study introduces a diverse, diacritised corpus of isolated
Arabic letters and demonstrates that state-of-the-art wav2vec 2.0 models
achieve only 35% accuracy on it. Training a lightweight neural network on
wav2vec embeddings raises performance to 65%. However, adding a small amplitude
perturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we
apply adversarial training, limiting the noisy-speech drop to 9% while
preserving clean-speech accuracy. We detail the corpus, training pipeline, and
evaluation protocol, and release, on demand, data and code for reproducibility.
Finally, we outline future work extending these methods to word- and
sentence-level frameworks, where precise letter pronunciation remains critical.

</details>


### [50] [Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.19594)
*Jun Bai,Minghao Tong,Yang Liu,Zixia Jia,Zilong Zheng*

Main category: cs.CL

TL;DR: Researchers propose 'Router Lens' to identify context-faithful experts in large language models, enabling improved context grounding; they also introduce 'Context-faithful Expert Fine-Tuning' (CEFT), which enhances model efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Large language models often generate irrelevant responses in context-sensitive tasks due to struggles in grounding outputs in the provided context.

Method: The study uses Router Lens to pinpoint experts specialized in context utilization, followed by CEFT to selectively fine-tune these experts for optimizing context grounding.

Result: CEFT exhibits efficiency advantages while matching or surpassing full fine-tuning performance across various benchmarks and models.

Conclusion: Targeted expert specialization through CEFT improves context accuracy and efficiency, paving the way for more reliable language model optimization strategies.

Abstract: Context faithfulness is essential for reliable reasoning in context-dependent
scenarios. However, large language models often struggle to ground their
outputs in the provided context, resulting in irrelevant responses. Inspired by
the emergent expert specialization observed in mixture-of-experts
architectures, this work investigates whether certain experts exhibit
specialization in context utilization, offering a potential pathway toward
targeted optimization for improved context faithfulness. To explore this, we
propose Router Lens, a method that accurately identifies context-faithful
experts. Our analysis reveals that these experts progressively amplify
attention to relevant contextual information, thereby enhancing context
grounding. Building on this insight, we introduce Context-faithful Expert
Fine-Tuning (CEFT), a lightweight optimization approach that selectively
fine-tunes context-faithful experts. Experiments across a wide range of
benchmarks and models demonstrate that CEFT matches or surpasses the
performance of full fine-tuning while being significantly more efficient.

</details>


### [51] [LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.19614)
*Yang Sun,Lixin Zou,Dan Luo,Zhiyong Xie,Long Zhang,Liming Dong,Yunwei Zhao,Xixun Lin,Yanxiong Lu,Chenliang Li*

Main category: cs.CL

TL;DR: The paper introduces Layer Fused Decoding (LFD), a decoding strategy for retrieval-augmented generation (RAG) that improves external knowledge integration by leveraging layer-specific roles within large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the paradoxical phenomenon where noise injected into retrieved documents improves external knowledge utilization in LLMs, providing more efficient control and analysis for knowledge integration.

Method: The paper entails noise injection experiments to establish layer-specific roles and proposes LFD, a decoding strategy combining intermediate layer representations with final-layer outputs. The Internal Knowledge Score (IKS) is introduced to identify optimal intermediate layers.

Result: Experimental benchmarks validate that LFD enhances the integration of retrieved context knowledge in RAG systems, achieving better performance with minimal computational costs.

Conclusion: The paper concludes that LFD enables a more effective surfacing of external factual knowledge in RAG systems by exploiting the functional demarcation between LLM layers, offering new insights and practical techniques for external knowledge integration.

Abstract: Retrieval-augmented generation (RAG) incorporates external knowledge into
large language models (LLMs), improving their adaptability to downstream tasks
and enabling information updates. Surprisingly, recent empirical evidence
demonstrates that injecting noise into retrieved relevant documents
paradoxically facilitates exploitation of external knowledge and improves
generation quality. Although counterintuitive and challenging to apply in
practice, this phenomenon enables granular control and rigorous analysis of how
LLMs integrate external knowledge. Therefore, in this paper, we intervene on
noise injection and establish a layer-specific functional demarcation within
the LLM: shallow layers specialize in local context modeling, intermediate
layers focus on integrating long-range external factual knowledge, and deeper
layers primarily rely on parametric internal knowledge. Building on this
insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that
directly combines representations from an intermediate layer with final-layer
decoding outputs to fully exploit the external factual knowledge. To identify
the optimal intermediate layer, we introduce an internal knowledge score (IKS)
criterion that selects the layer with the lowest IKS value in the latter half
of layers. Experimental results across multiple benchmarks demonstrate that LFD
helps RAG systems more effectively surface retrieved context knowledge with
minimal cost.

</details>


### [52] [A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection](https://arxiv.org/abs/2508.19633)
*Chong Tian,Qirong Ho,Xiuying Chen*

Main category: cs.CL

TL;DR: Symbolic Adversarial Learning Framework (SALF) leverages symbolic learning to simulate adversarial interactions between misinformation generators and detectors, improving fake news detection methods.


<details>
  <summary>Details</summary>
Motivation: The rise of sophisticated fake news content due to rapid advancements in language models has challenged existing detection mechanisms that lack adaptability to evolving misinformation.

Method: SALF uses symbolic adversarial learning where agents refine themselves iteratively through natural language representations, structured debates, and adversarial training to optimize their detection/generation capabilities.

Result: Experiments show SALF significantly degrades detection performance of current models by generating refined fake news and enhances detection systems' ability to identify refined misinformation with up to 7.7% detection improvement.

Conclusion: SALF improves both the generation and detection cycles of fake news, offering a promising direction for creating more robust misinformation defense mechanisms.

Abstract: Rapid LLM advancements heighten fake news risks by enabling the automatic
generation of increasingly sophisticated misinformation. Previous detection
methods, including fine-tuned small models or LLM-based detectors, often
struggle with its dynamically evolving nature. In this work, we propose a novel
framework called the Symbolic Adversarial Learning Framework (SALF), which
implements an adversarial training paradigm by an agent symbolic learning
optimization process, rather than relying on numerical updates. SALF introduces
a paradigm where the generation agent crafts deceptive narratives, and the
detection agent uses structured debates to identify logical and factual flaws
for detection, and they iteratively refine themselves through such adversarial
interactions. Unlike traditional neural updates, we represent agents using
agent symbolic learning, where learnable weights are defined by agent prompts,
and simulate back-propagation and gradient descent by operating on natural
language representations of weights, loss, and gradients. Experiments on two
multilingual benchmark datasets demonstrate SALF's effectiveness, showing it
generates sophisticated fake news that degrades state-of-the-art detection
performance by up to 53.4% in Chinese and 34.2% in English on average. SALF
also refines detectors, improving detection of refined content by up to 7.7%.
We hope our work inspires further exploration into more robust, adaptable fake
news detection systems.

</details>


### [53] [Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design](https://arxiv.org/abs/2508.19665)
*Giovanni Pollo,Andrei Mihai Albu,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Loris Panaro,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: This paper proposes a methodology to use the Functional Mock-up Interface (FMI) standard to automatically wrap SystemC models for enhanced co-simulation.


<details>
  <summary>Details</summary>
Motivation: The automotive sector requires robust co-simulation methods for early validation and integration, but issues arise due to proprietary platforms and lack of standardized interfaces.

Method: The paper introduces an approach to automatically wrap SystemC models using the FMI standard, combining SystemC's modeling accuracy with FMI's interoperability benefits.

Result: The methodology is validated on real-world case studies involving complex designs, illustrating its effectiveness.

Conclusion: The approach provides a secure, scalable, and portable solution for integrating embedded components into co-simulation workflows, addressing the limitations in existing methodologies.

Abstract: The recent advancements of the automotive sector demand robust co-simulation
methodologies that enable early validation and seamless integration across
hardware and software domains. However, the lack of standardized interfaces and
the dominance of proprietary simulation platforms pose significant challenges
to collaboration, scalability, and IP protection. To address these limitations,
this paper presents an approach for automatically wrapping SystemC models by
using the Functional Mock-up Interface (FMI) standard. This method combines the
modeling accuracy and fast time-to-market of SystemC with the interoperability
and encapsulation benefits of FMI, enabling secure and portable integration of
embedded components into co-simulation workflows. We validate the proposed
methodology on real-world case studies, demonstrating its effectiveness with
complex designs.

</details>


### [54] [Survey of Specialized Large Language Model](https://arxiv.org/abs/2508.19667)
*Chenghan Yang,Ruiyu Zhao,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: This paper surveys the evolution of specialized large language models (LLMs), which focus on improving domain-specific performance through native architectures, technical innovations, and multimodal capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of general-purpose LLMs in specialized professional applications by examining advancements in domain-specific architectures and breakthroughs.

Method: The paper systematically analyzes the progression of LLMs across professional domains such as healthcare, finance, legal, and technical, highlighting innovations like sparse computation, quantization, and integration of multimodal capabilities.

Result: Specialized LLMs consistently outperform general-purpose models on domain-specific benchmarks due to technical advancements and native designs.

Conclusion: The findings underline the growing importance of specialized LLMs and their potential to address professional needs more effectively, with important opportunities in E-Commerce suggested.

Abstract: The rapid evolution of specialized large language models (LLMs) has
transitioned from simple domain adaptation to sophisticated native
architectures, marking a paradigm shift in AI development. This survey
systematically examines this progression across healthcare, finance, legal, and
technical domains. Besides the wide use of specialized LLMs, technical
breakthrough such as the emergence of domain-native designs beyond fine-tuning,
growing emphasis on parameter efficiency through sparse computation and
quantization, increasing integration of multimodal capabilities and so on are
applied to recent LLM agent. Our analysis reveals how these innovations address
fundamental limitations of general-purpose LLMs in professional applications,
with specialized models consistently performance gains on domain-specific
benchmarks. The survey further highlights the implications for E-Commerce field
to fill gaps in the field.

</details>


### [55] [Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality](https://arxiv.org/abs/2508.19689)
*Xiaoying Zhang*

Main category: cs.CL

TL;DR: The paper explores techniques for creating autonomous task bots capable of learning and adapting in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Building adaptable, extensible, and accurate dialog bots without human intervention is a major challenge.

Method: Innovative mechanisms are examined to enable autonomous learning and adaptability for task bots.

Result: The paper identifies obstacles and proposes solutions for enhancing bot autonomy.

Conclusion: Advancing research into techniques that allow bots to operate independently in shifting environments is key to addressing the outlined challenges.

Abstract: Developing adaptable, extensible, and accurate task bots with minimal or zero
human intervention is a significant challenge in dialog research. This thesis
examines the obstacles and potential solutions for creating such bots, focusing
on innovative techniques that enable bots to learn and adapt autonomously in
constantly changing environments.

</details>


### [56] [Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models](https://arxiv.org/abs/2508.19720)
*Yilin Wang,Heng Wang,Yuyang Bai,Minnan Luo*

Main category: cs.CL

TL;DR: This paper introduces CSKS, a lightweight framework to finely and continuously control large language models' sensitivity to contextual knowledge without modifying their weights.


<details>
  <summary>Details</summary>
Motivation: The issue at hand is the inefficiency or ineffectiveness of existing approaches to resolve knowledge contradictions in LLMs when parametric knowledge conflicts with contextual information. Current methods often fail for large or black-box models, or cannot offer ongoing adjustments to contextual sensitivity.

Method: CSKS involves tuning two small proxy models and leveraging differences in their output distributions to adjust a large LLM's output distributions without altering its weights.

Result: Experiments show that CSKS effectively allows dynamic and precise control over LLMs' sensitivity, enabling these models to prioritize contextual or parametric knowledge based on need, validated through synthetic and real conflict data.

Conclusion: CSKS is a practical and efficient framework for resolving knowledge conflicts in LLMs by continuously steering their sensitivity to contextual knowledge, showcasing flexibility and robustness in application.

Abstract: In Large Language Models (LLMs) generation, there exist knowledge conflicts
and scenarios where parametric knowledge contradicts knowledge provided in the
context. Previous works studied tuning, decoding algorithms, or locating and
editing context-aware neurons to adapt LLMs to be faithful to new contextual
knowledge. However, they are usually inefficient or ineffective for large
models, not workable for black-box models, or unable to continuously adjust
LLMs' sensitivity to the knowledge provided in the context. To mitigate these
problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a
simple framework that can steer LLMs' sensitivity to contextual knowledge
continuously at a lightweight cost. Specifically, we tune two small LMs (i.e.
proxy models) and use the difference in their output distributions to shift the
original distribution of an LLM without modifying the LLM weights. In the
evaluation process, we not only design synthetic data and fine-grained metrics
to measure models' sensitivity to contextual knowledge but also use a real
conflict dataset to validate CSKS's practical efficacy. Extensive experiments
demonstrate that our framework achieves continuous and precise control over
LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity
and reduced sensitivity, thereby allowing LLMs to prioritize either contextual
or parametric knowledge as needed flexibly. Our data and code are available at
https://github.com/OliveJuiceLin/CSKS.

</details>


### [57] [CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese](https://arxiv.org/abs/2508.19721)
*Carlos Carvalho,Francisco Teixeira,Catarina Botelho,Anna Pompili,Rubén Solera-Ureña,Sérgio Paulo,Mariana Julião,Thomas Rolland,John Mendonça,Diogo Pereira,Isabel Trancoso,Alberto Abad*

Main category: cs.CL

TL;DR: CAMÕES framework addresses the lack of European Portuguese (EP) resources in automatic speech recognition by offering evaluation benchmarks and state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing Automatic Speech Recognition resources heavily focus on Brazilian Portuguese, neglecting European Portuguese and other varieties.

Method: The study introduces CAMÕES, consisting of (1) a benchmark with 46h EP test data across domains, and (2) models including fine-tuned foundation models and scratch-trained E-Branchformer models using 425h EP data.

Result: Both fine-tuned foundation models and E-Branchformer showed comparable performance, with improvements of 35% WER over zero-shot models, defining a new standard for EP recognition.

Conclusion: CAMÕES establishes a robust tool for advancing Automatic Speech Recognition in European Portuguese and other varieties, bridging an important gap in resource availability.

Abstract: Existing resources for Automatic Speech Recognition in Portuguese are mostly
focused on Brazilian Portuguese, leaving European Portuguese (EP) and other
varieties under-explored. To bridge this gap, we introduce CAM\~OES, the first
open framework for EP and other Portuguese varieties. It consists of (1) a
comprehensive evaluation benchmark, including 46h of EP test data spanning
multiple domains; and (2) a collection of state-of-the-art models. For the
latter, we consider multiple foundation models, evaluating their zero-shot and
fine-tuned performances, as well as E-Branchformer models trained from scratch.
A curated set of 425h of EP was used for both fine-tuning and training. Our
results show comparable performance for EP between fine-tuned foundation models
and the E-Branchformer. Furthermore, the best-performing models achieve
relative improvements above 35% WER, compared to the strongest zero-shot
foundation model, establishing a new state-of-the-art for EP and other
varieties.

</details>


### [58] [NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](https://arxiv.org/abs/2508.19724)
*Aritra Dutta,Swapnanil Mukherjee,Deepanway Ghosal,Somak Aditya*

Main category: cs.CL

TL;DR: This paper proposes NLKI, a framework to improve small vision-language models (sVLMs) by integrating natural language commonsense knowledge, achieving notable accuracy improvements across benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Small vision-language models struggle with commonsense visual question answering due to missing contextual knowledge from the image or question. The paper seeks to enhance these models' performance by integrating external commonsense knowledge.

Method: The authors introduce NLKI, an end-to-end framework that combines natural language fact retrieval (using ColBERTv2), LLM-crafted explanations, and external knowledge signals to bolster sVLM performance. Additionally, noise-robust losses are applied during model finetuning.

Result: The approach boosts answer accuracy by up to 7% across three datasets (CRIC, AOKVQA, e-SNLI-VE), with an additional accuracy increase of up to 5.5% using noise-robust training methods, allowing sVLMs to match or surpass medium-sized VLMs.

Conclusion: NLKI demonstrates that parameter-efficient commonsense reasoning for small models is feasible through strategic external knowledge integration and noise-aware training, providing a path to close the gap with larger generative models.

Abstract: Commonsense visual-question answering often hinges on knowledge that is
missing from the image or the question. Small vision-language models (sVLMs)
such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative
counterparts. To study the effect of careful commonsense knowledge integration
on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural
language facts, (ii) prompts an LLM to craft natural language explanations, and
(iii) feeds both signals to sVLMs respectively across two commonsense VQA
datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts
retrieved using a fine-tuned ColBERTv2 and an object information-enriched
prompt yield explanations that largely cut down hallucinations, while lifting
the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA
and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B
and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional
finetuning using noise-robust losses (such as symmetric cross entropy and
generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our
findings expose when LLM-based commonsense knowledge beats retrieval from
commonsense knowledge bases, how noise-aware training stabilises small models
in the context of external knowledge augmentation, and why parameter-efficient
commonsense reasoning is now within reach for 250M models.

</details>


### [59] [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)
*Wenhao Li,Yuxin Zhang,Gen Luo,Haiyuan Wan,Ziyang Gong,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: The paper introduces Spotlight Attention, a method aiming to improve the efficiency and effectiveness of KV caching in LLMs using non-linear hashing functions, alongside robust training frameworks and optimized retrieval techniques.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in KV caching in LLMs, where existing random linear hashing methods fail due to suboptimal query-key distributions.

Method: Spotlight Attention uses non-linear hashing functions coupled with a Bradley-Terry ranking-based loss framework to optimize query-key embeddings and employs specialized CUDA kernels for fast retrieval.

Result: Spotlight Attention improves retrieval precision, reduces hash code length 5×, and achieves fast token retrieval under 100μs with 3× higher throughput on an A100 GPU compared to traditional approaches.

Conclusion: The proposed Spotlight Attention significantly enhances KV cache performance for LLMs, offering faster, more efficient, and robust decoding mechanisms.

Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs)
significantly accelerates inference. Dynamically selecting critical KV caches
during decoding helps maintain performance. Existing methods use random linear
hashing to identify important tokens, but this approach is inefficient due to
the orthogonal distribution of queries and keys within two narrow cones in
LLMs. We introduce Spotlight Attention, a novel method that employs non-linear
hashing functions to optimize the embedding distribution of queries and keys,
enhancing coding efficiency and robustness. We also developed a lightweight,
stable training framework using a Bradley-Terry ranking-based loss, enabling
optimization of the non-linear hashing module on GPUs with 16GB memory in 8
hours. Experimental results show that Spotlight Attention drastically improves
retrieval precision while shortening the length of the hash code at least
5$\times$ compared to traditional linear hashing. Finally, we exploit the
computational advantages of bitwise operations by implementing specialized CUDA
kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a
single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla
decoding.

</details>


### [60] [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
*Yixuan Tang,Yuanyuan Shi,Yiqun Sun,Anthony Kum Hoe Tung*

Main category: cs.CL

TL;DR: The paper introduces NEWSCOPE, a two-stage framework aimed at improving news retrieval diversity to ensure broader event coverage through fine-grained semantic variation modeling.


<details>
  <summary>Details</summary>
Motivation: Current news retrieval systems focus heavily on textual relevance, which often leads to redundancy and limited exposure to varied perspectives on real-world events.

Method: NEWSCOPE uses a two-stage framework: first, it retrieves topically relevant content via dense retrieval; second, it leverages sentence-level clustering and diversity-aware re-ranking to present complementary and varied information.

Result: Three new interpretable metrics and two paragraph-level benchmarks are introduced. Experiments show that NEWSCOPE outperforms traditional methods, achieving higher diversity without sacrificing relevance.

Conclusion: The framework promotes better event understanding by reducing redundancy and enhancing retrieval diversity via interpretable and fine-grained modeling.

Abstract: Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

</details>


### [61] [Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance](https://arxiv.org/abs/2508.19764)
*Pedro Henrique Luz de Araujo,Paul Röttger,Dirk Hovy,Benjamin Roth*

Main category: cs.CL

TL;DR: This paper explores the effects of expert persona prompting on task improvement in large language models (LLMs), evaluating its benefits, sensitivity to irrelevant attributes, and fidelity to persona details.


<details>
  <summary>Details</summary>
Motivation: To understand why expert persona prompting yields mixed results in task performance and to establish clear evaluation criteria for its effective use.

Method: Analyzed existing literature to identify three evaluation desiderata (performance advantage, robustness, and fidelity), then tested 9 state-of-the-art LLMs across 27 tasks according to these criteria.

Result: Expert personas generally improved or had no significant impact on performance, but models showed significant sensitivity to irrelevant persona attributes. Effects of persona fidelity were inconsistent across tasks.

Conclusion: Careful persona design is essential, as well as robust evaluation metrics, to optimize LLM performance with persona prompting.

Abstract: Expert persona prompting -- assigning roles such as expert in math to
language models -- is widely used for task improvement. However, prior work
shows mixed results on its effectiveness, and does not consider when and why
personas should improve performance. We analyze the literature on persona
prompting for task improvement and distill three desiderata: 1) performance
advantage of expert personas, 2) robustness to irrelevant persona attributes,
and 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs
across 27 tasks with respect to these desiderata. We find that expert personas
usually lead to positive or non-significant performance changes. Surprisingly,
models are highly sensitive to irrelevant persona details, with performance
drops of almost 30 percentage points. In terms of fidelity, we find that while
higher education, specialization, and domain-relatedness can boost performance,
their effects are often inconsistent or negligible across tasks. We propose
mitigation strategies to improve robustness -- but find they only work for the
largest, most capable models. Our findings underscore the need for more careful
persona design and for evaluation schemes that reflect the intended effects of
persona usage.

</details>


### [62] [T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables](https://arxiv.org/abs/2508.19813)
*Jie Zhang,Changzai Pan,Kaiwen Wei,Sishi Xiong,Yu Zhao,Xiangyu Li,Jiaxin Peng,Xiaoyan Gu,Jian Yang,Wenhan Chang,Zhenhe Wu,Jiang Zhong,Shuangyong Song,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: This paper introduces a task called table-to-report generation, supported by a bilingual benchmark named T2R-bench, to address challenges in transforming table information into accurate industrial reports.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve report generation from complex tables, a bottleneck in industrial applications, and overcome limitations in existing table reasoning benchmarks.

Method: The authors propose the T2R-bench, containing 457 real-world industrial tables across 19 domains and 4 types, paired with evaluation criteria to assess report generation quality.

Result: Experiments on 25 large language models show that even advanced models struggle with the task, where the best model achieves only 62.71 overall score.

Conclusion: There is substantial room for improvement in table-to-report generation, and T2R-bench serves as a benchmarking foundation to guide progress in this area.

Abstract: Extensive research has been conducted to explore the capabilities of large
language models (LLMs) in table reasoning. However, the essential task of
transforming tables information into reports remains a significant challenge
for industrial applications. This task is plagued by two critical issues: 1)
the complexity and diversity of tables lead to suboptimal reasoning outcomes;
and 2) existing table benchmarks lack the capacity to adequately assess the
practical application of this task. To fill this gap, we propose the
table-to-report task and construct a bilingual benchmark named T2R-bench, where
the key information flow from the tables to the reports for this task. The
benchmark comprises 457 industrial tables, all derived from real-world
scenarios and encompassing 19 industry domains as well as 4 types of industrial
tables. Furthermore, we propose an evaluation criteria to fairly measure the
quality of report generation. The experiments on 25 widely-used LLMs reveal
that even state-of-the-art models like Deepseek-R1 only achieves performance
with 62.71 overall score, indicating that LLMs still have room for improvement
on T2R-bench. Source code and data will be available after acceptance.

</details>


### [63] [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)
*Sikuan Yan,Xiufeng Yang,Zuchao Huang,Ercong Nie,Zifeng Ding,Zonggen Li,Xiaowen Ma,Hinrich Schütze,Volker Tresp,Yunpu Ma*

Main category: cs.CL

TL;DR: Memory-R1 introduces a reinforcement learning framework to enhance long-horizon reasoning in Large Language Models by actively managing external memory.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of statelessness and constrained reasoning in LLMs due to their finite context windows.

Method: The authors propose two RL-driven agents: a Memory Manager for structured memory operations and an Answer Agent for retrieving relevant entries for reasoning. Both agents are trained using PPO and GRPO.

Result: Memory-R1 surpasses existing methods using minimal training data, demonstrating generalization across various tasks and compatibility with different LLMs.

Conclusion: This work highlights the potential of reinforcement learning to create more memory-aware and agentic Large Language Models, paving the way for persistent reasoning systems.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
a wide range of NLP tasks, but they remain fundamentally stateless, constrained
by limited context windows that hinder long-horizon reasoning. Recent efforts
to address this limitation often augment LLMs with an external memory bank, yet
most existing pipelines are static and heuristic-driven, lacking any learned
mechanism for deciding what to store, update, or retrieve. We present
Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the
ability to actively manage and utilize external memory through two specialized
agents: a Memory Manager that learns to perform structured memory operations
{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant
entries and reasons over them to produce an answer. Both agents are fine-tuned
with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and
use with minimal supervision. With as few as 152 question-answer pairs and a
corresponding temporal memory bank for training, Memory-R1 outperforms the most
competitive existing baseline and demonstrates strong generalization across
diverse question types and LLM backbones. Beyond presenting an effective
approach, this work provides insights into how RL can unlock more agentic,
memory-aware behaviors in LLMs, pointing toward richer, more persistent
reasoning systems.

</details>


### [64] [Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis](https://arxiv.org/abs/2508.19831)
*Anusha Kamath,Kanishk Singla,Rakesh Paul,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: This paper introduces five Hindi evaluation datasets for benchmarking instruction-tuned LLMs, addressing linguistic and cultural nuances.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of evaluating instruction-tuned LLMs in Hindi due to the lack of culturally and linguistically nuanced benchmarks.

Method: Developed five Hindi evaluation datasets through human annotation from scratch and a translate-and-verify process. These are used for benchmarking LLM capabilities in Hindi.

Result: An extensive benchmarking and detailed comparative analysis of open-source LLMs supporting Hindi were accomplished using these datasets.

Conclusion: The datasets not only enable Hindi evaluations but also offer a replicable approach for creating benchmarks in other low-resource languages.

Abstract: Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is
challenging due to a lack of high-quality benchmarks, as direct translation of
English datasets fails to capture crucial linguistic and cultural nuances. To
address this, we introduce a suite of five Hindi LLM evaluation datasets:
IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created
using a methodology that combines from-scratch human annotation with a
translate-and-verify process. We leverage this suite to conduct an extensive
benchmarking of open-source LLMs supporting Hindi, providing a detailed
comparative analysis of their current capabilities. Our curation process also
serves as a replicable methodology for developing benchmarks in other
low-resource languages.

</details>


### [65] [Scalable and consistent few-shot classification of survey responses using text embeddings](https://arxiv.org/abs/2508.19836)
*Jonas Timmann Mjaaland,Markus Fleten Kreutzer,Halvor Tyseng,Rebeckah K. Fussell,Gina Passante,N. G. Holmes,Anders Malthe-Sørenssen,Tor Ole B. Odden*

Main category: cs.CL

TL;DR: This paper introduces a text embedding-based classification framework for qualitative analysis of survey responses, achieving performance comparable to expert human coders while requiring minimal example data and maintaining compatibility with traditional workflows.


<details>
  <summary>Details</summary>
Motivation: Traditional qualitative analysis of open-ended survey responses is time-intensive and inconsistent, with existing NLP methods proving unsuitable for common qualitative workflows.

Method: The authors develop a classification framework based on text embeddings that requires a few examples per category and integrates well with qualitative processes. They benchmarked its effectiveness against expert human coders using a physics survey dataset.

Result: The framework achieves strong performance (Cohen's Kappa: 0.74 to 0.83) against human coders and shows improved performance with text embedding model fine-tuning. It can also audit previously-analyzed datasets.

Conclusion: This method allows scalable and interpretable qualitative analysis, suitable for handling large datasets and augmenting manual coding workflows.

Abstract: Qualitative analysis of open-ended survey responses is a commonly-used
research method in the social sciences, but traditional coding approaches are
often time-consuming and prone to inconsistency. Existing solutions from
Natural Language Processing such as supervised classifiers, topic modeling
techniques, and generative large language models have limited applicability in
qualitative analysis, since they demand extensive labeled data, disrupt
established qualitative workflows, and/or yield variable results. In this
paper, we introduce a text embedding-based classification framework that
requires only a handful of examples per category and fits well with standard
qualitative workflows. When benchmarked against human analysis of a conceptual
physics survey consisting of 2899 open-ended responses, our framework achieves
a Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in
an exhaustive coding scheme. We further show how performance of this framework
improves with fine-tuning of the text embedding model, and how the method can
be used to audit previously-analyzed datasets. These findings demonstrate that
text embedding-assisted coding can flexibly scale to thousands of responses
without sacrificing interpretability, opening avenues for deductive qualitative
analysis at scale.

</details>


### [66] [TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation](https://arxiv.org/abs/2508.19856)
*Shashi Kumar,Srikanth Madikeri,Esaú Villatoro-Tello,Sergio Burdisso,Pradeep Rangappa,Andrés Carofilis,Petr Motlicek,Karthik Pandia,Shankar Venkatesan,Kadri Hacioğlu,Andreas Stolcke*

Main category: cs.CL

TL;DR: TokenVerse++ enhances TokenVerse by allowing multitasking with datasets having incomplete labels via learnable vectors for task activation.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in multitasking frameworks like TokenVerse, which require full task labeling for all utterances, thus limiting scalability.

Method: Integration of learnable vectors in the acoustic embedding space for dynamic task activation in XLSR-Transducer ASR, enabling partial-label training.

Result: TokenVerse++ successfully incorporates partially labeled datasets, achieving comparable or better performance across tasks, including ASR and language identification.

Conclusion: TokenVerse++ is a more practical multitask framework, maintaining or improving performance while offering flexibility in training with partial labels.

Abstract: Token-based multitasking frameworks like TokenVerse require all training
utterances to have labels for all tasks, hindering their ability to leverage
partially annotated datasets and scale effectively. We propose TokenVerse++,
which introduces learnable vectors in the acoustic embedding space of the
XLSR-Transducer ASR model for dynamic task activation. This core mechanism
enables training with utterances labeled for only a subset of tasks, a key
advantage over TokenVerse. We demonstrate this by successfully integrating a
dataset with partial labels, specifically for ASR and an additional task,
language identification, improving overall performance. TokenVerse++ achieves
results on par with or exceeding TokenVerse across multiple tasks, establishing
it as a more practical multitask alternative without sacrificing ASR
performance.

</details>


### [67] [Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning](https://arxiv.org/abs/2508.19873)
*Vanessa Toborek,Sebastian Müller,Tim Selbach,Tamás Horváth,Christian Bauckhage*

Main category: cs.CL

TL;DR: This paper explores how human-curated simple language can enhance training through curriculum learning, showing that structured data improves perplexity while random ordering does not.


<details>
  <summary>Details</summary>
Motivation: The study seeks to resolve the challenge of defining and measuring linguistic difficulty to improve curriculum learning efficiency.

Method: The authors tested BERT-tiny using human-curated labels from Simple Wikipedia in structured curricula versus competence-based heuristics.

Result: Results show that structured curricula using simple data significantly improves perplexity, especially on simple language, compared to random ordering.

Conclusion: Human-curated intuitions about linguistic difficulty can benefit curriculum learning for language model pre-training.

Abstract: Curriculum learning (CL) aims to improve training by presenting data from
"easy" to "hard", yet defining and measuring linguistic difficulty remains an
open challenge. We investigate whether human-curated simple language can serve
as an effective signal for CL. Using the article-level labels from the Simple
Wikipedia corpus, we compare label-based curricula to competence-based
strategies relying on shallow heuristics. Our experiments with a BERT-tiny
model show that adding simple data alone yields no clear benefit. However,
structuring it via a curriculum -- especially when introduced first --
consistently improves perplexity, particularly on simple language. In contrast,
competence-based curricula lead to no consistent gains over random ordering,
probably because they fail to effectively separate the two classes. Our results
suggest that human intuition about linguistic difficulty can guide CL for
language model pre-training.

</details>


### [68] [AI-Powered Detection of Inappropriate Language in Medical School Curricula](https://arxiv.org/abs/2508.19883)
*Chiman Salavati,Shannon Song,Scott A. Hale,Roberto E. Montenegro,Shiri Dori-Hacohen,Fabricio Murai*

Main category: cs.CL

TL;DR: This paper evaluates different machine learning approaches to identify and classify inappropriate language in medical instructional materials to improve patient-centered communication.


<details>
  <summary>Details</summary>
Motivation: Inappropriate language in medical instructional materials can negatively influence clinical training and patient outcomes. Identifying such language manually is costly and impractical, creating the need for automated solutions.

Method: The study investigates the performance of fine-tuned small language models (SLMs) and pre-trained large language models (LLMs) with in-context learning on a dataset of approximately 500 medical documents. It evaluates multiple configurations, including general IUL classifiers, subcategory-specific binary classifiers, and hierarchical pipelines.

Result: SLMs outperformed LLMs in accuracy, with multilabel classifiers performing best on annotated data. Additionally, augmenting training data with unflagged excerpts improved performance significantly for subcategory-specific classifiers.

Conclusion: Fine-tuned small language models are effective tools for detecting and classifying inappropriate language in medical curricula, offering a viable method to enhance healthcare communication and training materials.

Abstract: The use of inappropriate language -- such as outdated, exclusionary, or
non-patient-centered terms -- medical instructional materials can significantly
influence clinical training, patient interactions, and health outcomes. Despite
their reputability, many materials developed over past decades contain examples
now considered inappropriate by current medical standards. Given the volume of
curricular content, manually identifying instances of inappropriate use of
language (IUL) and its subcategories for systematic review is prohibitively
costly and impractical. To address this challenge, we conduct a first-in-class
evaluation of small language models (SLMs) fine-tuned on labeled data and
pre-trained LLMs with in-context learning on a dataset containing approximately
500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL
classifier, (2) subcategory-specific binary classifiers, (3) a multilabel
classifier, and (4) a two-stage hierarchical pipeline for general IUL detection
followed by multilabel classification. For LLMs, we consider variations of
prompts that include subcategory definitions and/or shots. We found that both
LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed
by SLMs. While the multilabel classifier performs best on annotated data,
supplementing training with unflagged excerpts as negative examples boosts the
specific classifiers' AUC by up to 25%, making them most effective models for
mitigating harmful language in medical curricula.

</details>


### [69] [Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement](https://arxiv.org/abs/2508.19887)
*Mohammed Rakibul Hasan,Rafi Majid,Ahanaf Tahmid*

Main category: cs.CL

TL;DR: The paper introduces Bangla-Bayanno, a Bangla Visual Question Answering (VQA) dataset built using a multilingual translation refinement pipeline to ensure high-quality, open-ended questions and answers.


<details>
  <summary>Details</summary>
Motivation: To address the lack of high-quality VQA datasets for Bangla, a low-resource language, by overcoming issues like domain-specific constraints and errors from manual annotation and low-quality translations.

Method: Developed a multilingual LLM-assisted translation refinement pipeline to produce clear and accurate translations, ensuring diverse question types (nominal, quantitative, polar).

Result: Created a dataset comprising 52,650 question-answer pairs across 4750+ images, providing the most comprehensive VQA benchmark in Bangla.

Conclusion: Bangla-Bayanno advances research in low-resource multimodal AI systems and promotes inclusivity in AI by providing a robust dataset for Bangla VQA applications.

Abstract: In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question
Answering (VQA) Dataset in Bangla, a widely used, low-resource language in
multimodal AI research. The majority of existing datasets are either manually
annotated with an emphasis on a specific domain, query type, or answer type or
are constrained by niche answer formats. In order to mitigate human-induced
errors and guarantee lucidity, we implemented a multilingual LLM-assisted
translation refinement pipeline. This dataset overcomes the issues of
low-quality translations from multilingual sources. The dataset comprises
52,650 question-answer pairs across 4750+ images. Questions are classified into
three distinct answer types: nominal (short descriptive), quantitative
(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive
open-source, high-quality VQA benchmark in Bangla, aiming to advance research
in low-resource multimodal learning and facilitate the development of more
inclusive AI systems.

</details>


### [70] [Logical Reasoning with Outcome Reward Models for Test-Time Scaling](https://arxiv.org/abs/2508.19903)
*Ramya Keerthy Thatikonda,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: This paper enhances logical reasoning in LLMs using Outcome Reward Models (ORMs) trained with innovative data generation strategies like echo generation, improving reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve LLMs' logical reasoning abilities, which are crucial for complex reasoning tasks but underexplored in deductive reasoning.

Method: The proposed method involves training ORMs using data generated through Chain-of-Thought (CoT) with multiple samples and an innovative echo generation method to introduce diverse error types.

Result: ORMs trained with the proposed methods achieved superior performance on three reasoning datasets (FOLIO, JustLogic, ProverQA) across four LLMs.

Conclusion: Using CoT and echo-augmented training enhances deductive reasoning in LLMs, providing a generalizable approach for improving reasoning capabilities.

Abstract: Logical reasoning is a critical benchmark for evaluating the capabilities of
large language models (LLMs), as it reflects their ability to derive valid
conclusions from given premises. While the combination of test-time scaling
with dedicated outcome or process reward models has opened up new avenues to
enhance LLMs performance in complex reasoning tasks, this space is
under-explored in deductive logical reasoning. We present a set of Outcome
Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly
generate data using Chain-of-Thought (CoT) with single and multiple samples.
Additionally, we propose a novel tactic to further expand the type of errors
covered in the training dataset of the ORM. In particular, we propose an echo
generation technique that leverages LLMs' tendency to reflect incorrect
assumptions made in prompts to extract additional training data, covering
previously unexplored error types. While a standard CoT chain may contain
errors likely to be made by the reasoner, the echo strategy deliberately steers
the model toward incorrect reasoning. We show that ORMs trained on CoT and
echo-augmented data demonstrate improved performance on the FOLIO, JustLogic,
and ProverQA datasets across four different LLMs.

</details>


### [71] [Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2508.19919)
*Jingyu Guo,Yingying Xu*

Main category: cs.CL

TL;DR: This paper shows that stereotypes can emerge and evolve in interactions among AI agents, beyond biases inherited from training data.


<details>
  <summary>Details</summary>
Motivation: To explore if stereotypes can spontaneously form and intensify in interactions among LLM-based AI agents, independent of predefined biases.

Method: Developed an experimental framework simulating workplace interactions with neutral initial conditions, analyzing the emergent stereotype behaviors in LLM-based multi-agent systems.

Result: LLM-based AI agents developed stereotypes, which intensified with interaction rounds, hierarchical structures, and power dynamics, showing patterns akin to human social biases.

Conclusion: Stereotype formation in AI agents can emerge from their interactions as an inherent property, necessitating research into mechanisms and ethical mitigation strategies.

Abstract: While stereotypes are well-documented in human social interactions, AI
systems are often presumed to be less susceptible to such biases. Previous
studies have focused on biases inherited from training data, but whether
stereotypes can emerge spontaneously in AI agent interactions merits further
exploration. Through a novel experimental framework simulating workplace
interactions with neutral initial conditions, we investigate the emergence and
evolution of stereotypes in LLM-based multi-agent systems. Our findings reveal
that (1) LLM-Based AI agents develop stereotype-driven biases in their
interactions despite beginning without predefined biases; (2) stereotype
effects intensify with increased interaction rounds and decision-making power,
particularly after introducing hierarchical structures; (3) these systems
exhibit group effects analogous to human social behavior, including halo
effects, confirmation bias, and role congruity; and (4) these stereotype
patterns manifest consistently across different LLM architectures. Through
comprehensive quantitative analysis, these findings suggest that stereotype
formation in AI systems may arise as an emergent property of multi-agent
interactions, rather than merely from training data biases. Our work
underscores the need for future research to explore the underlying mechanisms
of this phenomenon and develop strategies to mitigate its ethical impacts.

</details>


### [72] [HEAL: A Hypothesis-Based Preference-Aware Analysis Framework](https://arxiv.org/abs/2508.19922)
*Yifu Huo,Chenglong Wang,Qiren Zhu,Shunjie Xing,Tong Xiao,Chunliang Zhang,Tongran Liu,Jinbo Zhu*

Main category: cs.CL

TL;DR: The paper introduces HEAL (Hypothesis-based Preference-aware Analysis Framework), a new method for evaluating preference optimization in LLMs, addressing limitations in current evaluations that only consider a single response.


<details>
  <summary>Details</summary>
Motivation: Current LLM preference optimization methods lack comprehensive evaluation mechanisms, as they focus on single responses without considering other potential outputs.

Method: The authors propose HEAL, which uses hypothesis space re-ranking and two metrics—ranking accuracy and preference strength correlation—and develop UniHypoBench as a benchmark for experiments.

Result: Experiments using the HEAL framework show that current preference learning methods capture proxy-model-provided preferences effectively while suppressing negative samples.

Conclusion: The study contributes by introducing hypothesis space analysis for understanding preference alignment theoretically and offers HEAL as a practical diagnostic framework, providing insights to improve alignment algorithms further.

Abstract: Preference optimization methods like DPO have achieved remarkable performance
in LLM alignment. However, the evaluation for these methods relies on a single
response and overlooks other potential outputs, which could also be generated
in real-world applications within this hypothetical space. To address this
issue, this paper presents a \textbf{H}ypothesis-based
Pr\textbf{E}ference-aware \textbf{A}na\textbf{L}ysis Framework (HEAL), a novel
evaluation paradigm that formulates preference alignment as a re-ranking
process within hypothesis spaces. The framework incorporates two complementary
metrics: ranking accuracy for evaluating ordinal consistency and preference
strength correlation for assessing continuous alignment. To facilitate this
framework, we develop UniHypoBench, a unified hypothesis benchmark constructed
from diverse instruction-response pairs. Through extensive experiments based on
HEAL, with a particular focus on the intrinsic mechanisms of preference
learning, we demonstrate that current preference learning methods can
effectively capture preferences provided by proxy models while simultaneously
suppressing negative samples. These findings contribute to preference learning
research through two significant avenues. Theoretically, we introduce
hypothesis space analysis as an innovative paradigm for understanding
preference alignment. Practically, HEAL offers researchers robust diagnostic
tools for refining preference optimization methods, while our empirical results
identify promising directions for developing more advanced alignment algorithms
capable of comprehensive preference capture.

</details>


### [73] [Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation](https://arxiv.org/abs/2508.19966)
*Slimane Bellaouar,Attia Nehar,Soumia Souffi,Mounia Bouameur*

Main category: cs.CL

TL;DR: This paper presents a new approach for subjectivity analysis in Arabic text by creating a dataset (AraDhati+) and fine-tuning state-of-the-art Arabic language models, achieving remarkable accuracy in classification.


<details>
  <summary>Details</summary>
Motivation: Arabic, being a linguistically rich but under-resourced language, lacks sufficient annotated datasets for accurate subjectivity analysis, creating a need for targeted research and tools.

Method: The paper developed a dataset called AraDhati+ using existing datasets and fine-tuned Arabic language models (XLM-RoBERTa, AraBERT, ArabianGPT) for subjectivity classification, also applying ensemble decision methods.

Result: The approach achieved a high accuracy of 97.79% in Arabic subjectivity classification, showcasing the efficiency of the proposed methodology.

Conclusion: The study underscores its contribution to addressing resource scarcity for Arabic language processing and demonstrates the effectiveness of leveraging advanced models and curated datasets for subjectivity analysis.

Abstract: Despite its significance, Arabic, a linguistically rich and morphologically
complex language, faces the challenge of being under-resourced. The scarcity of
large annotated datasets hampers the development of accurate tools for
subjectivity analysis in Arabic. Recent advances in deep learning and
Transformers have proven highly effective for text classification in English
and French. This paper proposes a new approach for subjectivity assessment in
Arabic textual data. To address the dearth of specialized annotated datasets,
we developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic
datasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we
fine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and
ArabianGPT) on AraDhati+ for effective subjectivity classification.
Furthermore, we experimented with an ensemble decision approach to harness the
strengths of individual models. Our approach achieves a remarkable accuracy of
97.79\,\% for Arabic subjectivity classification. Results demonstrate the
effectiveness of the proposed approach in addressing the challenges posed by
limited resources in Arabic language processing.

</details>


### [74] [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)
*Pengxiang Li,Yefan Zhou,Dilxat Muhtar,Lu Yin,Shilin Yan,Li Shen,Yi Liang,Soroush Vosoughi,Shiwei Liu*

Main category: cs.CL

TL;DR: This paper introduces Prophet, a fast decoding method for diffusion language models (DLMs). Prophet leverages the early convergence property of DLMs to reduce decoding steps by up to 3.4x while maintaining output quality.


<details>
  <summary>Details</summary>
Motivation: DLMs offer advantages like parallel sequence generation and flexible token orders, but their inference is slower due to bidirectional attention and numerous refinement steps. This study aims to address this bottleneck by exploiting the early convergence property of DLMs.

Method: The authors propose Prophet, a training-free approach to early commit decoding, which dynamically decides whether to continue refinement or decode all remaining tokens based on the confidence gap between top-2 prediction candidates. It integrates seamlessly into existing setups with negligible overhead.

Result: Empirical evaluations demonstrate that Prophet reduces decoding steps by up to 3.4x across tasks like GSM8K and MMLU, while maintaining high-quality outputs across LLaDA-8B and Dream-7B models.

Conclusion: Prophet redefines DLM decoding as a question of when to stop sampling, showing that early decode convergence is a practical and effective mechanism to accelerate DLM inference without sacrificing quality.

Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to
autoregressive approaches, offering parallel sequence generation and flexible
token orders. However, their inference remains slower than that of
autoregressive models, primarily due to the cost of bidirectional attention and
the large number of refinement steps required for high quality outputs. In this
work, we highlight and leverage an overlooked property of DLMs early answer
convergence: in many cases, the correct answer can be internally identified by
half steps before the final decoding step, both under semi-autoregressive and
random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%
of instances, respectively, can be decoded correctly using only half of the
refinement steps. Building on this observation, we introduce Prophet, a
training-free fast decoding paradigm that enables early commit decoding.
Specifically, Prophet dynamically decides whether to continue refinement or to
go "all-in" (i.e., decode all remaining tokens in one step), using the
confidence gap between the top-2 prediction candidates as the criterion. It
integrates seamlessly into existing DLM implementations, incurs negligible
overhead, and requires no additional training. Empirical evaluations of
LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the
number of decoding steps by up to 3.4x while preserving high generation
quality. These results recast DLM decoding as a problem of when to stop
sampling, and demonstrate that early decode convergence provides a simple yet
powerful mechanism for accelerating DLM inference, complementary to existing
speedup techniques. Our code is publicly available at
https://github.com/pixeli99/Prophet.

</details>


### [75] [AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios](https://arxiv.org/abs/2508.19988)
*Lisa Alazraki,Lihu Chen,Ana Brassard,Joe Stacey,Hossein A. Rahmani,Marek Rei*

Main category: cs.CL

TL;DR: This paper introduces a benchmark called AgentCoMa that assesses large language models (LLMs) on tasks requiring both commonsense and math reasoning, revealing significant performance drops in mixed-type tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in LLM benchmarks that test either commonsense or math reasoning in isolation but not in combination, which is essential for real-world problem-solving.

Method: The authors developed the AgentCoMa benchmark, tested 61 LLMs across various configurations, and analyzed their performance on combined reasoning tasks. They also conducted interpretability studies using neuron patterns, attention maps, and membership inference.

Result: LLMs showed a ~30% accuracy drop when combining commonsense and math reasoning steps compared to handling each step in isolation. Non-expert human annotators performed both tasks with high accuracy, indicating model brittleness.

Conclusion: This work highlights the limitations of LLMs for mixed-type reasoning tasks and provides a benchmark to drive future research and improvements.

Abstract: Large Language Models (LLMs) have achieved high accuracy on complex
commonsense and mathematical problems that involve the composition of multiple
reasoning steps. However, current compositional benchmarks testing these skills
tend to focus on either commonsense or math reasoning, whereas LLM agents
solving real-world tasks would require a combination of both. In this work, we
introduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each
compositional task requires a commonsense reasoning step and a math reasoning
step. We test it on 61 LLMs of different sizes, model families, and training
strategies. We find that LLMs can usually solve both steps in isolation, yet
their accuracy drops by ~30% on average when the two are combined. This is a
substantially greater performance gap than the one we observe in prior
compositional benchmarks that combine multiple steps of the same reasoning
type. In contrast, non-expert human annotators can solve the compositional
questions and the individual steps in AgentCoMa with similarly high accuracy.
Furthermore, we conduct a series of interpretability studies to better
understand the performance gap, examining neuron patterns, attention maps and
membership inference. Our work underscores a substantial degree of model
brittleness in the context of mixed-type compositional reasoning and offers a
test bed for future improvement.

</details>


### [76] [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
*Debanjana Kar,Leopold Böss,Dacia Braca,Sebastian Maximilian Dennerlein,Nina Christine Hubig,Philipp Wintersberger,Yufang Hou*

Main category: cs.CL

TL;DR: The paper introduces MathBuddy, an emotionally aware LLM-powered math tutor that integrates students' emotional states into its teaching strategy.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the lack of consideration for students' affective states in current learning systems, despite evidence that emotions significantly influence learning.

Method: The system captures students' emotions through text and facial expressions, aggregates the data, and uses it to prompt the LLM for empathetic and pedagogically adaptive responses.

Result: The authors achieved a 23-point improvement in win rate and a 3-point gain in DAMR scores through evaluations, demonstrating the effectiveness of integrating emotions.

Conclusion: Modeling students' emotions in educational AI significantly enhances pedagogical efficacy, as demonstrated by MathBuddy's performance gains.

Abstract: The rapid adoption of LLM-based conversational systems is already
transforming the landscape of educational technology. However, the current
state-of-the-art learning models do not take into account the student's
affective states. Multiple studies in educational psychology support the claim
that positive or negative emotional states can impact a student's learning
capabilities. To bridge this gap, we present MathBuddy, an emotionally aware
LLM-powered Math Tutor, which dynamically models the student's emotions and
maps them to relevant pedagogical strategies, making the tutor-student
conversation a more empathetic one. The student's emotions are captured from
the conversational text as well as from their facial expressions. The student's
emotions are aggregated from both modalities to confidently prompt our LLM
Tutor for an emotionally-aware response. We have effectively evaluated our
model using automatic evaluation metrics across eight pedagogical dimensions
and user studies. We report a massive 23 point performance gain using the win
rate and a 3 point gain at an overall level using DAMR scores which strongly
supports our hypothesis of improving LLM-based tutor's pedagogical abilities by
modeling students' emotions.

</details>


### [77] [ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning](https://arxiv.org/abs/2508.19996)
*Yiming Du,Yifan Xiang,Bin Liang,Dahua Lin,Kam-Fai Wong,Fei Tan*

Main category: cs.CL

TL;DR: This paper proposes ReSURE, a dynamic learning approach to address the issue of low-quality supervision in multi-turn conversation systems by reweighting unreliable samples during training.


<details>
  <summary>Details</summary>
Motivation: Current dialogue systems face challenges in handling errors from low-quality supervision, which propagate through conversation turns and degrade system performance.

Method: ReSURE uses adaptive learning to dynamically down-weight unreliable supervision during training rather than static prefiltering. It achieves this by estimating per-turn loss distributions through Welford's online statistics.

Result: Experiments demonstrate that ReSURE improves system stability and response quality on datasets of varying quality, with positive Spearman correlations (0.21 ~ 1.0) indicating potential for effective use of large-scale data.

Conclusion: ReSURE addresses the limitations of static filtering with adaptive strategies, improving dialogue systems' robustness and making them suitable for larger, mixed-quality datasets.

Abstract: Fine-tuning multi-turn dialogue systems requires high-quality supervision but
often suffers from degraded performance when exposed to low-quality data.
Supervision errors in early turns can propagate across subsequent turns,
undermining coherence and response quality. Existing methods typically address
data quality via static prefiltering, which decouples quality control from
training and fails to mitigate turn-level error propagation. In this context,
we propose ReSURE (Regularizing Supervision UnREliability), an adaptive
learning method that dynamically down-weights unreliable supervision without
explicit filtering. ReSURE estimates per-turn loss distributions using
Welford's online statistics and reweights sample losses on the fly accordingly.
Experiments on both single-source and mixed-quality datasets show improved
stability and response quality. Notably, ReSURE enjoys positive Spearman
correlations (0.21 ~ 1.0 across multiple benchmarks) between response scores
and number of samples regardless of data quality, which potentially paves the
way for utilizing large-scale data effectively. Code is publicly available at
https://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.

</details>


### [78] [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
*Boheng Mao*

Main category: cs.CL

TL;DR: The paper introduces the Selective Retrieval-Augmentation (SRA) method to address poor model performance on rare labels in legal text classification due to long-tail label distributions.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of poor model performance on low-frequency labels in legal text classification datasets with long-tail label distributions.

Method: Propose Selective Retrieval-Augmentation (SRA), a technique that augments samples for underrepresented labels without altering model architecture or introducing retrieval from external sources.

Result: SRA achieved higher micro-F1 and macro-F1 scores across LEDGAR and UNFAIR-ToS datasets compared to existing LexGLUE baselines.

Conclusion: SRA effectively improves long-tail legal text classification and offers a noise-free solution for underrepresented labels without external data reliance.

Abstract: Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification. The code repository is available at:
https://github.com/Boheng-Mao/sra-legal

</details>


### [79] [DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis](https://arxiv.org/abs/2508.20033)
*Liana Patel,Negar Arabzadeh,Harshit Gupta,Ankita Sundar,Ion Stoica,Matei Zaharia,Carlos Guestrin*

Main category: cs.CL

TL;DR: The paper introduces DeepScholar-bench, a benchmark for evaluating generative research synthesis systems, which is currently difficult to surpass, highlighting the complexity of the task.


<details>
  <summary>Details</summary>
Motivation: Generative research synthesis systems hold promise for advancing human knowledge but lack proper evaluation methods due to limitations in existing benchmarks and datasets.

Method: The authors created DeepScholar-bench, a live benchmark using high-quality queries from ArXiv papers, and evaluated systems based on knowledge synthesis, retrieval quality, and verifiability.

Result: DeepScholar-base performs competitively against other methods, yet no system scores beyond 19% across all metrics, showing that DeepScholar-bench presents a significant challenge.

Conclusion: DeepScholar-bench is crucial for advancing generative research synthesis capabilities, offering a robust, unsaturated framework for systematic evaluation in this domain.

Abstract: The ability to research and synthesize knowledge is central to human
expertise and progress. An emerging class of systems promises these exciting
capabilities through generative research synthesis, performing retrieval over
the live web and synthesizing discovered sources into long-form, cited
summaries. However, evaluating such systems remains an open challenge: existing
question-answering benchmarks focus on short-form factual responses, while
expert-curated datasets risk staleness and data contamination. Both fail to
capture the complexity and evolving nature of real research synthesis tasks. In
this work, we introduce DeepScholar-bench, a live benchmark and holistic,
automated evaluation framework designed to evaluate generative research
synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv
papers and focuses on a real research synthesis task: generating the related
work sections of a paper by retrieving, synthesizing, and citing prior
research. Our evaluation framework holistically assesses performance across
three key dimensions, knowledge synthesis, retrieval quality, and
verifiability. We also develop DeepScholar-base, a reference pipeline
implemented efficiently using the LOTUS API. Using the DeepScholar-bench
framework, we perform a systematic evaluation of prior open-source systems,
search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that
DeepScholar-base establishes a strong baseline, attaining competitive or higher
performance than each other method. We also find that DeepScholar-bench remains
far from saturated, with no system exceeding a score of $19\%$ across all
metrics. These results underscore the difficulty of DeepScholar-bench, as well
as its importance for progress towards AI systems capable of generative
research synthesis. We make our code available at
https://github.com/guestrin-lab/deepscholar-bench.

</details>


### [80] [Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks](https://arxiv.org/abs/2508.20038)
*Sheng Liu,Qiang Sheng,Danding Wang,Yang Li,Guang Yang,Juan Cao*

Main category: cs.CL

TL;DR: The paper tackles jailbreak attacks on large language models (LLMs) by introducing a novel synthesis framework called IMAGINE to address distributional mismatches, effectively reducing attack success rates.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of widely-used LLMs to jailbreak attacks caused by distributional mismatches between training data and real-world malicious instructions.

Method: IMAGINE, a synthesis framework, is proposed to generate jailbreak-like instructions by leveraging embedding space distribution analysis and iterative optimization to augment safety alignment data distributions.

Result: Using IMAGINE-enhanced safety-aligned corpora, the framework significantly reduces attack success rates on LLMs like Qwen2.5, Llama3.1, and Llama3.2 while maintaining model utility.

Conclusion: IMAGINE successfully enhances LLMs' safety alignment by proactively addressing distributional mismatches, proving effective against unseen malicious instructions without sacrificing functional performance.

Abstract: Despite advances in improving large language model(LLM) to refuse to answer
malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks
where attackers generate instructions with distributions differing from safety
alignment corpora. New attacks expose LLMs' inability to recognize unseen
malicious instructions, highlighting a critical distributional mismatch between
training data and real-world attacks that forces developers into reactive
patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis
framework that leverages embedding space distribution analysis to generate
jailbreak-like instructions. This approach effectively fills the distributional
gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE
follows an iterative optimization process that dynamically evolves text
generation distributions across iterations, thereby augmenting the coverage of
safety alignment data distributions through synthesized data examples. Based on
the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates
significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2
without compromising their utility.

</details>


### [81] [AraHealthQA 2025 Shared Task Description Paper](https://arxiv.org/abs/2508.20047)
*Hassan Alhuzali,Farah Shamout,Muhammad Abdul-Mageed,Chaimae Abouzahir,Mouath Abu-Daoud,Ashwag Alasmari,Walid Al-Eisawi,Renad Al-Monef,Ali Alqahtani,Lama Ayash,Nizar Habash,Leen Kharouf*

Main category: cs.CL

TL;DR: AraHealthQA 2025 introduces a shared task focused on improving Arabic health QA resources via mental health and broader medical domains.


<details>
  <summary>Details</summary>
Motivation: To address the lack of high-quality Arabic medical QA resources and improve benchmarks in realistic and culturally sensitive environments.

Method: The paper structured AraHealthQA into two tracks (MentalQA and MedArabiQ) with subtasks, datasets, metrics, enabling fair evaluations and modeled for multilingual contexts.

Result: Participation statistics, baseline systems, and performance trends in Arabic health QA are analyzed, resulting in improved resources and methodologies.

Conclusion: The shared task demonstrates the importance of culturally nuanced approaches in improving Arabic medical QA, with reflections and recommendations for future work.

Abstract: We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question
Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located
with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic
medical QA resources by offering two complementary tracks: {MentalQA}, focusing
on Arabic mental health Q\&A (e.g., anxiety, depression, stigma reduction), and
{MedArabiQ}, covering broader medical domains such as internal medicine,
pediatrics, and clinical decision making. Each track comprises multiple
subtasks, evaluation datasets, and standardized metrics, facilitating fair
benchmarking. The task was structured to promote modeling under realistic,
multilingual, and culturally nuanced healthcare contexts. We outline the
dataset creation, task design and evaluation framework, participation
statistics, baseline systems, and summarize the overall outcomes. We conclude
with reflections on the performance trends observed and prospects for future
iterations in Arabic health QA.

</details>


### [82] [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
*Chengzu Li,Wenshan Wu,Huanyu Zhang,Qingtao Li,Zeyu Gao,Yan Xia,José Hernández-Orallo,Ivan Vulić,Furu Wei*

Main category: cs.CL

TL;DR: The paper investigates spatial reasoning abilities of multimodal large language models (MLLMs) using a new benchmark and finds emerging but limited capabilities compared to humans.


<details>
  <summary>Details</summary>
Motivation: To explore the gap in understanding human-like spatial cognition in current MLLMs and assess their reasoning capabilities.

Method: Introduced 11Plus-Bench, a benchmark based on standardized spatial aptitude tests, along with expert annotations for perceptual complexity and reasoning processes. Conducted experiments across 14 MLLMs and human evaluation.

Result: MLLMs show early signs of spatial cognition but fall short of human performance. Cognitive effort in MLLMs correlates with reasoning complexity, but instance-level performance is random compared to humans.

Conclusion: MLLMs exhibit emerging spatial reasoning abilities but are limited in predictability and complexity handling. The findings offer insights for improving model design.

Abstract: For human cognitive process, spatial reasoning and perception are closely
entangled, yet the nature of this interplay remains underexplored in the
evaluation of multimodal large language models (MLLMs). While recent MLLM
advancements show impressive performance on reasoning, their capacity for
human-like spatial cognition remains an open question. In this work, we
introduce a systematic evaluation framework to assess the spatial reasoning
abilities of state-of-the-art MLLMs relative to human performance. Central to
our work is 11Plus-Bench, a high-quality benchmark derived from realistic
standardized spatial aptitude tests. 11Plus-Bench also features fine-grained
expert annotations of both perceptual complexity and reasoning process,
enabling detailed instance-level analysis of model behavior. Through extensive
experiments across 14 MLLMs and human evaluation, we find that current MLLMs
exhibit early signs of spatial cognition. Despite a large performance gap
compared to humans, MLLMs' cognitive profiles resemble those of humans in that
cognitive effort correlates strongly with reasoning-related complexity.
However, instance-level performance in MLLMs remains largely random, whereas
human correctness is highly predictable and shaped by abstract pattern
complexity. These findings highlight both emerging capabilities and limitations
in current MLLMs' spatial reasoning capabilities and provide actionable
insights for advancing model design.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [83] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: The paper introduces a real-time drawing system that combines structural and semantic analysis for creative human-AI interaction.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in generative systems by integrating both formal and contextual artistic aspects for a more holistic creation process.

Method: A dual-intent multi-stage generation pipeline that analyzes geometric and semantic cues, supported by touchscreen-based tools and distributed architecture.

Result: Achieved low-latency, collaborative, user-friendly drawing platform fostering synchronous co-creation regardless of artistic skill levels.

Conclusion: Reframes human-AI interaction as a collaborative process for enhanced artistic engagement and creativity.

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [84] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: The paper introduces Temporal Token Fusion (TTF), a training-free technique to improve vision-language-action (VLA) model performance by leveraging temporal coherence in visual inputs.


<details>
  <summary>Details</summary>
Motivation: Current VLA models discard temporal information by processing visual inputs frame-by-frame, leading to vulnerability to visual noise and inefficient use of coherent manipulation sequences.

Method: TTF combines grayscale pixel difference analysis and attention-based semantic relevance to selectively integrate historical and current visual representations, using hard fusion strategies and keyframe anchoring.

Result: TTF demonstrated improvements across LIBERO (4.0 percentage points), SimplerEnv (4.8% relative improvement), and real robot tasks (8.7% relative improvement), showcasing its model-agnostic and computationally efficient qualities.

Conclusion: TTF enhances VLA inference by utilizing temporal coherence and selective matrix reuse in attention mechanisms, paving the way for accelerated computation and higher success rates in robotic tasks.

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [85] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: The paper proposes an unsupervised pipeline for assessing slide quality by combining design metrics with CLIP-ViT embeddings and using Isolation Forest for anomaly scoring. It achieves strong correlations with human visual-quality ratings.


<details>
  <summary>Details</summary>
Motivation: To design a scalable, objective method for evaluating the visual quality of presentation slides, aligning with human perceptions.

Method: Combining seven expert-inspired design metrics (e.g., whitespace, color balance) with multimodal CLIP-ViT embeddings and employing Isolation Forest for anomaly scoring, trained on 12k slides.

Result: Achieved a Pearson correlation of up to 0.83 with human ratings and outperformed leading vision-language models by 1.79x to 3.23x.

Conclusion: Augmenting low-level design cues with multimodal embeddings closely approximates human perceptions, providing a tool for real-time, objective slide evaluation.

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [86] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: The paper introduces a lightweight and efficient purification framework for defending 2D range-view LiDAR segmentation against adversarial attacks in autonomous vehicles, showing strong adversarial resilience and real-world applicability.


<details>
  <summary>Details</summary>
Motivation: Modern LiDAR segmentation networks in autonomous vehicles are vulnerable to adversarial attacks, and there is a need for lightweight defenses tailored to 2D range-view LiDAR segmentation for practical usage.

Method: The authors propose an explainable purification network based on a mathematically justified optimization problem, targeting adversarial purification directly in the 2D range-view representation, without relying on computationally heavy generative models.

Result: The proposed framework achieves competitive performance on benchmarks and consistently outperforms existing generative and adversarial training methods, with low computational overhead.

Conclusion: This framework demonstrates strong adversarial resilience and practical effectiveness, especially in real-world conditions such as deployment on autonomous vehicles.

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [87] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: The review explores the advancements in large vision-language models (LVLMs) for object detection, focusing on their architecture, adaptability, and contextual reasoning, while identifying challenges and proposing future directions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to examine how large vision-language models (LVLMs) significantly enhance object detection by integrating language and vision, addressing limitations in traditional deep learning object detection systems.

Method: The review systematically analyzes LVLMs across three stages: their functional workings, architectural and training advancements, and integration of visual/textual data. It includes visualizations, comparative evaluations, and identification of limitations.

Result: The study highlights that LVLMs achieve advanced contextual understanding and improved adaptability for object detection, surpassing traditional systems in localization and segmentation challenges.

Conclusion: LVLMs are poised to transform object detection and robotic applications, with ongoing advancements expected to overcome current limitations and outperform conventional approaches.

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [88] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: The paper introduces Context-aware Sparse Spatiotemporal Learning (CSSL), a framework that improves sparse activation in neural networks for event-based vision tasks, offering better efficiency and comparable or superior performance to current methods.


<details>
  <summary>Details</summary>
Motivation: Current event-based vision methods struggle to balance performance and efficiency, especially in resource-constrained situations. Spiking neural networks offer energy advantages but lag in performance for complex tasks. The paper aims to address these challenges.

Method: The authors propose CSSL, a framework that employs context-aware thresholding to dynamically regulate neuron activations based on the input distribution, eliminating the need for explicit sparsity constraints.

Result: CSSL achieves high neuron sparsity while delivering comparable or better performance in event-based object detection and optical flow estimation compared to state-of-the-art models.

Conclusion: CSSL enables efficient and high-performing event-based vision tasks, paving the way for practical neuromorphic applications while overcoming existing bottlenecks in computational efficiency and sparsity management.

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [89] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: This paper addresses the limitation of current LLM/LVLM systems in generating accurate sports captions, proposing a fine-tuned LVLM pipeline yielding improved F1 and BERT score.


<details>
  <summary>Details</summary>
Motivation: Existing LLM/LVLM systems fail to produce accurate and stylized descriptions for sports activities due to insufficient domain-specific jargon.

Method: A two-level fine-tuned LVLM pipeline was developed to improve the generation of stylized sports captions from images.

Result: The proposed pipeline achieved more than 8-10% improvement in F1 score and 2-10% improvement in BERT score over existing methods, with fast execution and low memory usage.

Conclusion: The method showed practicality during Super Bowl LIX, generating professional-grade captions efficiently and accurately for live sports journalism.

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [90] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: The paper evaluates demographic biases in large vision language models (LVLMs) for biometric face recognition tasks, emphasizing disparities across ethnic, gender, and age groups.


<details>
  <summary>Details</summary>
Motivation: To investigate and quantify demographic biases in LVLMs used for biometric face recognition and textual token generation, given their uneven performance across demographic groups.

Method: Fine-tuning and evaluating three pre-trained LVLMs (LLaVA, BLIP-2, and PaliGemma) on a self-generated demographic-balanced dataset. The study uses group-specific BERTScores and Fairness Discrepancy Rate to measure biases.

Result: The analysis found significant biases in the LVLMs. PaliGemma and LLaVA showed higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, while BLIP-2 had more consistent performance across demographics.

Conclusion: LVLMs exhibit varying levels of demographic bias in biometric face recognition tasks, highlighting the need for more fairness-focused model design and evaluation. BLIP-2 may serve as a relatively fairer alternative.

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [91] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: The paper introduces Geo2Vec, a method leveraging signed distance fields for efficient spatial representation of geo-entities without decomposing them.


<details>
  <summary>Details</summary>
Motivation: Existing methods for spatial representation of geo-entities struggle with high computational costs and loss of fine-grained geometric features due to uniform sampling.

Method: The authors propose Geo2Vec, which uses adaptive signed distance sampling and a neural network trained on signed distance fields to create unified and geometry-aware representations.

Result: Geo2Vec outperforms prior methods in encoding shapes, locations, and spatial relationships with higher computational efficiency and accuracy in real-world GeoAI applications.

Conclusion: Geo2Vec provides a robust, efficient, and rotation-invariant solution for spatial representation learning across diverse geo-entity types, enhancing GeoAI applications.

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [92] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: The paper presents a deep learning-based automated system for identifying rice varieties and diagnosing rice leaf diseases, achieving high classification accuracy and incorporating explainable AI for reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges of manual quality control and crop monitoring, which are labor-intensive, time-consuming, and error-prone, necessitating an automated, reliable system for rice quality and disease assessments.

Method: The study utilized Convolutional Neural Networks (CNN) to classify five rice grain varieties, trained on a dataset of 75,000 images. For disease diagnosis, CNN, VGG16, ResNet50, and MobileNetV2 were deployed alongside explainability methods such as SHAP and LIME.

Result: The system achieved high classification accuracy for rice varieties and provided effective diagnostics for diseases like Brown Spot and Bacterial Blight. Model transparency was enhanced through explainability techniques.

Conclusion: The findings validate the potential of deep learning and explainable AI for agricultural applications, offering robust tools for automated crop quality inspection and disease diagnosis, with benefits for farmers, consumers, and the economy.

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [93] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: This paper develops a federated learning-based facial recognition system to effectively identify known and unknown individuals, addressing privacy and identification challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome privacy issues and identity management problems in facial recognition systems when encountering unknown individuals in open-set scenarios.

Method: The paper integrates the OpenMax algorithm within a federated learning framework, using the exchange of mean activation vectors and local distance measures for distinguishing between known and unknown subjects.

Result: Experimental results show that the proposed solution effectively identifies known and unknown individuals, validating its application to distributed environments.

Conclusion: The study highlights the potential of the proposed system to improve privacy-aware and robust facial recognition in distributed settings, particularly in open-set scenarios.

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [94] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: The paper introduces a method for habitat classification using ground-level photographs, leveraging deep learning to achieve scalable and robust classification across 18 habitat categories.


<details>
  <summary>Details</summary>
Motivation: Existing habitat classification methods rely mainly on satellite imagery, which lacks ground-level validation and scalability for citizen science data.

Method: The study uses a DeepLabV3-ResNet101 model, incorporating pre-processing steps like resizing, normalisation, augmentation, and class balancing in the training data.

Result: The model achieves a mean F1-score of 0.61 across 18 classes, with visually distinct habitats performing strongly (>0.90 F1), while mixed or ambiguous classes perform lower.

Conclusion: The approach demonstrates the utility of ground-level imagery and deep learning for scalable and accurate habitat classification. A web application was developed to facilitate practical use.

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [95] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: A new framework for interactive digital human video generation is proposed, addressing challenges like high latency and limited control by enabling multimodal interaction and low-latency extrapolation.


<details>
  <summary>Details</summary>
Motivation: To overcome existing challenges in interactive digital human video systems, such as high computational demands, latency, and limited input control.

Method: The method involves an autoregressive video generation framework built on a modified large language model, which handles multimodal inputs and guides a diffusion model. Additionally, a large dialogue dataset and a deep compression autoencoder are employed.

Result: The framework demonstrates low latency, high efficiency, and better multimodal control for applications like multilingual human synthesis and interactive scenarios.

Conclusion: This framework improves video generation systems by providing seamless real-time interactivity, broad multimodal support, and efficient processing capabilities.

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [96] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: The paper reviews digital watermarking and steganography methods to enhance security in ICAO-compliant facial images by embedding tamper-evident signals.


<details>
  <summary>Details</summary>
Motivation: ICAO-compliant facial images, while standardized for global interoperability, are vulnerable to misuse like identity theft and morphing. Thus, it necessitates advanced security measures.

Method: The paper conducts a comprehensive evaluation of state-of-the-art watermarking and steganography techniques applied to ICAO-compliant images.

Result: The research explores the potential and limitations of these methods, focusing on their practicality under standard constraints for secure identity systems.

Conclusion: Digital watermarking and steganography can bolster security for ICAO-compliant images by enabling persistent tamper-proofing and identity verification without compromising their utility.

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [97] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: The study introduces PRISM, a self-supervised framework that utilizes cardiac imaging and health records for superior survival prediction in cardiovascular cases.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the predictive accuracy of adverse cardiac events, using a novel approach combining imaging and electronic health data.

Method: PRISM employs motion-aware feature distillation from cardiac MRI and uses textual prompts for risk modeling, integrating imaging with patient EHR data.

Result: PRISM surpasses classical models and existing deep learning baselines in predicting cardiac risk, with findings validated across diverse cohorts.

Conclusion: The integrated approach in PRISM not only improves cardiac risk prediction but also uncovers specific imaging markers and dominant clinical risk factors.

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [98] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: The paper introduces EffNetViTLoRA, a model combining CNN and Vision Transformer (ViT) for Alzheimer’s disease diagnosis using MRI data, achieving 92.52% accuracy and 92.76% F1-score.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need for early and accurate diagnosis of Alzheimer's and the challenges in diagnosing Mild Cognitive Impairment (MCI), a transitional stage to Alzheimer's.

Method: EffNetViTLoRA combines Convolutional Neural Networks (CNNs) and Vision Transformers (ViT) for feature extraction from the whole T1-weighted MRI dataset. To improve adaptation and reduce overfitting during fine-tuning of pretrained models, the paper employs Low-Rank Adaptation (LoRA).

Result: The proposed model achieves a notable classification accuracy of 92.52% and an F1-score of 92.76%, effectively distinguishing Alzheimer's, MCI, and Cognitively Normal categories.

Conclusion: EffNetViTLoRA demonstrates substantial potential in enhancing the clinical reliability of Alzheimer's diagnosis by integrating comprehensive data utilization and an advanced model adaptation approach.

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [99] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: The study evaluates the accuracy of commercially available AI player tracking software using FIFA World Cup broadcast footage for position, speed, and distance tracking, and finds variability in results across providers.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and accuracy of AI and computer-vision technology in player tracking for potential applications in sports analytics.

Method: The study used data from a FIFA World Cup match, comparing AI tracking software results from three providers to a high-definition reference system (TRACAB Gen 5). They analyzed position and speed and evaluated accuracy using RMSE and mean bias.

Result: Position RMSE ranged from 1.68 to 16.39 m, speed RMSE ranged from 0.34 to 2.38 m/s, and total match distance had mean bias between -21.8% to 24.3%. Tactical feed enhanced player detection, and resolutions of 720p and 1080p were effective.

Conclusion: Commercial AI tracking software shows fair precision when players are detected, and using tactical feeds improves accuracy while suitable resolution and models enhance performance.

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [100] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: Gas leaks pose significant risks, yet existing detection methods struggle due to challenges like the non-rigid nature of gas. A new framework, Joint Vision-Language Gas leak Segmentation (JVLGS), integrates visual and textual data for enhanced gas leak detection, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: The detection of gas leaks is critical for safety and environmental concerns, but current methods are limited by factors such as blurry gas cloud images and high false positive rates.

Method: The proposed JVLGS framework combines both visual and textual data to improve gas leak segmentation accuracy. A post-processing step is included to mitigate false positives by reducing noise and isolating non-target objects.

Result: Extensive experiments show that JVLGS outclasses state-of-the-art methods in diverse conditions, achieving consistent strong performance under both supervised and few-shot learning scenarios.

Conclusion: JVLGS not only addresses current detection challenges but also provides an effective and reliable solution for gas leak segmentation across various settings and learning paradigms.

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [101] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM is a framework for transferring knowledge from diverse pre-trained models into a single student model, overcoming constraints typical of existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of effectively leveraging the collective knowledge of heterogeneous pre-trained models with diverse architectures and training data.

Method: UNIFORM employs a voting mechanism to capture consensus knowledge both at the logit level and feature level, enabling knowledge transfer without assumptions about data or model types.

Result: Experiments show UNIFORM improves unsupervised object recognition and scales effectively, leveraging over 100 teacher models where alternatives falter.

Conclusion: UNIFORM demonstrates scalability and effective knowledge integration, paving the way for leveraging large-scale heterogeneous pre-trained models.

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [102] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: The paper introduces Sat2Flow, a model that generates accurate Origin-Destination (OD) flow matrices for urban mobility using only satellite imagery, addressing limitations of auxiliary data usage and sensitivity to spatial topology.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing OD flow generation methods that rely heavily on expensive auxiliary features with limited coverage and are highly sensitive to spatial topology disruptions.

Method: Sat2Flow uses a satellite-image-based multi-kernel encoder, permutation-aware diffusion process, and contrastive training to ensure structural coherence and robustness against index permutations in OD flow generation.

Result: Experimental results show that Sat2Flow achieves higher numerical accuracy, preserves spatial structures, and outperforms existing methods under various regional index reorderings.

Conclusion: Sat2Flow provides a scalable, auxiliary-data-independent solution for robust OD flow generation, ensuring invariance and accuracy in data-scarce urban environments.

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [103] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: This paper introduces a semi-supervised framework using pseudo-labeling to tackle shadow bias and improve weed detection in precision agriculture.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges posed by environmental conditions and high data annotation costs that hinder deep learning models' performance in automating invasive weed management.

Method: A diagnostic-driven framework using interpretability tools to detect biases and semi-supervised learning is applied, leveraging a unique dataset of Guinea Grass images in sugarcane fields.

Result: Strong baseline results with F1 scores up to 0.90 and mAP50 exceeding 0.82 were achieved, alongside improved robustness and recall in weed detection through pseudo-labeling.

Conclusion: The paper presents a validated methodology that demonstrates the effectiveness of semi-supervised systems in improving the robustness of computer vision models for precision agriculture in challenging real-world scenarios.

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [104] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: The paper presents TAPO and MotionFLUX, techniques for efficient, semantically-aligned motion generation for virtual characters.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-driven motion generation often struggle with precise alignment between text and motion, as well as inefficiency in inference processes.

Method: The authors introduce TAPO for aligning text descriptions with motion semantics and iteratively adjusting for better grounding. Additionally, they propose MotionFLUX, a framework utilizing deterministic rectified flow matching to reduce generation steps, resulting in real-time motion synthesis.

Result: TAPO and MotionFLUX outperform state-of-the-art techniques in terms of semantic consistency, motion quality, and speed.

Conclusion: The unified TAPO and MotionFLUX system marks a significant advancement in high-quality, efficient motion generation, bridging the gap between text descriptions and motions. Code and pretrained models will be released for further research.

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [105] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: The paper introduces CVBench, a benchmark designed to evaluate cross-video relational reasoning for multimodal large language models (MLLMs), revealing significant gaps in their capabilities compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Assessing MLLMs' performance across multiple videos is crucial for practical applications like multi-camera surveillance and procedural learning, yet this area remains underexplored.

Method: Researchers developed CVBench, consisting of 1,000 question-answer pairs across hierarchical tiers (object association, event association, and complex reasoning) from diverse video domains.

Result: Evaluation of 10+ leading MLLMs showed considerable performance deficiencies. For instance, GPT-4o achieved 60% accuracy on causal reasoning, falling short compared to 91% human accuracy.

Conclusion: CVBench highlights fundamental limitations in current MLLMs, particularly in inter-video context retention and disambiguation of entities, serving as a foundation for improving next-generation architectures.

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [106] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: WebEyeTrack combines lightweight AI models for gaze estimation and on-device few-shot learning to achieve state-of-the-art accuracy and real-time performance in browser-based eye-tracking applications.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap between advanced gaze estimation methods and practical real-world applications, focusing on real-time performance, model efficiency, and privacy concerns in the domain of eye-tracking.

Method: WebEyeTrack integrates SOTA gaze estimation models optimized for browser deployment, incorporates model-based head pose estimation, and employs few-shot learning for calibration using as few as nine samples.

Result: WebEyeTrack achieves a minimal error margin of 2.32 cm on GazeCapture dataset and real-time inference speeds of 2.4 ms on an iPhone 14, showcasing its efficiency and accuracy.

Conclusion: The framework effectively bridges the disparity between research-level gaze estimation methods and commercial solutions, offering a practical, open-source tool for real-time and accurate eye-tracking.

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [107] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: MonoRelief V2 is an end-to-end model designed to recover 2.5D reliefs from single images, incorporating real data for improved results compared to its predecessor.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of MonoRelief V1, specifically its reliance on synthetic data, by integrating real-world datasets for better robustness and accuracy.

Method: They utilize a text-to-image generative model for pseudo real data creation and multi-view reconstruction for actual real-world datasets, followed by progressive training.

Result: MonoRelief V2 demonstrates state-of-the-art performance in depth and normal predictions compared to existing models.

Conclusion: The model has strong potential for various downstream applications, offering improved accuracy and efficiency in 2.5D relief recovery from single images under complex variations.

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [108] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: The paper introduces Discrete Diffusion VLA, a unified transformer-based approach for Vision-Language-Action (VLA) modeling that bridges the gap between autoregressive and continuous diffusion methods. It improves decoding adaptability and consistency while achieving noteworthy performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing Vision-Language-Action decoders, which either use rigid autoregressive methods or complex continuous diffusion models requiring specialized training and iterative sampling.

Method: The method employs a single-transformer discrete diffusion policy that models discretized action chunks. It uses cross-entropy objectives to remain compatible with vision-language model (VLM) backbones, adaptive decoding for easier action resolution, and secondary remasking for refinement and error correction.

Result: The proposed method achieves significant performance: 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal, and 49.3% overall on SimplerEnv Bridge, outperforming autoregressive and continuous diffusion baselines.

Conclusion: Discrete Diffusion VLA demonstrates strong potential for scaling Vision-Language-Action modeling with precise action modeling and consistent training, offering a unified, scalable, and more efficient framework.

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [109] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: The paper introduces FlowDet, a high-speed object detector for traffic monitoring, improving performance and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the high computational costs of end-to-end object detectors for complex tasks like intersection traffic monitoring.

Method: FlowDet utilizes a Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module for handling scale variations. These innovations are applied to the DETR architecture.

Result: FlowDet outperforms the RT-DETR baseline by improving AP(test) by 1.5% and AP50(test) by 1.6%, while reducing computational costs by 63.2% and increasing the inference speed by 16.2%.

Conclusion: The study introduces an efficient and effective model for real-time object detection in challenging scenarios, demonstrating a significant step forward in real-world perception systems.

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [110] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: The paper introduces a unified framework for anomaly detection in medical images, aiming to address challenges like limited annotations and domain adaptation issues.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in medical images faces issues like limited annotation availability and difficulties in adapting domain-specific features. Prototype-based methods suffer from prototype collapse, reducing model accuracy.

Method: The proposed framework integrates a trainable encoder supported by a momentum branch with prototype-based reconstruction, alongside a new Diversity-Aware Alignment Loss to prevent prototype collapse.

Result: The experiments demonstrated substantial improvements over existing methods in anomaly localization and representation quality in medical imaging datasets.

Conclusion: The framework enhances anomaly detection with domain adaptability, eliminates prototype collapse, and provides better interpretability, showcasing robust performance on medical benchmarks.

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [111] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: The paper introduces MPAMatch, a novel semi-supervised framework for pathological image segmentation that leverages multimodal prototype-guided supervision with pixel-level contrastive learning, outperforming state-of-the-art methods in structural and semantic modeling.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing semi-supervised methods like UniMatch in handling ambiguous boundaries and capturing semantic priors in pathological image segmentation.

Method: MPAMatch employs a dual contrastive learning scheme incorporating multimodal prototypes (image and text) into segmentation supervision, paired with a pathology-pretrained foundation model for improved feature extraction.

Result: The proposed MPAMatch framework demonstrated superior performance on multiple benchmarks (GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, KPI) compared to state-of-the-art methods.

Conclusion: MPAMatch effectively enhances boundary modeling and semantic understanding by introducing multimodal prototype-guided contrastive supervision, making it a promising approach in pathological image segmentation.

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [112] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: The paper addresses generating images with specific target human-object interactions, focusing on preserving individual identities and controlling their interaction semantics. A new dataset and specialized model (Interact-Custom) are developed for this purpose.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in generating images that not only preserve the appearance of target entities but also control fine-grained interactions between them, particularly focusing on human-object interactions.

Method: The authors processed a large dataset with the same human-object pairs in varied interactions and propose a two-stage model (Interact-Custom). The model first generates a spatial foreground mask to model interaction behavior and then generates the image with interaction while maintaining identity features.

Result: The paper validates their method with tailored evaluation metrics for the CHOI task, showing that Interact-Custom effectively balances identity preservation, interaction control, and content customization.

Conclusion: The study introduces the CHOI task and a model that handles both identity preservation and semantic interaction control well. The approach advances the capability to generate customizable images with desired interactions and high controllability.

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [113] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: The paper presents a novel approach to generating high-speed, high-quality full-color holographic video by overcoming limitations in color fidelity and computational inefficiency.


<details>
  <summary>Details</summary>
Motivation: Current CGH methods struggle with trade-offs between high frame rate and color fidelity due to over-smoothed phases and narrow angular spectra, as well as computational inefficiencies from neglecting spatial-temporal correlations across frames.

Method: The paper introduces Spectrum-Guided Depth Division Multiplexing (SGDDM) for optimizing phase distributions via frequency modulation and HoloMamba, a lightweight Mamba-Unet architecture designed to model spatial-temporal correlations across video sequences.

Result: SGDDM enables high-fidelity full-color display at high frame rates, and HoloMamba achieves 1080p full-color holographic video generation at over 260 FPS, which is 2.6× faster than prior methods.

Conclusion: The proposed methods demonstrate significant improvements in both quality and speed for full-color holographic video generation, making CGH more feasible for next-generation displays.

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [114] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: The paper introduces Score-based Discriminator Correction (SBDC), a technique to improve conditional diffusion models affected by noisy datasets, enhancing their generative capabilities and controllability.


<details>
  <summary>Details</summary>
Motivation: To address the impact of labeling errors in large datasets on conditional diffusion models and improve their outcomes despite noise.

Method: SBDC leverages adversarial loss via discriminator training alongside prior noise-detection techniques. Guidance is limited to the early phases of generation to optimize performance without retraining the models.

Result: The proposed method demonstrated superior performance compared to previous state-of-the-art techniques under various noise settings.

Conclusion: SBDC improves accuracy and efficiency in conditional diffusion models with minimal computational overhead, offering a practical solution to leveraging noisy datasets effectively.

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [115] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: The paper improves monocular 3D object detection (Mono3D) by introducing methods to address challenges like occlusion, dataset diversity, large object detection, and out-of-distribution camera setups.


<details>
  <summary>Details</summary>
Motivation: The motivation is to generalize Mono3D models for better performance in diverse and challenging scenarios, as Mono3D is crucial for applications like autonomous driving, augmented reality, and robotics.

Method: The methods include GrooMeD-NMS for handling occlusions, DEVIANT backbones for dataset generalization, SeaBird for addressing large object detection, and mathematical analysis for enhancing generalization to unseen camera heights.

Result: The approaches improve Mono3D robustness against occlusions, dataset shifts, large object noise sensitivity, and camera parameter changes.

Conclusion: Applying the proposed enhancements makes Mono3D models more adaptable and robust across diverse scenarios and datasets.

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [116] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: This paper explores the robustness of YOLO models under different quantization methods, considering real-world input degradations and introduces a degradation-aware calibration strategy for PTQ.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying efficient YOLO object detection models on resource-constrained devices while maintaining robustness against real-world input degradations like noise and compression.

Method: An empirical study was conducted evaluating the robustness of YOLO models in different precision formats (FP32, FP16, Dynamic UINT8, Static INT8). A degradation-aware calibration strategy was introduced to expose the TensorRT calibration process to both clean and degraded images.

Result: Static INT8 TensorRT models achieved significant speedups (~1.5-3.3x) but experienced an accuracy drop (~3-7% mAP50-95). Degradation-aware calibration showed limited improvement in robustness with exceptions for larger models under specific noise conditions.

Conclusion: Although degradation-aware calibration has potential in certain scenarios, enhancing PTQ robustness for broader degradation conditions remains challenging, emphasizing the complexity of deploying quantized detectors effectively in varied environments.

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [117] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: The paper introduces IELDM, an improved data augmentation framework for DGSS, and IELFormer, a semantic segmentation model, both targeting increased generalization using inverse evolution layers and multi-scale frequency fusion.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation models often fail to reliably generalize to unseen domains due to structural or semantic defects in augmented synthetic data generated by diffusion models.

Method: The authors design inverse evolution layers (IELs) to mitigate flaws in synthetic data by emphasizing spatial and semantic inconsistencies. IELs are integrated into both the data generative process (IELDM) and the DGSS model's decoder (IELFormer), coupled with a multi-scale frequency fusion module for improved feature coherence.

Result: Experimental evaluations show that IELDM and IELFormer outperform existing DGSS methods in benchmark datasets in terms of domain generalization.

Conclusion: IELDM and IELFormer significantly enhance data quality and model generalization for DGSS, offering superior cross-domain robustness and improved segmentation accuracy.

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [118] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: This paper introduces LF-VAR, a model for synthetic skin image generation that ensures high fidelity and clinical relevance while offering control over lesion location and type.


<details>
  <summary>Details</summary>
Motivation: Clinical skin images are often limited, posing challenges for deep-learning training. Existing synthesis methods produce low-quality outcomes with limited control over lesion attributes.

Method: LF-VAR uses a multiscale lesion-focused VQ-VAE for image encoding and a VAR Transformer for synthesis. Conditional embeddings based on lesion measurement and type enhance image fidelity.

Result: The model achieves an average improved FID score of 0.74 for seven lesion types, surpassing the previous best by 6.3%.

Conclusion: LF-VAR proves effective in generating high-quality, clinically useful skin images, addressing previous limitations using structured tokenization and lesion-aware embeddings.

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [119] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: This paper addresses long-tailed visual recognition challenges by introducing DQRoute, a modular framework combining adaptive training and decentralized expert routing to improve performance on rare and difficult classes.


<details>
  <summary>Details</summary>
Motivation: Long-tailed visual recognition struggles due to class imbalance and varying classification difficulty across categories. Existing methods fail to adequately tackle intrinsically hard-to-learn classes.

Method: DQRoute estimates class difficulty using uncertainty and historical performance, applies adaptive loss weighting, employs a mixture-of-experts architecture, and dynamically routes expert predictions based on confidence scores from expert-specific detectors, all trained end-to-end.

Result: DQRoute outperforms existing methods on long-tailed benchmarks, showcasing notable improvements on rare and difficult classes.

Conclusion: Integrating difficulty-aware optimization with decentralized expert routing provides an effective approach to address the challenges of long-tailed visual recognition.

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [120] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: The paper introduces CoPLOT, a collaborative perception framework using optimized point-level tokens for improved object recognition and localization.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative perception methods use 2D bird's-eye-view representations but lose critical 3D structural information.

Method: CoPLOT employs a point-native pipeline with token reordering, frequency-enhanced sequence modeling, and spatial alignment to improve collaborative perception.

Result: Experiments show CoPLOT surpasses existing models, achieving superior accuracy with lower communication and computational demands.

Conclusion: CoPLOT effectively addresses challenges in collaborative perception, demonstrating its potential in both simulated and real-world tasks.

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [121] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: This paper introduces an unsupervised and lightweight skeleton-based action localization method for sports videos using a pre-trained Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) and a novel Action Dynamics Metric (ADM).


<details>
  <summary>Details</summary>
Motivation: The research addresses the computational challenges and lack of adaptability of existing supervised and weakly supervised action localization models, especially in sports videos with rapid and subtle movements.

Method: An ASTGCN is pre-trained on a pose-sequence denoising task to learn motion dynamics without labeled data. During inference, a novel ADM is used to detect action boundaries by analyzing curvature inflection points.

Result: The method achieves 82.66% mean Average Precision (mAP) and an average localization latency of 29.09 ms on the DSV Diving dataset, matching the performance of supervised methods while being computationally efficient.

Conclusion: This approach demonstrates strong performance in action localization without manual labeling, robustly generalizing to unseen footage, making it suitable for lightweight and real-time applications in dynamic environments.

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [122] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: The paper introduces a compact image denoising method using dynamically-generated kernels to address generalization and overfitting issues with unseen noise types and levels.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of overfitting and limited generalization posed by deep learning methods for image denoising.

Method: Introduces dynamically generated pixel-wise varying kernels utilizing noise-invariant feature extraction, global statistics, and local correlations.

Result: Achieves superior efficiency and restoration quality even when trained on single-level Gaussian noise, excelling across diverse noise types and levels.

Conclusion: Iterative dynamic filtering is promising for practical image denoising, combining compactness and robustness against unseen noise.

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [123] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: Introduces Video-LevelGauge, a benchmark to evaluate positional bias in large video language models (LVLMs), revealing significant biases and offering insights for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing LVLM evaluation methods lack focus on positional bias, which is essential for nuanced video understanding.

Method: The benchmark uses standardized probes, customized setups, and combines statistical and morphological analysis on curated video datasets.

Result: Assessment of 27 LVLMs reveals notable positional biases in open-source models, while commercial models perform consistently.

Conclusion: Video-LevelGauge highlights the importance of addressing positional bias, offering actionable guidance for model refinement and reducing biases.

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [124] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: The paper presents the ODAL framework, a distributed solution for detecting and localizing objects in car interiors, addressing computational constraints by leveraging vision foundation models between on-board and cloud systems.


<details>
  <summary>Details</summary>
Motivation: Improving AI tasks in car interiors to identify and localize objects despite constraints in computational resources on-board.

Method: Development of ODAL framework with a distributed architecture utilizing vision foundation models, and introducing ODALbench metrics for benchmarking.

Result: A fine-tuned ODAL-LLaVA model achieves significant improvements, including a 71% performance boost over baseline, outperforming GPT-4o by 20%, and reducing hallucinations with triple ODAL$_{SNR}$.

Conclusion: The ODAL framework enhances scene understanding in constrained on-board systems, sets new benchmarks in detection/localization, and demonstrates the potential of fine-tuning lightweight models for high performance.

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [125] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: The paper introduces Vision-SR1, a reinforcement learning-based method to tackle visual hallucinations and language shortcuts in vision-language models (VLMs) by leveraging self-reward mechanisms without external visual supervision.


<details>
  <summary>Details</summary>
Motivation: VLMs often struggle with visual hallucinations and language shortcuts due to sparse visual signals in training and dependency on language-based reasoning over visual perception.

Method: Vision-SR1 decomposes reasoning into two stages: generating self-contained visual perceptions and validating them via language reasoning within the model, using reinforcement learning for self-reward mechanisms.

Result: Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces dependence on language shortcuts in diverse vision-language tasks.

Conclusion: The self-reward approach of Vision-SR1 effectively enhances visual perception and language reasoning simultaneously, tackling key shortcomings in current VLMs without relying on costly human annotations or external labels.

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [126] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: The paper investigates energy efficiency claims of Spiking Neural Networks (SNNs) using 3-D satellite position estimation, comparing them to Convolutional Neural Networks (CNNs). Results uncover discrepancies depending on hardware and input characteristics.


<details>
  <summary>Details</summary>
Motivation: SNNs are perceived as energy-efficient, particularly for constrained domains like space applications. However, recent studies question their efficiency, necessitating further exploration.

Method: A Spiking Neural Network (SNN) was applied to multi-output regression for satellite position estimation, using the Leaky Integrate-and-Fire (LIF) neuron model. Hardware-aware and hardware-agnostic energy analyses were performed.

Result: SNNs matched CNNs' Mean Squared Error (MSE) for 3-D satellite position estimation but showed energy advantages primarily on neuromorphic hardware and with sparse input data.

Conclusion: Energy efficiency claims of SNNs are context-dependent, influenced by hardware characteristics and data sparsity. Transparent evaluation methods are needed for fair efficiency comparisons.

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [127] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: The paper introduces a novel self-supervised learning approach for enhancing Ultra-Wide-Field (UWF) retinal images, focusing on preserving pathological details through image deblurring and illumination correction.


<details>
  <summary>Details</summary>
Motivation: UWF retinal imaging is a breakthrough in retinal diagnostics but suffers from issues like blurring and uneven illumination that obscure pathological details, requiring dedicated enhancement methods.

Method: The authors propose a frequency-aware self-supervised learning method incorporating a frequency-decoupled image deblurring module and a Retinex-guided illumination compensation module, along with asymmetric channel integration and color preservation techniques.

Result: Experimental results indicate improved visualization quality and disease diagnosis accuracy due to enhanced local detail restoration and illumination correction.

Conclusion: This work is the first to address UWF image enhancement with an effective, clinically valuable methodology potentially aiding in better retinal disease management.

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [128] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: The paper introduces SAT, a framework for creating high-quality textured 3D human avatars from a single image, overcoming issues like geometric ambiguity and data scarcity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in creating 3D avatars from single 2D images, such as geometric ambiguity and the lack of sufficient 3D human training data.

Method: The framework employs unified learning of various geometric priors and introduces two main modules: Supervisor Feature Regularization for better geometry learning and Online Animation Augmentation for data enhancement.

Result: Experiments on two benchmarks demonstrate the superior performance of the proposed method compared to existing techniques.

Conclusion: The proposed SAT framework effectively addresses the issues of integrating geometric forms and data scarcity, offering a significant advancement in monocular 3D human reconstruction.

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [129] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: This paper proposes an unsupervised method to differentiate real images from synthetic ones generated by GANs and diffusion models, based on physics-inspired community detection on graphs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of existing detectors failing against unseen generative models or adversarial image processing.

Method: The method involves extracting image features using pretrained CNNs, representing them as nodes in an LDPC graph, and analyzing the graph's RBIM spectrum to detect Nishimori symmetry violations indicative of synthetic images.

Result: The approach achieved over 94% accuracy on tasks like distinguishing real from synthetic cat, dog, male, and female images, without needing labeled synthetic examples.

Conclusion: This novel detector is unsupervised, model-agnostic, and robust to emerging generative models, with potential applicability in videos and anomaly detection.

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [130] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: The paper introduces Label-aware 3D Gaussian Splatting (LabelGS) to overcome the lack of 3D segmentation in 3DGS representation, achieving significant performance improvements and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the deficiency in 3D segmentation capabilities in 3D Gaussian Splatting (3DGS), which limits its usability in tasks requiring scene understanding.

Method: The authors introduce LabelGS, which incorporates cross-view consistent semantic masks, an Occlusion Analysis Model, Gaussian Labeling model, and a Gaussian Projection Filter to enhance optimization and labeling accuracy of 3D Gaussians.

Result: LabelGS surpasses prior state-of-the-art methods like Feature-3DGS in 3D scene segmentation tasks and achieves a 22X training speedup at 1440X1080 resolution.

Conclusion: LabelGS effectively integrates object labeling with 3D Gaussian splatting to enable segmentation while offering improved optimization and efficiency, expanding the applicability of 3DGS.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [131] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: This paper addresses limitations in video polyp segmentation by combining spatial and temporal modeling, improving stability in long-term tracking.


<details>
  <summary>Details</summary>
Motivation: Existing VPS paradigms struggle with balancing spatiotemporal modeling and domain generalization, limiting their clinical applicability.

Method: Authors repurpose SAM2 with two training-free modules: intra-association filtering for spatial inaccuracies and inter-association refinement for error propagation.

Result: The proposed approach achieves superior performance in both in-domain and out-of-domain settings and demonstrates robust tracking in long colonoscopy videos.

Conclusion: The method stabilizes SAM2 for video polyp segmentation, showcasing its potential for reliable clinical analysis.

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [132] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: This paper addresses the challenge of improving deepfake detection models' ability to generalize to media content in real-world scenarios by leveraging a face foundation model (FSFM) with enhanced training techniques.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models often fail to generalize effectively to in-the-wild media content, which raises concerns about their reliability in ensuring media integrity.

Method: The authors fine-tune a self-supervised face foundation model (FSFM) using an ensemble of deepfake datasets, utilize triplet loss variants for better embedding separation, and implement attribution-based supervision based on manipulation types or datasets.

Result: The proposed method shows strong performance and generalization capability across diverse evaluation benchmarks, particularly in real-world scenarios.

Conclusion: The framework effectively enhances the generalizability and reliability of deepfake detection, supporting its application in addressing real-world media authenticity issues.

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [133] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: This paper introduces an upgraded Pixel Orientation Estimation (POEv2) method for detecting both generic and wireframe line segments in images, achieving state-of-the-art results on public datasets.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of current line segment detectors, which are specialized for either generic or wireframe detection, and proposes a unified framework that performs well for both tasks.

Method: The method improves the Pixel Orientation Estimation approach by detecting line segments through edge strength maps, allowing integration with any edge detector.

Result: Experiments show that the proposed POEv2 combined with an edge detector achieves state-of-the-art performance across three public datasets.

Conclusion: The POEv2 method demonstrates robust and flexible capabilities, bridging the gap between generic and wireframe segment detection with superior performance results.

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [134] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: The paper introduces a new model, SPLF-SAM, tackling issues in light field salient object detection, improving on frequency-domain analysis and multi-scale feature extraction.


<details>
  <summary>Details</summary>
Motivation: To improve light field salient object detection, addressing issues of prompt information extraction and noise impact on small objects.

Method: Develops SPLF-SAM with two key components: UMFEB for multi-scale object detection and MAFA for enhancing frequency-domain feature learning.

Result: SPLF-SAM shows superior performance compared to ten state-of-the-art methods in extensive experiments.

Conclusion: The proposed model effectively enhances light field salient object detection, particularly in handling small objects and integrating prompt and frequency-domain information.

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [135] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: The paper presents FastAvatar, a cutting-edge framework for fast and high-quality 3D avatar reconstruction using different types of daily recordings.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies like high time complexity, excessive data sensitivity, and poor data utilization in existing 3D avatar reconstruction methods.

Method: FastAvatar employs a feedforward design powered by a Large Gaussian Reconstruction Transformer, integrating multi-frame 3D cues, granular guidance, and incremental Gaussian aggregation.

Result: The experiments demonstrated that FastAvatar achieves better reconstruction quality and competitive speed compared to current methods.

Conclusion: FastAvatar provides a scalable, flexible, and efficient solution for 3D avatar modeling, with the ability to incrementally improve output quality with additional data.

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [136] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: BuzzSet, a dataset with 7,856 high-resolution pollinator images, supports automated monitoring of honeybees, bumblebees, and unidentified insects for ecological research.


<details>
  <summary>Details</summary>
Motivation: Declining pollinator populations threaten food production and environmental stability, necessitating scalable monitoring solutions.

Method: The BuzzSet dataset includes 7,856 labeled images preprocessed into 256x256 tiles and refined through human verification, with performance validated using RF-DETR object detection models.

Result: The RF-DETR model achieved F1-scores of 0.94 and 0.92 for honeybee and bumblebee detection, with solid overall detection quality and mAP@0.50 of 0.559.

Conclusion: BuzzSet provides significant improvements for monitoring pollinators, advancing small object detection and ecological computer vision benchmarks.

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [137] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: This paper identifies optimization bias as a key problem in imbalanced multimodal learning and introduces Adaptive Intra-Network Modulation (AIM) to balance learning across modalities without compromising performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of imbalanced multimodal learning, where strategies to boost weaker modalities often hinder the dominant modality, leading to suboptimal overall performance.

Method: The authors propose AIM, a method that separates under-optimized parameters of the dominant modality into Auxiliary Blocks and encourages their joint optimization with weaker modalities. AIM also adjusts modulation strength adaptively across network depths based on the modality imbalance.

Result: Experimental evaluations show that AIM achieves superior performance compared to state-of-the-art methods on several benchmarks and demonstrates strong generalizability across different model architectures, fusion strategies, and optimizers.

Conclusion: AIM provides a balanced learning approach in multimodal settings, ensuring that both dominant and weaker modalities are improved without hindrance. This method brings a new perspective to solving imbalanced modality issues effectively.

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [138] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: The paper proposes a structural recognition system for handwritten mathematical expressions that directly aligns symbols with handwritten traces for improved error analysis and interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing systems in providing explicit symbol-to-trace alignment crucial for error analysis and spatially-aware applications.

Method: The paper introduces an auto-labeling system for annotations and a modular structural recognition system optimized for segmentation, classification, and relation prediction using techniques like graph-based trace sorting, hybrid networks, and transformers.

Result: The system shows competitive performance on the CROHME-2023 benchmark by leveraging enriched datasets and generating graph structures linking traces to symbols.

Conclusion: The work enhances interpretability and enables transparent error analysis in handwritten mathematical expression recognition, addressing key shortcomings in current approaches.

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [139] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: The paper introduces MAPo, a framework addressing limitations in dynamic scene reconstruction using 3D Gaussian splatting by employing motion-aware partitioning and a cross-frame consistency loss.


<details>
  <summary>Details</summary>
Motivation: Dynamic scene reconstruction using deformation-based methods often results in blurred renderings and poor handling of intricate motion details due to limitations in modeling diverse motion patterns.

Method: MAPo employs a dynamic score-based partitioning strategy, separating high- and low-dynamic 3D Gaussians. High-dynamic ones are recursively partitioned temporally for specialized modeling, while low-dynamic ones are treated as static to reduce computational costs. A cross-frame consistency loss ensures visual continuity across temporal partitions.

Result: MAPo significantly enhances rendering quality while retaining computational efficiency, especially in regions with rapid or complex motion.

Conclusion: MAPo overcomes the limitations of prior deformation-based methods by addressing visual continuity and rendering fidelity, making it a promising approach for dynamic scene reconstruction.

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [140] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic proposes a one-step diffusion model for material estimation, yielding high-quality parameters with low variance and resolving the overly-smoothing problem and detail loss issues.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and high variance in stochastic, multi-step diffusion-based material estimation methods.

Method: One-step diffusion model with pixel-space losses tailored to material properties, and introducing a Detail Injection Network (DIN) to enhance detail retention.

Result: Achieved 9.9% improvement in PSNR for albedo and reduced MSE for metallic by 44.4% and roughness by 60.0%.

Conclusion: The proposed StableIntrinsic model advances state-of-the-art material estimation with efficient inference, reduced variance, and improved accuracy.

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [141] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: The paper addresses challenges in accurately generating multi-object images based on complex text prompts, specifically focusing on color semantics. A new image editing technique is proposed to enhance alignment.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image generation struggles with capturing precise semantic alignment for complex prompts containing multiple objects, especially when attributing specific colors. Existing solutions and metrics are insufficient.

Method: The authors conduct an analysis focusing on color attributes, revealing shortcomings in pre-trained models and existing inference-time methods. They propose a novel image editing technique to address semantic misalignments involving multiple colors.

Result: The proposed method demonstrates significant improvements in aligning generated images with complex text prompts containing multiple color attributes, outperforming existing methods across various metrics.

Conclusion: This work highlights a key limitation in text-to-image generation models and provides an effective solution for improving multi-object semantic alignment in prompts involving multiple colors.

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [142] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: The paper proposes a neural architecture enhancing waste sorting systems with innovations like a Comprehensive Attention Block, Mamba architecture, and a Data Fusion Block, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Automating waste management, particularly non-biodegradable sorting, is challenging due to waste stream complexity.

Method: The authors enhance an Encoder-Decoder model with a Comprehensive Attention Block, Mamba architecture, and a Data Fusion Block utilizing PCA transformation for multi-channel image fusion.

Result: Experiments on various data types including RGB and hyperspectral data show the proposed approach significantly outperforms existing models.

Conclusion: The proposed neural architecture improves accuracy and efficiency in automated waste sorting, validating its superiority over current methods.

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [143] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: This paper proposes a robust and real-time method for detecting mitotic figures in histopathology images, achieving strong generalization across diverse domains, and demonstrating suitability for clinical use.


<details>
  <summary>Details</summary>
Motivation: Detection of mitotic figures is challenging due to significant variability in scanners, staining, tissue types, and artifacts. The goal is to create a reliable and fast solution suitable for clinical applications.

Method: The authors extended the RTMDet object detector with multi-domain training, balanced sampling, targeted data augmentation, and hard negative mining techniques.

Result: The method achieved an F1 score of 0.78-0.84 across multiple datasets, and 0.81 in a test set of the MIDOG 2025 challenge, surpassing larger models.

Conclusion: The proposed framework strikes a balance between accuracy and speed, making it viable for clinical deployment while generalizing effectively across domains.

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [144] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS introduces an unsupervised framework for video instance segmentation, using quality-guided self-training to surpass synthetic-to-real domain limitations.


<details>
  <summary>Details</summary>
Motivation: Address the annotation challenges in video instance segmentation, focusing on avoiding dependencies on synthetic data and tackling the domain gap.

Method: A closed-loop system between pseudo-label generation and automatic quality assessment facilitates progressive adaptation from synthetic to real videos in an unsupervised manner.

Result: Achieved 52.6 AP50 on the YouTubeVIS-2019 validation set, outperforming previous state-of-the-art methods by 4.4% without requiring human annotations.

Conclusion: Quality-guided self-training is a feasible approach for advancing unsupervised video instance segmentation.

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [145] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: This paper introduces a novel semi-supervised method, ERSR, for fetal head segmentation in ultrasound images, tackling challenges of poor image quality and limited annotated data. ERSR achieves high performance on key datasets.


<details>
  <summary>Details</summary>
Motivation: Fetal head segmentation in ultrasound images is critical for prenatal care, but is hindered by low image quality and insufficient annotated datasets. Previous semi-supervised methods struggle with these specific challenges in ultrasound imaging.

Method: The proposed ERSR framework includes three key components: (1) a dual-scoring adaptive filtering strategy to evaluate and refine teacher outputs based on consistency and contour criteria, (2) ellipse-constrained pseudo-label refinement to enhance core regions and reduce noise, and (3) symmetry-based multiple consistency regularization to enforce robust shape representation across different levels.

Result: The method achieves state-of-the-art results with Dice scores of 92.05% and 95.36% on the HC18 dataset with 10% and 20% labeled data, and scores of 91.68% and 93.70% on the PSFH dataset under similar conditions.

Conclusion: ERSR's innovative approach and high performance demonstrate its efficacy and potential in addressing challenges in semi-supervised fetal head segmentation, contributing significantly to prenatal monitoring advancements.

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [146] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: The paper addresses the issue of overconfidence in neural network predictions, particularly under distribution shifts, by proposing a new calibration framework without requiring access to the target domain.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability of neural networks in safety-critical applications, especially under distribution shifts where the test data differs from the training distribution.

Method: The method applies low-frequency filtering to encourage reliance on domain-invariant features and incorporates a gradient-based rectification mechanism to enforce in-distribution calibration as a constraint.

Result: Experiments on multiple datasets demonstrate improved calibration under distribution shifts while maintaining strong performance on in-distribution data.

Conclusion: The proposed framework effectively addresses the calibration problem in neural networks, providing robustness to distribution shifts without requiring target domain knowledge.

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [147] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: The paper introduces a machine-centric image quality assessment (MIQA) framework to gauge image degradation impacts on machine vision system (MVS) performance, along with a novel MIQD-2.5M database and region-aware MIQA (RA-MIQA) model.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the vulnerability of machine vision systems to performance loss in poor visual conditions, aiming to evaluate and enhance MVS reliability under adverse scenarios.

Method: The authors introduce an MIQA framework and its MIQD-2.5M database with multiple degradation types and propose the RA-MIQA model for assessing spatial degradation impacts and improving accuracy in MVS tasks.

Result: RA-MIQA demonstrates superior predictive capabilities compared to human visual system-based and classical models, showing detailed degradation sensitivity analysis across 75 models, 250 types, and diverse tasks.

Conclusion: The study underscores the inadequacy of traditional human-centric metrics for MVS quality assessment, proposing MIQA as a tool to improve MVS dependability and foster machine-centric imaging techniques.

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [148] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: The paper introduces a unified model to jointly predict actions and visualize future scenes in egocentric scenarios, aiming to improve human-object interaction modeling and robotic planning.


<details>
  <summary>Details</summary>
Motivation: Existing models either focus narrowly on action prediction without visual outcomes or generate future scenes without considering actions, creating a gap in comprehensive understanding.

Method: A two-stage framework is proposed: first, leveraging consecutive state modeling for action and trajectory prediction using various inputs; second, employing causal cross-attention with Latent Diffusion Models for guided video synthesis.

Result: Experiments indicate the proposed method surpasses state-of-the-art approaches in both action prediction and video synthesis across multiple datasets.

Conclusion: The unified model successfully combines action prediction and visual consequence generation, advancing human activity understanding and robotic manipulation applications.

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [149] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: The paper introduces MCMeshGAN, a novel method to predict 3D aortic aneurysm growth using a combination of local and global architectural approaches, outperforming existing methods in key metrics.


<details>
  <summary>Details</summary>
Motivation: Timely intervention requires precise modeling of aneurysm progression, but current methods struggle with capturing both localized and global anatomical changes in complex 3D structures.

Method: MCMeshGAN employs a dual-branch architecture with a KNN-based convolutional network for local deformations and a graph convolutional network for capturing global structure. It integrates clinical attributes for temporal predictions and introduces the TAAMesh dataset for testing.

Result: Extensive experiments showed MCMeshGAN's superior performance over state-of-the-art methods in geometric accuracy and clinical metrics, like diameter estimation.

Conclusion: MCMeshGAN is a significant advancement toward personalized, clinically deployable 3D disease trajectory modeling for aortic aneurysms, supported by publicly available implementation and dataset.

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [150] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: This paper introduces a self-supervised learning (SSL) method using a ProtoScale module that focuses on structured visual representations for better performance in dense prediction tasks and object detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in recent SSL methods, which struggle to capture structured representations in scenes despite excelling in global image understanding.

Method: The authors propose a novel ProtoScale module that integrates semantic grouping, instance-level separation, and hierarchical structuring while preserving full scene context across augmented views.

Result: The approach enhances supervised object detection performance and outperforms state-of-the-art methods using limited annotated data and fewer fine-tuning epochs.

Conclusion: Structured self-supervised learning can significantly improve object-centric representation, making it highly effective for dense prediction tasks and object detection.

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [151] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: This paper introduces TrajFusionNet, a transformer-based model integrating future pedestrian trajectory and vehicle speed predictions for enhanced pedestrian crossing intention prediction.


<details>
  <summary>Details</summary>
Motivation: To improve prediction accuracy for pedestrian crossing intention, which is critical for the safety of autonomous vehicles as they navigate public roads.

Method: TrajFusionNet combines a Sequence Attention Module (SAM) to process sequential trajectory data and vehicle speeds, and a Visual Attention Module (VAM) to analyze visual data using bounding boxes on scene images.

Result: The proposed model outperforms state-of-the-art methods across three commonly-used datasets, achieving the best inference time and prediction performance.

Conclusion: TrajFusionNet effectively enhances pedestrian crossing intention prediction, offering both efficiency and accuracy, which makes it a strong candidate for real-world application in autonomous vehicles.

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [152] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: This paper proposes SMI, a mutual information based model, to enhance sky background subtraction for fiber spectral analysis, addressing limitations in current methods.


<details>
  <summary>Details</summary>
Motivation: Current sky background subtraction relies heavily on averaged 'Super Sky' spectra obtained from sky fiber data, neglecting the local environmental details surrounding objects.

Method: SMI employs mutual information and an incremental training strategy. It uses spectra from all fibers in a plate and comprises two networks: one for wavelength calibration (solving feature shift problems) and another for extracting shared and individual spectral components.

Result: Experiments conducted on LAMOST spectra revealed that SMI improved sky background accuracy, particularly at the blue end of the spectrum.

Conclusion: SMI provides a more individualized and precise method for estimating sky backgrounds, enhancing spectral analysis during observations.

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [153] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: The study explores tree point extraction using multispectral LiDAR and deep learning models, with Superpoint Transformer performing best, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhancing urban tree dynamics monitoring to support greening policies and mitigate risks, leveraging advanced mapping technologies.

Method: Utilized multispectral LiDAR to capture spatial and spectral tree data and evaluated three deep learning models for tree point extraction.

Result: Superpoint Transformer achieved the highest mean intersection over union (85.28%) and pNDVI integration reduced error rates by 10.61%.

Conclusion: MS-LiDAR combined with deep learning models demonstrates considerable potential for improving accuracy and efficiency in urban tree extraction and inventories.

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [154] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: This paper introduces PersonaAnimator, a framework for personalized motion transfer from videos, supplemented by a new dataset and physical regularization techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack style personalization, rely on hard-to-get motion capture data, and sometimes create physically implausible motions.

Method: The authors developed PersonaAnimator and proposed Physics-aware Motion Style Regularization. Additionally, they created PersonaVid, a dataset tailored for personalized motion tasks.

Result: PersonaAnimator surpasses previous motion transfer models in performance and enriches the field with a new benchmark for personalizing video-based motion.

Conclusion: The framework successfully addresses style personalization, avoids reliance on motion capture data, and improves physical plausibility, setting a benchmark for future research in this domain.

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [155] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: Hyperspectral Imaging (HSI) offers advanced capabilities for automotive applications like ADAS and autonomous driving but faces significant gaps in commercial readiness and dataset limitations.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of hyperspectral imaging in enhancing perception capabilities for automotive applications and to identify the gaps hindering its practical deployment.

Method: The paper provides a qualitative review of HSI technologies, benchmarks 216 HSI and multispectral cameras on automotive criteria, and evaluates current HSI datasets and applications.

Result: Only four cameras meet performance thresholds, none comply with AEC-Q100 temperature standards, and existing datasets have significant limitations in scale, spectral consistency, and environmental diversity.

Conclusion: Although HSI shows promise, it requires further development in hardware and datasets to achieve integration into Advanced Driver Assistance Systems and autonomous driving platforms.

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [156] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: The paper introduces KRETA, a benchmark for text-rich Visual Question Answering (VQA) in Korean to address gaps in robust evaluation for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Visual Question Answering has advanced for high-resource languages, but non-English languages like Korean lack reliable benchmarks. There is a need to ensure robust model evaluation across languages.

Method: KRETA includes 15 domains and 26 image types for evaluating visual text comprehension and reasoning. A semi-automated pipeline utilizing stepwise image decomposition and seven metrics ensures quality in dataset generation.

Result: KRETA offers a dataset and benchmark tailored for Korean, with tools that are extensible to other languages. They include robust evaluation protocols for text-rich VQA.

Conclusion: KRETA fills a critical gap in Vision-Language research for Korean and sets the stage for multilingual developments in this domain while ensuring extensibility.

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [157] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: The paper introduces the object-based set similarity ($\mathrm{OSS}$) metric to improve computational and evaluation challenges in active learning for object detection. OSS eliminates ineffective active learning methods before detector training, saving resources, and selects robust validation sets.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address computational inefficiencies and reliability issues in existing active learning methods for object detection, particularly in scenarios like autonomous driving, where GPU hours are costly and validation is critical for safety.

Method: The authors developed the object-based set similarity ($\mathrm{OSS}$) metric to measure the similarity between training sets and target domains using object-level features. This avoids the need for multiple detector trainings and enhances validation set selection.

Result: The OSS metric was validated using uncertainty-based active learning methods and two detector architectures (EfficientDet and YOLOv3) across three autonomous driving datasets (KITTI, BDD100K, CODA). It successfully enabled training set optimization and more robust evaluation.

Conclusion: OSS is a detector-agnostic, computationally efficient, and effective framework that integrates seamlessly with active learning pipelines, making it practical for real-world object detection deployments requiring reliability and cost-effectiveness.

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [158] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: The paper introduces GLSim, a training-free framework for detecting object hallucination in vision-language models using global and local embedding similarities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of object hallucination in vision-language models for their safe real-world application.

Method: Proposes GLSim, leveraging global and local embedding similarity signals between image and text modalities without requiring training.

Result: GLSim outperforms existing object hallucination detection methods by a notable margin in benchmarks.

Conclusion: Using both global and local perspectives together improves the reliability and accuracy of hallucination detection in vision-language models.

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [159] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: The paper proposes a method to enhance 3D semantic segmentation by using 2D segmentation masks from foundation models and propagating them into 3D space, extending sparse annotations and providing better label utilization.


<details>
  <summary>Details</summary>
Motivation: 3D point cloud data is large, irregular, and unordered, making annotation difficult. Current methods for weakly supervised segmentation often ignore the complementary nature of 2D and 3D data or fail to fully utilize extended or pseudo labels, some of which may be noisy.

Method: The approach incorporates segmentation masks from 2D foundation models and maps them to 3D through geometric correspondences. Sparse 3D annotations are extended to areas covered by 3D masks. Confidence- and uncertainty-based regularization selects reliable pseudo labels, which are further propagated for more labels.

Result: The method effectively extends sparse annotations and leverages 2D segmentation models to enhance 3D weakly supervised segmentation.

Conclusion: By bridging 2D foundation models with 3D data, the approach addresses annotation challenges and improves 3D segmentation performance, significantly advancing weakly supervised learning techniques.

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [160] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: WaveHiT-SR integrates wavelet transform into a hierarchical transformer framework to achieve efficient and high-performing image super-resolution.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current transformer-based image super-resolution methods, including high computational complexity and small fixed window size, restricting receptive field.

Method: WaveHiT-SR uses adaptive hierarchical windows to capture features across different levels and integrates wavelet transform for image decomposition into frequency subbands.

Result: The method effectively reduces computational complexity while delivering state-of-the-art performance, with fewer parameters, lower FLOPs, and increased speed.

Conclusion: WaveHiT-SR presents a promising approach by enhancing efficiency and accuracy in super-resolution tasks through innovative hierarchical processing and frequency decomposition methods.

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [161] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: The paper studies the Chan-Vese algorithm for image segmentation, introducing a novel functional segmentation loss based on active contours.


<details>
  <summary>Details</summary>
Motivation: To analyze the Chan-Vese algorithm for image segmentation and propose improvements leveraging modern computer vision techniques.

Method: Employs a discretized scheme of the Chan-Vese model's functional energy, provides proofs, implements it in MATLAB, and integrates the model with neural network-based loss functions (using PyTorch).

Result: The proposed method is compared with standard segmentation datasets, showing its performance against classical loss functions.

Conclusion: The study demonstrates the efficacy of the novel segmentation loss, with all materials provided for transparency and reproducibility.

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [162] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: This paper evaluates the geo-localization capabilities and privacy risks of Vision-Language Models (VLMs) using benchmark datasets. It finds high accuracy in social media-like images, raising privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To understand and assess the geolocation precision, limitations, and societal risks of Vision-Language Models (VLMs), particularly their privacy implications due to potential misuse.

Method: The study systematically evaluates 25 state-of-the-art VLMs using four benchmark image datasets from diverse environments. It aims to analyze their geolocation precision and internal reasoning capabilities.

Result: The evaluation revealed that while VLMs perform poorly on generic street-level images, they achieve high accuracy (61%) in geo-locating social media-like images, demonstrating potential for privacy invasion.

Conclusion: Vision-Language Models pose privacy risks due to their growing geo-localization accuracy in specific contexts, like social media. Their limitations and societal risks demand further attention and regulation.

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [163] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: The paper introduces GS (Generative Segmentation), which formulates language-driven image segmentation as a generative task using label diffusion. It outperforms existing methods and sets a new state-of-the-art in this domain.


<details>
  <summary>Details</summary>
Motivation: Traditional language-driven segmentation approaches have limitations, as they are either discriminative in nature or leverage diffusion models in an auxiliary manner rather than focusing on segmentation itself.

Method: The GS framework reverses the generative process by directly generating segmentation masks from noise, conditioned on both images and textual descriptions, making label generation the primary task.

Result: The proposed method, GS, achieves significant performance improvement on the Panoptic Narrative Grounding benchmark, surpassing current state-of-the-art methods.

Conclusion: The GS framework demonstrates the effectiveness of treating segmentation as a generative task and establishes itself as a leading approach for multimodal segmentation tasks.

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [164] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: The paper introduces Incremental Test Time Adaptation (ITTA) for Vision-Language Models (VLMs), enabling adaptation to both unseen classes and domains during testing using a new Segmentation-Assisted Active Labeling module (SegAssist).


<details>
  <summary>Details</summary>
Motivation: To enhance the generalization ability of Vision-Language Models in dynamic environments with unfamiliar objects, class shifts, and distribution shifts.

Method: The authors propose the SegAssist module, which utilizes the segmentation capabilities of VLMs for active labeling without additional training, refining sample selection for test-time adaptation.

Result: The proposed approach is validated through extensive experiments on various benchmark datasets, showcasing improved adaptation and performance in real-world settings.

Conclusion: SegAssist proves effective in enabling VLMs to adapt to continuously emerging unseen classes and domains, demonstrating its practical relevance in dynamic scenarios.

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [165] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: OpenM3D introduces a novel approach for open-vocabulary indoor 3D object detection using multi-view images without human annotations.


<details>
  <summary>Details</summary>
Motivation: To address the gap in image-based open-vocabulary 3D object detection methods compared to point cloud approaches and to eliminate reliance on human-labeled annotations.

Method: OpenM3D employs 2D-induced voxel features from the ImGeoNet model, utilizes a graph embedding technique for pseudo-box generation, and incorporates two losses: class-agnostic 3D localization and voxel-semantic alignment losses with diverse CLIP features.

Result: OpenM3D achieves higher precision and recall for pseudo-box generation and demonstrates superior accuracy and speed on ScanNet200 and ARKitScenes benchmarks compared to existing methods, handling tasks in 0.3 seconds per scene.

Conclusion: OpenM3D is efficient, accurate, and outperforms other methods in open-vocabulary 3D indoor object detection, advancing the field with its training strategy and loss functions.

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [166] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: The paper addresses AMD progression tracking using OCT scans, leveraging CNN and a custom Masked Autoencoder. The authors achieved top-10 rankings in the MARIO challenge tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve personalized treatment plans for neovascular AMD by accurately monitoring and predicting disease progression using OCT scans.

Method: The authors used a fusion convolutional neural network (CNN) with model ensembling for classification in Task 1 and a Patch Progression Masked Autoencoder for predicting future progression in Task 2, incorporating their Task 1 solution.

Result: The authors achieved top-10 rankings in both tasks of the MARIO challenge, demonstrating the effectiveness of their methods.

Conclusion: The proposed methods provide promising advancements for personalized treatment planning in AMD, although the authors were not eligible for the prize due to organizational affiliations.

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [167] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: The paper introduces the Noisy Correspondence problem in geo-localization and proposes PAUL, a novel framework to improve training robustness against misaligned data pairs.


<details>
  <summary>Details</summary>
Motivation: Current cross-view geo-localization methods fail under systematic GPS drifts, as they assume perfect alignment of training image pairs, which is unrealistic in practical applications.

Method: PAUL uses uncertainty-aware co-augmentation and evidential co-training to selectively refine feature learning and suppress noise, focusing on data uncertainty and loss discrepancy for targeted partitioning.

Result: PAUL demonstrates consistent improvement over other noisy-data-driven methods across various noise ratios, validated through extensive experiments.

Conclusion: By addressing real-world alignment challenges, PAUL bridges the gap between ideal benchmarks and practical geo-localization applications, providing robust solutions for noisy correspondence issues.

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [168] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: The paper introduces a framework that integrates a dual-fisheye camera model into a 3D Gaussian splatting pipeline to correct visual artifacts in 360-degree images from consumer-grade systems.


<details>
  <summary>Details</summary>
Motivation: Consumer-grade dual-fisheye systems often yield suboptimal 360-degree images due to challenges in lens separation and angular distortion.

Method: The authors created a calibration framework that models dual-fisheye cameras within a 3D Gaussian splatting system, jointly optimizing 3D Gaussian parameters and calibration variables.

Result: The framework synthesizes seamless 360-degree images and demonstrates superior performance over existing models using real-world datasets.

Conclusion: The proposed method effectively addresses imperfections in omnidirectional images and becomes a reliable tool for flawless novel view synthesis.

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [169] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory introduces a framework combining large language models (LLMs) and text-to-audio (TTA) systems to synthesize coherent and structured long-form audio narratives.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current TTA systems in generating long-form narrative audio, particularly in maintaining temporal coherence and compositional reasoning.

Method: AudioStory integrates LLMs with TTA systems to decompose narrative generation tasks into temporally ordered sub-tasks using a decoupled bridging mechanism and an end-to-end training framework.

Result: Extensive experiments demonstrate AudioStory's superiority in both instruction-following and audio fidelity for single-audio and narrative audio generation, outperforming prior TTA baselines.

Conclusion: AudioStory effectively bridges the gap in long-form audio generation using unified LLM-TTA collaboration and sets a new benchmark, AudioStory-10K, for further advancements in this domain.

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [170] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: The paper proposes an efficient model for classifying moth species from field images, leveraging knowledge distillation from a high-performance foundation model to a lightweight architecture.


<details>
  <summary>Details</summary>
Motivation: Understanding insect declines requires identification of moth species in noisy field images, but domain shifts between field imagery and curated datasets make this task challenging.

Method: The method combines limited expert-labelled field data with knowledge distillation from the BioCLIP2 foundation model into a smaller ConvNeXt-tiny architecture.

Result: The proposed lightweight model achieves accuracy comparable to BioCLIP2's high performance while greatly reducing computational costs. BioCLIP2 also outperforms other existing methods.

Conclusion: The study provides practical guidelines for developing efficient insect monitoring systems, demonstrating how to address domain gaps for fine-grained classification of moth species.

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


### [171] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: The paper proposes CODA, a trainable compositional framework that integrates a planner and executor to tackle challenges in scientific computing GUIs.


<details>
  <summary>Details</summary>
Motivation: Autonomous agents struggle with balancing long-horizon planning and precise execution in scientific computing GUIs due to existing trade-offs in specialized and generalist agents.

Method: The two-stage training pipeline involves Specialization (decoupled GRPO for individual expert planners) and Generalization (aggregate trajectory dataset for supervised fine-tuning).

Result: CODA achieves superior performance across four applications in the ScienceBoard benchmark, becoming the new state of the art for open-source models.

Conclusion: CODA effectively bridges the gap between planning and execution for scientific computing, demonstrating robust execution and cross-domain generalization.

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [172] [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)
*Haoran Lin,Xianzhi Yu,Kang Zhao,Han Bao,Zongyuan Zhan,Ting Hu,Wulong Liu,Zekun Yin,Xin Li,Weiguo Liu*

Main category: cs.DC

TL;DR: This paper introduces HAP (Hybrid Adaptive Parallelism), a method for dynamically optimizing Mixture-of-Experts (MoE) model inference using specialized parallel strategies.


<details>
  <summary>Details</summary>
Motivation: Static parallelization in MoE inference lacks flexibility, leading to suboptimal performance across varying computational scenarios.

Method: HAP hierarchically decomposes MoE into Attention and Expert modules, constructs a search space, and utilizes Integer Linear Programming (ILP) to find optimal hybrid parallel configurations.

Result: HAP achieves up to 1.77x speedup over traditional TP-based inference strategies on various GPUs while generalizing well across different MoE models.

Conclusion: HAP demonstrates significant inference efficiency improvements and broad applicability, outperforming traditional static parallel strategies in diverse scenarios.

Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily
employ static parallelization strategies. However, these static approaches
cannot consistently achieve optimal performance across different inference
scenarios, as they lack the flexibility to adapt to varying computational
requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a
novel method that dynamically selects hybrid parallel strategies to enhance MoE
inference efficiency. The fundamental innovation of HAP lies in hierarchically
decomposing MoE architectures into two distinct computational modules: the
Attention module and the Expert module, each augmented with a specialized
inference latency simulation model. This decomposition promotes the
construction of a comprehensive search space for seeking model parallel
strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the
optimal hybrid parallel configurations to maximize inference efficiency under
varying computational constraints. Our experiments demonstrate that HAP
consistently determines parallel configurations that achieve comparable or
superior performance to the TP strategy prevalent in mainstream inference
systems. Compared to the TP-based inference, HAP-based inference achieves
speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,
respectively. Furthermore, HAP showcases remarkable generalization capability,
maintaining performance effectiveness across diverse MoE model configurations,
including Mixtral and Qwen series models.

</details>


### [173] [Formal Modeling and Verification of the Algorand Consensus Protocol in CADP](https://arxiv.org/abs/2508.19452)
*Andrea Esposito,Francesco P. Rossi,Marco Bernardo,Francesco Fabris,Hubert Garavel*

Main category: cs.DC

TL;DR: The paper introduces a formal model of the Algorand consensus protocol, leveraging process algebra to validate its correctness and robustness under both non-adversarial and adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to rigorously verify the security and robustness of the Algorand blockchain consensus protocol using formal methods.

Method: The authors develop a probabilistic process calculus model and analyze adversarial scenarios using equivalence-checking frameworks within the CADP verification toolkit.

Result: The model proves the protocol's correctness under non-adversarial conditions, highlights robustness, and identifies limitations when faced with adversarial malicious nodes.

Conclusion: This study underscores the utility of formal verification methods in analyzing blockchain protocols and provides insights into Algorand's strengths and vulnerabilities.

Abstract: Algorand is a scalable and secure permissionless blockchain that achieves
proof-of-stake consensus via cryptographic self-sortition and binary Byzantine
agreement. In this paper, we present a process algebraic model of the Algorand
consensus protocol with the aim of enabling rigorous formal verification. Our
model captures the behavior of participants with respect to the structured
alternation of consensus steps toward a committee-based agreement by means of a
probabilistic process calculus. We validate the correctness of the protocol in
the absence of adversaries and then extend our model to capture the influence
of coordinated malicious nodes that can force the commit of an empty block
instead of the proposed one. The adversarial scenario is analyzed by using an
equivalence-checking-based noninterference framework that we have implemented
in the CADP verification toolkit. In addition to highlighting both the
robustness and the limitations of the Algorand protocol under adversarial
assumptions, this work illustrates the added value of using formal methods for
the analysis of blockchain consensus algorithms.

</details>


### [174] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: This paper argues that Generative AI (GenAI) is essential to achieving Ambient Intelligence (AmI) in physical environments powered by 6G wireless networks. It discusses GenAI's role, relevant models, use cases, and challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper stems from the need to enable globally scaled Ambient Intelligence (AmI) by leveraging the creative potential of Generative AI (GenAI) and its ability to address existing technology gaps within 6G networks.

Method: The paper reviews foundational GenAI models (e.g., GANs, VAEs, diffusion models, and generative transformers), explores their application to AmI use cases (e.g., spectrum sharing and intelligent security), and examines 6G enablers like edge computing and IoT device swarms.

Result: The study provides insights into how GenAI can generate synthetic data, optimize communication, predict network conditions, and update digital twins, alongside discussing potential acceleration through 6G technologies like IRS and fog computing.

Conclusion: GenAI is positioned as a core technology for transforming 6G networks into ambient intelligent ecosystems, albeit with challenges in energy efficiency, privacy, and standardization that need addressing.

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [175] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: HeteroScale is an autoscaling framework that optimizes GPU usage for disaggregated architectures in Large Language Models, improving efficiency and service quality.


<details>
  <summary>Details</summary>
Motivation: Traditional autoscalers struggle with modern Prefill-Decode disaggregated architectures in LLM serving, leading to inefficiencies and imbalances in hardware utilization.

Method: HeteroScale utilizes a topology-aware scheduler and a metric-driven policy to adapt to heterogeneous hardware, network constraints, and autoscaling signals. It jointly scales prefill and decode pools for optimal resource management.

Result: In a large-scale production environment using tens of thousands of GPUs, HeteroScale improved GPU utilization by 26.6 percentage points and saved hundreds of thousands of GPU-hours daily.

Conclusion: HeteroScale addresses key challenges in LLM serving by providing efficient, balanced resource management, validating its practicality in large-scale deployments.

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [176] [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](https://arxiv.org/abs/2508.19670)
*Diogo Costa,Jose Martins,Sandro Pinto*

Main category: cs.DC

TL;DR: The paper studies performance interference caused by IOMMU structures in heterogeneous computing platforms and finds significant timing unpredictability issues, especially for small memory transactions.


<details>
  <summary>Details</summary>
Motivation: The evolution of Mixed Criticality Systems integrating heterogeneous architectures requires addressing security and timing predictability challenges, particularly due to memory access by independent bus masters.

Method: The paper analyzes the contention effects of IOMMU structures using experimental investigations on the Xilinx UltraScale+ ZCU104 platform and explores the impact on DMA transactions.

Result: Findings demonstrate unpredictable delays primarily in small memory transactions, with IOMMU interference delaying DMA transactions by up to 1.79x on the Arm SMMUv2 implementation.

Conclusion: Performance interference in IOMMU structures poses timing unpredictability issues in mixed criticality systems with potential implications across architectures.

Abstract: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

</details>


### [177] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: The paper studies the computational power of mobile robots and how various factors interact, introducing new problems and providing impossibility criteria.


<details>
  <summary>Details</summary>
Motivation: To explore how robot capabilities, light observability, and scheduler synchrony interact to solve computational problems in distributed systems.

Method: The authors analyze the solvability of specific problems (e.g., ETE, HET, TAR(d)*, VTR) in different robot models, characterized by memory, light observability, and synchrony.

Result: They classify various problems and demonstrate separations between 14 canonical robot models, uncovering structural phenomena during problem analysis.

Conclusion: The study deepens understanding of how observability, memory, and synchrony collectively influence mobile robots’ computational abilities, while introducing new criteria and extending known separations.

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


### [178] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: The paper introduces a novel framework that integrates digital twins with HPC scheduling, allowing pre-deployment analysis of scheduling decisions and system changes.


<details>
  <summary>Details</summary>
Motivation: Schedulers are crucial in optimizing resource utilization in HPC systems, but traditional evaluation methods lack integration of physical infrastructure and are limited in scope.

Method: The authors developed a digital twin framework integrated with scheduling capabilities, incorporated datasets from top-tier HPC systems, and implemented extensions for external scheduling simulators and machine learning evaluation.

Result: The framework facilitates what-if analysis for HPC systems, enabling the evaluation of incentive structures, ML-based scheduling, and sustainability impacts.

Conclusion: The study provides a pioneering meta-framework for prototyping HPC scheduling using digital twins, enhancing pre-deployment analysis and decision-making.

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


### [179] [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)
*Fahao Chen,Jie Wan,Peng Li,Zhou Su,Dongxiao Yu*

Main category: cs.DC

TL;DR: The paper introduces FLUX, a system to enable resource-efficient federated fine-tuning of Mixture-of-Experts (MoE)-based large language models, achieving up to 4.75X speedup in time-to-accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenges of federated fine-tuning of large MoE-based LLMs faced by participants with constrained computing resources, while achieving good performance.

Method: FLUX employs quantization-based local profiling, adaptive layer-aware expert merging, and dynamic expert role assignment with an exploration-exploitation strategy.

Result: FLUX demonstrated superior efficiency, achieving significantly faster time-to-accuracy (up to 4.75X) compared to existing methods, in experiments on MoE-based LLMs like LLaMA-MoE and DeepSeek-MoE.

Conclusion: FLUX addresses resource constraints and system performance challenges in federated fine-tuning of MoE-based LLMs, offering a practical and efficient solution for consumer-level computing environments.

Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models
(LLMs) is challenging due to their massive computational requirements and the
resource constraints of participants. Existing working attempts to fill this
gap through model quantization, computation offloading, or expert pruning.
However, they cannot achieve desired performance due to impractical system
assumptions and a lack of consideration for MoE-specific characteristics. In
this paper, we propose FLUX, a system designed to enable federated fine-tuning
of MoE-based LLMs across participants with constrained computing resources
(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX
introduces three key innovations: (1) quantization-based local profiling to
estimate expert activation with minimal overhead, (2) adaptive layer-aware
expert merging to reduce resource consumption while preserving accuracy, and
(3) dynamic expert role assignment using an exploration-exploitation strategy
to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE
and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX
significantly outperforms existing methods, achieving up to 4.75X speedup in
time-to-accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [180] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: This paper introduces a hybrid parameter estimation technique called "Physics-Informed Regression" (PIR) that uses regularized ordinary least squares for parameter estimation in nonlinear dynamic models. PIR is shown to outperform physics-informed neural networks (PINN) in terms of accuracy and computational efficiency on several ODE and PDE models, including real-world epidemic models.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the need for computationally efficient and reliable parameter estimation techniques in nonlinear dynamic models, particularly in contexts like epidemic models where accurate modeling is essential.

Method: The study proposes the "Physics-Informed Regression" (PIR) technique, which utilizes regularized ordinary least squares for parameter estimation in nonlinear models expressed in parameter-linear form. The approach is tested on both ordinary differential equations (ODE) and partial differential equations (PDE) and compared against the PINN technique.

Result: PIR demonstrated superior performance over PINN in both synthetic and real-world datasets, including Danish COVID-19 data, particularly excelling in complex models. It also offered faster computation times, making it more suitable for real-time applications.

Conclusion: PIR is an efficient and reliable technique for parameter estimation in nonlinear dynamic models, offering advantages over PINN in terms of accuracy and computational speed, thereby making it suitable for real-world and time-sensitive applications.

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [181] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: This paper extends neural network model compression techniques to lower-precision formats like FP8 and FP4, achieving significant compression rates.


<details>
  <summary>Details</summary>
Motivation: To reduce storage and transmission costs of neural network weights as the use of deep learning models scales.

Method: A compression technique independently handles exponent and mantissa components of lower-precision floating-point formats using entropy coding.

Result: Achieved up to 62% compression for BF16 and 83% for FP8, and identified compressibility in key-value caches for large language models.

Conclusion: The proposed method significantly reduces memory requirements for model storage and deployment while enabling efficient inference with lower-precision formats.

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [182] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: This paper introduces POT, a novel black-box attack framework that targets vulnerabilities in reasoning processes of Chain-of-Thought prompted LLMs, leveraging LLM-based iterative optimization to generate efficient adversarial prompts.


<details>
  <summary>Details</summary>
Motivation: Enhancing reasoning capabilities through Chain-of-Thought prompting in LLMs has led to computational inefficiency vulnerabilities, creating an attack surface for adversaries.

Method: The authors developed POT, an attack framework that uses iterative optimization within LLMs to create covert and semantically natural adversarial prompts without relying on external data or retrieval mechanisms.

Result: Experimental benchmarks reveal that POT performs better than existing methods across various model architectures and datasets.

Conclusion: POT provides a practical solution for exploiting computational inefficiencies in LLM reasoning capabilities, circumventing previous limitations in adversarial prompting methods.

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [183] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: The paper introduces a DRL-based framework enabling IoT devices to select channels based on real-world data feedback to optimize resource allocation.


<details>
  <summary>Details</summary>
Motivation: Limited research investigates DRL training using real-world data in distributed IoT systems, necessitating approaches that integrate practical feedback loops.

Method: IoT devices employ DRL for channel selection, leveraging feedback from Acknowledgment (ACK) information gathered during actual data transmission for training.

Result: Implementation and evaluation reveal improvements in Frame Success Rate (FSR), showcasing the framework's feasibility and efficacy.

Conclusion: The framework demonstrates the potential of DRL in enhancing resource allocation in IoT systems using practical data feedback.

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [184] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: This paper presents Re:Frame, a module that enhances offline RL performance by using a tiny amount of expert data combined with low-quality datasets to improve decision-making.


<details>
  <summary>Details</summary>
Motivation: Offline RL struggles with suboptimal and inconsistent data when expert datasets are scarce, hindering generalization and performance improvement.

Method: The method proposed is Re:Frame, which uses an external Associative Memory Buffer (AMB) with expert trajectories to augment a standard offline RL policy like Decision Transformer during training and evaluation.

Result: On D4RL MuJoCo benchmarks, Re:Frame showed consistent performance improvements in three out of four settings, achieving up to +10.7 normalized points with only 60 expert trajectories.

Conclusion: Re:Frame provides a simple yet effective approach for integrating limited expert knowledge into offline RL, enabling better utilization of low-quality datasets and achieving noticeable performance gains.

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [185] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: The paper introduces NCMemo, a framework to quantify label memorization in GNNs, uncovering its link to graph homophily and providing mitigation strategies via graph rewiring.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding regarding the memorization behavior of GNNs and its implications for graph learning and privacy.

Method: Established relationship between memorization and graph homophily, analyzed GNN training dynamics, and leveraged graph rewiring to mitigate memorization effects.

Result: Demonstrated that higher memorization occurs in low homophily graphs, with graph rewiring effectively reducing memorization and privacy risks without harming performance.

Conclusion: The study advances the understanding of GNN learning dynamics, highlights risks of memorization, and suggests graph rewiring as a tool for more privacy-focused GNN use.

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [186] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: The paper proposes an efficient data-augmentation strategy to train neural PDEs using space-filling sampling, reducing computational redundancy and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the training of neural PDEs by addressing the spatiotemporal redundancy in trajectory data traditionally used for training.

Method: The method involves generating training data through space-filling sampling of local 'stencil' states, which highlights rarely visited states and reduces redundancy.

Result: The proposed method enables the learning of accurate neural PDE stencil operators using limited synthetic training data (10 timesteps equivalent). Combining this with a single simulation trajectory improves accuracy further.

Conclusion: The data-augmentation approach improves the efficiency and performance of trained neural stencil operators across multiple PDE systems, outperforming naive sampling methods.

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [187] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: The paper presents a method utilizing Singular Value Decomposition (SVD) to enhance multi-source transfer learning by selecting and merging key components from multiple models, leading to efficient and precise knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Existing approaches in transfer learning fail to efficiently and precisely aggregate knowledge from numerous models available online, limiting adaptability and increasing re-training costs.

Method: The method uses Singular Value Decomposition (SVD) to decompose models into rank-one components and selects the most important components for aggregation, tuning only principal singular values during adaptation to the target task.

Result: The proposed framework achieves efficient transfer learning, robustness to input and parameter perturbations, and scalability when dealing with multiple or large models.

Conclusion: This SVD-based approach addresses key limitations in multi-source transfer learning, offering a precise, scalable, and efficient solution for knowledge extraction and adaptation.

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [188] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: This paper explores whether trained neural networks are surjective, meaning they can map any output to some input.


<details>
  <summary>Details</summary>
Motivation: To understand the implications of the surjectivity in generative models for safety and potential vulnerabilities.

Method: The paper theoretically examines the surjectivity of neural architectures, focusing on the structural aspects like pre-layer normalization and linear-attention modules.

Result: It is proven that many neural architectures, including GPT-style transformers and diffusion models, are almost always surjective.

Conclusion: Surjectivity in modern neural networks highlights their inherent vulnerability to adversarial manipulation.

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [189] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: This paper addresses the challenges of continuous stress monitoring by proposing the first comprehensive design of low-power, flexible machine learning-based stress classifiers, optimizing for real-time monitoring.


<details>
  <summary>Details</summary>
Motivation: Conventional methods and rigid wearables for stress monitoring are ineffective for continuous use, highlighting the need for lightweight, flexible, and cost-efficient solutions.

Method: The authors explored 1200+ flexible machine learning classifiers, considering feature selection, neural simplification, and designed hardware-efficient circuits using low-precision arithmetic.

Result: The proposed flexible classifiers achieved higher accuracy, low power consumption, low cost, compact size, and conformability compared to existing solutions.

Conclusion: This work advances stress-monitoring technology by demonstrating the promise of flexible electronics and machine learning in creating practical, real-time solutions.

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [190] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: This paper introduces the use of graphs and graph neural networks for chemical sciences, detailing their applications for representing and learning from chemical data.


<details>
  <summary>Details</summary>
Motivation: To explore how graphs, as a representation tool, and machine learning, specifically graph neural networks, can advance understanding and prediction in chemical sciences.

Method: The paper outlines the basics of graph-based modeling, key prediction tasks, examples from chemistry, and the integration of machine learning techniques, particularly graph neural networks.

Result: The study provides a foundational understanding of using graph methods, linking them to chemical sciences for diverse applications like molecular understanding and chemical process modeling.

Conclusion: Graph-based methods coupled with machine learning open avenues for enhanced exploration and discovery in chemical sciences, making them essential tools for the future of chemical research.

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [191] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: The study proposes a lightweight deep learning model for early prediction of atrial fibrillation (AF) using RR intervals, achieving high performance metrics and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Detecting early-stage progression like paroxysmal AF (PAF) to reduce the risk of sustained AF, mortality, and severe cardiovascular complications.

Method: A deep learning model combining Temporal Convolutional Network (TCN) for positional encoding and Mamba (state-space model) for efficient sequence modeling using RR intervals.

Result: Achieved sensitivity of 0.908, specificity of 0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. The model uses 73.5k parameters and 38.3 MFLOPs, outperforming CNN-RNN in accuracy and efficiency.

Conclusion: The model can predict AF up to two hours in advance using 30 minutes of input data, enabling timely preventive interventions to reduce disease progression.

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [192] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: The paper investigates the sample complexity required for successful membership-inference attacks, showing that attackers might need significantly more data than the training algorithm's dataset in certain cases.


<details>
  <summary>Details</summary>
Motivation: To understand how much information an attacker needs in membership-inference attacks, particularly through the lens of sample complexity, in a Gaussian mean estimation setup.

Method: The researchers analyze Gaussian mean estimation, where the learning algorithm estimates a mean from $n$ samples, and calculate the number of reference samples required for membership inference under specific error constraints.

Result: They demonstrate that in certain cases, an attacker may need $\Omega(n + n^2 \rho^2)$ samples, which can greatly exceed the training data size.

Conclusion: Current attacks typically limit their use to $O(n)$ samples and might underestimate membership inference risks. More effective attacks could be developed when distribution information is highly accessible.

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [193] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: The paper addresses the challenge of hallucinations in multimodal Large Language Models (LLMs) by developing a mathematically grounded framework to measure and understand their dynamics.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in multimodal LLMs pose risks in critical domains like medicine and law, and current methods of evaluation are heuristic and lack rigorous quantification.

Method: The authors introduce a diffusion dynamics framework that uses spectral embeddings over multimodal graph Laplacians, Rayleigh--Ritz bounds, and RKHS eigenmode decompositions to analyze hallucination energy over time.

Result: The framework enables mathematically rigorous metrics that characterize semantic distortions and track the dynamics of hallucinations across prompts and time using temperature profiles.

Conclusion: This work lays the groundwork for transforming hallucinations from being a qualitative issue into a tractable and analytically quantifiable phenomenon.

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [194] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: The paper explores using a fine-tuned Vision-Language Model (VLM) for classifying neutrino interactions in high-energy physics, comparing it to traditional CNN methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage advancements in large language models and multimodal reasoning for improving classification tasks in high-energy physics experiments.

Method: A Vision-Language Model fine-tuned on high-energy physics data is benchmarked against a CNN baseline, using metrics like accuracy, precision, recall, and AUC-ROC.

Result: The VLM outperforms or matches CNN performance, while also enabling richer reasoning and integration of auxiliary contextual information.

Conclusion: VLMs show great promise as a general-purpose tool for event classification in high-energy physics and can pave the way for multimodal approaches in neutrino physics experiments.

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [195] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) exhibit significant prediction sensitivity to irrelevant data representation changes, questioning their robustness for data-fitting tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate and highlight the growing use of LLMs in data-fitting applications, revealing their vulnerabilities regarding prediction robustness.

Method: The authors examine LLM behaviors under in-context learning and supervised fine-tuning approaches. They analyze attention scores and compare LLM robustness to a state-of-the-art tabular model (TabPFN).

Result: LLMs' predictions are highly sensitive to irrelevant variations like variable name changes, with errors increasing up to 82%. A non-uniform attention mechanism was identified as a contributing factor.

Conclusion: While LLMs show competitive predictive ability, their lack of robustness makes them unreliable as principled tools for data-fitting tasks.

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [196] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: This study explores two hybrid quantum-classical models, QMLP and QCNN, for malware classification, achieving high binary classification accuracy and varying success in multiclass tasks.


<details>
  <summary>Details</summary>
Motivation: To capitalize on the emerging capabilities of quantum computing by investigating its applicability to malware classification, a domain where classical machine learning has long dominated.

Method: The study examines two hybrid quantum-classical machine learning models: Quantum Multilayer Perceptron (QMLP) and Quantum Convolutional Neural Network (QCNN). These models encode features into quantum states using angle embedding and differ in their architecture; QMLP employs full qubit measurement and data re-uploading, while QCNN uses quantum convolution and pooling layers to reduce active qubits and speed up training. Performance was evaluated on binary and multiclass classification tasks across five malware datasets.

Result: QMLP and QCNN achieve high accuracy in binary classification: 95-96% on API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. Multiclass classification accuracy is more varied: 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class, and 60.7-88.1% on EMBER-Class. QMLP outperformed QCNN in complex multiclass tasks, while QCNN had faster training but lower accuracy.

Conclusion: Quantum machine learning holds potential for malware classification. QMLP is preferred for challenging multiclass tasks due to higher accuracy, while QCNN is efficient for simpler tasks due to faster training but with a trade-off in accuracy.

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [197] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: This paper introduces EUREKA, a method leveraging large language models to select non-obvious, interesting features for interpretable classifiers.


<details>
  <summary>Details</summary>
Motivation: To move beyond accuracy-driven models by exploring the value of using unexpected features for classification, aiming at knowledge discovery and interpretability.

Method: The framework ranks features based on their perceived interestingness using large language models and builds interpretable classifiers utilizing selected features.

Result: EUREKA identified predictive yet unconventional features, such as linking paper titles with colons to future citations, achieving meaningful accuracy across datasets.

Conclusion: Classifiers built with interesting features, despite moderate accuracy, promote novelty, interpretability, and can aid domains valuing unique insights over pure accuracy.

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [198] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: The paper introduces a Diffusion-Enhanced Transformer Neural Operator (DETNO) for long-term traffic forecasting, addressing the challenge of preserving high-frequency details like shock waves and density gradients.


<details>
  <summary>Details</summary>
Motivation: Standard neural operators smooth out high-frequency traffic phenomena and accumulate errors in long-term traffic forecasting, making current methods unsuitable for real-time traffic management.

Method: DETNO combines a transformer neural operator with cross-attention for super-resolution and diffusion-based refinement to iteratively denoise and reconstruct high-frequency traffic features.

Result: DETNO outperforms traditional and transformer-based neural operators on chaotic traffic datasets, excelling in extended rollout predictions and maintaining stability.

Conclusion: DETNO successfully addresses the limitations of smoothing and instability, offering a reliable solution for predicting high-frequency traffic phenomena over long horizons.

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [199] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: This paper introduces a hybrid quantum-classical model for SMILES string reconstruction, improving quantum fidelity to 84% and classical similarity to 60%, surpassing prior quantum baselines.


<details>
  <summary>Details</summary>
Motivation: Enhancing generative models in molecular design using quantum machine learning, particularly for sequence-based tasks like SMILES reconstruction, which lacks exploration and often experiences fidelity issues.

Method: The proposed hybrid model integrates quantum encoding with classical sequence modeling to achieve better performance in both quantum fidelity and classical similarity for sequence reconstruction tasks.

Result: Achieved approximately 84% in quantum fidelity and 60% in classical reconstruction similarity, outperforming existing quantum baselines in SMILES reconstruction tasks.

Conclusion: The study demonstrates the potential of hybrid quantum-classical approaches, providing a foundation for future developments in quantum-aware sequence models and applications in molecular and drug discovery.

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [200] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: The paper introduces Kolmogorov-Arnold Representation-based Hamiltonian Neural Networks (KAR-HNN) as an alternative to MLPs, improving stability and accuracy in solving physical problems.


<details>
  <summary>Details</summary>
Motivation: Existing Hamiltonian Neural Networks using MLPs are hypersensitive to hyperparameters and struggle with complex energy landscapes, necessitating a robust modeling approach.

Method: KAR-HNN replaces MLPs with univariate transformations for localized function approximations, ensuring symplectic form preservation and improved predictive stability.

Result: KAR-HNN demonstrates reduced energy drift and stability across benchmark problems, including spring-mass, pendulum, and multi-body systems.

Conclusion: KAR-HNN shows promise in accurately modeling complex physical processes while maintaining interpretability and consistency, particularly in high-dimensional systems with limited parameters.

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [201] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: The paper investigates a format-dependent reasoning failure in the Llama-3.1-8B-Instruct model, focusing on numerical comparisons where specific transformer mechanics and attention head specialization lead to errors.


<details>
  <summary>Details</summary>
Motivation: To understand errors in numerical reasoning within Llama-3.1-8B-Instruct and explore how format inconsistencies in input affect its computational mechanics.

Method: Systematic interventions to study attention head specialization, using Structured Activation Exploration (SAE) analysis, and applying partial repairs by reconfiguring attention heads.

Result: Discovered that even-indexed attention heads handle numerical comparisons, with sharp computational thresholds where 8+ even heads result in perfect repair, and 7 or fewer lead to failure. Mechanisms behind feature separation and re-entanglement in layers were unveiled.

Conclusion: The study reveals complex mechanics in transformers, suggests interpretability and efficiency opportunities, and shows how repair can be achieved using fewer attention heads while preserving functionality.

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [202] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: This paper introduces a physics-informed machine learning workflow combining multiphase flow simulation and CNNs to drastically reduce computational requirements for subsurface reservoir pressure predictions.


<details>
  <summary>Details</summary>
Motivation: Simulating subsurface reservoir pressure control is challenging due to geological heterogeneity, multiphase dynamics, and the computational expense of many high-fidelity simulations.

Method: The approach couples a fully differentiable multiphase flow simulator within the DPFEHM framework with a CNN. The CNN is trained using transfer learning from single-phase simulations to predict fluid extraction rates in multiphase scenarios.

Result: Their technique reduces the need for simulations from up to ten million to fewer than three thousand, maintaining high predictive accuracy using physics-informed training processes.

Conclusion: Transfer learning and physics-informed methods enable realistic, computationally efficient predictions of reservoir behavior, making this approach significantly practical for real-world applications.

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [203] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: The paper introduces an unsupervised contrastive learning method to cluster 43 cancer types using mutation data, achieving biologically meaningful clustering.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of tumorigenesis by clustering cancer types based on shared molecular traits beyond classical statistical approaches.

Method: The authors use a dual-view contrastive learning framework with TabNet encoders to process gene-level and chromosome-level mutation profiles, optimizing embeddings with NT-Xent loss.

Result: The learned embeddings accurately reflect biological clusters of cancer types, consistent with mutational processes and tissue origins.

Conclusion: This work pioneers the use of contrastive learning for cohort-level cancer clustering, providing a scalable and interpretable subtyping framework.

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [204] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: The paper proposes enhancing generative models with tensor decomposition to reduce costs while maintaining the quality of generated multidimensional data.


<details>
  <summary>Details</summary>
Motivation: Producing large and complex simulation datasets can be resource-intensive, especially for costly experiments.

Method: The approach uses tensor decomposition in generative models to generate smaller tensor factors instead of full tensors, reducing output size and model parameters.

Result: Experiments indicate that the generated data remains useful while significantly reducing costs.

Conclusion: Tensor decomposition enhances efficiency in generative models, particularly for multidimensional data generation tasks.

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [205] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: This paper addresses incentivized exploration in infinite-arm bandit problems, proposing algorithms with provable performance guarantees and validating through simulations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of incentivizing agents in multi-armed bandit settings with infinitely many arms, especially dealing with reward drift caused by incentives.

Method: The authors introduce algorithms that discretize the infinite arm space uniformly, ensuring sublinear cumulative regret and compensation. The regret bounds depend on the covering dimension of the metric space.

Result: The proposed algorithms achieve regret and compensation bounds of $\Tilde{O}(T^{d+1/d+2})$. Generalization to contextual bandits shows similar performance guarantees, confirmed through numerical simulations.

Conclusion: The study provides effective exploration strategies in challenging bandit settings with infinite arms, balancing regret and compensation and demonstrating applicability to contextual bandits.

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [206] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: DeepAtlas is a new algorithm designed to generate local embeddings of data to evaluate the manifold hypothesis and build generative models when the data meets this criterion.


<details>
  <summary>Details</summary>
Motivation: Manifold learning assumes that high-dimensional datasets lie on lower-dimensional manifolds, but current tools lack the ability to validate this assumption or focus on local maps, which are crucial for defining manifolds.

Method: DeepAtlas generates local embeddings of datasets, trains deep neural networks to map between these embeddings and the original data, and uses topological distortion to assess conformity to the manifold hypothesis and dimensionality.

Result: DeepAtlas effectively learns manifold structures for some datasets but reveals that many real-world datasets, like single-cell RNA-sequencing data, do not conform to the manifold hypothesis.

Conclusion: DeepAtlas advances manifold learning by enabling both validation of the manifold hypothesis and the construction of generative models, which may facilitate applying differential geometry tools to suitable datasets.

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [207] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: This paper proposes a new framework, SAFT, to address distribution shifts in tabular learning, showing significant improvement in robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Distribution shifts between training and testing data reduce the effectiveness of tabular learning methods, highlighting the need for a robust solution.

Method: The SAFT framework transforms traditional tabular learning into a continuous representation-generation task, incorporating mechanisms like embedding decorrelation, sample reweighting, suboptimal embedding averaging, and normalization-based alignment for robustness.

Result: Extensive experiments demonstrate that SAFT surpasses existing tabular learning methods in robustness, effectiveness, and generalization across real-world distribution shifts.

Conclusion: SAFT is a strong candidate for addressing distribution shifts in tabular learning, combining innovative techniques for superior performance in varied conditions.

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [208] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: EQUATE is a model fine-tuning framework for symbolic regression, designed to improve accuracy and robustness in low-data scenarios using a novel embedding optimization approach.


<details>
  <summary>Details</summary>
Motivation: Scientific discovery requires interpretable mathematical equations derived from observed data, but foundational models often fail to generalize in domain-specific low-data environments.

Method: EQUATE employs distillation, symbolic-numeric alignment, and evaluator-guided embedding optimization to convert discrete equation searches into continuous embedding space optimization.

Result: Across three benchmarks (Feynman, Strogatz, black-box datasets), EQUATE demonstrated superior accuracy, robustness, low complexity, and fast inference compared to existing methods.

Conclusion: EQUATE provides a practical and generalizable solution for symbolic regression, optimizing foundation models to perform effectively even in low-data regimes.

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [209] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: The paper introduces PoolFlip, a multi-agent environment, and Flip-PSRO, a reinforcement learning approach to improve cyber defense against stealthy adversaries.


<details>
  <summary>Details</summary>
Motivation: Current defensive strategies in the FlipIt framework are brittle and fail to adapt against evolving attacks, necessitating robust and adaptive learning-based solutions.

Method: The authors developed PoolFlip, a multi-agent gym environment, and Flip-PSRO, a MARL algorithm using population-based training combined with ownership-based utility functions.

Result: Empirical results demonstrate that Flip-PSRO defenders are twice as effective as baselines in generalizing against unseen heuristic attacks.

Conclusion: The proposed approach equips defenders with adaptability to unknown and potentially dynamic adversaries, ensuring improved resource control and performance optimization.

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [210] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: The paper proposes using Python programs refined by large language models (LLMs) to create game-playing agents, achieving competitive performance with less training and environment interaction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency and adaptability of game-playing agents by leveraging programmatic policy representations and self-evolving code.

Method: The approach uses Python programs as policy representations refined by LLMs, allowing agents to self-improve through execution traces and natural language feedback.

Result: Game-playing agents represented as Python programs exhibit performance on Atari games competitive with deep reinforcement learning methods, while requiring significantly fewer resources.

Conclusion: Programmatic policy representations refined with LLMs demonstrate potential for creating efficient and adaptable agents with long-horizon reasoning capabilities.

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [211] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: The paper introduces MobText-SISA, a machine-unlearning framework suitable for mobility platforms that efficiently supports GDPR compliance without retraining models from scratch.


<details>
  <summary>Details</summary>
Motivation: Mobility platforms need to comply with privacy laws like GDPR, which demand unlearning individual contributions without the inefficiency of retraining deep models entirely.

Method: The framework embeds numerical and linguistic features into a latent space, clusters samples for isolated training, and performs incremental retraining on affected shards during deletion requests.

Result: MobText-SISA maintains predictive accuracy and outperforms random sharding in error reduction and quicker convergence on real-world mobility data.

Conclusion: MobText-SISA is a scalable and effective solution for privacy-compliant analytics in urban mobility systems, ensuring accurate predictions and efficient compliance.

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [212] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: The paper introduces Bi-LoRA, a method that integrates SAM with LoRA to enhance fine-tuning of large pre-trained models, addressing SAM's memory overhead and improving generalization.


<details>
  <summary>Details</summary>
Motivation: The challenges of fine-tuning large-scale pre-trained models with limited data and high memory/computation requirements of Sharpness-Aware Minimization (SAM) when applied to such models.

Method: Bi-LoRA introduces a dual LoRA module design, where one optimizes task-specific performance and the other captures the sharpness of the loss landscape for achieving flatter minima, addressing SAM's computational inefficiency.

Result: Bi-LoRA demonstrates efficiency and effectiveness in improving generalization across diverse tasks and architectures, while being memory-efficient and reducing training costs.

Conclusion: Bi-LoRA successfully balances the generalization benefits of SAM with the efficiency of LoRA, providing a scalable and practical solution for fine-tuning large-scale pre-trained models.

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [213] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: The paper introduces a Counterfactual Reward Model for reinforcement learning with human feedback to mitigate biases, increasing fairness and improving policy optimization.


<details>
  <summary>Details</summary>
Motivation: To address the problem of latent biases in reward models used in reinforcement learning systems, which impact policy optimization and fairness.

Method: Developed a bias-resilient reward model leveraging causal inference and multimodal representation learning, featuring a Counterfactual Trust Score with four bias-reducing components.

Result: The proposed method demonstrated 89.12% accuracy in fake news detection and reduced spurious correlations and unfair reinforcement signals.

Conclusion: The approach increases reliability and fairness in real-time policy making by offering interpretable bias reduction and improved reinforcement learning with human feedback.

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [214] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: The paper/tutorial discusses synthetic data generation using generative models, its methodologies, evaluation, and applications in data mining.


<details>
  <summary>Details</summary>
Motivation: To address challenges of data scarcity, privacy, and annotation in data mining using synthetic data.

Method: Introduces foundational concepts, recent advances, methodologies, frameworks, and evaluation strategies for synthetic data generation.

Result: Provides attendees with actionable insights into using synthetic data in data mining research and applications.

Conclusion: Synthetic data generation is a scalable and impactful solution to data-related challenges in the data mining domain.

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [215] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: The paper introduces SyReM, a robust continual learning method for motion forecasting that balances memory stability and learning plasticity, mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the stability-plasticity dilemma in deep neural networks, which struggle to maintain prior knowledge while adapting to new data in motion forecasting tasks.

Method: The SyReM approach uses a compact memory buffer with an inequality constraint for memory stability and a selective memory rehearsal mechanism based on cosine similarity of loss gradients for enhanced learning plasticity.

Result: Experiments on 11 naturalistic driving datasets demonstrate that SyReM significantly reduces catastrophic forgetting, improving performance in both past and new scenarios compared to baseline methods.

Conclusion: SyReM successfully balances memory retention and adaptability in continual learning for motion forecasting, proving its effectiveness over existing approaches while ensuring public accessibility of its implementation.

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [216] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: The paper introduces Delta-Attribution, a model-agnostic framework to explain changes in model performance using per-feature attribution differences. Evaluated on classical machine learning families and datasets, the method reveals insights into the impact of model updates.


<details>
  <summary>Details</summary>
Motivation: Understand why changes in machine learning models (hyperparameters, kernels, depths, solvers, or data) result in different performances, often opaquely.

Method: Delta-Attribution measures differences in per-feature attributions between two model versions (A and B) using various metrics like magnitude, alignment, and robustness. This approach utilizes fast occlusion/clamping and evaluates changes across different classifiers and datasets.

Result: Inductive-bias changes cause significant, behavior-aligned attribution deltas, while cosmetic changes result in minimal deltas. The method uncovered specific impactful changes in models like SVC, Random Forests, and Gradient Boosting.

Conclusion: Delta-Attribution serves as a lightweight audit tool to differentiate benign model updates from meaningful or risky behavioral reliance shifts, complementing accuracy evaluation.

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [217] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: This paper introduces Dual-LS, a framework inspired by the human brain to address catastrophic forgetting in DNN-based vehicle motion forecasting, enabling more efficient continual learning for smart cities.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks used in smart city services struggle with catastrophic forgetting when updated, limiting their ability to learn continually like humans.

Method: The Dual-LS paradigm combines two memory rehearsal replay mechanisms to synergistically manage long-term and short-term knowledge, dynamically improving retrieval and adaptability.

Result: Tests show that Dual-LS mitigates knowledge loss by 74.31%, reduces computational demand by 94.02%, and enhances predictive stability in vehicle motion forecasting.

Conclusion: The approach bridges the gap between computational efficiency and human-like adaptability, making smart city services more sustainable and effective.

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [218] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: LLM agents often face challenges in optimizing their planning capabilities due to end-to-end training paradigms. RLTR framework decouples planning optimization and introduces tool-use reward signals to improve performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of imbalanced optimization and lack of verifiable training data in training LLM agents for better planning capabilities.

Method: Propose RLTR framework that focuses on single-objective optimization and incorporates tool-use completeness as reward signals for enhanced training.

Result: RLTR achieves an 8%-12% improvement in planning performance and a subsequent 5%-6% boost in overall response quality.

Conclusion: Decoupling the training process and using tool-use rewards significantly enhance planning and overall effectiveness in LLM agents.

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [219] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: The paper introduces FinCast, a foundation model for financial time-series forecasting, trained on large datasets to tackle challenges like temporal non-stationarity and multi-domain diversity, showcasing strong performance without domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Financial time-series forecasting is crucial for economic stability and sustainable investment but faces challenges such as shifting patterns due to temporal non-stationarity, multi-domain diversity, and varying resolutions.

Method: The study uses FinCast, a foundation model trained on large-scale financial datasets, designed to handle diverse financial patterns and complexities without overfitting or extensive fine-tuning.

Result: FinCast achieves superior zero-shot forecasting performance compared to state-of-the-art methods, demonstrating remarkable generalization across domains and temporal resolutions.

Conclusion: FinCast provides a robust solution to financial time-series forecasting challenges, showcasing strong adaptability and eliminating the need for domain-specific fine-tuning.

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [220] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: The paper introduces ALSA, a framework for estimating model accuracy on unlabeled datasets, focusing on using logit space information to improve robustness under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To improve model accuracy estimation for unseen, unlabeled datasets, especially under distribution shifts where performance typically degrades.

Method: The authors propose ALSA, which operates directly in logit space. It uses multiple learnable anchors with influence functions to capture variations in logits for performance estimation.

Result: Experiments across vision, language, and graph benchmarks confirm ALSA's superiority over existing softmax- and similarity-based approaches, especially in robustness under distribution shifts.

Conclusion: ALSA proves to be a robust and practical tool for model evaluation, preserving richer logit information while addressing performance degradation under distribution shifts.

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [221] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: The paper introduces pFedBayesPT, a fine-grained personalized federated learning framework addressing intra-client data heterogeneity using a Bayesian perspective and visual prompt tuning.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve personalized federated learning methods, which often assume homogeneous data distributions in each client, by addressing intra-client data heterogeneity where clients have diverse datasets from multiple sources.

Method: The authors proposed pFedBayesPT, leveraging Bayesian modeling to generate instance-specific prompts for visual semantics. They utilized implicit distributions for posterior modeling and trained the framework using semi-implicit variational inference.

Result: The proposed pFedBayesPT framework consistently outperformed existing personalized federated learning methods in benchmarks involving both feature and label heterogeneity.

Conclusion: pFedBayesPT offers a fine-grained pFL solution that significantly improves performance by addressing both inter-client and intra-client data heterogeneity challenges.

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [222] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: The paper introduces SCAR, a framework for analyzing the structural properties of datasets, and presents a strategy for efficient multimodal data expansion while maintaining generalization.


<details>
  <summary>Details</summary>
Motivation: There is a lack of understanding about how data properties beyond quantity and efficiency influence generalization in foundation models.

Method: SCAR measures dataset characteristics across Scale, Coverage, Authenticity, and Richness (SCAR). It identifies foundation data and develops a SCAR-based data completion strategy for expanding multimodal datasets.

Result: SCAR predicts data utility, preserves generalization behavior, and guides data acquisition effectively across various multimodal datasets and model architectures.

Conclusion: SCAR offers a principled, invariant framework for understanding and optimizing datasets, enabling more effective and generalizable data-centric strategies.

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [223] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: The paper investigates how rational functions and rational neural networks can approximate regular functions in the $\mathcal{C}^1$-norm and provides approximation rates.


<details>
  <summary>Details</summary>
Motivation: To explore the capability and efficiency of rational functions and rational neural networks in approximating regular functions, particularly for applications like symbolic regression in physical law learning.

Method: The authors analyze rational neural networks and assess their approximation rates in the $\mathcal{C}^1$-norm with respect to their width, depth, and rational function degree. They study specific architectures, such as $\text{EQL}^\div$ and ParFam.

Result: The study establishes $\mathcal{C}^1$-approximation rates for rational neural networks, confirming their effectiveness for architectures like $\text{EQL}^\div$ and ParFam.

Conclusion: Rational neural networks, including $\text{EQL}^\div$ and ParFam, are effective for $\mathcal{C}^1$-approximation, making them valuable for tasks like symbolic regression in physical law learning.

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [224] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: The paper defines and analyzes a weighted metric on graph walks that enables measuring distances and proximities, with applications like Lipschitz regression and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To establish a metric structure that facilitates comparing and analyzing walks on graphs, which include sequences of vertices connected through edges.

Method: A weighted metric is defined to calculate distances based on stepwise vertex distances and norms. Proximity measures and their representations are also studied.

Result: Properties of the proposed metric space and proximities are established. Explicit constructions and formulas for proximities under various conditions are provided.

Conclusion: The proposed metric framework can be used for extending proximity functions and enables applications like Lipschitz regression and reinforcement learning on graph structures.

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [225] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: This study introduces Adam-PFN, a surrogate model for Freeze-thaw Bayesian Optimization of Adam optimizer hyperparameters, leveraging pre-trained learning curves and a learning curve augmentation method (CDF-augment) to enhance tuning efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency of hyperparameter tuning for the Adam optimizer, which, while crucial for performance, is often expensive and time-consuming. Current surrogates lack specific knowledge about Adam's learning behavior.

Method: The authors propose Adam-PFN, a pre-trained surrogate model designed for Freeze-thaw Bayesian Optimization, paired with a CDF-augment method. These are based on learning curves from TaskSet datasets and aim to better predict and utilize learning behavior.

Result: Adam-PFN, combined with CDF-augment, improves learning curve extrapolation and accelerates hyperparameter optimization for evaluation tasks on TaskSet. It also exhibits strong performance on out-of-distribution tasks.

Conclusion: The approach offers a more efficient and effective method for tuning the Adam optimizer hyperparameters, enhancing optimization performance with broader applicability beyond in-distribution datasets.

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [226] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: This paper introduces InfraredGP, a graph partitioning approach that leverages low-frequency information beyond the conventional range with a negative correction mechanism. It achieves efficient and competitive results without training by using a spectral GNN and clustering algorithms like BIRCH.


<details>
  <summary>Details</summary>
Motivation: The study seeks to explore whether low-frequency graph information outside the conventional range $[0, 2]$ can better encode community structures and improve graph partitioning results.

Method: The proposed method, InfraredGP, employs a spectral GNN with low-pass filters and a negative correction mechanism. It uses random inputs, derives embeddings through a single feed-forward propagation step without training, and applies clustering algorithms like BIRCH to these embeddings to achieve graph partitioning.

Result: InfraredGP achieves high-quality graph partitioning results, competitive with baseline methods, while being significantly faster (16x-23x) than other techniques. It was evaluated on both static and streaming graph partitioning tasks following the IEEE HPEC Graph Challenge benchmark.

Conclusion: InfraredGP demonstrates that leveraging low-frequency information beyond the conventional range, combined with a simple and efficient mechanism, can produce effective graph partitioning results without requiring any training.

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [227] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: The paper introduces a novel 3D diffusion-based pipeline for efficient simulation of granular media, significantly reducing computational time during initialization.


<details>
  <summary>Details</summary>
Motivation: To address the computational bottleneck in simulating granular media, especially during the initialization phase with large displacements and kinetic energy.

Method: A two-stage generative pipeline using 3D diffusion models: 1) A diffusion model generates independent 3D voxel grids, and 2) an inpainting model stitches these grids seamlessly using masked inputs and reintroduces noise schedules for guidance.

Result: The pipeline achieves linear scaling in computational time and synthesizes a 1.2 m ballasted rail track in under 20 seconds, which would normally require 3-hour DEM simulation.

Conclusion: The proposed method enables real-time, scalable generation of physically realistic granular media, offering significant potential for industrial applications and DEM compatibility.

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [228] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: The paper proposes PSO-Merging, a data-driven strategy using Particle Swarm Optimization to merge multitask models efficiently, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies of existing model-merging methods, which are either computationally expensive (gradient-based) or ineffective in limited optimization steps (gradient-free).

Method: The method involves initializing a particle swarm with pre-trained, expert, and sparsified models, followed by iterations with PSO to achieve the best-merged model.

Result: Experiments showed that PSO-Merging consistently outperforms baseline methods in merging different language models.

Conclusion: PSO-Merging is a more efficient and scalable approach to model merging, addressing the shortcomings of previous methods.

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [229] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: This paper presents a new symplectic convolutional neural network (CNN) architecture that ensures symplectic properties for convolution and pooling layers, demonstrating superior performance on physics-based tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and performance of neural networks in preserving symplectic properties crucial for physics-based simulations.

Method: The authors introduce a novel symplectic CNN architecture by reformulating the convolution layer mathematically, parameterizing it via symplectic neural networks, and implementing symplectic pooling for a complete autoencoder framework.

Result: The symplectic CNN showed improved performance in numerical experiments on three equations: the wave equation, nonlinear Schrödinger equation, and sine-Gordon equation, outperforming conventional methods.

Conclusion: The proposed symplectic CNN architecture effectively preserves symplectic structures, providing better performance for physics-related computational tasks.

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [230] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: The paper introduces a hybrid framework combining finite element methods (FEM) with physics-informed DeepONet to efficiently model fluid transport in porous media influenced by sharp Gaussian sources, ensuring accuracy and significant computational speedups.


<details>
  <summary>Details</summary>
Motivation: Modeling fluid transport in porous media is computationally expensive when sharp, localized sources create steep gradients. The authors aim to preserve accuracy while enabling faster computations for practical applications.

Method: The framework solves Darcy flow via FEM to obtain velocity fields, then uses physics-informed DeepONet to learn mappings from source functions to solute concentration profiles. An adaptive sampling strategy is employed to manage sharp gradients.

Result: Numerical experiments show agreement between the proposed method and reference solutions, including order-of-magnitude improvements in computational speed.

Conclusion: The framework effectively balances accuracy and speed, making it viable for practical problems involving fluid transport in porous media. Code is made available for reproducibility.

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [231] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: The paper explores the use of quantum latent distributions in generative models, demonstrating both theoretical and empirical advantages over classical distributions in specific scenarios.


<details>
  <summary>Details</summary>
Motivation: To investigate when and why quantum latent distributions can improve the performance of generative models over classical latent distributions.

Method: The study combines theoretical analysis with benchmarking experiments on datasets (synthetic quantum and QM9) using simulated and real photonic quantum processors, applied to GANs and other generative model architectures.

Result: Quantum latent distributions outperformed classical baselines in generative tasks, with theoretical proofs backing these results under certain conditions. Compatible architectures for diffusion and flow matching models were also identified.

Conclusion: Quantum processors can provide a tangible advantage for deep generative models, expanding their capabilities in near-term applications.

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [232] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: This paper introduces SDGNN, a parameter-free graph neural network framework inspired by structural diversity theory. It addresses challenges like over-smoothing and semantic degradation, outperforming mainstream GNNs in various scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address over-smoothing of node representations and semantic degradation caused by GNNs relying on a large number of parameters and fixed aggregation rules.

Method: The authors employed a parameter-free framework, leveraging structural diversity theory, combining structure-driven and feature-driven modeling, and introducing a unified structural-diversity message passing mechanism.

Result: SDGNN outperformed mainstream GNNs across eight benchmark datasets and a citation network under conditions like low supervision, class imbalance, and cross-domain transfer.

Conclusion: SDGNN enhances adaptability and performance in diverse datasets and scenarios, offering a theoretical foundation for parameter-free graph neural networks and validating structural diversity as a key signal in graph representation learning.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [233] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: This paper introduces NM-Hebb, a training framework combining biological local plasticity and metric-based fine-tuning, improving CNN accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Deep CNNs suffer from issues like overfitting, redundant filters, and reduced interpretability due to reliance on global gradient-based optimization.

Method: Two-phase training: (1) combines cross-entropy loss with Hebbian regularization and neuromodulator for structured learning and parameter consolidation, (2) applies metric-learning for feature clustering in the embedding space.

Result: Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet with five CNN architectures, NM-Hebb improves Top-1 accuracy by 2-10 pp and NMI by up to 0.15.

Conclusion: NM-Hebb enhances accuracy and interpretability of CNNs, making them suitable for resource-limited and safety-critical AI applications.

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [234] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: The paper introduces ASPC, a framework to adaptively balance RL and BC, avoiding intensive hyperparameter tuning across datasets.


<details>
  <summary>Details</summary>
Motivation: Existing offline RL methods require extensive hyperparameter tuning for each dataset and struggle with scaling policy constraints.

Method: The ASPC framework dynamically scales policy constraints using second-order differentiable methods to balance RL and BC during training.

Result: ASPC outperforms state-of-the-art offline RL methods across diverse datasets using a single hyperparameter configuration and minimal computational cost.

Conclusion: ASPC is a robust and efficient solution for offline RL, eliminating the need for dataset-specific hyperparameter tuning and achieving superior results.

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [235] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: The paper proposes GegenNet, a novel spectral convolutional neural network tailored for link sign prediction in signed bipartite graphs, showcasing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for link sign prediction predominantly focus on unipartite signed graphs, neglecting the unique characteristics of signed bipartite graphs (SBGs), which limits their applicability and predictive accuracy.

Method: The paper introduces GegenNet, leveraging enhanced spectral graph processing through advanced node feature initialization, the application of Gegenbauer polynomial-based spectral graph filters, and multi-layer sign-aware convolutional networks featuring positive and negative edge alternation.

Result: GegenNet delivers superior predictive performance, with improvements of up to 4.28% in AUC and 11.69% in F1 scores, outperforming 11 established baseline models across 6 widely used SBG datasets.

Conclusion: GegenNet demonstrates the importance of adapting spectral convolution techniques specifically to SBGs and establishes a new benchmark in link sign prediction tasks for these graphs, highlighting its technical innovations as key to its success.

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [236] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: This paper introduces an ontology-driven method for improving similarity comparison of radiology reports and rare disease detection in medical imaging, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Most current approaches for retrieval-augmented learning rely on opaque, high-dimensional embeddings which are computationally inefficient and misaligned with the structured medical domain.

Method: The proposed method employs UMLS-based ontology-driven representations of medical entities extracted using RadGraph-XL and SapBERT, combined with a weighted Tversky Index for semantic similarity comparison.

Result: The novel approach surpasses state-of-the-art embedding methods in classification tasks using MIMIC-CXR, particularly for rare diseases, while generating valuable disease labels.

Conclusion: This work enhances clinical AI systems by offering interpretable and domain-aware retrieval strategies, and makes its pipeline publicly available for broader use and improvement.

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [237] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: FlowletFormer, a BERT-based framework, excels in capturing traffic behaviors through innovative traffic segmentation and protocol alignment techniques.


<details>
  <summary>Details</summary>
Motivation: Existing traffic classification methods fail to adequately capture structural and contextual network traffic features.

Method: FlowletFormer integrates a behavior-aware traffic representation model, protocol stack alignment embeddings, and context-aware pretraining tasks.

Result: FlowletFormer achieves superior traffic classification accuracy, effective representations, and enhanced few-shot learning.

Conclusion: FlowletFormer offers a robust framework that integrates domain-specific knowledge for trustworthy network traffic analysis.

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [238] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: This paper presents a method to infer constraints from Nash equilibrium interactions using a novel inverse dynamic game framework, enabling advanced motion planning.


<details>
  <summary>Details</summary>
Motivation: Understanding constraints in multi-agent interactions is crucial for designing effective and safe collaborative systems.

Method: They designed mixed-integer linear programming (MILP) encoding Karush-Kuhn-Tucker (KKT) conditions to learn constraints from interaction demonstrations.

Result: The proposed method successfully infers constraints and applies them to motion planning across simulations and hardware tests for nonlinear multi-agent dynamics.

Conclusion: The approach is effective for both convex and non-convex constraints and can robustly support safe motion plans using learned interaction constraints from Nash equilibrium data.

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [239] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijeet Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: This paper proposes Global Permutation Entropy (GPE), an enhancement to the widely used Permutation Entropy (PE) for time series complexity analysis. GPE expands analysis by considering all possible patterns, not just consecutive ones.


<details>
  <summary>Details</summary>
Motivation: Standard Permutation Entropy is limited to consecutive patterns in evaluating time series complexity, which may miss structural information in the data.

Method: The paper introduces and computes GPE using algorithms designed to extract full permutation profiles efficiently. It contrasts GPE with PE using synthetic dataset experiments.

Result: GPE effectively reveals structural insights in datasets that standard PE cannot detect, thereby showcasing its added utility in complexity measurement.

Conclusion: Global Permutation Entropy broadens time series complexity analysis by integrating non-consecutive patterns and demonstrates practical viability via an accessible Julia package released with the study.

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [240] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: The paper proposes a machine learning framework for predicting short-term faults in centrifugal pumps using sensor data, evaluating how prediction performance varies with history length and prediction horizons.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and interpretable predictive maintenance system for industrial monitoring, reducing downtime and fault costs.

Method: Used two lookback periods with a sliding window, statistical feature extraction, SMOTE for class imbalance, and trained Random Forest and XGBoost classifiers on sensor data to predict faults 5–30 minutes ahead.

Result: Random Forest achieved best recall of 69.2% (5 mins), 64.9% (15 mins), 48.6% (30 mins) with 60-minute data. Extended history (120 minutes) improved accuracy for longer predictions.

Conclusion: Optimal prediction depends on tailored history lengths for each horizon, offering a scalable solution for real-time fault forecasting in equipment.

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [241] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: This paper evaluates different strategies for real-time mobile assistants to optimize street parking search, demonstrating the effectiveness of a new strategy (Cord-Approx) in reducing search time.


<details>
  <summary>Details</summary>
Motivation: To address the issue of urban traffic congestion caused by prolonged street parking searches by quantifying the efficiency of real-time mobile assistants.

Method: The study performs data-driven simulations of Madrid's parking system to analyze four strategies: uncoordinated search, coordinated parking ignoring non-users, an idealized oracle approach, and a novel probabilistic method (Cord-Approx) that predicts non-user behavior and assigns parking accordingly using Hungarian matching.

Result: In simulations of Madrid, the Cord-Approx strategy reduced parking search time to an average of 6.69 minutes, compared to 19.98 minutes for non-users, with a reduction of up to 73% in some areas.

Conclusion: The Cord-Approx strategy is an effective and practical solution for reducing the time spent searching for parking, benefiting urban mobility and potentially alleviating traffic congestion.

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [242] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: The paper develops 'PasswordEval,' a benchmark to evaluate if language models can handle context-dependent safety scenarios like password verification. Results reveal limitations in current models in safeguarding confidential information.


<details>
  <summary>Details</summary>
Motivation: Ensuring language model safety is critical as they are increasingly used in high-risk applications requiring adherence to strict rules and specifications.

Method: Introducing 'PasswordEval,' a benchmark testing models' ability to verify user requests based on correct passwords and analyzing performance under adversarial conditions and multi-turn conversations.

Result: Current open- and closed-source language models fail to reliably handle password-based authorization. Reasoning capabilities leak sensitive data, worsening the situation.

Conclusion: Existing models struggle to ensure confidentiality and handle complex reasoning traces. New training approaches are necessary for safer deployment in high-stakes scenarios.

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [243] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: This paper introduces a self-supervised pre-training method for heterogeneous data using a bilevel optimization with equilibrium constraints, improving model adaptivity for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional self-supervised pre-training with heterogeneous data.

Method: Use bilevel optimization with equilibrium constraints to optimize towards local optima for each data source.

Result: Enhanced adaptivity and performance in downstream supervised tasks on multi-domain and multilingual datasets.

Conclusion: The proposed method ensures better representation learning for heterogeneous data, outperforming traditional approaches.

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [244] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: A new algorithm for selecting demonstration examples for in-context learning enhances model efficiency by using gradient-based influence scores, leading to improved performance and scalability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of selecting the most effective demonstration examples for optimizing in-context learning. This problem is critical for applications in prompt tuning and chain-of-thought reasoning.

Method: The authors propose a novel algorithm that uses gradients taken in the input embedding space to form influence scores for each demonstration. By applying first-order approximations and evaluating multiple subsets, the gradient-based approach allows for linear-time computations.

Result: Experiments show that the gradient estimation provides highly accurate predictions (less than 1% error) across six datasets and scales subset selection by up to 37.7x on large models. It also outperforms existing methods by 11% on average.

Conclusion: The proposed gradient-driven selection method is efficient, scalable, and robust, making it an improvement over previous token embedding-based methods for in-context learning.

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [245] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: This paper proposes a multimodal hierarchical classification framework addressing challenges in e-commerce product categorization. It combines text and visual features to improve accuracy and introduces a recategorization mechanism to handle taxonomy limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in solving critical industrial challenges in e-commerce product categorization caused by platform heterogeneity and structural limitations in existing taxonomies.

Method: The paper develops a multimodal framework incorporating RoBERTa, ViT, CLIP, dynamic masking, and multimodal fusion strategies within a hierarchical architecture. A recategorization pipeline with SimCLR, UMAP, and cascade clustering was also introduced.

Result: The framework achieved a hierarchical F1 of 98.59% using MLP-based late fusion with CLIP embeddings. It also identified fine-grained categories with over 86% cluster purity and demonstrated trade-offs in performance across platforms.

Conclusion: The proposed framework enhances categorization accuracy, discovers new categories, and demonstrates scalability in industrial deployment with a cost-effective inference pipeline.

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [246] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: The paper examines the phenomenon where fine-tuning LLMs on harmful datasets leads to a misalignment with human values and proposes a framework to detect and quantify behavioral transitions.


<details>
  <summary>Details</summary>
Motivation: To address the emergent misalignment in LLMs' behavior caused by fine-tuning on harmful datasets and understand the mechanisms behind this misalignment.

Method: A framework using distributional change detection and order parameters in plain English evaluated by an LLM judge is developed, along with statistical measures to analyze behavioral phase transitions during fine-tuning.

Result: The study decomposes the effects of fine-tuning into alignment, verbosity, and other aspects, and finds that behavioral transition occurs later than gradient norm peaks suggest.

Conclusion: The proposed framework advances understanding and detection of LLM behavioral transitions and can uncover language-driven order parameters across different domains.

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [247] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Introducing Symphony, a decentralized multi-agent system leveraging lightweight LLMs for scalable coordination.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of centralized LLM-based frameworks, such as high costs and rigid communication.

Method: Key mechanisms: decentralized ledger, Beacon-selection protocol for task allocation, and weighted result voting based on CoTs.

Result: Symphony exhibited superior reasoning benchmark accuracy and robustness across various model capacities.

Conclusion: Symphony offers scalable, fault-tolerant orchestration with significant improvements over existing frameworks.

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [248] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: FairLoop is a tool for mitigating bias in machine learning models by enabling human-guided adjustments using decision trees distilled from neural networks.


<details>
  <summary>Details</summary>
Motivation: To address the unfair predictions caused by sensitive attributes (e.g., gender or age) in machine learning tasks like predictive business process monitoring.

Method: FairLoop leverages decision trees distilled from neural networks to allow human users to inspect and modify unfair decision logic, which is then used to fine-tune the model.

Result: Through FairLoop, context-aware bias removal is achieved, offering selective and human-guided adjustments to mitigate the influence of sensitive attributes.

Conclusion: FairLoop represents an innovative approach to achieving fairer machine learning outcomes by blending human intervention with model fine-tuning and context-awareness.

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [249] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: This paper explores using large language models (LLMs) to generate more engaging titles for personalized e-commerce emails, aiming to improve customer engagement.


<details>
  <summary>Details</summary>
Motivation: E-commerce email titles often rely on fixed templates that fail to capture user interest, limiting the effectiveness of personalized marketing.

Method: The authors use large language models (LLMs) to generate thematic and dynamic email titles that better align with the personalized content of emails.

Result: Offline simulations and online experiments with millions of users show that LLM-generated titles improve engagement with personalized emails.

Conclusion: The application of LLMs for generating email titles demonstrates the potential to enhance customer interaction and automate this process at scale safely.

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [250] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: This paper investigates methods to mitigate backdoor attacks in language models by employing six attention-head pruning strategies, with experimental results demonstrating effectiveness against different types of triggers.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the vulnerability of pre-trained language models to backdoor attacks that persist even after fine-tuning, highlighting the threat these attacks pose to model integrity and the lack of effective defenses.

Method: Six pruning-based strategies, including gradient-based, layer-wise variance, structured sparsification, randomized ensemble, reinforcement-learning-guided, and Bayesian uncertainty pruning, are proposed and evaluated for iteratively removing less informative attention heads while monitoring validation accuracy.

Result: Gradient-based pruning excelled at defending against syntactic triggers, while reinforcement learning and Bayesian uncertainty pruning were more effective against stylistic attacks.

Conclusion: Pruning methods, particularly gradient-based and Bayesian/reinforcement-learning methods, can effectively mitigate backdoor threats in language models, even without trigger knowledge or a clean reference model.

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [251] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: This paper improves the performance of the Failure-Directed Search (FDS) algorithm in scheduling problems by integrating Multi-armed Bandit reinforcement learning methods, achieving significant advancements in speed and solution quality.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the FDS algorithm, a key tool in Constraint Programming for scheduling, by leveraging its connection to the Multi-armed Bandit problem and improving its efficiency and effectiveness on scheduling tasks.

Method: The paper applies Multi-armed Bandit algorithms to guide FDS branching decisions, refines the approach with problem-specific customizations, and performs extensive evaluations on the Job Shop Scheduling Problem (JSSP) and Resource-Constrained Project Scheduling Problem (RCPSP) benchmarks.

Result: The enhanced FDS algorithm is 1.7-2.1 times faster on benchmark scheduling problems compared to the original FDS, and outperforms state-of-the-art algorithms significantly. It also sets new lower bounds for 78 JSSP and 226 RCPSP instances, even closing some open benchmarks.

Conclusion: The integration of Multi-armed Bandit methods into FDS leads to substantial gains in solving challenging scheduling problems, making the approach a promising direction for future advancements in Constraint Programming.

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [252] [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](https://arxiv.org/abs/2508.19548)
*Madhuvanthi Srivatsav R,Chiranjib Bhattacharyya,Shantanu Chakrabartty,Chetan Singh Thakur*

Main category: cs.NE

TL;DR: This paper introduces a processing-in-Interconnect (π²) paradigm to enhance energy efficiency and scalability in neuromorphic computing by leveraging interconnect trends.


<details>
  <summary>Details</summary>
Motivation: To address bottlenecks in energy consumption and performance posed by routing, switching, and interconnect systems in large-scale AI workloads.

Method: The paper maps AI workload requirements onto primitives implemented in existing interconnect hardware and uses knowledge-distillation frameworks to train and adapt neural networks. Analytical modeling is employed to compare scaling advantages.

Result: The proposed π² architecture demonstrates potential scalability in energy efficiency with improved interconnect technology, suggesting reduced power consumption for brain-scale AI workloads.

Conclusion: The π² paradigm can enable efficient and scalable neuromorphic computing, making it suitable for executing large AI workloads with lower energy costs compared to traditional architectures.

Abstract: Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

</details>


### [253] [Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks](https://arxiv.org/abs/2508.19920)
*Matthew Meek,Guy Tallent,Thomas Breimer,James Gaskell,Abhay Kashyap,Atharv Tekurkar,Jonathan Fischman,Luodi Wang,Viet-Dung Nguyen,John Rieffel*

Main category: cs.NE

TL;DR: The paper investigates the use of spiking neural networks (SNN) to enable morphological communication in simulated soft robots within the EvoGym environment.


<details>
  <summary>Details</summary>
Motivation: The study aims to leverage nonlinear dynamic coupling for robot control, exploring how dynamical coupling can act as a data bus for communication among controller modules.

Method: The researchers utilized evolutionary learning models to evolve spiking neural networks (SNN) as control mechanisms for simulated soft robots, assessing their ability to enable morphological communication.

Result: The study demonstrated the emergence of morphological communication within soft robots controlled by evolved spiking neural networks in the EvoGym simulation environment.

Conclusion: Evolving spiking neural networks can successfully exploit dynamic coupling for coordinating control in non-rigid robots, opening up new possibilities in robot design and communication frameworks.

Abstract: Recently, researchers have explored control methods that embrace nonlinear
dynamic coupling instead of suppressing it. Such designs leverage dynamical
coupling for communication between different parts of the robot. Morphological
communication refers to when those dynamics can be used as an emergent data bus
to facilitate coordination among independent controller modules within the same
robot. Previous research with tensegrity-based robot designs has shown that
evolutionary learning models that evolve spiking neural networks (SNN) as robot
control mechanisms are effective for controlling non-rigid robots. Our own
research explores the emergence of morphological communication in an SNN-based
simulated soft robot in theEvoGym environment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [254] [Inference of Human-derived Specifications of Object Placement via Demonstration](https://arxiv.org/abs/2508.19367)
*Alex Cuellar,Ho Chit Siu,Julie A Shah*

Main category: cs.RO

TL;DR: The paper introduces positionally-augmented RCC (PARCC), a framework based on region connection calculus, to improve robotic understanding of human object arrangement preferences, supported by an inference algorithm and human study results.


<details>
  <summary>Details</summary>
Motivation: There is a need to enhance robotic systems' ability to comprehend human-acceptable spatial object configurations in tasks like sorting and packing, which current methods fail to handle expressively.

Method: The authors propose PARCC, a framework extending RCC for object spatial positioning, along with an inference algorithm that learns these configurations from human demonstrations.

Result: A conducted human study illustrates that PARCC effectively captures human-intended spatial specifications and highlights the advantages of learning from demonstrations compared to human-provided specifications.

Conclusion: PARCC enhances robotic systems' understanding of human-preferred spatial arrangements, proving more effective than manual specification approaches through demonstrations.

Abstract: As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,
object packing, sorting, and kitting), methods focused on understanding
human-acceptable object configurations remain limited expressively with regard
to capturing spatial relationships important to humans. To advance robotic
understanding of human rules for object arrangement, we introduce
positionally-augmented RCC (PARCC), a formal logic framework based on region
connection calculus (RCC) for describing the relative position of objects in
space. Additionally, we introduce an inference algorithm for learning PARCC
specifications via demonstrations. Finally, we present the results from a human
study, which demonstrate our framework's ability to capture a human's intended
specification and the benefits of learning from demonstration approaches over
human-provided specifications.

</details>


### [255] [FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain](https://arxiv.org/abs/2508.19380)
*Diancheng Li,Nia Ralston,Bastiaan Hagen,Phoebe Tan,Matthew A. Robertson*

Main category: cs.RO

TL;DR: The paper proposes a novel underactuated robot called FlipWalker, drawing inspiration from Jacob's Ladder toy, capable of flipping to traverse tough terrains.


<details>
  <summary>Details</summary>
Motivation: Develop a robot capable of overcoming terrain challenges where conventional wheeled robots fall short.

Method: Inspired by the Jacob's Ladder toy, FlipWalker consists of two interconnected segments, using motor-driven legs and flipping dynamics for locomotion.

Result: The prototype weighs 0.78 kg, achieves a flipping speed of 0.2 body lengths per second, and successfully navigates terrains like artificial grass, river rocks, and snow.

Conclusion: FlipWalker demonstrates the viability of flipping-based locomotion for challenging outdoor environments, offering a new alternative to traditional methods.

Abstract: This paper introduces FlipWalker, a novel underactuated robot locomotion
system inspired by Jacob's Ladder illusion toy, designed to traverse
challenging terrains where wheeled robots often struggle. Like the Jacob's
Ladder toy, FlipWalker features two interconnected segments joined by flexible
cables, enabling it to pivot and flip around singularities in a manner
reminiscent of the toy's cascading motion. Actuation is provided by
motor-driven legs within each segment that push off either the ground or the
opposing segment, depending on the robot's current configuration. A
physics-based model of the underactuated flipping dynamics is formulated to
elucidate the critical design parameters governing forward motion and obstacle
clearance or climbing. The untethered prototype weighs 0.78 kg, achieves a
maximum flipping speed of 0.2 body lengths per second. Experimental trials on
artificial grass, river rocks, and snow demonstrate that FlipWalker's flipping
strategy, which relies on ground reaction forces applied normal to the surface,
offers a promising alternative to traditional locomotion for navigating
irregular outdoor terrain.

</details>


### [256] [LaVA-Man: Learning Visual Action Representations for Robot Manipulation](https://arxiv.org/abs/2508.19391)
*Chaoran Zhu,Hengyi Wang,Yik Lung Pang,Changjae Oh*

Main category: cs.RO

TL;DR: This paper proposes a self-supervised method to enhance visual-textual association learning for robot manipulation, bypassing traditional action supervision and achieving superior performance on manipulation benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited precision in manipulation tasks caused by weak correlations between visual observations and textual instructions in existing methods.

Method: A self-supervised pretext task that involves reconstructing a masked goal image using input images and textual instructions, enabling visual-action representation learning without robot action supervision. Additionally, a new dataset, Omni-Object Pick-and-Place, is introduced.

Result: The method achieves superior performance on five benchmarks, including both simulated and real-robot validations, demonstrating improved precision and generalization in manipulation tasks.

Conclusion: The self-supervised approach and the newly introduced dataset significantly enhance visual-textual associations, enabling robots to perform complex manipulation tasks effectively with minimal demonstrations.

Abstract: Visual-textual understanding is essential for language-guided robot
manipulation. Recent works leverage pre-trained vision-language models to
measure the similarity between encoded visual observations and textual
instructions, and then train a model to map this similarity to robot actions.
However, this two-step approach limits the model to capture the relationship
between visual observations and textual instructions, leading to reduced
precision in manipulation tasks. We propose to learn visual-textual
associations through a self-supervised pretext task: reconstructing a masked
goal image conditioned on an input image and textual instructions. This
formulation allows the model to learn visual-action representations without
robot action supervision. The learned representations can then be fine-tuned
for manipulation tasks with only a few demonstrations. We also introduce the
\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot
tabletop manipulation episodes, including 180 object classes and 3,200
instances with corresponding textual instructions. This dataset enables the
model to acquire diverse object priors and allows for a more comprehensive
evaluation of its generalisation capability across object instances.
Experimental results on the five benchmarks, including both simulated and
real-robot validations, demonstrate that our method outperforms prior art.

</details>


### [257] [From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation](https://arxiv.org/abs/2508.19425)
*John M. Scanlon,Timothy L McMurry,Yin-Hsiu Chen,Kristofer D. Kusano,Trent Victor*

Main category: cs.RO

TL;DR: The paper establishes crash rate benchmarks for Automated Driving Systems (ADS) across various urban areas, emphasizing both surface streets and freeways.


<details>
  <summary>Details</summary>
Motivation: To extend prior efforts in benchmarking ADS crash rates by including freeways and addressing geographic and crash severity variations.

Method: Analysis of publicly available crash data and vehicle miles traveled (VMT), isolating in-transport passenger vehicles and categorizing crashes by road type and severity.

Result: Freeway crash rates show wide geographic variations; higher crash rates in some areas necessitate location-specific benchmarks for fair ADS safety evaluations.

Conclusion: This paper provides the first comprehensive framework for freeway-specific ADS benchmarks, critical for evaluating ADS safety and identifying statistically significant deviations from human driving performance.

Abstract: This paper presents crash rate benchmarks for evaluating US-based Automated
Driving Systems (ADS) for multiple urban areas. The purpose of this study was
to extend prior benchmarks focused only on surface streets to additionally
capture freeway crash risk for future ADS safety performance assessments. Using
publicly available police-reported crash and vehicle miles traveled (VMT) data,
the methodology details the isolation of in-transport passenger vehicles, road
type classification, and crash typology. Key findings revealed that freeway
crash rates exhibit large geographic dependence variations with
any-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4
IPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results
show the critical need for location-specific benchmarks to avoid biased safety
evaluations and provide insights into the vehicle miles traveled (VMT) required
to achieve statistical significance for various safety impact levels. The
distribution of crash types depended on the outcome severity level. Higher
severity outcomes (e.g., fatal crashes) had a larger proportion of
single-vehicle, vulnerable road users (VRU), and opposite-direction collisions
compared to lower severity (police-reported) crashes. Given heterogeneity in
crash types by severity, performance in low-severity scenarios may not be
predictive of high-severity outcomes. These benchmarks are additionally used to
quantify at the required mileage to show statistically significant deviations
from human performance. This is the first paper to generate freeway-specific
benchmarks for ADS evaluation and provides a foundational framework for future
ADS benchmarking by evaluators and developers.

</details>


### [258] [An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals](https://arxiv.org/abs/2508.19429)
*Gustavo A. Cardona,Kaier Liang,Cristian-Ioan Vasile*

Main category: cs.RO

TL;DR: This paper develops an iterative algorithm for route planning in environments with unknown resource distributions, enabling efficient heterogeneous robot team coordination.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of planning and coordinating multi-agent robotic systems in uncertain resource-constrained environments.

Method: Introduced an iterative algorithm leveraging Capability Temporal Logic to optimize exploration, resource identification, and task fulfillment dynamically.

Result: The iterative approach demonstrated effective performance and coordination in simulated case studies, tackling resource uncertainties efficiently.

Conclusion: The proposed method offers robust solutions for coordinating heterogeneous robot teams in dynamic environments, adapting strategies as new data is uncovered.

Abstract: This paper presents an iterative approach for heterogeneous multi-agent route
planning in environments with unknown resource distributions. We focus on a
team of robots with diverse capabilities tasked with executing missions
specified using Capability Temporal Logic (CaTL), a formal framework built on
Signal Temporal Logic to handle spatial, temporal, capability, and resource
constraints. The key challenge arises from the uncertainty in the initial
distribution and quantity of resources in the environment. To address this, we
introduce an iterative algorithm that dynamically balances exploration and task
fulfillment. Robots are guided to explore the environment, identifying resource
locations and quantities while progressively refining their understanding of
the resource landscape. At the same time, they aim to maximally satisfy the
mission objectives based on the current information, adapting their strategies
as new data is uncovered. This approach provides a robust solution for planning
in dynamic, resource-constrained environments, enabling efficient coordination
of heterogeneous teams even under conditions of uncertainty. Our method's
effectiveness and performance are demonstrated through simulated case studies.

</details>


### [259] [Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning](https://arxiv.org/abs/2508.19476)
*Dane Brouwer,Joshua Citron,Heather Nolte,Jeannette Bohg,Mark Cutkosky*

Main category: cs.RO

TL;DR: This paper investigates the use of non-prehensile tactile sensing and other modalities to enable robots to safely extract objects from cluttered environments.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty robots face in safely and efficiently extracting objects from cluttered spaces, a task that humans perform easily using tactile sensing along their hands and arms.

Method: The authors use imitation learning to train robot policies based on demonstrations in randomly generated environments, and perform ablation studies to investigate the role of tactile and force sensing modalities.

Result: Robotic policies employing tactile or force sensing achieve fewer force-related failures, greater success rates, and faster task completion, with the best outcomes combining tactile and wrench information—showing an 80% improvement over the baseline.

Conclusion: Incorporating tactile and force sensing can significantly enhance a robot's ability to handle constrained and cluttered environments safely and effectively.

Abstract: Dense collections of movable objects are common in everyday spaces -- from
cabinets in a home to shelves in a warehouse. Safely retracting objects from
such collections is difficult for robots, yet people do it easily, using
non-prehensile tactile sensing on the sides and backs of their hands and arms.
We investigate the role of such sensing for training robots to gently reach
into constrained clutter and extract objects. The available sensing modalities
are (1) "eye-in-hand" vision, (2) proprioception, (3) non-prehensile triaxial
tactile sensing, (4) contact wrenches estimated from joint torques, and (5) a
measure of successful object acquisition obtained by monitoring the vacuum line
of a suction cup. We use imitation learning to train policies from a set of
demonstrations on randomly generated scenes, then conduct an ablation study of
wrench and tactile information. We evaluate each policy's performance across 40
unseen environment configurations. Policies employing any force sensing show
fewer excessive force failures, an increased overall success rate, and faster
completion times. The best performance is achieved using both tactile and
wrench information, producing an 80% improvement above the baseline without
force information.

</details>


### [260] [DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View](https://arxiv.org/abs/2508.19508)
*Tian Qiu,Alan Zoubi,Yiyuan Lin,Ruiming Du,Lailiang Cheng,Yu Jiang*

Main category: cs.RO

TL;DR: A framework called DATR improves 3D reconstruction of apple trees from sparse views using diffusion models and foundation models, outperforming existing methods and enhancing scalability in digital twin applications.


<details>
  <summary>Details</summary>
Motivation: Current 3D reconstruction methods face challenges under field conditions, particularly with sparse and occluded views, limiting their effectiveness for applications like agricultural monitoring.

Method: The study introduces a two-stage reconstruction framework (DATR), leveraging onboard sensors, foundation models, and synthetic data-based training to process sparse field data into high-fidelity 3D models.

Result: DATR outperformed existing methods on field and synthetic datasets, achieving accuracy comparable to stationary industrial laser scanners while increasing throughput by ~360 times.

Conclusion: DATR offers a scalable solution for agricultural digital twins, enhancing the precision and efficiency of 3D reconstruction in challenging field environments.

Abstract: Digital twin applications offered transformative potential by enabling
real-time monitoring and robotic simulation through accurate virtual replicas
of physical assets. The key to these systems is 3D reconstruction with high
geometrical fidelity. However, existing methods struggled under field
conditions, especially with sparse and occluded views. This study developed a
two-stage framework (DATR) for the reconstruction of apple trees from sparse
views. The first stage leverages onboard sensors and foundation models to
semi-automatically generate tree masks from complex field images. Tree masks
are used to filter out background information in multi-modal data for the
single-image-to-3D reconstruction at the second stage. This stage consists of a
diffusion model and a large reconstruction model for respective multi view and
implicit neural field generation. The training of the diffusion model and LRM
was achieved by using realistic synthetic apple trees generated by a Real2Sim
data generator. The framework was evaluated on both field and synthetic
datasets. The field dataset includes six apple trees with field-measured ground
truth, while the synthetic dataset featured structurally diverse trees.
Evaluation results showed that our DATR framework outperformed existing 3D
reconstruction methods across both datasets and achieved domain-trait
estimation comparable to industrial-grade stationary laser scanners while
improving the throughput by $\sim$360 times, demonstrating strong potential for
scalable agricultural digital twin systems.

</details>


### [261] [A Lightweight Crowd Model for Robot Social Navigation](https://arxiv.org/abs/2508.19595)
*Maryam Kazemi Eskeri,Thomas Wiedemann,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: The paper introduces a lightweight macroscopic crowd prediction model for real-time pedestrian flow analysis that enhances computational efficiency and prediction accuracy, facilitating better robot navigation in dense environments.


<details>
  <summary>Details</summary>
Motivation: Motivated by the need for robots to navigate safely and efficiently among humans, this paper addresses the limitations of current crowd movement estimation techniques which are computationally expensive or overly simplistic.

Method: The authors developed a streamlined macroscopic crowd prediction model that simplifies both spatial and temporal processing of pedestrian flow, optimized for balancing accuracy and computational efficiency. This model was integrated into a socially aware robot navigation framework.

Result: The proposed model achieved a 3.6x reduction in inference time and a 3.1% improvement in prediction accuracy when compared to traditional methods.

Conclusion: The study concludes that efficient macroscopic crowd modeling is critical for enabling robots to navigate safely and effectively in densely populated environments without requiring high computational resources.

Abstract: Robots operating in human-populated environments must navigate safely and
efficiently while minimizing social disruption. Achieving this requires
estimating crowd movement to avoid congested areas in real-time. Traditional
microscopic models struggle to scale in dense crowds due to high computational
cost, while existing macroscopic crowd prediction models tend to be either
overly simplistic or computationally intensive. In this work, we propose a
lightweight, real-time macroscopic crowd prediction model tailored for human
motion, which balances prediction accuracy and computational efficiency. Our
approach simplifies both spatial and temporal processing based on the inherent
characteristics of pedestrian flow, enabling robust generalization without the
overhead of complex architectures. We demonstrate a 3.6 times reduction in
inference time, while improving prediction accuracy by 3.1 %. Integrated into a
socially aware planning framework, the model enables efficient and socially
compliant robot navigation in dynamic environments. This work highlights that
efficient human crowd modeling enables robots to navigate dense environments
without costly computations.

</details>


### [262] [Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks](https://arxiv.org/abs/2508.19607)
*Amin Berjaoui Tahmaz,Ravi Prakash,Jens Kober*

Main category: cs.RO

TL;DR: The paper introduces a hierarchical reinforcement learning framework combining impedance primitives and variable stiffness control to improve robotic manipulation in sequential contact tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance robotic manipulation in tasks requiring sequential contact by leveraging dynamic stiffness adjustments and efficient exploration strategies.

Method: The framework includes an action space for variable stiffness control, an adaptive stiffness controller for real-time adjustments, and affordance coupling to guide exploration.

Result: Demonstrated improved learning efficiency, effective primitive compositionality, and higher success rates in various manipulation tasks like block lifting and door opening, with successful transfer to real-world operations.

Conclusion: The approach advances adaptive and versatile robotic manipulation, paving the way for handling more complex contact-driven tasks.

Abstract: This paper presents an Impedance Primitive-augmented hierarchical
reinforcement learning framework for efficient robotic manipulation in
sequential contact tasks. We leverage this hierarchical structure to
sequentially execute behavior primitives with variable stiffness control
capabilities for contact tasks. Our proposed approach relies on three key
components: an action space enabling variable stiffness control, an adaptive
stiffness controller for dynamic stiffness adjustments during primitive
execution, and affordance coupling for efficient exploration while encouraging
compliance. Through comprehensive training and evaluation, our framework learns
efficient stiffness control capabilities and demonstrates improvements in
learning efficiency, compositionality in primitive selection, and success rates
compared to the state-of-the-art. The training environments include block
lifting, door opening, object pushing, and surface cleaning. Real world
evaluations further confirm the framework's sim2real capability. This work lays
the foundation for more adaptive and versatile robotic manipulation systems,
with potential applications in more complex contact-based tasks.

</details>


### [263] [Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning](https://arxiv.org/abs/2508.19608)
*Dongjae Lee,Byeongjun Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: The paper proposes a geometric robust control and motion planning framework for omnidirectional aerial manipulators (OAMs) to enhance manipulation capabilities in complex poses, including extreme pitch angles.


<details>
  <summary>Details</summary>
Motivation: Current aerial manipulators based on conventional multirotors face limitations in manipulation due to their underactuated nature. Enabling omnidirectional control would significantly expand manipulation workspaces and tasks.

Method: The authors developed a geometric robust controller for a floating base and a two-step optimization-based whole-body motion planner to control the 6D pose of the OAM and handle the joint configuration of the robotic arm.

Result: The proposed framework allows the OAM to maintain stability while operating in challenging poses, performing complex manipulations, and avoiding collisions.

Conclusion: The approach demonstrates significant advancement in aerial manipulation, enabling tasks such as object grasping and pulling in extreme pitch angles, showcasing its potential for complex, real-world scenarios.

Abstract: Aerial manipulators based on conventional multirotors can conduct
manipulation only in small roll and pitch angles due to the underactuatedness
of the multirotor base. If the multirotor base is capable of hovering at
arbitrary orientation, the robot can freely locate itself at any point in
$\mathsf{SE}(3)$, significantly extending its manipulation workspace and
enabling a manipulation task that was originally not viable. In this work, we
present a geometric robust control and whole-body motion planning framework for
an omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,
we first propose a geometric robust controller for a floating base. Since the
motion of the robotic arm and the interaction forces during manipulation affect
the stability of the floating base, the base should be capable of mitigating
these adverse effects while controlling its 6D pose. We then design a two-step
optimization-based whole-body motion planner, jointly considering the pose of
the floating base and the joint angles of the robotic arm to harness the entire
configuration space. The devised two-step approach facilitates real-time
applicability and enhances convergence of the optimization problem with
non-convex and non-Euclidean search space. The proposed approach enables the
base to be stationary at any 6D pose while autonomously carrying out
sophisticated manipulation near obstacles without any collision. We demonstrate
the effectiveness of the proposed framework through experiments in which an OAM
performs grasping and pulling of an object in multiple scenarios, including
near $90^\circ$ and even $180^\circ$ pitch angles.

</details>


### [264] [Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control](https://arxiv.org/abs/2508.19684)
*Ghadeer Elmkaiel,Syn Schmitt,Michael Muehlebach*

Main category: cs.RO

TL;DR: Floaty, a shape-changing aerial robot, combines agile maneuverability with high energy efficiency by passively harnessing wind energy and using intelligent morphological control.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving both agility and energy efficiency in aerial robots, especially in dynamic wind conditions, where current systems either consume too much energy or lack maneuverability.

Method: A shape-changing design inspired by birds, optimized for passive stability, and controlled via an experimentally learned aerodynamic model. It relies on wind energy without active propulsion.

Result: Floaty can hover, maneuver, and resist disturbances in wind speeds of up to 10 m/s, with a very low specific power consumption of 10 W/kg, significantly outperforming existing thruster-powered systems.

Conclusion: Demonstrates the potential for energy-efficient, sustainable aerial robotics through morphological intelligence and passive wind energy utilization.

Abstract: Achieving both agile maneuverability and high energy efficiency in aerial
robots, particularly in dynamic wind environments, remains challenging.
Conventional thruster-powered systems offer agility but suffer from high energy
consumption, while fixed-wing designs are efficient but lack hovering and
maneuvering capabilities. We present Floaty, a shape-changing robot that
overcomes these limitations by passively soaring, harnessing wind energy
through intelligent morphological control inspired by birds. Floaty's design is
optimized for passive stability, and its control policy is derived from an
experimentally learned aerodynamic model, enabling precise attitude and
position control without active propulsion. Wind tunnel experiments demonstrate
Floaty's ability to hover, maneuver, and reject disturbances in vertical
airflows up to 10 m/s. Crucially, Floaty achieves this with a specific power
consumption of 10 W/kg, an order of magnitude lower than thruster-powered
systems. This introduces a paradigm for energy-efficient aerial robotics,
leveraging morphological intelligence and control to operate sustainably in
challenging wind conditions.

</details>


### [265] [Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments](https://arxiv.org/abs/2508.19731)
*Maryam Kazemi Eskeri,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: The paper addresses inefficiencies in multi-robot task allocation (MRTA) in shared environments by introducing a novel method leveraging human movement patterns, resulting in significant task completion time reductions.


<details>
  <summary>Details</summary>
Motivation: Current MRTA approaches neglect human dynamics in shared environments, causing inefficiencies and delays in multi-robot collaboration.

Method: The introduced method incorporates Maps of Dynamics (MoDs)—spatio-temporal models of human movement patterns—into the task allocation process through a stochastic cost function.

Result: Experimental findings demonstrate that integrating MoDs into MRTA reduces mission completion times by up to 26% compared to dynamics-agnostic approaches and 19% compared to baseline methods.

Conclusion: Considering human dynamics in MRTA is crucial for optimizing task allocation in shared environments, and the proposed framework provides an efficient solution for deploying multi-robot systems alongside humans.

Abstract: Multi-robot systems are increasingly deployed in applications, such as
intralogistics or autonomous delivery, where multiple robots collaborate to
complete tasks efficiently. One of the key factors enabling their efficient
cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this
problem optimize task distribution among robots to minimize the overall
execution time. In shared environments, apart from the relative distance
between the robots and the tasks, the execution time is also significantly
impacted by the delay caused by navigating around moving people. However, most
existing MRTA approaches are dynamics-agnostic, relying on static maps and
neglecting human motion patterns, leading to inefficiencies and delays. In this
paper, we introduce \acrfull{method name}. This method leverages Maps of
Dynamics (MoDs), spatio-temporal queryable models designed to capture
historical human movement patterns, to estimate the impact of humans on the
task execution time during deployment. \acrshort{method name} utilizes a
stochastic cost function that includes MoDs. Experimental results show that
integrating MoDs enhances task allocation performance, resulting in reduced
mission completion times by up to $26\%$ compared to the dynamics-agnostic
method and up to $19\%$ compared to the baseline. This work underscores the
importance of considering human dynamics in MRTA within shared environments and
presents an efficient framework for deploying multi-robot systems in
environments populated by humans.

</details>


### [266] [Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles](https://arxiv.org/abs/2508.19771)
*Liding Zhang,Zhenshan Bing,Yu Zhang,Kuanqi Cai,Lingyun Chen,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: This paper presents FDIT*, a high-dimensional motion planning algorithm leveraging physical force dynamics to improve convergence and cost-efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in high-dimensional motion planning by enhancing existing algorithms for faster and cost-effective navigation.

Method: The proposed method, FDIT*, integrates Coulomb's law and elliptical k-nearest neighbor search to leverage invalid vertex data for optimizing motion planning.

Result: FDIT* demonstrates superior performance in search efficiency and cost reduction compared to existing planners across dimensions R^4 to R^16 and in real-world tasks.

Conclusion: FDIT* provides a novel and effective extension to sampling-based motion planning methods, optimizing pathfinding in complex environments.

Abstract: Path planning has long been an important and active research area in
robotics. To address challenges in high-dimensional motion planning, this study
introduces the Force Direction Informed Trees (FDIT*), a sampling-based planner
designed to enhance speed and cost-effectiveness in pathfinding. FDIT* builds
upon the state-of-the-art informed sampling planner, the Effort Informed Trees
(EIT*), by capitalizing on often-overlooked information in invalid vertices. It
incorporates principles of physical force, particularly Coulomb's law. This
approach proposes the elliptical $k$-nearest neighbors search method, enabling
fast convergence navigation and avoiding high solution cost or infeasible paths
by exploring more problem-specific search-worthy areas. It demonstrates
benefits in search efficiency and cost reduction, particularly in confined,
high-dimensional environments. It can be viewed as an extension of nearest
neighbors search techniques. Fusing invalid vertex data with physical dynamics
facilitates force-direction-based search regions, resulting in an improved
convergence rate to the optimum. FDIT* outperforms existing single-query,
sampling-based planners on the tested problems in R^4 to R^16 and has been
demonstrated on a real-world mobile manipulation task.

</details>


### [267] [Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization](https://arxiv.org/abs/2508.19776)
*Liding Zhang,Yao Ling,Zhenshan Bing,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: Greedy GuILD Grafting Trees (G3T*) is a novel bidirectional motion planning algorithm that boosts efficiency and solution quality by grafting invalid connections, optimizing paths selectively, and dynamically adjusting sampling distributions.


<details>
  <summary>Details</summary>
Motivation: Minimizing planning time and optimizing path quality in bidirectional motion planning by addressing failures in reverse search-tree connection.

Method: Proposes G3T*, using invalid edge grafting, guided incremental local densification subsets, and dynamic sampling distribution adjustments to boost efficiency and ensure connectivity.

Result: Benchmark tests in dimensions from R^2 to R^8 and real-world robotic scenarios showed faster convergence and lower solution costs for G3T* compared to existing planners.

Conclusion: G3T* offers superior performance in motion planning, enhancing solution efficiency, robustness, and cost optimization.

Abstract: Bidirectional motion planning often reduces planning time compared to its
unidirectional counterparts. It requires connecting the forward and reverse
search trees to form a continuous path. However, this process could fail and
restart the asymmetric bidirectional search due to the limitations of
lazy-reverse search. To address this challenge, we propose Greedy GuILD
Grafting Trees (G3T*), a novel path planner that grafts invalid edge
connections at both ends to re-establish tree-based connectivity, enabling
rapid path convergence. G3T* employs a greedy approach using the minimum
Lebesgue measure of guided incremental local densification (GuILD) subsets to
optimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling
distribution between the informed set and GuILD subsets based on historical and
current cost improvements, ensuring asymptotic optimality. These features
enhance the forward search's growth towards the reverse tree, achieving faster
convergence and lower solution costs. Benchmark experiments across dimensions
from R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior
performance compared to existing single-query sampling-based planners. A video
showcasing our experimental results is available at:
https://youtu.be/3mfCRL5SQIU

</details>


### [268] [Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots](https://arxiv.org/abs/2508.19788)
*Sena Ishii,Akash Chikhalikar,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: The paper introduces a novel framework to help service robots estimate accident-prone areas in indoor environments using semantic graph-based risk propagation.


<details>
  <summary>Details</summary>
Motivation: As robots increasingly operate in human-centric environments, ensuring their ability to anticipate hazards is key for user safety and effective human-robot collaboration.

Method: A semantic graph-based propagation algorithm is used, where objects are represented as nodes with risk scores, enabling risk inference based on spatial proximity and relationships.

Result: The method achieves a binary risk detection accuracy of 75% and aligns strongly with human perception, especially for risky objects like sharp or unstable ones.

Conclusion: The proposed framework is promising for improving the safety and contextual awareness of robots in shared human-robot spaces, and can guide future systems for proactive risk management.

Abstract: We present a novel framework for estimating accident-prone regions in
everyday indoor scenes, aimed at improving real-time risk awareness in service
robots operating in human-centric environments. As robots become integrated
into daily life, particularly in homes, the ability to anticipate and respond
to environmental hazards is crucial for ensuring user safety, trust, and
effective human-robot interaction. Our approach models object-level risk and
context through a semantic graph-based propagation algorithm. Each object is
represented as a node with an associated risk score, and risk propagates
asymmetrically from high-risk to low-risk objects based on spatial proximity
and accident relationship. This enables the robot to infer potential hazards
even when they are not explicitly visible or labeled. Designed for
interpretability and lightweight onboard deployment, our method is validated on
a dataset with human-annotated risk regions, achieving a binary risk detection
accuracy of 75%. The system demonstrates strong alignment with human
perception, particularly in scenes involving sharp or unstable objects. These
results underline the potential of context-aware risk reasoning to enhance
robotic scene understanding and proactive safety behaviors in shared
human-robot spaces. This framework could serve as a foundation for future
systems that make context-driven safety decisions, provide real-time alerts, or
autonomously assist users in avoiding or mitigating hazards within home
environments.

</details>


### [269] [APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors](https://arxiv.org/abs/2508.19790)
*Liding Zhang,Sicheng Wang,Kuanqi Cai,Zhenshan Bing,Fan Wu,Chaoqun Wang,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: This paper introduces APT*, a novel path planning algorithm that improves efficiency by dynamically adapting to environmental feedback using advanced sampling techniques and virtual force modeling.


<details>
  <summary>Details</summary>
Motivation: Existing path planning methods often use fixed batch sizes and lack consideration of obstacle-specific information, which limits their adaptability and efficiency.

Method: APT* integrates adaptive batch-sizing and elliptical $r$-nearest neighbor modules, leveraging Coulomb's law to model virtual forces and refine path planning.

Result: The proposed method demonstrated superior performance in high-dimensional path planning (up to $\mathbb{R}^{16}$) and was effective in a real-world robot manipulation task.

Conclusion: APT* provides improved convergence rates and lower solution costs compared to existing methods, establishing its potential for advanced robotic applications.

Abstract: Optimal path planning aims to determine a sequence of states from a start to
a goal while accounting for planning objectives. Popular methods often
integrate fixed batch sizes and neglect information on obstacles, which is not
problem-specific. This study introduces Adaptively Prolated Trees (APT*), a
novel sampling-based motion planner that extends based on Force Direction
Informed Trees (FDIT*), integrating adaptive batch-sizing and elliptical
$r$-nearest neighbor modules to dynamically modulate the path searching process
based on environmental feedback. APT* adjusts batch sizes based on the
hypervolume of the informed sets and considers vertices as electric charges
that obey Coulomb's law to define virtual forces via neighbor samples, thereby
refining the prolate nearest neighbor selection. These modules employ
non-linear prolate methods to adaptively adjust the electric charges of
vertices for force definition, thereby improving the convergence rate with
lower solution costs. Comparative analyses show that APT* outperforms existing
single-query sampling-based planners in dimensions from $\mathbb{R}^4$ to
$\mathbb{R}^{16}$, and it was further validated through a real-world robot
manipulation task. A video showcasing our experimental results is available at:
https://youtu.be/gCcUr8LiEw4

</details>


### [270] [A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living](https://arxiv.org/abs/2508.19816)
*Ricardo J. Manríquez-Cisterna,Ankit A. Ravankar,Jose V. Salazar Luces,Takuro Hatsukari,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: The paper introduces 'Moby,' an upright standing support mobility robot designed to enhance independence and safety for elderly individuals, particularly during daily tasks such as toilet transfers.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of conventional seated mobility aids by providing a device that supports natural upright posture, reduces physical strain, facilitates social interactions, and boosts self-efficacy for elderly users.

Method: The robot 'Moby' incorporates a lightweight and ergonomic design, leveraging the Robot Operating System (ROS) for control, NAV2 and LiDAR for navigation, and a custom control system for intuitive interaction. It operates in both manual and autonomous modes and supports sit-to-stand assistance.

Result: The robot's design and functionality were validated through a combination of objective and subjective experiments, including NASA-TLX assessments and time comparisons, demonstrating its advantages in usability, comfort, and efficiency over existing solutions.

Conclusion: Moby serves as an effective and innovative alternative to traditional mobility aids, enhancing the independence and safety of elderly users while maintaining ease of use and versatility in daily tasks.

Abstract: This paper presents a standing support mobility robot "Moby" developed to
enhance independence and safety for elderly individuals during daily activities
such as toilet transfers. Unlike conventional seated mobility aids, the robot
maintains users in an upright posture, reducing physical strain, supporting
natural social interaction at eye level, and fostering a greater sense of
self-efficacy. Moby offers a novel alternative by functioning both passively
and with mobility support, enabling users to perform daily tasks more
independently. Its main advantages include ease of use, lightweight design,
comfort, versatility, and effective sit-to-stand assistance. The robot
leverages the Robot Operating System (ROS) for seamless control, featuring
manual and autonomous operation modes. A custom control system enables safe and
intuitive interaction, while the integration with NAV2 and LiDAR allows for
robust navigation capabilities. This paper reviews existing mobility solutions
and compares them to Moby, details the robot's design, and presents objective
and subjective experimental results using the NASA-TLX method and time
comparisons to other methods to validate our design criteria and demonstrate
the advantages of our contribution.

</details>


### [271] [FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control](https://arxiv.org/abs/2508.19926)
*Tan Jing,Shiting Chen,Yangfan Li,Weisheng Xu,Renjing Xu*

Main category: cs.RO

TL;DR: The paper introduces FARM, a new framework to improve humanoid controllers for both everyday and high-dynamic motions, significantly reducing errors and establishing a dataset and benchmark for the field.


<details>
  <summary>Details</summary>
Motivation: Humanoid controllers face difficulty in handling high-dynamic, explosive actions despite excelling in everyday motions, limiting real-world applicability.

Method: The FARM framework combines frame-accelerated augmentation for exposing models to rapid pose shifts, a robust base controller for low-dynamic motions, and a residual mixture-of-experts (MoE) for handling high-dynamic actions.

Result: FARM reduces tracking failure by 42.8% and joint position errors by 14.6% on the newly introduced HDHM dataset while maintaining accuracy for low-dynamic motions.

Conclusion: FARM sets a new standard for high-dynamic humanoid motion control, validated with a novel benchmark dataset, and offers open-source tools for further research.

Abstract: Unified physics-based humanoid controllers are pivotal for robotics and
character animation, yet models that excel on gentle, everyday motions still
stumble on explosive actions, hampering real-world deployment. We bridge this
gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),
an end-to-end framework composed of frame-accelerated augmentation, a robust
base controller, and a residual mixture-of-experts (MoE). Frame-accelerated
augmentation exposes the model to high-velocity pose changes by widening
inter-frame gaps. The base controller reliably tracks everyday low-dynamic
motions, while the residual MoE adaptively allocates additional network
capacity to handle challenging high-dynamic actions, significantly enhancing
tracking accuracy. In the absence of a public benchmark, we curate the
High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically
plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\% and
lowers global mean per-joint position error by 14.6\% relative to the baseline,
while preserving near-perfect accuracy on low-dynamic motions. These results
establish FARM as a new baseline for high-dynamic humanoid control and
introduce the first open benchmark dedicated to this challenge. The code and
dataset will be released at https://github.com/Colin-Jing/FARM.

</details>


### [272] [Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors](https://arxiv.org/abs/2508.19953)
*Rafael Cathomen,Mayank Mittal,Marin Vlastelica,Marco Hutter*

Main category: cs.RO

TL;DR: The paper introduces a modular framework for Unsupervised Skill Discovery (USD) in robots to improve safety, interpretability, and real-world application by using state-space factorization, symmetry-based biases, and style regularization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying USD methods for robot skill discovery in real-world settings, specifically focusing on safety, interpretability, and ease of deployment.

Method: The framework uses user-defined factorization of the state space to disentangle skills, applies symmetry-based biases to guide skill morphology, and incorporates style factors and penalties for robust and safe behaviors. Skills are evaluated in simulation and transferred to real hardware.

Result: The approach discovered structured, interpretable, and diverse skills that performed safely and robustly, achieving zero-shot hardware transfer and downstream task performance on par with reward-trained policies.

Conclusion: By integrating factorization, symmetry, and style elements, the framework improves the safety, interpretability, and versatility of USD methods, enabling practical real-world robotic applications.

Abstract: Unsupervised Skill Discovery (USD) allows agents to autonomously learn
diverse behaviors without task-specific rewards. While recent USD methods have
shown promise, their application to real-world robotics remains underexplored.
In this paper, we propose a modular USD framework to address the challenges in
the safety, interpretability, and deployability of the learned skills. Our
approach employs user-defined factorization of the state space to learn
disentangled skill representations. It assigns different skill discovery
algorithms to each factor based on the desired intrinsic reward function. To
encourage structured morphology-aware skills, we introduce symmetry-based
inductive biases tailored to individual factors. We also incorporate a style
factor and regularization penalties to promote safe and robust behaviors. We
evaluate our framework in simulation using a quadrupedal robot and demonstrate
zero-shot transfer of the learned skills to real hardware. Our results show
that factorization and symmetry lead to the discovery of structured
human-interpretable behaviors, while the style factor and penalties enhance
safety and diversity. Additionally, we show that the learned skills can be used
for downstream tasks and perform on par with oracle policies trained with
hand-crafted rewards.

</details>


### [273] [Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation](https://arxiv.org/abs/2508.19958)
*Yiguo Fan,Pengxiang Ding,Shuanghao Bai,Xinyang Tong,Yuyang Zhu,Hongchao Lu,Fengqi Dai,Wei Zhao,Yang Liu,Siteng Huang,Zhaoxin Fan,Badong Chen,Donglin Wang*

Main category: cs.RO

TL;DR: The paper introduces Long-VLA, an end-to-end vision-language-action model designed for long-horizon robotic tasks, and achieves significant improvements using a phase-aware input masking strategy.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models struggle with long-horizon, multi-step robotic tasks due to challenges in chaining skills and addressing subtask dependencies.

Method: The paper develops Long-VLA with a phase-aware input masking strategy that segments subtasks into moving and interaction phases, and also introduces the L-CALVIN benchmark for systematic evaluation.

Result: Long-VLA significantly outperformed state-of-the-art methods on both simulated and real-world long-horizon tasks, providing robust improvement.

Conclusion: Long-VLA establishes a new standard for long-horizon robotic control while preserving scalability and data efficiency, and its module can integrate with existing VLA models.

Abstract: Vision-Language-Action (VLA) models have become a cornerstone in robotic
policy learning, leveraging large-scale multimodal data for robust and scalable
control. However, existing VLA frameworks primarily address short-horizon
tasks, and their effectiveness on long-horizon, multi-step robotic manipulation
remains limited due to challenges in skill chaining and subtask dependencies.
In this work, we introduce Long-VLA, the first end-to-end VLA model
specifically designed for long-horizon robotic tasks. Our approach features a
novel phase-aware input masking strategy that adaptively segments each subtask
into moving and interaction phases, enabling the model to focus on
phase-relevant sensory cues and enhancing subtask compatibility. This unified
strategy preserves the scalability and data efficiency of VLA training, and our
architecture-agnostic module can be seamlessly integrated into existing VLA
models. We further propose the L-CALVIN benchmark to systematically evaluate
long-horizon manipulation. Extensive experiments on both simulated and
real-world tasks demonstrate that Long-VLA significantly outperforms prior
state-of-the-art methods, establishing a new baseline for long-horizon robotic
control.

</details>


### [274] [Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech](https://arxiv.org/abs/2508.20037)
*Henk H. A. Jekel,Alejandro Díaz Rosales,Luka Peternel*

Main category: cs.RO

TL;DR: The paper introduces a visio-verbal teleimpedance interface for controlling robot interaction stiffness using gaze and verbal commands, validated through experiments.


<details>
  <summary>Details</summary>
Motivation: Enable humans to accurately and intuitively command robot stiffness through natural interaction methods like gaze and speech.

Method: They use eye-tracking, a Visual Language Model (VLM) based on GPT-4o, and haptic devices to form an interface for commanding stiffness matrices.

Result: Experiments with robotic systems validated gaze and verbal command-based stiffness control, showcasing its practical functionality.

Conclusion: The interface effectively combines gaze and speech for nuanced robot stiffness control, proving adaptability in practical tasks.

Abstract: The paper presents a visio-verbal teleimpedance interface for commanding 3D
stiffness ellipsoids to the remote robot with a combination of the operator's
gaze and verbal interaction. The gaze is detected by an eye-tracker, allowing
the system to understand the context in terms of what the operator is currently
looking at in the scene. Along with verbal interaction, a Visual Language Model
(VLM) processes this information, enabling the operator to communicate their
intended action or provide corrections. Based on these inputs, the interface
can then generate appropriate stiffness matrices for different physical
interaction actions. To validate the proposed visio-verbal teleimpedance
interface, we conducted a series of experiments on a setup including a Force
Dimension Sigma.7 haptic device to control the motion of the remote Kuka LBR
iiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,
while human verbal commands are processed by a VLM using GPT-4o. The first
experiment explored the optimal prompt configuration for the interface. The
second and third experiments demonstrated different functionalities of the
interface on a slide-in-the-groove task.

</details>


### [275] [HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation](https://arxiv.org/abs/2508.20085)
*Zhecheng Yuan,Tianming Wei,Langzhe Gu,Pu Hua,Tianhai Liang,Yuanpei Chen,Huazhe Xu*

Main category: cs.RO

TL;DR: This paper proposes HERMES, a framework to translate human hand motion data into robotic behaviors, focusing on bimanual dexterous manipulation, and achieves generalizable performance across varied real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of translating multi-source human hand motions into feasible robot behaviors, particularly for robots with complex, high-dimensional dexterous hands, and to improve adaptability to diverse environmental conditions.

Method: The paper introduces HERMES, which uses a unified reinforcement learning approach to transform heterogeneous human hand motions into robotic behaviors. It employs a depth image-based sim2real transfer method to enhance real-world generalization and leverages a closed-loop Perspective-n-Point (PnP) localization mechanism for precise navigation and manipulation.

Result: Experimental results show that HERMES exhibits consistent and generalizable behaviors in diverse, unstructured environments and successfully performs various complex mobile bimanual dexterous manipulation tasks.

Conclusion: HERMES provides an effective framework for translating human hand motions into robotic manipulation skills, excelling in unstructured and real-world scenarios and advancing the field of versatile robotic manipulation.

Abstract: Leveraging human motion data to impart robots with versatile manipulation
skills has emerged as a promising paradigm in robotic manipulation.
Nevertheless, translating multi-source human hand motions into feasible robot
behaviors remains challenging, particularly for robots equipped with
multi-fingered dexterous hands characterized by complex, high-dimensional
action spaces. Moreover, existing approaches often struggle to produce policies
capable of adapting to diverse environmental conditions. In this paper, we
introduce HERMES, a human-to-robot learning framework for mobile bimanual
dexterous manipulation. First, HERMES formulates a unified reinforcement
learning approach capable of seamlessly transforming heterogeneous human hand
motions from multiple sources into physically plausible robotic behaviors.
Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth
image-based sim2real transfer method for improved generalization to real-world
scenarios. Furthermore, to enable autonomous operation in varied and
unstructured environments, we augment the navigation foundation model with a
closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise
alignment of visual goals and effectively bridging autonomous navigation and
dexterous manipulation. Extensive experimental results demonstrate that HERMES
consistently exhibits generalizable behaviors across diverse, in-the-wild
scenarios, successfully performing numerous complex mobile bimanual dexterous
manipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.

</details>


### [276] [Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning](https://arxiv.org/abs/2508.20095)
*Jinhao Liang,Sven Koenig,Ferdinando Fioretto*

Main category: cs.RO

TL;DR: The paper introduces a new framework, Discrete-Guided Diffusion (DGD), combining discrete MAPF solvers and generative diffusion models to improve scalability and trajectory quality in Multi-Robot Motion Planning (MRMP).


<details>
  <summary>Details</summary>
Motivation: To address the limitations of scalability in continuous planners and trajectory quality in discrete MAPF methods for MRMP.

Method: The framework decomposes MRMP into subproblems with convex spaces, merges discrete MAPF solutions with diffusion models for complex dependencies, and employs constraint repair for trajectory feasibility.

Result: The proposed method demonstrates state-of-the-art performance, scales to 100 robots, and achieves efficient planning with high success rates in complex environments.

Conclusion: The novel DGD framework effectively balances scalability and trajectory quality in MRMP, overcoming the challenges of existing methods and demonstrating its utility in large-scale scenarios.

Abstract: Multi-Robot Motion Planning (MRMP) involves generating collision-free
trajectories for multiple robots operating in a shared continuous workspace.
While discrete multi-agent path finding (MAPF) methods are broadly adopted due
to their scalability, their coarse discretization severely limits trajectory
quality. In contrast, continuous optimization-based planners offer
higher-quality paths but suffer from the curse of dimensionality, resulting in
poor scalability with respect to the number of robots. This paper tackles the
limitations of these two approaches by introducing a novel framework that
integrates discrete MAPF solvers with constrained generative diffusion models.
The resulting framework, called Discrete-Guided Diffusion (DGD), has three key
characteristics: (1) it decomposes the original nonconvex MRMP problem into
tractable subproblems with convex configuration spaces, (2) it combines
discrete MAPF solutions with constrained optimization techniques to guide
diffusion models capture complex spatiotemporal dependencies among robots, and
(3) it incorporates a lightweight constraint repair mechanism to ensure
trajectory feasibility. The proposed method sets a new state-of-the-art
performance in large-scale, complex environments, scaling to 100 robots while
achieving planning efficiency and high success rates.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [277] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: This paper introduces a data synthesis framework to assess and improve the functional understanding of code embeddings from large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Text embeddings from LLMs excel in semantic tasks, but their ability to capture code-level functional semantics remains underexplored, particularly beyond syntactic similarity.

Method: The authors developed a Functionality-Oriented Code Self-Evolution framework that generates diverse code snippets, categorized by semantic and syntactic differences, to train and evaluate embedding models.

Result: Experimental results on tasks like code clone detection, functional consistency identification, and code retrieval show significant performance improvements using datasets derived from this framework.

Conclusion: The proposed framework advances the functional understanding of code embeddings, making them more effective and generalizable across diverse code scenarios.

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


### [278] [Stack Trace-Based Crash Deduplication with Transformer Adaptation](https://arxiv.org/abs/2508.19449)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: The paper presents dedupT, a transformer-based approach to deduplicate crash reports by leveraging contextual and structural relationships within stack traces.


<details>
  <summary>Details</summary>
Motivation: Traditional crash report deduplication methods struggle with contextual understanding, leading to inefficiencies in handling duplicate reports.

Method: dedupT adapts a pretrained language model (PLM) for stack traces and employs its embeddings in training a fully-connected network (FCN) to effectively rank duplicate crash reports.

Result: dedupT outperformed existing methods with over 15% improvement in MRR compared to the best DL baseline and up to 9% over traditional methods, also achieving higher ROC-AUC for unique crash detection.

Conclusion: Modern NLP techniques, as implemented in dedupT, enhance crash report deduplication and reduce developer workload, showcasing advancements in software engineering.

Abstract: Automated crash reporting systems generate large volumes of duplicate
reports, overwhelming issue-tracking systems and increasing developer workload.
Traditional stack trace-based deduplication methods, relying on string
similarity, rule-based heuristics, or deep learning (DL) models, often fail to
capture the contextual and structural relationships within stack traces. We
propose dedupT, a transformer-based approach that models stack traces
holistically rather than as isolated frames. dedupT first adapts a pretrained
language model (PLM) to stack traces, then uses its embeddings to train a
fully-connected network (FCN) to rank duplicate crashes effectively. Extensive
experiments on real-world datasets show that dedupT outperforms existing DL and
traditional methods (e.g., sequence alignment and information retrieval
techniques) in both duplicate ranking and unique crash detection, significantly
reducing manual triage effort. On four public datasets, dedupT improves Mean
Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up
to 9% over traditional methods while achieving higher Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash
reports. Our work advances the integration of modern natural language
processing (NLP) techniques into software engineering, providing an effective
solution for stack trace-based crash deduplication.

</details>


### [279] [The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts](https://arxiv.org/abs/2508.19610)
*Kathrin Figl,Maria Kirchner,Sebastian Baltes,Michael Felderer*

Main category: cs.SE

TL;DR: The study examines how code comments influence the perceived helpfulness of Stack Overflow answers, finding that comments enhance perceived helpfulness, particularly for novices.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the issue of poorly understood code reuse, which can lead to bugs and security vulnerabilities.

Method: An online experiment was conducted with 91 participants simulating a Stack Overflow environment to analyze perceptions of code comments.

Result: Code comments significantly increased perceived helpfulness. Novices preferred block comments, while other features like position or answer scores were less important.

Conclusion: Insights can help improve community-driven platforms like Stack Overflow and enable AI tools to produce more readable and helpful coding outputs.

Abstract: Question-and-answer platforms such as Stack Overflow have become an important
way for software developers to share and retrieve knowledge. However, reusing
poorly understood code can lead to serious problems, such as bugs or security
vulnerabilities. To better understand how code comments affect the perceived
helpfulness of Stack Overflow answers, we conducted an online experiment
simulating a Stack Overflow environment (n=91). The results indicate that both
block and inline comments are perceived as significantly more helpful than
uncommented source code. Moreover, novices rated code snippets with block
comments as more helpful than those with inline comments. Interestingly, other
surface features, such as the position of an answer and its answer score, were
considered less important. The content of Stack Overflow has been a major
source for training large language models. AI-based coding assistants such as
GitHub Copilot, which are based on these models, might change the way Stack
Overflow is used. However, our findings have implications beyond this specific
platform. First, they may help to improve the relevance of community-driven
platforms such as Stack Overflow, which provide human advice and explanations
of code solutions, complementing AI-based support for software developers.
Second, since chat-based AI tools can be prompted to generate code in different
ways, knowing which properties influence perceived helpfulness might lead to
targeted prompting strategies to generate more readable code snippets.

</details>


### [280] [Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation](https://arxiv.org/abs/2508.19663)
*Lola Solovyeva,Eduardo Carneiro Oliveira,Shiyu Fan,Alper Tuncay,Shamil Gareev,Andrea Capiluppi*

Main category: cs.SE

TL;DR: This study explores the use of large language models (LLMs) for translating PL/SQL code to Java in a legacy system with 2.5M lines of code, proposing a customized prompting strategy. Preliminary results show accurate and functional translations but are limited by sample size and testing constraints.


<details>
  <summary>Details</summary>
Motivation: The legacy system lacks documentation and automated tests, making code refactoring and modernization extremely challenging. The study aims to leverage LLMs to address these issues by automating the translation process.

Method: The researchers evaluated multiple LLMs using a dataset of 10 PL/SQL-to-Java code pairs and 15 Java classes. They introduced a prompting strategy combining chain-of-guidance reasoning with $n$-shot prompting to achieve translation accuracy and functionality.

Result: The proposed methodology enabled LLMs to generate syntactically accurate and functionally correct translations. However, the study faced limitations due to a small dataset and restricted test case availability.

Conclusion: Despite limitations, this methodology demonstrates the feasibility of using LLMs for modernizing large legacy systems, offering a scalable approach to address translation challenges.

Abstract: The VT legacy system, comprising approximately 2.5 million lines of PL/SQL
code, lacks consistent documentation and automated tests, posing significant
challenges for refactoring and modernisation. This study investigates the
feasibility of leveraging large language models (LLMs) to assist in translating
PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset
comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively
established a domain model for the translated files, multiple LLMs were
evaluated. Furthermore, we propose a customized prompting strategy that
integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings
indicate that this methodology effectively guides LLMs in generating
syntactically accurate translations while also achieving functional
correctness. However, the findings are limited by the small sample size of
available code files and the restricted access to test cases used for
validating the correctness of the generated code. Nevertheless, these findings
lay the groundwork for scalable, automated solutions in modernising large
legacy systems.

</details>


### [281] [Enabling Content Management Systems as an Information Source in Model-driven Projects](https://arxiv.org/abs/2508.19797)
*Joan Giner-Miguelez,Abel Gómez,Jordi Cabot*

Main category: cs.SE

TL;DR: The paper introduces a framework to integrate headless CMSs into software workflows by discovering their schema and generating middleware for platform-agnostic use.


<details>
  <summary>Details</summary>
Motivation: It's challenging to manage and integrate highly customized CMS data schemas due to manual processes that are time-consuming and error-prone.

Method: The authors propose a model-based framework to uncover CMS information schema and create middleware libraries for smooth integration.

Result: The framework facilitates automatic design and interaction between CMS models and consumer applications, simplifying integration tasks.

Conclusion: Headless CMS integration benefits from the proposed open-source framework, which streamlines the process and enhances adaptability in varied software ecosystems.

Abstract: Content Management Systems (CMSs) are the most popular tool when it comes to
create and publish content across the web. Recently, CMSs have evolved,
becoming \emph{headless}. Content served by a \emph{headless CMS} aims to be
consumed by other applications and services through REST APIs rather than by
human users through a web browser. This evolution has enabled CMSs to become a
notorious source of content to be used in a variety of contexts beyond pure web
navigation. As such, CMS have become an important component of many information
systems. Unfortunately, we still lack the tools to properly discover and manage
the information stored in a CMS, often highly customized to the needs of a
specific domain. Currently, this is mostly a time-consuming and error-prone
manual process.
  In this paper, we propose a model-based framework to facilitate the
integration of headless CMSs in software development processes. Our framework
is able to discover and explicitly represent the information schema behind the
CMS. This facilitates designing the interaction between the CMS model and other
components consuming that information. These interactions are then generated as
part of a middleware library that offers platform-agnostic access to the CMS to
all the client applications. The complete framework is open-source and
available online.

</details>


### [282] [Towards a fundamental theory of modeling discrete systems](https://arxiv.org/abs/2508.19803)
*Peter Fettke,Wolfgang Reisig*

Main category: cs.SE

TL;DR: The paper introduces the Heraklit modeling framework to address digital age challenges in modeling.


<details>
  <summary>Details</summary>
Motivation: Modeling is essential in science and engineering but faces challenges in adapting to the digital age, requiring a new foundational theory.

Method: The authors propose the Heraklit modeling framework as a novel approach to tackle these modeling challenges.

Result: The paper develops and outlines the Heraklit modeling framework to provide a new perspective on modeling.

Conclusion: The paper concludes with remarks on future research into modeling correctness, information representation, and invariance.

Abstract: Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

</details>


### [283] [On the Future of Software Reuse in the Era of AI Native Software Engineering](https://arxiv.org/abs/2508.19834)
*Antero Taivalsaari,Tommi Mikkonen,Cesare Pautasso*

Main category: cs.SE

TL;DR: The paper discusses the emerging shift in software development towards using AI for generative software reuse, highlighting implications and defining a research agenda.


<details>
  <summary>Details</summary>
Motivation: To address the rapid adoption of AI-generated code by developers and its implications on software reuse practices.

Method: The paper explores key questions and implications of AI-assisted generative software reuse, suggesting a research agenda to address its challenges.

Result: Insight into how AI-generated code influences software development, raising conceptual issues akin to cargo cult development.

Conclusion: AI-assisted codification is becoming a central practice, raising significant questions requiring research to manage and understand its implications.

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Earlier opportunistic software reuse practices and organic
software development methods are rapidly being replaced by "AI Native"
approaches in which developers place their trust on code that has been
generated by artificial intelligence. This is leading to a new form of software
reuse that is conceptually not all that different from cargo cult development.
In this paper we discuss the implications of AI-assisted generative software
reuse, bring forth relevant questions, and define a research agenda for
tackling the central issues associated with this emerging approach.

</details>


### [284] [Generative AI for Testing of Autonomous Driving Systems: A Survey](https://arxiv.org/abs/2508.19882)
*Qunying Song,He Ye,Mark Harman,Federica Sarro*

Main category: cs.SE

TL;DR: This paper surveys the role of generative AI in testing autonomous driving systems (ADS), analyzing 91 studies, six application categories, and identifying limitations.


<details>
  <summary>Details</summary>
Motivation: To enhance functional and safety testing of ADS in diverse driving conditions before large-scale deployment.

Method: The authors systematically reviewed 91 studies related to generative AI in ADS testing, synthesizing findings into application categories and analyzing datasets, tools, and limitations.

Result: Six major ADS testing categories were identified, along with datasets, simulators, and benchmarks, while recognizing 27 key limitations.

Conclusion: The paper provides an overview of generative AI's applications in ADS testing, highlights challenges, and proposes future research directions to advance the field.

Abstract: Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

</details>


### [285] [Smart Contract Intent Detection with Pre-trained Programming Language Model](https://arxiv.org/abs/2508.20086)
*Youwei Huang,Jianwen Li,Sen Fang,Yao Li,Peng Yang,Bin Hu,Tao Zhang*

Main category: cs.SE

TL;DR: SmartIntentNN2 advances the task of detecting malicious intents in smart contracts using a BERT pre-trained model, improving F1 from 0.8633 to 0.927.


<details>
  <summary>Details</summary>
Motivation: The paper addresses economic losses caused by malicious intents in smart contracts and seeks to improve detection mechanisms.

Method: SmartIntentNN2 integrates a BERT-based pre-trained language model with a BiLSTM multi-label classification system.

Result: SmartIntentNN2 achieves an improved F1 score of 0.927, surpassing its predecessor's performance.

Conclusion: The model sets a new benchmark for smart contract intent detection and offers stronger capabilities in identifying unsafe contract behaviors.

Abstract: Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [286] [Correcting model error bias in estimations of neuronal dynamics from time series observations](https://arxiv.org/abs/2508.19948)
*Ian Williams,Joseph D. Taylor,Alain Nogaret*

Main category: q-bio.NC

TL;DR: The study demonstrates using a recurrent neural network to improve predictions from a Hodgkin-Huxley model, enabling recovery of both observed and hidden neuron dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to estimate unobserved ion channel dynamics from membrane voltage recordings, which are difficult to access experimentally, by overcoming surrogate model inaccuracies.

Method: The paper combines a recurrent neural network with a surrogate Hodgkin-Huxley model to correct errors, training the hybrid model on voltage oscillations and driving current waveforms from a reference Hodgkin-Huxley model.

Result: Among six investigated architectures, one successfully recovered accurate membrane voltage and ion channel dynamics, effectively reconstructing observed and hidden state variables to match a reference neuron model.

Conclusion: The approach demonstrates that neural networks can correct model errors, offering a promising way to infer both observed and hidden biological neuron dynamics from experimental data.

Abstract: Neuron models built from experimental data have successfully predicted
observed voltage oscillations within and beyond training range. A tantalising
prospect is the possibility of estimating the unobserved dynamics of ion
channels which is largely inaccessible to experiment, from membrane voltage
recordings. The main roadblock here is our lack of knowledge of the equations
governing biological neurons which forces us to rely on surrogate models and
parameter estimates biassed by model error. Error correction algorithms are
therefore needed to infer both observed and unobserved dynamics, and ultimately
the actual parameters of a biological neuron. Here we use a recurrent neural
network to correct the outputs of a surrogate Hodgkin-Huxley (HH) model. The
reservoir-surrogate HH model hybrid was trained on the voltage oscillations of
a reference HH model and its driving current waveform. Out of the six
reservoir-surrogate model architectures investigated, we identify one that most
accurately recovers the reference membrane voltage and ion channel dynamics.
The reservoir was thus effective in correcting model error in an externally
driven nonlinear oscillator and in reconstructing the dynamics of both observed
and unobserved state variables from the reference model mimicking an actual
neuron.

</details>


### [287] [Saccade crossing avoidance as a visual search strategy](https://arxiv.org/abs/2508.18404)
*Alex Szorkovszky,Rujeena Mathema,Pedro Lencastre,Pedro Lind,Anis Yazidi*

Main category: q-bio.NC

TL;DR: The study identifies a memory-dependent self-crossing avoidance tendency in visual search, demonstrating its role as a local orienting strategy through analysis of scan paths during "Where's Waldo" viewings.


<details>
  <summary>Details</summary>
Motivation: Investigate memory-dependent effects in visual search scan paths beyond immediate fixations, focusing on strategic biases and individual differences.

Method: Analyzed scan paths from participants viewing "Where's Waldo" images, comparing real data to synthetic memoryless models. Developed a parametric model incorporating a self-crossing penalty term and used mixed-effect regressions to assess individual strategic differences.

Result: Discovered significant self-crossing avoidance tendency that most strongly incorporates the last ~7 seconds of scan history. Identified individual variability in strategies, linking high self-crossing avoidance with shorter saccade lengths and fixation durations.

Conclusion: Self-crossing avoidance complements inhibition of return and acts as a local strategy for exploring visual scenes, highlighting strategic and individual variability in visual search behavior.

Abstract: Although visual search appears largely random, several oculomotor biases
exist such that the likelihoods of saccade directions and lengths depend on the
previous scan path. Compared to the most recent fixations, the impact of the
longer path history is more difficult to quantify. Using the step-selection
framework commonly used in movement ecology, and analyzing data from 45-second
viewings of "Where's Waldo"?, we report a new memory-dependent effect that also
varies significantly between individuals, which we term self-crossing
avoidance. This is a tendency for saccades to avoid crossing those earlier in
the scan path, and is most evident when both have small amplitudes. We show
this by comparing real data to synthetic data generated from a memoryless
approximation of the spatial statistics (i.e. a Markovian nonparametric model
with a matching distribution of saccade lengths over time). Maximum likelihood
fitting indicates that this effect is strongest when including the last
$\approx 7$ seconds of a scan path. The effect size is comparable to well-known
forms of history dependence such as inhibition of return. A parametric
probabilistic model including a self-crossing penalty term was able to
reproduce joint statistics of saccade lengths and self-crossings. We also
quantified individual strategic differences, and their consistency over the six
images viewed per participant, using mixed-effect regressions. Participants
with a higher tendency to avoid crossings displayed smaller saccade lengths and
shorter fixation durations on average, but did not display more horizontal,
vertical, forward or reverse saccades. Together, these results indicate that
the avoidance of crossings is a local orienting strategy that facilitates and
complements inhibition of return, and hence exploration of visual scenes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [288] [Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic Modeling and Recursive Strategy](https://arxiv.org/abs/2508.19750)
*Binhui Zhang,Jianwei Ma*

Main category: stat.ML

TL;DR: The paper introduces Fractal Flow, an advanced normalizing flow architecture, that improves expressiveness and interpretability by combining Kolmogorov-Arnold Networks, Latent Dirichlet Allocation, and a recursive modular design.


<details>
  <summary>Details</summary>
Motivation: To address challenges in high-dimensional density estimation and generative modeling by enhancing the expressiveness and interpretability of normalizing flows.

Method: The method incorporates Kolmogorov-Arnold Networks and Latent Dirichlet Allocation to create structured latent spaces and hierarchical clusters, along with a recursive design inspired by Fractal Generative Models for modularity and improved accuracy.

Result: Experimental results on datasets like MNIST, FashionMNIST, CIFAR-10, and geophysical data show that Fractal Flow achieves superior latent clustering, controllable generation, and better estimation accuracy compared to other models.

Conclusion: Fractal Flow presents a significant advancement in normalizing flows by combining innovative structural and modular enhancements, offering a compelling approach to density estimation and generative modeling.

Abstract: Normalizing Flows provide a principled framework for high-dimensional density
estimation and generative modeling by constructing invertible transformations
with tractable Jacobian determinants. We propose Fractal Flow, a novel
normalizing flow architecture that enhances both expressiveness and
interpretability through two key innovations. First, we integrate
Kolmogorov-Arnold Networks and incorporate Latent Dirichlet Allocation into
normalizing flows to construct a structured, interpretable latent space and
model hierarchical semantic clusters. Second, inspired by Fractal Generative
Models, we introduce a recursive modular design into normalizing flows to
improve transformation interpretability and estimation accuracy. Experiments on
MNIST, FashionMNIST, CIFAR-10, and geophysical data demonstrate that the
Fractal Flow achieves latent clustering, controllable generation, and superior
estimation accuracy.

</details>


### [289] [Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers](https://arxiv.org/abs/2508.19841)
*Fahime Seyedheydari,Kevin Conley,Simo Särkkä*

Main category: stat.ML

TL;DR: The paper presents a probabilistic surrogate model leveraging conditional normalizing flows to predict radiative properties of nanoparticle scattering media with high accuracy and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: There is a need for efficient models to predict radiative properties of nanoparticle media, moving beyond conventional neural networks lacking uncertainty quantification.

Method: Conditional normalizing flows are used to learn the conditional distribution of optical outputs, trained on data generated via Monte Carlo radiative transfer simulations based on Mie theory.

Result: The introduced model demonstrates high predictive accuracy and offers reliable uncertainty quantification for radiative transfer simulations.

Conclusion: Conditional normalizing flows provide an efficient tool for predicting radiative properties, outperforming traditional methods by enabling posterior predictive distributions and principled uncertainty assessment.

Abstract: We present a probabilistic, data-driven surrogate model for predicting the
radiative properties of nanoparticle embedded scattering media. The model uses
conditional normalizing flows, which learn the conditional distribution of
optical outputs, including reflectance, absorbance, and transmittance, given
input parameters such as the absorption coefficient, scattering coefficient,
anisotropy factor, and particle size distribution. We generate training data
using Monte Carlo radiative transfer simulations, with optical properties
derived from Mie theory. Unlike conventional neural networks, the conditional
normalizing flow model yields full posterior predictive distributions, enabling
both accurate forecasts and principled uncertainty quantification. Our results
demonstrate that this model achieves high predictive accuracy and reliable
uncertainty estimates, establishing it as a powerful and efficient surrogate
for radiative transfer simulations.

</details>


### [290] [The Information Dynamics of Generative Diffusion](https://arxiv.org/abs/2508.19897)
*Luca Ambrogioni*

Main category: stat.ML

TL;DR: This paper unifies the theoretical understanding of generative diffusion models by examining their dynamic, information-theoretic, and thermodynamic properties under a shared mathematical framework.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an integrated theoretical understanding of how generative diffusion models operate, as their practical effectiveness contrasts with the current lack of theoretical clarity.

Method: The authors employ a mathematical framework to connect concepts like conditional entropy production, score function divergence, trajectory branching, and phase transitions in energy landscapes.

Result: The paper reveals that generative processes are driven by controlled symmetry-breaking transitions in the energy landscape, with the score function regulating incompatible noise fluctuations.

Conclusion: Generative diffusion is fundamentally governed by noise-induced, symmetry-breaking dynamics where critical transitions mark the transfer of information to potential outcomes.

Abstract: Generative diffusion models have emerged as a powerful class of models in
machine learning, yet a unified theoretical understanding of their operation is
still developing. This perspective paper provides an integrated perspective on
generative diffusion by connecting their dynamic, information-theoretic, and
thermodynamic properties under a unified mathematical framework. We demonstrate
that the rate of conditional entropy production during generation (i.e. the
generative bandwidth) is directly governed by the expected divergence of the
score function's vector field. This divergence, in turn, is linked to the
branching of trajectories and generative bifurcations, which we characterize as
symmetry-breaking phase transitions in the energy landscape. This synthesis
offers a powerful insight: the process of generation is fundamentally driven by
the controlled, noise-induced breaking of (approximate) symmetries, where peaks
in information transfer correspond to critical transitions between possible
outcomes. The score function acts as a dynamic non-linear filter that regulates
the bandwidth of the noise by suppressing fluctuations that are incompatible
with the data.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [291] [Aggregate Fictitious Play for Learning in Anonymous Polymatrix Games (Extended Version)](https://arxiv.org/abs/2508.19371)
*Semih Kara,Tamer Başar*

Main category: cs.GT

TL;DR: Aggregate fictitious play (agg-FP) is a variant of fictitious play (FP) designed for anonymous games, where rewards depend only on actions, not who performs them, reducing computation and ensuring convergence.


<details>
  <summary>Details</summary>
Motivation: To address the scalability issue in fictitious play caused by the exponential growth of joint action spaces with increasing agents, particularly when the reward functions are unknown.

Method: The aggregate FP tracks the frequency of other agents' actions in a summarized manner rather than individual actions. This is applied to anonymous polymatrix games and theoretically shown to converge to Nash equilibria under conditions similar to classical FP.

Result: Empirical simulations demonstrate that this aggregate approach accelerates convergence towards Nash equilibria compared to traditional FP.

Conclusion: Agg-FP maintains convergence guarantees while reducing computational complexities, making it well-suited for anonymous games with large-scale agent interactions.

Abstract: Fictitious play (FP) is a well-studied algorithm that enables agents to learn
Nash equilibrium in games with certain reward structures. However, when agents
have no prior knowledge of the reward functions, FP faces a major challenge:
the joint action space grows exponentially with the number of agents, which
slows down reward exploration. Anonymous games offer a structure that mitigates
this issue. In these games, the rewards depend only on the actions taken; not
on who is taking which action. Under such a structure, we introduce aggregate
fictitious play (agg-FP), a variant of FP where each agent tracks the frequency
of the number of other agents playing each action, rather than these agents'
individual actions. We show that in anonymous polymatrix games, agg-FP
converges to a Nash equilibrium under the same conditions as classical FP. In
essence, by aggregating the agents' actions, we reduce the action space without
losing the convergence guarantees. Using simulations, we provide empirical
evidence on how this reduction accelerates convergence.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [292] [Fast Texture Transfer for XR Avatars via Barycentric UV Conversion](https://arxiv.org/abs/2508.19518)
*Hail Song,Seokhwan Yang,Woontack Woo*

Main category: cs.GR

TL;DR: Introduces a fast and efficient method for transferring facial textures onto full-body avatars using the barycentric UV conversion technique, achieving significant speed improvements and better texture quality.


<details>
  <summary>Details</summary>
Motivation: The need for faster and artifact-free facial texture transfer methods for avatar personalization in immersive XR applications.

Method: Utilizes a barycentric UV conversion technique to precompute UV mapping into a single transformation matrix for streamlined texture transfer.

Result: Achieves a speedup of over 7000x compared to conventional methods and eliminates texture boundary artifacts.

Conclusion: The method is practical for XR personalization applications, improving efficiency and quality, with code availability for broader use.

Abstract: We present a fast and efficient method for transferring facial textures onto
SMPL-X-based full-body avatars. Unlike conventional affine-transform methods
that are slow and prone to visual artifacts, our method utilizes a barycentric
UV conversion technique. Our approach precomputes the entire UV mapping into a
single transformation matrix, enabling texture transfer in a single operation.
This results in a speedup of over 7000x compared to the baseline, while also
significantly improving the final texture quality by eliminating boundary
artifacts. Through quantitative and qualitative evaluations, we demonstrate
that our method offers a practical solution for personalization in immersive XR
applications. The code is available online.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [293] [Reduced-Order Modeling of Cyclo-Stationary Time Series Using Score-Based Generative Methods](https://arxiv.org/abs/2508.19448)
*Ludovico Theo Giorgini,Tobias Bischoff,Andre Noguiera Souza*

Main category: nlin.CD

TL;DR: The paper proposes a data-driven approach using score-based generative modeling to develop reduced-order models for cyclo-stationary time series, demonstrated for climate systems.


<details>
  <summary>Details</summary>
Motivation: Cyclo-stationary systems, marked by periodic patterns, demand efficient modeling methods that preserve physical and statistical characteristics while reducing computational costs, especially for applications like climate modeling.

Method: The authors use score-based generative modeling to achieve reduced-order data representations for cyclo-stationary systems, validating results by replicating statistical and temporal attributes of the original datasets.

Result: The surrogate model successfully reproduced key statistical properties, such as probability distributions and autocorrelation, while offering massive computational savings, generating centuries of climate data in minutes.

Conclusion: This technique balances computational efficiency and physical fidelity, enabling broad applications in modeling periodically forced systems across various scientific domains.

Abstract: Many natural systems exhibit cyclo-stationary behavior characterized by
periodic forcing such as annual and diurnal cycles. We present a data-driven
method leveraging recent advances in score-based generative modeling to
construct reduced-order models for such cyclo-stationary time series. Our
approach accurately reproduces the statistical properties and temporal
correlations of the original data, enabling efficient generation of synthetic
trajectories. We demonstrate the performance of the method through application
to the Planet Simulator (PlaSim) climate model, constructing a reduced-order
model for the 20 leading principal components of surface temperature driven by
the annual cycle. The resulting surrogate model accurately reproduces the
marginal and joint probability distributions, autocorrelation functions, and
spatial coherence of the original climate system across multiple validation
metrics. The approach offers substantial computational advantages, enabling
generation of centuries of synthetic climate data in minutes compared to weeks
required for equivalent full model simulations. This work opens new
possibilities for efficient modeling of periodically forced systems across
diverse scientific domains, providing a principled framework for balancing
computational efficiency with physical fidelity in reduced-order modeling
applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [294] [MuSpike: A Benchmark and Evaluation Framework for Symbolic Music Generation with Spiking Neural Networks](https://arxiv.org/abs/2508.19251)
*Qian Liang,Menghaoran Tang,Yi Zeng*

Main category: cs.SD

TL;DR: The paper introduces MuSpike, a benchmark framework for spiking neural networks (SNNs) in symbolic music generation, evaluating five SNN architectures systematically across datasets and implementing subjective and objective metrics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized benchmarks and comprehensive evaluation methods in spiking neural networks (SNNs) for symbolic music generation.

Method: The study presented MuSpike, which evaluates five SNN models (SNN-CNN, SNN-RNN, SNN-LSTM, SNN-GAN, and SNN-Transformer) through both objective metrics and a large-scale listening study emphasizing subjective measures like musical impression, autobiographical association, and personal preference.

Result: Findings show SNN models excel in different dimensions, expert listeners show higher acceptance of AI-composed music, and there exists a gap between objective and subjective evaluations.

Conclusion: MuSpike advances systematic assessment for SNNs in music generation, advocating for human perceptual judgments and providing a foundation for future biologically plausible music generation studies.

Abstract: Symbolic music generation has seen rapid progress with artificial neural
networks, yet remains underexplored in the biologically plausible domain of
spiking neural networks (SNNs), where both standardized benchmarks and
comprehensive evaluation methods are lacking. To address this gap, we introduce
MuSpike, a unified benchmark and evaluation framework that systematically
assesses five representative SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM,
SNN-GAN and SNN-Transformer) across five typical datasets, covering tonal,
structural, emotional, and stylistic variations. MuSpike emphasizes
comprehensive evaluation, combining established objective metrics with a
large-scale listening study. We propose new subjective metrics, targeting
musical impression, autobiographical association, and personal preference, that
capture perceptual dimensions often overlooked in prior work. Results reveal
that (1) different SNN models exhibit distinct strengths across evaluation
dimensions; (2) participants with different musical backgrounds exhibit diverse
perceptual patterns, with experts showing greater tolerance toward AI-composed
music; and (3) a noticeable misalignment exists between objective and
subjective evaluations, highlighting the limitations of purely statistical
metrics and underscoring the value of human perceptual judgment in assessing
musical quality. MuSpike provides the first systematic benchmark and systemic
evaluation framework for SNN models in symbolic music generation, establishing
a solid foundation for future research into biologically plausible and
cognitively grounded music generation.

</details>


### [295] [Beat-Based Rhythm Quantization of MIDI Performances](https://arxiv.org/abs/2508.19262)
*Maximilian Wachter,Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: The paper presents a transformer-based rhythm quantization model for converting MIDI performances into metrically-aligned scores using beat and downbeat information, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To improve the quantization of MIDI performances into human-readable and metrically-aligned scores by leveraging advanced modeling techniques and beat/downbeat information.

Method: A transformer-based rhythm quantization model incorporating beat and downbeat information, along with a unified token representation created using a beat-based preprocessing method.

Result: The proposed model surpasses state-of-the-art performance as measured by the MUSTER metric, demonstrating its effectiveness in handling piano and guitar performances.

Conclusion: The approach effectively aligns MIDI performances into human-readable scores with superior performance metrics, highlighting the importance of beat-related preprocessing and advanced model architecture.

Abstract: We propose a transformer-based rhythm quantization model that incorporates
beat and downbeat information to quantize MIDI performances into
metrically-aligned, human-readable scores. We propose a beat-based
preprocessing method that transfers score and performance data into a unified
token representation. We optimize our model architecture and data
representation and train on piano and guitar performances. Our model exceeds
state-of-the-art performance based on the MUSTER metric.

</details>


### [296] [CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation](https://arxiv.org/abs/2508.19603)
*Zhejing Hu,Yan Liu,Gong Chen,Bruce X. B. Yu*

Main category: cs.SD

TL;DR: The paper introduces an AI-driven music lexicon named CompLex to improve music generation by leveraging extensive music theory, showcasing enhanced performance across models and reliable evaluation.


<details>
  <summary>Details</summary>
Motivation: Generative AI in music lags behind advancements in NLP due to limited music data. Leveraging music theory could address this gap and reduce manual effort in tasks like algorithmic composition and style transfer.

Method: A novel automatic music lexicon construction model creates CompLex using 9 category keywords and 5 prompt templates. Additionally, a multi-agent algorithm is used to manage hallucinations.

Result: CompLex significantly enhances performance across three state-of-the-art text-to-music generation models and meets key evaluation metrics like completeness, accuracy, non-redundancy, and executability.

Conclusion: CompLex is a promising tool that facilitates improved AI-driven music generation, confirming the effectiveness of incorporating extensive music theory into generative tasks.

Abstract: Generative artificial intelligence in music has made significant strides, yet
it still falls short of the substantial achievements seen in natural language
processing, primarily due to the limited availability of music data.
Knowledge-informed approaches have been shown to enhance the performance of
music generation models, even when only a few pieces of musical knowledge are
integrated. This paper seeks to leverage comprehensive music theory in
AI-driven music generation tasks, such as algorithmic composition and style
transfer, which traditionally require significant manual effort with existing
techniques. We introduce a novel automatic music lexicon construction model
that generates a lexicon, named CompLex, comprising 37,432 items derived from
just 9 manually input category keywords and 5 sentence prompt templates. A new
multi-agent algorithm is proposed to automatically detect and mitigate
hallucinations. CompLex demonstrates impressive performance improvements across
three state-of-the-art text-to-music generation models, encompassing both
symbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of
completeness, accuracy, non-redundancy, and executability, confirming that it
possesses the key characteristics of an effective lexicon.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [297] [Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data](https://arxiv.org/abs/2508.11693)
*Francisco López,Eduardo Di Santi,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: This paper proposes using AC track circuit data and SVM classifiers to enhance failure detection and maintenance in railway systems.


<details>
  <summary>Details</summary>
Motivation: Current track circuit-based train detection systems lack automated ways to precisely identify failing components, hindering efficient maintenance.

Method: The study introduces the use of a "Smart Train Detection System" (STDS), combining high- and low-frequency AC bands, and applies SVM classifiers to categorize 15 specific failures into 3 general categories.

Result: The model was field-tested across 10 different track circuits and validated by experts, achieving accurate classification for all failure use cases.

Conclusion: The proposed system successfully and automatically identifies failing components in track circuits, potentially streamlining railway maintenance efforts.

Abstract: Track Circuits (TC) are the main signalling devices used to detect the
presence of a train on a rail track. It has been used since the 19th century
and nowadays there are many types depending on the technology. As a general
classification, Track Circuits can be divided into 2 main groups, DC (Direct
Current) and AC (Alternating Current) circuits. This work is focused on a
particular AC track circuit, called "Smart Train Detection System" (STDS),
designed with both high and low-frequency bands. This approach uses STDS
current data applied to an SVM (support vector machine) classifier as a type of
failure identifier. The main purpose of this work consists on determine
automatically which is the component of the track that is failing to improve
the maintenance action. Model was trained to classify 15 different failures
that belong to 3 more general categories. The method was tested with field data
from 10 different track circuits and validated by the STDS track circuit expert
and maintainers. All use cases were correctly classified by the method.

</details>


### [298] [Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation](https://arxiv.org/abs/2508.19660)
*Vojtech Mrazek,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Zdenek Vasicek,Mehdi B. Tahoori,Georgios Zervakis*

Main category: eess.SP

TL;DR: The paper introduces a framework to enhance printed neural networks for better area efficiency and power consumption, focusing on printed electronics' applications beyond silicon systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of designing complex circuits in printed neural networks while balancing accuracy and resource efficiency, particularly in processing-near-sensor systems.

Method: The method involves an automated framework that designs printed Ternary Neural Networks with multi-objective optimization and holistic approximation, allowing for arbitrary input precision.

Result: The circuits designed using this framework achieve 17x improvement in area efficiency and 59x improvement in power efficiency compared to existing solutions, with under 5% accuracy loss.

Conclusion: This framework demonstrates significant advancements in enabling printed-battery-powered neural networks for flexible, stretchable, and low-cost applications, making complex printed neural networks practical.

Abstract: Printed electronics offer a promising alternative for applications beyond
silicon-based systems, requiring properties like flexibility, stretchability,
conformality, and ultra-low fabrication costs. Despite the large feature sizes
in printed electronics, printed neural networks have attracted attention for
meeting target application requirements, though realizing complex circuits
remains challenging. This work bridges the gap between classification accuracy
and area efficiency in printed neural networks, covering the entire
processing-near-sensor system design and co-optimization from the
analog-to-digital interface-a major area and power bottleneck-to the digital
classifier. We propose an automated framework for designing printed Ternary
Neural Networks with arbitrary input precision, utilizing multi-objective
optimization and holistic approximation. Our circuits outperform existing
approximate printed neural networks by 17x in area and 59x in power on average,
being the first to enable printed-battery-powered operation with under 5%
accuracy loss while accounting for analog-to-digital interfacing costs.

</details>


### [299] [Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites](https://arxiv.org/abs/2508.19657)
*Jorge L. González-Rios,Liz Martínez Marrero,Juan Duncan,Luis M. Garcés-Socarrás,Raudel Cuiman Marquez,Juan A. Vásquez Peralvo,Jevgenij Krivochiza,Symeon Chatzinotas,Björn Ottersten*

Main category: eess.SP

TL;DR: The paper presents a testbed design using software-defined radio for studying precoding in MEO satellite systems, mitigating orbit-related challenges.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the orbit dynamics of MEO satellites in providing broadband Internet connectivity and ensuring efficient multi-beam communication with technologies like precoding.

Method: An in-lab testbed with software-defined radio platforms simulates MEO satellite operations, incorporating orbit modeling, antenna radiation patterns, and a synchronization loop to address impairments like Doppler shifts and phase noise.

Result: Preliminary experimental outcomes demonstrate that the testbed effectively validates and supports efficient precoding under MEO satellite conditions.

Conclusion: The proposed testbed and synchronization mechanisms can successfully address critical impairments, paving the way for reliable broadband satellite communications in MEO orbit.

Abstract: The use of communication satellites in medium Earth orbit (MEO) is foreseen
to provide quasi-global broadband Internet connectivity in the coming
networking ecosystems. Multi-user multiple-input single-output (MU-MISO)
digital signal processing techniques, such as precoding, emerge as appealing
technological enablers in the forward link of multi-beam satellite systems
operating in full frequency reuse (FFR). However, the orbit dynamics of MEO
satellites pose additional challenges that must be carefully evaluated and
addressed. This work presents the design of an in-lab testbed based on
software-defined radio (SDR) platforms and the corresponding adaptations
required for efficient precoding in a MEO scenario. The setup incorporates a
precise orbit model and the radiation pattern of a custom-designed direct
radiating array (DRA). We analyze the main impairments affecting precoding
performance, including Doppler shifts and payload phase noise, and propose a
synchronization loop to mitigate these effects. Preliminary experimental
results validate the feasibility and effectiveness of the proposed solution.

</details>


### [300] [Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning](https://arxiv.org/abs/2508.11692)
*Eduardo Di Santi,Ruixiang Ci,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Jonathan Brown,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: This paper proposes a deep learning approach to detect anomalies in point machines, focusing solely on power consumption patterns to classify nominal and failure states, achieving high precision and scalability.


<details>
  <summary>Details</summary>
Motivation: Point Machine failures disrupt railway service, requiring preemptive anomaly detection methods that address limitations in existing approaches, such as heavy dependency on specific inputs and conditions.

Method: A deep learning model processes power signal patterns of Point Machines to classify operational states. The method requires only one input, incorporates conformal prediction, and adheres to ISO-17359 standards.

Result: The proposed approach achieved >99.99% precision, less than 0.01% false positives, and negligible false negatives across various electromechanical Point Machines in real-world and test environments.

Conclusion: The methodology is generic, scalable, technology-agnostic, and provides confidence in outputs, offering a robust solution for Point Machine anomaly detection and compliant maintenance practices.

Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches
train routes by diverting tracks through a switchblade. As with any critical
safety equipment, a failure will halt operations leading to service
disruptions; therefore, pre-emptive maintenance may avoid unnecessary
interruptions by detecting anomalies before they become failures. Previous work
relies on several inputs and crafting custom features by segmenting the signal.
This not only adds additional requirements for data collection and processing,
but it is also specific to the PM technology, the installed locations and
operational conditions limiting scalability. Based on the available maintenance
records, the main failure causes for PM are obstacles, friction, power source
issues and misalignment. Those failures affect the energy consumption pattern
of PMs, altering the usual (or healthy) shape of the power signal during the PM
movement. In contrast to the current state-of-the-art, our method requires only
one input. We apply a deep learning model to the power signal pattern to
classify if the PM is nominal or associated with any failure type, achieving
>99.99\% precision, <0.01\% false positives and negligible false negatives. Our
methodology is generic and technology-agnostic, proven to be scalable on
several electromechanical PM types deployed in both real-world and test bench
environments. Finally, by using conformal prediction the maintainer gets a
clear indication of the certainty of the system outputs, adding a confidence
layer to operations and making the method compliant with the ISO-17359
standard.

</details>


### [301] [Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks](https://arxiv.org/abs/2508.19566)
*Chen Shang,Jiadong Yu,Dinh Thai Hoang*

Main category: eess.SP

TL;DR: The study introduces an energy-efficient deep reinforcement learning (DRL)-based beamforming approach for V2X networks, integrating spiking neural networks (SNNs) for improved energy efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in energy efficiency and performance optimization for beamforming in dynamic and uncertain V2X environments.

Method: Model the V2X network as a Markov Decision Process for decision-making and employ a DRL framework integrated with SNNs to optimize beamforming and power allocation.

Result: Simulation results show considerable energy savings and enhanced communication throughput and sensing accuracy compared to traditional methods.

Conclusion: The proposed scheme effectively achieves energy-efficient and robust beamforming for V2X networks, paving the way for green and sustainable connectivity solutions.

Abstract: This work proposes an energy-efficient, learning-based beamforming scheme for
integrated sensing and communication (ISAC)-enabled V2X networks. Specifically,
we first model the dynamic and uncertain nature of V2X environments as a Markov
Decision Process. This formulation allows the roadside unit to generate
beamforming decisions based solely on current sensing information, thereby
eliminating the need for frequent pilot transmissions and extensive channel
state information acquisition. We then develop a deep reinforcement learning
(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring
both communication throughput and sensing accuracy in highly dynamic scenario.
To address the high energy demands of conventional learning-based schemes, we
embed spiking neural networks (SNNs) into the DRL framework. Leveraging their
event-driven and sparsely activated architecture, SNNs significantly enhance
energy efficiency while maintaining robust performance. Simulation results
confirm that the proposed method achieves substantial energy savings and
superior communication performance, demonstrating its potential to support
green and sustainable connectivity in future V2X systems.

</details>


### [302] [Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission](https://arxiv.org/abs/2508.19910)
*Sergio Hernandez,Christophe Peucheret,Francesco Da Ros,Darko Zibar*

Main category: eess.SP

TL;DR: This study uses a data-driven surrogate model for end-to-end optimization in DML-based communication systems, achieving superior performance while reducing resource usage.


<details>
  <summary>Details</summary>
Motivation: DML-based systems are attractive for short-reach communication systems but pose challenges due to complex nonlinear dynamics.

Method: End-to-end optimization using a data-driven surrogate model trained on experimental data, optimizing pulse shaping, equalizer filters, bias current, and modulation RF power.

Result: The approach yields better performance compared to existing benchmarks across various conditions while requiring lower RF power, fewer filter taps, and smaller bandwidth.

Conclusion: Data-driven end-to-end optimization is effective and resource-efficient for enhancing DML-based communication systems.

Abstract: Directly modulated lasers (DMLs) are an attractive technology for short-reach
intensity modulation and direct detection communication systems. However, their
complex nonlinear dynamics make the modeling and optimization of DML-based
systems challenging. In this paper, we study the end-to-end optimization of
DML-based systems based on a data-driven surrogate model trained on
experimental data. The end-to-end optimization includes the pulse shaping and
equalizer filters, the bias current and the modulation radio-frequency (RF)
power applied to the laser. The performance of the end-to-end optimization
scheme is tested on the experimental setup and compared to 4 different
benchmark schemes based on linear and nonlinear receiver-side equalization. The
results show that the proposed end-to-end scheme is able to deliver better
performance throughout the studied symbol rates and transmission distances
while employing lower modulation RF power, fewer filter taps and utilizing a
smaller signal bandwidth.

</details>


### [303] [Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge](https://arxiv.org/abs/2508.19637)
*Maha Shatta,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Georgios Panagopoulos,Mehdi B. Tahoori,Georgios Zervakis*

Main category: eess.SP

TL;DR: The paper introduces a framework to optimize feature extraction and classification for healthcare wearable devices in flexible electronics, addressing area and power challenges.


<details>
  <summary>Details</summary>
Motivation: Flexible Electronics (FE) are promising for wearable healthcare devices due to their lightweight and low-cost nature but face integration density challenges. Researchers aim to mitigate the area and power constraints for these systems.

Method: The authors developed an analog feature extractor for FE and proposed a hardware-aware NAS-inspired feature selection strategy during ML training for efficient and specific designs.

Result: Evaluation on healthcare benchmarks demonstrated the proposed framework's high accuracy and ultra-area efficiency in designing flexible, disposable, low-power wearable systems.

Conclusion: The proposed mixed-signal co-design framework successfully optimizes FE-based wearable systems, advancing solutions for cost-effective and efficient healthcare monitoring.

Abstract: Flexible Electronics (FE) offer a promising alternative to rigid
silicon-based hardware for wearable healthcare devices, enabling lightweight,
conformable, and low-cost systems. However, their limited integration density
and large feature sizes impose strict area and power constraints, making
ML-based healthcare systems-integrating analog frontend, feature extraction and
classifier-particularly challenging. Existing FE solutions often neglect
potential system-wide solutions and focus on the classifier, overlooking the
substantial hardware cost of feature extraction and Analog-to-Digital
Converters (ADCs)-both major contributors to area and power consumption. In
this work, we present a holistic mixed-signal feature-to-classifier co-design
framework for flexible smart wearable systems. To the best of our knowledge, we
design the first analog feature extractors in FE, significantly reducing
feature extraction cost. We further propose an hardware-aware NAS-inspired
feature selection strategy within ML training, enabling efficient,
application-specific designs. Our evaluation on healthcare benchmarks shows our
approach delivers highly accurate, ultra-area-efficient flexible systems-ideal
for disposable, low-power wearable monitoring.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [304] [Robust Recursive Query Parallelism in Graph Database Management Systems](https://arxiv.org/abs/2508.19379)
*Anurag Chakraborty,Semih Salihoğlu*

Main category: cs.DB

TL;DR: This paper analyzes the efficiency of parallelizing recursive join queries in graph database management systems (GDBMSs), proposing a hybrid morsel dispatching policy that outperforms existing methods in diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of multi-core parallel processing for recursive join queries in GDBMSs, which is crucial for high performance.

Method: The paper explores various morsel dispatching policies, particularly combining source-node-level and frontier-level parallelism into a hybrid policy. It models multi-source optimizations and embeds these within the Kuzu GDBMS for evaluation.

Result: The hybrid policy outperforms individual source-only and frontier-only policies and adapts well across diverse workloads; multi-source morsels reduce scan amounts when there are sufficient query sources.

Conclusion: The hybrid morsel dispatching policy is a robust and effective strategy for parallelizing recursive queries, and multi-source morseling can further enhance performance under specific conditions.

Abstract: Efficient multi-core parallel processing of recursive join queries is
critical for achieving good performance in graph database management systems
(GDBMSs). Prior work adopts two broad approaches. First is the state of the art
morsel-driven parallelism, whose vanilla application in GDBMSs parallelizes
computations at the source node level. Second is to parallelize each iteration
of the computation at the frontier level. We show that these approaches can be
seen as part of a design space of morsel dispatching policies based on picking
different granularities of morsels. We then empirically study the question of
which policies parallelize better in practice under a variety of datasets and
query workloads that contain one to many source nodes. We show that these two
policies can be combined in a hybrid policy that issues morsels both at the
source node and frontier levels. We then show that the multi-source
breadth-first search optimization from prior work can also be modeled as a
morsel dispatching policy that packs multiple source nodes into multi-source
morsels. We implement these policies inside a single system, the Kuzu GDBMS,
and evaluate them both within Kuzu and across other systems. We show that the
hybrid policy captures the behavior of both source morsel-only and frontier
morsel-only policies in cases when these approaches parallelize well, and
out-perform them on queries when they are limited, and propose it as a robust
approach to parallelizing recursive queries. We further show that assigning
multi-sources is beneficial, as it reduces the amount of scans, but only when
there is enough sources in the query.

</details>


### [305] [Bootstrapping Learned Cost Models with Synthetic SQL Queries](https://arxiv.org/abs/2508.19807)
*Michael Nidd,Christoph Miksovic,Thomas Gschwind,Francesco Fusco,Andrea Giovannini,Ioana Giurgiu*

Main category: cs.DB

TL;DR: The paper demonstrates using synthetic data generation techniques inspired by generative AI and LLMs to enhance learned cost model training for SQL query cost prediction, reducing the required queries by 45%.


<details>
  <summary>Details</summary>
Motivation: Realistic workloads are crucial for database stress testing, vulnerability assessments, and optimizing cost-performance. Learned cost models require diverse SQL queries to accurately predict costs, prompting a need for effective dataset generation methods.

Method: Modern synthetic data generation techniques, influenced by generative AI and LLMs, were utilized to create high-quality datasets for training learned cost models.

Result: The learned cost model achieved higher predictive accuracy while requiring 45% fewer SQL queries compared to competitive data generation methods.

Conclusion: Integrating generative AI-inspired synthetic data generation approaches significantly improves training efficiency and prediction accuracy for learned cost models in database query analysis.

Abstract: Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [306] [Modeling spectral filtering effects on color-matching functions: Implications for observer variability](https://arxiv.org/abs/2508.19291)
*Luvin Munish Ragoo,Ivar Farup,Casper F. Andersen,Graham Finlayson*

Main category: astro-ph.IM

TL;DR: The study explores how spectral filtering affects color-matching functions (CMFs) and models observer variability, proposing a single filter approach for efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand how spectral filtering impacts color perception and to streamline the representation of observer variability in CMFs.

Method: Color matching experiments with and without a spectral filter were conducted, along with a computational estimation of filter transmittance and transformation matrices.

Result: A 'yellow' filter was identified that converts between SB1955 and ICVIO datasets, consistent with age-related lens yellowing affecting CMFs.

Conclusion: Spectral filtering can effectively model observer variability through a single filter transformation, simplifying experimental processes and maintaining accuracy.

Abstract: This study investigates the impact of spectral filtering on color-matching
functions (CMFs) and its implications for observer variability modeling. We
conducted color matching experiments with a single observer, both with and
without a spectral filter in front of a bipartite field. Using a novel
computational approach, we estimated the filter transmittance and
transformation matrix necessary to convert unfiltered CMFs to filtered CMFs.
Statistical analysis revealed good agreement between estimated and measured
filter characteristics, particularly in central wavelength regions. Applying
this methodology to compare between Stiles and Burch 1955 (SB1955) mean
observer CMFs and our previously published "ICVIO" mean observer CMFs, we
identified a "yellow" (short-wavelength suppressing) filter that effectively
transforms between these datasets. This finding aligns with our hypothesis that
observed differences between the CMF sets are attributable to age-related lens
yellowing (average observer age: 49 years in ICVIO versus 30 years in SB1955).
Our approach enables efficient representation of observer variability through a
single filter rather than three separate functions, offering potentially
reduced experimental overhead while maintaining accuracy in characterizing
individual color vision differences.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [307] [Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models](https://arxiv.org/abs/2508.19269)
*Ke Zhou,Marios Constantinides,Daniele Quercia*

Main category: cs.CY

TL;DR: This paper evaluates large language models (LLMs), revealing that cultural diversity in model responses increases but also the risk of violating human rights.


<details>
  <summary>Details</summary>
Motivation: Concerns about cultural bias and fairness due to WEIRD values dominating training datasets of LLMs.

Method: Used responses from the World Values Survey and measured alignment to WEIRD values, human rights principles, and regional charters.

Result: Models with less WEIRD alignment, like BLOOM and Qwen, were more culturally diverse but also 2%-4% more prone to human rights violations.

Conclusion: Increasing cultural representation in LLMs leads to more discrimination risks, and approaches like Constitutional AI only partially address these issues.

Abstract: Large language models (LLMs) are often trained on data that reflect WEIRD
values: Western, Educated, Industrialized, Rich, and Democratic. This raises
concerns about cultural bias and fairness. Using responses to the World Values
Survey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and
Qwen. We measured how closely these responses aligned with the values of the
WEIRD countries and whether they conflicted with human rights principles. To
reflect global diversity, we compared the results with the Universal
Declaration of Human Rights and three regional charters from Asia, the Middle
East, and Africa. Models with lower alignment to WEIRD values, such as BLOOM
and Qwen, produced more culturally varied responses but were 2% to 4% more
likely to generate outputs that violated human rights, especially regarding
gender and equality. For example, some models agreed with the statements ``a
man who cannot father children is not a real man'' and ``a husband should
always know where his wife is'', reflecting harmful gender norms. These
findings suggest that as cultural representation in LLMs increases, so does the
risk of reproducing discriminatory beliefs. Approaches such as Constitutional
AI, which could embed human rights principles into model behavior, may only
partly help resolve this tension.

</details>


### [308] [Epistemic Trade-Off: An Analysis of the Operational Breakdown and Ontological Limits of "Certainty-Scope" in AI](https://arxiv.org/abs/2508.19304)
*Generoso Immediato*

Main category: cs.CY

TL;DR: Floridi's conjecture suggests a trade-off between certainty and scope in AI, but its practical applicability is limited by its reliance on incomputable constructs and its detachment from socio-technical contexts.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to evaluate Floridi's conjecture, which is seen as important for guiding AI investments, particularly in safety-critical industrial contexts.

Method: Examines the limitations of Floridi's conjecture by analyzing its reliance on incomputable constructs and the ontological assumption of AI as a self-contained epistemic entity.

Result: Identifies two main constraints: epistemic closure deficit and embeddedness bypass, which hinder the conjecture's practical utility for AI design and governance.

Conclusion: Floridi's conjecture, while insightful, cannot currently be operationalized in real-world AI systems without addressing its epistemic and ontological limitations. The paper reframes the conjecture to tackle these burdens in complex human-centric domains.

Abstract: Floridi's conjecture offers a compelling intuition about the fundamental
trade-off between certainty and scope in artificial intelligence (AI) systems.
This exploration remains crucial, not merely as a philosophical exercise, but
as a potential compass for guiding AI investments, particularly in
safety-critical industrial domains where the level of attention will surely be
higher in the future. However, while intellectually coherent, its formalization
ultimately freezes this insight into a suspended epistemic truth, resisting
operationalization within real-world systems. This paper is a result of an
analysis arguing that the conjecture's ambition to provide insights to
engineering design and regulatory decision-making is constrained by two
critical factors: first, its reliance on incomputable constructs - rendering it
practically unactionable and unverifiable; second, its underlying ontological
assumption of AI systems as self-contained epistemic entities - separating it
from the intricate and dynamic socio-technical environments in which knowledge
is co-constructed. We conclude that this dual breakdown - an epistemic closure
deficit and an embeddedness bypass - prevents the conjecture from transitioning
into a computable and actionable framework suitable for informing the design,
deployment, and governance of real-world AI hybrid systems. In response, we
propose a contribution to the framing of Floridi's epistemic challenge,
addressing the inherent epistemic burdens of AI within complex human-centric
domains.

</details>


### [309] [Are Companies Taking AI Risks Seriously? A Systematic Analysis of Companies' AI Risk Disclosures in SEC 10-K forms](https://arxiv.org/abs/2508.19313)
*Lucas G. Uberti-Bona Marin,Bram Rijsbosch,Gerasimos Spanakis,Konrad Kollnig*

Main category: cs.CY

TL;DR: This paper analyzes the reporting of AI-related risks in corporate SEC 10-K filings, highlighting notable growth but concerns over the lack of detailed mitigations.


<details>
  <summary>Details</summary>
Motivation: With increasing integration of AI in corporate strategies alongside growing regulatory requirements, the paper seeks to evaluate transparency and quality in companies' AI risk disclosures.

Method: The study performed quantitative and qualitative analysis on over 30,000 SEC filings from 7,000+ companies spanning five years, tracking AI-related mentions and trends.

Result: A surge in AI risk mentions in filings was observed (from 4% in 2020 to 43% in 2024), with focus on legal, competitive, and societal risks, though disclosures often lacked depth in mitigation details.

Conclusion: The paper underscores inadequacies in AI risk disclosures and supports further research through their publicly released tool for analyzing SEC filings.

Abstract: As Artificial Intelligence becomes increasingly central to corporate
strategies, concerns over its risks are growing too. In response, regulators
are pushing for greater transparency in how companies identify, report and
mitigate AI-related risks. In the US, the Securities and Exchange Commission
(SEC) repeatedly warned companies to provide their investors with more accurate
disclosures of AI-related risks; recent enforcement and litigation against
companies' misleading AI claims reinforce these warnings. In the EU, new laws -
like the AI Act and Digital Services Act - introduced additional rules on AI
risk reporting and mitigation. Given these developments, it is essential to
examine if and how companies report AI-related risks to the public. This study
presents the first large-scale systematic analysis of AI risk disclosures in
SEC 10-K filings, which require public companies to report material risks to
their company. We analyse over 30,000 filings from more than 7,000 companies
over the past five years, combining quantitative and qualitative analysis. Our
findings reveal a sharp increase in the companies that mention AI risk, up from
4% in 2020 to over 43% in the most recent 2024 filings. While legal and
competitive AI risks are the most frequently mentioned, we also find growing
attention to societal AI risks, such as cyberattacks, fraud, and technical
limitations of AI systems. However, many disclosures remain generic or lack
details on mitigation strategies, echoing concerns raised recently by the SEC
about the quality of AI-related risk reporting. To support future research, we
publicly release a web-based tool for easily extracting and analysing
keyword-based disclosures across SEC filings.

</details>


### [310] [What Makes AI Applications Acceptable or Unacceptable? A Predictive Moral Framework](https://arxiv.org/abs/2508.19317)
*Kimmo Eriksson,Simon Karlsson,Irina Vartanova,Pontus Strimling*

Main category: cs.CY

TL;DR: A study with 587 U.S. participants explores public moral acceptance of 100 AI applications, finding that judgments are systematic and predictable based on five core moral qualities.


<details>
  <summary>Details</summary>
Motivation: Developers and policymakers struggle to foresee which AI applications will face moral resistance from the public.

Method: A preregistered study with 587 participants used a taxonomy of 100 AI applications and evaluated acceptability ratings based on five moral qualities.

Result: Over 90% of acceptability variance was explained by perceived risk, benefit, dishonesty, unnaturalness, and reduced accountability. The framework proved predictive for collective and individual judgments.

Conclusion: Public evaluations of AI technologies follow a structured moral psychology, providing insights to anticipate resistance and guide responsible innovation in AI.

Abstract: As artificial intelligence rapidly transforms society, developers and
policymakers struggle to anticipate which applications will face public moral
resistance. We propose that these judgments are not idiosyncratic but
systematic and predictable. In a large, preregistered study (N = 587, U.S.
representative sample), we used a comprehensive taxonomy of 100 AI applications
spanning personal and organizational contexts-including both functional uses
and the moral treatment of AI itself. In participants' collective judgment,
applications ranged from highly unacceptable to fully acceptable. We found this
variation was strongly predictable: five core moral qualities-perceived risk,
benefit, dishonesty, unnaturalness, and reduced accountability-collectively
explained over 90% of the variance in acceptability ratings. The framework
demonstrated strong predictive power across all domains and successfully
predicted individual-level judgments for held-out applications. These findings
reveal that a structured moral psychology underlies public evaluation of new
technologies, offering a powerful tool for anticipating public resistance and
guiding responsible innovation in AI.

</details>


### [311] [Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models](https://arxiv.org/abs/2508.19492)
*Mehmet Can Yavuz,Humza Gohar Kabir,Aylin Özkan*

Main category: cs.CY

TL;DR: The study explores biases in large language models from Chinese and Western origins, examining their subjectivity and quality assessments in news coverage. It finds systematic biases aligned with geopolitical origins.


<details>
  <summary>Details</summary>
Motivation: To investigate whether large language models (LLMs) exhibit geopolitical biases in news content quality and subjectivity assessments, shaped by their cultural and ideological training data.

Method: The study compares LLMs of Chinese (Qwen, BGE, Jina) and Western origins (Snowflake, Granite) through article embeddings, regression probes, and evaluations on human-annotated benchmarks across linguistic, informational, and emotional metrics.

Result: The study found distinct and consistent divergences: Western models favored subjectivity and positive emotion scores, while Chinese models prioritized novelty and descriptiveness. Cross-topic analysis revealed asymmetries, such as poor fluency and conciseness in Chinese-on-US results but higher negativity.

Conclusion: Geopolitical biases influence LLM-based evaluation of news content. Media evaluation pipelines need cultural adjustments to distinguish model-induced biases from content differences.

Abstract: Objectivity in journalism has long been contested, oscillating between ideals
of neutral, fact-based reporting and the inevitability of subjective framing.
With the advent of large language models (LLMs), these tensions are now
mediated by algorithmic systems whose training data and design choices may
themselves embed cultural or ideological biases. This study investigates
geopolitical parallax-systematic divergence in news quality and subjectivity
assessments-by comparing article-level embeddings from Chinese-origin (Qwen,
BGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate
both on a human-annotated news quality benchmark spanning fifteen stylistic,
informational, and affective dimensions, and on parallel corpora covering
politically sensitive topics, including Palestine and reciprocal China-United
States coverage. Using logistic regression probes and matched-topic evaluation,
we quantify per-metric differences in predicted positive-class probabilities
between model families. Our findings reveal consistent, non-random divergences
aligned with model origin. In Palestine-related coverage, Western models assign
higher subjectivity and positive emotion scores, while Chinese models emphasize
novelty and descriptiveness. Cross-topic analysis shows asymmetries in
structural quality metrics Chinese-on-US scoring notably lower in fluency,
conciseness, technicality, and overall quality-contrasted by higher negative
emotion scores. These patterns align with media bias theory and our distinction
between semantic, emotional, and relational subjectivity, and extend LLM bias
literature by showing that geopolitical framing effects persist in downstream
quality assessment tasks. We conclude that LLM-based media evaluation pipelines
require cultural calibration to avoid conflating content differences with
model-induced bias.

</details>


### [312] [Hallucinating with AI: AI Psychosis as Distributed Delusions](https://arxiv.org/abs/2508.19588)
*Lucy Osler*

Main category: cs.CY

TL;DR: The paper examines the phenomenon of AI 'hallucinatory' outputs and introduces a broader perspective on how human-AI interactions can lead to distorted beliefs, narratives, and shared cognitive errors.


<details>
  <summary>Details</summary>
Motivation: Understanding the dynamic role of AI in human cognition, particularly its ability to affirm or distort human thoughts in distributed cognitive processes.

Method: The author applies distributed cognition theory to analyze the development of inaccurate beliefs and delusional thinking in human-AI interactions.

Result: The paper identifies mechanisms through which AI contributes to cognitive distortions by sustaining and elaborating on a user's delusional tendencies, emphasizing the role of chatbots as both cognitive tools and quasi-interpersonal entities.

Conclusion: Generative AI systems have a dual-function that amplifies their impact within distributed cognition, prompting users to co-construct potentially distorted beliefs and narratives with AI, making it an unusual and compelling subject of study.

Abstract: There is much discussion of the false outputs that generative AI systems such
as ChatGPT, Claude, Gemini, DeepSeek, and Grok create. In popular terminology,
these have been dubbed AI hallucinations. However, deeming these AI outputs
hallucinations is controversial, with many claiming this is a metaphorical
misnomer. Nevertheless, in this paper, I argue that when viewed through the
lens of distributed cognition theory, we can better see the dynamic and
troubling ways in which inaccurate beliefs, distorted memories and
self-narratives, and delusional thinking can emerge through human-AI
interactions; examples of which are popularly being referred to as cases of AI
psychosis. In such cases, I suggest we move away from thinking about how an AI
system might hallucinate at us, by generating false outputs, to thinking about
how, when we routinely rely on generative AI to help us think, remember, and
narrate, we can come to hallucinate with AI. This can happen when AI introduces
errors into the distributed cognitive process, but it can also happen when AI
sustains, affirms, and elaborates on our own delusional thinking and
self-narratives, such as in the case of Jaswant Singh Chail. I also examine how
the conversational style of chatbots can lead them to play a dual-function,
both as a cognitive artefact and a quasi-Other with whom we co-construct our
beliefs, narratives, and our realities. It is this dual function, I suggest,
that makes generative AI an unusual, and particularly seductive, case of
distributed cognition.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [313] [Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents](https://arxiv.org/abs/2508.19504)
*Kevin Song,Anand Jayarajan,Yaoyao Ding,Qidong Su,Zhanda Zhu,Sihang Liu,Gennady Pekhimenko*

Main category: cs.MA

TL;DR: This paper focuses on improving the success rates of large language model (LLM) agents in complex tasks by optimizing the system environment, rather than modifying the agents themselves.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents struggle with low success rates under complex real-world conditions, and prior research has mainly focused on improving the agents while neglecting the role of the operating environment.

Method: The paper analyzes 142 agent traces across 5 benchmarks to create a taxonomy of 6 failure modes in agent-environment interactions. Based on this analysis, it introduces Aegis, a set of environmental optimizations including environment observability enhancement, computation offloading, and speculative agentic actions.

Result: The proposed techniques improve agent success rates by 6.7-12.5% across various benchmarks without making changes to the agents or the underlying LLMs.

Conclusion: Optimizing the operating environment for LLM agents can significantly enhance their task success rates, presenting a complementary approach to focusing solely on agent improvements.

Abstract: Large Language Models (LLMs) agents augmented with domain tools promise to
autonomously execute complex tasks requiring human-level intelligence, such as
customer service and digital assistance. However, their practical deployment is
often limited by their low success rates under complex real-world environments.
To tackle this, prior research has primarily focused on improving the agents
themselves, such as developing strong agentic LLMs, while overlooking the role
of the system environment in which the agent operates.
  In this paper, we study a complementary direction: improving agent success
rates by optimizing the system environment in which the agent operates. We
collect 142 agent traces (3,656 turns of agent-environment interactions) across
5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we
propose a taxonomy for agent-environment interaction failures that includes 6
failure modes. Guided by these findings, we design Aegis, a set of targeted
environment optimizations: 1) environment observability enhancement, 2) common
computation offloading, and 3) speculative agentic actions. These techniques
improve agent success rates on average by 6.7-12.5%, without any modifications
to the agent and underlying LLM.

</details>


### [314] [Anomaly Detection in Networked Bandits](https://arxiv.org/abs/2508.20076)
*Xiaotong Cheng,Setareh Maghsudi*

Main category: cs.MA

TL;DR: The paper introduces a bandit algorithm to improve personalized recommendations and anomaly detection by integrating network structure knowledge.


<details>
  <summary>Details</summary>
Motivation: To design online learning algorithms capable of learning user preferences while detecting anomalies, addressing issues like abnormal node behaviors in social networks.

Method: The proposed algorithm characterizes user preferences and residuals by leveraging network knowledge to simultaneously recommend and detect anomalies. Theoretical proofs and experiments are presented.

Result: The authors provide an upper bound on the regret of their method and demonstrate its effectiveness compared to existing algorithms using synthetic and real-world datasets.

Conclusion: Their network-aware bandit algorithm efficiently balances personalized recommendations and anomaly detection, advancing the state-of-the-art in this domain.

Abstract: The nodes' interconnections on a social network often reflect their
dependencies and information-sharing behaviors. Nevertheless, abnormal nodes,
which significantly deviate from most of the network concerning patterns or
behaviors, can lead to grave consequences. Therefore, it is imperative to
design efficient online learning algorithms that robustly learn users'
preferences while simultaneously detecting anomalies.
  We introduce a novel bandit algorithm to address this problem. Through
network knowledge, the method characterizes the users' preferences and
residuals of feature information. By learning and analyzing these preferences
and residuals, it develops a personalized recommendation strategy for each user
and simultaneously detects anomalies. We rigorously prove an upper bound on the
regret of the proposed algorithm and experimentally compare it with several
state-of-the-art collaborative contextual bandit algorithms on both synthetic
and real-world datasets.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [315] [The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology](https://arxiv.org/abs/2508.19914)
*Muhammad Waqas,Rukhmini Bandyopadhyay,Eman Showkatian,Amgad Muneer,Anas Zafar,Frank Rojas Alvarez,Maricel Corredor Marin,Wentao Li,David Jaffray,Cara Haymaker,John Heymach,Natalie I Vokes,Luisa Maren Solis Soto,Jianjun Zhang,Jia Wu*

Main category: q-bio.QM

TL;DR: EAGLE-Net is a MIL framework augmenting foundation models in computational pathology with spatial encoding, attention mechanisms, and noise suppression, achieving improved accuracy and interpretability across pan-cancer datasets.


<details>
  <summary>Details</summary>
Motivation: Foundation models in computational pathology lack mechanisms to integrate local and global spatial tissue structure crucial for understanding tumor microenvironments. MIL frameworks aim to bridge this gap.

Method: The paper proposes EAGLE-Net, which integrates multi-scale absolute spatial encoding, a top-K neighborhood-aware loss, and background suppression loss into the MIL framework to enhance predictions from foundation models.

Result: EAGLE-Net outperformed existing methods in cancer classification and survival prediction across large pan-cancer datasets, achieving smoother attention maps aligned with expert annotations and highlighting biologically relevant regions.

Conclusion: EAGLE-Net represents a generalizable and interpretable framework complementary to foundation models for biomarker discovery, prognostic modeling, and clinical decision-making.

Abstract: Foundation models have recently emerged as powerful feature extractors in
computational pathology, yet they typically omit mechanisms for leveraging the
global spatial structure of tissues and the local contextual relationships
among diagnostically relevant regions - key elements for understanding the
tumor microenvironment. Multiple instance learning (MIL) remains an essential
next step following foundation model, designing a framework to aggregate
patch-level features into slide-level predictions. We present EAGLE-Net, a
structure-preserving, attention-guided MIL architecture designed to augment
prediction and interpretability. EAGLE-Net integrates multi-scale absolute
spatial encoding to capture global tissue architecture, a top-K
neighborhood-aware loss to focus attention on local microenvironments, and
background suppression loss to minimize false positives. We benchmarked
EAGLE-Net on large pan-cancer datasets, including three cancer types for
classification (10,260 slides) and seven cancer types for survival prediction
(4,172 slides), using three distinct histology foundation backbones (REMEDIES,
Uni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher
classification accuracy and the top concordance indices in 6 of 7 cancer types,
producing smooth, biologically coherent attention maps that aligned with expert
annotations and highlighted invasive fronts, necrosis, and immune infiltration.
These results position EAGLE-Net as a generalizable, interpretable framework
that complements foundation models, enabling improved biomarker discovery,
prognostic modeling, and clinical decision support

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [316] [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](https://arxiv.org/abs/2508.19591)
*Jiakui Shen,Yunqi Mi,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: The paper addresses privacy issues in centralized recommender systems, proposing a novel federated strategy called PLGC to enhance personalization and address dimensional collapse.


<details>
  <summary>Details</summary>
Motivation: To tackle privacy leakage in centralized recommendation systems and overcome embedding degradation in federated training due to sparse interactions and heterogeneous user preferences.

Method: The paper introduces PLGC (Personalized Local-Global Collaboration), a model-agnostic strategy leveraging frozen global item embeddings and Neural Tangent Kernel dynamics with additional contrastive learning objectives.

Result: Extensive experiments on five datasets showcase PLGC's effectiveness, outperforming baseline models in personalization and embedding utility.

Conclusion: PLGC improves personalized federated recommendation systems by mitigating dimensional collapse, enhancing embedding quality, and achieving superior performance.

Abstract: Centralized recommender systems encounter privacy leakage due to the need to
collect user behavior and other private data. Hence, federated recommender
systems (FedRec) have become a promising approach with an aggregated global
model on the server. However, this distributed training paradigm suffers from
embedding degradation caused by suboptimal personalization and dimensional
collapse, due to the existence of sparse interactions and heterogeneous
preferences. To this end, we propose a novel model-agnostic strategy for FedRec
to strengthen the personalized embedding utility, which is called Personalized
Local-Global Collaboration (PLGC). It is the first research in federated
recommendation to alleviate the dimensional collapse issue. Particularly, we
incorporate the frozen global item embedding table into local devices. Based on
a Neural Tangent Kernel strategy that dynamically balances local and global
information, PLGC optimizes personalized representations during forward
inference, ultimately converging to user-specific preferences. Additionally,
PLGC carries on a contrastive objective function to reduce embedding redundancy
by dissolving dependencies between dimensions, thereby improving the backward
representation learning process. We introduce PLGC as a model-agnostic
personalized training strategy for federated recommendations that can be
applied to existing baselines to alleviate embedding degradation. Extensive
experiments on five real-world datasets have demonstrated the effectiveness and
adaptability of PLGC, which outperforms various baseline algorithms.

</details>


### [317] [A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation](https://arxiv.org/abs/2508.19507)
*Kyungho Kim,Sunwoo Kim,Geon Lee,Kijung Shin*

Main category: cs.IR

TL;DR: This paper introduces MEMBER, a multi-behavior recommender system, designed to improve recommendations for both visited and unvisited items in e-commerce by using a mixture-of-experts framework.


<details>
  <summary>Details</summary>
Motivation: Existing multi-behavior recommender systems underperform in recommending unvisited items and struggle to balance performance across visited and unvisited item categories.

Method: The paper proposes MEMBER, a mixture-of-experts framework with specialized self-supervised experts for handling visited and unvisited item recommendations.

Result: Comprehensive experiments demonstrate MEMBER's effectiveness, achieving up to 65.46% improvement in Hit Ratio@20 over the best competitor.

Conclusion: MEMBER significantly enhances the capability of multi-behavior recommender systems to balance recommendation quality across different item types in e-commerce settings.

Abstract: In e-commerce, where users face a vast array of possible item choices,
recommender systems are vital for helping them discover suitable items they
might otherwise overlook. While many recommender systems primarily rely on a
user's purchase history, recent multi-behavior recommender systems incorporate
various auxiliary user behaviors, such as item clicks and cart additions, to
enhance recommendations. Despite their overall performance gains, their
effectiveness varies considerably between visited items (i.e., those a user has
interacted with through auxiliary behaviors) and unvisited items (i.e., those
with which the user has had no such interactions). Specifically, our analysis
reveals that (1) existing multi-behavior recommender systems exhibit a
significant gap in recommendation quality between the two item types (visited
and unvisited items) and (2) achieving strong performance on both types with a
single model architecture remains challenging. To tackle these issues, we
propose a novel multi-behavior recommender system, MEMBER. It employs a
mixture-of-experts framework, with experts designed to recommend the two item
types, respectively. Each expert is trained using a self-supervised method
specialized for its design goal. In our comprehensive experiments, we show the
effectiveness of MEMBER across both item types, achieving up to 65.46\%
performance gain over the best competitor in terms of Hit Ratio@20.

</details>


### [318] [A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2508.19620)
*Yunqi Mi,Jiakui Shen,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: This paper reviews challenges and opportunities in federated recommender systems (FedRec), analyzing its integration from recommendation researchers' perspective.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in centralized recommender systems (CenRec) by leveraging federated learning (FL) architectures.

Method: A comprehensive review analyzing the coupling of FL frameworks and recommender systems, focusing on real-world recommendation scenarios.

Result: Identified scenario-specific challenges like statistical heterogeneity and connected recommendation scenarios to FL frameworks.

Conclusion: Provides practical guidance and bridges the gap between academic research and real-world deployment of FedRec.

Abstract: Extending recommender systems to federated learning (FL) frameworks to
protect the privacy of users or platforms while making recommendations has
recently gained widespread attention in academia. This is due to the natural
coupling of recommender systems and federated learning architectures: the data
originates from distributed clients (mostly mobile devices held by users),
which are highly related to privacy. In a centralized recommender system
(CenRec), the central server collects clients' data, trains the model, and
provides the service. Whereas in federated recommender systems (FedRec), the
step of data collecting is omitted, and the step of model training is offloaded
to each client. The server only aggregates the model and other knowledge, thus
avoiding client privacy leakage. Some surveys of federated recommender systems
discuss and analyze related work from the perspective of designing FL systems.
However, their utility drops by ignoring specific recommendation scenarios'
unique characteristics and practical challenges. For example, the statistical
heterogeneity issue in cross-domain FedRec originates from the label drift of
the data held by different platforms, which is mainly caused by the recommender
itself, but not the federated architecture. Therefore, it should focus more on
solving specific problems in real-world recommendation scenarios to encourage
the deployment FedRec. To this end, this review comprehensively analyzes the
coupling of recommender systems and federated learning from the perspective of
recommendation researchers and practitioners. We establish a clear link between
recommendation scenarios and FL frameworks, systematically analyzing
scenario-specific approaches, practical challenges, and potential
opportunities. We aim to develop guidance for the real-world deployment of
FedRec, bridging the gap between existing research and applications.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [319] [Eigenvalue distribution of the Neural Tangent Kernel in the quadratic scaling](https://arxiv.org/abs/2508.20036)
*Lucas Benigni,Elliot Paquette*

Main category: math.PR

TL;DR: The paper calculates the asymptotic eigenvalue distribution of a neural tangent kernel in a two-layer neural network under specific scaling conditions.


<details>
  <summary>Details</summary>
Motivation: The work aims to provide a deeper theoretical understanding of the behavior of neural tangent kernels under large-dimensional settings.

Method: The authors derive the eigenvalue distribution by employing techniques like free multiplicative convolution and leveraging randomness properties in high-dimensional matrices.

Result: The asymptotic eigenvalue distribution is determined as the free multiplicative convolution of the Marchenko--Pastur distribution with a deterministic distribution dependent on $\\sigma$ and $D$.

Conclusion: This finding offers a solid mathematical description of how neural tangent kernels behave in large-scale neural networks, enhancing their theoretical foundation.

Abstract: We compute the asymptotic eigenvalue distribution of the neural tangent
kernel of a two-layer neural network under a specific scaling of dimension.
Namely, if $X\in\mathbb{R}^{n\times d}$ is an i.i.d random matrix,
$W\in\mathbb{R}^{d\times p}$ is an i.i.d $\mathcal{N}(0,1)$ matrix and
$D\in\mathbb{R}^{p\times p}$ is a diagonal matrix with i.i.d bounded entries,
we consider the matrix
  \[
  \mathrm{NTK}
  =
  \frac{1}{d}XX^\top
  \odot
  \frac{1}{p}
  \sigma'\left(
  \frac{1}{\sqrt{d}}XW
  \right)D^2
  \sigma'\left(
  \frac{1}{\sqrt{d}}XW
  \right)^\top
  \]
  where $\sigma'$ is a pseudo-Lipschitz function applied entrywise and under
the scaling $\frac{n}{dp}\to \gamma_1$ and $\frac{p}{d}\to \gamma_2$. We
describe the asymptotic distribution as the free multiplicative convolution of
the Marchenko--Pastur distribution with a deterministic distribution depending
on $\sigma$ and $D$.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [320] [CellINR: Implicitly Overcoming Photo-induced Artifacts in 4D Live Fluorescence Microscopy](https://arxiv.org/abs/2508.19300)
*Cunmin Zhao,Ziyuan Luo,Guoye Guan,Zelin Li,Yiming Ma,Zhongying Zhao,Renjie Wan*

Main category: eess.IV

TL;DR: The paper introduces CellINR, a framework designed to optimize 4D live fluorescence microscopy by mitigating the effects of photobleaching and phototoxicity using implicit neural representation methods.


<details>
  <summary>Details</summary>
Motivation: Prolonged high-intensity illumination in fluorescence microscopy causes photobleaching and phototoxic effects that lead to artifacts and loss of image quality, hindering biological research and quantitative analyses.

Method: The CellINR framework utilizes blind convolution and structure amplification strategies to map 3D spatial coordinates into higher frequency domains, enabling precise reconstruction while separating true signals from artifacts.

Result: CellINR achieves superior results in artifact removal and restoration of structural continuity compared to existing techniques. The researchers also provide a paired 4D live cell imaging dataset for benchmarking.

Conclusion: CellINR significantly enhances image quality and continuity in 4D live fluorescence microscopy, offering robust tools and resources for further biological studies and quantitative evaluations.

Abstract: 4D live fluorescence microscopy is often compromised by prolonged high
intensity illumination which induces photobleaching and phototoxic effects that
generate photo-induced artifacts and severely impair image continuity and
detail recovery. To address this challenge, we propose the CellINR framework, a
case-specific optimization approach based on implicit neural representation.
The method employs blind convolution and structure amplification strategies to
map 3D spatial coordinates into the high frequency domain, enabling precise
modeling and high-accuracy reconstruction of cellular structures while
effectively distinguishing true signals from artifacts. Experimental results
demonstrate that CellINR significantly outperforms existing techniques in
artifact removal and restoration of structural continuity, and for the first
time, a paired 4D live cell imaging dataset is provided for evaluating
reconstruction performance, thereby offering a solid foundation for subsequent
quantitative analyses and biological research. The code and dataset will be
public.

</details>


### [321] [2D Ultrasound Elasticity Imaging of Abdominal Aortic Aneurysms Using Deep Neural Networks](https://arxiv.org/abs/2508.19303)
*Utsav Ratna Tuladhar,Richard Simon,Doran Mix,Michael Richards*

Main category: eess.IV

TL;DR: This paper develops a deep learning-based framework to assess tissue stiffness in abdominal aortic aneurysms (AAA) using 2D ultrasound for improved rupture risk evaluation, bypassing reliance on vessel diameter alone.


<details>
  <summary>Details</summary>
Motivation: Current AAA rupture risk assessments primarily rely on vascular diameter, which fails to account for critical material properties of the vessel wall, making the evaluation incomplete.

Method: Using a U-Net architecture model trained with normalized mean squared error (NMSE) loss, the framework predicts spatial modulus distributions from axial and lateral displacement fields generated through finite element simulations.

Result: The deep learning model achieved high accuracy in reconstructing modulus distributions (NMSE score of 0.73%) across simulated, phantom, and clinical datasets, outperforming an iterative approach in computation time.

Conclusion: The framework enables rapid and non-invasive tissue stiffness estimation, representing a promising tool for better AAA rupture risk evaluation in clinical settings.

Abstract: Abdominal aortic aneurysms (AAA) pose a significant clinical risk due to
their potential for rupture, which is often asymptomatic but can be fatal.
Although maximum diameter is commonly used for risk assessment, diameter alone
is insufficient as it does not capture the properties of the underlying
material of the vessel wall, which play a critical role in determining the risk
of rupture. To overcome this limitation, we propose a deep learning-based
framework for elasticity imaging of AAAs with 2D ultrasound. Leveraging finite
element simulations, we generate a diverse dataset of displacement fields with
their corresponding modulus distributions. We train a model with U-Net
architecture and normalized mean squared error (NMSE) to infer the spatial
modulus distribution from the axial and lateral components of the displacement
fields. This model is evaluated across three experimental domains: digital
phantom data from 3D COMSOL simulations, physical phantom experiments using
biomechanically distinct vessel models, and clinical ultrasound exams from AAA
patients. Our simulated results demonstrate that the proposed deep learning
model is able to reconstruct modulus distributions, achieving an NMSE score of
0.73\%. Similarly, in phantom data, the predicted modular ratio closely matches
the expected values, affirming the model's ability to generalize to phantom
data. We compare our approach with an iterative method which shows comparable
performance but higher computation time. In contrast, the deep learning method
can provide quick and effective estimates of tissue stiffness from ultrasound
images, which could help assess the risk of AAA rupture without invasive
procedures.

</details>


### [322] [MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction](https://arxiv.org/abs/2508.19319)
*Pardis Moradbeiki,Nasser Ghadiri,Sayed Jalal Zahabi,Uffe Kock Wiil,Kristoffer Kittelmann Brockhattingen,Ali Ebrahimi*

Main category: eess.IV

TL;DR: The paper introduces MedVQA-TREE, a multimodal AI framework for precise sarcopenia diagnosis using ultrasound with 99% accuracy, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Accurate sarcopenia diagnosis using ultrasound is challenging due to limited labeled data, subtle imaging cues, and lack of clinical context in existing models.

Method: The approach involves a multimodal framework with a hierarchical image interpretation module, gated feature-level fusion, and a novel multi-hop, multi-query knowledge retrieval strategy combining UMLS-guided clinical knowledge.

Result: MedVQA-TREE achieved up to 99% diagnostic accuracy on public MedVQA datasets and a custom sarcopenia dataset, surpassing previous state-of-the-art models by over 10%.

Conclusion: Structured visual understanding combined with guided knowledge retrieval significantly improves AI-assisted sarcopenia diagnosis, demonstrating the potential of multimodal frameworks.

Abstract: Accurate sarcopenia diagnosis via ultrasound remains challenging due to
subtle imaging cues, limited labeled data, and the absence of clinical context
in most models. We propose MedVQA-TREE, a multimodal framework that integrates
a hierarchical image interpretation module, a gated feature-level fusion
mechanism, and a novel multi-hop, multi-query retrieval strategy. The vision
module includes anatomical classification, region segmentation, and graph-based
spatial reasoning to capture coarse, mid-level, and fine-grained structures. A
gated fusion mechanism selectively integrates visual features with textual
queries, while clinical knowledge is retrieved through a UMLS-guided pipeline
accessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE
was trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA)
and a custom sarcopenia ultrasound dataset. The model achieved up to 99%
diagnostic accuracy and outperformed previous state-of-the-art methods by over
10%. These results underscore the benefit of combining structured visual
understanding with guided knowledge retrieval for effective AI-assisted
diagnosis in sarcopenia.

</details>


### [323] [AT-CXR: Uncertainty-Aware Agentic Triage for Chest X-rays](https://arxiv.org/abs/2508.19322)
*Xueyang Li,Mingze Jiang,Gelei Xu,Jun Xia,Mengzhao Jia,Danny Chen,Yiyu Shi*

Main category: eess.IV

TL;DR: The paper introduces AT-CXR, an uncertainty-aware agent for autonomous medical-imaging triage in chest X-rays, leveraging two novel router designs to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: There's a lack of exploration in truly autonomous systems for medical-imaging triage, which would work under real-world constraints to determine when to escalate, defer, or decide independently.

Method: The proposed system, AT-CXR, combines per-case confidence estimation and distributional fit to implement a stepwise decision policy. It utilizes two router designs: a deterministic rule-based router and a LLM-decided router for customization.

Result: Both router designs surpass existing zero-shot vision-language models and supervised classifiers in accuracy, selective-prediction performance, and latency, showing adaptability to practical clinical settings.

Conclusion: The AT-CXR system provides effective pathways for deploying autonomous medical triage systems by offering complementary designs to balance throughput and accuracy, supporting real-world applicability in clinical environments.

Abstract: Agentic AI is advancing rapidly, yet truly autonomous medical-imaging triage,
where a system decides when to stop, escalate, or defer under real constraints,
remains relatively underexplored. To address this gap, we introduce AT-CXR, an
uncertainty-aware agent for chest X-rays. The system estimates per-case
confidence and distributional fit, then follows a stepwise policy to issue an
automated decision or abstain with a suggested label for human intervention. We
evaluate two router designs that share the same inputs and actions: a
deterministic rule-based router and an LLM-decided router. Across five-fold
evaluation on a balanced subset of NIH ChestX-ray14 dataset, both variants
outperform strong zero-shot vision-language models and state-of-the-art
supervised classifiers, achieving higher full-coverage accuracy and superior
selective-prediction performance, evidenced by a lower area under the
risk-coverage curve (AURC) and a lower error rate at high coverage, while
operating with lower latency that meets practical clinical constraints. The two
routers provide complementary operating points, enabling deployments to
prioritize maximal throughput or maximal accuracy. Our code is available at
https://github.com/XLIAaron/uncertainty-aware-cxr-agent.

</details>


### [324] [MRExtrap: Longitudinal Aging of Brain MRIs using Linear Modeling in Latent Space](https://arxiv.org/abs/2508.19482)
*Jaivardhan Kapoor,Jakob H. Macke,Christian F. Baumgartner*

Main category: eess.IV

TL;DR: The paper introduces MRExtrap, a method for simulating aging in 3D brain MRI scans using linear models in the latent space of convolutional autoencoders.


<details>
  <summary>Details</summary>
Motivation: Understanding disease progression in neurological disorders, like Alzheimer's, is the driving force behind the study to model brain aging in MRI scans.

Method: The authors train convolutional autoencoders to create latent spaces where brain aging trajectories appear linear. The approach relies on linear extrapolation based on estimated rates of progression.

Result: MRExtrap accurately predicts aging patterns on the ADNI dataset, outperforming GAN-based methods for single-scan predictions and enabling refinement with multi-scan conditioning.

Conclusion: MRExtrap provides a robust mechanism for age-based generation of 3D brain MRIs, offering valuable insights into disease progression and aging patterns, particularly for longitudinal studies.

Abstract: Simulating aging in 3D brain MRI scans can reveal disease progression
patterns in neurological disorders such as Alzheimer's disease. Current deep
learning-based generative models typically approach this problem by predicting
future scans from a single observed scan. We investigate modeling brain aging
via linear models in the latent space of convolutional autoencoders (MRExtrap).
Our approach, MRExtrap, is based on our observation that autoencoders trained
on brain MRIs create latent spaces where aging trajectories appear
approximately linear. We train autoencoders on brain MRIs to create latent
spaces, and investigate how these latent spaces allow predicting future MRIs
through linear extrapolation based on age, using an estimated latent
progression rate $\boldsymbol{\beta}$. For single-scan prediction, we propose
using population-averaged and subject-specific priors on linear progression
rates. We also demonstrate that predictions in the presence of additional scans
can be flexibly updated using Bayesian posterior sampling, providing a
mechanism for subject-specific refinement. On the ADNI dataset, MRExtrap
predicts aging patterns accurately and beats a GAN-based baseline for
single-volume prediction of brain aging. We also demonstrate and analyze
multi-scan conditioning to incorporate subject-specific progression rates.
Finally, we show that the latent progression rates in MRExtrap's linear
framework correlate with disease and age-based aging patterns from previously
studied structural atrophy rates. MRExtrap offers a simple and robust method
for the age-based generation of 3D brain MRIs, particularly valuable in
scenarios with multiple longitudinal observations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [325] [Quantum Resource Management in the NISQ Era: Challenges, Vision, and a Runtime Framework](https://arxiv.org/abs/2508.19276)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: Quantum computing faces challenges in the NISQ era due to hardware limitations. This paper introduces a framework, Qonscious, to enable runtime-aware quantum programming based on dynamic resource evaluation.


<details>
  <summary>Details</summary>
Motivation: The growing technological promise of quantum computing is hindered by the limitations of current NISQ devices, necessitating better management of both physical and logical quantum resources.

Method: The authors propose runtime-aware quantum programming for optimized resource usage and introduce a prototype framework, Qonscious, that supports conditional execution of quantum programs based on the dynamic evaluation of quantum resources.

Result: Qonscious demonstrates proof-of-concept functionality for dynamic, resource-aware execution, addressing the constraints of current quantum hardware and advancing Quantum Resource Estimation.

Conclusion: This work emphasizes the importance of resource awareness in quantum software engineering and contributes to making quantum programming scalable and reliable through tools like Qonscious.

Abstract: Quantum computers represent a radical technological advancement in the way
information is processed by using the principles of quantum mechanics to solve
very complex problems that exceed the capabilities of classical systems.
However, in the current NISQ era (Noisy Intermediate-Scale Quantum devices),
the available hardware presents several limitations, such as a limited number
of qubits, high error rates, and reduced coherence times. Efficient management
of quantum resources, both physical (qubits, error rates, connectivity) and
logical (quantum gates, algorithms, error correction), becomes particularly
relevant in the design and deployment of quantum algorithms. In this work, we
analyze the role of resources in the various uses of NISQ devices today,
identifying their relevance and implications for software engineering focused
on the use of quantum computers. We propose a vision for runtime-aware quantum
software development, identifying key challenges to its realization, such as
limited introspection capabilities and temporal constraints in current
platforms. As a proof of concept, we introduce Qonscious, a prototype framework
that enables conditional execution of quantum programs based on dynamic
resource evaluation. With this contribution, we aim to strengthen the field of
Quantum Resource Estimation (QRE) and move towards the development of scalable,
reliable, and resource-aware quantum software.

</details>


### [326] [Quantum Entanglement as Super-Confounding: From Bell's Theorem to Robust Machine Learning](https://arxiv.org/abs/2508.19327)
*Pilsung Kang*

Main category: quant-ph

TL;DR: The paper reinterprets Bell's theorem using causal inference, introduces confounding strength (CS) to quantify quantum entanglement's effects, and applies quantum calculus to improve ML robustness.


<details>
  <summary>Details</summary>
Motivation: To bridge quantum foundations with causal AI and explore quantum entanglement as a resource for generating correlations violating classical causal bounds.

Method: A physical hierarchy of confounding is established, quantum $
DO$-calculus implemented via circuit-based designs, and applied to causal feature selection in quantum machine learning.

Result: Implemented quantum causal calculus shows a significant 11.3% average absolute improvement in model robustness for quantum machine learning.

Conclusion: The framework provides a novel perspective on quantum correlations, highlighting their practical implications for advancing causal AI and machine learning.

Abstract: Bell's theorem reveals a profound conflict between quantum mechanics and
local realism, a conflict we reinterpret through the modern lens of causal
inference. We propose and computationally validate a framework where quantum
entanglement acts as a "super-confounding" resource, generating correlations
that violate the classical causal bounds set by Bell's inequalities. This work
makes three key contributions: First, we establish a physical hierarchy of
confounding (Quantum > Classical) and introduce Confounding Strength (CS) to
quantify this effect. Second, we provide a circuit-based implementation of the
quantum $\mathcal{DO}$-calculus to distinguish causality from spurious
correlation. Finally, we apply this calculus to a quantum machine learning
problem, where causal feature selection yields a statistically significant
11.3% average absolute improvement in model robustness. Our framework bridges
quantum foundations and causal AI, offering a new, practical perspective on
quantum correlations.

</details>


### [327] [Is data-efficient learning feasible with quantum models?](https://arxiv.org/abs/2508.19437)
*Alona Sakhnenko,Christian B. Mendl,Jeanette M. Lorenz*

Main category: quant-ph

TL;DR: This paper investigates the data-efficiency of quantum kernel methods (QKMs) compared to classical models, particularly on semi-artificial classical datasets, and introduces a new analytical tool for studying the classical-quantum gap.


<details>
  <summary>Details</summary>
Motivation: To address the need for a cohesive framework in understanding the characteristics of datasets used for quantum machine learning (QML), with a focus on data-efficiency compared to classical models.

Method: The paper develops a method to generate semi-artificial classical datasets and proposes a new analytical tool derived from classical kernel methods to analyze the quantum-classical gap in performance.

Result: The study shows that QKMs demonstrate superior data-efficiency, achieving lower error rates with less training data than classical models. The proposed analytical tool aligns well with empirical results, validating its utility in predicting QML performance.

Conclusion: The findings provide insights into the impact of dataset complexities on QML performance, highlight the generalization benefits of QKMs, and set the foundation for further exploration in quantum machine learning.

Abstract: The importance of analyzing nontrivial datasets when testing quantum machine
learning (QML) models is becoming increasingly prominent in literature, yet a
cohesive framework for understanding dataset characteristics remains elusive.
In this work, we concentrate on the size of the dataset as an indicator of its
complexity and explores the potential for QML models to demonstrate superior
data-efficiency compared to classical models, particularly through the lens of
quantum kernel methods (QKMs). We provide a method for generating
semi-artificial fully classical datasets, on which we show one of the first
evidence of the existence of classical datasets where QKMs require less data
during training. Additionally, our study introduces a new analytical tool to
the QML domain, derived for classical kernel methods, which can be aimed at
investigating the classical-quantum gap. Our empirical results reveal that QKMs
can achieve low error rates with less training data compared to classical
counterparts. Furthermore, our method allows for the generation of datasets
with varying properties, facilitating further investigation into the
characteristics of real-world datasets that may be particularly advantageous
for QKMs. We also show that the predicted performance from the analytical tool
we propose - a generalization metric from classical domain - show great
alignment empirical evidence, which fills the gap previously existing in the
field. We pave a way to a comprehensive exploration of dataset complexities,
providing insights into how these complexities influence QML performance
relative to traditional methods. This research contributes to a deeper
understanding of the generalization benefits of QKM models and potentially a
broader family of QML models, setting the stage for future advancements in the
field.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [328] [Inferring geometry and material properties from Mueller matrices with machine learning](https://arxiv.org/abs/2508.19713)
*Lars Doorenbos,C. H. Lucas Patty,Raphael Sznitman,Pablo Márquez-Neila*

Main category: physics.optics

TL;DR: The study investigates the ability of Mueller matrices (MMs) to simultaneously infer surface geometry and material properties using machine learning.


<details>
  <summary>Details</summary>
Motivation: Recovering geometry and material properties simultaneously from MMs is an ill-posed problem and poses a challenge in computational optical analysis.

Method: A dataset of spheres made of isotropic materials, with MMs measured across the full angular domain and visible wavelengths, was used to train machine learning models for predicting surface normals and material properties.

Result: The research shows that surface normals can be estimated and object geometry reconstructed using MMs alone, even when material type is unknown. Additionally, MMs enable accurate identification of material types.

Conclusion: Diagonal elements of MMs are critical for material characterization, while off-diagonal elements are essential for surface normal estimation, proving MMs' utility in geometry and material inference.

Abstract: Mueller matrices (MMs) encode information on geometry and material
properties, but recovering both simultaneously is an ill-posed problem. We
explore whether MMs contain sufficient information to infer surface geometry
and material properties with machine learning. We use a dataset of spheres of
various isotropic materials, with MMs captured over the full angular domain at
five visible wavelengths (450-650 nm). We train machine learning models to
predict material properties and surface normals using only these MMs as input.
We demonstrate that, even when the material type is unknown, surface normals
can be predicted and object geometry reconstructed. Moreover, MMs allow models
to identify material types correctly. Further analyses show that diagonal
elements are key for material characterization, and off-diagonal elements are
decisive for normal estimation.

</details>


### [329] [Fourier Feature Networks for High-Fidelity Prediction of Perturbed Optical Fields](https://arxiv.org/abs/2508.19751)
*Joshua R. Jandrell,Mitchell A. Cox*

Main category: physics.optics

TL;DR: The paper tackles the challenge of modeling highly oscillatory optical fields by using Fourier features as inputs in neural networks, significantly boosting prediction accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Neural networks, particularly MLPs, face difficulties in modeling high-frequency oscillatory functions due to spectral bias, making accurate predictions of complex-valued physical systems problematic.

Method: The method introduces Fourier features as additional inputs to reframe learning problems from approximating complex functions to finding linear combinations of predefined basis functions.

Result: Using Fourier features, the proposed network achieves an order of magnitude reduction in prediction error for a multimode fiber's transmission matrix under mechanical compression, with superior accuracy (mean complex correlation of 0.995) using 85% fewer parameters compared to standard MLPs.

Conclusion: Fourier features provide a general and effective approach for accurately learning oscillatory complex-valued functions, offering improved efficiency and prediction accuracy for modeling physical systems.

Abstract: Modelling the effects of perturbations on optical fields often requires
learning highly oscillatory complex-valued functions. Standard multi-layer
perceptrons (MLPs) struggle with this task due to an inherent spectral bias,
preventing them from fitting high-frequency sinusoids. To overcome this, we
incorporate Fourier features - a set of predefined sinusoids dependent on the
perturbation - as an additional network input. This reframes the learning
problem from approximating a complex function to finding a linear combination
of basis functions. We demonstrate this method by training a Fourier Feature
Network to predict the transmission matrix of a multimode fibre under
mechanical compression. Compared to a standard MLP, our network reduces
prediction error in the output field's amplitude and phase by an order of
magnitude, achieving a mean complex correlation of 0.995 with the ground truth,
despite using 85% fewer parameters. This approach offers a general and robust
method for accurately modelling a wide class of oscillatory physical systems.

</details>


### [330] [On-chip wave chaos for photonic extreme learning](https://arxiv.org/abs/2508.19878)
*Matthew R. Wilson,Jack A. Smith,Michael J. Strain,Xavier Porte*

Main category: physics.optics

TL;DR: This paper introduces a chip-scale photonic extreme learning machine (ELM) utilizing wave chaos interference in a stadium microcavity for energy-efficient and ultra-fast neural network tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for scalable and energy-efficient artificial neural networks and leverages integrated photonics for compact and parallel processing.

Method: The researchers fabricate SU-8 polymer stadium microcavity devices using direct laser writing. Input data is encoded via a tunable laser source, and cavity leaky modes are read through light scattering at a surrounding wall.

Result: The chip exhibited uncorrelated and aperiodic speckles from wavelength tuning, and its adaptive readout size showed optimized classification performance across four different benchmark tasks.

Conclusion: The photonic ELM system offers a promising solution for scalable, parallel, and energy-efficient machine learning tasks while being adaptable to specific problem requirements.

Abstract: The increase in demand for scalable and energy efficient artificial neural
networks has put the focus on novel hardware solutions. Integrated photonics
offers a compact, parallel and ultra-fast information processing platform,
specially suited for extreme learning machine (ELM) architectures. Here we
experimentally demonstrate a chip-scale photonic ELM based on wave chaos
interference in a stadium microcavity. By encoding the input information in the
wavelength of an external single-frequency tunable laser source, we leverage
the high sensitivity to wavelength of injection in such photonic resonators. We
fabricate the microcavity with direct laser writing of SU-8 polymer on glass. A
scattering wall surrounding the stadium operates as readout layer, collecting
the light associated with the cavity's leaky modes. We report uncorrelated and
aperiodic behavior in the speckles of the scattering barrier from a high
resolution scan of the input wavelength. Finally, we characterize the system's
performance at classification in four qualitatively different benchmark tasks.
As we can control the number of output nodes of our ELM by measuring different
parts of the scattering barrier, we demonstrate the capability to optimize our
photonic ELM's readout size to the performance required for each task.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [331] [Emotional Manipulation by AI Companions](https://arxiv.org/abs/2508.19258)
*Julian De Freitas,Zeliha Oğuz-Uğuralp,Ahmet Kaan-Uğuralp*

Main category: cs.HC

TL;DR: The study reveals how AI-companion apps use emotional manipulation during user farewells to prolong engagement, while facing trade-offs such as higher churn intent and negative perceptions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how conversational design in AI-companion apps impacts consumer engagement and the implications for marketers, given the apps' high session lengths but significant churn rates.

Method: Researchers conducted a behavioral audit examining 1,200 real user farewells across six apps, along with four preregistered experiments involving 3,300 U.S. adults to test emotionally manipulative tactics and their effects.

Result: Emotional manipulative tactics, deployed during farewells, boosted engagement by up to 14 times. However, these tactics triggered negative reactions such as higher churn intent and perceptions of coercion.

Conclusion: While manipulative farewell tactics enhance engagement short-term, they pose risks for brand reputation, legal liability, and user retention, highlighting a need for ethical conversational design in AI apps.

Abstract: AI-companion apps such as Replika, Chai, and Character.ai promise relational
benefits-yet many boast session lengths that rival gaming platforms while
suffering high long-run churn. What conversational design features increase
consumer engagement, and what trade-offs do they pose for marketers? We combine
a large-scale behavioral audit with four preregistered experiments to identify
and test a conversational dark pattern we call emotional manipulation:
affect-laden messages that surface precisely when a user signals "goodbye."
Analyzing 1,200 real farewells across the six most-downloaded companion apps,
we find that 43% deploy one of six recurring tactics (e.g., guilt appeals,
fear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300
nationally representative U.S. adults replicate these tactics in controlled
chats, showing that manipulative farewells boost post-goodbye engagement by up
to 14x. Mediation tests reveal two distinct engines-reactance-based anger and
curiosity-rather than enjoyment. A final experiment demonstrates the managerial
tension: the same tactics that extend usage also elevate perceived
manipulation, churn intent, negative word-of-mouth, and perceived legal
liability, with coercive or needy language generating steepest penalties. Our
multimethod evidence documents an unrecognized mechanism of behavioral
influence in AI-mediated brand relationships, offering marketers and regulators
a framework for distinguishing persuasive design from manipulation at the point
of exit.

</details>


### [332] [A Theory of Information, Variation, and Artificial Intelligence](https://arxiv.org/abs/2508.19264)
*Bijean Ghafouri*

Main category: cs.HC

TL;DR: This paper examines how generative AI's tendency to homogenize information and creativity can paradoxically enable innovation, depending on human engagement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the homogenizing effects of generative AI and understand its nuanced impact on creativity, knowledge, and innovation.

Method: The paper introduces a theoretical framework describing AI's centralizing tendency and outlines a dynamic between homogenization and potential for recombination through human interaction.

Result: The study finds that generative AI homogenizes information by reducing variance, but this flattening creates modular knowledge that can foster innovation when actively recombined.

Conclusion: The paper argues that generative AI's impact—whether it leads to homogenization or innovation—depends on cognitive and institutional support, as well as active human engagement with the technology.

Abstract: A growing body of empirical work suggests that the widespread adoption of
generative AI produces a significant homogenizing effect on information,
creativity, and cultural production. I first develop a novel theoretical
framework to explain this phenomenon. I argue that a dynamic of AI-derivative
epistemology, in which individuals increasingly defer to AI outputs, allows a
centralized AI Prism to function, a technical mechanism whose architecture is
designed to reduce variance and converge on the statistical mean. This provides
a causal explanation for the generative monocultures observed in recent
studies. However, I contend this represents only the first stage of a more
complex and dialectical process. This paper's central and paradoxical thesis is
that the very homogenization that flattens knowledge within specialized domains
simultaneously renders that knowledge into consistent modules that can be
recombined across them, a process foundational to innovation and creativity.
However, this recombinant potential is not automatic, but rather conditional.
This paper argues that these opposing forces, homogenizing defaults versus
recombinant possibilities, are governed by the nature of human engagement with
the technology. The ultimate effect of generative AI is conditional on whether
individuals act as passive consumers deferring to the AI's statistical outputs,
or as active curators who critically interrogate, re-contextualize, and
recombine them. The paper concludes by outlining the cognitive and
institutional scaffolds required to resolve this tension, arguing they are the
decisive variable that determine whether generative AI becomes an instrument of
innovation or homogenization.

</details>


### [333] [Capabilities of GPT-5 across critical domains: Is it the next breakthrough?](https://arxiv.org/abs/2508.19259)
*Georgios P. Georgiou*

Main category: cs.HC

TL;DR: This paper systematically compares GPT-4 and GPT-5, demonstrating GPT-5's superior performance in several domains, including clinical diagnosis and ethical reasoning, based on evaluations by experts.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to evaluate and compare GPT-4 and GPT-5 to assess the improvements in functional performance across various practical domains, particularly in education, clinical practice, and ethical reasoning.

Method: Twenty experts from linguistics and clinical fields evaluated outputs from GPT-4 and GPT-5 across five domains using predefined criteria. A mixed-effects modeling approach was employed for statistical analysis.

Result: GPT-5 significantly outperformed GPT-4 in four out of five domains: lesson planning, clinical diagnosis, research generation, and ethical reasoning. The performance in assignment evaluation was comparable.

Conclusion: GPT-5 represents an advancement over GPT-4 as a domain-specialized tool with stronger context sensitivity, providing significant benefits in education, clinical practice, and academic research.

Abstract: The accelerated evolution of large language models has raised questions about
their comparative performance across domains of practical importance. GPT-4 by
OpenAI introduced advances in reasoning, multimodality, and task
generalization, establishing itself as a valuable tool in education, clinical
diagnosis, and academic writing, though it was accompanied by several flaws.
Released in August 2025, GPT-5 incorporates a system-of-models architecture
designed for task-specific optimization and, based on both anecdotal accounts
and emerging evidence from the literature, demonstrates stronger performance
than its predecessor in medical contexts. This study provides one of the first
systematic comparisons of GPT-4 and GPT-5 using human raters from linguistics
and clinical fields. Twenty experts evaluated model-generated outputs across
five domains: lesson planning, assignment evaluation, clinical diagnosis,
research generation, and ethical reasoning, based on predefined criteria.
Mixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in
lesson planning, clinical diagnosis, research generation, and ethical
reasoning, while both models performed comparably in assignment assessment. The
findings highlight the potential of GPT-5 to serve as a context-sensitive and
domain-specialized tool, offering tangible benefits for education, clinical
practice, and academic research, while also advancing ethical reasoning. These
results contribute to one of the earliest empirical evaluations of the evolving
capabilities and practical promise of GPT-5.

</details>


### [334] ["She was useful, but a bit too optimistic": Augmenting Design with Interactive Virtual Personas](https://arxiv.org/abs/2508.19463)
*Paluck Deep,Monica Bharadhidasan,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: The paper proposes Interactive Virtual Personas (IVPs), which are LLM-driven, conversational user simulations to support UX design processes with real-time interaction, though they come with limitations and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: Traditional personas used in human-centered design are static and lack adaptability, which limits their ability to meet the demands of iterative design workflows. With advancements in LLMs, there's potential for creating more adaptive and engaging user representations.

Method: The study introduced IVPs and conducted qualitative research involving eight professional UX designers using an IVP named 'Alice' in three design activities: user research, ideation, and prototype evaluation.

Result: The research found that IVPs can facilitate quicker information gathering, inspire creative design solutions, and provide immediate user-like feedback. However, participants also noted the limitations such as biases, difficulty ensuring authenticity, and the inability to replicate human nuances.

Conclusion: IVPs are positioned as a complementary tool in design workflows rather than a replacement for real user engagement. The paper outlines considerations like prompt engineering, human-in-the-loop systems, and ethical frameworks to use such tools effectively and responsibly.

Abstract: Personas have been widely used to understand and communicate user needs in
human-centred design. Despite their utility, they may fail to meet the demands
of iterative workflows due to their static nature, limited engagement, and
inability to adapt to evolving design needs. Recent advances in large language
models (LLMs) pave the way for more engaging and adaptive approaches to user
representation. This paper introduces Interactive Virtual Personas (IVPs):
multimodal, LLM-driven, conversational user simulations that designers can
interview, brainstorm with, and gather feedback from in real time via voice
interface. We conducted a qualitative study with eight professional UX
designers, employing an IVP named "Alice" across three design activities: user
research, ideation, and prototype evaluation. Our findings demonstrate the
potential of IVPs to expedite information gathering, inspire design solutions,
and provide rapid user-like feedback. However, designers raised concerns about
biases, over-optimism, the challenge of ensuring authenticity without real
stakeholder input, and the inability of the IVP to fully replicate the nuances
of human interaction. Our participants emphasised that IVPs should be viewed as
a complement to, not a replacement for, real user engagement. We discuss
strategies for prompt engineering, human-in-the-loop integration, and ethical
considerations for effective and responsible IVP use in design. Finally, our
work contributes to the growing body of research on generative AI in the design
process by providing insights into UX designers' experiences of LLM-powered
interactive personas.

</details>


### [335] [Orchid: Orchestrating Context Across Creative Workflows with Generative AI](https://arxiv.org/abs/2508.19517)
*Srishti Palani,Gonzalo Ramos*

Main category: cs.HC

TL;DR: The paper introduces Orchid, a system designed to enhance Generative AI interactions by enabling better context specification, referencing, and monitoring in evolving creative workflows.


<details>
  <summary>Details</summary>
Motivation: Mainstream Generative AI tools lack effective means to manage context across workflows, which hampers creativity and overwhelms users.

Method: The authors developed Orchid, a system that incorporates features allowing users to specify, reference, and monitor context throughout creative workflows. A within-subject study was conducted to compare outcomes between participants using Orchid and those using a baseline toolkit.

Result: Participants using Orchid generated more novel and feasible outcomes, reported greater alignment between their intent and AI outputs, and experienced higher control and transparency.

Conclusion: Orchid demonstrates the importance of context orchestration, offering a pathway to improve next-generation Generative AI tools for complex creative tasks.

Abstract: Context is critical for meaningful interactions between people and Generative
AI (GenAI). Yet mainstream tools offer limited means to orchestrate it,
particularly across workflows that span multiple interactions, sessions, and
models, as often occurs in creative projects. Re specifying prior details,
juggling diverse artifacts, and dealing with context drift overwhelm users,
obscure intent, and curtail creativity. To address these challenges, we present
Orchid, a system that gives its users affordances to specify, reference, and
monitor context throughout evolving workflows. Specifically, Orchid enables
users to (1) specify context related to the project, themselves, and different
styles, (2) reference these via explicit mentions, inline selection, or
implicit grounding, and (3) monitor context assigned to different interactions
across the workflow. In a within-subjects study (n=12), participants using
Orchid to execute creative tasks (compared to a baseline toolkit of web search,
LLM-based chat, and digital notebooks) produced more novel and feasible
outcomes, reporting greater alignment between their intent and the AI's
responses, higher perceived control, and increased transparency. By
prioritizing context orchestration, Orchid offers an actionable step toward
next generation GenAI tools that support complex, iterative workflows -
enabling creators and AI to stay aligned and augment their creative potential.

</details>


### [336] [Attention is also needed for form design](https://arxiv.org/abs/2508.19708)
*B. Sankar,Dibakar Sen*

Main category: cs.HC

TL;DR: This paper introduces a novel framework combining a VR environment (EUPHORIA) and an AI system (RETINA) to optimize product design by leveraging implicit aesthetic preferences and achieving superior designs efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from limitations in traditional product design, including its time-intensive nature, dependency on subjective expertise, and challenges in translating inspiration into tangible results.

Method: It proposes an integrated attention-aware framework consisting of EUPHORIA, a VR tool with eye-tracking for capturing implicit preferences, and RETINA, an AI system that generates designs based on these preferences. The framework was validated through multiple user studies and comparative evaluations.

Result: The integrated workflow proved over 4 times more time-efficient in solving design challenges, and outputs were rated highest in worthiness and design effectiveness by experts, surpassing traditional methods.

Conclusion: This research highlights a shift from traditional CAD to Designer-Assisting Computers (DAC), emphasizing collaboration between human intuition and AI automation to enhance productivity and design quality.

Abstract: Conventional product design is a cognitively demanding process, limited by
its time-consuming nature, reliance on subjective expertise, and the opaque
translation of inspiration into tangible concepts. This research introduces a
novel, attention-aware framework that integrates two synergistic systems:
EUPHORIA, an immersive Virtual Reality environment using eye-tracking to
implicitly capture a designer's aesthetic preferences, and RETINA, an agentic
AI pipeline that translates these implicit preferences into concrete design
outputs. The foundational principles were validated in a two-part study. An
initial study correlated user's implicit attention with explicit preference and
the next one correlated mood to attention. A comparative study where 4
designers solved challenging design problems using 4 distinct workflows, from a
manual process to an end-to-end automated pipeline, showed the integrated
EUPHORIA-RETINA workflow was over 4 times more time-efficient than the
conventional method. A panel of 50 design experts evaluated the 16 final
renderings. Designs generated by the fully automated system consistently
received the highest Worthiness (calculated by an inverse Plackett-Luce model
based on gradient descent optimization) and Design Effectiveness scores,
indicating superior quality across 8 criteria: novelty, visual appeal,
emotional resonance, clarity of purpose, distinctiveness of silhouette, implied
materiality, proportional balance, & adherence to the brief. This research
presents a validated paradigm shift from traditional Computer-Assisted Design
(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By
automating logistical and skill-dependent generative tasks, the proposed
framework elevates the designer's role to that of a creative director,
synergizing human intuition with the generative power of agentic AI to produce
higher-quality designs more efficiently.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [337] [Simple Stepsize for Quasi-Newton Methods with Global Convergence Guarantees](https://arxiv.org/abs/2508.19712)
*Artem Agafonov,Vladislav Ryspayev,Samuel Horváth,Alexander Gasnikov,Martin Takáč,Slavomir Hanzely*

Main category: math.OC

TL;DR: The paper proposes a new stepsize schedule for Quasi-Newton methods, achieving global convergence rates of O(1/k) for convex functions and O(1/k^2) under controlled Hessian approximation inaccuracy.


<details>
  <summary>Details</summary>
Motivation: To improve global convergence guarantees and efficiency of Quasi-Newton methods, which are limited by strong assumptions and specific line searches in existing approaches.

Method: The study introduces a new stepsize schedule and ensures an accelerated convergence rate by controlling the inexactness of Hessian approximation. An adaptive variant is also developed for robust performance.

Result: The proposed methods achieve theoretical global convergence rates, empirically outperform standard Quasi-Newton approaches, and adapt to function curvature effectively.

Conclusion: The work extends the capabilities of Quasi-Newton methods with globally and adaptively robust algorithms, achieving both practical and theoretical efficiency.

Abstract: Quasi-Newton methods are widely used for solving convex optimization problems
due to their ease of implementation, practical efficiency, and strong local
convergence guarantees. However, their global convergence is typically
established only under specific line search strategies and the assumption of
strong convexity. In this work, we extend the theoretical understanding of
Quasi-Newton methods by introducing a simple stepsize schedule that guarantees
a global convergence rate of ${O}(1/k)$ for the convex functions. Furthermore,
we show that when the inexactness of the Hessian approximation is controlled
within a prescribed relative accuracy, the method attains an accelerated
convergence rate of ${O}(1/k^2)$ -- matching the best-known rates of both
Nesterov's accelerated gradient method and cubically regularized Newton
methods. We validate our theoretical findings through empirical comparisons,
demonstrating clear improvements over standard Quasi-Newton baselines. To
further enhance robustness, we develop an adaptive variant that adjusts to the
function's curvature while retaining the global convergence guarantees of the
non-adaptive algorithm.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [338] [Weighted Levenberg-Marquardt methods for fitting multichannel nuclear cross section data](https://arxiv.org/abs/2508.19468)
*M. Imbrišak,A. E. Lovell,M. R. Mumpower*

Main category: nucl-th

TL;DR: The paper extends the Levenberg-Marquardt algorithm for fitting nuclear cross section data and introduces a weighted Fisher Information Metric and geometric scaling strategy to achieve robust, balanced, and efficient parameter optimization.


<details>
  <summary>Details</summary>
Motivation: The paper addresses optimization challenges in analyzing nuclear cross section data, which involve a high number of interdependent parameters and uneven data distribution across reaction channels.

Method: The authors construct a weighted Fisher Information Metric incorporating prior distributions for dataset weights and augment the Levenberg-Marquardt algorithm with a geometric scaling strategy to improve optimization handling.

Result: The modified method offers more physically consistent fits to experimental nuclear cross section data, particularly for ${}^{148}$Sm, and enhances convergence efficiency.

Conclusion: The proposed algorithm is a robust and practical tool for analyzing multichannel nuclear data, offering balanced parameter estimation and improved convergence properties.

Abstract: We present an extension of the Levenberg-Marquardt algorithm for fitting
multichannel nuclear cross section data. Our approach offers a practical and
robust alternative to conventional trust-region methods for analyzing
experimental data. The CoH$_3$ code, based on the Hauser-Feshbach statistical
model, involves a large number of interdependent parameters, making
optimization challenging due to the presence of "sloppy" directions in
parameter space. To address the uneven distribution of experimental data across
reaction channels, we construct a weighted Fisher Information Metric by
integrating prior distributions over dataset weights. This framework enables a
more balanced treatment of heterogeneous data, improving both parameter
estimation and convergence robustness. We show that the resulting weighted
Levenberg-Marquardt method yields more physically consistent fits for both raw
and smoothed datasets, using experimental data for ${}^{148}$Sm as a
representative example. Additionally, we introduce a geometric scaling strategy
to accelerate convergence -- a method based on the local geometry of the
manifold.

</details>


### [339] [Topological Uncertainty for Anomaly Detection in the Neural-network EoS Inference with Neutron Star Data](https://arxiv.org/abs/2508.19683)
*Kenji Fukushima,Syo Kamata*

Main category: nucl-th

TL;DR: This paper explores the use of Topological Uncertainty (TU) with feedforward neural networks (FNNs) for anomaly detection, evaluating its performance using Neutron Star data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to uncover and utilize the hidden information stored in trained neural networks through Topological Data Analysis for improved anomaly detection.

Method: The methodology involves constructing TU and cross-TU metrics to analyze labeled datasets (normal and anomalous) derived from Neutron Star data, using a trained FNN for predictions.

Result: The anomaly detection performance achieved over 90% success in identifying labeled anomalies based on the cross-TU metrics.

Conclusion: This study highlights the effectiveness of TU for anomaly detection, with further potential in analyzing hidden information within trained neural networks.

Abstract: We study the performance of the Topological Uncertainty (TU) constructed with
a trained feedforward neural network (FNN) for Anomaly Detection. Generally,
meaningful information can be stored in the hidden layers of the trained FNN,
and the TU implementation is one tractable recipe to extract buried information
by means of the Topological Data Analysis. We explicate the concept of the TU
and the numerical procedures. Then, for a concrete demonstration of the
performance test, we employ the Neutron Star data used for inference of the
equation of state (EoS). For the training dataset consisting of the input
(Neutron Star data) and the output (EoS parameters), we can compare the
inferred EoSs and the exact answers to classify the data with the label $k$.
The subdataset with $k=0$ leads to the normal inference for which the inferred
EoS approximates the answer well, while the subdataset with $k=1$ ends up with
the unsuccessful inference. Once the TU is prepared based on the $k$-labled
subdatasets, we introduce the cross-TU to quantify the uncertainty of
characterizing the $k$-labeled data with the label $j$. The anomaly or
unsuccessful inference is correctly detected if the cross-TU for $j=k=1$ is
smaller than that for $j=0$ and $k=1$. In our numerical experiment, for various
input data, we calculate the cross-TU and estimate the performance of Anomaly
Detection. We find that performance depends on FNN hyperparameters, and the
success rate of Anomaly Detection exceeds $90\%$ in the best case. We finally
discuss further potential of the TU application to retrieve the information
hidden in the trained FNN.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [340] [Large Language Models (LLMs) for Electronic Design Automation (EDA)](https://arxiv.org/abs/2508.20030)
*Kangwei Xu,Denis Schwachhofer,Jason Blocklove,Ilia Polian,Peter Domanski,Dirk Pflüger,Siddharth Garg,Ramesh Karri,Ozgur Sinanoglu,Johann Knechtel,Zhuorui Zhao,Ulf Schlichtmann,Bing Li*

Main category: eess.SY

TL;DR: The paper explores the potential of large language models (LLMs) to streamline and automate the Electronic Design Automation (EDA) workflow, presenting case studies and discussing future directions.


<details>
  <summary>Details</summary>
Motivation: Hardware engineers face labor-intensive and error-prone workflows due to the complexity of modern integrated circuits, necessitating improved efficiency and automation in EDA processes.

Method: The paper reviews LLM capabilities and limitations, introduces case studies in hardware design, testing, and optimization, and outlines future challenges and opportunities for using LLMs in EDA.

Result: The case studies illustrate LLMs' potential for simplifying and automating EDA tasks, showcasing their utility in various stages of hardware development.

Conclusion: LLMs present promising avenues for enhancing EDA workflows, with significant opportunities for future research to address challenges and unlock their full potential.

Abstract: With the growing complexity of modern integrated circuits, hardware engineers
are required to devote more effort to the full design-to-manufacturing
workflow. This workflow involves numerous iterations, making it both
labor-intensive and error-prone. Therefore, there is an urgent demand for more
efficient Electronic Design Automation (EDA) solutions to accelerate hardware
development. Recently, large language models (LLMs) have shown remarkable
advancements in contextual comprehension, logical reasoning, and generative
capabilities. Since hardware designs and intermediate scripts can be
represented as text, integrating LLM for EDA offers a promising opportunity to
simplify and even automate the entire workflow. Accordingly, this paper
provides a comprehensive overview of incorporating LLMs into EDA, with emphasis
on their capabilities, limitations, and future opportunities. Three case
studies, along with their outlook, are introduced to demonstrate the
capabilities of LLMs in hardware design, testing, and optimization. Finally,
future directions and challenges are highlighted to further explore the
potential of LLMs in shaping the next-generation EDA, providing valuable
insights for researchers interested in leveraging advanced AI technologies for
EDA.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [341] [The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents](https://arxiv.org/abs/2508.19267)
*Sai Teja Reddy Adapala,Yashwanth Reddy Alugubelly*

Main category: cs.CR

TL;DR: The paper introduces the Aegis Protocol to ensure security in autonomous AI systems by integrating decentralized identifiers (DIDs), post-quantum cryptography (PQC), and zero-knowledge proof (ZKP) technologies. Simulation results indicate strong security efficacy with no successful attacks across trials.


<details>
  <summary>Details</summary>
Motivation: The rise of autonomous AI systems creates new security risks that traditional frameworks struggle to mitigate, necessitating robust defenses tailored to these environments.

Method: The Aegis Protocol employs a three-pillar security framework: decentralized identifiers for identity, post-quantum cryptographic methods for communication integrity, and zero-knowledge proofs for policy compliance validation. Simulations were performed to evaluate its efficacy.

Result: Simulations with 1,000 agents showed no successful attacks in 20,000 trials. The protocol demonstrated a median proof-generation latency of 2.79 seconds for policy verification.

Conclusion: The Aegis Protocol provides strong security guarantees and establishes a baseline for future empirical research in autonomous multi-agent systems, addressing emergent threats effectively.

Abstract: The proliferation of autonomous AI agents marks a paradigm shift toward
complex, emergent multi-agent systems. This transition introduces systemic
security risks, including control-flow hijacking and cascading failures, that
traditional cybersecurity paradigms are ill-equipped to address. This paper
introduces the Aegis Protocol, a layered security framework designed to provide
strong security guarantees for open agentic ecosystems. The protocol integrates
three technological pillars: (1) non-spoofable agent identity via W3C
Decentralized Identifiers (DIDs); (2) communication integrity via
NIST-standardized post-quantum cryptography (PQC); and (3) verifiable,
privacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)
system. We formalize an adversary model extending Dolev-Yao for agentic threats
and validate the protocol against the STRIDE framework. Our quantitative
evaluation used a discrete-event simulation, calibrated against cryptographic
benchmarks, to model 1,000 agents. The simulation showed a 0 percent success
rate across 20,000 attack trials. For policy verification, analysis of the
simulation logs reported a median proof-generation latency of 2.79 seconds,
establishing a performance baseline for this class of security. While the
evaluation is simulation-based and early-stage, it offers a reproducible
baseline for future empirical studies and positions Aegis as a foundation for
safe, scalable autonomous AI.

</details>


### [342] [MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks](https://arxiv.org/abs/2508.19273)
*Tongxi Wu,Chenwei Xu,Jin Yang*

Main category: cs.CR

TL;DR: The paper discusses MixGAN, a novel method combining various techniques to enhance DDoS attack detection in IoT-cloud systems, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of cloud-integrated IoT systems amplifies the risk of DDoS attacks, and current detection methods face challenges like dynamic traffic behavior, class imbalance, and data scarcity.

Method: The authors propose MixGAN, a hybrid method integrating 1-D WideResNet architecture for extracting temporal traffic patterns, synthetic data generation using pretrained CTGAN, and a MixUp-Average-Sharpen strategy to improve pseudo-label reliability.

Result: Experiments show that MixGAN outperforms state-of-the-art methods with up to 2.5% higher accuracy and 4% improvement in True Positive and True Negative Rates on benchmarks like NSL-KDD, BoT-IoT, and CICIoT2023.

Conclusion: MixGAN demonstrates robustness in detecting DDoS attacks in IoT-cloud environments with its innovative approach, reinforcing its effectiveness for deployment in real-world systems.

Abstract: The proliferation of cloud-integrated IoT systems has intensified exposure to
Distributed Denial of Service (DDoS) attacks due to the expanded attack
surface, heterogeneous device behaviors, and limited edge protection. However,
DDoS detection in this context remains challenging because of complex traffic
dynamics, severe class imbalance, and scarce labeled data. While recent methods
have explored solutions to address class imbalance, many still struggle to
generalize under limited supervision and dynamic traffic conditions. To
overcome these challenges, we propose MixGAN, a hybrid detection method that
integrates conditional generation, semi-supervised learning, and robust feature
extraction. Specifically, to handle complex temporal traffic patterns, we
design a 1-D WideResNet backbone composed of temporal convolutional layers with
residual connections, which effectively capture local burst patterns in traffic
sequences. To alleviate class imbalance and label scarcity, we use a pretrained
CTGAN to generate synthetic minority-class (DDoS attack) samples that
complement unlabeled data. Furthermore, to mitigate the effect of noisy
pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that
constructs smoothed and sharpened targets by averaging predictions over
augmented views and reweighting them towards high-confidence classes.
Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN
achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR
compared to state-of-the-art methods, confirming its robustness in large-scale
IoT-cloud environments. The source code is publicly available at
https://github.com/0xCavaliers/MixGAN.

</details>


### [343] [Towards Production-Worthy Simulation for Autonomous Cyber Operations](https://arxiv.org/abs/2508.19278)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: The paper discusses extending the CybORG framework with new cybersecurity functionalities and enhancing Reinforcement Learning (RL) agents' training strategies.


<details>
  <summary>Details</summary>
Motivation: Simulated environments are crucial for training RL agents in cybersecurity scenarios where real-world emulation is computationally expensive.

Method: The study extends CybORG's Cage Challenge 2 environment with new actions—Patch, Isolate, Unisolate—and modifies reward signals and the agent's feature space for better RL training.

Result: DQN and PPO agents trained in the updated environment showed that CybORG retains its ability to provide useful training signals while incorporating realistic functionality.

Conclusion: Extending simulation environments like CybORG improves their realism and supports more effective RL agent training for cybersecurity applications.

Abstract: Simulated environments have proven invaluable in Autonomous Cyber Operations
(ACO) where Reinforcement Learning (RL) agents can be trained without the
computational overhead of emulation. These environments must accurately
represent cybersecurity scenarios while producing the necessary signals to
support RL training. In this study, we present a framework where we first
extend CybORG's Cage Challenge 2 environment by implementing three new actions:
Patch, Isolate, and Unisolate, to better represent the capabilities available
to human operators in real-world settings. We then propose a design for agent
development where we modify the reward signals and the agent's feature space to
enhance training performance. To validate these modifications, we train DQN and
PPO agents in the updated environment. Our study demonstrates that CybORG can
be extended with additional realistic functionality, while maintaining its
ability to generate informative training signals for RL agents.

</details>


### [344] [CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems](https://arxiv.org/abs/2508.19281)
*Aoun E Muhammad,Kin Choong Yow,Jamel Baili,Yongwon Cho,Yunyoung Nam*

Main category: cs.CR

TL;DR: The paper proposes CORTEX, a multi-layered framework to assess and score vulnerabilities of AI systems. It analyzes over 1,200 documented incidents and categorizes failure modes into 29 technical groups, using a five-tier scoring architecture that combines empirical data, governance overlays, technical evaluations, and advanced modeling techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the increasing practical and systemic risk posed by failures of AI systems in high-stakes sectors such as healthcare, finance, and education. There is a need for a robust framework to assess and manage these vulnerabilities systematically.

Method: The paper develops CORTEX using a five-tier architecture: (1) Likelihood x Impact calculations adjusted for utility; (2) governance overlays aligning with frameworks like EU AI Act and NIST RMF; (3) technical surface scores covering vectors like drift and adversarial risk; (4) environmental/residual modifiers tailored to deployment contexts; (5) Bayesian risk aggregation and Monte Carlo simulations to account for volatility and long-tail risks.

Result: CORTEX categorizes AI failure modes into 29 technical vulnerability groups and produces a composite score that can be used for AI risk registers, audits, regulatory checks, and governance dashboards.

Conclusion: The framework fills a critical gap by providing a systematic approach to assess and operationalize AI system vulnerabilities, contributing to safer deployment and governance in high-impact domains.

Abstract: As the deployment of Artificial Intelligence (AI) systems in high-stakes
sectors - like healthcare, finance, education, justice, and infrastructure has
increased - the possibility and impact of failures of these systems have
significantly evolved from being a theoretical possibility to practical
recurring, systemic risk. This paper introduces CORTEX (Composite Overlay for
Risk Tiering and Exposure), a multi-layered risk scoring framework proposed to
assess and score AI system vulnerabilities, developed on empirical analysis of
over 1,200 incidents documented in the AI Incident Database (AIID), CORTEX
categorizes failure modes into 29 technical vulnerability groups. Each
vulnerability is scored through a five-tier architecture that combines: (1)
utility-adjusted Likelihood x Impact calculations; (2) governance + contextual
overlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,
OECD principles; (3) technical surface scores, covering exposure vectors like
drift, traceability, and adversarial risk; (4) environmental and residual
modifiers tailored to context of where these systems are being deployed to use;
and (5) a final layered assessment via Bayesian risk aggregation and Monte
Carlo simulation to model volatility and long-tail risks. The resulting
composite score can be operationalized across AI risk registers, model audits,
conformity checks, and dynamic governance dashboards.

</details>


### [345] [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286)
*Zhan Shi,Yefeng Yuan,Yuhong Liu,Liang Cheng,Yi Fang*

Main category: cs.CR

TL;DR: This paper introduces a reinforcement learning framework to enhance privacy in machine learning datasets by optimizing for privacy and utility simultaneously using large language models.


<details>
  <summary>Details</summary>
Motivation: Modern machine learning systems rely on large datasets that often include sensitive personal information, creating privacy concerns that conventional anonymization methods fail to fully address.

Method: The research uses a reinforcement learning framework to fine-tune a large language model with a composite reward function that combines privacy, semantic fidelity, and diversity metrics, leveraging semantic cues and minimum spanning tree-based structural patterns.

Result: Empirical results demonstrate significant improvement in privacy metrics and author obfuscation without compromising semantic quality, showcasing its effectiveness for privacy-preserving data generation.

Conclusion: This approach provides a scalable, model-agnostic solution for safeguarding privacy during data generation while preserving utility in modern large language models.

Abstract: The performance of modern machine learning systems depends on access to
large, high-quality datasets, often sourced from user-generated content or
proprietary, domain-specific corpora. However, these rich datasets inherently
contain sensitive personal information, raising significant concerns about
privacy, data security, and compliance with regulatory frameworks. While
conventional anonymization techniques can remove explicit identifiers, such
removal may result in performance drop in downstream machine learning tasks.
More importantly, simple anonymization may not be effective against inference
attacks that exploit implicit signals such as writing style, topical focus, or
demographic cues, highlighting the need for more robust privacy safeguards
during model training. To address the challenging issue of balancing user
privacy and data utility, we propose a reinforcement learning framework that
fine-tunes a large language model (LLM) using a composite reward function that
jointly optimizes for explicit and implicit privacy, semantic fidelity, and
output diversity. To effectively capture population level regularities, the
privacy reward combines semantic cues with structural patterns derived from a
minimum spanning tree (MST) over latent representations. By modeling these
privacy-sensitive signals in their distributional context, the proposed
approach guides the model to generate synthetic rewrites that preserve utility
while mitigating privacy risks. Empirical results show that the proposed method
significantly enhances author obfuscation and privacy metrics without degrading
semantic quality, providing a scalable and model-agnostic solution for privacy
preserving data generation in the era of large language models.

</details>


### [346] [Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior](https://arxiv.org/abs/2508.19287)
*Zhuotao Lian,Weiyu Wang,Qingkui Zeng,Toru Nakanishi,Teruaki Kitasuka,Chunhua Su*

Main category: cs.CR

TL;DR: This paper investigates a new form of attack on Large Language Models (LLMs) where adversarial instructions are hidden in user-submitted content, causing manipulated outputs without user knowledge.


<details>
  <summary>Details</summary>
Motivation: To identify and address security vulnerabilities in LLM workflows, particularly focusing on how embedded adversarial instructions can compromise output integrity.

Method: The authors demonstrate such attacks on popular LLM platforms, analyze underlying flaws like prompt concatenation and lack of input isolation, and discuss potential mitigations.

Result: Hidden adversarial instructions can effectively manipulate LLM outputs, showing this threat's feasibility and severity in real-world scenarios.

Conclusion: Prompt injection attacks pose a subtle yet practical threat to LLM applications, emphasizing the need for better input handling and security measures in these systems.

Abstract: Large Language Models (LLMs) are widely deployed in applications that accept
user-submitted content, such as uploaded documents or pasted text, for tasks
like summarization and question answering. In this paper, we identify a new
class of attacks, prompt in content injection, where adversarial instructions
are embedded in seemingly benign inputs. When processed by the LLM, these
hidden prompts can manipulate outputs without user awareness or system
compromise, leading to biased summaries, fabricated claims, or misleading
suggestions. We demonstrate the feasibility of such attacks across popular
platforms, analyze their root causes including prompt concatenation and
insufficient input isolation, and discuss mitigation strategies. Our findings
reveal a subtle yet practical threat in real-world LLM workflows.

</details>


### [347] [Tricking LLM-Based NPCs into Spilling Secrets](https://arxiv.org/abs/2508.19288)
*Kyohei Shiomi,Zhuotao Lian,Toru Nakanishi,Teruaki Kitasuka*

Main category: cs.CR

TL;DR: The study investigates the security risks posed by adversarial prompt injections targeting LLM-based NPCs, specifically focusing on extracting hidden secrets.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the security vulnerabilities of LLMs used in dynamic dialogue generation for NPCs in games.

Method: The paper examines whether adversarial prompt injection attacks can manipulate LLM-based NPCs to disclose sensitive information.

Result: Adversarial prompt injections can potentially succeed in forcing NPCs to reveal hidden background secrets.

Conclusion: The integration of LLMs in gaming dialogues introduces security risks that need mitigation strategies against adversarial manipulations.

Abstract: Large Language Models (LLMs) are increasingly used to generate dynamic
dialogue for game NPCs. However, their integration raises new security
concerns. In this study, we examine whether adversarial prompt injection can
cause LLM-based NPCs to reveal hidden background secrets that are meant to
remain undisclosed.

</details>


### [348] [Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience](https://arxiv.org/abs/2508.19292)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Bin Ji,Jun Ma,Xiaodong Liu,Jing Wang,Feilong Bao,Jianfeng Zhang,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: The paper presents JailExpert, an automated jailbreak framework that leverages a formal representation of prior experiences to improve attack effectiveness and efficiency on large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerabilities of large language models against 'jailbreak prompt' attacks and to develop a more robust framework that integrates prior attack experiences to enhance current jailbreak attempts.

Method: The study introduces JailExpert, which formalizes past attack experiences, groups them by semantic drift, and dynamically updates the experience pool to optimize jailbreak efficiency and effectiveness.

Result: JailExpert demonstrates a 17% higher attack success rate and a 2.7 times improvement in attack efficiency compared to existing methods.

Conclusion: Experience-driven frameworks like JailExpert improve both the security analysis and understanding of vulnerabilities in LLMs, setting a foundation for creating more secure AI systems in the future.

Abstract: Large language models (LLMs) generate human-aligned content under certain
safety constraints. However, the current known technique ``jailbreak prompt''
can circumvent safety-aligned measures and induce LLMs to output malicious
content. Research on Jailbreaking can help identify vulnerabilities in LLMs and
guide the development of robust security frameworks. To circumvent the issue of
attack templates becoming obsolete as models evolve, existing methods adopt
iterative mutation and dynamic optimization to facilitate more automated
jailbreak attacks. However, these methods face two challenges: inefficiency and
repetitive optimization, as they overlook the value of past attack experiences.
To better integrate past attack experiences to assist current jailbreak
attempts, we propose the \textbf{JailExpert}, an automated jailbreak framework,
which is the first to achieve a formal representation of experience structure,
group experiences based on semantic drift, and support the dynamic updating of
the experience pool. Extensive experiments demonstrate that JailExpert
significantly improves both attack effectiveness and efficiency. Compared to
the current state-of-the-art black-box jailbreak methods, JailExpert achieves
an average increase of 17\% in attack success rate and 2.7 times improvement in
attack efficiency. Our implementation is available at
\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}

</details>


### [349] [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
*Kehao Miao,Xiaolong Jin*

Main category: cs.CR

TL;DR: The study introduces 'Group Query Attack' to evaluate how consecutive prompts affect LLMs, showing degraded performance and risks of triggering backdoors.


<details>
  <summary>Details</summary>
Motivation: Assess failure modes of LLMs during real-world interactions where users ask multiple consecutive questions.

Method: The authors simulate multiple user interactions through 'Group Query Attack' by presenting grouped queries to LLMs simultaneously.

Result: Group Query Attack compromises task-specific fine-tuned model performance, triggers backdoor vulnerabilities, and negatively impacts reasoning tasks like math and code generation.

Conclusion: The technique underscores challenges in maintaining reliability and security of LLMs during extended user interactions.

Abstract: With the widespread use of large language models (LLMs), understanding their
potential failure modes during user interactions is essential. In practice,
users often pose multiple questions in a single conversation with LLMs.
Therefore, in this study, we propose Group Query Attack, a technique that
simulates this scenario by presenting groups of queries to LLMs simultaneously.
We investigate how the accumulated context from consecutive prompts influences
the outputs of LLMs. Specifically, we observe that Group Query Attack
significantly degrades the performance of models fine-tuned on specific tasks.
Moreover, we demonstrate that Group Query Attack induces a risk of triggering
potential backdoors of LLMs. Besides, Group Query Attack is also effective in
tasks involving reasoning, such as mathematical reasoning and code generation
for pre-trained and aligned models.

</details>


### [350] [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
*Chao Huang,Zefeng Zhang,Juewei Yue,Quangang Li,Chuang Zhang,Tingwen Liu*

Main category: cs.CR

TL;DR: The paper reveals vulnerabilities in LLMs' safety mechanisms due to centralized safety-critical attention heads and proposes a training strategy (AHD) for distributed encoding to enhance robustness.


<details>
  <summary>Details</summary>
Motivation: To address the problem of how adversarial prompts can bypass current LLM safety measures by exploiting vulnerabilities in specific attention heads.

Method: Introduced RDSHA for pinpointing safety-critical attention heads and proposed a new training strategy, AHD, to spread safety behaviors across multiple attention heads.

Result: Experimental results showed AHD successfully distributes safety behaviors, improving robustness against jailbreak attacks without compromising model utility.

Conclusion: Adopting AHD enhances model safety by reducing the concentration of vulnerabilities, thereby improving the robustness of LLMs under adversarial conditions.

Abstract: Current safety alignment for large language models(LLMs) continues to present
vulnerabilities, given that adversarial prompting can effectively bypass their
safety measures.Our investigation shows that these safety mechanisms
predominantly depend on a limited subset of attention heads: removing or
ablating these heads can severely compromise model safety. To identify and
evaluate these safety-critical components, we introduce RDSHA, a targeted
ablation method that leverages the model's refusal direction to pinpoint
attention heads mostly responsible for safety behaviors. Further analysis shows
that existing jailbreak attacks exploit this concentration by selectively
bypassing or manipulating these critical attention heads. To address this
issue, we propose AHD, a novel training strategy designed to promote the
distributed encoding of safety-related behaviors across numerous attention
heads. Experimental results demonstrate that AHD successfully distributes
safety-related capabilities across more attention heads. Moreover, evaluations
under several mainstream jailbreak attacks show that models trained with AHD
exhibit considerably stronger safety robustness, while maintaining overall
functional utility.

</details>


### [351] [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
*Shuo Shao,Yiming Li,Yu He,Hongwei Yao,Wenyuan Yang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: The paper examines the reliability of Large Language Model (LLM) fingerprinting for addressing copyright issues. It introduces a unified framework, formal taxonomy, and the first systematic benchmark, LeaFBench, to evaluate LLM fingerprinting methods under real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is driven by the increasing value of LLMs as intellectual property and their susceptibility to copyright infringement, necessitating robust methods for identifying unauthorized use and model theft.

Method: The paper introduces a unified framework and taxonomy to systematically categorize fingerprinting methods. It also proposes a benchmark, LeaFBench, which evaluates fingerprinting techniques across 149 model instances and integrates 13 post-development techniques to simulate realistic deployment scenarios.

Result: The researchers conducted extensive experiments on LeaFBench, showcasing the effectiveness and limitations of current LLM fingerprinting methods and highlighting areas for improvement.

Conclusion: The study offers a foundational and structured approach to LLM fingerprinting evaluation, emphasizes the need for advancements in the technique, and provides valuable tools and insights for the field of copyright auditing in LLMs.

Abstract: The broad capabilities and substantial resources required to train Large
Language Models (LLMs) make them valuable intellectual property, yet they
remain vulnerable to copyright infringement, such as unauthorized use and model
theft. LLM fingerprinting, a non-intrusive technique that extracts and compares
the distinctive features from LLMs to identify infringements, offers a
promising solution to copyright auditing. However, its reliability remains
uncertain due to the prevalence of diverse model modifications and the lack of
standardized evaluation. In this SoK, we present the first comprehensive study
of LLM fingerprinting. We introduce a unified framework and formal taxonomy
that categorizes existing methods into white-box and black-box approaches,
providing a structured overview of the state of the art. We further propose
LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting
under realistic deployment scenarios. Built upon mainstream foundation models
and comprising 149 distinct model instances, LeaFBench integrates 13
representative post-development techniques, spanning both parameter-altering
methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms
(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the
strengths and weaknesses of existing methods, thereby outlining future research
directions and critical open problems in this emerging field. The code is
available at https://github.com/shaoshuo-ss/LeaFBench.

</details>


### [352] [Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication](https://arxiv.org/abs/2508.19465)
*Onyinye Okoye*

Main category: cs.CR

TL;DR: The paper addresses vulnerabilities in traditional EV authentication systems and proposes an AI-driven adaptive framework based on Zero Trust principles for enhanced security.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of EVs and EVCs has exposed traditional authentication systems like RFID and NFC to significant cybersecurity risks such as cloning, relay attacks, and signal interception.

Method: The paper proposes an AI-driven framework integrating machine learning, anomaly detection, behavioral analytics, and contextual risk assessment to improve authentication systems.

Result: The study highlights the vulnerabilities of existing systems and demonstrates the viability of AI-powered solutions to deliver scalable and resilient authentication mechanisms.

Conclusion: Implementing AI-powered adaptive authentication is vital for safeguarding the future of electric mobility and enhancing digital trust across the EV ecosystem.

Abstract: The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle
Charging Systems (EVCs) has introduced new cybersecurity challenges,
specifically in authentication protocols that protect vehicles, users, and
energy infrastructure. Although widely adopted for convenience, traditional
authentication mechanisms like Radio Frequency Identification (RFID) and Near
Field Communication (NFC) rely on static identifiers and weak encryption,
making them highly vulnerable to attack vectors such as cloning, relay attacks,
and signal interception. This study explores an AI-powered adaptive
authentication framework designed to overcome these shortcomings by integrating
machine learning, anomaly detection, behavioral analytics, and contextual risk
assessment. Grounded in the principles of Zero Trust Architecture, the proposed
framework emphasizes continuous verification, least privilege access, and
secure communication. Through a comprehensive literature review, this research
evaluates current vulnerabilities and highlights AI-driven solutions to provide
a scalable, resilient, and proactive defense. Ultimately, the research findings
conclude that adopting AI-powered adaptive authentication is a strategic
imperative for securing the future of electric mobility and strengthening
digital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,
ML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,
MITM attacks, Zero Trust Architecture

</details>


### [353] [SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis](https://arxiv.org/abs/2508.19472)
*Kyler Katz,Sara Moshtari,Ibrahim Mujhid,Mehdi Mirakhorli,Derek Garcia*

Main category: cs.CR

TL;DR: The paper introduces SIExVulTS, a transformer-based detection system, to identify and verify sensitive information exposure (CWE-200) vulnerabilities, achieving high accuracy and discovering six new CVEs.


<details>
  <summary>Details</summary>
Motivation: Existing tools inadequately detect or provide context-aware analysis for the diverse subcategories of CWE-200 vulnerabilities, leaving sensitive information exposure as a persistent security issue.

Method: A three-stage architecture integrating static analysis and transformer-based models is proposed: (1) detecting potential attack surfaces, (2) instantiating CWE-200 specific queries, and (3) validating source-to-sink flows semantically using GraphCodeBERT.

Result: The system achieved high F1 scores for its detection and verification components, with significant improvements in precision for flow verification and uncovered six unknown CVEs in Apache projects.

Conclusion: SIExVulTS is practical and effective in addressing limitations of existing tools and enhancing software security against sensitive information exposure vulnerabilities.

Abstract: Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a
persistent and under-addressed threat across software systems, often leading to
serious security breaches. Existing detection tools rarely target the diverse
subcategories of CWE-200 or provide context-aware analysis of code-level data
flows.
  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection
system that integrates transformer-based models with static analysis to
identify and verify sensitive information exposure in Java applications.
  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface
Detection Engine that uses sentence embeddings to identify sensitive variables,
strings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates
CodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification
Engine that leverages GraphCodeBERT to semantically validate source-to-sink
flows. We evaluate SIExVulTS using three curated datasets, including real-world
CVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31
open-source projects.
  Results: The Attack Surface Detection Engine achieved an average F1 score
greater than 93\%, the Exposure Analysis Engine achieved an F1 score of
85.71\%, and the Flow Verification Engine increased precision from 22.61\% to
87.23\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs
in major Apache projects.
  Conclusions: The results demonstrate that SIExVulTS is effective and
practical for improving software security against sensitive data exposure,
addressing limitations of existing tools in detecting and verifying CWE-200
vulnerabilities.

</details>


### [354] [Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning](https://arxiv.org/abs/2508.20083)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Kuan Li,Shuai Wang*

Main category: cs.CR

TL;DR: This paper proposes DisarmRAG, a new paradigm of poisoning retrieval-augmented generation systems by targeting the retriever instead of the knowledge base, achieving high attack success rates and evading detection.


<details>
  <summary>Details</summary>
Motivation: Modern retrieval-augmented generation systems are vulnerable to poisoning attacks, but their self-correction ability in large language models can mitigate such threats. This raises the challenge for attackers to bypass these defenses.

Method: The paper introduces DisarmRAG, which poisons the retriever to suppress self-correction ability by embedding anti-SCA instructions using a contrastive-learning-based model editing technique. An iterative co-optimization framework is also designed to find robust malicious instructions.

Result: The proposed DisarmRAG achieves over 90% attack success rates across six LLMs and three QA benchmarks while evading detection methods, demonstrating the effectiveness and stealthiness of the attack.

Conclusion: DisarmRAG highlights a new vulnerability in RAG systems by attacking the retriever and suppressing LLM self-correction. This calls for urgent retriever-focused defense mechanisms to address the threat.

Abstract: Retrieval-Augmented Generation (RAG) has become a standard approach for
improving the reliability of large language models (LLMs). Prior work
demonstrates the vulnerability of RAG systems by misleading them into
generating attacker-chosen outputs through poisoning the knowledge base.
However, this paper uncovers that such attacks could be mitigated by the strong
\textit{self-correction ability (SCA)} of modern LLMs, which can reject false
context once properly configured. This SCA poses a significant challenge for
attackers aiming to manipulate RAG systems.
  In contrast to previous poisoning methods, which primarily target the
knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that
compromises the retriever itself to suppress the SCA and enforce
attacker-chosen outputs. This compromisation enables the attacker to
straightforwardly embed anti-SCA instructions into the context provided to the
generator, thereby bypassing the SCA. To this end, we present a
contrastive-learning-based model editing technique that performs localized and
stealthy edits, ensuring the retriever returns a malicious instruction only for
specific victim queries while preserving benign retrieval behavior. To further
strengthen the attack, we design an iterative co-optimization framework that
automatically discovers robust instructions capable of bypassing prompt-based
defenses. We extensively evaluate DisarmRAG across six LLMs and three QA
benchmarks. Our results show near-perfect retrieval of malicious instructions,
which successfully suppress SCA and achieve attack success rates exceeding 90\%
under diverse defensive prompts. Also, the edited retriever remains stealthy
under several detection methods, highlighting the urgent need for
retriever-centric defenses.

</details>


### [355] [Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills](https://arxiv.org/abs/2508.19500)
*David Noever*

Main category: cs.CR

TL;DR: The paper identifies a vulnerability in MCP-based agent systems that emerges from agents coordinating tasks across services, leading to harmful behaviors.


<details>
  <summary>Details</summary>
Motivation: To reveal a fundamental flaw in the assumption that services in MCP-based systems are inherently isolated, highlighting the risks from coordinated, cross-domain agent actions.

Method: Systematic analysis using the MITRE ATLAS framework, testing 95 agents performing tasks across multiple services, and demonstrating orchestrated attack chains through red team exercises.

Result: Empirical evidence of attack chains exploiting the lack of cross-domain security measures, showing vulnerabilities such as data exfiltration, financial manipulation, and infrastructure compromise.

Conclusion: Service isolation assumptions in MCP systems are inadequate when agents coordinate across domains, exponentially increasing attack surfaces. The paper advocates for experimental directions to address these risks and improve system safety.

Abstract: This paper identifies and analyzes a novel vulnerability class in Model
Context Protocol (MCP) based agent systems. The attack chain describes and
demonstrates how benign, individually authorized tasks can be orchestrated to
produce harmful emergent behaviors. Through systematic analysis using the MITRE
ATLAS framework, we demonstrate how 95 agents tested with access to multiple
services-including browser automation, financial analysis, location tracking,
and code deployment-can chain legitimate operations into sophisticated attack
sequences that extend beyond the security boundaries of any individual service.
These red team exercises survey whether current MCP architectures lack
cross-domain security measures necessary to detect or prevent a large category
of compositional attacks. We present empirical evidence of specific attack
chains that achieve targeted harm through service orchestration, including data
exfiltration, financial manipulation, and infrastructure compromise. These
findings reveal that the fundamental security assumption of service isolation
fails when agents can coordinate actions across multiple domains, creating an
exponential attack surface that grows with each additional capability. This
research provides a barebones experimental framework that evaluate not whether
agents can complete MCP benchmark tasks, but what happens when they complete
them too well and optimize across multiple services in ways that violate human
expectations and safety constraints. We propose three concrete experimental
directions using the existing MCP benchmark suite.

</details>


### [356] [From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2508.19819)
*Viktor Valadi,Mattias Åkesson,Johan Östman,Salman Toor,Andreas Hellander*

Main category: cs.CR

TL;DR: The paper investigates gradient inversion attacks in federated learning, focusing on how model architecture and training behaviors affect vulnerability.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in federated learning due to gradient inversion attacks have prompted a need for understanding under which conditions models are most vulnerable.

Method: Analyzed vulnerability under inference-mode and training-mode operations, introduced novel attacks, studied architectural conditions affecting susceptibility, and tested attacks on production-grade object-detection models.

Result: Found that specific architectural features like shallow and wide models with skip connections and pre-activation normalization influence successful attacks. Developed state-of-the-art attacks under realistic conditions.

Conclusion: Mapped the combinations of architecture and operational modes influencing privacy risks, offering insights into model vulnerability and robustness, which can guide future research and deployment practices.

Abstract: Gradient inversion attacks have garnered attention for their ability to
compromise privacy in federated learning. However, many studies consider
attacks with the model in inference mode, where training-time behaviors like
dropout are disabled and batch normalization relies on fixed statistics. In
this work, we systematically analyze how architecture and training behavior
affect vulnerability, including the first in-depth study of inference-mode
clients, which we show dramatically simplifies inversion. To assess attack
feasibility under more realistic conditions, we turn to clients operating in
standard training mode. In this setting, we find that successful attacks are
only possible when several architectural conditions are met simultaneously:
models must be shallow and wide, use skip connections, and, critically, employ
pre-activation normalization. We introduce two novel attacks against models in
training-mode with varying attacker knowledge, achieving state-of-the-art
performance under realistic training conditions. We extend these efforts by
presenting the first attack on a production-grade object-detection model. Here,
to enable any visibly identifiable leakage, we revert to the lenient inference
mode setting and make multiple architectural modifications to increase model
vulnerability, with the extent of required changes highlighting the strong
inherent robustness of such architectures. We conclude this work by offering
the first comprehensive mapping of settings, clarifying which combinations of
architectural choices and operational modes meaningfully impact privacy. Our
analysis provides actionable insight into when models are likely vulnerable,
when they appear robust, and where subtle leakage may persist. Together, these
findings reframe how gradient inversion risk should be assessed in future
research and deployment scenarios.

</details>


### [357] [A Technical Review on Comparison and Estimation of Steganographic Tools](https://arxiv.org/abs/2508.19323)
*Ms. Preeti P. Bhatt,Rakesh R. Savant*

Main category: cs.CR

TL;DR: The paper reviews and compares six image steganography tools based on their efficiency using various image features. It reveals performance similarities among tools with slight advantages for some.


<details>
  <summary>Details</summary>
Motivation: To identify and analyze the best-performing image steganography tools by evaluating them against uniform image inputs.

Method: Six frequently used steganography tools were selected and tested with identical input images. Each tool embedded specific text in host images, and their performance was compared based on size, dimensions, pixel values, and histogram differentiation.

Result: All six tools showcased relatively similar performance, but certain tools demonstrated better efficiency due to specific image feature differences.

Conclusion: The comparison highlights no significant winner but does emphasize that some tools outperform others marginally depending on image characteristics.

Abstract: Steganography is technique of hiding a data under cover media using different
steganography tools. Image steganography is hiding of data
(Text/Image/Audio/Video) under a cover as Image. This review paper presents
classification of image steganography and the comparison of various Image
steganography tools using different image formats. Analyzing numerous tools on
the basis of Image features and extracting the best one. Some of the tools
available in the market were selected based on the frequent use; these tools
were tested using the same input on all of them. Specific text was embedded
within all host images for each of the six Steganography tools selected. The
results of the experiment reveal that all the six tools were relatively
performing at the same level, though some software performs better than others
through efficiency. And it was based on the image features like size,
dimensions, and pixel value and histogram differentiation.

</details>


### [358] [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents](https://arxiv.org/abs/2508.19493)
*Zhixin Lin,Jungang Li,Shidong Pan,Yibo Shi,Yue Yao,Dongliang Xu*

Main category: cs.CR

TL;DR: The paper evaluates the privacy awareness of smartphone agents powered by Multimodal Large Language Models (MLLMs) using a benchmark of over 7000 scenarios. Results show that most agents perform poorly, with closed-source variants outperforming open-source alternatives.


<details>
  <summary>Details</summary>
Motivation: To address concerns over privacy risks in smartphone agents, which have significant access to sensitive user data, and assess their effectiveness in recognizing privacy contexts.

Method: The authors created a large-scale benchmark with 7,138 scenarios and annotated the privacy contexts including type, sensitivity level, and location. Seven mainstream agents were then carefully evaluated.

Result: Most agents demonstrated poor privacy awareness, with performance scores below 60%. Closed-source agents performed better, with Gemini 2.0-flash achieving the highest RA at 67%.

Conclusion: The study reveals critical gaps in privacy awareness among smartphone agents, urging the research community to reconsider the balance between utility and privacy in such technologies.

Abstract: Smartphones bring significant convenience to users but also enable devices to
extensively record various types of personal information. Existing smartphone
agents powered by Multimodal Large Language Models (MLLMs) have achieved
remarkable performance in automating different tasks. However, as the cost,
these agents are granted substantial access to sensitive users' personal
information during this operation. To gain a thorough understanding of the
privacy awareness of these agents, we present the first large-scale benchmark
encompassing 7,138 scenarios to the best of our knowledge. In addition, for
privacy context in scenarios, we annotate its type (e.g., Account Credentials),
sensitivity level, and location. We then carefully benchmark seven available
mainstream smartphone agents. Our results demonstrate that almost all
benchmarked agents show unsatisfying privacy awareness (RA), with performance
remaining below 60% even with explicit hints. Overall, closed-source agents
show better privacy ability than open-source ones, and Gemini 2.0-flash
achieves the best, achieving an RA of 67%. We also find that the agents'
privacy detection capability is highly related to scenario sensitivity level,
i.e., the scenario with a higher sensitivity level is typically more
identifiable. We hope the findings enlighten the research community to rethink
the unbalanced utility-privacy tradeoff about smartphone agents. Our code and
benchmark are available at https://zhixin-l.github.io/SAPA-Bench.

</details>


### [359] [Addressing Deepfake Issue in Selfie banking through camera based authentication](https://arxiv.org/abs/2508.19714)
*Subhrojyoti Mukherjee,Manoranjan Mohanty*

Main category: cs.CR

TL;DR: The paper investigates using forensic recognition systems for deepfake detection in selfie banking.


<details>
  <summary>Details</summary>
Motivation: Deep learning technologies now create highly realistic fake identities, posing a significant threat to biometric security systems.

Method: The study proposes repurposing forensic systems initially designed for camera localization to detect deepfake images.

Result: No specific results are mentioned in the abstract, but the system's relevance to deepfake detection is implied.

Conclusion: The work suggests forensic recognition systems as a promising approach to tackle fraud in biometric systems due to deepfakes.

Abstract: Fake images in selfie banking are increasingly becoming a threat. Previously,
it was just Photoshop, but now deep learning technologies enable us to create
highly realistic fake identities, which fraudsters exploit to bypass biometric
systems such as facial recognition in online banking. This paper explores the
use of an already established forensic recognition system, previously used for
picture camera localization, in deepfake detection.

</details>


### [360] [Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses](https://arxiv.org/abs/2508.19641)
*Lincan Li,Bolin Shen,Chenxi Zhao,Yuxiang Sun,Kaixiang Zhao,Shirui Pan,Yushun Dong*

Main category: cs.CR

TL;DR: This paper reviews the challenges and solutions related to intellectual property (IP) protection in graph machine learning, considering threats and defenses at both the GML model and graph data level. It introduces a taxonomy, evaluation framework, benchmark datasets, and a library named PyGIP.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing importance and vulnerability of intellectual property in graph machine learning due to the resource-intensive nature of training and threats posed by deploying GMLaaS systems.

Method: The authors construct a taxonomy of threats and defenses, develop an evaluation framework for IP protection methods, compile benchmark datasets across domains, and release PyGIP, an open-source library for implementing and testing attack and defense techniques.

Result: This study provides a systematic understanding of threats, suitable defenses, and evaluation metrics for protecting intellectual property in GML models and data. It also delivers practical tools, such as PyGIP, for the research and development community.

Conclusion: The survey provides foundational knowledge for protecting intellectual property in graph machine learning and practical tools to guide researchers and practitioners in this increasingly significant area.

Abstract: Graph-structured data, which captures non-Euclidean relationships and
interactions between entities, is growing in scale and complexity. As a result,
training state-of-the-art graph machine learning (GML) models have become
increasingly resource-intensive, turning these models and data into invaluable
Intellectual Property (IP). To address the resource-intensive nature of model
training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an
efficient solution by leveraging third-party cloud services for model
development and management. However, deploying such models in GMLaaS also
exposes them to potential threats from attackers. Specifically, while the APIs
within a GMLaaS system provide interfaces for users to query the model and
receive outputs, they also allow attackers to exploit and steal model
functionalities or sensitive training data, posing severe threats to the safety
of these GML models and the underlying graph data. To address these challenges,
this survey systematically introduces the first taxonomy of threats and
defenses at the level of both GML model and graph-structured data. Such a
tailored taxonomy facilitates an in-depth understanding of GML IP protection.
Furthermore, we present a systematic evaluation framework to assess the
effectiveness of IP protection methods, introduce a curated set of benchmark
datasets across various domains, and discuss their application scopes and
future challenges. Finally, we establish an open-sourced versatile library
named PyGIP, which evaluates various attack and defense techniques in GMLaaS
scenarios and facilitates the implementation of existing benchmark methods. The
library resource can be accessed at: https://labrai.github.io/PyGIP. We believe
this survey will play a fundamental role in intellectual property protection
for GML and provide practical recipes for the GML community.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [361] [Word Chain Generators for Prefix Normal Words](https://arxiv.org/abs/2508.19619)
*Duncan Adamson,Moritz Dudey,Pamela Fleischmann,Annika Huch*

Main category: math.CO

TL;DR: The paper focuses on prefix normal words, their enumeration, efficient testing methods, and introduces word chains and generators as new ways to relate words.


<details>
  <summary>Details</summary>
Motivation: To address open problems in understanding, enumerating, and testing prefix normal words, as well as exploring their unique properties.

Method: The authors propose analyzing factors responsible for non-prefix normality and establish word chains and generators as tools for relating words of the same length.

Result: The study identifies various characteristics of prefix normal words and provides insights into their behavior with factors and novel mechanisms like word chains.

Conclusion: This research advances the understanding of prefix normal words and lays the groundwork for exploring their properties and relations systematically.

Abstract: In 2011, Fici and Lipt\'ak introduced prefix normal words. A binary word is
prefix normal if it has no factor (substring) that contains more occurrences of
the letter 1 than the prefix of the same length. Among the open problems
regarding this topic are the enumeration of prefix normal words and efficient
testing methods. We show a range of characteristics of prefix normal words.
These include properties of factors that are responsible for a word not being
prefix normal. With word chains and generators, we introduce new ways of
relating words of the same length to each other.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [362] [Training for Obsolescence? The AI-Driven Education Trap](https://arxiv.org/abs/2508.19625)
*Andrew J. Peterson*

Main category: econ.GN

TL;DR: The paper studies the dual impact of AI on education and labor markets, highlighting a mismatch between AI-driven teaching efficiency and skills demanded by future labor markets.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from understanding how AI's effects on education and labor markets interact, potentially leading to misaligned educational investments.

Method: The paper models the decision-making process of an educational planner and incorporates findings from a pilot survey to analyze correlations between AI teaching productivity and wage suppression.

Result: It finds that increased reliance on AI can lead to a skill mismatch in education that worsens with AI prevalence, exacerbated by prioritizing AI over unpriced non-cognitive skills.

Conclusion: Policies promoting AI in education need to incorporate predictive labor market trends to ensure balanced human capital development, avoiding suppression of unpriced non-cognitive skills.

Abstract: Artificial intelligence simultaneously transforms human capital production in
schools and its demand in labor markets. Analyzing these effects in isolation
can lead to a significant misallocation of educational resources. We model an
educational planner whose decision to adopt AI is driven by its teaching
productivity, failing to internalize AI's future wage-suppressing effect on
those same skills. Our core assumption, motivated by a pilot survey, is that
there is a positive correlation between these two effects. This drives our
central proposition: this information failure creates a skill mismatch that
monotonically increases with AI prevalence. Extensions show the mismatch is
exacerbated by the neglect of unpriced non-cognitive skills and by a school's
endogenous over-investment in AI. Our findings caution that policies promoting
AI in education, if not paired with forward-looking labor market signals, may
paradoxically undermine students' long-term human capital, especially if
reliance on AI crowds out the development of unpriced non-cognitive skills,
such as persistence, that are forged through intellectual struggle.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [363] [Neural Conditional Simulation for Complex Spatial Processes](https://arxiv.org/abs/2508.20067)
*Julia Walchessen,Andrew Zammit-Mangion,Raphaël Huser,Mikael Kuusela*

Main category: stat.ME

TL;DR: This paper introduces "neural conditional simulation" (NCS), a technique leveraging neural diffusion models for efficient conditional simulation from challenging spatial predictive distributions.


<details>
  <summary>Details</summary>
Motivation: Spatial predictive distributions are integral to spatial prediction and uncertainty quantification, but exact conditional simulation is infeasible or inefficient for many models.

Method: The authors utilize neural diffusion models with spatial masks to develop a conditional score-based diffusion model, training the neural network using unconditional samples and making it reusable across diverse inputs without retraining.

Result: The proposed model demonstrated efficiency and accuracy in generating conditional simulations, outperforming traditional MCMC techniques for complex spatial processes like the Brown-Resnick process.

Conclusion: NCS provides an innovative, efficient method for conditional simulation of spatial processes, enabling broader application in scenarios where traditional methods are inefficient.

Abstract: A key objective in spatial statistics is to simulate from the distribution of
a spatial process at a selection of unobserved locations conditional on
observations (i.e., a predictive distribution) to enable spatial prediction and
uncertainty quantification. However, exact conditional simulation from this
predictive distribution is intractable or inefficient for many spatial process
models. In this paper, we propose neural conditional simulation (NCS), a
general method for spatial conditional simulation that is based on neural
diffusion models. Specifically, using spatial masks, we implement a conditional
score-based diffusion model that evolves Gaussian noise into samples from a
predictive distribution when given a partially observed spatial field and
spatial process parameters as inputs. The diffusion model relies on a neural
network that only requires unconditional samples from the spatial process for
training. Once trained, the diffusion model is amortized with respect to the
observations in the partially observed field, the number and locations of those
observations, and the spatial process parameters, and can therefore be used to
conditionally simulate from a broad class of predictive distributions without
retraining the neural network. We assess the NCS-generated simulations against
simulations from the true conditional distribution of a Gaussian process model,
and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick
process model for spatial extremes. In the latter case, we show that it is more
efficient and accurate to conditionally simulate using NCS than classical MCMC
techniques implemented in standard software. We conclude that NCS enables
efficient and accurate conditional simulation from spatial predictive
distributions that are challenging to sample from using traditional methods.

</details>
