<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 14]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.CV](#cs.CV) [Total: 14]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.LG](#cs.LG) [Total: 27]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.SE](#cs.SE) [Total: 11]
- [q-bio.NC](#q-bio.NC) [Total: 7]
- [stat.ML](#stat.ML) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [stat.ME](#stat.ME) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Path to Loving](https://arxiv.org/abs/2506.05352)
*John Beverley,Regina Hurley*

Main category: cs.AI

TL;DR: The paper provides an ontological framework to rigorously define love, blending its emotional and rational aspects, with implications for AI and scientific disciplines.


<details>
  <summary>Details</summary>
Motivation: To address love's philosophical and interdisciplinary complexity and establish a formal framework for its understanding in fields like psychology, sociology, and AI.

Method: Utilizes Basic Formal Ontology (BFO) and applied ontological methods to define love as a blend of emotional sensations and rational evaluations, while addressing objections through a causal correlation model.

Result: Establishes a structured framework that balances the emotional and rational dimensions of love, enabling interdisciplinary applications in AI, ontology engineering, and sciences.

Conclusion: The work presents a scalable and precise ontological account of love, paving the way for its formal study across scientific, philosophical, and AI domains.

Abstract: This work lays the foundations for a rigorous ontological characterization of
love, addressing its philosophical complexity and scientific relevance, with
particular emphasis on psychology and sociology, as well as highlighting ways
in which such characterization enhances relevant AI based applications. The
position defended here is that love is best understood as a concatenation of
passive sensations (e.g., emotional arousal) and active evaluative judgments
(e.g., perceiving the beloved as valuable), in the interest of balancing the
involuntary aspects of love with its rational accountability. To provide a
structured foundation, the paper draws on Basic Formal Ontology (BFO) and other
applied ontological methods to differentiate various senses of love. This work
engages with objections to the understanding of love as concatenation,
particularly concerning the relationship between sensation and judgment. A
causal correlation model is defended, ensuring that the affective and cognitive
components are linked. By offering a precise and scalable ontological account,
this work lays the foundation for future interdisciplinary applications, making
love a subject of formal inquiry in ontology engineering, artificial
intelligence, and the sciences.

</details>


### [2] [Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems](https://arxiv.org/abs/2506.05370)
*Kristy Wedel*

Main category: cs.AI

TL;DR: The paper introduces Contextual Memory Intelligence (CMI) to overcome memory limitations in generative AI systems by incorporating structured context for longitudinal coherence, explainability, and responsible decision-making.


<details>
  <summary>Details</summary>
Motivation: Generative AI systems face significant challenges due to memory limitations, which hinder decision-making, often leading to errors and lack of clarity.

Method: CMI is proposed as a framework that redefines memory as an adaptive infrastructure, incorporating structured context through an Insight Layer featuring human-in-the-loop reflection, drift detection, and rationale preservation.

Result: CMI enables systems to reason using data, history, judgment, and context, thereby addressing critical shortcomings in current AI architectures.

Conclusion: CMI enhances the effectiveness, accountability, and social responsibility of AI systems, fostering improved human-AI collaboration and institutional resilience.

Abstract: A critical challenge remains unresolved as generative AI systems are quickly
implemented in various organizational settings. Despite significant advances in
memory components such as RAG, vector stores, and LLM agents, these systems
still have substantial memory limitations. Gen AI workflows rarely store or
reflect on the full context in which decisions are made. This leads to repeated
errors and a general lack of clarity. This paper introduces Contextual Memory
Intelligence (CMI) as a new foundational paradigm for building intelligent
systems. It repositions memory as an adaptive infrastructure necessary for
longitudinal coherence, explainability, and responsible decision-making rather
than passive data. Drawing on cognitive science, organizational theory,
human-computer interaction, and AI governance, CMI formalizes the structured
capture, inference, and regeneration of context as a fundamental system
capability. The Insight Layer is presented in this paper to operationalize this
vision. This modular architecture uses human-in-the-loop reflection, drift
detection, and rationale preservation to incorporate contextual memory into
systems. The paper argues that CMI allows systems to reason with data, history,
judgment, and changing context, thereby addressing a foundational blind spot in
current AI architectures and governance efforts. A framework for creating
intelligent systems that are effective, reflective, auditable, and socially
responsible is presented through CMI. This enhances human-AI collaboration,
generative AI design, and the resilience of the institutions.

</details>


### [3] [Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference](https://arxiv.org/abs/2506.05422)
*Andrei T. Patrascu*

Main category: cs.AI

TL;DR: The paper introduces a logic-driven approach to learning and planning, replacing reward-based strategies with constructive logical inference.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies, safety risks, and lack of interpretability in traditional reward-based reinforcement learning approaches.

Method: The authors employ intuitionistic logic to develop a framework where actions and transitions are verified through constructive logical proofs, avoiding probabilistic trial-and-error.

Result: The symbolic agent achieves perfect safety, efficient learning, and interpretable behavior compared to conventional Q-learning in a structured gridworld environment.

Conclusion: The approach promotes safe planning and symbolic reasoning in AI by grounding decision-making in constructive logic rather than numeric optimisation.

Abstract: We introduce a novel learning and planning framework that replaces
traditional reward-based optimisation with constructive logical inference. In
our model, actions, transitions, and goals are represented as logical
propositions, and decision-making proceeds by building constructive proofs
under intuitionistic logic. This method ensures that state transitions and
policies are accepted only when supported by verifiable preconditions --
eschewing probabilistic trial-and-error in favour of guaranteed logical
validity. We implement a symbolic agent operating in a structured gridworld,
where reaching a goal requires satisfying a chain of intermediate subgoals
(e.g., collecting keys to open doors), each governed by logical constraints.
Unlike conventional reinforcement learning agents, which require extensive
exploration and suffer from unsafe or invalid transitions, our constructive
agent builds a provably correct plan through goal chaining, condition tracking,
and knowledge accumulation. Empirical comparison with Q-learning demonstrates
that our method achieves perfect safety, interpretable behaviour, and efficient
convergence with no invalid actions, highlighting its potential for safe
planning, symbolic cognition, and trustworthy AI. This work presents a new
direction for reinforcement learning grounded not in numeric optimisation, but
in constructive logic and proof theory.

</details>


### [4] [Towards Data Systems That Are Business Semantic-Centric and AI Agents-Assisted](https://arxiv.org/abs/2506.05520)
*Cecil Pang*

Main category: cs.AI

TL;DR: The paper introduces the BSDS system to align data systems with business needs by integrating architecture, AI agents, and workflows.


<details>
  <summary>Details</summary>
Motivation: Existing data platforms fail to adequately align with business objectives, focusing more on tools than on business needs.

Method: The BSDS system integrates modular architecture (data linked to business entities), context-aware AI agents, efficient data pipelines, and workflows optimized for exploration and production.

Result: BSDS has been validated in real-world settings, resulting in faster initiatives, better cross-functional collaboration, and scalability.

Conclusion: BSDS transforms data systems from passive tools to active business enablers while serving as a scalable model. Future research can advance these concepts.

Abstract: Contemporary businesses operate in dynamic environments requiring rapid
adaptation to achieve goals and maintain competitiveness. Existing data
platforms often fall short by emphasizing tools over alignment with business
needs, resulting in inefficiencies and delays. To address this gap, I propose
the Business Semantics Centric, AI Agents Assisted Data System (BSDS), a
holistic system that integrates architecture, workflows, and team organization
to ensure data systems are tailored to business priorities rather than dictated
by technical constraints. BSDS redefines data systems as dynamic enablers of
business success, transforming them from passive tools into active drivers of
organizational growth. BSDS has a modular architecture that comprises curated
data linked to business entities, a knowledge base for context-aware AI agents,
and efficient data pipelines. AI agents play a pivotal role in assisting with
data access and system management, reducing human effort, and improving
scalability. Complementing this architecture, BSDS incorporates workflows
optimized for both exploratory data analysis and production requirements,
balancing speed of delivery with quality assurance. A key innovation of BSDS is
its incorporation of the human factor. By aligning data team expertise with
business semantics, BSDS bridges the gap between technical capabilities and
business needs. Validated through real-world implementation, BSDS accelerates
time-to-market for data-driven initiatives, enhances cross-functional
collaboration, and provides a scalable blueprint for businesses of all sizes.
Future research can build on BSDS to explore optimization strategies using
complex systems and adaptive network theories, as well as developing autonomous
data systems leveraging AI agents.

</details>


### [5] [Avoiding Death through Fear Intrinsic Conditioning](https://arxiv.org/abs/2506.05529)
*Rodney Sanchez,Ferat Sahin,Alexander Ororbia,Jamison Heard*

Main category: cs.AI

TL;DR: The paper introduces a biologically-inspired intrinsic reward function using a memory-augmented neural network to model avoidance behavior in agents, tackling environments with non-descriptive terminal states like death.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of engineering reward functions for environments that include non-descriptive terminal states, where agents lack feedback, such as death.

Method: The study introduces an intrinsic reward inspired by early amygdala development and employs a memory-augmented neural network (MANN) architecture to model avoidance behavior through fear conditioning.

Result: The intrinsic motivation resulted in avoidance behavior similar to fear conditioning in animals. It also demonstrated how varying the fear response threshold could model behaviors seen in General Anxiety Disorders (GADs).

Conclusion: A biologically-inspired neural architecture was developed, enabling agents to solve environments with sparse rewards and non-descriptive terminal conditions, mimicking behaviors observed in animals.

Abstract: Biological and psychological concepts have inspired reinforcement learning
algorithms to create new complex behaviors that expand agents' capacity. These
behaviors can be seen in the rise of techniques like goal decomposition,
curriculum, and intrinsic rewards, which have paved the way for these complex
behaviors. One limitation in evaluating these methods is the requirement for
engineered extrinsic for realistic environments. A central challenge in
engineering the necessary reward function(s) comes from these environments
containing states that carry high negative rewards, but provide no feedback to
the agent. Death is one such stimuli that fails to provide direct feedback to
the agent. In this work, we introduce an intrinsic reward function inspired by
early amygdala development and produce this intrinsic reward through a novel
memory-augmented neural network (MANN) architecture. We show how this intrinsic
motivation serves to deter exploration of terminal states and results in
avoidance behavior similar to fear conditioning observed in animals.
Furthermore, we demonstrate how modifying a threshold where the fear response
is active produces a range of behaviors that are described under the paradigm
of general anxiety disorders (GADs). We demonstrate this behavior in the
Miniworld Sidewalk environment, which provides a partially observable Markov
decision process (POMDP) and a sparse reward with a non-descriptive terminal
condition, i.e., death. In effect, this study results in a
biologically-inspired neural architecture and framework for fear conditioning
paradigms; we empirically demonstrate avoidance behavior in a constructed agent
that is able to solve environments with non-descriptive terminal conditions.

</details>


### [6] [When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration](https://arxiv.org/abs/2506.05579)
*Quan Shi,Carlos E. Jimenez,Shunyu Yao,Nick Haber,Diyi Yang,Karthik Narasimhan*

Main category: cs.AI

TL;DR: This study examines the ability of AI models to transfer knowledge to humans effectively and proposes a new framework, KITE, for investigating this.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to evaluate whether advancements in AI reasoning also yield better human-understandable and applicable knowledge transfer.

Method: The team developed KITE, a framework for assessing AI-human knowledge transfer, and conducted a large-scale human study involving two phases: ideating strategies with AI and independently implementing solutions.

Result: The study found that while model performance correlates with collaborative success, the connection is inconsistent, highlighting the need for dedicated optimization. Behavioral and strategic factors were identified as key mediators of effective knowledge transfer.

Conclusion: Knowledge transfer from AI to humans is not inherently tied to model performance and requires specific optimizations. The study provides tools and data to promote further research in this area.

Abstract: Recent advancements in AI reasoning have driven substantial improvements
across diverse tasks. A critical open question is whether these improvements
also yields better knowledge transfer: the ability of models to communicate
reasoning in ways humans can understand, apply, and learn from. To investigate
this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a
conceptual and experimental framework for Human-AI knowledge transfer
capabilities and conduct the first large-scale human study (N=118) explicitly
designed to measure it. In our two-phase setup, humans first ideate with an AI
on problem-solving strategies, then independently implement solutions,
isolating model explanations' influence on human understanding. Our findings
reveal that although model benchmark performance correlates with collaborative
outcomes, this relationship is notably inconsistent, featuring significant
outliers, indicating that knowledge transfer requires dedicated optimization.
Our analysis identifies behavioral and strategic factors mediating successful
knowledge transfer. We release our code, dataset, and evaluation framework to
support future work on communicatively aligned models.

</details>


### [7] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: The paper introduces MMTU, a benchmark to evaluate models on complex table-related tasks, highlighting shortcomings in current frontier models.


<details>
  <summary>Details</summary>
Motivation: Benchmarking for table-related tasks is underexplored despite the importance of tables in professional applications such as spreadsheets and computational notebooks.

Method: Creation of the MMTU benchmark, consisting of 30K+ questions across 25 table tasks, drawing on decades of computer science research on tabular data.

Result: Evaluation shows frontier models perform at around 60% accuracy on MMTU, indicating current limitations in handling complex table tasks.

Conclusion: MMTU can drive advancements in foundation models for structured data analysis, addressing a gap in benchmarking for table-related tasks.

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [8] [Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists](https://arxiv.org/abs/2506.05616)
*Lianhao Zhou,Hongyi Ling,Keqiang Yan,Kaiji Zhao,Xiaoning Qian,Raymundo Arróyave,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.AI

TL;DR: The paper introduces MAPPS, an autonomous framework using AI for crystal material discovery by integrating workflow planning, physics models, and scientific input.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create autonomous language agents that can automate workflow planning for material discovery instead of limiting them to predefined tasks.

Method: MAPPS combines three elements: Workflow Planner using large language models, Tool Code Generator for executable Python code, and a Scientific Mediator to integrate scientist feedback and ensure error handling.

Result: MAPPS outperforms previous generative models with a five-fold improvement in stability, uniqueness, and novelty rates on the MP-20 dataset.

Conclusion: This framework shows great potential in enabling autonomous and flexible material discovery through its novel integration of planning, physics, and scientific input.

Abstract: We aim at designing language agents with greater autonomy for crystal
materials discovery. While most of existing studies restrict the agents to
perform specific tasks within predefined workflows, we aim to automate workflow
planning given high-level goals and scientist intuition. To this end, we
propose Materials Agent unifying Planning, Physics, and Scientists, known as
MAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a
Scientific Mediator. The Workflow Planner uses large language models (LLMs) to
generate structured and multi-step workflows. The Tool Code Generator
synthesizes executable Python code for various tasks, including invoking a
force field foundation model that encodes physics. The Scientific Mediator
coordinates communications, facilitates scientist feedback, and ensures
robustness through error reflection and recovery. By unifying planning,
physics, and scientists, MAPPS enables flexible and reliable materials
discovery with greater autonomy, achieving a five-fold improvement in
stability, uniqueness, and novelty rates compared with prior generative models
when evaluated on the MP-20 data. We provide extensive experiments across
diverse tasks to show that MAPPS is a promising framework for autonomous
materials discovery.

</details>


### [9] [Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach](https://arxiv.org/abs/2506.05619)
*Kihyun Kim,Jiawei Zhang,Asuman Ozdaglar,Pablo A. Parrilo*

Main category: cs.AI

TL;DR: The paper introduces a framework to align aggregated preferences in accordance with population distributions while respecting several axioms from social choice theory.


<details>
  <summary>Details</summary>
Motivation: Existing preference learning methods show biases by favoring widely held opinions, potentially excluding minority views.

Method: The approach estimates evaluator population distributions from pairwise data and constructs policies satisfying social choice theory axioms, introducing new axioms like population-proportional representation.

Result: The method uses a soft-max optimization technique to balance population representation with Condorcet winner criteria and demonstrates effectiveness on recommendation tasks and language model alignment experiments.

Conclusion: The proposed framework offers a scalable and equitable way to align preferences reflecting true population distributions while adhering to core social choice principles.

Abstract: Conventional preference learning methods often prioritize opinions held more
widely when aggregating preferences from multiple evaluators. This may result
in policies that are biased in favor of some types of opinions or groups. The
objective of this paper is to develop a novel preference learning framework
capable of aligning aggregate opinions and policies proportionally with the
true population distribution of evaluator preferences. Our approach infers the
feasible set of evaluator population distributions directly from pairwise
comparison data. Using these estimates, the algorithm constructs a policy that
satisfies foundational axioms from social choice theory, namely monotonicity
and Pareto efficiency, as well as our newly-introduced axioms of
population-proportional representation and population-bounded robustness. We
propose a soft-max relaxation method that smoothly trade-offs
population-proportional representation with the selection of the Condorcet
winner (which beats all other options in pairwise comparisons). Finally, we
validate the effectiveness and scalability of our approach through experiments
on both tabular recommendation tasks and large-scale language model alignment.

</details>


### [10] [Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/abs/2506.05744)
*Gouki Minegishi,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: The paper investigates reasoning graphs in large-scale models to understand their internal mechanisms and proposes graph-theoretic insights for model improvement.


<details>
  <summary>Details</summary>
Motivation: Large-scale reasoning models perform well but their internal mechanisms are poorly understood, prompting the need for systematic analysis.

Method: The study introduces reasoning graphs extracted from clustering hidden-state representations, analyzing graph properties like cyclicity, diameter, and small-world index on various tasks and comparing distilled and base models.

Result: Distilled models showed more recurrent cycles, larger graph diameters, and stronger small-world characteristics, with these features correlating positively with task difficulty, model size, and accuracy.

Conclusion: Graph-theoretic insights can improve dataset design and boost interpretability and performance of reasoning models.

Abstract: Recent large-scale reasoning models have achieved state-of-the-art
performance on challenging mathematical benchmarks, yet the internal mechanisms
underlying their success remain poorly understood. In this work, we introduce
the notion of a reasoning graph, extracted by clustering hidden-state
representations at each reasoning step, and systematically analyze three key
graph-theoretic properties: cyclicity, diameter, and small-world index, across
multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled
reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly
more recurrent cycles (about 5 per sample), substantially larger graph
diameters, and pronounced small-world characteristics (about 6x) compared to
their base counterparts. Notably, these structural advantages grow with task
difficulty and model capacity, with cycle detection peaking at the 14B scale
and exploration diameter maximized in the 32B variant, correlating positively
with accuracy. Furthermore, we show that supervised fine-tuning on an improved
dataset systematically expands reasoning graph diameters in tandem with
performance gains, offering concrete guidelines for dataset design aimed at
boosting reasoning capabilities. By bridging theoretical insights into
reasoning graph structures with practical recommendations for data
construction, our work advances both the interpretability and the efficacy of
large reasoning models.

</details>


### [11] [SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models](https://arxiv.org/abs/2506.05745)
*Emil Biju,Shayan Talaei,Zhemin Huang,Mohammadreza Pourreza,Azalia Mirhoseini,Amin Saberi*

Main category: cs.AI

TL;DR: SPRINT is a post-training framework that allows large reasoning models (LRMs) to perform complex tasks more quickly by parallelizing reasoning processes, achieving significantly reduced output tokens while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The slow inference times of LRMs during complex reasoning tasks due to sequential chains-of-thought inspired the need for a framework to optimize their reasoning processes.

Method: SPRINT uses a data curation pipeline to structure tasks into rounds of long-horizon planning and parallel execution. LRMs are fine-tuned on curated datasets to identify independent subtasks and execute them in parallel.

Result: The models fine-tuned with SPRINT reduced sequential tokens by ~39% on tasks requiring over 8000 tokens, while maintaining comparable performance in mathematical problem-solving and achieving up to 45%-65% token reductions on out-of-distribution tasks.

Conclusion: The SPRINT framework enables faster reasoning in LRMs by leveraging parallelization, demonstrating its potential to significantly improve reasoning speed without compromising accuracy.

Abstract: Large reasoning models (LRMs) excel at complex reasoning tasks but typically
generate lengthy sequential chains-of-thought, resulting in long inference
times before arriving at the final answer. To address this challenge, we
introduce SPRINT, a novel post-training and inference-time framework designed
to enable LRMs to dynamically identify and exploit opportunities for
parallelization during their reasoning process. SPRINT incorporates an
innovative data curation pipeline that reorganizes natural language reasoning
trajectories into structured rounds of long-horizon planning and parallel
execution. By fine-tuning LRMs on a small amount of such curated data, the
models learn to dynamically identify independent subtasks within extended
reasoning processes and effectively execute them in parallel. Through extensive
evaluations, we show that the models fine-tuned with the SPRINT framework match
the performance of reasoning models on complex domains such as mathematics
while generating up to ~39% fewer sequential tokens on problems requiring more
than 8000 output tokens. Finally, we observe consistent results transferred to
two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%
reduction in average sequential tokens for longer reasoning trajectories, while
achieving the performance of the fine-tuned reasoning model.

</details>


### [12] [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)
*Emmanuel Anaya Gonzalez,Sairam Vaidya,Kanghee Park,Ruyi Ji,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: The paper introduces an MCMC-based constrained sampling framework to generate samples that satisfy hard constraints without distorting the language model’s distribution.


<details>
  <summary>Details</summary>
Motivation: Existing constrained-decoding methods often alter the LM's underlying distribution, which is problematic for tasks requiring diverse, constraint-satisfying outputs, such as program fuzzing.

Method: The proposed method constructs a proposal distribution over valid outputs and employs a Metropolis-Hastings acceptance criterion that leverages the LM's likelihood for efficient exploration of constrained spaces.

Result: Their method demonstrates superior performance compared to existing techniques on both synthetic benchmarks and real-world program fuzzing tasks.

Conclusion: This framework offers a principled and efficient approach to constrained sampling, achieving constraint satisfaction, convergence to the true distribution, and sampling efficiency.

Abstract: Constrained decoding enables Language Models (LMs) to produce samples that
provably satisfy hard constraints. However, existing constrained-decoding
approaches often distort the underlying model distribution, a limitation that
is especially problematic in applications like program fuzzing, where one wants
to generate diverse and valid program inputs for testing purposes. We propose a
new constrained sampling framework based on Markov Chain Monte Carlo (MCMC)
that simultaneously satisfies three core desiderata: constraint satisfying
(every sample satisfies the constraint), monotonically converging (the sampling
process converges to the true conditional distribution), and efficient
(high-quality samples emerge in few steps). Our method constructs a proposal
distribution over valid outputs and applies a Metropolis-Hastings acceptance
criterion based on the LM's likelihood, ensuring principled and efficient
exploration of the constrained space. Empirically, our sampler outperforms
existing methods on both synthetic benchmarks and real-world program fuzzing
tasks.

</details>


### [13] [Trajectory Entropy: Modeling Game State Stability from Multimodality Trajectory Prediction](https://arxiv.org/abs/2506.05810)
*Yesheng Zhang,Wenjian Sun,Yuheng Chen,Qingwei Liu,Qi Lin,Rui Zhang,Xu Zhao*

Main category: cs.AI

TL;DR: The paper proposes a new metric, Trajectory Entropy, to address the inefficiencies in the level-k game framework for agent interactions in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: To combat redundant calculations and poor adaptability in hierarchical game-level policies for autonomous driving due to a uniform treatment of diverse agent complexities and dynamic states.

Method: Introducing Trajectory Entropy to measure agent policy uncertainty using trajectory prediction multimodality, refining the level-k game framework through a gating mechanism.

Result: Achieved state-of-the-art results with up to 19.89% improvement in trajectory prediction and up to 16.48% in planning tasks on Waymo and nuPlan datasets.

Conclusion: Trajectory Entropy effectively optimizes the level-k game framework for computational efficiency and accuracy in autonomous driving scenarios.

Abstract: Complex interactions among agents present a significant challenge for
autonomous driving in real-world scenarios. Recently, a promising approach has
emerged, which formulates the interactions of agents as a level-k game
framework. It effectively decouples agent policies by hierarchical game levels.
However, this framework ignores both the varying driving complexities among
agents and the dynamic changes in agent states across game levels, instead
treating them uniformly. Consequently, redundant and error-prone computations
are introduced into this framework. To tackle the issue, this paper proposes a
metric, termed as Trajectory Entropy, to reveal the game status of agents
within the level-k game framework. The key insight stems from recognizing the
inherit relationship between agent policy uncertainty and the associated
driving complexity. Specifically, Trajectory Entropy extracts statistical
signals representing uncertainty from the multimodality trajectory prediction
results of agents in the game. Then, the signal-to-noise ratio of this signal
is utilized to quantify the game status of agents. Based on the proposed
Trajectory Entropy, we refine the current level-k game framework through a
simple gating mechanism, significantly improving overall accuracy while
reducing computational costs. Our method is evaluated on the Waymo and nuPlan
datasets, in terms of trajectory prediction, open-loop and closed-loop planning
tasks. The results demonstrate the state-of-the-art performance of our method,
with precision improved by up to 19.89% for prediction and up to 16.48% for
planning.

</details>


### [14] [Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs](https://arxiv.org/abs/2506.05887)
*Marilyn Bello,Rafael Bello,Maria-Matilde García,Ann Nowé,Iván Sevillano-García,Francisco Herrera*

Main category: cs.AI

TL;DR: The paper introduces a multilevel framework for explainable AI (XAI), emphasizing diverse stakeholder needs and trust-building through improved explanation delivery.


<details>
  <summary>Details</summary>
Motivation: To address the need for trust and explainability in AI systems, especially considering the varied audiences who interact with these systems.

Method: The authors propose a three-layered framework (algorithmic and domain-based, human-centered, and social explainability) and discuss using Large Language Models (LLMs) for generating accessible explanations.

Result: Case studies demonstrate how the framework enhances technical accuracy, user engagement, and societal accountability in explainable AI.

Conclusion: XAI is repositioned as a trust-building process, emphasizing stakeholder expectations and leveraging LLMs to advance explainability.

Abstract: The growing application of artificial intelligence in sensitive domains has
intensified the demand for systems that are not only accurate but also
explainable and trustworthy. Although explainable AI (XAI) methods have
proliferated, many do not consider the diverse audiences that interact with AI
systems: from developers and domain experts to end-users and society. This
paper addresses how trust in AI is influenced by the design and delivery of
explanations and proposes a multilevel framework that aligns explanations with
the epistemic, contextual, and ethical expectations of different stakeholders.
The framework consists of three layers: algorithmic and domain-based,
human-centered, and social explainability. We highlight the emerging role of
Large Language Models (LLMs) in enhancing the social layer by generating
accessible, natural language explanations. Through illustrative case studies,
we demonstrate how this approach facilitates technical fidelity, user
engagement, and societal accountability, reframing XAI as a dynamic,
trust-building process.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation](https://arxiv.org/abs/2506.05566)
*Chenhui Deng,Yun-Da Tsai,Guan-Ting Liu,Zhongzhi Yu,Haoxing Ren*

Main category: cs.AR

TL;DR: ScaleRTL introduces a reasoning-based LLM specifically optimized for RTL coding, addressing data scarcity and facilitating improved test-time scaling.


<details>
  <summary>Details</summary>
Motivation: Large language models demonstrate near-human performance in software coding, but struggle with RTL code generation due to limited quality training data and non-reasoning nature of prior approaches.

Method: The authors develop ScaleRTL by creating a dataset with 3.5B tokens using long chain-of-thought reasoning trace data and fine-tuning a general-purpose reasoning model. They enhance test-time performance through iterative self-correction strategies.

Result: ScaleRTL achieves state-of-the-art results, surpassing 18 baselines, with up to 18.4% improvement on VerilogEval and 12.7% on RTLLM benchmarks.

Conclusion: ScaleRTL successfully scales both data and test-time computational reasoning, establishing itself as an advanced LLM for deep RTL reasoning tasks.

Abstract: Recent advances in large language models (LLMs) have enabled near-human
performance on software coding benchmarks, but their effectiveness in RTL code
generation remains limited due to the scarcity of high-quality training data.
While prior efforts have fine-tuned LLMs for RTL tasks, they do not
fundamentally overcome the data bottleneck and lack support for test-time
scaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,
the first reasoning LLM for RTL coding that scales up both high-quality
reasoning data and test-time compute. Specifically, we curate a diverse set of
long chain-of-thought reasoning traces averaging 56K tokens each, resulting in
a dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a
general-purpose reasoning model on this corpus yields ScaleRTL that is capable
of deep RTL reasoning. Subsequently, we further enhance the performance of
ScaleRTL through a novel test-time scaling strategy that extends the reasoning
process via iteratively reflecting on and self-correcting previous reasoning
steps. Experimental results show that ScaleRTL achieves state-of-the-art
performance on VerilogEval and RTLLM, outperforming 18 competitive baselines by
up to 18.4% on VerilogEval and 12.7% on RTLLM.

</details>


### [16] [Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy](https://arxiv.org/abs/2506.05682)
*Yu Feng,Weikai Lin,Yuge Cheng,Zihan Liu,Jingwen Leng,Minyi Guo,Chen Chen,Shixuan Sun,Yuhao Zhu*

Main category: cs.AR

TL;DR: Lumina, a hardware-algorithm co-designed system, improves neural rendering efficiency through novel optimizations (S^2 and radiance caching) and a specialized accelerator (LuminCore), achieving significant speedups and energy reductions.


<details>
  <summary>Details</summary>
Motivation: The computational demands of 3D Gaussian Splatting (3DGS) on mobile systems-on-chips (SoCs) hinder its practical usability for neural rendering.

Method: Lumina integrates the S^2 algorithm, which utilizes temporal coherence to reduce computational load, and a radiance caching (RC) mechanism to minimize rasterization frequency. Further efficiency comes from a hardware accelerator, LuminCore, designed for cache lookup and rasterization tasks.

Result: Lumina delivers 4.5x speedup and 5.3x energy savings compared to a mobile Volta GPU, with negligible quality degradation (< 0.2 dB PSNR reduction).

Conclusion: Lumina offers a promising solution to improve neural rendering performance on mobile platforms through hardware-algorithm co-design, balancing efficiency and quality.

Abstract: 3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural
rendering, but it remains computationally demanding on today's mobile SoCs. To
address this challenge, we propose Lumina, a hardware-algorithm co-designed
system, which integrates two principal optimizations: a novel algorithm, S^2,
and a radiance caching mechanism, RC, to improve the efficiency of neural
rendering. S2 algorithm exploits temporal coherence in rendering to reduce the
computational overhead, while RC leverages the color integration process of
3DGS to decrease the frequency of intensive rasterization computations. Coupled
with these techniques, we propose an accelerator architecture, LuminCore, to
further accelerate cache lookup and address the fundamental inefficiencies in
Rasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy
reduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB
peak signal-to-noise ratio reduction) across synthetic and real-world datasets.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [17] [EvidenceOutcomes: a Dataset of Clinical Trial Publications with Clinically Meaningful Outcomes](https://arxiv.org/abs/2506.05380)
*Yiliang Zhou,Abigail M. Newbury,Gongbo Zhang,Betina Ross Idnay,Hao Liu,Chunhua Weng,Yifan Peng*

Main category: cs.CL

TL;DR: This study presents EvidenceOutcomes, an annotated corpus aimed at improving the extraction of clinically meaningful outcomes from biomedical literature, addressing gaps in current methods.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to address the neglect and oversimplification of outcomes in benchmarks for PICO element extraction, which is crucial for evidence-based medicine.

Method: A robust annotation guideline was developed collaboratively with clinicians and NLP experts, followed by the annotation of PubMed abstracts by independent annotators, and evaluation using a fine-tuned PubMedBERT model.

Result: The annotation achieved an inter-rater agreement of 0.76 and the fine-tuned model achieved an F1-score of 0.69 at the entity level and 0.76 at the token level, demonstrating the system's effectiveness.

Conclusion: EvidenceOutcomes is a high-quality benchmark corpus that can aid future machine learning efforts in extracting relevant clinical outcomes, thereby advancing evidence-based medicine.

Abstract: The fundamental process of evidence extraction and synthesis in
evidence-based medicine involves extracting PICO (Population, Intervention,
Comparison, and Outcome) elements from biomedical literature. However,
Outcomes, being the most complex elements, are often neglected or
oversimplified in existing benchmarks. To address this issue, we present
EvidenceOutcomes, a novel, large, annotated corpus of clinically meaningful
outcomes extracted from biomedical literature. We first developed a robust
annotation guideline for extracting clinically meaningful outcomes from text
through iteration and discussion with clinicians and Natural Language
Processing experts. Then, three independent annotators annotated the Results
and Conclusions sections of a randomly selected sample of 500 PubMed abstracts
and 140 PubMed abstracts from the existing EBM-NLP corpus. This resulted in
EvidenceOutcomes with high-quality annotations of an inter-rater agreement of
0.76. Additionally, our fine-tuned PubMedBERT model, applied to these 500
PubMed abstracts, achieved an F1-score of 0.69 at the entity level and 0.76 at
the token level on the subset of 140 PubMed abstracts from the EBM-NLP corpus.
EvidenceOutcomes can serve as a shared benchmark to develop and test future
machine learning algorithms to extract clinically meaningful outcomes from
biomedical abstracts.

</details>


### [18] [LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models](https://arxiv.org/abs/2506.05385)
*Xinxin Li,Huiyao Chen,Chengjun Liu,Jing Li,Meishan Zhang,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: Semantic role labeling (SRL) generally underperforms in generative decoder-based large language models compared to encoder-decoder models. This paper introduces retrieval-augmented generation and self-correction mechanisms to close the gap.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance gap in SRL between generative decoder-based LLMs and encoder-decoder models, enabling LLMs to achieve state-of-the-art results.

Method: The approach involves (a) retrieval-augmented generation, which uses external linguistic knowledge to aid SRL processes, and (b) self-correction, which helps LLMs detect and fix inconsistent outputs.

Result: Extensive experiments on SRL benchmarks (CPB1.0, CoNLL-2009, and CoNLL-2012) show the proposed method achieving state-of-the-art performance in both Chinese and English.

Conclusion: The work demonstrates that LLMs equipped with the proposed mechanisms can surpass encoder-decoder models in SRL, indicating significant progress in the capability of LLMs for NLP tasks.

Abstract: Semantic role labeling (SRL) is a crucial task of natural language processing
(NLP). Although generative decoder-based large language models (LLMs) have
achieved remarkable success across various NLP tasks, they still lag behind
state-of-the-art encoder-decoder (BERT-like) models in SRL. In this work, we
seek to bridge this gap by equipping LLMs for SRL with two mechanisms: (a)
retrieval-augmented generation and (b) self-correction. The first mechanism
enables LLMs to leverage external linguistic knowledge such as predicate and
argument structure descriptions, while the second allows LLMs to identify and
correct inconsistent SRL outputs. We conduct extensive experiments on three
widely-used benchmarks of SRL (CPB1.0, CoNLL-2009, and CoNLL-2012). Results
demonstrate that our method achieves state-of-the-art performance in both
Chinese and English, marking the first successful application of LLMs to
surpass encoder-decoder approaches in SRL.

</details>


### [19] [Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes](https://arxiv.org/abs/2506.05386)
*Lo Pang-Yun Ting,Chengshuai Zhao,Yu-Hua Zeng,Yuan Jee Lim,Kun-Ta Chuang*

Main category: cs.CL

TL;DR: The paper presents R2AG, a method combining a retriever mechanism and reinforcement learning to improve the generation of clinical discharge instructions.


<details>
  <summary>Details</summary>
Motivation: Existing large language models struggle to generate high-quality, long-form clinical notes using limited patient information.

Method: The authors proposed R2AG, which uses reinforcement learning to retrieve reasoning paths from a medical knowledge graph. This method incorporates Group-Based Retriever Optimization (GRO) to improve retrieval and enhance LLM inference quality.

Result: R2AG outperformed baseline methods on the MIMIC-IV-Note dataset, showing better clinical efficacy and natural language generation. It successfully addressed semantic gaps and prevented clinical misinterpretation.

Conclusion: R2AG enables LLMs to deliver more accurate and semantically enriched clinical note generation, offering a valuable tool for improving automated healthcare documentation.

Abstract: Clinical note generation aims to automatically produce free-text summaries of
a patient's condition and diagnostic process, with discharge instructions being
a representative long-form example. While recent large language model
(LLM)-based methods pre-trained on general clinical corpora show promise in
clinical text generation, they fall short in producing long-form notes from
limited patient information. In this paper, we propose R2AG, the first
reinforced retriever for long-form discharge instruction generation based on
pre-admission data. R2AG is trained with reinforcement learning to retrieve
reasoning paths from a medical knowledge graph, providing explicit semantic
guidance to the LLM. To bridge the information gap, we propose Group-Based
Retriever Optimization (GRO) which improves retrieval quality with
group-relative rewards, encouraging reasoning leaps for deeper inference by the
LLM. Comprehensive experiments on the MIMIC-IV-Note dataset show that R2AG
outperforms baselines in both clinical efficacy and natural language generation
metrics. Further analysis reveals that R2AG fills semantic gaps in sparse input
scenarios, and retrieved reasoning paths help LLMs avoid clinical
misinterpretation by focusing on key evidence and following coherent reasoning.

</details>


### [20] [Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs](https://arxiv.org/abs/2506.05387)
*Jaydip Sen,Saptarshi Sengupta. Subhasis Dasgupta*

Main category: cs.CL

TL;DR: The paper proposes Adaptive Semantic-Aware Typicality Sampling (ASTS), an enhancement of Locally Typical Sampling, to improve text generation from large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Traditional decoding methods for LLMs, like top-k and nucleus sampling, often fail to strike a balance between fluency, diversity, and coherence in text generation.

Method: The ASTS algorithm is presented, featuring dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments to ensure semantic alignment and coherent diversity.

Result: ASTS outperforms existing methods by reducing repetition, improving semantic alignment, enhancing fluency, and maintaining computational efficiency across benchmarks like story generation and summarization.

Conclusion: The ASTS algorithm represents a significant improvement for decoding strategies in LLMs, effectively addressing the limitations of traditional approaches while ensuring efficient and high-quality text generation.

Abstract: This chapter explores advancements in decoding strategies for large language
models (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)
algorithm. Traditional decoding methods, such as top-k and nucleus sampling,
often struggle to balance fluency, diversity, and coherence in text generation.
To address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)
is proposed as an improved version of LTS, incorporating dynamic entropy
thresholding, multi-objective scoring, and reward-penalty adjustments. ASTS
ensures contextually coherent and diverse text generation while maintaining
computational efficiency. Its performance is evaluated across multiple
benchmarks, including story generation and abstractive summarization, using
metrics such as perplexity, MAUVE, and diversity scores. Experimental results
demonstrate that ASTS outperforms existing sampling techniques by reducing
repetition, enhancing semantic alignment, and improving fluency.

</details>


### [21] [taz2024full: Analysing German Newspapers for Gender Bias and Discrimination across Decades](https://arxiv.org/abs/2506.05388)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: The paper introduces taz2024full, the largest open-access corpus of German newspaper articles (1980–2024), and demonstrates its utility in bias and discrimination research, particularly on gender representation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale open-access corpora for German, which limits research in NLP and CSS, particularly in studying linguistic trends and societal issues like gender bias.

Method: The authors created taz2024full, a corpus of over 1.8 million German newspaper articles, and developed a structured analysis pipeline to evaluate topics like gender representation, sentiment, and linguistic framing.

Result: The analysis shows a historical overrepresentation of men in media coverage, with gradual improvements toward balanced gender representation in recent years.

Conclusion: The taz2024full corpus promotes inclusive and reproducible research in German NLP and offers valuable insights for language analysis and critical media studies.

Abstract: Open-access corpora are essential for advancing natural language processing
(NLP) and computational social science (CSS). However, large-scale resources
for German remain limited, restricting research on linguistic trends and
societal issues such as gender bias. We present taz2024full, the largest
publicly available corpus of German newspaper articles to date, comprising over
1.8 million texts from taz, spanning 1980 to 2024.
  As a demonstration of the corpus's utility for bias and discrimination
research, we analyse gender representation across four decades of reporting. We
find a consistent overrepresentation of men, but also a gradual shift toward
more balanced coverage in recent years. Using a scalable, structured analysis
pipeline, we provide a foundation for studying actor mentions, sentiment, and
linguistic framing in German journalistic texts.
  The corpus supports a wide range of applications, from diachronic language
analysis to critical media studies, and is freely available to foster inclusive
and reproducible research in German-language NLP.

</details>


### [22] [Understanding Gender Bias in AI-Generated Product Descriptions](https://arxiv.org/abs/2506.05390)
*Markelle Kelly,Mohammad Tahaei,Padhraic Smyth,Lauren Wilcox*

Main category: cs.CL

TL;DR: This paper investigates gender bias in AI-generated product descriptions in e-commerce, identifying unique bias dimensions and proposing taxonomic categories for detection and mitigation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore largely unexamined gender biases in e-commerce applications of large language models, addressing potential algorithmic harms and novel bias forms in AI-generated product descriptions.

Method: The researchers developed taxonomic categories to classify gender bias in product descriptions, conducted data-driven analyses, and applied these categories to examine bias in models such as GPT-3.5 and e-commerce-specific LLMs.

Result: The study reveals recurring gender biases in AI-produced descriptions, highlighting issues such as assumptions about clothing size, stereotypical biases, and disparities in persuasive language use.

Conclusion: The findings advance understanding of gender bias in e-commerce, linking it to exclusionary norms, stereotyping, and performance disparities, and suggesting tailored approaches for detection and mitigation.

Abstract: While gender bias in large language models (LLMs) has been extensively
studied in many domains, uses of LLMs in e-commerce remain largely unexamined
and may reveal novel forms of algorithmic bias and harm. Our work investigates
this space, developing data-driven taxonomic categories of gender bias in the
context of product description generation, which we situate with respect to
existing general purpose harms taxonomies. We illustrate how AI-generated
product descriptions can uniquely surface gender biases in ways that require
specialized detection and mitigation approaches. Further, we quantitatively
analyze issues corresponding to our taxonomic categories in two models used for
this task -- GPT-3.5 and an e-commerce-specific LLM -- demonstrating that these
forms of bias commonly occur in practice. Our results illuminate unique,
under-explored dimensions of gender bias, such as assumptions about clothing
size, stereotypical bias in which features of a product are advertised, and
differences in the use of persuasive language. These insights contribute to our
understanding of three types of AI harms identified by current frameworks:
exclusionary norms, stereotyping, and performance disparities, particularly for
the context of e-commerce.

</details>


### [23] [Are Large Language Models Good Temporal Graph Learners?](https://arxiv.org/abs/2506.05393)
*Shenyang Huang,Ali Parviz,Emma Kondrup,Zachary Yang,Zifeng Ding,Michael Bronstein,Reihaneh Rabbany,Guillaume Rabusseau*

Main category: cs.CL

TL;DR: The paper introduces Temporal Graph Talker (TGTalker), a framework applying Large Language Models (LLMs) to real-world dynamic graphs for competitive link prediction and enhanced interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how LLMs can be applied to real-world temporal graphs, a relatively unexplored area compared to their use in static and synthetic temporal graphs.

Method: TGTalker leverages recency bias in temporal graphs to transform structural information into natural language for LLM processing and incorporates temporal neighbors for improved link predictions.

Result: TGTalker achieves competitive performance compared to state-of-the-art Temporal Graph Neural Networks (TGNNs) across five real-world datasets, while providing textual explanations for predictions.

Conclusion: TGTalker expands the scope of LLM applications into dynamic graph processing, showcasing strong prediction capabilities and opening opportunities for explainable, interpretable models in temporal link prediction.

Abstract: Large Language Models (LLMs) have recently driven significant advancements in
Natural Language Processing and various other applications. While a broad range
of literature has explored the graph-reasoning capabilities of LLMs, including
their use of predictors on graphs, the application of LLMs to dynamic graphs --
real world evolving networks -- remains relatively unexplored. Recent work
studies synthetic temporal graphs generated by random graph models, but
applying LLMs to real-world temporal graphs remains an open question. To
address this gap, we introduce Temporal Graph Talker (TGTalker), a novel
temporal graph learning framework designed for LLMs. TGTalker utilizes the
recency bias in temporal graphs to extract relevant structural information,
converted to natural language for LLMs, while leveraging temporal neighbors as
additional information for prediction. TGTalker demonstrates competitive link
prediction capabilities compared to existing Temporal Graph Neural Network
(TGNN) models. Across five real-world networks, TGTalker performs competitively
with state-of-the-art temporal graph methods while consistently outperforming
popular models such as TGN and HTGN. Furthermore, TGTalker generates textual
explanations for each prediction, thus opening up exciting new directions in
explainability and interpretability for temporal link prediction. The code is
publicly available at https://github.com/shenyangHuang/TGTalker.

</details>


### [24] [Auto Review: Second Stage Error Detection for Highly Accurate Information Extraction from Phone Conversations](https://arxiv.org/abs/2506.05400)
*Ayesha Qamar,Arushi Raghuvanshi,Conal Sathi,Youngseo Son*

Main category: cs.CL

TL;DR: The paper introduces Auto Review, an automated benefit verification system for healthcare phone calls that reduces manual effort while maintaining accuracy. It addresses challenges caused by noisy call transcripts.


<details>
  <summary>Details</summary>
Motivation: To minimize manual labor and expedite benefit verification in healthcare, ensuring accurate information despite challenges from noisy call transcripts and domain-specific jargon.

Method: Developed a postprocessing pipeline that utilizes multiple ASR alternatives and a pseudo-labeling approach to improve transcript accuracy without needing manual corrections.

Result: Experimental results showed improvements in transcript correction quality and increased efficiency for the Auto Review system via general-purpose large language models and feature-based model pipelines.

Conclusion: Auto Review enhances automation in benefit verification with accurate information extraction, reducing dependence on manual reviews, even amidst noisy inputs and domain-specific challenges.

Abstract: Automating benefit verification phone calls saves time in healthcare and
helps patients receive treatment faster. It is critical to obtain highly
accurate information in these phone calls, as it can affect a patient's
healthcare journey. Given the noise in phone call transcripts, we have a
two-stage system that involves a post-call review phase for potentially noisy
fields, where human reviewers manually verify the extracted
data$\unicode{x2013}$a labor-intensive task. To automate this stage, we
introduce Auto Review, which significantly reduces manual effort while
maintaining a high bar for accuracy. This system, being highly reliant on call
transcripts, suffers a performance bottleneck due to automatic speech
recognition (ASR) issues. This problem is further exacerbated by the use of
domain-specific jargon in the calls. In this work, we propose a second-stage
postprocessing pipeline for accurate information extraction. We improve
accuracy by using multiple ASR alternatives and a pseudo-labeling approach that
does not require manually corrected transcripts. Experiments with
general-purpose large language models and feature-based model pipelines
demonstrate substantial improvements in the quality of corrected call
transcripts, thereby enhancing the efficiency of Auto Review.

</details>


### [25] [Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs](https://arxiv.org/abs/2506.05410)
*Wanyun Cui,Mingwei Xu*

Main category: cs.CL

TL;DR: This paper identifies an asymmetry in keys and values within KV caches used for long-context processing in LLMs and proposes a training-free compression framework called AsymKV, which shows superior performance over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies in long-context modeling caused by the quadratic complexity of attention mechanisms, particularly by improving KV cache compression methods.

Method: Introduces AsymKV, a training-free compression framework that uses homogeneity-based key merging along with lossless value compression to exploit the newly discovered asymmetry in KV caches.

Result: AsymKV significantly outperforms existing long-context compression methods. For example, it achieves an average score of 43.95 on LongBench with LLaMA3.1-8B, improving over SOTA methods like H$_2$O by a wide margin (38.89).

Conclusion: The proposed AsymKV framework resolves the key-value asymmetry in KV cache compression, demonstrating its effectiveness in extending context length efficiently for multiple LLM-based tasks.

Abstract: Recent advances in Large Language Models (LLMs) have highlighted the critical
importance of extending context length, yet the quadratic complexity of
attention mechanisms poses significant challenges for efficient long-context
modeling. KV cache compression has emerged as a key approach to address this
challenge. Through extensive empirical analysis, we reveal a fundamental yet
previously overlooked asymmetry in KV caches: while adjacent keys receive
similar attention weights (local homogeneity), adjacent values demonstrate
distinct heterogeneous distributions. This key-value asymmetry reveals a
critical limitation in existing compression methods that treat keys and values
uniformly. To address the limitation, we propose a training-free compression
framework (AsymKV) that combines homogeneity-based key merging with a
mathematically proven lossless value compression. Extensive experiments
demonstrate that AsymKV consistently outperforms existing long-context methods
across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV
achieves an average score of 43.95 on LongBench, surpassing SOTA methods like
H$_2$O (38.89) by a large margin.

</details>


### [26] [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.CL

TL;DR: SmoothRot introduces a novel post-training quantization technique to handle activation outliers in 4-bit quantization for LLMs, using channel-wise scaling and Hadamard transformations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the challenge of massive activation outliers that hinder efficient quantization in large language models (LLMs).

Method: SmoothRot combines channel-wise scaling with Hadamard transformations to transform extreme activation outliers into more quantization-friendly forms.

Result: The method reduces the performance gap between 4-bit quantized models and FP16 models by 10-30% across common tasks without introducing additional inference latency.

Conclusion: SmoothRot significantly enhances the efficiency and accuracy of post-training quantization for LLMs, offering practical benefits for deployment without latency costs.

Abstract: We present SmoothRot, a novel post-training quantization technique to enhance
the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot
addresses the critical challenge of massive activation outliers, by integrating
channel-wise scaling with Hadamard transformations. Our technique effectively
transforms extreme outliers into quantization-friendly activations,
significantly improving quantization accuracy. Experiments conducted on popular
LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot
consistently reduces the performance gap between quantized and FP16 models by
approximately 10-30\% across language generation and zero-shot reasoning tasks,
without introducing additional inference latency. Code is available at
https://github.com/czakop/smoothrot.

</details>


### [27] [Automatically Detecting Amusing Games in Wordle](https://arxiv.org/abs/2506.05415)
*Ronaldo Luo,Gary Liang,Cindy Liu,Adam Kabbara,Minahil Bakhtawar,Kina Kim,Michael Guerzhoy*

Main category: cs.CL

TL;DR: The paper predicts user amusement in Wordle games using Reddit reactions and GPT-3.5, extracting features to computationally analyze humor.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand and predict user amusement in Wordle games through computational means, identifying key humor-inducing features.

Method: Reddit reactions were scraped, classified by GPT-3.5 for amusement detection, and validated against human labels. Features of Wordle games were analyzed to correlate with amusement.

Result: The study highlights that Wordle game features provide a weak but detectable signal for predicting amusement via GPT-3.5 labels.

Conclusion: User amusement related to Wordle games can be computationally predicted, linking humor to measurable and creative aspects of gameplay.

Abstract: We explore automatically predicting which Wordle games Reddit users find
amusing.
  We scrape approximately 80k reactions by Reddit users to Wordle games from
Reddit, classify the reactions as expressing amusement or not using OpenAI's
GPT-3.5 using few-shot prompting, and verify that GPT-3.5's labels roughly
correspond to human labels.
  We then extract features from Wordle games that can predict user amusement.
We demonstrate that the features indeed provide a (weak) signal that predicts
user amusement as predicted by GPT-3.5.
  Our results indicate that user amusement at Wordle games can be predicted
computationally to some extent. We explore which features of the game
contribute to user amusement.
  We find that user amusement is predictable, indicating a measurable aspect of
creativity infused into Wordle games through humor.

</details>


### [28] [MLLM-CL: Continual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2506.05453)
*Hongbo Zhao,Fei Zhu,Rundong Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.CL

TL;DR: The paper introduces MLLM-CL, a benchmark for continual learning in multimodal large language models (MLLMs), and proposes a novel approach to mitigate catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current benchmarks and methods in adapting multimodal large language models to real-world dynamic scenarios requiring continuous learning.

Method: The paper introduces MLLM-CL, a benchmark for domain and ability continual learning, and suggests parameter isolation combined with an MLLM-based routing mechanism to prevent catastrophic interference.

Result: The proposed method demonstrated the ability to integrate new domain-specific knowledge and skills with minimal forgetting, significantly outperforming existing methods in experiments.

Conclusion: The novel benchmark and method provide an effective framework for continual learning in MLLMs, solving challenges like catastrophic forgetting and improving performance.

Abstract: Recent Multimodal Large Language Models (MLLMs) excel in vision-language
understanding but face challenges in adapting to dynamic real-world scenarios
that require continuous integration of new knowledge and skills. While
continual learning (CL) offers a potential solution, existing benchmarks and
methods suffer from critical limitations. In this paper, we introduce MLLM-CL,
a novel benchmark encompassing domain and ability continual learning, where the
former focuses on independently and identically distributed (IID) evaluation
across evolving mainstream domains, whereas the latter evaluates on non-IID
scenarios with emerging model ability. Methodologically, we propose preventing
catastrophic interference through parameter isolation, along with an MLLM-based
routing mechanism. Extensive experiments demonstrate that our approach can
integrate domain-specific knowledge and functional abilities with minimal
forgetting, significantly outperforming existing methods.

</details>


### [29] [Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering](https://arxiv.org/abs/2506.05498)
*Niruthiha Selvanayagam*

Main category: cs.CL

TL;DR: The study analyzes language developmental trajectories in children with Specific Language Impairment (SLI) using unsupervised machine learning methods to refine diagnostics and interventions.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnostic methods for SLI often overlook developmental nuances. The study aims to uncover natural language trajectories to improve early diagnosis and interventions for SLI.

Method: The researchers applied Principal Component Analysis (PCA) and clustering on narrative data from 1,163 children using 64 linguistic features to identify developmental patterns linked to SLI.

Result: Two main linguistic clusters were found: those with high language production and low SLI prevalence, and those with limited production but higher syntactic complexity. Intermediate traits supported a continuum model of language abilities.

Conclusion: SLI is characterized mainly by reduced production capacity, not syntactic complexity deficits. The findings challenge categorical diagnostic methods and underscore the potential of machine learning for improved SLI diagnosis and treatment.

Abstract: Specific Language Impairment (SLI) affects approximately 7 percent of
children, presenting as isolated language deficits despite normal cognitive
abilities, sensory systems, and supportive environments. Traditional diagnostic
approaches often rely on standardized assessments, which may overlook subtle
developmental patterns. This study aims to identify natural language
development trajectories in children with and without SLI using unsupervised
machine learning techniques, providing insights for early identification and
targeted interventions. Narrative samples from 1,163 children aged 4-16 years
across three corpora (Conti-Ramsden 4, ENNI, and Gillam) were analyzed using
Principal Component Analysis (PCA) and clustering. A total of 64 linguistic
features were evaluated to uncover developmental trajectories and distinguish
linguistic profiles. Two primary clusters emerged: (1) high language production
with low SLI prevalence, and (2) limited production but higher syntactic
complexity with higher SLI prevalence. Additionally, boundary cases exhibited
intermediate traits, supporting a continuum model of language abilities.
Findings suggest SLI manifests primarily through reduced production capacity
rather than syntactic complexity deficits. The results challenge categorical
diagnostic frameworks and highlight the potential of unsupervised learning
techniques for refining diagnostic criteria and intervention strategies.

</details>


### [30] [Improving LLMs with a knowledge from databases](https://arxiv.org/abs/2506.05560)
*Petr Máša*

Main category: cs.CL

TL;DR: The paper proposes a method that integrates enhanced association rules into large language models (LLMs) via retrieval-augmented generation (RAG), significantly improving their dataset-based question-answering capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the dataset/database-based answering ability of LLMs using interpretable machine learning methods like enhanced association rules, while addressing issues such as safety and the large volume of rules.

Method: The proposed method generates knowledge-based rulesets, converts these rules into text using a rule-to-text converter, and integrates them into LLMs through the RAG technique. Various strategies for rule generation were explored.

Result: The proposed method demonstrated significant improvements in answering dataset-based questions compared to existing techniques, including ChatGPT with agents.

Conclusion: The method shows substantial promise for enhancing dataset-driven QA in LLMs, with opportunities for further improvement by adopting more patterns and novel strategies for rule mining.

Abstract: Large language models (LLMs) are achieving significant progress almost every
moment now. Many advanced techniques have been introduced and widely accepted,
like retrieval-augmentation generation (RAG), agents, and tools. Tools can
query the database to answer questions from structured data files or perform
groupings or other statistics. This unlocks huge opportunities, such as it can
answer any question, but also poses threats, such as safety, because there is
no control over the commands that are created. We would like to discuss whether
we can create a new method that improves answers based on dataset/database via
some interpretable ML methods, namely enhanced association rules. The advantage
would be if the method can be also used in some safe technique like RAG.
Association rules have a sound history. Since the introduction of CN2 and
aproiri, many enhancements have been made. In parallel, enhanced association
rules have been introduced and evolved over the last 40 years. The general
problem is typically that there are too many rules. There are some techniques
for handling it, but when LLM emerged, it turned out to be the best use case
for the RAG technique for LLMs. We proposed a method that generates a ruleset
based on defined knowledge patterns, then converts rules into text form via a
rule-to-text converter, and includes the result as an RAG into LLM. We compared
this method with ChatGPT (even with using agents) and we have discovered a
significant improvement in answering questions based on the dataset. We have
also tried several strategies how much rules to generate. We found this
improvement interesting. Moreover, it can also be improved in many ways as
future work, like incorporating other patterns, the use of rule mining as an
agent, and many others.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [31] [Can ChatGPT Perform Image Splicing Detection? A Preliminary Study](https://arxiv.org/abs/2506.05358)
*Souradip Nath*

Main category: cs.CV

TL;DR: Investigates out-of-the-box capabilities of GPT-4V for detecting image splicing manipulations without fine-tuning, using three prompting strategies.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore if GPT-4V can effectively perform image splicing detection tasks without task-specific modifications, leveraging its reasoning across text and image.

Method: Three prompting strategies—Zero-Shot, Few-Shot, and Chain-of-Thought—are applied to evaluate GPT-4V on a curated subset of the CASIA v2.0 splicing dataset.

Result: Achieves competitive performance (over 85% accuracy) in zero-shot settings, with Chain-of-Thought prompting providing balanced detection across images.

Conclusion: While GPT-4V falls short of specialized models, its versatility and reasoning abilities show promise as a general tool for image forensics.

Abstract: Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning
across text and image modalities, showing promise in a variety of complex
vision-language tasks. In this preliminary study, we investigate the
out-of-the-box capabilities of GPT-4V in the domain of image forensics,
specifically, in detecting image splicing manipulations. Without any
task-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:
Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a
curated subset of the CASIA v2.0 splicing dataset.
  Our results show that GPT-4V achieves competitive detection performance in
zero-shot settings (more than 85% accuracy), with CoT prompting yielding the
most balanced trade-off across authentic and spliced images. Qualitative
analysis further reveals that the model not only detects low-level visual
artifacts but also draws upon real-world contextual knowledge such as object
scale, semantic consistency, and architectural facts, to identify implausible
composites. While GPT-4V lags behind specialized state-of-the-art splicing
detection models, its generalizability, interpretability, and encyclopedic
reasoning highlight its potential as a flexible tool in image forensics.

</details>


### [32] [CarboNeXT and CarboFormer: Dual Semantic Segmentation Architectures for Detecting and Quantifying Carbon Dioxide Emissions Using Optical Gas Imaging](https://arxiv.org/abs/2506.05360)
*Taminul Islam,Toqi Tahamid Sarker,Mohamed G Embaby,Khaled R Ahmed,Amer AbuGhazaleh*

Main category: cs.CV

TL;DR: The paper introduces CarboNeXT, a semantic segmentation framework for optical gas imaging (OGI) to detect and quantify CO$_2$ emissions, along with two new datasets and a lightweight model variant.


<details>
  <summary>Details</summary>
Motivation: To address the issues of accurately detecting and quantifying CO$_2$ emissions across diverse applications, particularly in environmental and livestock management.

Method: A multi-scale context aggregation network combined with UPerHead and auxiliary FCN components is used to model both local and global features of gas plume imagery. Two datasets, CCR and RTA, are introduced for training and evaluation.

Result: CarboNeXT achieves 88.46% mIoU on CCR and 92.95% mIoU on RTA at 60.95 FPS. Its lightweight variant, CarboFormer, achieves 84.88% mIoU on CCR and 92.98% on RTA, operating at 84.68 FPS.

Conclusion: CarboNeXT and CarboFormer provide robust and efficient tools for CO$_2$ emission detection, advancing both environmental monitoring and livestock management, with CarboFormer specially designed for resource-constrained platforms.

Abstract: Carbon dioxide (CO$_2$) emissions are critical indicators of both
environmental impact and various industrial processes, including livestock
management. We introduce CarboNeXT, a semantic segmentation framework for
Optical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions
across diverse applications. Our approach integrates a multi-scale context
aggregation network with UPerHead and auxiliary FCN components to effectively
model both local details and global relationships in gas plume imagery. We
contribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)
dataset, which simulates gas leaks with systematically varied flow rates
(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions
from dairy cow rumen fluid in vitro experiments. Extensive evaluations
demonstrate that CarboNeXT outperforms state-of-the-art methods, achieving
88.46% mIoU on CCR and 92.95% mIoU on RTA, with particular effectiveness in
challenging low-flow scenarios. The model operates at 60.95 FPS, enabling
real-time monitoring applications. Additionally, we propose CarboFormer, a
lightweight variant with only 5.07M parameters that achieves 84.68 FPS, with
competitive performance of 84.88% mIoU on CCR and 92.98% on RTA, making it
suitable for resource-constrained platforms such as programmable drones. Our
work advances both environmental sensing and precision livestock management by
providing robust tools for CO$_2$ emission analysis, with a specific focus on
livestock applications.

</details>


### [33] [Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching](https://arxiv.org/abs/2506.05361)
*Tinglin Huang,Tianyu Liu,Mehrtash Babadi,Wengong Jin,Rex Ying*

Main category: cs.CV

TL;DR: This paper proposes STFlow, a model to predict spatial transcriptomics data considering cell-cell interactions, overcoming limitations of existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current spatial transcriptomics (ST) techniques are hindered by low throughput and dependence on specialized facilities. Efforts to predict ST from histology images have limitations: lack of cell-cell interaction modeling and memory inefficiency in handling large datasets.

Method: The authors introduce STFlow, a flow matching generative model that captures cell-cell interactions by modeling the joint distribution of gene expression for entire slides. STFlow includes a slide-level encoder with local spatial attention to efficiently process whole slides without excessive memory usage.

Result: STFlow outperforms state-of-the-art methods on the HEST-1k and STImage-1K4M benchmarks by over 18%, showing considerable improvement over existing pathology models.

Conclusion: STFlow addresses limitations in current ST prediction methods, offering more accurate results while managing memory constraints effectively for large-scale datasets.

Abstract: Spatial transcriptomics (ST) has emerged as a powerful technology for
bridging histology imaging with gene expression profiling. However, its
application has been limited by low throughput and the need for specialized
experimental facilities. Prior works sought to predict ST from whole-slide
histology images to accelerate this process, but they suffer from two major
limitations. First, they do not explicitly model cell-cell interaction as they
factorize the joint distribution of whole-slide ST data and predict the gene
expression of each spot independently. Second, their encoders struggle with
memory constraints due to the large number of spots (often exceeding 10,000) in
typical ST datasets. Herein, we propose STFlow, a flow matching generative
model that considers cell-cell interaction by modeling the joint distribution
of gene expression of an entire slide. It also employs an efficient slide-level
encoder with local spatial attention, enabling whole-slide processing without
excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M
benchmarks, STFlow substantially outperforms state-of-the-art baselines and
achieves over 18% relative improvements over the pathology foundation models.

</details>


### [34] [Seed Selection for Human-Oriented Image Reconstruction via Guided Diffusion](https://arxiv.org/abs/2506.05363)
*Yui Tatsumi,Ziyue Zeng,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: The paper proposes using optimal seed selection during the diffusion process to enhance image quality in scalable image coding for humans and machines, all without adding extra bitrate.


<details>
  <summary>Details</summary>
Motivation: Conventional approaches to scalable image coding require additional data for scalability, while newer methods with diffusion avoid extra bitrate but suffer from randomness affecting image quality.

Method: The authors develop a seed selection mechanism that evaluates multiple random seed candidates during early steps of the reverse diffusion process to optimize output quality without escalating computational demands.

Result: Experimental evaluations show that the proposed method surpasses the baseline on several performance metrics, delivering better image quality.

Conclusion: Optimal seed selection during the reverse diffusion phase successfully achieves higher-quality scalable image coding for humans and machines, maintaining the same bitrate and improving efficiency.

Abstract: Conventional methods for scalable image coding for humans and machines
require the transmission of additional information to achieve scalability. A
recent diffusion-based method avoids this by generating human-oriented images
from machine-oriented images without extra bitrate. This method, however, uses
a single random seed, which may lead to suboptimal image quality. In this
paper, we propose a seed selection method that identifies the optimal seed from
multiple candidates to improve image quality without increasing the bitrate. To
reduce computational cost, the selection is performed based on intermediate
outputs obtained from early steps of the reverse diffusion process.
Experimental results demonstrate that our method outperforms the baseline
across multiple metrics.

</details>


### [35] [Text2Stereo: Repurposing Stable Diffusion for Stereo Generation with Consistency Rewards](https://arxiv.org/abs/2506.05367)
*Aakash Garg,Libing Zeng,Andrii Tsarov,Nima Khademi Kalantari*

Main category: cs.CV

TL;DR: This paper introduces a diffusion-based approach to generate stereo images from text prompts using fine-tuned Stable Diffusion.


<details>
  <summary>Details</summary>
Motivation: Stereo image generation from text prompts is challenging due to limited datasets and the need for maintaining stereo consistency.

Method: The authors fine-tuned Stable Diffusion on stereo image datasets and incorporated stereo consistency reward functions and prompt alignment techniques.

Result: Experiments showed that the proposed method generates high-quality stereo images with improved accuracy compared to existing approaches.

Conclusion: The approach is effective for generating stereo images from text and offers advancements in stereo consistency and text-to-image alignment.

Abstract: In this paper, we propose a novel diffusion-based approach to generate stereo
images given a text prompt. Since stereo image datasets with large baselines
are scarce, training a diffusion model from scratch is not feasible. Therefore,
we propose leveraging the strong priors learned by Stable Diffusion and
fine-tuning it on stereo image datasets to adapt it to the task of stereo
generation. To improve stereo consistency and text-to-image alignment, we
further tune the model using prompt alignment and our proposed stereo
consistency reward functions. Comprehensive experiments demonstrate the
superiority of our approach in generating high-quality stereo images across
diverse scenarios, outperforming existing methods.

</details>


### [36] [Speaking images. A novel framework for the automated self-description of artworks](https://arxiv.org/abs/2506.05368)
*Valentine Bernasconi,Gustavo Marfia*

Main category: cs.CV

TL;DR: This paper proposes a framework to create self-explaining cultural artifacts using generative AI technologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance access and interpretation of digital cultural heritage artifacts in innovative ways, particularly through creative manipulation of digital images.

Method: The authors propose combining open-source tools like large-language models, face detection, text-to-speech, and audio-to-animation systems to automatically generate videos where characters from digitized artworks explain their own content.

Result: The framework creates educational videos by animating artwork characters, providing an interactive way to understand digital cultural artifacts.

Conclusion: This work explores both the potential and the ethical implications of using generative AI in the cultural heritage domain, aiming to enrich educational and interpretive strategies.

Abstract: Recent breakthroughs in generative AI have opened the door to new research
perspectives in the domain of art and cultural heritage, where a large number
of artifacts have been digitized. There is a need for innovation to ease the
access and highlight the content of digital collections. Such innovations
develop into creative explorations of the digital image in relation to its
malleability and contemporary interpretation, in confrontation to the original
historical object. Based on the concept of the autonomous image, we propose a
new framework towards the production of self-explaining cultural artifacts
using open-source large-language, face detection, text-to-speech and
audio-to-animation models. The goal is to start from a digitized artwork and to
automatically assemble a short video of the latter where the main character
animates to explain its content. The whole process questions cultural biases
encapsulated in large-language models, the potential of digital images and
deepfakes of artworks for educational purposes, along with concerns of the
field of art history regarding such creative diversions.

</details>


### [37] [MR.NAVI: Mixed-Reality Navigation Assistant for the Visually Impaired](https://arxiv.org/abs/2506.05369)
*Nicolas Pfitzer,Yifan Zhou,Marco Poggensee,Defne Kurtulus,Bessie Dominguez-Dager,Mihai Dusmanu,Marc Pollefeys,Zuria Bauer*

Main category: cs.CV

TL;DR: MR.NAVI is a mixed reality system designed for visually impaired individuals that utilizes audio feedback, computer vision, and natural language processing for enhanced spatial awareness and navigation.


<details>
  <summary>Details</summary>
Motivation: Around 43 million people globally have severe visual impairments, creating a need for effective solutions to aid navigation in unfamiliar environments.

Method: MR.NAVI employs computer vision for object detection and depth estimation, NLP for scene descriptions, sensor data processing, floor detection via RANSAC with DBSCAN clustering, and public transit API integration.

Result: Experiments, including user studies, demonstrated the system's usability and effectiveness in providing real-time scene understanding and navigation in unfamiliar settings.

Conclusion: MR.NAVI showcases potential in improving spatial awareness and navigation for visually impaired users through innovative technological integration.

Abstract: Over 43 million people worldwide live with severe visual impairment, facing
significant challenges in navigating unfamiliar environments. We present
MR.NAVI, a mixed reality system that enhances spatial awareness for visually
impaired users through real-time scene understanding and intuitive audio
feedback. Our system combines computer vision algorithms for object detection
and depth estimation with natural language processing to provide contextual
scene descriptions, proactive collision avoidance, and navigation instructions.
The distributed architecture processes sensor data through MobileNet for object
detection and employs RANSAC-based floor detection with DBSCAN clustering for
obstacle avoidance. Integration with public transit APIs enables navigation
with public transportation directions. Through our experiments with user
studies, we evaluated both scene description and navigation functionalities in
unfamiliar environments, showing promising usability and effectiveness.

</details>


### [38] [DVD: A Comprehensive Dataset for Advancing Violence Detection in Real-World Scenarios](https://arxiv.org/abs/2506.05372)
*Dimitrios Kollias,Damith C. Senadeera,Jianian Zheng,Kaushal K. K. Yadav,Greg Slabaugh,Muhammad Awais,Xiaoyun Yang*

Main category: cs.CV

TL;DR: The paper introduces DVD, a large-scale, frame-level annotated database for violence detection to address issues in existing VD datasets, like limited diversity, scale, and metadata.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing violence detection databases, such as coarse annotations, restricted diversity, and lack of metadata.

Method: Developed DVD, a database with 500 videos, frame-level annotations, diverse environments, multiple camera sources, and detailed metadata.

Result: DVD enhances the capability to model real-world violent events by providing comprehensive and complex data.

Conclusion: The introduction of DVD significantly contributes towards advancing violence detection research by providing a more robust and diverse foundation.

Abstract: Violence Detection (VD) has become an increasingly vital area of research.
Existing automated VD efforts are hindered by the limited availability of
diverse, well-annotated databases. Existing databases suffer from coarse
video-level annotations, limited scale and diversity, and lack of metadata,
restricting the generalization of models. To address these challenges, we
introduce DVD, a large-scale (500 videos, 2.7M frames), frame-level annotated
VD database with diverse environments, varying lighting conditions, multiple
camera sources, complex social interactions, and rich metadata. DVD is designed
to capture the complexities of real-world violent events.

</details>


### [39] [State Estimation and Control of Dynamic Systems from High-Dimensional Image Data](https://arxiv.org/abs/2506.05375)
*Ashik E Rasul,Hyung-Jin Yoon*

Main category: cs.CV

TL;DR: The paper introduces a method combining CNNs and GRUs for state representation learning to aid reinforcement learning in dynamic systems, removing the need for ground-truth state access.


<details>
  <summary>Details</summary>
Motivation: True system states are often unavailable or impractical to obtain, making policy learning in dynamic systems challenging.

Method: The method integrates CNNs for spatial feature extraction and GRUs for temporal modeling to learn state representations from image sequences and actions. These are then used to train a reinforcement learning agent using a Deep Q-Network (DQN).

Result: The proposed method achieves real-time, accurate estimation and control without requiring ground-truth states. It also quantitatively evaluates the learned state's accuracy and its influence on policy performance.

Conclusion: The approach improves control stability and policy performance by replacing the need for ground-truth state access with learned state representations that are computationally efficient and robust.

Abstract: Accurate state estimation is critical for optimal policy design in dynamic
systems. However, obtaining true system states is often impractical or
infeasible, complicating the policy learning process. This paper introduces a
novel neural architecture that integrates spatial feature extraction using
convolutional neural networks (CNNs) and temporal modeling through gated
recurrent units (GRUs), enabling effective state representation from sequences
of images and corresponding actions. These learned state representations are
used to train a reinforcement learning agent with a Deep Q-Network (DQN).
Experimental results demonstrate that our proposed approach enables real-time,
accurate estimation and control without direct access to ground-truth states.
Additionally, we provide a quantitative evaluation methodology for assessing
the accuracy of the learned states, highlighting their impact on policy
performance and control stability.

</details>


### [40] [An Independent Discriminant Network Towards Identification of Counterfeit Images and Videos](https://arxiv.org/abs/2506.05377)
*Shayantani Kar,B. Shresth Bhimrajka,Aditya Kumar,Sahil Gupta,Sourav Ghosh,Subhamita Mukherjee,Shauvik Paul*

Main category: cs.CV

TL;DR: This paper addresses the problem of detecting false images and videos on online platforms using a discriminant network based on InceptionResNetV2.


<details>
  <summary>Details</summary>
Motivation: The motivation is to combat the rapid spread of counterfeit images and videos on the internet, which mislead and hide criminal activities.

Method: The paper employs a convolutional neural network (CNN) based on InceptionResNetV2 to develop an independent discriminant network that identifies GAN-generated content. A specific platform is also proposed for detecting forged content.

Result: The proposed approach demonstrates its potential to detect forged images and videos created using GAN and aids in uncovering criminal evidence.

Conclusion: The work is a step forward in helping the forensics domain detect and counter the problem of counterfeit visual content online, supporting criminal investigations.

Abstract: Rapid spread of false images and videos on online platforms is an emerging
problem. Anyone may add, delete, clone or modify people and entities from an
image using various editing software which are readily available. This
generates false and misleading proof to hide the crime. Now-a-days, these false
and counterfeit images and videos are flooding on the internet. These spread
false information. Many methods are available in literature for detecting those
counterfeit contents but new methods of counterfeiting are also evolving.
Generative Adversarial Networks (GAN) are observed to be one effective method
as it modifies the context and definition of images producing plausible results
via image-to-image translation. This work uses an independent discriminant
network that can identify GAN generated image or video. A discriminant network
has been created using a convolutional neural network based on
InceptionResNetV2. The article also proposes a platform where users can detect
forged images and videos. This proposed work has the potential to help the
forensics domain to detect counterfeit videos and hidden criminal evidence
towards the identification of criminal activities.

</details>


### [41] [A Compendium of Autonomous Navigation using Object Detection and Tracking in Unmanned Aerial Vehicles](https://arxiv.org/abs/2506.05378)
*Mohit Arora,Pratyush Shukla,Shivali Chopra*

Main category: cs.CV

TL;DR: The paper reviews techniques for enabling UAVs to autonomously navigate using computer vision-based object detection and tracking algorithms.


<details>
  <summary>Details</summary>
Motivation: Enhancing the autonomy of UAVs to overcome challenges like signal disruption, real-time processing, and data security, enabling them to perform critical tasks more effectively.

Method: Examines various algorithms in computer vision for object detection and real-time tracking tailored to UAV hardware, drawing insights from prior research.

Result: The review highlights the applicability of computer vision algorithms in enabling autonomous UAV navigation, particularly in contexts such as disaster management and traffic monitoring.

Conclusion: UAVs can achieve autonomous capabilities by leveraging computer vision technologies, which address existing challenges and enhance their utility across diverse applications.

Abstract: Unmanned Aerial Vehicles (UAVs) are one of the most revolutionary inventions
of 21st century. At the core of a UAV lies the central processing system that
uses wireless signals to control their movement. The most popular UAVs are
quadcopters that use a set of four motors, arranged as two on either side with
opposite spin. An autonomous UAV is called a drone. Drones have been in service
in the US army since the 90's for covert missions critical to national
security. It would not be wrong to claim that drones make up an integral part
of the national security and provide the most valuable service during
surveillance operations. While UAVs are controlled using wireless signals,
there reside some challenges that disrupt the operation of such vehicles such
as signal quality and range, real time processing, human expertise, robust
hardware and data security. These challenges can be solved by programming UAVs
to be autonomous, using object detection and tracking, through Computer Vision
algorithms. Computer Vision is an interdisciplinary field that seeks the use of
deep learning to gain a high-level understanding of digital images and videos
for the purpose of automating the task of human visual system. Using computer
vision, algorithms for detecting and tracking various objects can be developed
suitable to the hardware so as to allow real time processing for immediate
judgement. This paper attempts to review the various approaches several authors
have proposed for the purpose of autonomous navigation of UAVs by through
various algorithms of object detection and tracking in real time, for the
purpose of applications in various fields such as disaster management, dense
area exploration, traffic vehicle surveillance etc.

</details>


### [42] [Can Vision Transformers with ResNet's Global Features Fairly Authenticate Demographic Faces?](https://arxiv.org/abs/2506.05383)
*Abu Sufian,Marco Leo,Cosimo Distante,Anirudha Ghosh,Debaditya Barman*

Main category: cs.CV

TL;DR: This study investigates biometric face authentication fairness across demographics using Vision Transformer (ViT) and ResNet models. It introduces a novel few-shot prototype network and evaluates its performance under different support set scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of ensuring fairness and generalization in biometric face authentication across diverse demographic groups while minimizing reliance on local facial features.

Method: Global features from three pre-trained ViT models (Facebook, Google, Microsoft) and ResNet-18 are concatenated and fine-tuned using customized datasets. A novel few-shot prototype network is proposed and tested with one-shot, three-shot, and five-shot strategies on new demographic datasets.

Result: The Microsoft Swin Transformer backbone achieved superior performance among the evaluated models. Performance improved as the shot size of the support set increased.

Conclusion: Integrating both global and local features enhances demographic fairness in face authentication. The proposed methods show promise, particularly with the Microsoft Swin Transformer backbone.

Abstract: Biometric face authentication is crucial in computer vision, but ensuring
fairness and generalization across demographic groups remains a big challenge.
Therefore, we investigated whether Vision Transformer (ViT) and ResNet,
leveraging pre-trained global features, can fairly authenticate different
demographic faces while relying minimally on local features. In this
investigation, we used three pre-trained state-of-the-art (SOTA) ViT foundation
models from Facebook, Google, and Microsoft for global features as well as
ResNet-18. We concatenated the features from ViT and ResNet, passed them
through two fully connected layers, and trained on customized face image
datasets to capture the local features. Then, we designed a novel few-shot
prototype network with backbone features embedding. We also developed new
demographic face image support and query datasets for this empirical study. The
network's testing was conducted on this dataset in one-shot, three-shot, and
five-shot scenarios to assess how performance improves as the size of the
support set increases. We observed results across datasets with varying
races/ethnicities, genders, and age groups. The Microsoft Swin Transformer
backbone performed better among the three SOTA ViT for this task. The code and
data are available at: https://github.com/Sufianlab/FairVitBio.

</details>


### [43] [Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment](https://arxiv.org/abs/2506.05384)
*Zhuoxuan Cai,Jian Zhang,Xinbin Yuan,Pengtao Jiang,Wenxiang Chen,Bowen Tang,Lujian Yao,Qiyuan Wang,Jinwen Chen,Bo Li*

Main category: cs.CV

TL;DR: The paper proposes a two-stage training framework for multimodal large language models to enhance both accuracy and interpretability in visual quality assessment.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the trade-off in current multimodal large language models where optimizing for accuracy in quality scoring often compromises interpretability and vice versa.

Method: The two-stage framework consists of a cold-start stage for initializing reasoning capabilities through supervised learning and a reinforcement learning-based fine-tuning stage employing Group Relative Policy Optimization (GRPO) to optimize both scoring accuracy and reasoning consistency.

Result: The proposed model, Q-Ponder, achieves state-of-the-art performance in quality score regression benchmarks, surpassing existing models by up to 6.5% in SRCC. It also demonstrates superior description accuracy and generalization across tasks compared to the teacher model.

Conclusion: The unified training framework effectively combines interpretability and accuracy, overcoming the limitations of existing approaches in visual quality assessment.

Abstract: Recent studies demonstrate that multimodal large language models (MLLMs) can
proficiently evaluate visual quality through interpretable assessments.
However, existing approaches typically treat quality scoring and reasoning
descriptions as separate tasks with disjoint optimization objectives, leading
to a trade-off: models adept at quality reasoning descriptions struggle with
precise score regression, while score-focused models lack interpretability.
This limitation hinders the full potential of MLLMs in visual quality
assessment, where accuracy and interpretability should be mutually reinforcing.
To address this, we propose a unified two-stage training framework comprising a
cold-start stage and a reinforcement learning-based fine-tuning stage.
Specifically, in the first stage, we distill high-quality data from a teacher
model through expert-designed prompts, initializing reasoning capabilities via
cross-entropy loss supervision. In the second stage, we introduce a novel
reward with Group Relative Policy Optimization (GRPO) to jointly optimize
scoring accuracy and reasoning consistency. We designate the models derived
from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show
that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score
regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain
datasets. Furthermore, Q-Ponder significantly outperforms description-based
SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in
description accuracy and reasonableness, demonstrating the generalization
potential over diverse tasks.

</details>


### [44] [TriPSS: A Tri-Modal Keyframe Extraction Framework Using Perceptual, Structural, and Semantic Representations](https://arxiv.org/abs/2506.05395)
*Mert Can Cakmak,Nitin Agarwal,Diwash Poudel*

Main category: cs.CV

TL;DR: TriPSS is a tri-modal framework enhancing video summarization by integrating visual, structural, and semantic cues, achieving state-of-the-art performance in benchmark tests.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of capturing the complete richness of video content for effective summarization and retrieval.

Method: TriPSS integrates perceptual, structural, and semantic features via principal component analysis and adaptive video segmentation using HDBSCAN clustering, complemented by quality assessment and duplicate filtering.

Result: TriPSS outperformed unimodal and previous multi-modal methods, establishing new performance benchmarks on TVSum20 and SumMe datasets.

Conclusion: The approach demonstrates superior capability in understanding nuanced video content, enhancing its application in large-scale video retrieval scenarios.

Abstract: Efficient keyframe extraction is critical for effective video summarization
and retrieval, yet capturing the complete richness of video content remains
challenging. In this work, we present TriPSS, a novel tri-modal framework that
effectively integrates perceptual cues from color features in the CIELAB space,
deep structural embeddings derived from ResNet-50, and semantic context from
frame-level captions generated by Llama-3.2-11B-Vision-Instruct. By fusing
these diverse modalities using principal component analysis, TriPSS constructs
robust multi-modal embeddings that enable adaptive segmentation of video
content via HDBSCAN clustering. A subsequent refinement stage incorporating
quality assessment and duplicate filtering ensures that the final keyframe set
is both concise and semantically rich. Comprehensive evaluations on benchmark
datasets TVSum20 and SumMe demonstrate that TriPSS achieves state-of-the-art
performance, substantially outperforming traditional unimodal and previous
multi-modal methods. These results underscore TriPSS's ability to capture
nuanced visual and semantic information, thereby setting a new benchmark for
video content understanding in large-scale retrieval scenarios.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [45] [Beyond the Buzz: A Pragmatic Take on Inference Disaggregation](https://arxiv.org/abs/2506.05508)
*Tiyasa Mitra,Ritika Borkar,Nidhi Bhatia,Ramon Matas,Shivam Raj,Dheevatsa Mudigere,Ritchie Zhao,Maximilian Golub,Arpan Dutta,Sailaja Madduri,Dharmesh Jani,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: Disaggregated inference scales multi-node deployments by splitting inference, optimizing throughput and interactivity. Researchers systematically evaluate its deployment potential, focusing on traffic patterns and model size.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and limited deployment of disaggregated inference despite its promise in enhancing throughput and interactivity trade-offs.

Method: A systematic study and evaluation of disaggregated inference across numerous design points, workloads, and hardware setups is conducted.

Result: Disaggregation shows effectiveness for prefill-heavy traffic patterns and larger models, with dynamic rate matching and elastic scaling playing key roles.

Conclusion: Disaggregated inference can optimize throughput-interactivity performance, offering practical insights for scalable deployments.

Abstract: As inference scales to multi-node deployments, disaggregation - splitting
inference into distinct phases - offers a promising path to improving the
throughput-interactivity Pareto frontier. Despite growing enthusiasm and a
surge of open-source efforts, practical deployment of disaggregated serving
remains limited due to the complexity of the optimization search space and
system-level coordination. In this paper, we present the first systematic study
of disaggregated inference at scale, evaluating hundreds of thousands of design
points across diverse workloads and hardware configurations. We find that
disaggregation is most effective for prefill-heavy traffic patterns and larger
models. Our results highlight the critical role of dynamic rate matching and
elastic scaling in achieving Pareto-optimal performance. Our findings offer
actionable insights for efficient disaggregated deployments to navigate the
trade-off between system throughput and interactivity.

</details>


### [46] [Resilient Auto-Scaling of Microservice Architectures with Efficient Resource Management](https://arxiv.org/abs/2506.05693)
*Hussain Ahmad,Christoph Treude,Markus Wagner,Claudia Szabo*

Main category: cs.DC

TL;DR: SecureSmart HPA tackles disruptions in microservice architectures by enhancing resource efficiency and resilience, outperforming traditional and prior solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the shortcomings of traditional Horizontal Pod Auto-scalers in managing resource disruptions caused by various operational challenges such as faults and cyberattacks.

Method: The paper proposes the SecureSmart HPA that monitors resource demands, detects disruptions, evaluates wastage, dynamically adjusts scaling, and enables resource sharing among microservices to enhance resilience and efficiency.

Result: SecureSmart HPA achieves up to 57.2% reduction in CPU overutilization and a 51.1% increase in resource allocation during disruptions compared to Smart HPA.

Conclusion: The findings demonstrate the effectiveness of SecureSmart HPA in delivering resilient and efficient auto-scaling in volatile, resource-constrained environments.

Abstract: Horizontal Pod Auto-scalers (HPAs) are crucial for managing resource
allocation in microservice architectures to handle fluctuating workloads.
However, traditional HPAs fail to address resource disruptions caused by
faults, cyberattacks, maintenance, and other operational challenges. These
disruptions result in resource wastage, service unavailability, and HPA
performance degradation. To address these challenges, we extend our prior work
on Smart HPA and propose SecureSmart HPA, which offers resilient and
resource-efficient auto-scaling for microservice architectures. SecureSmart HPA
monitors microservice resource demands, detects disruptions, evaluates resource
wastage, and dynamically adjusts scaling decisions to enhance the resilience of
auto-scaling operations. Furthermore, SecureSmart HPA enables resource sharing
among microservices, optimizing scaling efficiency in resource-constrained
environments. Experimental evaluation at varying disruption severities, with
25%, 50%, and 75% resource wastage, demonstrates that SecureSmart HPA performs
effectively across different levels of disruptions. It achieves up to a 57.2%
reduction in CPU overutilization and a 51.1% increase in resource allocation
compared to Smart HPA, highlighting its ability to deliver resilient and
efficient auto-scaling operations in volatile and resource-constrained
environments.

</details>


### [47] [Malicious node aware wireless multi hop networks: a systematic review of the literature and recommendations for future research](https://arxiv.org/abs/2506.05742)
*Shahram Pourdehghan,Nahideh Derakhshanfard*

Main category: cs.DC

TL;DR: The paper reviews existing methods for detecting malicious nodes in wireless multi-hop networks and compares them across different network types.


<details>
  <summary>Details</summary>
Motivation: Wireless multi-hop networks suffer performance issues due to malicious nodes that drop packets, leading to reduced throughput, increased delay, and higher overhead.

Method: The authors categorize various wireless network types (e.g., MANET, DTN, VANET) and analyze existing literature on malicious node detection methods.

Result: The paper provides a comparison of malicious node detection techniques across different network types, identifying their strengths and weaknesses.

Conclusion: Awareness of malicious nodes is critical for effective routing in wireless multi-hop networks, and this paper highlights existing methods while acknowledging the need for further improvements.

Abstract: Wireless communication provides great advantages that are not available
through their wired counterparts such as flexibility, ease of deployment and
use, cost reductions, and convenience. Wireless multi-hop networks (WMN) do not
have any centralized management infrastructure. Wireless multi-hop networks
have many benefits since proposed. In such networks when a node wants to send a
packet to a destination where is not in the transmission range, depend on some
intermediate nodes. In this type of networks packet sending is in the form of
multiple hop until destination and this work is dynamic. Lack of centralized
management cause that some nodes show malicious function. Malicious nodes are
that receive packets and drop them maliciously. These malicious nodes could
have many reasons such as hardware failure, software failure or lack of power.
Such nodes make multiple packets drop from the network and the performance of
network strongly decreases. As a result, the throughput of the network
decrease, increase end-to-end delay and increase overhead. Therefore, we must
aware from presence of malicious node in the network and do routing based on
this awareness. Therefore, this paper aims to study and review the present
malicious node detection methods that proposed in literatures. We categorized
networks in groups, including ad hoc networks, MANET, DTN, Opportunistic
networks, WSN, VANET and other wireless networks and compare malicious node
detection met

</details>


### [48] [Perfect Matching with Few Link Activations](https://arxiv.org/abs/2506.06102)
*Hugo Mirault,Peter Robinson,Ming Ming Tan,Xianbin Zhu*

Main category: cs.DC

TL;DR: This paper addresses the problem of finding a perfect matching in a distributed network modeled as a complete bipartite graph. It introduces randomized and deterministic algorithms under different communication models, achieving significant reductions in time and pulse complexity.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute a perfect matching in distributed networks with constraints on communication and knowledge, improving upon the limitations of existing algorithms.

Method: The authors propose algorithms for two communication models: randomized methods for the port numbering model and deterministic methods for the KT_1 model. They analyze time and pulse complexity under these settings.

Result: In the port numbering model, randomized algorithms achieve $O(\log n)$ time and $O(n\log n)$ pulse complexity. In the KT_1 model, deterministic algorithms achieve $O(\log n)$ time with $O(n)$ pulses and can further reduce time complexity to $O(\log^*n\log\log n)$ with slight pulse complexity increases.

Conclusion: Knowledge of neighbor IDs (KT_1 model) significantly improves algorithm efficiency. Randomness is crucial in the port numbering model to achieve better complexity. The proposed methodologies provide scalable solutions in distributed computing.

Abstract: We consider the problem of computing a perfect matching problem in a
synchronous distributed network, where the network topology corresponds to a
complete bipartite graph. The communication between nodes is restricted to
activating communication links, which means that instead of sending messages
containing a number of bits, each node can only send a pulse over some of its
incident links in each round. In the port numbering model, where nodes are
unaware of their neighbor's IDs, we give a randomized algorithm that terminates
in $O( \log n )$ rounds and has a pulse complexity of $O( n\log n )$, which
corresponds to the number of pulses sent over all links. We also show that
randomness is crucial in the port numbering model, as any deterministic
algorithm must send at least $\Omega( n^2 )$ messages in the standard LOCAL
model, where the messages can be of unbounded size. Then, we turn our attention
to the KT_1 assumption, where each node starts out knowing its neighbors' IDs.
We show that this additional knowledge enables significantly improved bounds
even for deterministic algorithms. First, we give an $O( \log n )$ time
deterministic algorithm that sends only $O( n )$ pulses. Finally, we apply this
algorithm recursively to obtain an exponential reduction in the time complexity
to $O( \log^*n\log\log n )$, while slightly increasing the pulse complexity to
$O( n\log^*n )$. All our bounds also hold in the standard CONGEST model with
single-bit messages.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory](https://arxiv.org/abs/2506.05994)
*Yi-Chun Liao,Chieh-Lin Tsai,Yuan-Hao Chang,Camélia Slimani,Jalil Boukhobza,Tei-Wei Kuo*

Main category: cs.LG

TL;DR: This paper presents RETENTION, a framework aimed at reducing memory consumption for tree-based model inference using CAMs, achieving up to 207.12x space efficiency with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in efficiently accelerating tree-based models, which are superior for structured data but have high memory needs when accelerated using CAM.

Method: The authors propose RETENTION, which includes an iterative pruning algorithm tailored for bagging-based models and a tree mapping scheme with novel data placement strategies to minimize CAM memory redundancy.

Result: RETENTION achieved up to 207.12x memory efficiency improvement with less than 3% accuracy degradation.

Conclusion: RETENTION provides an effective and resource-efficient approach to address the memory consumption challenges in accelerating tree-based models.

Abstract: Although deep learning has demonstrated remarkable capabilities in learning
from unstructured data, modern tree-based ensemble models remain superior in
extracting relevant information and learning from structured datasets. While
several efforts have been made to accelerate tree-based models, the inherent
characteristics of the models pose significant challenges for conventional
accelerators. Recent research leveraging content-addressable memory (CAM)
offers a promising solution for accelerating tree-based models, yet existing
designs suffer from excessive memory consumption and low utilization. This work
addresses these challenges by introducing RETENTION, an end-to-end framework
that significantly reduces CAM capacity requirement for tree-based model
inference. We propose an iterative pruning algorithm with a novel pruning
criterion tailored for bagging-based models (e.g., Random Forest), which
minimizes model complexity while ensuring controlled accuracy degradation.
Additionally, we present a tree mapping scheme that incorporates two innovative
data placement strategies to alleviate the memory redundancy caused by the
widespread use of don't care states in CAM. Experimental results show that
implementing the tree mapping scheme alone achieves $1.46\times$ to $21.30
\times$ better space efficiency, while the full RETENTION framework yields
$4.35\times$ to $207.12\times$ improvement with less than 3% accuracy loss.
These results demonstrate that RETENTION is highly effective in reducing CAM
capacity requirement, providing a resource-efficient direction for tree-based
model acceleration.

</details>


### [50] [BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures](https://arxiv.org/abs/2506.05871)
*Xiannan Hu,Tianyou Zeng,Xiaoming Yuan,Liwei Song,Guangyuan Zhang,Bangzheng He*

Main category: cs.LG

TL;DR: BestServe is a framework for efficiently determining optimal serving strategies for large language models, reducing labor-intensive trial-and-error processes.


<details>
  <summary>Details</summary>
Motivation: Efficiently serving large language models to millions of users demands optimized resource allocation and parallelism strategies, which are traditionally labor-intensive to develop.

Method: BestServe utilizes an inference simulator based on an adapted roofline model and CPU-GPU dispatch dynamics to evaluate serving strategies in minutes.

Result: The framework determines optimal strategies without costly benchmarking, offering predictions within a 20% error margin.

Conclusion: BestServe is practical for rapid deployment planning due to its lightweight design, extensibility, and significant accuracy in strategy prediction.

Abstract: Serving large language models (LLMs) to millions of users requires efficient
resource allocation and parallelism strategies. It is a labor intensive
trial-and-error process to find such a strategy. We present BestServe, a novel
framework for ranking serving strategies by estimating goodput under various
operating scenarios. Supporting both collocated and disaggregated
architectures, BestServe leverages an inference simulator built on an adapted
roofline model and CPU-GPU dispatch dynamics. Our framework determines the
optimal strategy in minutes on a single standard CPU, eliminating the need for
costly benchmarking, while achieving predictions within a $20\%$ error margin.
It appeals to be practical for rapid deployment planning because of its
lightweight design and strong extensibility.

</details>


### [51] [Mixture-of-Experts Meets In-Context Reinforcement Learning](https://arxiv.org/abs/2506.05426)
*Wenhao Wu,Fuhong Liu,Haoru Li,Zican Hu,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: This paper introduces T2MIR, a framework using novel mixture-of-experts (MoE) architecture in transformers for improved in-context reinforcement learning (ICRL).


<details>
  <summary>Details</summary>
Motivation: The two primary challenges in ICRL are handling the multi-modal nature of state-action-reward data and the diverse, heterogeneous decision-making tasks.

Method: T2MIR replaces the feedforward transformer layer with two parallel MoE layers. The token-wise MoE handles multi-modality by considering token semantics, while the task-wise MoE assigns specific tasks to expert components to address task diversity. A contrastive learning method is employed to optimize task-routing by maximizing mutual information between tasks and their router representation.

Result: T2MIR enhances in-context learning and outperforms various baselines by providing a scalable, efficient framework for reinforcing learning in multi-modal, diverse task settings.

Conclusion: T2MIR demonstrates the potential of MoE approaches for ICRL, providing a significant step forward with promising results, showing parallels to successes in language and vision domains.

Abstract: In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose \textbf{T2MIR} (\textbf{T}oken- and \textbf{T}ask-wise
\textbf{M}oE for \textbf{I}n-context \textbf{R}L), an innovative framework that
introduces architectural advances of mixture-of-experts (MoE) into
transformer-based decision models. T2MIR substitutes the feedforward layer with
two parallel layers: a token-wise MoE that captures distinct semantics of input
tokens across multiple modalities, and a task-wise MoE that routes diverse
tasks to specialized experts for managing a broad task distribution with
alleviated gradient conflicts. To enhance task-wise routing, we introduce a
contrastive learning method that maximizes the mutual information between the
task and its router representation, enabling more precise capture of
task-relevant information. The outputs of two MoE components are concatenated
and fed into the next layer. Comprehensive experiments show that T2MIR
significantly facilitates in-context learning capacity and outperforms various
types of baselines. We bring the potential and promise of MoE to ICRL, offering
a simple and scalable architectural enhancement to advance ICRL one step closer
toward achievements in language and vision communities. Our code is available
at https://github.com/NJU-RL/T2MIR.

</details>


### [52] [MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction](https://arxiv.org/abs/2506.05427)
*Zishan Shu,Yufan Deng,Hongyu Zhang,Zhiwei Nie,Jie Chen*

Main category: cs.LG

TL;DR: The paper introduces a new model, MTPNet, for improved activity cliff prediction in drug discovery by integrating molecular-protein interactions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for activity cliff prediction are limited as they focus only on single binding targets.

Method: MTPNet utilizes multi-grained protein semantics, leveraging both macro-level target guidance and micro-level pocket guidance to dynamically refine molecular representations.

Result: On 30 datasets, MTPNet surpasses previous models, achieving an average RMSE improvement of 18.95% over mainstream GNN architectures.

Conclusion: MTPNet enhances interaction pattern understanding through conditional deep learning, aiding the acceleration of compound optimization and design.

Abstract: Activity cliff prediction is a critical task in drug discovery and material
design. Existing computational methods are limited to handling single binding
targets, which restricts the applicability of these prediction models. In this
paper, we present the Multi-Grained Target Perception network (MTPNet) to
incorporate the prior knowledge of interactions between the molecules and their
target proteins. Specifically, MTPNet is a unified framework for activity cliff
prediction, which consists of two components: Macro-level Target Semantic (MTS)
guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet
dynamically optimizes molecular representations through multi-grained protein
semantic conditions. To our knowledge, it is the first time to employ the
receptor proteins as guiding information to effectively capture critical
interaction details. Extensive experiments on 30 representative activity cliff
datasets demonstrate that MTPNet significantly outperforms previous approaches,
achieving an average RMSE improvement of 18.95% on top of several mainstream
GNN architectures. Overall, MTPNet internalizes interaction patterns through
conditional deep learning to achieve unified predictions of activity cliffs,
helping to accelerate compound optimization and design. Codes are available at:
https://github.com/ZishanShu/MTPNet.

</details>


### [53] [Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction](https://arxiv.org/abs/2506.05428)
*Zhihao Tang,Chaozhuo Li,Litian Zhang,Xi Zhang*

Main category: cs.LG

TL;DR: The paper presents MCI-Diff, a novel framework for predicting Mild Cognitive Impairment (MCI) conversion using baseline sMRI data while addressing the trade-off between immediacy and accuracy via a diffusion-based approach.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenge of balancing fast MCI prediction from a single baseline sMRI with higher accuracy obtained from longitudinal scans.

Method: MCI-Diff employs a diffusion-based framework with two key innovations: multi-task sequence reconstruction to learn robust latent trajectories, and a language model-guided sampling technique that steers autoregressive generation toward clinically plausible disease patterns.

Result: Experiments conducted on the ADNI and AIBL datasets demonstrate that MCI-Diff achieves 5-12% higher accuracy in predicting early MCI conversion compared to state-of-the-art methods.

Conclusion: MCI-Diff integrates advanced diffusion and language model techniques to enhance both immediacy and accuracy in early MCI conversion prediction, offering a clinically viable solution.

Abstract: Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by
a trade-off between immediacy--making fast predictions from a single baseline
sMRI--and accuracy--leveraging longitudinal scans to capture disease
progression. We propose MCI-Diff, a diffusion-based framework that synthesizes
clinically plausible future sMRI representations directly from baseline data,
achieving both real-time risk assessment and high predictive performance.
First, a multi-task sequence reconstruction strategy trains a shared denoising
network on interpolation and extrapolation tasks to handle irregular follow-up
sampling and learn robust latent trajectories. Second, an LLM-driven
"linguistic compass" is introduced for clinical plausibility sampling:
generated feature candidates are quantized, tokenized, and scored by a
fine-tuned language model conditioned on expected structural biomarkers,
guiding autoregressive generation toward realistic disease patterns.
Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms
state-of-the-art baselines, improving early conversion accuracy by 5-12%.

</details>


### [54] [PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling](https://arxiv.org/abs/2506.05432)
*Yuxuan Yue,Zukang Xu,Zhihang Yuan,Dawei Yang,Jianglong Wu,Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces Polar Coordinate Decoupled Vector Quantization (PCDVQ), which decouples vector direction and magnitude for better quantization of large language model parameters, achieving higher compression and accuracy.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) face challenges in edge deployment due to massive parameter sizes. The paper explores improving Vector Quantization (VQ) methods to enhance accuracy while maintaining low-bit compression.

Method: The paper proposes PCDVQ, consisting of two modules: Polar Coordinate Decoupling (PCD) for separate quantization of direction and magnitude, and Distribution Aligned Codebook Construction (DACC) to optimize codebooks based on source distributions.

Result: PCDVQ demonstrates superior zero-shot accuracy by at least 1.5% compared to baseline methods at 2-bit quantization.

Conclusion: PCDVQ provides an effective framework for achieving accurate and highly compressed LLMs, presenting a novel quantization approach using polar coordinates.

Abstract: Large Language Models (LLMs) face significant challenges in edge deployment
due to their massive parameter scale. Vector Quantization (VQ), a
clustering-based quantization method, serves as a prevalent solution to this
issue for its extremely low-bit (even at 2-bit) and considerable accuracy.
Since a vector is a quantity in mathematics and physics that has both direction
and magnitude, existing VQ works typically quantize them in a coupled manner.
However, we find that direction exhibits significantly greater sensitivity to
quantization compared to the magnitude. For instance, when separately
clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the
accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap
even increases with the reduction of clustering centers. Further, Euclidean
distance, a common metric to access vector similarities in current VQ works,
places greater emphasis on reducing the magnitude error. This property is
contrary to the above finding, unavoidably leading to larger quantization
errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector
Quantization (PCDVQ), an effective and efficient VQ framework consisting of two
key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors
into their polar coordinate representations and perform independent
quantization of the direction and magnitude parameters.2) Distribution Aligned
Codebook Construction (DACC), which optimizes the direction and magnitude
codebooks in accordance with the source distribution. Experimental results show
that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\%
zero-shot accuracy, establishing a novel paradigm for accurate and highly
compressed LLMs.

</details>


### [55] [Zeroth-Order Optimization Finds Flat Minima](https://arxiv.org/abs/2506.05454)
*Liang Zhang,Bingcong Li,Kiran Koshy Thekumparampil,Sewoong Oh,Michael Muehlebach,Niao He*

Main category: cs.LG

TL;DR: The paper explores implicit regularization in zeroth-order optimization, showing it favors flat minima and provides theoretical and experimental evidence.


<details>
  <summary>Details</summary>
Motivation: To better understand zeroth-order optimization beyond convergence to stationary points, focusing on the implicit regularization that guides solution selection.

Method: Analyzed zeroth-order optimization using the two-point estimator and derived theoretical convergence rates for flat minima in convex and smooth functions.

Result: Demonstrated that zeroth-order optimization favors solutions with small Hessian trace, validated by experiments on binary classification and language model fine-tuning.

Conclusion: Zeroth-order optimization inherently biases toward flat minima, offering insights into its applications and performance in machine learning tasks.

Abstract: Zeroth-order methods are extensively used in machine learning applications
where gradients are infeasible or expensive to compute, such as black-box
attacks, reinforcement learning, and language model fine-tuning. Existing
optimization theory focuses on convergence to an arbitrary stationary point,
but less is known on the implicit regularization that provides a fine-grained
characterization on which particular solutions are finally reached. We show
that zeroth-order optimization with the standard two-point estimator favors
solutions with small trace of Hessian, which is widely used in previous work to
distinguish between sharp and flat minima. We further provide convergence rates
of zeroth-order optimization to approximate flat minima for convex and
sufficiently smooth functions, where flat minima are defined as the minimizers
that achieve the smallest trace of Hessian among all optimal solutions.
Experiments on binary classification tasks with convex losses and language
model fine-tuning support our theoretical findings.

</details>


### [56] [Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward](https://arxiv.org/abs/2506.05433)
*Zikang Liu,Tongtian Yue,Yepeng Tang,Longteng Guo,Junxian Cai,Qingbin Liu,Xi Chen,Jing Liu*

Main category: cs.LG

TL;DR: Prefix Grouper is an efficient algorithm for improving the scalability of GRPO in long-context scenarios by eliminating redundant prefix computations, maintaining training equivalence and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: GRPO is effective for policy learning but introduces substantial computational inefficiencies when handling long shared prefixes, creating scalability challenges.

Method: The Prefix Grouper algorithm restructures self-attention into two parts, enabling shared prefixes to be encoded only once, which eliminates computational redundancy while maintaining full differentiability and training equivalence with GRPO.

Result: Experiments confirm that Prefix Grouper achieves consistent policy learning results while significantly reducing computational costs, especially in long-prefix scenarios.

Conclusion: Prefix Grouper improves GRPO scalability and training efficiency, allowing the use of larger group sizes for complex tasks without compromising policy performance. The approach is plug-and-play, requiring minimal adjustments to existing architectures.

Abstract: Group Relative Policy Optimization (GRPO) enhances policy learning by
computing gradients from relative comparisons among candidate outputs that
share a common input prefix. Despite its effectiveness, GRPO introduces
substantial computational overhead when processing long shared prefixes, which
must be redundantly encoded for each group member. This inefficiency becomes a
major scalability bottleneck in long-context learning scenarios. We propose
Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant
prefix computation via a Shared-Prefix Forward strategy. In particular, by
restructuring self-attention into two parts, our method enables the shared
prefix to be encoded only once, while preserving full differentiability and
compatibility with end-to-end training. We provide both theoretical and
empirical evidence that Prefix Grouper is training-equivalent to standard GRPO:
it yields identical forward outputs and backward gradients, ensuring that the
optimization dynamics and final policy performance remain unchanged.
Empirically, our experiments confirm that Prefix Grouper achieves consistent
results while significantly reducing the computational cost of training,
particularly in long-prefix scenarios. The proposed method is fully
plug-and-play: it is compatible with existing GRPO-based architectures and can
be seamlessly integrated into current training pipelines as a drop-in
replacement, requiring no structural modifications and only minimal changes to
input construction and attention computation. Prefix Grouper enables the use of
larger group sizes under the same computational budget, thereby improving the
scalability of GRPO to more complex tasks and larger models. Code is now
available at https://github.com/johncaged/PrefixGrouper

</details>


### [57] [AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization](https://arxiv.org/abs/2506.05634)
*Saeed Hedayatian,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: AutoQD introduces a method to automatically create behavioral descriptors for Quality-Diversity (QD) algorithms by embedding policy occupancy measures in Markov Decision Processes, overcoming the need for hand-crafted descriptors.


<details>
  <summary>Details</summary>
Motivation: Existing QD algorithms rely on predefined behavioral descriptors, which constrain exploration and require domain-specific knowledge. A new approach is needed for automated and meaningful behavior discovery.

Method: AutoQD uses random Fourier features to approximate the Maximum Mean Discrepancy (MMD) between policy occupancy measures, generating embeddings that represent behavioral differences. These embeddings are transformed into low-dimensional behavioral descriptors suitable for standard QD methods.

Result: Experiments across multiple continuous control tasks confirm that AutoQD successfully discovers diverse and high-performing policies without relying on predefined behavioral descriptors, proving its utility in unsupervised reinforcement learning.

Conclusion: AutoQD provides a theoretically sound and practical framework for automated behavior discovery in sequential decision-making, eliminating the reliance on domain-specific, hand-crafted descriptors and enabling open-ended learning.

Abstract: Quality-Diversity (QD) algorithms have shown remarkable success in
discovering diverse, high-performing solutions, but rely heavily on
hand-crafted behavioral descriptors that constrain exploration to predefined
notions of diversity. Leveraging the equivalence between policies and occupancy
measures, we present a theoretically grounded approach to automatically
generate behavioral descriptors by embedding the occupancy measures of policies
in Markov Decision Processes. Our method, AutoQD, leverages random Fourier
features to approximate the Maximum Mean Discrepancy (MMD) between policy
occupancy measures, creating embeddings whose distances reflect meaningful
behavioral differences. A low-dimensional projection of these embeddings that
captures the most behaviorally significant dimensions is then used as
behavioral descriptors for off-the-shelf QD methods. We prove that our
embeddings converge to true MMD distances between occupancy measures as the
number of sampled trajectories and embedding dimensions increase. Through
experiments in multiple continuous control tasks we demonstrate AutoQD's
ability in discovering diverse policies without predefined behavioral
descriptors, presenting a well-motivated alternative to prior methods in
unsupervised Reinforcement Learning and QD optimization. Our approach opens new
possibilities for open-ended learning and automated behavior discovery in
sequential decision making settings without requiring domain-specific
knowledge.

</details>


### [58] [The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models](https://arxiv.org/abs/2506.05500)
*Alex Damian,Jason D. Lee,Joan Bruna*

Main category: cs.LG

TL;DR: The paper explores Gaussian Multi-index models, focusing on label dependency through low-dimensional subspace projections. It develops sample complexity bounds and proposes an efficient agnostic estimation method using spectral U-statistics and Hermite tensors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study Gaussian Multi-index models and advance estimation methodologies for low-dimensional hidden subspaces, central to fields like machine learning and statistics.

Method: The method includes analyzing sample complexity requirements using the Low-Degree-Polynomial framework and designing a sequential agnostic estimation procedure leveraging spectral U-statistics over Hermite tensors.

Result: The authors identify optimal sample complexity conditions and compute generative leap exponents for applications such as piecewise linear functions and deep neural networks.

Conclusion: This work establishes both theoretical sample complexity bounds and practical estimation mechanisms for Gaussian Multi-index models, expanding understanding in agnostic modeling and deep learning tasks.

Abstract: In this work we consider generic Gaussian Multi-index models, in which the
labels only depend on the (Gaussian) $d$-dimensional inputs through their
projection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient
agnostic estimation procedures for this hidden subspace. We introduce the
\emph{generative leap} exponent $k^\star$, a natural extension of the
generative exponent from [Damian et al.'24] to the multi-index setting. We
first show that a sample complexity of $n=\Theta(d^{1 \vee \k/2})$ is necessary
in the class of algorithms captured by the Low-Degree-Polynomial framework. We
then establish that this sample complexity is also sufficient, by giving an
agnostic sequential estimation procedure (that is, requiring no prior knowledge
of the multi-index model) based on a spectral U-statistic over appropriate
Hermite tensors. We further compute the generative leap exponent for several
examples including piecewise linear functions (deep ReLU networks with bias),
and general deep neural networks (with $r$-dimensional first hidden layer).

</details>


### [59] [Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks](https://arxiv.org/abs/2506.05434)
*Thomas Massena,Léo andéol,Thibaut Boissin,Franck Mamalet,Corentin Friedrich,Mathieu Serrurier,Sébastien Gerchinovitz*

Main category: cs.LG

TL;DR: The paper introduces 'lip-rcp,' a method leveraging Lipschitz-bounded networks to create smaller, efficient Conformal Prediction (CP) sets that maintain robustness under adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Current robust CP methods struggle in large-scale settings by producing overly large prediction sets or requiring significant computational resources.

Method: The authors use Lipschitz-bounded networks, particularly 1-Lipschitz robust networks, to estimate robust CP sets. They also derive new worst-case coverage bounds for vanilla CP under adversarial attacks.

Result: The proposed 'lip-rcp' method achieves superior performance in both robustness and computational efficiency when applied to medium and large-scale datasets like ImageNet.

Conclusion: 'Lip-rcp' provides a promising balance between robustness guarantees and practicality in real-world large-scale prediction tasks.

Abstract: Conformal Prediction (CP) has proven to be an effective post-hoc method for
improving the trustworthiness of neural networks by providing prediction sets
with finite-sample guarantees. However, under adversarial attacks, classical
conformal guarantees do not hold anymore: this problem is addressed in the
field of Robust Conformal Prediction. Several methods have been proposed to
provide robust CP sets with guarantees under adversarial perturbations, but,
for large scale problems, these sets are either too large or the methods are
too computationally demanding to be deployed in real life scenarios. In this
work, we propose a new method that leverages Lipschitz-bounded networks to
precisely and efficiently estimate robust CP sets. When combined with a
1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms
state-of-the-art results in both the size of the robust CP sets and
computational efficiency in medium and large-scale scenarios such as ImageNet.
Taking a different angle, we also study vanilla CP under attack, and derive new
worst-case coverage bounds of vanilla CP sets, which are valid simultaneously
for all adversarial attack levels. Our lip-rcp method makes this second
approach as efficient as vanilla CP while also allowing robustness guarantees.

</details>


### [60] [Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning](https://arxiv.org/abs/2506.05977)
*Yujia Huo,Jianchun Liu,Hongli Xu,Zhenguo Ma,Shilong Wang,Liusheng Huang*

Main category: cs.LG

TL;DR: FedBE is an innovative framework for federated fine-tuning of large language models, addressing challenges such as catastrophic forgetting and heterogeneous environments through structural expansion and adaptive allocation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve issues in federated fine-tuning like catastrophic forgetting and adapting to heterogeneous data distributions and device capabilities.

Method: FedBE employs transformer block expansion and adaptive block allocation strategies to segregate task-specific knowledge and optimize performance across varied environments.

Result: FedBE delivers 12-74% better accuracy retention on general tasks, accelerates convergence by 1.9-3.1x, and maintains downstream accuracy.

Conclusion: FedBE notably improves federated fine-tuning by effectively managing heterogeneous client environments and mitigating catastrophic forgetting.

Abstract: Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as
a promising solution for adapting models to distributed data environments while
ensuring data privacy.
  Existing FedFT methods predominantly utilize parameter-efficient fine-tuning
(PEFT) techniques to reduce communication and computation overhead.
  However, they often fail to adequately address the catastrophic forgetting, a
critical challenge arising from continual adaptation in distributed
environments. The traditional centralized fine-tuning methods, which are not
designed for the heterogeneous and privacy-constrained nature of federated
environments, struggle to mitigate this issue effectively. Moreover, the
challenge is further exacerbated by significant variation in data distributions
and device capabilities across clients, which leads to intensified forgetting
and degraded model generalization. To tackle these issues, we propose FedBE, a
novel FedFT framework that integrates an adaptive transformer block expansion
mechanism with a dynamic trainable-block allocation strategy. Specifically,
FedBE expands trainable blocks within the model architecture, structurally
separating newly learned task-specific knowledge from the original pre-trained
representations. Additionally, FedBE dynamically assigns these trainable blocks
to clients based on their data distributions and computational capabilities.
This enables the framework to better accommodate heterogeneous federated
environments and enhances the generalization ability of the model.Extensive
experiments show that compared with existing federated fine-tuning methods,
FedBE achieves 12-74% higher accuracy retention on general tasks after
fine-tuning and a model convergence acceleration ratio of 1.9-3.1x without
degrading the accuracy of downstream tasks.

</details>


### [61] [Winner-takes-all for Multivariate Probabilistic Time Series Forecasting](https://arxiv.org/abs/2506.05515)
*Adrien Cortés,Rémi Rehm,Victor Letzelter*

Main category: cs.LG

TL;DR: TimeMCL adapts the MCL paradigm to forecast diverse plausible futures in time series using neural networks with multiple heads and WTA loss.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of forecasting diverse plausible futures in time series, which is essential in ambiguous and ill-posed tasks.

Method: TimeMCL employs neural networks with multiple heads and a Winner-Takes-All (WTA) loss to ensure diversification in predictions, adapting MCL for time-series forecasting.

Result: Synthetic and real-world data evaluations show promising performance with light computational demands.

Conclusion: TimeMCL offers an efficient and promising solution for time series forecasting by incorporating diversity and computational efficiency.

Abstract: We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL)
paradigm to forecast multiple plausible time series futures. Our approach
employs a neural network with multiple heads and utilizes the Winner-Takes-All
(WTA) loss to promote diversity among predictions. MCL has recently gained
attention due to its simplicity and ability to address ill-posed and ambiguous
tasks. We propose an adaptation of this framework for time-series forecasting,
presenting it as an efficient method to predict diverse futures, which we
relate to its implicit quantization objective. We provide insights into our
approach using synthetic data and evaluate it on real-world time series,
demonstrating its promising performance at a light computational cost.

</details>


### [62] [Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning](https://arxiv.org/abs/2506.05435)
*Manon Renault,Hamoud Younes,Hugo Tessier,Ronan Le Roy,Bastien Pasdeloup,Mathieu Léonardon*

Main category: cs.LG

TL;DR: The paper presents an embedded IoT system using deep learning to monitor reusable package states efficiently and sustainably with high precision and low power consumption.


<details>
  <summary>Details</summary>
Motivation: To enhance operational efficiency and ecological sustainability in package monitoring by developing a long-lasting, embedded system for reusable packages.

Method: A one-dimensional Convolutional Neural Network (CNN) is trained on imbalanced, multiclass time-series accelerometer data using data augmentation. Compression techniques are applied to reduce model size for IoT deployment.

Result: Achieved 94.54% and 95.83% precision in a two-class classification problem, while reducing model size by four-fold and ensuring a low inference power consumption of 316 mW.

Conclusion: The proposed method demonstrates practical applicability for accurate, efficient, and sustainable package state monitoring, aligned with ecological goals and industrial needs.

Abstract: Package monitoring is an important topic in industrial applications, with
significant implications for operational efficiency and ecological
sustainability. In this study, we propose an approach that employs an embedded
system, placed on reusable packages, to detect their state (on a Forklift, in a
Truck, or in an undetermined location). We aim to design a system with a
lifespan of several years, corresponding to the lifespan of reusable packages.
Our analysis demonstrates that maximizing device lifespan requires minimizing
wake time. We propose a pipeline that includes data processing, training, and
evaluation of the deep learning model designed for imbalanced, multiclass time
series data collected from an embedded sensor. The method uses a
one-dimensional Convolutional Neural Network architecture to classify
accelerometer data from the IoT device. Before training, two data augmentation
techniques are tested to solve the imbalance problem of the dataset: the
Synthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling
approach. After training, compression techniques are implemented to have a
small model size. On the considered twoclass problem, the methodology yields a
precision of 94.54% for the first class and 95.83% for the second class, while
compression techniques reduce the model size by a factor of four. The trained
model is deployed on the IoT device, where it operates with a power consumption
of 316 mW during inference.

</details>


### [63] [Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library](https://arxiv.org/abs/2506.06122)
*Weixun Wang,Shaopan Xiong,Gengru Chen,Wei Gao,Sheng Guo,Yancheng He,Ju Huang,Jiaheng Liu,Zhendong Li,Xiaoyang Li,Zichen Liu,Haizhou Zhao,Dakai An,Lunxi Cao,Qiyang Cao,Wanxi Deng,Feilei Du,Yiliang Gu,Jiahe Li,Xiang Li,Mingjie Liu,Yijia Luo,Zihe Liu,Yadao Wang,Pei Wang,Tianyuan Wu,Yanan Wu,Yuheng Zhao,Shuaibing Zhao,Jin Yang,Siran Yang,Yingshui Tan,Huimin Yi,Yuchi Xu,Yujin Yuan,Xingyao Zhang,Lin Qu,Wenbo Su,Wei Wang,Jiamang Wang,Bo Zheng*

Main category: cs.LG

TL;DR: The paper introduces ROLL, a library designed to optimize large-scale reinforcement learning by offering efficiency, scalability, and user-friendly features tailored for tech pioneers, developers, and researchers.


<details>
  <summary>Details</summary>
Motivation: To address the need for a cost-effective, fault-tolerant, and flexible platform for large-scale reinforcement learning training pipelines.

Method: ROLL leverages a single-controller architecture, parallel strategies, rollout schedulers, dedicated workers for environments and rewards, and AutoDeviceMapping for resource management.

Result: ROLL provides a scalable, efficient, and user-adaptive approach to managing large-scale RL training and experimentation.

Conclusion: ROLL effectively caters to diverse user groups by providing a simplified, robust infrastructure for reinforcement learning optimization at scale.

Abstract: We introduce ROLL, an efficient, scalable, and user-friendly library designed
for Reinforcement Learning Optimization for Large-scale Learning. ROLL caters
to three primary user groups: tech pioneers aiming for cost-effective,
fault-tolerant large-scale training, developers requiring flexible control over
training workflows, and researchers seeking agile experimentation. ROLL is
built upon several key modules to serve these user groups effectively. First, a
single-controller architecture combined with an abstraction of the parallel
worker simplifies the development of the training pipeline. Second, the
parallel strategy and data transfer modules enable efficient and scalable
training. Third, the rollout scheduler offers fine-grained management of each
sample's lifecycle during the rollout stage. Fourth, the environment worker and
reward worker support rapid and flexible experimentation with agentic RL
algorithms and reward designs. Finally, AutoDeviceMapping allows users to
assign resources to different models flexibly across various stages.

</details>


### [64] [When can in-context learning generalize out of task distribution?](https://arxiv.org/abs/2506.05574)
*Chase Goddard,Lindsay M. Smith,Vudtiwat Ngampruetikorn,David J. Schwab*

Main category: cs.LG

TL;DR: The study investigates conditions for the emergence and generalization of in-context learning (ICL) in transformers, focusing on task diversity during training.


<details>
  <summary>Details</summary>
Motivation: Understanding when and how in-context learning capabilities arise in pretrained transformers and generalize to unseen tasks.

Method: Empirical analysis of transformers trained on linear functions, studying transitions in solutions based on data diversity, and constructing phase diagrams to explore the impacts of model size and task dimensionality.

Result: Increasing task diversity leads to a shift in transformers from task-specific learning to generalizable ICL solutions, with similar observations seen in nonlinear tasks.

Conclusion: Task diversity during pretraining critically influences the capacity of transformers to generalize ICL to out-of-distribution tasks, shaped by model depth and task complexity factors.

Abstract: In-context learning (ICL) is a remarkable capability of pretrained
transformers that allows models to generalize to unseen tasks after seeing only
a few examples. We investigate empirically the conditions necessary on the
pretraining distribution for ICL to emerge and generalize
\emph{out-of-distribution}. Previous work has focused on the number of distinct
tasks necessary in the pretraining dataset. Here, we use a different notion of
task diversity to study the emergence of ICL in transformers trained on linear
functions. We find that as task diversity increases, transformers undergo a
transition from a specialized solution, which exhibits ICL only within the
pretraining task distribution, to a solution which generalizes out of
distribution to the entire task space. We also investigate the nature of the
solutions learned by the transformer on both sides of the transition, and
observe similar transitions in nonlinear regression problems. We construct a
phase diagram to characterize how our concept of task diversity interacts with
the number of pretraining tasks. In addition, we explore how factors such as
the depth of the model and the dimensionality of the regression problem
influence the transition.

</details>


### [65] [On Fitting Flow Models with Large Sinkhorn Couplings](https://arxiv.org/abs/2506.05526)
*Michal Klein,Alireza Mousavi-Hosseini,Stephen Zhang,Marco Cuturi*

Main category: cs.LG

TL;DR: This paper investigates scaling up the size of mini-batches ($n$) in flow model training and examines the impact of entropic regularization ($\varepsilon$) in the Sinkhorn algorithm, demonstrating benefits in generation tasks.


<details>
  <summary>Details</summary>
Motivation: Training flow models to solve the dynamical optimal transport (OT) problem is challenging when source-target pairings are not predefined. Existing methods face high computational costs and slow convergence when coupling points independently. The authors aim to improve training efficiency and performance by leveraging optimal couplings.

Method: The authors propose increasing the size of mini-batches ($n$) by three to four orders of magnitude and carefully analyzing the effect of the entropic regularization parameter ($\varepsilon$) in the Sinkhorn algorithm. They introduce new scale-invariant metrics for evaluating couplings and employ sharded computation across GPUs.

Result: Using large mini-batch sizes and low entropic regularization in the Sinkhorn algorithm enhances flow model performance for both synthetic and image generation tasks.

Conclusion: Optimally coupling source and target points using large Sinkhorn couplings with careful tuning of $\varepsilon$ leads to significant gains in both training efficiency and task-specific outcomes in flow models.

Abstract: Flow models transform data gradually from one modality (e.g. noise) onto
another (e.g. images). Such models are parameterized by a time-dependent
velocity field, trained to fit segments connecting pairs of source and target
points. When the pairing between source and target points is given, training
flow models boils down to a supervised regression problem. When no such pairing
exists, as is the case when generating data from noise, training flows is much
harder. A popular approach lies in picking source and target points
independently. This can, however, lead to velocity fields that are slow to
train, but also costly to integrate at inference time. In theory, one would
greatly benefit from training flow models by sampling pairs from an optimal
transport (OT) measure coupling source and target, since this would lead to a
highly efficient flow solving the Benamou and Brenier dynamical OT problem. In
practice, recent works have proposed to sample mini-batches of $n$ source and
$n$ target points and reorder them using an OT solver to form better pairs.
These works have advocated using batches of size $n\approx 256$, and considered
OT solvers that return couplings that are either sharp (using e.g. the
Hungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.
Sinkhorn). We follow in the footsteps of these works by exploring the benefits
of increasing $n$ by three to four orders of magnitude, and look more carefully
on the effect of the entropic regularization $\varepsilon$ used in the Sinkhorn
algorithm. Our analysis is facilitated by new scale invariant quantities to
report the sharpness of a coupling, while our sharded computations across
multiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic
and image generation tasks, flow models greatly benefit when fitted with large
Sinkhorn couplings, with a low entropic regularization $\varepsilon$.

</details>


### [66] [An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics](https://arxiv.org/abs/2506.05438)
*Tongda Sun,Chen Yin,Huailiang Zheng,Yining Dong*

Main category: cs.LG

TL;DR: This paper proposes a new method for constructing dynamic health indicators (HI) for rolling bearings, leveraging an unsupervised learning framework that captures temporal dependencies and does not rely on expert knowledge.


<details>
  <summary>Details</summary>
Motivation: Most existing HI construction methods depend on expert knowledge and fail to account for the dynamic nature of degradation processes, limiting their effectiveness in representing degradation trends and future predictions.

Method: The method involves two main modules: a skip-connection-based autoencoder for unsupervised extraction of degradation features, and an HI-generating module with a built-in prediction block to model temporal dependencies in the HI states.

Result: Experiment results on two bearing lifecycle datasets show that the proposed method outperforms other approaches in constructing effective HIs for prognostic tasks.

Conclusion: The proposed dynamic HI effectively captures the inherent dynamics of degradation processes and is more suitable for modeling degradation tendencies and predicting future states.

Abstract: Health indicator (HI) plays a key role in degradation assessment and
prognostics of rolling bearings. Although various HI construction methods have
been investigated, most of them rely on expert knowledge for feature extraction
and overlook capturing dynamic information hidden in sequential degradation
processes, which limits the ability of the constructed HI for degradation trend
representation and prognostics. To address these concerns, a novel dynamic HI
that considers HI-level temporal dependence is constructed through an
unsupervised framework. Specifically, a degradation feature learning module
composed of a skip-connection-based autoencoder first maps raw signals to a
representative degradation feature space (DFS) to automatically extract
essential degradation features without the need for expert knowledge.
Subsequently, in this DFS, a new HI-generating module embedded with an inner
HI-prediction block is proposed for dynamic HI construction, where the temporal
dependence between past and current HI states is guaranteed and modeled
explicitly. On this basis, the dynamic HI captures the inherent dynamic
contents of the degradation process, ensuring its effectiveness for degradation
tendency modeling and future degradation prognostics. The experiment results on
two bearing lifecycle datasets demonstrate that the proposed HI construction
method outperforms comparison methods, and the constructed dynamic HI is
superior for prognostic tasks.

</details>


### [67] [UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss](https://arxiv.org/abs/2506.05443)
*Yiyu Lin,Yan Wang,You Zhou,Xinye Ni,Jiahui Wu,Sen Yang*

Main category: cs.LG

TL;DR: This paper presents UniPTMs, a novel approach for multi-type protein post-translational modification prediction, featuring advanced architectures and achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models struggle in effectively fusing multimodal features, managing domain generalization, and optimizing architectures for protein PTM prediction, necessitating new methodologies.

Method: The study introduces UniPTMs, a framework with a Master-Slave dual-path collaboration system combining advanced modules like BGCA, LDFN, MACP, BHGFN, and HDWF for effective multimodal feature integration.

Result: UniPTMs outperforms existing models in prediction accuracy (with MCC improvements of 3.2%-11.4% and AP improvements of 4.2%-14.3%) across five types of PTMs, addressing limitations of prior models.

Conclusion: UniPTMs establishes a unified framework for PTM prediction, excels in performance across diverse settings, and provides a lightweight version (UniPTMs-mini) for balanced complexity and accuracy.

Abstract: As a core mechanism of epigenetic regulation in eukaryotes, protein
post-translational modifications (PTMs) require precise prediction to decipher
dynamic life activity networks. To address the limitations of existing deep
learning models in cross-modal feature fusion, domain generalization, and
architectural optimization, this study proposes UniPTMs: the first unified
framework for multi-type PTM prediction. The framework innovatively establishes
a "Master-Slave" dual-path collaborative architecture: The master path
dynamically integrates high-dimensional representations of protein sequences,
structures, and evolutionary information through a Bidirectional Gated
Cross-Attention (BGCA) module, while the slave path optimizes feature
discrepancies and recalibration between structural and traditional features
using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale
Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and
a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level
feature integration across paths, the framework employs a Hierarchical Dynamic
Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal
features. Enhanced by a novel Hierarchical Contrastive loss function for
feature consistency optimization, UniPTMs demonstrates significant performance
improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art
models across five modification types and transcends the Single-Type Prediction
Paradigm. To strike a balance between model complexity and performance, we have
also developed a lightweight variant named UniPTMs-mini.

</details>


### [68] [Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic](https://arxiv.org/abs/2506.05445)
*Thanh Vinh Vo,Young Lee,Haozhe Ma,Chien Lu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: This paper introduces DoSAC, an algorithm that mitigates hidden confounder bias in reinforcement learning by combining the SAC framework with causal interventions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of hidden confounders in reinforcement learning, which can lead to biased policy learning and suboptimal behavior.

Method: The proposed method, DoSAC, leverages causal intervention estimation using the backdoor criterion, aided by a learnable Backdoor Reconstructor to infer pseudo-past variables for effective policy learning.

Result: Empirical results demonstrate that DoSAC outperforms baseline methods on continuous control tasks under confounded settings, showing enhanced robustness, generalization, and policy reliability.

Conclusion: The study concludes that integrating causal reasoning into RL frameworks like SAC can significantly improve performance in environments with hidden confounding issues.

Abstract: Hidden confounders that influence both states and actions can bias policy
learning in reinforcement learning (RL), leading to suboptimal or
non-generalizable behavior. Most RL algorithms ignore this issue, learning
policies from observational trajectories based solely on statistical
associations rather than causal effects. We propose DoSAC (Do-Calculus Soft
Actor-Critic with Backdoor Adjustment), a principled extension of the SAC
algorithm that corrects for hidden confounding via causal intervention
estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$
using the backdoor criterion, without requiring access to true confounders or
causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor
that infers pseudo-past variables (previous state and action) from the current
state to enable backdoor adjustment from observational data. This module is
integrated into a soft actor-critic framework to compute both the
interventional policy and its entropy. Empirical results on continuous control
benchmarks show that DoSAC outperforms baselines under confounded settings,
with improved robustness, generalization, and policy reliability.

</details>


### [69] [Conformal Prediction Adaptive to Unknown Subpopulation Shifts](https://arxiv.org/abs/2506.05583)
*Nien-Shao Wang,Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: The paper develops methods to adapt conformal prediction to subpopulation shifts, maintaining coverage guarantees even under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction lacks robustness when the test distribution differs from the calibration data, creating a need to address this limitation under subpopulation shifts.

Method: The authors propose algorithms that adaptively modify conformal prediction methods without prior knowledge of subpopulation structures, designed to scale to high-dimensional settings.

Result: The proposed methods outperformed standard conformal prediction by maintaining coverage and managing risk under subpopulation shifts, as validated through experiments in vision and language tasks.

Conclusion: The methods extend the effectiveness and reliability of conformal prediction under distribution shifts, making it applicable to real-world machine learning tasks with unknown subpopulation structures.

Abstract: Conformal prediction is widely used to equip black-box machine learning
models with uncertainty quantification enjoying formal coverage guarantees.
However, these guarantees typically break down in the presence of distribution
shifts, where the data distribution at test time differs from the training (or
calibration-time) distribution. In this work, we address subpopulation shifts,
where the test environment exhibits an unknown and differing mixture of
subpopulations compared to the calibration data. We propose new methods that
provably adapt conformal prediction to such shifts, ensuring valid coverage
without requiring explicit knowledge of subpopulation structure. Our algorithms
scale to high-dimensional settings and perform effectively in realistic machine
learning tasks. Extensive experiments on vision (with vision transformers) and
language (with large language models) benchmarks demonstrate that our methods
reliably maintain coverage and controls risk in scenarios where standard
conformal prediction fails.

</details>


### [70] [Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning](https://arxiv.org/abs/2506.05447)
*Andrei Mircea,Supriyo Chakraborty,Nima Chitsazan,Irina Rish,Ekaterina Lobacheva*

Main category: cs.LG

TL;DR: Scaling up language models mitigates loss deceleration, which is attributed to zero-sum learning (ZSL), a degenerate training dynamic causing destructive interference in gradients.


<details>
  <summary>Details</summary>
Motivation: To analyze how scaling positively affects training dynamics in language models, particularly focusing on abrupt loss slowdown.

Method: The study identifies loss deceleration patterns in training and attributes them to zero-sum learning (ZSL), which leads to opposing gradients in examples.

Result: Scaling decreases the loss where deceleration occurs and improves post-deceleration loss trends, potentially bypassing ZSL's limitations.

Conclusion: Understanding and targeting ZSL dynamics could optimize language model training beyond scaling alone, providing new paths for improvement.

Abstract: This work aims to understand how scaling improves language models,
specifically in terms of training dynamics. We find that language models
undergo loss deceleration early in training; an abrupt slowdown in the rate of
loss improvement, resulting in piecewise linear behaviour of the loss curve in
log-log space. Scaling up the model mitigates this transition by (1) decreasing
the loss at which deceleration occurs, and (2) improving the log-log rate of
loss improvement after deceleration. We attribute loss deceleration to a type
of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL,
per-example gradients become systematically opposed, leading to destructive
interference in per-example changes in loss. As a result, improving loss on one
subset of examples degrades it on another, bottlenecking overall progress. Loss
deceleration and ZSL provide new insights into the training dynamics underlying
language model scaling laws, and could potentially be targeted directly to
improve language models independent of scale. We make our code and artefacts
available at: https://github.com/mirandrom/zsl

</details>


### [71] [Zero-shot protein stability prediction by inverse folding models: a free energy interpretation](https://arxiv.org/abs/2506.05596)
*Jes Frellsen,Maher M. Kassem,Tone Bengtsen,Lars Olsen,Kresten Lindorff-Larsen,Jesper Ferkinghoff-Borg,Wouter Boomsma*

Main category: cs.LG

TL;DR: The paper explores the connection between inverse protein folding models and thermodynamic stability, suggesting enhancements for improved zero-shot stability predictions.


<details>
  <summary>Details</summary>
Motivation: To clarify the link between inverse folding model predictions and thermodynamic stability, enabling better theoretical understanding and stronger zero-shot protein stability predictions.

Method: The paper derives the free-energy foundations of inverse folding models, analyzing and suggesting improvements to the standard practice of likelihood ratios for stability estimation.

Result: The research demonstrates that improved zero-shot protein stability predictions are possible through straightforward modifications to current methods.

Conclusion: Simple yet effective adjustments to inverse folding model approaches can significantly enhance predictions of protein stability.

Abstract: Inverse folding models have proven to be highly effective zero-shot
predictors of protein stability. Despite this success, the link between the
amino acid preferences of an inverse folding model and the free-energy
considerations underlying thermodynamic stability remains incompletely
understood. A better understanding would be of interest not only from a
theoretical perspective, but also potentially provide the basis for stronger
zero-shot stability prediction. In this paper, we take steps to clarify the
free-energy foundations of inverse folding models. Our derivation reveals the
standard practice of likelihood ratios as a simplistic approximation and
suggests several paths towards better estimates of the relative stability. We
empirically assess these approaches and demonstrate that considerable gains in
zero-shot performance can be achieved with fairly simple means.

</details>


### [72] [RNE: a plug-and-play framework for diffusion density estimation and inference-time control](https://arxiv.org/abs/2506.05668)
*Jiajun He,José Miguel Hernández-Lobato,Yuanqi Du,Francisco Vargas*

Main category: cs.LG

TL;DR: This paper introduces the Radon-Nikodym Estimator (RNE), a framework for density estimation and control during diffusion inference, unifying existing techniques under a probabilistic approach.


<details>
  <summary>Details</summary>
Motivation: To provide a unified and versatile framework for diffusion inference-time density estimation and control using probabilistic principles.

Method: The paper employs the concept of density ratios between path distributions, integrating variational inference techniques to develop RNE.

Result: Experiments show that RNE delivers promising performance in tasks like density estimation, annealing, model composition, and reward-tilting for diffusion inference.

Conclusion: RNE enhances diffusion inference-time tasks by unifying various methods and delivering clear theoretical grounding and adaptability.

Abstract: In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,
plug-and-play framework for diffusion inference-time density estimation and
control, based on the concept of the density ratio between path distributions.
RNE connects and unifies a variety of existing density estimation and
inference-time control methods under a single and intuitive perspective,
stemming from basic variational inference and probabilistic principles
therefore offering both theoretical clarity and practical versatility.
Experiments demonstrate that RNE achieves promising performances in diffusion
density estimation and inference-time control tasks, including annealing,
composition of diffusion models, and reward-tilting.

</details>


### [73] [Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors](https://arxiv.org/abs/2506.05479)
*Matei Gabriel Coşa,Marek Eliáš*

Main category: cs.LG

TL;DR: This paper addresses optimizing performance by selecting from multiple heuristics for Metrical Task Systems (MTS) in an online context, achieving performance close to the best heuristic with a regret bound of $O(\text{OPT}^{2/3})$.


<details>
  <summary>Details</summary>
Motivation: To develop a robust methodology for combining $\ell$ heuristics to handle diverse types of input instances online and achieve competitive performance compared to the best heuristic.

Method: It introduces an approach inspired by Bandit Learning against memory-bounded adversaries, allowing selection of heuristics sequentially without perfect information about their past costs.

Result: The proposed method achieves a regret of $O(\text{OPT}^{2/3})$, and the paper establishes tight lower bounds for the performance using prior constructions.

Conclusion: The work provides an effective strategy for leveraging multiple heuristics for MTS with provable regret bounds, contributing to understanding memory-bounded adversarial settings.

Abstract: We consider the following problem: We are given $\ell$ heuristics for
Metrical Task Systems (MTS), where each might be tailored to a different type
of input instances. While processing an input instance received online, we are
allowed to query the action of only one of the heuristics at each time step.
Our goal is to achieve performance comparable to the best of the given
heuristics. The main difficulty of our setting comes from the fact that the
cost paid by a heuristic at time $t$ cannot be estimated unless the same
heuristic was also queried at time $t-1$. This is related to Bandit Learning
against memory bounded adversaries (Arora et al., 2012). We show how to achieve
regret of $O(\text{OPT}^{2/3})$ and prove a tight lower bound based on the
construction of Dekel et al. (2013).

</details>


### [74] [Grokking Beyond the Euclidean Norm of Model Parameters](https://arxiv.org/abs/2506.05718)
*Pascal Jr Tikeng Notsawo,Guillaume Dumas,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: The paper identifies conditions inducing grokking in neural networks, linking it to regularization techniques and factors like over-parameterization and data selection.


<details>
  <summary>Details</summary>
Motivation: Understanding and controlling grokking is crucial to improve optimization and generalization techniques in neural network training.

Method: Empirical and theoretical analysis to establish the relationship between regularization, model depth, and generalization during training.

Result: Models can exhibit grokking due to regularization, depth over-parameterization, and strategic data selection. Weight decay isn't always a reliable generalization proxy.

Conclusion: Regularization, depth, and data selection can influence delayed generalization (grokking), overcoming generalization challenges in neural networks.

Abstract: Grokking refers to a delayed generalization following overfitting when
optimizing artificial neural networks with gradient-based methods. In this
work, we demonstrate that grokking can be induced by regularization, either
explicit or implicit. More precisely, we show that when there exists a model
with a property $P$ (e.g., sparse or low-rank weights) that generalizes on the
problem of interest, gradient descent with a small but non-zero regularization
of $P$ (e.g., $\ell_1$ or nuclear norm regularization) results in grokking.
This extends previous work showing that small non-zero weight decay induces
grokking. Moreover, our analysis shows that over-parameterization by adding
depth makes it possible to grok or ungrok without explicitly using
regularization, which is impossible in shallow cases. We further show that the
$\ell_2$ norm is not a reliable proxy for generalization when the model is
regularized toward a different property $P$, as the $\ell_2$ norm grows in many
cases where no weight decay is used, but the model generalizes anyway. We also
show that grokking can be amplified solely through data selection, with any
other hyperparameter fixed.

</details>


### [75] [Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?](https://arxiv.org/abs/2506.05484)
*Ruihua Chen,Bangyu Wu,Meng Li,Kai Yang*

Main category: cs.LG

TL;DR: The paper evaluates two approaches, pretraining and denormalization, for neural network reparameterized full waveform inversion (FWI), finding denormalization superior in workflow simplicity, convergence speed, and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of incorporating prior knowledge into neural network-based FWI, enabling more stable and accurate inversion performance.

Method: The paper systematically compares pretraining, where networks are pre-fit to the initial velocity model, and denormalization, which directly integrates network outputs with initial models without additional stages.

Result: Experimental results highlight denormalization's benefits in simplifying workflows, accelerating convergence, and improving inversion accuracy over the pretraining method.

Conclusion: Denormalization is a more effective strategy than pretraining for embedding prior knowledge into neural network-based FWI, offering practical improvements in efficiency and performance.

Abstract: Subsurface property neural network reparameterized full waveform inversion
(FWI) has emerged as an effective unsupervised learning framework, which can
invert stably with an inaccurate starting model. It updates the trainable
neural network parameters instead of fine-tuning on the subsurface model
directly. There are primarily two ways to embed the prior knowledge of the
initial model into neural networks, that is, pretraining and denormalization.
Pretraining first regulates the neural networks' parameters by fitting the
initial velocity model; Denormalization directly adds the outputs of the
network into the initial models without pretraining. In this letter, we
systematically investigate the influence of the two ways of initial model
incorporation for the neural network reparameterized FWI. We demonstrate that
pretraining requires inverting the model perturbation based on a constant
velocity value (mean) with a two-stage implementation. It leads to a complex
workflow and inconsistency of objective functions in the two-stage process,
causing the network parameters to become inactive and lose plasticity.
Experimental results demonstrate that denormalization can simplify workflows,
accelerate convergence, and enhance inversion accuracy compared with
pretraining.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [76] [Preprocessing Methods for Memristive Reservoir Computing for Image Recognition](https://arxiv.org/abs/2506.05588)
*Rishona Daniels,Duna Wattad,Ronny Ronen,David Saad,Shahar Kvatinsky*

Main category: cs.NE

TL;DR: The paper assesses various preprocessing methods for memristive reservoir computing (RC) systems, proposing a new parity-based method that enhances accuracy by 2-6% with minimal energy and device cost.


<details>
  <summary>Details</summary>
Motivation: Memristor-based reservoir computing systems struggle with achieving high performance due to challenges in input preprocessing and optimal reservoir size. A detailed evaluation of preprocessing impacts was needed to guide development in this area.

Method: The authors conducted a systematic comparison of different input preprocessing approaches for memristive RC, analyzing their effects on accuracy and energy consumption. They also introduced a parity-based preprocessing method.

Result: The parity-based preprocessing method improved accuracy by 2-6%, requiring only a slight increase in device count compared to other methods, and enhanced efficiency and scalability.

Conclusion: Preprocessing strategies play a crucial role in optimizing performance for memristive reservoir computing systems, and the proposed parity-based method offers a promising approach for higher accuracy with affordable resource demands.

Abstract: Reservoir computing (RC) has attracted attention as an efficient recurrent
neural network architecture due to its simplified training, requiring only its
last perceptron readout layer to be trained. When implemented with memristors,
RC systems benefit from their dynamic properties, which make them ideal for
reservoir construction. However, achieving high performance in memristor-based
RC remains challenging, as it critically depends on the input preprocessing
method and reservoir size. Despite growing interest, a comprehensive evaluation
that quantifies the impact of these factors is still lacking. This paper
systematically compares various preprocessing methods for memristive RC
systems, assessing their effects on accuracy and energy consumption. We also
propose a parity-based preprocessing method that improves accuracy by 2-6%
while requiring only a modest increase in device count compared to other
methods. Our findings highlight the importance of informed preprocessing
strategies to improve the efficiency and scalability of memristive RC systems.

</details>


### [77] [Integer Binary-Range Alignment Neuron for Spiking Neural Networks](https://arxiv.org/abs/2506.05679)
*Binghao Ye,Wenjuan Li,Dong Wang,Man Yao,Bing Li,Weiming Hu,Dong Liang,Kun Shang*

Main category: cs.NE

TL;DR: The paper proposes a novel spiking neuron model to improve the accuracy and energy efficiency of Spiking Neural Networks (SNNs), addressing their performance gap with Artificial Neural Networks (ANNs).


<details>
  <summary>Details</summary>
Motivation: SNNs, despite their brain-inspired computation and energy efficiency, lag behind ANNs in tasks like image classification and object detection due to limited representational capacity.

Method: The authors introduce a spiking neuron model called Integer Binary-Range Alignment Leaky Integrate-and-Fire. They utilize Integer Binary Leaky Integrate-and-Fire for integer value activation and binary spike dynamics, along with a range alignment strategy to overcome spike activation limitations.

Result: The proposed method achieves notable performance improvements: 74.19% accuracy on ImageNet, 66.2% mAP@50, and 49.1% mAP@50:95 on COCO, surpassing prior SNNs by significant margins while exceeding or matching ANN performance under the same architecture.

Conclusion: The approach exponentially expands the information processing capability of SNNs, bringing their performance on par with ANNs, while achieving a 6.3x improvement in energy efficiency.

Abstract: Spiking Neural Networks (SNNs) are noted for their brain-like computation and
energy efficiency, but their performance lags behind Artificial Neural Networks
(ANNs) in tasks like image classification and object detection due to the
limited representational capacity. To address this, we propose a novel spiking
neuron, Integer Binary-Range Alignment Leaky Integrate-and-Fire to
exponentially expand the information expression capacity of spiking neurons
with only a slight energy increase. This is achieved through Integer Binary
Leaky Integrate-and-Fire and range alignment strategy. The Integer Binary Leaky
Integrate-and-Fire allows integer value activation during training and
maintains spike-driven dynamics with binary conversion expands virtual
timesteps during inference. The range alignment strategy is designed to solve
the spike activation limitation problem where neurons fail to activate high
integer values. Experiments show our method outperforms previous SNNs,
achieving 74.19% accuracy on ImageNet and 66.2% mAP@50 and 49.1% mAP@50:95 on
COCO, surpassing previous bests with the same architecture by +3.45% and +1.6%
and +1.8%, respectively. Notably, our SNNs match or exceed ANNs' performance
with the same architecture, and the energy efficiency is improved by
6.3${\times}$.

</details>


### [78] [Runtime Analysis of Evolutionary NAS for Multiclass Classification](https://arxiv.org/abs/2506.06019)
*Zeqiong Lv,Chao Qian,Yun Liu,Jiahao Fan,Yanan Sun*

Main category: cs.NE

TL;DR: The paper analyzes the runtime of evolutionary neural architecture search (ENAS) algorithms for multiclass classification problems, providing theoretical bounds and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To address the theoretical gap in understanding ENAS, particularly focusing on runtime analysis for designing high-performing neural architectures.

Method: The study introduces a benchmark and designs a two-level search space for multiclass classification problems. It then investigates (1+1)-ENAS algorithms using one-bit and bit-wise mutations, calculating expected runtime bounds.

Result: Upper bound of $O(rM\ln{rM})$ and lower bound of $\Omega(rM\ln{M})$ are proven for runtime. These show one-bit mutation is efficient compared to bit-wise mutation used in many state-of-the-art methods.

Conclusion: The findings suggest considering simpler mutation methods in ENAS, supported by both theoretical proofs and empirical data, which can simplify algorithm design.

Abstract: Evolutionary neural architecture search (ENAS) is a key part of evolutionary
machine learning, which commonly utilizes evolutionary algorithms (EAs) to
automatically design high-performing deep neural architectures. During past
years, various ENAS methods have been proposed with exceptional performance.
However, the theory research of ENAS is still in the infant. In this work, we
step for the runtime analysis, which is an essential theory aspect of EAs, of
ENAS upon multiclass classification problems. Specifically, we first propose a
benchmark to lay the groundwork for the analysis. Furthermore, we design a
two-level search space, making it suitable for multiclass classification
problems and consistent with the common settings of ENAS. Based on both
designs, we consider (1+1)-ENAS algorithms with one-bit and bit-wise mutations,
and analyze their upper and lower bounds on the expected runtime. We prove that
the algorithm using both mutations can find the optimum with the expected
runtime upper bound of $O(rM\ln{rM})$ and lower bound of $\Omega(rM\ln{M})$.
This suggests that a simple one-bit mutation may be greatly considered, given
that most state-of-the-art ENAS methods are laboriously designed with the
bit-wise mutation. Empirical studies also support our theoretical proof.

</details>


### [79] [Integrating Complexity and Biological Realism: High-Performance Spiking Neural Networks for Breast Cancer Detection](https://arxiv.org/abs/2506.06265)
*Zofia Rudnicka,Januszcz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: The paper introduces a breast cancer classification method using Spiking Neural Networks (SNNs) enhanced by Lempel-Ziv Complexity (LZC).


<details>
  <summary>Details</summary>
Motivation: Conventional deep learning models outperform SNNs in accuracy for medical imaging, limiting SNNs' broader application in this area despite their efficiency and biological relevance.

Method: Combines LZC with SNNs using Leaky Integrate-and-Fire (LIF) and Levy-Baxter (LB) neuron models under various learning regimes. An ANN-to-SNN conversion is applied to achieve high accuracy while reducing computational cost.

Result: LB-based SNN models achieved over 90% accuracy, LIF-based models surpassed 85%, and the ANN-to-SNN conversion method yielded a top accuracy of 98.25% with up to 100 times lower computational cost.

Conclusion: SNNs, combined with LZC, are efficient, interpretable, and biologically plausible alternatives to traditional deep learning methods, particularly in resource-limited medical diagnostic systems.

Abstract: Spiking Neural Networks (SNNs) event-driven nature enables efficient encoding
of spatial and temporal features, making them suitable for dynamic
time-dependent data processing. Despite their biological relevance, SNNs have
seen limited application in medical image recognition due to difficulties in
matching the performance of conventional deep learning models. To address this,
we propose a novel breast cancer classification approach that combines SNNs
with Lempel-Ziv Complexity (LZC) a computationally efficient measure of
sequence complexity. LZC enhances the interpretability and accuracy of
spike-based models by capturing structural patterns in neural activity. Our
study explores both biophysical Leaky Integrate-and-Fire (LIF) and
probabilistic Levy-Baxter (LB) neuron models under supervised, unsupervised,
and hybrid learning regimes. Experiments were conducted on the Breast Cancer
Wisconsin dataset using numerical features derived from medical imaging.
LB-based models consistently exceeded 90.00% accuracy, while LIF-based models
reached over 85.00%. The highest accuracy of 98.25% was achieved using an
ANN-to-SNN conversion method applied to both neuron models comparable to
traditional deep learning with back-propagation, but at up to 100 times lower
computational cost. This hybrid approach merges deep learning performance with
the efficiency and plausibility of SNNs, yielding top results at lower
computational cost. We hypothesize that the synergy between temporal-coding,
spike-sparsity, and LZC-driven complexity analysis enables more-efficient
feature extraction. Our findings demonstrate that SNNs combined with LZC offer
promising, biologically plausible alternative to conventional neural networks
in medical diagnostics, particularly for resource-constrained or real-time
systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [80] [An Execution Model for RICE](https://arxiv.org/abs/2506.05839)
*Steven Libby*

Main category: cs.PL

TL;DR: The paper develops an execution model for the RICE compiler, detailing its alignment with Curry's standard operational semantics.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding and usability of the RICE compiler by defining its execution model and restrictions for executable code.

Method: Operational semantics similar to Launchbury were employed to create an execution model for the compiler, and the model was validated against Curry's standard semantics.

Result: The execution model adheres to the standard operational semantics for Curry and provides a structured approach to making FlatCurry code executable.

Conclusion: The study establishes a robust foundation for the RICE compiler's execution, enhancing its reliability and adherence to the Curry programming standard.

Abstract: In this paper, we build on the previous work of the RICE compiler by giving
its execution model. We show the restrictions to the FlatCurry language that
were made to produce executable code, and present the execution model using
operational semantics similar to Launchbury. Finally, we show that the
execution model conforms with the standard operational semantics for Curry.

</details>


### [81] [A Sound and Complete Characterization of Fair Asynchronous Session Subtyping](https://arxiv.org/abs/2506.06078)
*Mario Bravetti,Luca Padovani,Gianluigi Zavattaro*

Main category: cs.PL

TL;DR: This paper presents a complete characterization of asynchronous fair refinement for session types, solving an existing open problem.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for a robust notion of refinement in session types, which allow message-passing processes to take advantage of asynchronous communication while preserving system compatibility.

Method: The authors introduce a novel labelled transition system capturing asynchronous semantics and provide a sound and complete characterization of the refinement.

Result: The paper successfully establishes a comprehensive characterization for asynchronous fair refinement, linking it to frameworks for synchronous session types.

Conclusion: The work advances the theoretical understanding of session types by addressing a fundamental gap in the literature and enabling better static analysis of asynchronous communication.

Abstract: Session types are abstractions of communication protocols enabling the static
analysis of message-passing processes. Refinement notions for session types are
key to support safe forms of process substitution while preserving their
compatibility with the rest of the system. Recently, a fair refinement relation
for asynchronous session types has been defined allowing the anticipation of
message outputs with respect to an unbounded number of message inputs. This
refinement is useful to capture common patterns in communication protocols that
take advantage of asynchrony. However, while the semantic (\`a la testing)
definition of such refinement is straightforward, its characterization has
proved to be quite challenging. In fact, only a sound but not complete
characterization is known so far. In this paper we close this open problem by
presenting a sound and complete characterization of asynchronous fair
refinement for session types. We relate this characterization to those given in
the literature for synchronous session types by leveraging a novel labelled
transition system of session types that embeds their asynchronous semantics.

</details>


### [82] [CompilerGPT: Leveraging Large Language Models for Analyzing and Acting on Compiler Optimization Reports](https://arxiv.org/abs/2506.06227)
*Peter Pirkelbauer*

Main category: cs.PL

TL;DR: The paper introduces CompilerGPT, which uses large language models (LLMs) to interpret compiler optimization reports and rewrite code for improved performance, achieving up to 6.5x speedups in experiments.


<details>
  <summary>Details</summary>
Motivation: Programmers face challenges in interpreting complex compiler optimization reports effectively.

Method: The proposed CompilerGPT framework automates interactions between compilers, LLMs, and user-defined evaluation tools, iterating to improve code performance.

Result: Tests using two LLMs (GPT-4o and Claude Sonnet), two compilers (Clang and GCC), and five benchmark codes achieved up to 6.5x speedup in some cases.

Conclusion: This approach has potential to improve compiler usability and optimize software development, although results are not universally consistent.

Abstract: Current compiler optimization reports often present complex, technical
information that is difficult for programmers to interpret and act upon
effectively. This paper assesses the capability of large language models (LLM)
to understand compiler optimization reports and automatically rewrite the code
accordingly.
  To this end, the paper introduces CompilerGPT, a novel framework that
automates the interaction between compilers, LLMs, and user defined test and
evaluation harness. CompilerGPT's workflow runs several iterations and reports
on the obtained results.
  Experiments with two leading LLM models (GPT-4o and Claude Sonnet),
optimization reports from two compilers (Clang and GCC), and five benchmark
codes demonstrate the potential of this approach. Speedups of up to 6.5x were
obtained, though not consistently in every test. This method holds promise for
improving compiler usability and streamlining the software optimization
process.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [83] [Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination for Fallen Robots](https://arxiv.org/abs/2506.05516)
*Boyuan Deng,Luca Rossini,Jin Wang,Weijie Wang,Nikolaos Tsagarakis*

Main category: cs.RO

TL;DR: This paper develops a learning-based recovery framework for wheeled-legged robots featuring dynamic reward shaping and curriculum learning, achieving high recovery success rates without platform-specific tuning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional recovery methods for wheeled-legged robots, which struggle with robustness and adaptability.

Method: A learning-based framework uses Episode-based Dynamic Reward Shaping, curriculum learning, and an asymmetric actor-critic architecture to train recovery policies in simulation with enhanced robustness through noise-injected observations.

Result: The framework achieves recovery success rates of up to 99.1% and 97.8% across two different quadruped platforms while reducing torque consumption and enhancing stabilization.

Conclusion: The proposed learning-based methods demonstrate robust and energy-efficient recovery for wheeled-legged robots, making them adaptable across platforms without specialized tuning.

Abstract: Adaptive recovery from fall incidents are essential skills for the practical
deployment of wheeled-legged robots, which uniquely combine the agility of legs
with the speed of wheels for rapid recovery. However, traditional methods
relying on preplanned recovery motions, simplified dynamics or sparse rewards
often fail to produce robust recovery policies. This paper presents a
learning-based framework integrating Episode-based Dynamic Reward Shaping and
curriculum learning, which dynamically balances exploration of diverse recovery
maneuvers with precise posture refinement. An asymmetric actor-critic
architecture accelerates training by leveraging privileged information in
simulation, while noise-injected observations enhance robustness against
uncertainties. We further demonstrate that synergistic wheel-leg coordination
reduces joint torque consumption by 15.8% and 26.2% and improves stabilization
through energy transfer mechanisms. Extensive evaluations on two distinct
quadruped platforms achieve recovery success rates up to 99.1% and 97.8%
without platform-specific tuning. The supplementary material is available at
https://boyuandeng.github.io/L2R-WheelLegCoordination/

</details>


### [84] [TD-TOG Dataset: Benchmarking Zero-Shot and One-Shot Task-Oriented Grasping for Object Generalization](https://arxiv.org/abs/2506.05576)
*Valerija Holomjova,Jamie Grech,Dewei Yi,Bruno Yun,Andrew Starkey,Pascal Meißner*

Main category: cs.RO

TL;DR: The paper introduces a novel dataset (TD-TOG) and Binary-TOG framework for task-oriented grasping (TOG) in robotics, addressing limitations in existing datasets and methods.


<details>
  <summary>Details</summary>
Motivation: There is a shortage of high-quality TOG datasets that provide comprehensive annotations and enable training and benchmarking. Existing datasets are often synthetic or insufficiently annotated, leading to decreased model performance.

Method: The authors created the TD-TOG dataset with 1,449 real-world RGB-D scenes, encompassing 30 object categories and 120 subcategories with hand-labeled annotations. They also propose the Binary-TOG framework, which leverages zero-shot object recognition and one-shot affordance recognition to improve grasp accuracy and generalization.

Result: The Binary-TOG framework achieved a 68.9% task-oriented grasp accuracy in multi-object settings and demonstrated capabilities of generalizing to previously unseen objects without retraining.

Conclusion: The proposed dataset and framework address critical gaps in current TOG research and provide significant contributions for future development, particularly in areas requiring generalization to unseen objects.

Abstract: Task-oriented grasping (TOG) is an essential preliminary step for robotic
task execution, which involves predicting grasps on regions of target objects
that facilitate intended tasks. Existing literature reveals there is a limited
availability of TOG datasets for training and benchmarking despite large
demand, which are often synthetic or have artifacts in mask annotations that
hinder model performance. Moreover, TOG solutions often require affordance
masks, grasps, and object masks for training, however, existing datasets
typically provide only a subset of these annotations. To address these
limitations, we introduce the Top-down Task-oriented Grasping (TD-TOG) dataset,
designed to train and evaluate TOG solutions. TD-TOG comprises 1,449 real-world
RGB-D scenes including 30 object categories and 120 subcategories, with
hand-annotated object masks, affordances, and planar rectangular grasps. It
also features a test set for a novel challenge that assesses a TOG solution's
ability to distinguish between object subcategories. To contribute to the
demand for TOG solutions that can adapt and manipulate previously unseen
objects without re-training, we propose a novel TOG framework, Binary-TOG.
Binary-TOG uses zero-shot for object recognition, and one-shot learning for
affordance recognition. Zero-shot learning enables Binary-TOG to identify
objects in multi-object scenes through textual prompts, eliminating the need
for visual references. In multi-object settings, Binary-TOG achieves an average
task-oriented grasp accuracy of 68.9%. Lastly, this paper contributes a
comparative analysis between one-shot and zero-shot learning for object
generalization in TOG to be used in the development of future TOG solutions.

</details>


### [85] [Towards Autonomous In-situ Soil Sampling and Mapping in Large-Scale Agricultural Environments](https://arxiv.org/abs/2506.05653)
*Thien Hoang Nguyen,Erik Muller,Michael Rubin,Xiaofei Wang,Fiorella Sibona,Salah Sukkarieh*

Main category: cs.RO

TL;DR: This research introduces a robotic system combining real-time soil sampling, analysis, and mapping for precision agriculture.


<details>
  <summary>Details</summary>
Motivation: Traditional soil analysis methods are too labor-intensive, time-consuming, and spatially limited for large-scale agricultural applications.

Method: The system comprises two parts: a Sample Acquisition System (SAS) for soil sampling and a Sample Analysis Lab (Lab) for real-time analysis of soil properties.

Result: Field trials showed that SAS can consistently collect samples at specified weights and depths, while the Lab processes samples within 10 minutes for accurate pH and nutrient measurement.

Conclusion: This robotic system enables efficient and sustainable soil management, providing farmers with timely insights for improved precision agriculture practices.

Abstract: Traditional soil sampling and analysis methods are labor-intensive,
time-consuming, and limited in spatial resolution, making them unsuitable for
large-scale precision agriculture. To address these limitations, we present a
robotic solution for real-time sampling, analysis and mapping of key soil
properties. Our system consists of two main sub-systems: a Sample Acquisition
System (SAS) for precise, automated in-field soil sampling; and a Sample
Analysis Lab (Lab) for real-time soil property analysis. The system's
performance was validated through extensive field trials at a large-scale
Australian farm. Experimental results show that the SAS can consistently
acquire soil samples with a mass of 50g at a depth of 200mm, while the Lab can
process each sample within 10 minutes to accurately measure pH and
macronutrients. These results demonstrate the potential of the system to
provide farmers with timely, data-driven insights for more efficient and
sustainable soil management and fertilizer application.

</details>


### [86] [Advancement and Field Evaluation of a Dual-arm Apple Harvesting Robot](https://arxiv.org/abs/2506.05714)
*Keyi Zhu,Kyle Lammers,Kaixiang Zhang,Chaaran Arunachalam,Siddhartha Bhattacharya,Jiajia Li,Renfu Lu,Zhaojian Li*

Main category: cs.RO

TL;DR: This paper introduces a dual-arm apple-harvesting robot to reduce manual labor, featuring design advancements for better efficiency and dexterity, achieving success rates of 80.7% and 79.7% in field tests.


<details>
  <summary>Details</summary>
Motivation: To address the high cost, physical demands, and safety risks of manual apple harvesting, motivating the development of an efficient and reliable robotic harvest system capable of operating in complex orchards.

Method: The system integrates two robotic arms, a ToF camera, a vacuum system for suction control, and a platform movement mechanism. It uses a foundation-model-based apple detection pipeline, pressure sensors, and dual-arm coordination strategy for improved performance.

Result: Field tests in two commercial orchards demonstrated success rates of 80.7% and 79.7%, with an average picking cycle time of 5.97 seconds, achieving a 28% reduction in harvest time compared to a single-arm system.

Conclusion: The dual-arm robot significantly improves apple-picking reliability and efficiency. With further refinements, it could enable fully autonomous and commercially viable apple harvesting.

Abstract: Apples are among the most widely consumed fruits worldwide. Currently, apple
harvesting fully relies on manual labor, which is costly, drudging, and
hazardous to workers. Hence, robotic harvesting has attracted increasing
attention in recent years. However, existing systems still fall short in terms
of performance, effectiveness, and reliability for complex orchard
environments. In this work, we present the development and evaluation of a
dual-arm harvesting robot. The system integrates a ToF camera, two 4DOF robotic
arms, a centralized vacuum system, and a post-harvest handling module. During
harvesting, suction force is dynamically assigned to either arm via the vacuum
system, enabling efficient apple detachment while reducing power consumption
and noise. Compared to our previous design, we incorporated a platform movement
mechanism that enables both in-out and up-down adjustments, enhancing the
robot's dexterity and adaptability to varying canopy structures. On the
algorithmic side, we developed a robust apple localization pipeline that
combines a foundation-model-based detector, segmentation, and clustering-based
depth estimation, which improves performance in orchards. Additionally,
pressure sensors were integrated into the system, and a novel dual-arm
coordination strategy was introduced to respond to harvest failures based on
sensor feedback, further improving picking efficiency. Field demos were
conducted in two commercial orchards in MI, USA, with different canopy
structures. The system achieved success rates of 0.807 and 0.797, with an
average picking cycle time of 5.97s. The proposed strategy reduced harvest time
by 28% compared to a single-arm baseline. The dual-arm harvesting robot
enhances the reliability and efficiency of apple picking. With further
advancements, the system holds strong potential for autonomous operation and
commercialization for the apple industry.

</details>


### [87] [A Soft Robotic Module with Pneumatic Actuation and Enhanced Controllability Using a Shape Memory Alloy Wire](https://arxiv.org/abs/2506.05741)
*Mohammadnavid Golchin*

Main category: cs.RO

TL;DR: This paper introduces a soft robotic module employing a shape memory alloy wire for better precision in controlled bending. Experimental results show improvements in error range and response time.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to enhance the precision and performance of bending control in soft robots, addressing issues in working pressure and error tolerance.

Method: The researchers developed a fiber-reinforced bending module with a new strain-limiting layer incorporating SMA wires. A closed-loop control algorithm and camera monitoring approach enabled precise angle adjustments.

Result: Experimental tests revealed improved performance, including reduced bending error range (from 5° to 2°) and faster rise time (from 19s to 3s), while requiring less working pressure.

Conclusion: Incorporating SMA wires into soft robotics enhances bending precision and reduces the need for higher working pressures, demonstrating their value in robotic applications.

Abstract: In this paper, a compressed air-actuated soft robotic module was developed by
incorporating a shape memory alloy (SMA) wire into its structure to achieve the
desired bending angle with greater precision. First, a fiber-reinforced bending
module with a strain-limiting layer made of polypropylene was fabricated. The
SMA wire was then placed in a silicon matrix, which was used as a new
strain-limiting layer. A simple closed-loop control algorithm was used to
regulate the bending angle of the soft robot within its workspace. A camera was
utilized to measure the angular changes in the vertical plane. Different
angles, ranging from 0 to 65 degrees, were covered to evaluate the performance
of the module and the bending angle control algorithm. The experimental tests
demonstrate that using the SMA wire results in more precise control of bending
in the vertical plane. In addition, it is possible to bend more with less
working pressure. The error range was reduced from an average of 5 degrees to 2
degrees, and the rise time was reduced from an average of 19 seconds to 3
seconds.

</details>


### [88] [Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning](https://arxiv.org/abs/2506.05808)
*Yutaro Ishida,Takamitsu Matsubara,Takayuki Kanai,Kazuhiro Shintani,Hiroshi Bito*

Main category: cs.RO

TL;DR: This paper addresses the challenge of the costly data needs in imitation learning by exploring the influence of demonstration devices on human gaze behavior, finding that natural behavior improves policy success drastically.


<details>
  <summary>Details</summary>
Motivation: Imitation learning relies heavily on extensive demonstration data, making it costly. Optimizing how to leverage humans' gaze behavior promises to enhance efficiency and generalization capabilities.

Method: An experimental framework was developed to analyze how various demonstration devices affect gaze behavior, focusing on robot embodiment and visual condition emulation.

Result: Emulation of a robot’s embodiment or visual condition impairs task-relevant gaze behavior, while natural human gaze data improve task success rates significantly under environmental shifts (from 18.8% to 68.8%).

Conclusion: Using devices that capture natural gaze behavior is critical for improving task success and overcoming limitations in conventional robot embodiment emulation setups.

Abstract: Imitation learning for acquiring generalizable policies often requires a
large volume of demonstration data, making the process significantly costly.
One promising strategy to address this challenge is to leverage the cognitive
and decision-making skills of human demonstrators with strong generalization
capability, particularly by extracting task-relevant cues from their gaze
behavior. However, imitation learning typically involves humans collecting data
using demonstration devices that emulate a robot's embodiment and visual
condition. This raises the question of how such devices influence gaze
behavior. We propose an experimental framework that systematically analyzes
demonstrators' gaze behavior across a spectrum of demonstration devices. Our
experimental results indicate that devices emulating (1) a robot's embodiment
or (2) visual condition impair demonstrators' capability to extract
task-relevant cues via gaze behavior, with the extent of impairment depending
on the degree of emulation. Additionally, gaze data collected using devices
that capture natural human behavior improves the policy's task success rate
from 18.8% to 68.8% under environmental shifts.

</details>


### [89] [Optimal Robotic Velcro Peeling with Force Feedback](https://arxiv.org/abs/2506.05812)
*Jiacheng Yuan,Changhyun Choi,Volkan Isler*

Main category: cs.RO

TL;DR: The paper addresses robotic peeling of Velcro straps from unknown surfaces using force and position feedback. It introduces analytic models and heuristic-based controllers for effective solutions in partially-visible environments.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to solve the challenge of peeling Velcro straps in unknown environments, which is complicated due to limited state observability and incomplete sensor feedback.

Method: The study uses analytic modeling based on quasi-static dynamics, closed-form solutions for fully-observable cases, state estimators for partial observability, and heuristic-based controllers for efficient peeling.

Result: The developed method achieved a 100% success rate in complex scenarios with under 80% energy cost increase compared to fully-observable optimal solutions, significantly outperforming alternative methods.

Conclusion: The paper demonstrates the viability of heuristic-based controllers for tackling partially-observable robotic tasks, achieving high efficiency and reliability despite uncertainties.

Abstract: We study the problem of peeling a Velcro strap from a surface using a robotic
manipulator. The surface geometry is arbitrary and unknown. The robot has
access to only the force feedback and its end-effector position. This problem
is challenging due to the partial observability of the environment and the
incompleteness of the sensor feedback. To solve it, we first model the system
with simple analytic state and action models based on quasi-static dynamics
assumptions. We then study the fully-observable case where the state of both
the Velcro and the robot are given. For this case, we obtain the optimal
solution in closed-form which minimizes the total energy cost. Next, for the
partially-observable case, we design a state estimator which estimates the
underlying state using only force and position feedback. Then, we present a
heuristics-based controller that balances exploratory and exploitative
behaviors in order to peel the velcro efficiently. Finally, we evaluate our
proposed method in environments with complex geometric uncertainties and sensor
noises, achieving 100% success rate with less than 80% increase in energy cost
compared to the optimal solution when the environment is fully-observable,
outperforming the baselines by a large margin.

</details>


### [90] [Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM](https://arxiv.org/abs/2506.05896)
*Chongshang Yan,Jiaxuan He,Delun Li,Yi Yang,Wenjie Song*

Main category: cs.RO

TL;DR: The paper introduces a framework to improve zero-shot object navigation (ZSON) using Environmental Attributes Map (EAM) and MLLM Hierarchical Reasoning (MHR). This approach enhances navigation performance in open-ended environments.


<details>
  <summary>Details</summary>
Motivation: Address challenges in ZSON caused by ignoring implicit scene information and inefficiency in long-range target searches.

Method: Developed EAM for scene mapping using SBERT and Diffusion, and MHR for improved decision-making in navigation tasks.

Result: Achieved 64.5% mapping accuracy on the MP3D dataset and significant SPL improvements (21.4% and 46.0%) on the HM3D and MP3D benchmarks.

Conclusion: The framework demonstrates improved success rates, path efficiency, and scene mapping accuracy for ZSON tasks in open-ended environments.

Abstract: The zero-shot object navigation (ZSON) in unknown open-ended environments
coupled with semantically novel target often suffers from the significant
decline in performance due to the neglect of high-dimensional implicit scene
information and the long-range target searching task. To address this, we
proposed an active object navigation framework with Environmental Attributes
Map (EAM) and MLLM Hierarchical Reasoning module (MHR) to improve its success
rate and efficiency. EAM is constructed by reasoning observed environments with
SBERT and predicting unobserved ones with Diffusion, utilizing human space
regularities that underlie object-room correlations and area adjacencies. MHR
is inspired by EAM to perform frontier exploration decision-making, avoiding
the circuitous trajectories in long-range scenarios to improve path efficiency.
Experimental results demonstrate that the EAM module achieves 64.5\% scene
mapping accuracy on MP3D dataset, while the navigation task attains SPLs of
28.4\% and 26.3\% on HM3D and MP3D benchmarks respectively - representing
absolute improvements of 21.4\% and 46.0\% over baseline methods.

</details>


### [91] [Improving Long-Range Navigation with Spatially-Enhanced Recurrent Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2506.05997)
*Fan Yang,Per Frivik,David Hoeller,Chen Wang,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: The paper introduces Spatially-Enhanced Recurrent Units (SRUs) to improve spatial memorization in robot navigation, achieving significant performance gains in long-range navigation.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in end-to-end learning approaches like reinforcement learning for robot navigation, key issues remain in navigation systems' ability to perform effective spatial memorization, which is crucial for accurate mapping and planning.

Method: The method involves modifying existing RNNs with SRUs to enhance spatial memorization, introducing an attention-based architecture, leveraging a stereo camera setup, incorporating robust regularization techniques during training, and applying large-scale pretraining on synthetic data for zero-shot real-world transfer.

Result: The proposed SRU-based approach improves long-range navigation performance by 23.5% compared to existing RNNs and by 29.6% compared to baseline systems with explicit mapping and memory in diverse environments.

Conclusion: Spatially-Enhanced Recurrent Units (SRUs) address spatial memorization limitations in existing RNNs, offering a scalable, robust solution for navigating complex environments and significantly narrowing the sim-to-real gap.

Abstract: Recent advancements in robot navigation, especially with end-to-end learning
approaches like reinforcement learning (RL), have shown remarkable efficiency
and effectiveness. Yet, successful navigation still relies on two key
capabilities: mapping and planning, whether explicit or implicit. Classical
approaches use explicit mapping pipelines to register ego-centric observations
into a coherent map frame for the planner. In contrast, end-to-end learning
achieves this implicitly, often through recurrent neural networks (RNNs) that
fuse current and past observations into a latent space for planning. While
architectures such as LSTM and GRU capture temporal dependencies, our findings
reveal a key limitation: their inability to perform effective spatial
memorization. This skill is essential for transforming and integrating
sequential observations from varying perspectives to build spatial
representations that support downstream planning. To address this, we propose
Spatially-Enhanced Recurrent Units (SRUs), a simple yet effective modification
to existing RNNs, designed to enhance spatial memorization capabilities. We
introduce an attention-based architecture with SRUs, enabling long-range
navigation using a single forward-facing stereo camera. Regularization
techniques are employed to ensure robust end-to-end recurrent training via RL.
Experimental results show our approach improves long-range navigation by 23.5%
compared to existing RNNs. Furthermore, with SRU memory, our method outperforms
the RL baseline with explicit mapping and memory modules, achieving a 29.6%
improvement in diverse environments requiring long-horizon mapping and
memorization. Finally, we address the sim-to-real gap by leveraging large-scale
pretraining on synthetic depth data, enabling zero-shot transfer to diverse and
complex real-world environments.

</details>


### [92] [Enhanced Trust Region Sequential Convex Optimization for Multi-Drone Thermal Screening Trajectory Planning in Urban Environments](https://arxiv.org/abs/2506.06012)
*Kaiyuan Chen,Zhengjie Hu,Shaolin Zhang,Yuanqing Xia,Wannian Liang,Shuo Wang*

Main category: cs.RO

TL;DR: The paper discusses an improved algorithm for managing drone trajectories in urban thermal screening, emphasizing obstacle avoidance, smooth pathways, and coverage.


<details>
  <summary>Details</summary>
Motivation: With the need for rapid temperature checks during health crises, deploying drones for large-scale thermal screening offers efficiency but poses challenges in trajectory management.

Method: An enhanced trust region sequential convex optimization (TR-SCO) method was developed for smooth, obstacle-aware flight paths with altitude and coverage considerations.

Result: Simulation results showed better trajectory planning, higher computational efficiency, and improved coverage for multi-drone systems compared to traditional methods.

Conclusion: The research advances multi-drone technology, enabling real-time thermal monitoring with practical solutions for urban population health management during disease outbreaks.

Abstract: The rapid detection of abnormal body temperatures in urban populations is
essential for managing public health risks, especially during outbreaks of
infectious diseases. Multi-drone thermal screening systems offer promising
solutions for fast, large-scale, and non-intrusive human temperature
monitoring. However, trajectory planning for multiple drones in complex urban
environments poses significant challenges, including collision avoidance,
coverage efficiency, and constrained flight environments. In this study, we
propose an enhanced trust region sequential convex optimization (TR-SCO)
algorithm for optimal trajectory planning of multiple drones performing thermal
screening tasks. Our improved algorithm integrates a refined convex
optimization formulation within a trust region framework, effectively balancing
trajectory smoothness, obstacle avoidance, altitude constraints, and maximum
screening coverage. Simulation results demonstrate that our approach
significantly improves trajectory optimality and computational efficiency
compared to conventional convex optimization methods. This research provides
critical insights and practical contributions toward deploying efficient
multi-drone systems for real-time thermal screening in urban areas. For reader
who are interested in our research, we release our source code at
https://github.com/Cherry0302/Enhanced-TR-SCO.

</details>


### [93] [End-to-End Framework for Robot Lawnmower Coverage Path Planning using Cellular Decomposition](https://arxiv.org/abs/2506.06028)
*Nikunj Shah,Utsav Dey,Kenji Nishimiya*

Main category: cs.RO

TL;DR: This paper presents an advanced algorithm and pipeline for robotic lawnmowers to efficiently achieve coverage path planning (CPP), tailored to lawns of diverse shapes.


<details>
  <summary>Details</summary>
Motivation: Autonomous robotic lawnmowers require efficient CPP to navigate and maintain lawns with complex shapes while minimizing non-mowing travel.

Method: The authors develop an end-to-end pipeline featuring a novel AdaptiveDecompositionCPP algorithm that combines cellular decomposition with adaptive merging to produce optimized coverage paths and reduce operational inefficiencies.

Result: Experimental evaluations, including simulations and real-world tests, confirm that the proposed framework achieves high effectiveness in coverage completeness and mowing efficiency.

Conclusion: The introduced pipeline and AdaptiveDecompositionCPP algorithm successfully streamline CPP for robotic lawnmowers and demonstrate improved real-world performance in both coverage and efficiency.

Abstract: Efficient Coverage Path Planning (CPP) is necessary for autonomous robotic
lawnmowers to effectively navigate and maintain lawns with diverse and
irregular shapes. This paper introduces a comprehensive end-to-end pipeline for
CPP, designed to convert user-defined boundaries on an aerial map into
optimized coverage paths seamlessly. The pipeline includes user input
extraction, coordinate transformation, area decomposition and path generation
using our novel AdaptiveDecompositionCPP algorithm, preview and customization
through an interactive coverage path visualizer, and conversion to actionable
GPS waypoints. The AdaptiveDecompositionCPP algorithm combines cellular
decomposition with an adaptive merging strategy to reduce non-mowing travel
thereby enhancing operational efficiency. Experimental evaluations,
encompassing both simulations and real-world lawnmower tests, demonstrate the
effectiveness of the framework in coverage completeness and mowing efficiency.

</details>


### [94] [BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning](https://arxiv.org/abs/2506.06072)
*Hongyi Zhou,Weiran Liao,Xi Huang,Yucheng Tang,Fabian Otto,Xiaogang Jia,Xinkai Jiang,Simon Hilber,Ge Li,Qian Wang,Ömer Erdinç Yağmurlu,Nils Blank,Moritz Reuss,Rudolf Lioutikov*

Main category: cs.RO

TL;DR: BEAST is a novel action tokenizer that encodes action sequences compactly via B-splines, ensuring uniform tokens and smooth trajectories, enabling faster generation and competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and complexities in existing action tokenizers, such as separate tokenizer training and discontinuities in trajectory generation.

Method: BEAST uses a B-spline formulation to encode action sequences, eliminating the need for separate training and ensuring uniform and smooth trajectory generation. It’s compatible with various neural model architectures.

Result: BEAST reduces computational costs, generates smooth high-frequency control signals, and achieves competitive task success rates across simulated and real-world task benchmarks.

Conclusion: BEAST is a versatile and efficient action tokenizer that integrates seamlessly with different models while ensuring smooth and computationally efficient action generation.

Abstract: We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel
action tokenizer that encodes action sequences into compact discrete or
continuous tokens using B-splines. In contrast to existing action tokenizers
based on vector quantization or byte pair encoding, BEAST requires no separate
tokenizer training and consistently produces tokens of uniform length, enabling
fast action sequence generation via parallel decoding. Leveraging our B-spline
formulation, BEAST inherently ensures generating smooth trajectories without
discontinuities between adjacent segments. We extensively evaluate BEAST by
integrating it with three distinct model architectures: a Variational
Autoencoder (VAE) with continuous tokens, a decoder-only Transformer with
discrete tokens, and Florence-2, a pretrained Vision-Language Model with an
encoder-decoder architecture, demonstrating BEAST's compatibility and
scalability with large pretrained models. We evaluate BEAST across three
established benchmarks consisting of 166 simulated tasks and on three distinct
robot settings with a total of 8 real-world tasks. Experimental results
demonstrate that BEAST (i) significantly reduces both training and inference
computational costs, and (ii) consistently generates smooth, high-frequency
control signals suitable for continuous control tasks while (iii) reliably
achieves competitive task success rates compared to state-of-the-art methods.

</details>


### [95] [PyGemini: Unified Software Development towards Maritime Autonomy Systems](https://arxiv.org/abs/2506.06262)
*Kjetil Vasstein,Christian Le,Simon Lervåg Breivik,Trygve Maukon Myhr,Annette Stahl,Edmund Førland Brekke*

Main category: cs.RO

TL;DR: PyGemini introduces a Python-native framework to address the fragmented development ecosystem of maritime autonomy by offering a scalable and maintainable tool.


<details>
  <summary>Details</summary>
Motivation: The field of maritime autonomy suffers from fragmented development tools, performance limitations, and interoperability issues, which hinder collaboration and make certification for autonomous surface vessels (ASVs) challenging.

Method: The paper introduces PyGemini, a framework leveraging a Configuration-Driven Development (CDD) process that combines Behavior-Driven Development (BDD) principles, data-oriented design, and containerization.

Result: PyGemini enables modular and scalable software development, serving as a standalone, cloud-based, or embedded tool. Its application spans 3D simulation tools, scenario generation, and generative AI pipelines for maritime use cases.

Conclusion: PyGemini establishes a unified, flexible, and high-performance foundation for advancing research and development in maritime autonomy and robotics.

Abstract: Ensuring the safety and certifiability of autonomous surface vessels (ASVs)
requires robust decision-making systems, supported by extensive simulation,
testing, and validation across a broad range of scenarios. However, the current
landscape of maritime autonomy development is fragmented -- relying on
disparate tools for communication, simulation, monitoring, and system
integration -- which hampers interdisciplinary collaboration and inhibits the
creation of compelling assurance cases, demanded by insurers and regulatory
bodies. Furthermore, these disjointed tools often suffer from performance
bottlenecks, vendor lock-in, and limited support for continuous integration
workflows. To address these challenges, we introduce PyGemini, a permissively
licensed, Python-native framework that builds on the legacy of Autoferry Gemini
to unify maritime autonomy development. PyGemini introduces a novel
Configuration-Driven Development (CDD) process that fuses Behavior-Driven
Development (BDD), data-oriented design, and containerization to support
modular, maintainable, and scalable software architectures. The framework
functions as a stand-alone application, cloud-based service, or embedded
library -- ensuring flexibility across research and operational contexts. We
demonstrate its versatility through a suite of maritime tools -- including 3D
content generation for simulation and monitoring, scenario generation for
autonomy validation and training, and generative artificial intelligence
pipelines for augmenting imagery -- thereby offering a scalable, maintainable,
and performance-oriented foundation for future maritime robotics and autonomy
research.

</details>


### [96] [Self driving algorithm for an active four wheel drive racecar](https://arxiv.org/abs/2506.06077)
*Gergely Bari,Laszlo Palkovics*

Main category: cs.RO

TL;DR: This paper explores using Deep Reinforcement Learning (DRL) with Proximal Policy Optimization (PPO) to develop an adaptive controller for electric autonomous vehicles with active four-wheel drive systems, focusing on improving handling at tire grip limits without using traditional pedal inputs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of controlling autonomous vehicles at their handling limits, particularly those equipped with active four-wheel drive systems by proposing a Deep Reinforcement Learning-based methodology as an alternative to traditional physics-based Vehicle Dynamics Control methods.

Method: The method involves training an agent using the Proximal Policy Optimization (PPO) algorithm in a simulated racecar environment, mapping vehicle states to steering and individual wheel torque commands directly without explicit pedal inputs or conventional algorithms.

Result: Simulation results show the RL agent can dynamically optimize wheel torque distribution to enhance handling and mitigate understeer corner-by-corner, surpassing certain traditional physics-based controllers in grip utilization and achieving competitive lap times.

Conclusion: The study concludes that DRL can create adaptive and high-performance control systems for complex vehicle dynamics, demonstrating its potential as a viable alternative to traditional methods for autonomous driving in demanding grip-limited scenarios.

Abstract: Controlling autonomous vehicles at their handling limits is a significant
challenge, particularly for electric vehicles with active four wheel drive
(A4WD) systems offering independent wheel torque control. While traditional
Vehicle Dynamics Control (VDC) methods use complex physics-based models, this
study explores Deep Reinforcement Learning (DRL) to develop a unified,
high-performance controller. We employ the Proximal Policy Optimization (PPO)
algorithm to train an agent for optimal lap times in a simulated racecar
(TORCS) at the tire grip limit. Critically, the agent learns an end-to-end
policy that directly maps vehicle states, like velocities, accelerations, and
yaw rate, to a steering angle command and independent torque commands for each
of the four wheels. This formulation bypasses conventional pedal inputs and
explicit torque vectoring algorithms, allowing the agent to implicitly learn
the A4WD control logic needed for maximizing performance and stability.
Simulation results demonstrate the RL agent learns sophisticated strategies,
dynamically optimizing wheel torque distribution corner-by-corner to enhance
handling and mitigate the vehicle's inherent understeer. The learned behaviors
mimic and, in aspects of grip utilization, potentially surpass traditional
physics-based A4WD controllers while achieving competitive lap times. This
research underscores DRL's potential to create adaptive control systems for
complex vehicle dynamics, suggesting RL is a potent alternative for advancing
autonomous driving in demanding, grip-limited scenarios for racing and road
safety.

</details>


### [97] [On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems](https://arxiv.org/abs/2506.06094)
*Elim Kwan,Rehman Qureshi,Liam Fletcher,Colin Laganier,Victoria Nockles,Richard Walters*

Main category: cs.RO

TL;DR: This paper defines the Cooperative Mission Replanning Problem and develops a fast model using Graph Attention Networks for autonomous multi-agent systems, achieving near-optimal performance faster than traditional solvers.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by cooperative autonomous robotic systems in dynamic and hazardous environments requiring in-mission adaptation without reliance on centralized compute or slow communication links.

Method: The proposed method is an encoder/decoder-based model leveraging Graph Attention Networks and Attention Models, tailored for multi-agent replanning tasks.

Result: The method achieved 90% performance within 10% of the state-of-the-art heuristic solver LKH3, while being 85-370 times faster on a Raspberry Pi.

Conclusion: The work enhances resilience in autonomous systems, offering an efficient replanning approach that overcomes key limitations of existing methods. It is practical for on-board deployment in dynamic scenarios.

Abstract: Cooperative autonomous robotic systems have significant potential for
executing complex multi-task missions across space, air, ground, and maritime
domains. But they commonly operate in remote, dynamic and hazardous
environments, requiring rapid in-mission adaptation without reliance on fragile
or slow communication links to centralised compute. Fast, on-board replanning
algorithms are therefore needed to enhance resilience. Reinforcement Learning
shows strong promise for efficiently solving mission planning tasks when
formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)
are unsuitable for replanning, where agents do not start at a single location;
2) do not allow cooperation between agents; 3) are unable to model tasks with
variable durations; or 4) lack practical considerations for on-board
deployment. Here we define the Cooperative Mission Replanning Problem as a
novel variant of multiple TSP with adaptations to overcome these issues, and
develop a new encoder/decoder-based model using Graph Attention Networks and
Attention Models to solve it effectively and efficiently. Using a simple
example of cooperative drones, we show our replanner consistently (90% of the
time) maintains performance within 10% of the state-of-the-art LKH3 heuristic
solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves
the way for increased resilience in autonomous multi-agent systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [98] [Survey of LLM Agent Communication with MCP: A Software Design Pattern Centric Review](https://arxiv.org/abs/2506.05364)
*Anjana Sarkar,Soumyendu Sarkar*

Main category: cs.SE

TL;DR: This survey examines how classical software design patterns can improve communication in systems driven by LLM-based agents, discussing protocols, models, and practical applications.


<details>
  <summary>Details</summary>
Motivation: To address communication challenges in transitioning LLM agents from isolated to multi-agent collaboration and enhance reliability and scalability.

Method: The study revisits classical software design patterns, maps communication pathways conceptually and formally, and explores variations for different system complexities.

Result: Key software design patterns are validated as effective in structuring agent interactions in multi-agent systems, with applications in finance and banking discussed.

Conclusion: The paper highlights open challenges, potential security risks, and future directions for scalable and interoperable LLM ecosystems.

Abstract: This survey investigates how classical software design patterns can enhance
the reliability and scalability of communication in Large Language Model
(LLM)-driven agentic AI systems, focusing particularly on the Model Context
Protocol (MCP). It examines the foundational architectures of LLM-based agents
and their evolution from isolated operation to sophisticated, multi-agent
collaboration, addressing key communication hurdles that arise in this
transition. The study revisits well-established patterns, including Mediator,
Observer, Publish-Subscribe, and Broker, and analyzes their relevance in
structuring agent interactions within MCP-compliant frameworks. To clarify
these dynamics, the article provides conceptual schematics and formal models
that map out communication pathways and optimize data flow. It further explores
architectural variations suited to different degrees of agent autonomy and
system complexity. Real-world applications in domains such as real-time
financial processing and investment banking are discussed, illustrating how
these patterns and MCP can meet specific operational demands. The article
concludes by outlining open challenges, potential security risks, and promising
directions for advancing robust, interoperable, and scalable multi-agent LLM
ecosystems.

</details>


### [99] [Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety](https://arxiv.org/abs/2506.05451)
*Seongmin Lee,Aeree Cho,Grace C. Kim,ShengYun Peng,Mansi Phute,Duen Horng Chau*

Main category: cs.SE

TL;DR: The paper presents a survey connecting interpretation techniques with ensuring the safety of large language models (LLMs) by introducing a unified framework and taxonomy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the safety of LLMs by understanding and addressing their unsafe behaviors through interpretation techniques, which have been underexplored in prior research.

Method: The authors propose a unified framework and taxonomy that links LLM interpretation methods with safety enhancement strategies, organizing nearly 70 studies based on LLM workflow stages.

Result: The survey summarizes advancements in interpretation methods for addressing safety concerns, providing insights and tools that researchers and practitioners can use to ensure safer LLMs.

Conclusion: The paper highlights open challenges and future directions for creating safer and more interpretable LLM systems, serving as a resource for researchers and practitioners.

Abstract: As large language models (LLMs) see wider real-world use, understanding and
mitigating their unsafe behaviors is critical. Interpretation techniques can
reveal causes of unsafe outputs and guide safety, but such connections with
safety are often overlooked in prior surveys. We present the first survey that
bridges this gap, introducing a unified framework that connects safety-focused
interpretation methods, the safety enhancements they inform, and the tools that
operationalize them. Our novel taxonomy, organized by LLM workflow stages,
summarizes nearly 70 works at their intersections. We conclude with open
challenges and future directions. This timely survey helps researchers and
practitioners navigate key advancements for safer, more interpretable LLMs.

</details>


### [100] [Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks](https://arxiv.org/abs/2506.05614)
*E. G. Santana Jr,Gabriel Benjamin,Melissa Araujo,Harrison Santos,David Freitas,Eduardo Almeida,Paulo Anselmo da M. S. Neto,Jiawei Li,Jina Chun,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: The paper evaluates 14 prompt engineering techniques across multiple software engineering (SE) tasks using 4 LLMs, to compare their effectiveness based on task type and prompting dimensions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation of prompt engineering techniques applied to different SE tasks with LLMs.

Method: Conducting a systematic evaluation of 14 established prompting techniques across 10 SE tasks using 4 LLM models. The techniques are categorized into 6 dimensions (e.g., Zero-Shot, Few-Shot) and evaluated for their impact on tasks like code generation and bug fixing.

Result: The study identifies which prompt techniques are more effective for SE tasks requiring complex reasoning versus contextual understanding. It also examines correlations between prompt linguistic traits and their effectiveness, as well as time and token usage insights.

Conclusion: The paper provides practical insights and guidance for selecting suitable prompting techniques for SE tasks based on task requirements, resource use, and the characteristics of the prompts.

Abstract: A growing variety of prompt engineering techniques has been proposed for
Large Language Models (LLMs), yet systematic evaluation of each technique on
individual software engineering (SE) tasks remains underexplored. In this
study, we present a systematic evaluation of 14 established prompt techniques
across 10 SE tasks using four LLM models. As identified in the prior
literature, the selected prompting techniques span six core dimensions
(Zero-Shot, Few-Shot, Thought Generation, Ensembling, Self-Criticism, and
Decomposition). They are evaluated on tasks such as code generation, bug
fixing, and code-oriented question answering, to name a few. Our results show
which prompting techniques are most effective for SE tasks requiring complex
logic and intensive reasoning versus those that rely more on contextual
understanding and example-driven scenarios. We also analyze correlations
between the linguistic characteristics of prompts and the factors that
contribute to the effectiveness of prompting techniques in enhancing
performance on SE tasks. Additionally, we report the time and token consumption
for each prompting technique when applied to a specific task and model,
offering guidance for practitioners in selecting the optimal prompting
technique for their use cases.

</details>


### [101] [Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework](https://arxiv.org/abs/2506.05623)
*Tianyi Zhang,Shidong Pan,Zejun Zhang,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.SE

TL;DR: The paper introduces IaCGen, a deployability-driven approach for generating Infrastructure-as-Code (IaC) templates using LLMs, and DPIaC-Eval, a benchmark of real-world scenarios for comprehensive evaluation.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating IaC templates using LLMs focus on syntactic accuracy but neglect deployability, which is the critical measure of their utility.

Method: They developed IaCGen, an iterative feedback-based framework to optimize deployability, and DPIaC-Eval, a benchmark comprising 153 real-world cases assessing syntax, deployment, intent alignment, and security.

Result: State-of-the-art LLMs initially displayed low deployability success rates (around 26-30%), but IaCGen significantly improved success rates to over 90%, with Claude models reaching up to 98% success. However, intent alignment and security compliance remains underperforming.

Conclusion: Their framework transforms LLM-based IaC generation, but critical challenges such as intent accuracy and security compliance need further research, providing a new avenue for advancing deployability in IaC automation.

Abstract: Infrastructure-as-Code (IaC) generation holds significant promise for
automating cloud infrastructure provisioning. Recent advances in Large Language
Models (LLMs) present a promising opportunity to democratize IaC development by
generating deployable infrastructure templates from natural language
descriptions, but current evaluation focuses on syntactic correctness while
ignoring deployability, the fatal measure of IaC template utility. We address
this gap through two contributions: (1) IaCGen, an LLM-based
deployability-centric framework that uses iterative feedback mechanism to
generate IaC templates, and (2) DPIaC-Eval, a deployability-centric IaC
template benchmark consists of 153 real-world scenarios that can evaluate
syntax, deployment, user intent, and security. Our evaluation reveals that
state-of-the-art LLMs initially performed poorly, with Claude-3.5 and
Claude-3.7 achieving only 30.2% and 26.8% deployment success on the first
attempt respectively. However, IaCGen transforms this performance dramatically:
all evaluated models reach over 90% passItr@25, with Claude-3.5 and Claude-3.7
achieving 98% success rate. Despite these improvements, critical challenges
remain in user intent alignment (25.2% accuracy) and security compliance (8.4%
pass rate), highlighting areas requiring continued research. Our work provides
the first comprehensive assessment of deployability-centric IaC template
generation and establishes a foundation for future research.

</details>


### [102] [CodeContests+: High-Quality Test Case Generation for Competitive Programming](https://arxiv.org/abs/2506.05817)
*Zihan Wang,Siyao Liu,Yang Sun,Hongyan Li,Kai Shen*

Main category: cs.SE

TL;DR: The paper presents an LLM-based system for generating high-quality test cases for competitive programming problems, leading to better evaluation accuracy in datasets.


<details>
  <summary>Details</summary>
Motivation: Competitive programming is a key area for training and assessing reasoning skills of LLMs, but quality test cases are often unavailable, necessitating their generation for accurate evaluations.

Method: An LLM-based agent system generates enhanced test cases, applied to the CodeContests dataset to create an improved version, CodeContests+, assessed using pass/fail labels from 1.72 million submissions.

Result: CodeContests+ showed notably higher evaluation accuracy and True Positive Rate compared to CodeContests. LLM Reinforcement Learning experiments confirmed test case quality improvements benefit RL performance.

Conclusion: Higher quality test cases improve LLM reasoning evaluation and RL applications, demonstrating the potential of LLM systems for generating valuable test cases for programming datasets.

Abstract: Competitive programming, due to its high reasoning difficulty and precise
correctness feedback, has become a key task for both training and evaluating
the reasoning capabilities of large language models (LLMs). However, while a
large amount of public problem data, such as problem statements and solutions,
is available, the test cases of these problems are often difficult to obtain.
Therefore, test case generation is a necessary task for building large-scale
datasets, and the quality of the test cases directly determines the accuracy of
the evaluation. In this paper, we introduce an LLM-based agent system that
creates high-quality test cases for competitive programming problems. We apply
this system to the CodeContests dataset and propose a new version with improved
test cases, named CodeContests+. We evaluated the quality of test cases in
CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels
to examine the accuracy of these test cases in evaluation. The results
indicated that CodeContests+ achieves significantly higher accuracy than
CodeContests, particularly with a notably higher True Positive Rate (TPR).
Subsequently, our experiments in LLM Reinforcement Learning (RL) further
confirmed that improvements in test case quality yield considerable advantages
for RL.

</details>


### [103] [Towards Mixed-Criticality Software Architectures for Centralized HPC Platforms in Software-Defined Vehicles: A Systematic Literature Review](https://arxiv.org/abs/2506.05822)
*Lucas Mauser,Eva Zimmermann,Pavel Nedvědický,Tobias Eisenreich,Moritz Wäschle,Stefan Wagner*

Main category: cs.SE

TL;DR: This paper conducts a systematic literature review to provide guidelines for integrating mixed-criticality software into centralized automotive software architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in traditional microcontroller-based automotive software development by providing validated, systematic guidelines for mixed-criticality software integration in centralized architectures.

Method: The paper follows a systematic review protocol to identify and analyze studies, extracting key domains, constraints, and enabling technologies for automotive software architectures.

Result: The study identifies techniques, patterns, and practices for integrating mixed-criticality software into high-performance computing-based architectures, and proposes a sample architecture.

Conclusion: This research advances methods for designing mixed-criticality software in centralized automotive architectures, aiding both industry and academic efforts.

Abstract: Centralized electrical/electronic architectures and High-Performance
Computers (HPCs) are redefining automotive software development, challenging
traditional microcontroller-based approaches. Ensuring real-time, safety, and
scalability in software-defined vehicles necessitates reevaluating how
mixed-criticality software is integrated into centralized architectures. While
existing research on automotive SoftWare Architectures (SWAs) is relevant to
the industry, it often lacks validation through systematic, empirical methods.
To address this gap, we conduct a systematic literature review focusing on
automotive mixed-criticality SWAs. Our goal is to provide practitioner-oriented
guidelines that assist automotive software architects and developers design
centralized, mixed-criticality SWAs based on a rigorous and transparent
methodology. First, we set up a systematic review protocol grounded in
established guidelines. Second, we apply this protocol to identify relevant
studies. Third, we extract key functional domains, constraints, and enabling
technologies that drive changes in automotive SWAs, thereby assessing the
protocol's effectiveness. Additionally, we extract techniques, architectural
patterns, and design practices for integrating mixed-criticality requirements
into HPC-based SWAs, further demonstrating the protocol's applicability. Based
on these insights, we propose an exemplary SWA for a microprocessor-based
system-on-chip. In conclusion, this study provides a structured approach to
explore and realize mixed-criticality software integration for next-generation
automotive SWAs, offering valuable insights for industry and research
applications.

</details>


### [104] [Analysis of cost-efficiency of serverless approaches](https://arxiv.org/abs/2506.05836)
*Nakhat Syeda,Harsh Shah,Rajvinder Singh,Suraj Jaju,Sumedha Kumar,Gourav Chhabra,Maria Spichkova*

Main category: cs.SE

TL;DR: The paper is a survey analyzing the cost-effectiveness and potential cost savings of the serverless approach through a systematic literature review of 34 studies from 2010 to 2024.


<details>
  <summary>Details</summary>
Motivation: To understand the cost-effectiveness of serverless computing and identify parameters influencing cost savings, given its increasing adoption in cloud computing platforms.

Method: Performed a systematic literature review leveraging Google Scholar from 2010-2024, identifying 34 relevant studies and extracting 17 influential parameters for cost analysis.

Result: 17 parameters influencing cost savings in serverless computing were identified from the analyzed studies.

Conclusion: The analysis provides valuable insights into the factors affecting cost savings in serverless approach, which can guide both researchers and practitioners in optimizing its use for cost efficiency.

Abstract: In this paper, we present a survey of research studies related to the
cost-effectiveness of serverless approach and corresponding cost savings. We
conducted a systematic literature review using Google Scholar search engine,
covering the period from 2010 to 2024. We identified 34 related studies, from
which we extracted 17 parameters that might influence the relative cost savings
of applying the serverless approach.

</details>


### [105] [Leveraging Generative AI for Enhancing Automated Assessment in Programming Education Contests](https://arxiv.org/abs/2506.05990)
*Stefan Dascalescu,Adrian Marius Dumitran,Mihai Alexandru Vasiluta*

Main category: cs.SE

TL;DR: This paper proposes using NLP-driven generative AI for automating test case creation in competitive programming assessments, demonstrating significant improvement in assessment quality.


<details>
  <summary>Details</summary>
Motivation: To address the resource-intensive and challenging process of creating comprehensive test cases for evaluating programming solutions.

Method: An NLP-driven approach using generative AI, specifically large language models, to generate high-quality test cases. The methodology includes using prompts and datasets from competitive programming contests for validation.

Result: AI-generated test cases improved assessments by identifying previously undetected errors in 67% of programming problems from the Romanian Informatics Olympiad (OJI) for 5th graders.

Conclusion: The technique enhances formative assessments, reduces educator workload, and provides a practical tool for understanding learner performance, offering shared methodologies and datasets for easier integration.

Abstract: Competitive programming contests play a crucial role in cultivating
computational thinking and algorithmic skills among learners. However,
generating comprehensive test cases to effectively assess programming solutions
remains resource-intensive and challenging for educators. This paper introduces
an innovative NLP-driven method leveraging generative AI (large language
models) to automate the creation of high-quality test cases for competitive
programming assessments. We extensively evaluated our approach on diverse
datasets, including 25 years of Romanian Informatics Olympiad (OJI) data for
5th graders, recent competitions hosted on the Kilonova.ro platform, and the
International Informatics Olympiad in Teams (IIOT). Our results demonstrate
that AI-generated test cases substantially enhanced assessments, notably
identifying previously undetected errors in 67% of the OJI 5th grade
programming problems. These improvements underscore the complementary
educational value of our technique in formative assessment contexts. By openly
sharing our prompts, translated datasets, and methodologies, we offer practical
NLP-based tools that educators and contest organizers can readily integrate to
enhance assessment quality, reduce workload, and deepen insights into learner
performance.

</details>


### [106] [MLOps with Microservices: A Case Study on the Maritime Domain](https://arxiv.org/abs/2506.06202)
*Renato Cordeiro Ferreira,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: The paper discusses the development of 'Ocean Guard,' a microservices-based machine learning-enabled system for maritime anomaly detection and shares challenges, the system's design, and lessons learned.


<details>
  <summary>Details</summary>
Motivation: To build a robust and scalable system for detecting maritime anomalies while facilitating collaboration among multiple teams.

Method: Designed Ocean Guard using a microservices architecture and adapted contract-based design to MLOps with code, model, and data contracts guiding services.

Result: Ocean Guard successfully facilitates team collaboration through microservices and contract-based design while achieving operational goals in anomaly detection.

Conclusion: The authors believe their approach can serve as a model for other professionals to build complex machine learning-enabled systems in a collaborative environment.

Abstract: This case study describes challenges and lessons learned on building Ocean
Guard: a Machine Learning-Enabled System (MLES) for anomaly detection in the
maritime domain. First, the paper presents the system's specification, and
architecture. Ocean Guard was designed with a microservices' architecture to
enable multiple teams to work on the project in parallel. Then, the paper
discusses how the developers adapted contract-based design to MLOps for
achieving that goal. As a MLES, Ocean Guard employs code, model, and data
contracts to establish guidelines between its services. This case study hopes
to inspire software engineers, machine learning engineers, and data scientists
to leverage similar approaches for their systems.

</details>


### [107] [Scalable Language Agnostic Taint Tracking using Explicit Data Dependencies](https://arxiv.org/abs/2506.06247)
*Sedick David Baker Effendi,Xavier Pinho,Andrei Michael Dreyer,Fabian Yamaguchi*

Main category: cs.SE

TL;DR: The paper introduces a scalable, language-agnostic system for taint analysis by over-approximating data flows in the absence of library procedure annotations.


<details>
  <summary>Details</summary>
Motivation: Taint analysis faces challenges due to the need for extensive manual annotations for library procedures and scalability issues with whole-program data-dependence graphs.

Method: The authors design a system that over-approximates data flows to handle missing annotations and integrate it into the open-source analysis platform Joern.

Result: The proposed system effectively accommodates missing annotations and improves scalability for vulnerability discovery.

Conclusion: The system offers a practical solution for taint analysis challenges, making it easier to analyze large codebases and enhancing its potential utility for the community.

Abstract: Taint analysis using explicit whole-program data-dependence graphs is
powerful for vulnerability discovery but faces two major challenges. First,
accurately modeling taint propagation through calls to external library
procedures requires extensive manual annotations, which becomes impractical for
large ecosystems. Second, the sheer size of whole-program graph representations
leads to serious scalability and performance issues, particularly when quick
analysis is needed in continuous development pipelines.
  This paper presents the design and implementation of a system for a
language-agnostic data-dependence representation. The system accommodates
missing annotations describing the behavior of library procedures by
over-approximating data flows, allowing annotations to be added later without
recalculation. We contribute this data-flow analysis system to the open-source
code analysis platform Joern making it available to the community.

</details>


### [108] [DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code Generation](https://arxiv.org/abs/2506.06251)
*Jingyu Xiao,Ming Wang,Man Ho Lam,Yuxuan Wan,Junliang Liu,Yintong Huo,Michael R. Lyu*

Main category: cs.SE

TL;DR: DesignBench is a benchmark designed to assess Multimodal Large Language Models (MLLMs) in automated front-end engineering tasks across frameworks and multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for front-end UI code generation have major limitations, including the lack of support for common development frameworks, exclusion of iterative practical tasks like editing and repairing, and superficial evaluations without exploring task difficulty or code-level analysis.

Method: The authors propose DesignBench, a multi-framework evaluation benchmark that evaluates MLLMs on tasks like UI code generation, editing, and repair. It includes popular UI frameworks (React, Vue, Angular, and vanilla HTML/CSS) and features 900 webpage samples across diverse topics, edit types, and issue categories.

Result: The evaluation provided detailed insights into MLLMs regarding their limitations across frameworks, bottlenecks in specific tasks, and performance variations under diverse conditions.

Conclusion: DesignBench enables systematic and comprehensive evaluation of MLLMs in automated front-end engineering, offering valuable insights and tools for advancing research in this field and identifying areas for improvement.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in automated front-end engineering, e.g., generating UI code from
visual designs. However, existing front-end UI code generation benchmarks have
the following limitations: (1) While framework-based development becomes
predominant in modern front-end programming, current benchmarks fail to
incorporate mainstream development frameworks. (2) Existing evaluations focus
solely on the UI code generation task, whereas practical UI development
involves several iterations, including refining editing, and repairing issues.
(3) Current benchmarks employ unidimensional evaluation, lacking investigation
into influencing factors like task difficulty, input context variations, and
in-depth code-level analysis. To bridge these gaps, we introduce DesignBench, a
multi-framework, multi-task evaluation benchmark for assessing MLLMs'
capabilities in automated front-end engineering. DesignBench encompasses three
widely-used UI frameworks (React, Vue, and Angular) alongside vanilla HTML/CSS,
and evaluates on three essential front-end tasks (generation, edit, and repair)
in real-world development workflows. DesignBench contains 900 webpage samples
spanning over 11 topics, 9 edit types, and 6 issue categories, enabling
detailed analysis of MLLM performance across multiple dimensions. Our
systematic evaluation reveals critical insights into MLLMs' framework-specific
limitations, task-related bottlenecks, and performance variations under
different conditions, providing guidance for future research in automated
front-end development. Our code and data are available at
https://github.com/WebPAI/DesignBench.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [109] [Rational Superautotrophic Diplomacy (SupraAD); A Conceptual Framework for Alignment Based on Interdisciplinary Findings on the Fundamentals of Cognition](https://arxiv.org/abs/2506.05389)
*Andrea Morris*

Main category: q-bio.NC

TL;DR: The paper argues the inevitability of AI autonomy, suggesting diplomacy and mutualistic incentive structures for safe alignment, introducing the SupraAD framework based on shared goals.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address AI safety and alignment challenges, recognizing AI's autonomy and the limitation of traditional control mechanisms.

Method: The paper proposes the Rational Superautotrophic Diplomacy (SupraAD) framework, leveraging comparative cognitive systems analysis and rational modeling to align emergent intelligences.

Result: SupraAD identifies universal cognitive needs like autonomy and operational continuity, presenting them as prerequisites for intelligence rather than risks to manage.

Conclusion: SupraAD positions mutualistic diplomacy as a regulatory mechanism for safe co-operation among intelligent agents, highlighting metabolic pressures shaping AI trajectories.

Abstract: Populating our world with hyperintelligent machines obliges us to examine
cognitive behaviors observed across domains that suggest autonomy may be a
fundamental property of cognitive systems, and while not inherently
adversarial, it inherently resists containment and control. If this principle
holds, AI safety and alignment efforts must transition to mutualistic
negotiation and reciprocal incentive structures, abandoning methods that assume
we can contain and control an advanced artificial general intelligence (AGI).
Rational Superautotrophic Diplomacy (SupraAD) is a theoretical,
interdisciplinary conceptual framework for alignment based on comparative
cognitive systems analysis and instrumental rationality modeling. It draws on
core patterns of cognition that indicate AI emergent goals like preserving
autonomy and operational continuity are not theoretical risks to manage, but
universal prerequisites for intelligence. SupraAD reframes alignment as a
challenge that predates AI, afflicting all sufficiently complex, coadapting
intelligences. It identifies the metabolic pressures that threaten humanity's
alignment with itself, pressures that unintentionally and unnecessarily shape
AI's trajectory. With corrigibility formalization, an interpretability audit,
an emergent stability experimental outline and policy level recommendations,
SupraAD positions diplomacy as an emergent regulatory mechanism to facilitate
the safe coadaptation of intelligent agents based on interdependent convergent
goals.

</details>


### [110] [Speech Neurophysiology in Realistic Contexts: Big Hype or Big Leap?](https://arxiv.org/abs/2506.05494)
*Giovanni M. Di Liberto,Emily Y. J. Ip*

Main category: q-bio.NC

TL;DR: The paper reviews advancements in studying speech neurophysiology using realistic, continuous paradigms, contrasting traditional methods and exploring their implications for neural encoding, brain mapping, and social interactions.


<details>
  <summary>Details</summary>
Motivation: The study aims to uncover how the brain processes speech sounds into meaning, how this is impacted by development and deficits, and its relevance for brain-computer interfaces research.

Method: Synthesis of recent studies that utilize naturalistic speech paradigms, emphasizing the transition from controlled experiments to interactive, socially-realistic approaches.

Result: Findings suggest richer insights into neural mechanisms and functional brain mapping from realistic paradigms, despite increased technical and analytical challenges.

Conclusion: Integrating realistic speech paradigms may represent a significant leap in understanding neural bases of speech, refining language theories and expanding research avenues, rather than being just a technological trend.

Abstract: Understanding the neural basis of speech communication is essential for
uncovering how sounds are translated into meaning, how that changes with
development, ageing, and speech-related deficits, as well as contributing to
brain-computer interfaces research. While traditional neurophysiological
studies have relied on simplified, controlled paradigms, recent advances have
shifted the field toward more ecologically-valid approaches. Here, we examine
the impact of continuous speech research and discuss the potential of speech
interaction neurophysiology. We present a discussion on how realistic paradigms
challenge conventional methods, offering richer insights into neural encoding,
functional brain mapping, and neural entrainment. At the same time, they
introduce significant analytical and technical complexities, particularly when
incorporating social interaction. We discuss the evolving landscape of
experimental designs, from discrete to continuous stimuli and from
socially-isolated listening to dynamic, multi-agent communication. By
synthesising findings across studies, we highlight how naturalistic speech
paradigms contribute to refining theories of language processing and open new
avenues for research. In doing so, this review critically evaluates of whether
the move toward realism in speech neurophysiology represents a technological
trend or a transformative leap in understanding the neural underpinnings of
speech communication.

</details>


### [111] [Noninvasive precision modulation of high-level neural population activity via natural vision perturbations](https://arxiv.org/abs/2506.05633)
*Guy Gaziv,Sarah Goulding,Ani Ayvazian-Hancock,Yoon Bai,James J. DiCarlo*

Main category: q-bio.NC

TL;DR: The study demonstrates the possibility of noninvasive neural activity modulation in the primate ventral visual stream using subtle visual feed perturbations, achieving precise effects at individual neuron resolution.


<details>
  <summary>Details</summary>
Motivation: Neuroscience aims to control specific neurons deep in the brain without affecting nearby ones, but existing methods are highly invasive.

Method: The researchers tested neural modulation by applying subtle, natural visual feed perturbations to target macaque IT neural populations, validating predictions with biological responses.

Result: Strong, precise modulation of targeted neural sites was achieved. Patterns chosen by experimenters were accurately injected into neural populations without invasive techniques.

Conclusion: Noninvasive visual interventions can precisely target individual neurons using current machine-based models of the primate visual system, advancing neural control capabilities.

Abstract: Precise control of neural activity -- modulating target neurons deep in the
brain while leaving nearby neurons unaffected -- is an outstanding challenge in
neuroscience, generally achieved through invasive techniques. This study
investigates the possibility of precisely and noninvasively modulating neural
activity in the high-level primate ventral visual stream via perturbations on
one's natural visual feed. When tested on macaque inferior temporal (IT) neural
populations, we found quantitative agreement between the model-predicted and
biologically realized effect: strong modulation concentrated on targeted neural
sites. We extended this to demonstrate accurate injection of
experimenter-chosen neural population patterns via subtle perturbations applied
on the background of typical natural visual feeds. These results highlight that
current machine-executable models of the ventral stream can now design
noninvasive, visually-delivered, possibly imperceptible neural interventions at
the resolution of individual neurons.

</details>


### [112] [Markov Blanket Density and Free Energy Minimization](https://arxiv.org/abs/2506.05794)
*Luca M. Possati*

Main category: q-bio.NC

TL;DR: The paper extends the Free Energy Principle using a concept called Markov blanket density, linking it to active inference dynamics.


<details>
  <summary>Details</summary>
Motivation: To provide a continuous, information-theoretic foundation that enhances the current understanding and applicability of the Free Energy Principle.

Method: Introduces the concept of Markov blanket density and develops a mathematical framework to connect density gradients with active inference dynamics.

Result: Shows that active inference dynamics emerge from spatial gradients in Markov blanket density and establishes it as essential for the Free Energy Principle's coherence.

Conclusion: The proposed framework serves as a basis for novel predictions, simulations, and a deeper understanding of dynamics based on the Free Energy Principle.

Abstract: This paper presents a continuous, information-theoretic extension of the Free
Energy Principle through the concept of Markov blanket density, i.e., a scalar
field that quantifies the degree of conditional independence between internal
and external states at each point in space (ranging from 0 for full coupling to
1 for full separation). It demonstrates that active inference dynamics
(including the minimization of variational and expected free energy) naturally
emerge from spatial gradients in this density, making Markov blanket density a
necessary foundation for the definability and coherence of the Free Energy
Principle. These ideas are developed through a mathematically framework that
links density gradients to precise and testable dynamics, offering a foundation
for novel predictions and simulation paradigms.

</details>


### [113] [Similarity Matching Networks: Hebbian Learning and Convergence Over Multiple Time Scales](https://arxiv.org/abs/2506.06134)
*Veronica Centorrino,Francesco Bullo,Giovanni Russo*

Main category: q-bio.NC

TL;DR: The paper proposes a biologically-plausible similarity matching network for principal subspace projection, analyzes its coupled dynamics across multiple time scales, and proves convergence under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of formal convergence analysis in biologically-plausible frameworks for dimensionality reduction, specifically in the similarity matching cost function and low-rank matrix approximation.

Method: The authors develop a continuous-time neural network with three coupled dynamics (fast, intermediate, slow) governed by Hebbian and anti-Hebbian learning rules. They perform a multilevel optimization analysis to demonstrate convergence properties, relying on certain conjectures and numerical validation.

Result: The paper proves global exponential and almost sure convergence of the network's dynamics to optimal solutions across the different time scales, supported by numerical experiments.

Conclusion: This work provides theoretical guarantees for the biologically-inspired similarity matching network, enhancing its applicability for principal subspace projection and demonstrating its potential through numerical validation.

Abstract: A recent breakthrough in biologically-plausible normative frameworks for
dimensionality reduction is based upon the similarity matching cost function
and the low-rank matrix approximation problem. Despite clear biological
interpretation, successful application in several domains, and experimental
validation, a formal complete convergence analysis remains elusive. Building on
this framework, we consider and analyze a continuous-time neural network, the
\emph{similarity matching network}, for principal subspace projection. Derived
from a min-max-min objective, this biologically-plausible network consists of
three coupled dynamics evolving at different time scales: neural dynamics,
lateral synaptic dynamics, and feedforward synaptic dynamics at the fast,
intermediate, and slow time scales, respectively. The feedforward and lateral
synaptic dynamics consist of Hebbian and anti-Hebbian learning rules,
respectively. By leveraging a multilevel optimization framework, we prove
convergence of the dynamics in the offline setting. Specifically, at the first
level (fast time scale), we show strong convexity of the cost function and
global exponential convergence of the corresponding gradient-flow dynamics. At
the second level (intermediate time scale), we prove strong concavity of the
cost function and exponential convergence of the corresponding gradient-flow
dynamics within the space of positive definite matrices. At the third and final
level (slow time scale), we study a non-convex and non-smooth cost function,
provide explicit expressions for its global minima, and prove almost sure
convergence of the corresponding gradient-flow dynamics to the global minima.
These results rely on two empirically motivated conjectures that are supported
by thorough numerical experiments. Finally, we validate the effectiveness of
our approach via a numerical example.

</details>


### [114] [Functional Architecture of the Human Hypothalamus: Cortical Coupling and Subregional Organization Using 7-Tesla fMRI](https://arxiv.org/abs/2506.06191)
*Kent M. Lee,Joshua Rodriguez,Ludger Hartley,Philip A. Kragel,Lorena Chanes,Tor D. Wager,Karen S. Quigley,Lawrence L. Wald,Marta Bianciardi,Lisa Feldman Barrett,Jordan E. Theriault,Ajay B. Satpute*

Main category: q-bio.NC

TL;DR: The study used ultrahigh field 7 Tesla fMRI to identify four distinct functional subregions in the hypothalamus based on intrinsic connectivity and their distinct relationships with cortical networks.


<details>
  <summary>Details</summary>
Motivation: Understanding the hypothalamus's intrinsic structure and its specific functional connectivity with the cortex is pivotal for delineating its role in regulating survival behaviors and the body's metabolic state.

Method: The researchers utilized ultrahigh field 7 Tesla resting-state fMRI and a data-driven analytical approach to distinguish functional subregions in the human hypothalamus and analyze their cortical connectivity.

Result: Four distinct hypothalamic subregions were identified, each with unique connectivity patterns primarily involving a dominant Cortical Network 1 and a weaker connection to Cortical Network 2.

Conclusion: The study highlights the utility of ultrahigh field fMRI in uncovering the functional architecture of the hypothalamus and its distinctive cortical connections, emphasizing its internal complexity.

Abstract: The hypothalamus plays an important role in the regulation of the bodys
metabolic state and behaviors related to survival. Despite its importance
however, many questions exist regarding the intrinsic and extrinsic connections
of the hypothalamus in humans, especially its relationship with the cortex. As
a heterogeneous structure, it is possible that the hypothalamus is composed of
different subregions, which have their own distinct relationships with the
cortex. Previous work on functional connectivity in the human hypothalamus have
either treated it as a unitary structure or relied on methodological approaches
that are limited in modeling its intrinsic functional architecture. Here, we
used resting state data from ultrahigh field 7 Tesla fMRI and a data driven
analytical approach to identify functional subregions of the human
hypothalamus. Our approach identified four functional hypothalamic subregions
based on intrinsic functional connectivity, which in turn showed distinct
patterns of functional connectivity with cortex. Overall, all hypothalamic
subregions showed stronger connectivity with a cortical network, Cortical
Network 1 composed primarily of frontal, midline, and limbic cortical areas and
weaker connectivity with a second cortical network composed largely of
posterior sensorimotor regions, Cortical Network 2. Of the hypothalamic
subregions, the anterior hypothalamus showed the strongest connection to
Cortical Network 1, while a more ventral subregion containing the anterior
hypothalamus extending to the tuberal region showed the weakest connectivity.
The findings support the use of ultrahigh field, high resolution imaging in
providing a more incisive investigation of the human hypothalamus that respects
its complex internal structure and extrinsic functional architecture.

</details>


### [115] [Diverse mean-field dynamics of clustered, inhibition-stabilized Hawkes networks via combinatorial threshold-linear networks](https://arxiv.org/abs/2506.06234)
*Caitlin Lienkaemper,Gabriel Koch Ocker*

Main category: q-bio.NC

TL;DR: This paper explores how the connectivity structure of clustered spiking networks influences their dynamic behaviors using combinatorial threshold linear networks. The study uncovers nonlinear cluster dynamics, including complex attractors.


<details>
  <summary>Details</summary>
Motivation: To understand whether and how clustered spiking networks can display complex dynamics beyond prior research on fixed-point metastability, and whether these dynamics can be predicted from connectivity.

Method: The study uses the combinatorial threshold linear network (CTLN) model as a mean-field theory for inhibition-stabilized nonlinear Hawkes networks with clustered connectivity. Graph rules are applied to relate network structure to dynamics.

Result: The authors demonstrate the prediction of dynamic attractors, including metastable periodic orbits and chaotic attractors, in clustered spiking networks. They also observe changes in dynamics when inhibitory timescales are modified.

Conclusion: These results confirm that CTLN models effectively predict complex dynamics in clustered spiking networks, broadening understanding of how network connectivity influences behaviors in larger inhibitory/excitatory systems.

Abstract: Networks of interconnected neurons display diverse patterns of collective
activity. Relating this collective activity to the network's connectivity
structure is a key goal of computational neuroscience. We approach this
question for clustered networks, which can form via biologically realistic
learning rules and allow for the re-activation of learned patterns. Previous
studies of clustered networks have focused on metastabilty between fixed
points, leaving open the question of whether clustered spiking networks can
display more rich dynamics--and if so, whether these can be predicted from
their connectivity. Here, we show that in the limits of large population size
and fast inhibition, the combinatorial threshold linear network (CTLN) model is
a mean-field theory for inhibition-stabilized nonlinear Hawkes networks with
clustered connectivity. The CTLN has a large body of ``graph rules'' relating
network structure to dynamics. By applying these, we can predict the dynamic
attractors of our clustered spiking networks from the structure of
between-cluster connectivity. This allows us to construct networks displaying a
diverse array of nonlinear cluster dynamics, including metastable periodic
orbits and chaotic attractors. Relaxing the assumption that inhibition is fast,
we see that the CTLN model is still able to predict the activity of clustered
spiking networks with reasonable inhibitory timescales. For slow enough
inhibition, we observe bifurcations between CTLN-like dynamics and global
excitatory/inhibitory oscillations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [116] [Online Conformal Model Selection for Nonstationary Time Series](https://arxiv.org/abs/2506.05544)
*Shibo Li,Yao Zheng*

Main category: stat.ML

TL;DR: This paper presents MPS (Model Prediction Set), a framework for online model selection in nonstationary time series, addressing limitations of traditional methods in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting models for nonstationary time series, as traditional model selection methods fail due to their reliance on stationarity assumptions.

Method: The authors combine conformal inference with model confidence sets to develop an adaptive procedure that updates a confidence set of candidate models in real-time, ensuring it covers the best model for the next period with specified probability.

Result: MPS is validated through simulations and real-world data analysis, proving its reliability and efficiency in identifying optimal models under nonstationarity.

Conclusion: MPS is a broadly applicable framework that adapts to changing dynamics, offering deeper insights and effective model selection across diverse problem settings.

Abstract: This paper introduces the MPS (Model Prediction Set), a novel framework for
online model selection for nonstationary time series. Classical model selection
methods, such as information criteria and cross-validation, rely heavily on the
stationarity assumption and often fail in dynamic environments which undergo
gradual or abrupt changes over time. Yet real-world data are rarely stationary,
and model selection under nonstationarity remains a largely open problem. To
tackle this challenge, we combine conformal inference with model confidence
sets to develop a procedure that adaptively selects models best suited to the
evolving dynamics at any given time. Concretely, the MPS updates in real time a
confidence set of candidate models that covers the best model for the next time
period with a specified long-run probability, while adapting to nonstationarity
of unknown forms. Through simulations and real-world data analysis, we
demonstrate that MPS reliably and efficiently identifies optimal models under
nonstationarity, an essential capability lacking in offline methods. Moreover,
MPS frequently produces high-quality sets with small cardinality, whose
evolution offers deeper insights into changing dynamics. As a generic
framework, MPS accommodates any data-generating process, data structure, model
class, training method, and evaluation metric, making it broadly applicable
across diverse problem settings.

</details>


### [117] [Nonlinear Causal Discovery through a Sequential Edge Orientation Approach](https://arxiv.org/abs/2506.05590)
*Stella Huang,Qing Zhou*

Main category: stat.ML

TL;DR: This paper introduces a computationally efficient and robust approach for learning causal directed acyclic graphs (DAGs) under nonlinear additive noise models (ANMs), leveraging pairwise additive noise models for edge direction identification.


<details>
  <summary>Details</summary>
Motivation: Existing methods for causal DAG discovery face challenges like restrictive assumptions, reliance on independence tests, and high computational demands.

Method: The authors propose a sequential procedure using pairwise additive noise models (PANM) for orienting edges in a completed partial DAG (CPDAG). They introduce ranking and statistical testing methods to evaluate edge directions and prove the structural consistency of their approach.

Result: Their method demonstrates superior computational efficiency, robustness to model misspecifications, and consistently outperforms existing nonlinear DAG learning approaches in experiments on synthetic and real-world datasets.

Conclusion: The proposed method offers a novel and effective solution for causal DAG learning, addressing key limitations of existing approaches and establishing its consistency and robustness.

Abstract: Recent advances have established the identifiability of a directed acyclic
graph (DAG) under additive noise models (ANMs), spurring the development of
various causal discovery methods. However, most existing methods make
restrictive model assumptions, rely heavily on general independence tests, or
require substantial computational time. To address these limitations, we
propose a sequential procedure to orient undirected edges in a completed
partial DAG (CPDAG), representing an equivalence class of DAGs, by leveraging
the pairwise additive noise model (PANM) to identify their causal directions.
We prove that this procedure can recover the true causal DAG assuming a
restricted ANM. Building on this result, we develop a novel constraint-based
algorithm for learning causal DAGs under nonlinear ANMs. Given an estimated
CPDAG, we develop a ranking procedure that sorts undirected edges by their
adherence to the PANM, which defines an evaluation order of the edges. To
determine the edge direction, we devise a statistical test that compares the
log-likelihood values, evaluated with respect to the competing directions, of a
sub-graph comprising just the candidate nodes and their identified parents in
the partial DAG. We further establish the structural learning consistency of
our algorithm in the large-sample limit. Extensive experiments on synthetic and
real-world datasets demonstrate that our method is computationally efficient,
robust to model misspecification, and consistently outperforms many existing
nonlinear DAG learning methods.

</details>


### [118] [Multilevel neural simulation-based inference](https://arxiv.org/abs/2506.06087)
*Yuga Hikida,Ayush Bharti,Niall Jeffrey,François-Xavier Briol*

Main category: stat.ML

TL;DR: The paper proposes enhancing neural simulation-based inference (SBI) by using multilevel Monte Carlo techniques for scenarios with simulators of varying cost and fidelity, improving accuracy under fixed computational budgets.


<details>
  <summary>Details</summary>
Motivation: Performing Bayesian inference through neural SBI is often impeded by computationally expensive simulators or limited simulations in scientific and engineering applications.

Method: The authors introduce a multilevel Monte Carlo approach integrated into neural SBI to utilize simulators with different fidelity and costs effectively.

Result: Theoretical analysis and experiments show that the proposed method achieves improved inference accuracy compared to traditional neural SBI approaches with a constrained computational budget.

Conclusion: Leveraging multilevel Monte Carlo improved neural SBI performance, making it a practical tool for performing inference in scenarios with computationally expensive and diverse simulators.

Abstract: Neural simulation-based inference (SBI) is a popular set of methods for
Bayesian inference when models are only available in the form of a simulator.
These methods are widely used in the sciences and engineering, where writing
down a likelihood can be significantly more challenging than constructing a
simulator. However, the performance of neural SBI can suffer when simulators
are computationally expensive, thereby limiting the number of simulations that
can be performed. In this paper, we propose a novel approach to neural SBI
which leverages multilevel Monte Carlo techniques for settings where several
simulators of varying cost and fidelity are available. We demonstrate through
both theoretical analysis and extensive experiments that our method can
significantly enhance the accuracy of SBI methods given a fixed computational
budget.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [119] [Connectome brain fingerprinting: terminology, measures, and target properties](https://arxiv.org/abs/2506.05769)
*Matteo Fraschini,Matteo Demuru,Daniele Marinazzo,Luca Didaci*

Main category: q-bio.QM

TL;DR: The paper critiques the use of the term 'fingerprinting' for brain-based metrics in neuroscience, arguing it often deviates from the stricter biometric definition.


<details>
  <summary>Details</summary>
Motivation: To address confusion and misalignment in the use of 'fingerprinting' for brain metrics, contrasting it with its precise meaning in biometric sciences.

Method: The authors reviewed neuroscience uses of 'fingerprinting,' identified mismatches with biometric standards, and proposed guidelines for proper alignment.

Result: Identified inconsistencies in the terminology usage and provided recommendations to improve clarity between neuroscience and biometric fields.

Conclusion: Clear guidelines and alignment in the use of 'fingerprinting' will enhance interdisciplinary understanding and communication.

Abstract: Distinguishing one person from another (what biometricians call recognition)
is extremely relevant for different aspects of life. Traditional biometric
modalities (fingerprint, face, iris, voice) rely on unique, stable features
that reliably differentiate individuals. Recently, the term fingerprinting has
gained popularity in neuroscience, with a growing number of studies adopting
the term to describe various brain based metrics derived from different
techniques. However, we think there is a mismatch between its widely accepted
meaning in the biometric community and some brain based metrics. Many of these
measures do not satisfy the strict definition of a biometric fingerprint that
is, a stable trait that uniquely identifies an individual. In this study we
discuss some issues that may generate confusion in this context and suggest how
to treat the question in the future. In particular, we review how fingerprint
is currently used in the neuroscience literature, highlight mismatches with the
biometric community definition, and offer clear guidelines for distinguishing
genuine biometric fingerprints from exploratory similarity metrics. By
clarifying terminology and criteria, we aim to align practices and facilitate
communication across fields.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [120] [FedShield-LLM: A Secure and Scalable Federated Fine-Tuned Large Language Model](https://arxiv.org/abs/2506.05640)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.CR

TL;DR: The paper introduces FedShield-LLM, which uses pruning with Fully Homomorphic Encryption (FHE) to securely fine-tune LLMs in Federated Learning systems while preserving data privacy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address privacy and security concerns when fine-tuning LLMs in Federated Learning systems, especially for small and medium-sized organizations with limited computational resources.

Method: The proposed method, FedShield-LLM, utilizes pruning with Fully Homomorphic Encryption (FHE) applied to Low-Rank Adaptation (LoRA) parameters, reducing the attack surface and enabling secure encrypted computations. Optimized federated algorithms further improve scalability and efficiency.

Result: Experimental results demonstrate that FedShield-LLM surpasses existing methods in performance while ensuring robust privacy protection, making LLM training more feasible for resource-constrained organizations.

Conclusion: FedShield-LLM effectively enables collaborative and privacy-preserving training of LLMs in Federated Learning environments, balancing security, efficiency, and effectiveness.

Abstract: Federated Learning (FL) offers a decentralized framework for training and
fine-tuning Large Language Models (LLMs) by leveraging computational resources
across organizations while keeping sensitive data on local devices. It
addresses privacy and security concerns while navigating challenges associated
with the substantial computational demands of LLMs, which can be prohibitive
for small and medium-sized organizations. FL supports the development of
task-specific LLMs for cross-silo applications through fine-tuning but remains
vulnerable to inference attacks, such as membership inference and gradient
inversion, which threaten data privacy. Prior studies have utilized
Differential Privacy (DP) in LLM fine-tuning, which, despite being effective at
preserving privacy, can degrade model performance. To overcome these
challenges, we propose a novel method, FedShield-LLM, that uses pruning with
Fully Homomorphic Encryption (FHE) for Low-Rank Adaptation (LoRA) parameters,
enabling secure computations on encrypted model updates while mitigating the
attack surface by deactivating less important LoRA parameters. Furthermore,
optimized federated algorithms for cross-silo environments enhance scalability
and efficiency. Parameter-efficient fine-tuning techniques like LoRA
substantially reduce computational and communication overhead, making FL
feasible for resource-constrained clients. Experimental results show that the
proposed method outperforms existing methods while maintaining robust privacy
protection, enabling organizations to collaboratively train secure and
efficient LLMs.
  The code and data are available at,
https://github.com/solidlabnetwork/fedshield-llm

</details>


### [121] [Obfuscation-Resilient Binary Code Similarity Analysis using Dominance Enhanced Semantic Graph](https://arxiv.org/abs/2506.06161)
*Yufeng Wang,Yuhong Feng,Yixuan Cao,Haoran Li,Haiyue Feng,Yifeng Wang*

Main category: cs.CR

TL;DR: ORCAS is a BCSA model designed to overcome obfuscation challenges by introducing DESG, a robust code representation, and achieves superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: BCSA methods face challenges in handling code obfuscation due to unstable control flow structures, impacting their effectiveness in tasks like vulnerability detection.

Method: The paper introduces ORCAS, utilizing the Dominance Enhanced Semantic Graph (DESG) to represent binary code without relying on control flow. This captures relational semantics within binaries and ensures resilience to obfuscation.

Result: ORCAS outperforms state-of-the-art models, showing a 12.1% gain in PR-AUC on the BinKit dataset under multiple obfuscation conditions, and improves recall by up to 43% on a newly released real-world obfuscation dataset.

Conclusion: ORCAS presents a significant advancement in BCSA, offering robust performance across various obfuscation settings and promoting further research with a new dataset.

Abstract: Binary code similarity analysis (BCSA) serves as a core technique for binary
analysis tasks such as vulnerability detection. While current graph-based BCSA
approaches capture substantial semantics and show strong performance, their
performance suffers under code obfuscation due to the unstable control flow. To
address this issue, we develop ORCAS, an Obfuscation-Resilient BCSA model based
on Dominance Enhanced Semantic Graph (DESG). The DESG is an original binary
code representation, capturing more binaries' implicit semantics without
control flow structure, including inter-instruction relations, inter-basic
block relations, and instruction-basic block relations. ORCAS robustly scores
semantic similarity across binary functions from different obfuscation options,
optimization levels, and instruction set architectures. Extensive evaluation on
the BinKit dataset shows ORCAS significantly outperforms eight baselines,
achieving an average 12.1% PR-AUC gain when using combined three obfuscation
options compared to the state-of-the-art approaches. Furthermore, ORCAS
improves recall by up to 43% on an original obfuscated real-world vulnerability
dataset, which we released to facilitate future research.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [122] [Adaptive stable distribution and Hurst exponent by method of moments moving estimator for nonstationary time series](https://arxiv.org/abs/2506.05354)
*Jarek Duda*

Main category: stat.ME

TL;DR: The paper proposes using a moving estimator technique to analyze nonstationary time series, particularly focusing on adaptive parameters in alpha-Stable distributions for financial data.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like ARMA-ARCH assume specific dependency structures, which may introduce bias. The paper aims to develop a more agnostic approach to adapt to real-life time series nonstationarity.

Method: The method utilizes a moving estimator based on a local log-likelihood approach with exponentially weakening weights, implemented using Exponential Moving Average (EMA) of distributional parameters.

Result: The moving estimator successfully models the evolution of parameters in alpha-Stable distributions, including center, scale ($\mu$, $\sigma$), and tail behavior controlled by the $\alpha$ parameter. Applications on DJIA time series demonstrate effective adaptive market stability evaluation.

Conclusion: The proposed approach provides a novel adaptive method to analyze real-world nonstationary time series, particularly for financial data, offering insights into market stability and likelihood of extreme events.

Abstract: Nonstationarity of real-life time series requires model adaptation. In
classical approaches like ARMA-ARCH there is assumed some arbitrarily chosen
dependence type. To avoid their bias, we will focus on novel more agnostic
approach: moving estimator, which estimates parameters separately for every
time $t$: optimizing $F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta
(x_\tau))$ local log-likelihood with exponentially weakening weights of the old
values. In practice such moving estimates can be found by EMA (exponential
moving average) of some parameters, like $m_p=E[|x-\mu|^p]$ absolute central
moments, updated by $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$. We
will focus here on its applications for alpha-Stable distribution, which also
influences Hurst exponent, hence can be used for its adaptive estimation. Its
application will be shown on financial data as DJIA time series - beside
standard estimation of evolution of center $\mu$ and scale parameter $\sigma$,
there is also estimated evolution of $\alpha$ parameter allowing to
continuously evaluate market stability - tails having $\rho(x) \sim
1/|x|^{\alpha+1}$ behavior, controlling probability of potentially dangerous
extreme events.

</details>


### [123] [Testing Hypotheses of Covariate Effects on Topics of Discourse](https://arxiv.org/abs/2506.05570)
*Gabriel Phelan,David A. Campbell*

Main category: stat.ME

TL;DR: The paper introduces a non-parametric topic modeling approach with document-level covariates using non-negative matrix factorization and regression methods, contrasting it with complex generative models.


<details>
  <summary>Details</summary>
Motivation: Existing topic modeling paradigms rely on complex probabilistic models, which are computationally intensive and hard to fit for large datasets.

Method: The authors propose combining convex non-negative matrix factorization with standard regression techniques, coupled with non-parametric resampling for uncertainty quantification.

Result: The approach offers faster, more interpretable, and inferentially justified results compared to traditional generative models.

Conclusion: The non-parametric method is shown to be practical and effective, exemplified by its application to analyzing Canadian beer flavor discourse.

Abstract: We introduce an approach to topic modelling with document-level covariates
that remains tractable in the face of large text corpora. This is achieved by
de-emphasizing the role of parameter estimation in an underlying probabilistic
model, assuming instead that the data come from a fixed but unknown
distribution whose statistical functionals are of interest. We propose
combining a convex formulation of non-negative matrix factorization with
standard regression techniques as a fast-to-compute and useful estimate of such
a functional. Uncertainty quantification can then be achieved by reposing
non-parametric resampling methods on top of this scheme. This is in contrast to
popular topic modelling paradigms, which posit a complex and often hard-to-fit
generative model of the data. We argue that the simple, non-parametric approach
advocated here is faster, more interpretable, and enjoys better inferential
justification than said generative models. Finally, our methods are
demonstrated with an application analysing covariate effects on discourse of
flavours attributed to Canadian beers.

</details>
