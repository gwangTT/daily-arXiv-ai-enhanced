<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 52]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 109]
- [cs.CV](#cs.CV) [Total: 121]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.LG](#cs.LG) [Total: 125]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 47]
- [cs.SE](#cs.SE) [Total: 16]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 20]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.SD](#cs.SD) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards mitigating information leakage when evaluating safety monitors](https://arxiv.org/abs/2509.21344)
*Gerard Boxo,Aman Neelappa,Shivam Raval*

Main category: cs.AI

TL;DR: The paper proposes strategies to evaluate and mitigate unintended data leakage in white box monitors detecting harmful behaviors in large language models.


<details>
  <summary>Details</summary>
Motivation: Detecting harmful behaviors in large language models often shows inflated performance due to leakage of elicitation-related data into the monitoring system, creating a need for genuine evaluation techniques.

Method: The paper introduces three evaluation strategies: content filtering, score filtering, and the use of fine-tuned model organisms to reduce leakage effects. These approaches were tested using deception detection as a case study.

Result: Experiments demonstrated that content filtering effectively reduced monitoring performance (AUROC drop by 30%), score filtering resulted in an AUROC drop by 15%, and model organisms enhanced evaluation but decreased monitoring performance by up to 40%.

Conclusion: The proposed strategies improve the reliability of evaluating monitoring systems but also highlight the challenges in mitigating leakage effects and retaining high monitoring performance.

Abstract: White box monitors that analyze model internals offer promising advantages
for detecting potentially harmful behaviors in large language models, including
lower computational costs and integration into layered defense systems.However,
training and evaluating these monitors requires response exemplars that exhibit
the target behaviors, typically elicited through prompting or fine-tuning. This
presents a challenge when the information used to elicit behaviors inevitably
leaks into the data that monitors ingest, inflating their effectiveness. We
present a systematic framework for evaluating a monitor's performance in terms
of its ability to detect genuine model behavior rather than superficial
elicitation artifacts. Furthermore, we propose three novel strategies to
evaluate the monitor: content filtering (removing deception-related text from
inputs), score filtering (aggregating only over task-relevant tokens), and
prompt distilled fine-tuned model organisms (models trained to exhibit
deceptive behavior without explicit prompting). Using deception detection as a
representative case study, we identify two forms of leakage that inflate
monitor performance: elicitation leakage from prompts that explicitly request
harmful behavior, and reasoning leakage from models that verbalize their
deceptive actions. Through experiments on multiple deception benchmarks, we
apply our proposed mitigation strategies and measure performance retention. Our
evaluation of the monitors reveal three crucial findings: (1) Content filtering
is a good mitigation strategy that allows for a smooth removal of elicitation
signal and can decrease probe AUROC by 30\% (2) Score filtering was found to
reduce AUROC by 15\% but is not as straightforward to attribute to (3) A
finetuned model organism improves monitor evaluations but reduces their
performance by upto 40\%, even when re-trained.

</details>


### [2] [Correct Reasoning Paths Visit Shared Decision Pivots](https://arxiv.org/abs/2509.21549)
*Dongkyu Cho,Amy B. Z. Zhang,Bilel Fehri,Sheng Wang,Rumi Chunara,Rui Song,Hengrui Cai*

Main category: cs.AI

TL;DR: The paper introduces decision pivots to verify reasoning paths of LLMs, proposing a self-training pipeline that mines and leverages shared pivot points.


<details>
  <summary>Details</summary>
Motivation: To address the unsolved challenge of verifying the reasoning processes of large language models at scale, especially in Chain-of-thought reasoning.

Method: A self-training pipeline is developed that samples diverse reasoning paths, identifies shared decision pivots, compresses pathways into minimal verifiable traces, and post-trains the model using its outputs.

Result: Experimental results on benchmarks like LogiQA, MedQA, and MATH500 demonstrate the method's effectiveness in improving reasoning alignment.

Conclusion: By leveraging decision pivots, the approach improves reasoning alignment in LLMs without requiring ground truth reasoning data or external metrics.

Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.

</details>


### [3] [AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need](https://arxiv.org/abs/2509.21553)
*Ahmed Jaber,Wangshu Zhu,Karthick Jayavelu,Justin Downes,Sameer Mohamed,Candace Agonafir,Linnia Hawkins,Tian Zheng*

Main category: cs.AI

TL;DR: The paper introduces a system combining a curated knowledge graph with AI agents to make climate data science more accessible by overcoming technical and format-related barriers.


<details>
  <summary>Details</summary>
Motivation: Climate data science is hindered by fragmented data sources, varied formats, and high technical expertise requirements, impacting participation, discovery, and reproducibility.

Method: The authors integrate a curated knowledge graph with AI agents powered by generative AI for interacting with cloud-native APIs, natural language processing, and automated data analysis.

Result: The system significantly simplifies data identification and analysis for non-specialist users and demonstrates scalable workflows using cloud-ready APIs.

Conclusion: The proposed system democratizes access to climate data and provides a reproducible framework for human-AI collaboration in scientific research.

Abstract: Climate data science faces persistent barriers stemming from the fragmented
nature of data sources, heterogeneous formats, and the steep technical
expertise required to identify, acquire, and process datasets. These challenges
limit participation, slow discovery, and reduce the reproducibility of
scientific workflows. In this paper, we present a proof of concept for
addressing these barriers through the integration of a curated knowledge graph
(KG) with AI agents designed for cloud-native scientific workflows. The KG
provides a unifying layer that organizes datasets, tools, and workflows, while
AI agents -- powered by generative AI services -- enable natural language
interaction, automated data access, and streamlined analysis. Together, these
components drastically lower the technical threshold for engaging in climate
data science, enabling non-specialist users to identify and analyze relevant
datasets. By leveraging existing cloud-ready API data portals, we demonstrate
that "a knowledge graph is all you need" to unlock scalable and agentic
workflows for scientific inquiry. The open-source design of our system further
supports community contributions, ensuring that the KG and associated tools can
evolve as a shared commons. Our results illustrate a pathway toward
democratizing access to climate data and establishing a reproducible,
extensible framework for human--AI collaboration in scientific research.

</details>


### [4] [EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks](https://arxiv.org/abs/2509.21567)
*Mohammad Parsa Afshar,Aryan Azimi*

Main category: cs.AI

TL;DR: This paper explores the use of EEG data and machine learning models, including classical methods and Graph Neural Networks (GNNs), to predict consumer behavior.


<details>
  <summary>Details</summary>
Motivation: The study aims to deepen the understanding of consumer decision-making by leveraging EEG data for neural activity analysis and comparing various machine learning techniques.

Method: EEG data from the NeuMa dataset was preprocessed, and brain connectivity features were generated for GNN models. Classical machine learning models and different GNN architectures were applied and compared.

Result: The study found that while there was no significant overall difference, GNNs generally outperformed classical models in certain criteria where the latter were less effective.

Conclusion: Combining EEG signal analysis with machine learning offers insights into consumer behavior. The study provides a comparative evaluation of traditional models like SVM and relatively novel methods like GNNs in neuromarketing.

Abstract: Prediction of consumer behavior is one of the important purposes in
marketing, cognitive neuroscience, and human-computer interaction. The
electroencephalography (EEG) data can help analyze the decision process by
providing detailed information about the brain's neural activity. In this
research, a comparative approach is utilized for predicting consumer behavior
by EEG data. In the first step, the features of the EEG data from the NeuMa
dataset were extracted and cleaned. For the Graph Neural Network (GNN) models,
the brain connectivity features were created. Different machine learning
models, such as classical models and Graph Neural Networks, are used and
compared. The GNN models with different architectures are implemented to have a
comprehensive comparison; furthermore, a wide range of classical models, such
as ensemble models, are applied, which can be very helpful to show the
difference and performance of each model on the dataset. Although the results
did not show a significant difference overall, the GNN models generally
performed better in some basic criteria where classical models were not
satisfactory. This study not only shows that combining EEG signal analysis and
machine learning models can provide an approach to deeper understanding of
consumer behavior, but also provides a comprehensive comparison between the
machine learning models that have been widely used in previous studies in the
EEG-based neuromarketing such as Support Vector Machine (SVM), and the models
which are not used or rarely used in the field, like Graph Neural Networks.

</details>


### [5] [GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models](https://arxiv.org/abs/2509.21593)
*Peng Luo,Xiayin Lou,Yu Zheng,Zhuo Zheng,Stefano Ermon*

Main category: cs.AI

TL;DR: GeoEvolve is a framework that integrates domain knowledge and evolutionary search to automatically design algorithms for geospatial problems, surpassing traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of current large language model-based algorithm discovery systems, which lack the domain-specific reasoning and theoretical guidance necessary for solving complex geospatial challenges.

Method: GeoEvolve employs a multi-agent framework with nested loops: an inner loop generates and mutates algorithm candidates, while an outer controller evaluates solutions using a domain-specific knowledge base called GeoKnowRAG.

Result: GeoEvolve improves spatial interpolation error (RMSE) by 13â€“21% and enhances uncertainty estimation by 17%, outperforming classical models by incorporating geospatial theory.

Conclusion: GeoEvolve demonstrates potential as a scalable, knowledge-driven tool for automated geospatial modeling, paving the way for more trustworthy AI-based discoveries in science and sustainability.

Abstract: Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.

</details>


### [6] [Automated and Interpretable Survival Analysis from Multimodal Data](https://arxiv.org/abs/2509.21600)
*Mafalda Malafaia,Peter A. N. Bosman,Coen Rasch,Tanja Alderliesten*

Main category: cs.AI

TL;DR: The paper introduces MultiFIX, an interpretable multimodal AI framework for automated survival analysis in oncology, integrating clinical variables and imaging with strong performance metrics.


<details>
  <summary>Details</summary>
Motivation: Oncology survival analysis often faces challenges in accuracy and interpretability. With diverse data modalities and demands for transparency, solutions are needed to support clinical trust and precision.

Method: MultiFIX integrates clinical variables and CT imaging using deep learning. Imaging is explained via Grad-CAM, clinical data via symbolic expressions using genetic programming, and risk estimation involves Cox regression for group stratification.

Result: MultiFIX, tested on the RADCURE dataset for head and neck cancer, achieved a C-index of 0.838 for prediction and 0.826 for stratification, outperforming baseline approaches and aligning with known prognostic indicators.

Conclusion: MultiFIX demonstrates the potential of interpretable multimodal AI in precision oncology, showing improved performance and transparency for survival analysis.

Abstract: Accurate and interpretable survival analysis remains a core challenge in
oncology. With growing multimodal data and the clinical need for transparent
models to support validation and trust, this challenge increases in complexity.
We propose an interpretable multimodal AI framework to automate survival
analysis by integrating clinical variables and computed tomography imaging. Our
MultiFIX-based framework uses deep learning to infer survival-relevant features
that are further explained: imaging features are interpreted via Grad-CAM,
while clinical variables are modeled as symbolic expressions through genetic
programming. Risk estimation employs a transparent Cox regression, enabling
stratification into groups with distinct survival outcomes. Using the
open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a
C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the
clinical and academic baseline approaches and aligning with known prognostic
markers. These results highlight the promise of interpretable multimodal AI for
precision oncology with MultiFIX.

</details>


### [7] [Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries](https://arxiv.org/abs/2509.21633)
*Georgios Chochlakis,Jackson Trager,Vedant Jhaveri,Nikhil Ravichandran,Alexandros Potamianos,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: The authors introduce Semantic F1 Scores as an evaluation metric that accounts for semantic relatedness between predicted and labeled data, allowing fairer evaluation in subjectively categorized tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional F1 metrics fail to account for semantic similarity between predictions and labels, leading to overly rigid evaluations, especially in tasks with subjective or fuzzy categorization.

Method: Semantic F1 is calculated using a label similarity matrix that enables soft precision-like and recall-like scores, accommodating partial credit for semantically related but nonidentical labels without forcing rigid matches.

Result: Through theoretical and empirical validation, Semantic F1 scores show improved interpretability and ecological validity compared to traditional metrics, establishing its ability to handle subjective disagreements and fuzzy boundaries.

Conclusion: Semantic F1 provides a more practical and generalized method for evaluation across tasks, granting fairness and robustness in domains characterized by fuzziness and human disagreement.

Abstract: We propose Semantic F1 Scores, novel evaluation metrics for subjective or
fuzzy multi-label classification that quantify semantic relatedness between
predicted and gold labels. Unlike the conventional F1 metrics that treat
semantically related predictions as complete failures, Semantic F1 incorporates
a label similarity matrix to compute soft precision-like and recall-like
scores, from which the Semantic F1 scores are derived. Unlike existing
similarity-based metrics, our novel two-step precision-recall formulation
enables the comparison of label sets of arbitrary sizes without discarding
labels or forcing matches between dissimilar labels. By granting partial credit
for semantically related but nonidentical labels, Semantic F1 better reflects
the realities of domains marked by human disagreement or fuzzy category
boundaries. In this way, it provides fairer evaluations: it recognizes that
categories overlap, that annotators disagree, and that downstream decisions
based on similar predictions lead to similar outcomes. Through theoretical
justification and extensive empirical validation on synthetic and real data, we
show that Semantic F1 demonstrates greater interpretability and ecological
validity. Because it requires only a domain-appropriate similarity matrix,
which is robust to misspecification, and not a rigid ontology, it is applicable
across tasks and modalities.

</details>


### [8] [Can AI Perceive Physical Danger and Intervene?](https://arxiv.org/abs/2509.21651)
*Abhishek Jindal,Dmitry Kalashnikov,Oscar Chang,Divya Garikapati,Anirudha Majumdar,Pierre Sermanet,Vikas Sindhwani*

Main category: cs.AI

TL;DR: The paper explores physical safety in Embodied AI, benchmarks their understanding of safety risks, and proposes a post-training paradigm for improving safety reasoning.


<details>
  <summary>Details</summary>
Motivation: To address safety challenges in Embodied AI systems interacting with the physical world, where risks involve direct physical harm.

Method: Developed scalable safety benchmarking using real-world injury narratives, generated visual safety scenarios, evaluated foundation models' safety understanding, and introduced a post-training paradigm to improve and interpret safety reasoning.

Result: Generated benchmarks, evaluated state-of-the-art models, and improved safety reasoning performance through transparent thinking traces and explicit safety constraints.

Conclusion: The proposed methods enhanced Embodied AI's readiness for safety-critical applications and will be made available via an open benchmark for broader impact.

Abstract: When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2

</details>


### [9] [Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization](https://arxiv.org/abs/2509.21718)
*Shehzeen Hussain,Paarth Neekhara,Xuesong Yang,Edresson Casanova,Subhankar Ghosh,Roy Fejgin,Ryan Langman,Mikyas Desta,Leili Tavabi,Jason Li*

Main category: cs.AI

TL;DR: The paper introduces a novel method using Group Relative Policy Optimization (GRPO) to adapt multilingual TTS models for low-resource languages, leveraging pretrained ASR models and fine-tuning approaches.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in developing high-quality TTS systems for low-resource languages due to limited paired text-speech data, utilizing advancements in ASR model accessibility from multilingual pretraining.

Method: The approach involves training a multilingual TTS model with IPA tokens, fine-tuning it with limited paired data, and further optimizing it with GRPO using unpaired text and speaker prompts, guided by multi-objective rewards.

Result: The framework achieves intelligible and speaker-consistent speech in low-resource languages, outperforms fine-tuning alone, and shows improvements in high-resource languages.

Conclusion: The proposed GRPO-based TTS adaptation framework enhances low-resource language TTS performance and surpasses traditional alignment methods in intelligibility, speaker similarity, and audio quality.

Abstract: Developing high-quality text-to-speech (TTS) systems for low-resource
languages is challenging due to the scarcity of paired text and speech data. In
contrast, automatic speech recognition (ASR) models for such languages are
often more accessible, owing to large-scale multilingual pre-training efforts.
We propose a framework based on Group Relative Policy Optimization (GRPO) to
adapt an autoregressive, multilingual TTS model to new languages. Our method
first establishes a language-agnostic foundation for TTS synthesis by training
a multilingual baseline with International Phonetic Alphabet (IPA) tokens.
Next, we fine-tune this model on limited paired data of the new languages to
capture the target language's prosodic features. Finally, we apply GRPO to
optimize the model using only unpaired text and speaker prompts, guided by a
multi-objective reward from pretrained ASR, speaker verification, and audio
quality estimation models. Experiments demonstrate that this pipeline produces
intelligible and speaker-consistent speech in low-resource languages,
substantially outperforming fine-tuning alone. Furthermore, our GRPO-based
framework also improves TTS performance in high-resource languages, surpassing
offline alignment methods such as Direct Preference Optimization (DPO) yielding
superior intelligibility, speaker similarity, and audio quality.

</details>


### [10] [Guiding Evolution of Artificial Life Using Vision-Language Models](https://arxiv.org/abs/2509.22447)
*Nikhil Baid,Hannah Erlebach,Paul Hellegouarch,Frederico Wieser*

Main category: cs.AI

TL;DR: Foundation models (FMs) are leveraged to introduce ASAL++, which guides artificial life (ALife) simulations through multimodal tools, enabling open-ended evolutionary targets.


<details>
  <summary>Details</summary>
Motivation: To expand the potential of ALife simulations using foundation models that automate and enrich search processes, advancing the exploration of increasingly complex evolutionary targets.

Method: ASAL++ employs multimodal FMs to guide ALife simulations. A secondary FM generates new evolutionary targets based on visual histories, with strategies for matching single prompts (EST) or a sequence of prompts (ETT). Tests are conducted in the Lenia substrate.

Result: EST leads to greater visual novelty in simulations, whereas ETT results in more coherent and interpretable evolutionary trajectories.

Conclusion: ASAL++ demonstrates the potential of FMs to drive ALife discovery, offering new avenues for open-ended simulation exploration and increasing complexity.

Abstract: Foundation models (FMs) have recently opened up new frontiers in the field of
artificial life (ALife) by providing powerful tools to automate search through
ALife simulations. Previous work aligns ALife simulations with natural language
target prompts using vision-language models (VLMs). We build on Automated
Search for Artificial Life (ASAL) by introducing ASAL++, a method for
open-ended-like search guided by multimodal FMs. We use a second FM to propose
new evolutionary targets based on a simulation's visual history. This induces
an evolutionary trajectory with increasingly complex targets.
  We explore two strategies: (1) evolving a simulation to match a single new
prompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving a
simulation to match the entire sequence of generated prompts (Evolved Temporal
Targets: ETT). We test our method empirically in the Lenia substrate using
Gemma-3 to propose evolutionary targets, and show that EST promotes greater
visual novelty, while ETT fosters more coherent and interpretable evolutionary
sequences.
  Our results suggest that ASAL++ points towards new directions for FM-driven
ALife discovery with open-ended characteristics.

</details>


### [11] [Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts](https://arxiv.org/abs/2509.21743)
*Ammar Ahmed,Azal Ahmad Khan,Ayaan Ahmad,Sheng Di,Zirui Liu,Ali Anwar*

Main category: cs.AI

TL;DR: The paper introduces "Retrieval-of-Thought" (RoT), a method to improve reasoning efficiency by reusing prior reasoning steps, significantly reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address efficiency issues in reasoning models that produce long reasoning traces, which inflate latency and cost.

Method: RoT employs a thought graph structure to store reusable reasoning steps. During inference, it retrieves relevant nodes and dynamically constructs templates to guide problem-solving, minimizing redundant computation.

Result: RoT achieves efficiency gains by reducing output tokens by up to 40%, inference latency by 82%, and cost by 59% without sacrificing accuracy.

Conclusion: The approach presents a scalable and efficient way for reasoning models to maintain performance while markedly reducing inference overheads.

Abstract: Large reasoning models improve accuracy by producing long reasoning traces,
but this inflates latency and cost, motivating inference-time efficiency. We
propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable
``thought" steps to guide new problems. RoT organizes steps into a thought
graph with sequential and semantic edges to enable fast retrieval and flexible
recombination. At inference, RoT retrieves query-relevant nodes and applies
reward-guided traversal to assemble a problem-specific template that guides
generation. This dynamic template reuse reduces redundant exploration and,
therefore, reduces output tokens while preserving accuracy. We evaluate RoT on
reasoning benchmarks with multiple models, measuring accuracy, token usage,
latency, and memory overhead. Findings show small prompt growth but substantial
efficiency gains, with RoT reducing output tokens by up to 40%, inference
latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a
scalable paradigm for efficient LRM reasoning via dynamic template construction
through retrieval.

</details>


### [12] [Lifelong Learning with Behavior Consolidation for Vehicle Routing](https://arxiv.org/abs/2509.21765)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.AI

TL;DR: This paper introduces LLR-BC, a lifelong learning framework for neural solvers addressing sequentially arising VRP tasks, with strategies to mitigate catastrophic forgetting and improve generalization.


<details>
  <summary>Details</summary>
Motivation: There is a need for neural solvers for routing problems to handle sequentially emerging tasks with varying distributions without suffering from catastrophic forgetting or relying solely on fine-tuning and zero-shot generalization.

Method: The paper proposes an approach called LLR-BC that aligns solver behaviors trained on new tasks with buffered behaviors from previous tasks. It prioritizes decisions with lower confidence during consolidation to strengthen crucial experiences.

Result: Experiments on capacitated vehicle routing problems and traveling salesman problems show that LLR-BC maintains solver plasticity, addresses catastrophic forgetting, and improves zero-shot generalization capabilities.

Conclusion: LLR-BC is effective for lifelong learning in neural solvers, providing strong performance on new and previous tasks while addressing key challenges like catastrophic forgetting and generalization between tasks.

Abstract: Recent neural solvers have demonstrated promising performance in learning to
solve routing problems. However, existing studies are primarily based on
one-off training on one or a set of predefined problem distributions and
scales, i.e., tasks. When a new task arises, they typically rely on either
zero-shot generalization, which may be poor due to the discrepancies between
the new task and the training task(s), or fine-tuning the pretrained solver on
the new task, which possibly leads to catastrophic forgetting of knowledge
acquired from previous tasks. This paper explores a novel lifelong learning
paradigm for neural VRP solvers, where multiple tasks with diverse
distributions and scales arise sequentially over time. Solvers are required to
effectively and efficiently learn to solve new tasks while maintaining their
performance on previously learned tasks. Consequently, a novel framework called
Lifelong Learning Router with Behavior Consolidation (LLR-BC) is proposed.
LLR-BC consolidates prior knowledge effectively by aligning behaviors of the
solver trained on a new task with the buffered ones in a decision-seeking way.
To encourage more focus on crucial experiences, LLR-BC assigns greater
consolidated weights to decisions with lower confidence. Extensive experiments
on capacitated vehicle routing problems and traveling salesman problems
demonstrate LLR-BC's effectiveness in training high-performance neural solvers
in a lifelong learning setting, addressing the catastrophic forgetting issue,
maintaining their plasticity, and improving zero-shot generalization ability.

</details>


### [13] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: The UltraHorizon benchmark is introduced to address the gap in evaluating autonomous agents' performance in long-horizon and partially observable tasks. Current AI agents struggle with these challenges compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the gap in systematic evaluation for autonomous agents in long-term, partially observable scenarios critical to real-world applications like software development and scientific discovery.

Method: The paper introduces UltraHorizon, a benchmark that evaluates foundational capabilities for long-horizon discovery tasks, using three distinct environments and tasks requiring reasoning, planning, memory, and tool management.

Result: Experiments demonstrate that LLM-agents consistently underperform in long-horizon tasks compared to humans. Simple scaling does not improve performance. Analysis identifies eight error types caused by in-context locking and capability gaps.

Conclusion: Autonomous agents exhibit considerable weaknesses in long-horizon environments, and current methods and scaling are insufficient to bridge the gap with human performance. The UltraHorizon benchmark is a step toward systematic evaluation in this area.

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [14] [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782)
*Junliang Liu,Jingyu Xiao,Wenxin Tang,Wenxuan Wang,Zhixian Wang,Minrui Zhang,Shuanghe Yu*

Main category: cs.AI

TL;DR: The paper introduces WebRSSBench, a benchmark for evaluating multimodal large language models (MLLMs) on reasoning, robustness, and safety in web applications across eight tasks using data from 729 websites and 3799 QA pairs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs predominantly focus on visual perception or UI code generation but fail to cover reasoning, robustness, and safety, which are critical for building end-to-end complex web applications.

Method: The benchmark, WebRSSBench, includes standardized prompts, deterministic evaluation scripts, and multi-stage quality assurance to test 12 MLLMs across eight tasks using multi-step inference over web page structures and interactions.

Result: Significant gaps were identified in the models' abilities, including issues with compositional and cross-element reasoning, robustness against interface perturbations, and cautiousness in recognizing safety-critical actions.

Conclusion: Multimodal models require further development to handle real-world web-based reasoning, robustness, and safety challenges effectively.

Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.

</details>


### [15] [D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/abs/2509.21799)
*Hongze Mi,Yibo Feng,Wenjie Lu,Yuqi Wang,Jinyuan Li,Song Cao,He Cui,Tengfei Tian,Xuelin Zhang,Haotian Luo,Di Sun,Naiqiang Tan,Gang Pan*

Main category: cs.AI

TL;DR: D-Artemis is a framework improving GUI agents by mimicking human cognitive processes to achieve state-of-the-art performance without extensive training datasets.


<details>
  <summary>Details</summary>
Motivation: The paper addresses key challenges in GUI agent tasks: data scarcity for end-to-end training, delayed error detection costs, and contradictory guidance.

Method: The method involves three cognitive stages: fine-grained tip retrieval, pre-execution alignment with TAC Check and ACA modules, and post-execution reflection for strategic learning.

Result: D-Artemis achieves new SOTA benchmarks with a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.

Conclusion: The framework boosts the performance of general-purpose MLLMs for GUI tasks, offering efficient generalization without complex trajectory dataset training.

Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.

</details>


### [16] [ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration](https://arxiv.org/abs/2509.21823)
*Gaole Dai,Shiqi Jiang,Ting Cao,Yuqing Yang,Yuanchun Li,Rui Tan,Mo Li,Lili Qiu*

Main category: cs.AI

TL;DR: ProRe, a proactive reward system targeting GUI agents, enhances reward accuracy and F1 scores significantly by actively probing and interacting with environments through a general-purpose reasoner and domain-specific evaluator agents.


<details>
  <summary>Details</summary>
Motivation: Existing reward systems for language models struggle with limited accuracy and generalizability in GUI agent settings due to the lack of ground-truth trajectories and reliance on static evaluations.

Method: ProRe incorporates a general-purpose reasoner that assigns tasks for domain-specific evaluator agents to interact with environments and collect extra observations, enabling more accurate reward computation.

Result: Empirical tests on over 3,000 trajectories show ProRe improving reward accuracy by 5.3%, F1 score by 19.4%, and success rate for policy agents by 22.4%.

Conclusion: ProRe offers a more precise and reliable approach to reward computation, demonstrating its effectiveness in enhancing accuracy and policy agent success rates for GUI-focused tasks.

Abstract: Reward is critical to the evaluation and training of large language models
(LLMs). However, existing rule-based or model-based reward methods struggle to
generalize to GUI agents, where access to ground-truth trajectories or
application databases is often unavailable, and static trajectory-based
LLM-as-a-Judge approaches suffer from limited accuracy. To address these
challenges, we propose ProRe, a proactive reward system that leverages a
general-purpose reasoner and domain-specific evaluator agents (actors). The
reasoner schedules targeted state probing tasks, which the evaluator agents
then execute by actively interacting with the environment to collect additional
observations. This enables the reasoner to assign more accurate and verifiable
rewards to GUI agents. Empirical results on over 3K trajectories demonstrate
that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,
respectively. Furthermore, integrating ProRe with state-of-the-art policy
agents yields a success rate improvement of up to 22.4%.

</details>


### [17] [DS-STAR: Data Science Agent via Iterative Planning and Verification](https://arxiv.org/abs/2509.21825)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Tomas Pfister*

Main category: cs.AI

TL;DR: The paper introduces DS-STAR, a data science agent designed to address challenges in analyzing heterogeneous data formats and generating optimal analysis plans using iterative refinement.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing large language models in handling diverse data formats and producing verified, optimal analysis plans for complex data-driven tasks.

Method: DS-STAR uses a three-part approach: (1) a module for analyzing diverse data files; (2) an LLM-based judge to evaluate the sufficiency of analysis plans; (3) iterative refinement of analysis plans until verified sufficiency is achieved.

Result: Experiments demonstrate that DS-STAR achieves state-of-the-art performance on benchmarks like DABStep, KramaBench, and DA-Code, particularly excelling in tasks requiring multi-file heterogeneous data analysis.

Conclusion: DS-STAR represents a significant advancement in automating complex data science tasks, reliably handling diverse data formats and iterative plan refinement to achieve verified sufficiency.

Abstract: Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.

</details>


### [18] [Axiomatic Choice and the Decision-Evaluation Paradox](https://arxiv.org/abs/2509.21836)
*Ben Abramowitz,Nicholas Mattei*

Main category: cs.AI

TL;DR: The paper introduces a framework for modeling decisions through axioms, outlines a taxonomy of decision axioms, and identifies a paradox between making and evaluating decisions.


<details>
  <summary>Details</summary>
Motivation: To address and explore tensions and challenges surrounding the application of decision axioms in decision-making and evaluation processes.

Method: A framework is introduced to model decisions using axioms, and a taxonomy of these axioms is developed alongside identifying structural tensions (Decision-Evaluation Paradox).

Result: The Decision-Evaluation Paradox is identified and shown to arise in realistic axiom structures, highlighting the complexity and caution needed when using decision axioms.

Conclusion: The paradox reveals why training models or applying axioms to make and evaluate decisions requires exceptional care due to potential tensions in decision-making structures.

Abstract: We introduce a framework for modeling decisions with axioms that are
statements about decisions, e.g., ethical constraints. Using our framework we
define a taxonomy of decision axioms based on their structural properties and
demonstrate a tension between the use of axioms to make decisions and the use
of axioms to evaluate decisions which we call the Decision-Evaluation Paradox.
We argue that the Decision-Evaluation Paradox arises with realistic axiom
structures, and the paradox illuminates why one must be exceptionally careful
when training models on decision data or applying axioms to make and evaluate
decisions.

</details>


### [19] [DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents](https://arxiv.org/abs/2509.21842)
*Yansong Ning,Rui Liu,Jun Wang,Kai Chen,Wei Li,Jun Fang,Kan Zheng,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: DeepTravel introduces an autonomous travel planning agent using agentic reinforcement learning, able to plan, execute, and refine travel itineraries effectively, outperforming existing large language models in this domain.


<details>
  <summary>Details</summary>
Motivation: Improving travel planning agents by addressing limitations of hand-crafted prompts and fixed workflows to achieve greater flexibility and autonomy.

Method: Proposed an agentic reinforcement learning framework paired with a robust sandbox environment for training, a hierarchical reward system for spatiotemporal and detail validation, and a reply-augmented reinforcement learning approach.

Result: DeepTravel demonstrated superior performance in travel planning tasks, significantly outperforming state-of-the-art language models like OpenAI's o1/o3 and DeepSeek R1.

Conclusion: DeepTravel showcases the potential of small-size LLMs, combined with advanced training techniques, to create more capable and reliable travel planning agents for practical deployment.

Abstract: Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

</details>


### [20] [Reimagining Agent-based Modeling with Large Language Model Agents via Shachi](https://arxiv.org/abs/2509.21862)
*So Kuroki,Yingtao Tian,Kou Misaki,Takashi Ikegami,Takuya Akiba,Yujin Tang*

Main category: cs.AI

TL;DR: The paper presents Shachi, a methodology and framework to study emergent behaviors in multi-agent systems with large language models.


<details>
  <summary>Details</summary>
Motivation: Understanding and modeling emergent behaviors in multi-agent systems driven by large language models is difficult due to the lack of systematic experimental frameworks.

Method: The authors propose Shachi, a framework that decomposes an agent's policy into Configuration, Memory, and Tools, orchestrated by an LLM reasoning engine, enabling controlled experimentation.

Result: The study validates Shachi across a 10-task benchmark and simulates a real-world event (U.S. tariff shock), showing the methodâ€™s ability to align agent behavior with market reactions when the architecture is properly configured.

Conclusion: The paper offers an open-source, systematic foundation for evaluating LLM agents, setting the stage for more scientifically robust research in this field.

Abstract: The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.

</details>


### [21] [TRACE: Learning to Compute on Graphs](https://arxiv.org/abs/2509.21886)
*Ziyang Zheng,Jiaying Zhu,Jingyi Zhou,Qiang Xu*

Main category: cs.AI

TL;DR: This paper introduces TRACE, a novel approach to learn computational graph functionalities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing message-passing neural networks (MPNNs) and Transformer models cannot effectively model the position-aware, hierarchical nature of computational graphs.

Method: TRACE utilizes a Hierarchical Transformer for step-by-step computation and introduces function shift learning, predicting the discrepancy between true global functions and simple approximations.

Result: TRACE outperforms prior architectures in electronic circuit benchmarks, showcasing its superiority in understanding computational graphs.

Conclusion: TRACE's aligned architecture and decoupled learning objective provide a more robust framework for learning computational graph behaviors.

Abstract: Learning to compute, the ability to model the functional behavior of a
computational graph, is a fundamental challenge for graph representation
learning. Yet, the dominant paradigm is architecturally mismatched for this
task. This flawed assumption, central to mainstream message passing neural
networks (MPNNs) and their conventional Transformer-based counterparts,
prevents models from capturing the position-aware, hierarchical nature of
computation. To resolve this, we introduce \textbf{TRACE}, a new paradigm built
on an architecturally sound backbone and a principled learning objective.
First, TRACE employs a Hierarchical Transformer that mirrors the step-by-step
flow of computation, providing a faithful architectural backbone that replaces
the flawed permutation-invariant aggregation. Second, we introduce
\textbf{function shift learning}, a novel objective that decouples the learning
problem. Instead of predicting the complex global function directly, our model
is trained to predict only the \textit{function shift}, the discrepancy between
the true global function and a simple local approximation that assumes input
independence. We validate this paradigm on electronic circuits, one of the most
complex and economically critical classes of computational graphs. Across a
comprehensive suite of benchmarks, TRACE substantially outperforms all prior
architectures. These results demonstrate that our architecturally-aligned
backbone and decoupled learning objective form a more robust paradigm for the
fundamental challenge of learning to compute on graphs.

</details>


### [22] [GenesisGeo: Technical Report](https://arxiv.org/abs/2509.21896)
*Minfeng Zhu,Zi Wang,Sizhe Ji,Zhengtong Du,Junming Ke,Xiao Deng,Zanlang Yin,Xiuqi Huang,Heyu Wang,Wei Chen*

Main category: cs.AI

TL;DR: GenesisGeo is an automated theorem prover specializing in Euclidean geometry, achieving significant performance improvements and solving complex mathematical problems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance automated theorem proving in geometry by combining symbolic deduction and neuro-symbolic approaches.

Method: The approach involves a 120x acceleration of the symbolic deduction engine using theorem matching, C++ implementation, and leveraging Qwen3-0.6B-Base for neuro-symbolic reasoning.

Result: GenesisGeo solves 24 of 30 silver-level IMO problems with a single model and reaches gold-level performance with a dual-model ensemble.

Conclusion: GenesisGeo demonstrates the feasibility of advanced neuro-symbolic methods for tackling challenging geometry problems, setting new benchmarks in automated theorem proving.

Abstract: We present GenesisGeo, an automated theorem prover in Euclidean geometry. We
have open-sourced a large-scale geometry dataset of 21.8 million geometric
problems, over 3 million of which contain auxiliary constructions. Specially,
we significantly accelerate the symbolic deduction engine DDARN by 120x through
theorem matching, combined with a C++ implementation of its core components.
Furthermore, we build our neuro-symbolic prover, GenesisGeo, upon
Qwen3-0.6B-Base, which solves 24 of 30 problems (IMO silver medal level) in the
IMO-AG-30 benchmark using a single model, and achieves 26 problems (IMO gold
medal level) with a dual-model ensemble.

</details>


### [23] [DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop Scheduling](https://arxiv.org/abs/2509.21902)
*Ruiqi Chen,Yi Mei,Fangfang Zhang,Mengjie Zhang*

Main category: cs.AI

TL;DR: The paper presents DyRo-MCTS, a robust scheduling approach that integrates action robustness estimation into MCTS for dynamic job shop environments. It significantly improves offline policies and surpasses vanilla MCTS.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in dynamic job shop scheduling influenced by disruptions such as new job arrivals, where current machine learning methods and MCTS struggle to optimize under unpredictability.

Method: The authors propose DyRo-MCTS, which incorporates action robustness estimation into the MCTS algorithm to guide production states and improve adaptability to disturbances.

Result: Experimental results show DyRo-MCTS enhances the performance of offline-learned policies and consistently outperforms vanilla MCTS, with negligible online planning costs.

Conclusion: DyRo-MCTS makes robust scheduling decisions, achieving sustainable performance despite disturbances, improving on both short-term and long-term scheduling efficacy.

Abstract: Dynamic job shop scheduling, a fundamental combinatorial optimisation problem
in various industrial sectors, poses substantial challenges for effective
scheduling due to frequent disruptions caused by the arrival of new jobs.
State-of-the-art methods employ machine learning to learn scheduling policies
offline, enabling rapid responses to dynamic events. However, these offline
policies are often imperfect, necessitating the use of planning techniques such
as Monte Carlo Tree Search (MCTS) to improve performance at online decision
time. The unpredictability of new job arrivals complicates online planning, as
decisions based on incomplete problem information are vulnerable to
disturbances. To address this issue, we propose the Dynamic Robust MCTS
(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.
DyRo-MCTS guides the production environment toward states that not only yield
good scheduling outcomes but are also easily adaptable to future job arrivals.
Extensive experiments show that DyRo-MCTS significantly improves the
performance of offline-learned policies with negligible additional online
planning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across
various scheduling scenarios. Further analysis reveals that its ability to make
robust scheduling decisions leads to long-term, sustainable performance gains
under disturbances.

</details>


### [24] [Outlier Detection in Plantar Pressure: Human-Centered Comparison of Statistical Parametric Mapping and Explainable Machine Learning](https://arxiv.org/abs/2509.21943)
*Carlo Dindorf,Jonas Dully,Steven Simon,Dennis Perchthaler,Stephan Becker,Hannah Ehmann,Kjell Heitmann,Bernd Stetter,Christian Diers,Michael FrÃ¶hlich*

Main category: cs.AI

TL;DR: The study contrasts Statistical Parametric Mapping (SPM) and an explainable machine learning (ML) method for detecting outliers in plantar pressure datasets. The ML approach, using SHAP explanations, outperformed SPM in terms of accuracy while remaining interpretable and trustworthy to domain experts.


<details>
  <summary>Details</summary>
Motivation: Plantar pressure mapping datasets often contain outliers due to technical or procedural errors, requiring robust detection methods for reliable diagnostics and sports science applications.

Method: The study compared two methods: (i) an SPM approach sensitive to alignment and (ii) a convolutional neural network (CNN) explained via SHAP. Cross-validation and expert surveys gauged performance and explanation quality.

Result: The ML approach demonstrated high accuracy in outlier detection and surpassed SPM, which failed to classify certain meaningful variations. Both explanation methods were deemed clear and trustworthy by experts.

Conclusion: Explainable ML methods like CNN with SHAP present superior accuracy and equally interpretable analyses compared to SPM, promoting effective decision-making in plantar pressure data analysis.

Abstract: Plantar pressure mapping is essential in clinical diagnostics and sports
science, yet large heterogeneous datasets often contain outliers from technical
errors or procedural inconsistencies. Statistical Parametric Mapping (SPM)
provides interpretable analyses but is sensitive to alignment and its capacity
for robust outlier detection remains unclear. This study compares an SPM
approach with an explainable machine learning (ML) approach to establish
transparent quality-control pipelines for plantar pressure datasets. Data from
multiple centers were annotated by expert consensus and enriched with synthetic
anomalies resulting in 798 valid samples and 2000 outliers. We evaluated (i) a
non-parametric, registration-dependent SPM approach and (ii) a convolutional
neural network (CNN), explained using SHapley Additive exPlanations (SHAP).
Performance was assessed via nested cross-validation; explanation quality via a
semantic differential survey with domain experts. The ML model reached high
accuracy and outperformed SPM, which misclassified clinically meaningful
variations and missed true outliers. Experts perceived both SPM and SHAP
explanations as clear, useful, and trustworthy, though SPM was assessed less
complex. These findings highlight the complementary potential of SPM and
explainable ML as approaches for automated outlier detection in plantar
pressure data, and underscore the importance of explainability in translating
complex model outputs into interpretable insights that can effectively inform
decision-making.

</details>


### [25] [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)
*Zhimin Wang,Shaokang He,Duo Wu,Jinghe Wang,Linjia Kang,Jing Yu,Zhi Wang*

Main category: cs.AI

TL;DR: This paper introduces CoBel-World, a framework that enhances collaboration in large language model (LLM) agents by equipping them with collaborative belief modeling for reasoning about collaborators' intents.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of inconsistent planning and redundant communication by enabling LLMs to better infer collaborators' intents in partially observable environments.

Method: It introduces CoBel-World, which uses a symbolic belief language and zero-shot Bayesian-style belief updates to model the physical environment and collaborators' mental states, allowing for proactive miscoordination detection and adaptive communication.

Result: Experiments on TDW-MAT and C-WAH benchmarks showed a 22-60% reduction in communication costs and a 4-28% improvement in task completion efficiency compared to the strongest baseline.

Conclusion: Explicit and intent-aware belief modeling is critical for creating efficient and human-like collaboration in LLM-powered multi-agent systems.

Abstract: Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

</details>


### [26] [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
*Renqi Chen,Zeyin Tao,Jianming Guo,Jingzhe Zhu,Yiheng Peng,Qingqing Sun,Tianyi Zhang,Shuai Chen*

Main category: cs.AI

TL;DR: The paper introduces RISK, a framework to automate complex web interactions for e-commerce risk management by addressing limitations of traditional GUI agents and scraping methods.


<details>
  <summary>Details</summary>
Motivation: Traditional scraping methods and GUI agents are unable to handle dynamic, multi-step, and stateful interactions crucial for e-commerce risk management.

Method: The RISK framework comprises three components: (1) RISK-Data, a dataset of interaction trajectories; (2) RISK-Bench, a benchmark for evaluation; and (3) RISK-R1, a reinforcement fine-tuning framework addressing output formatting, single-step, multi-step, and task level improvements.

Result: RISK-R1 achieved a 6.8% improvement in offline single-step tasks, an 8.8% improvement in offline multi-step tasks, and a 70.5% top task success rate in online evaluations.

Conclusion: RISK advances automation in e-commerce risk management by providing a scalable and domain-specific solution to handle complex interactions surpassing existing methods.

Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.

</details>


### [27] [Bilinear relational structure fixes reversal curse and enables consistent model editing](https://arxiv.org/abs/2509.21993)
*Dong-Kyum Kim,Minsung Kim,Jea Kwon,Nakyeong Yang,Meeyoung Cha*

Main category: cs.AI

TL;DR: The paper investigates the 'reversal curse' in language models (LMs) and shows that it can be mitigated by training on relational knowledge datasets, which induce bilinear internal representations.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the 'reversal curse,' where language models fail to infer a reverse fact (e.g., 'B is A') from a learned fact ('A is B'), and explore how model representation impacts logical consistency, especially during knowledge editing.

Method: The method involves training LMs from scratch on synthetic datasets of relational knowledge graphs, analyzing whether bilinear relational structures emerge in hidden representations, and assessing the impact of these structures on inference and model editing.

Result: The paper finds that training on relational knowledge datasets leads to the emergence of bilinear structures in LMs, which alleviate the reversal curse. These structures also ensure that knowledge updates lead to logical consistency, with edits correctly propagating to reverse and dependent facts.

Conclusion: LMs can overcome the reversal curse and achieve logical consistency in editing if their internal representations have the suitable bilinear geometry, highlighting the importance of the representational basis for successful knowledge update methods.

Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen
fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a
fundamental limitation. We show that this is not an inherent failure but an
artifact of how models encode knowledge. By training LMs from scratch on a
synthetic dataset of relational knowledge graphs, we demonstrate that bilinear
relational structure emerges in their hidden representations. This structure
substantially alleviates the reversal curse, enabling LMs to infer unseen
reverse facts. Crucially, we also find that this bilinear structure plays a key
role in consistent model editing. When a fact is updated in a LM with this
structure, the edit correctly propagates to its reverse and other logically
dependent facts. In contrast, models lacking this representation not only
suffer from the reversal curse but also fail to generalize edits, further
introducing logical inconsistencies. Our results establish that training on a
relational knowledge dataset induces the emergence of bilinear internal
representations, which in turn enable LMs to behave in a logically consistent
manner after editing. This implies that the success of model editing depends
critically not just on editing algorithms but on the underlying
representational geometry of the knowledge being modified.

</details>


### [28] [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998)
*Hanlin Zhu,Tianyu Guo,Song Mei,Stuart Russell,Nikhil Ghosh,Alberto Bietti,Jiantao Jiao*

Main category: cs.AI

TL;DR: The paper introduces GSM-Agent, a benchmark focusing on agentic reasoning in LLMs to assess their ability to solve grade-school reasoning tasks using tools instead of advanced skills. It identifies shortcomings in agentic reasoning and proposes a method to enhance it.


<details>
  <summary>Details</summary>
Motivation: There is a lack of benchmarks isolating agentic reasoning in LLM agents since current tests mix basic reasoning skills with advanced capabilities like expert-level knowledge.

Method: GSM-Agent benchmark presents LLMs with grade-school reasoning tasks without premises and requires them to collect necessary information via tools. Agentic reasoning graphs are used to analyze agentic reasoning patterns.

Result: Frontier models like GPT-5 achieve only 67% on GSM-Agent tasks. Revisiting previously accessed nodes, a crucial reasoning pattern, is often absent in agentic reasoning across many models.

Conclusion: The tool-augmented test-time scaling method improves performance by encouraging revisits to nodes. GSM-Agent and the reasoning framework could become essential for future studies in advancing agentic reasoning in LLM agents.

Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.

</details>


### [29] [The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging](https://arxiv.org/abs/2509.22034)
*Xiaochong Lan,Yu Zheng,Shiteng Cao,Yong Li*

Main category: cs.AI

TL;DR: The study explores model merging as a method to calibrate reasoning abilities and computational costs in large language models, offering tunable performance through arithmetically combining weights.


<details>
  <summary>Details</summary>
Motivation: The growing demand for customizable reasoning capabilities in large language models highlights the need for efficient methods to balance reasoning depth and computational cost.

Method: The authors conducted a large-scale empirical study to evaluate model merging techniques across reasoning benchmarks, systematically varying merging strengths to understand accuracy-efficiency trade-offs.

Result: Model merging demonstrated controllable calibration of reasoning vs. efficiency trade-offs and instances of Pareto Improvement, combining increased accuracy with reduced token consumption.

Conclusion: Model merging serves as a practical, training-free method for tailoring LLM reasoning profiles, offering valuable insights for diverse application requirements.

Abstract: The growing demand for large language models (LLMs) with tunable reasoning
capabilities in many real-world applications highlights a critical need for
methods that can efficiently produce a spectrum of models balancing reasoning
depth and computational cost. Model merging has emerged as a promising,
training-free technique to address this challenge by arithmetically combining
the weights of a general-purpose model with a specialized reasoning model.
While various merging techniques exist, their potential to create a spectrum of
models with fine-grained control over reasoning abilities remains largely
unexplored. This work presents a large-scale empirical study evaluating a range
of model merging techniques across multiple reasoning benchmarks. We
systematically vary merging strengths to construct accuracy-efficiency curves,
providing the first comprehensive view of the tunable performance landscape.
Our findings reveal that model merging offers an effective and controllable
method for calibrating the trade-off between reasoning accuracy and token
efficiency, even when parent models have highly divergent weight spaces.
Crucially, we identify instances of Pareto Improvement, where a merged model
achieves both higher accuracy and lower token consumption than one of its
parents. Our study provides the first comprehensive analysis of this tunable
space, offering practical guidelines for creating LLMs with specific reasoning
profiles to meet diverse application demands.

</details>


### [30] [A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning](https://arxiv.org/abs/2509.22044)
*Ziqi Wang,Boye Niu,Zhongli Li,Linghui Meng,Jing Liu,Zhi Zheng,Tong Xu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.AI

TL;DR: The paper introduces the A2R framework, a two-stage reasoning system that boosts model performance in solving complex tasks. It achieves this by separating solution exploration and synthesis, leading to better accuracy and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the gap between a model's realized and inherent capabilities in solving complex reasoning tasks, especially the disparity evident between single-attempt performance and latent potential.

Method: The A2R framework employs a two-stage process: 1) An "explorer" model generates multiple potential solutions in parallel via repeated sampling. 2) A "synthesizer" model refines and integrates these solutions into a more accurate output. Additionally, the framework offers an asymmetric scaling strategy (A2R-Efficient) that uses smaller models for exploration and larger models for synthesis.

Result: The framework significantly improves model performance, with the Qwen3-8B-distill model showing a 75% boost over its baseline. The A2R-Efficient variant outperforms a monolithic Qwen3-32B model at nearly 30% lower cost.

Conclusion: A2R is an effective plug-and-play reasoning framework that enhances both performance and cost-efficiency for real-world complex reasoning tasks.

Abstract: Recent Large Reasoning Models have achieved significant improvements in
complex task-solving capabilities by allocating more computation at the
inference stage with a "thinking longer" paradigm. Even as the foundational
reasoning capabilities of models advance rapidly, the persistent gap between a
model's performance in a single attempt and its latent potential, often
revealed only across multiple solution paths, starkly highlights the disparity
between its realized and inherent capabilities. To address this, we present
A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge
the gap between a model's potential and its actual performance. In this
framework, an "explorer" model first generates potential solutions in parallel
through repeated sampling. Subsequently,a "synthesizer" model integrates these
references for a more refined, second stage of reasoning. This two-stage
process allows computation to be scaled orthogonally to existing sequential
methods. Our work makes two key innovations: First, we present A2R as a
plug-and-play parallel reasoning framework that explicitly enhances a model's
capabilities on complex questions. For example, using our framework, the
Qwen3-8B-distill model achieves a 75% performance improvement compared to its
self-consistency baseline. Second, through a systematic analysis of the
explorer and synthesizer roles, we identify an effective asymmetric scaling
paradigm. This insight leads to A2R-Efficient, a "small-to-big" variant that
combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration
surpasses the average performance of a monolithic Qwen3-32B model at a nearly
30% lower cost. Collectively, these results show that A2R is not only a
performance-boosting framework but also an efficient and practical solution for
real-world applications.

</details>


### [31] [Generalizing Multi-Objective Search via Objective-Aggregation Functions](https://arxiv.org/abs/2509.22085)
*Hadar Peer,Eyal Weiss,Ron Alterovitz,Oren Salzman*

Main category: cs.AI

TL;DR: This paper introduces a generalized MOS framework using aggregation functions for hidden objectives, compatible with standard MOS algorithms and applicable to various robotics scenarios.


<details>
  <summary>Details</summary>
Motivation: Real-world robotic systems require optimization across multiple conflicting objectives, and existing MOS algorithms often cannot address the complex interactions between these objectives.

Method: The authors propose a framework that uses aggregation functions for hidden objectives, allowing existing MOS algorithms to function after extending their core operations.

Result: The extended MOS algorithms showed significant performance improvement over their vanilla versions in diverse robotics planning scenarios such as motion planning, manipulation, medical systems, and inspection planning.

Conclusion: The proposed approach successfully generalizes MOS problem formulation, demonstrating scalability and efficiency across various robotics applications while outperforming traditional methods.

Abstract: Multi-objective search (MOS) has become essential in robotics, as real-world
robotic systems need to simultaneously balance multiple, often conflicting
objectives. Recent works explore complex interactions between objectives,
leading to problem formulations that do not allow the usage of out-of-the-box
state-of-the-art MOS algorithms. In this paper, we suggest a generalized
problem formulation that optimizes solution objectives via aggregation
functions of hidden (search) objectives. We show that our formulation supports
the application of standard MOS algorithms, necessitating only to properly
extend several core operations to reflect the specific aggregation functions
employed. We demonstrate our approach in several diverse robotics planning
problems, spanning motion-planning for navigation, manipulation and planning fr
medical systems under obstacle uncertainty as well as inspection planning, and
route planning with different road types. We solve the problems using
state-of-the-art MOS algorithms after properly extending their core operations,
and provide empirical evidence that they outperform by orders of magnitude the
vanilla versions of the algorithms applied to the same problems but without
objective aggregation.

</details>


### [32] [Ground-Truthing AI Energy Consumption: Validating CodeCarbon Against External Measurements](https://arxiv.org/abs/2509.22092)
*Raphael Fischer*

Main category: cs.AI

TL;DR: The paper evaluates the accuracy of AI energy estimation tools, revealing errors of up to 40% and providing guidelines for improvement.


<details>
  <summary>Details</summary>
Motivation: The rapid development of ML and AI has environmental impacts, necessitating reliable energy consumption and carbon emission estimation tools.

Method: The study uses a validation framework to systematically compare static and dynamic energy estimation tools against ground-truth measurements from hundreds of AI experiments.

Result: Energy estimation tools often deviate by up to 40%, highlighting inaccuracies in current methodologies.

Conclusion: Empirical insights validate existing tools, propose improvements, and contribute to sustainable AI practices.

Abstract: Although machine learning (ML) and artificial intelligence (AI) present
fascinating opportunities for innovation, their rapid development is also
significantly impacting our environment. In response to growing
resource-awareness in the field, quantification tools such as the ML Emissions
Calculator and CodeCarbon were developed to estimate the energy consumption and
carbon emissions of running AI models. They are easy to incorporate into AI
projects, however also make pragmatic assumptions and neglect important
factors, raising the question of estimation accuracy. This study systematically
evaluates the reliability of static and dynamic energy estimation approaches
through comparisons with ground-truth measurements across hundreds of AI
experiments. Based on the proposed validation framework, investigative insights
into AI energy demand and estimation inaccuracies are provided. While generally
following the patterns of AI energy consumption, the established estimation
approaches are shown to consistently make errors of up to 40%. By providing
empirical evidence on energy estimation quality and errors, this study
establishes transparency and validates widely used tools for sustainable AI
development. It moreover formulates guidelines for improving the
state-of-the-art and offers code for extending the validation to other domains
and tools, thus making important contributions to resource-aware ML and AI
sustainability research.

</details>


### [33] [Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach](https://arxiv.org/abs/2509.22137)
*Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: Log2Plan introduces a two-level planning framework for GUI automation, addressing issues of fragility and inefficiency in existing planner-executor systems using task mining and structured task mapping.


<details>
  <summary>Details</summary>
Motivation: Existing planner-executor agents for GUI automation struggle with poor generalization, inefficiency, and a lack of coherence for long tasks due to reliance on single-shot reasoning or static plans.

Method: Log2Plan uses a two-level planning framework: high-level plans are generated from a structured task dictionary based on user commands, and task mining identifies user-specific patterns. Plans are grounded into low-level actions in real time using GUI context.

Result: Log2Plan improved success rates and reduced execution time in evaluations with 200 real-world tasks, achieving over 60% success in long-horizon task sequences.

Conclusion: Log2Plan is robust and adaptable, excelling in complex and dynamic GUI automation scenarios by leveraging structured planning and user-specific task mining.

Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.

</details>


### [34] [Clinical Uncertainty Impacts Machine Learning Evaluations](https://arxiv.org/abs/2509.22242)
*Simone Lionetti,Fabian GrÃ¶ger,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Alexander A. Navarini,Marc Pouly*

Main category: cs.AI

TL;DR: Clinical dataset labels often come with uncertainties due to annotator disagreements and varying confidence levels. The paper advocates for using probabilistic metrics in machine-learning evaluations to account for this uncertainty.


<details>
  <summary>Details</summary>
Motivation: Current aggregation methods, like majority voting, fail to reflect annotation variability, leading to skewed model evaluations in clinical machine learning tasks.

Method: The authors propose using probabilistic evaluation metrics that work on label distributions. These metrics are independent of how annotations are generated, such as counting, confidence ratings, or probabilistic models.

Result: In experiments on medical imaging, incorporating confidence in binary labels significantly impacted model rankings, showing the usefulness of the proposed metrics.

Conclusion: The paper recommends releasing raw dataset annotations and adopting uncertainty-aware metrics to improve the reliability of performance evaluations in clinical data applications.

Abstract: Clinical dataset labels are rarely certain as annotators disagree and
confidence is not uniform across cases. Typical aggregation procedures, such as
majority voting, obscure this variability. In simple experiments on medical
imaging benchmarks, accounting for the confidence in binary labels
significantly impacts model rankings. We therefore argue that machine-learning
evaluations should explicitly account for annotation uncertainty using
probabilistic metrics that directly operate on distributions. These metrics can
be applied independently of the annotations' generating process, whether
modeled by simple counting, subjective confidence ratings, or probabilistic
response models. They are also computationally lightweight, as closed-form
expressions have linear-time implementations once examples are sorted by model
score. We thus urge the community to release raw annotations for datasets and
to adopt uncertainty-aware evaluation so that performance estimates may better
reflect clinical data.

</details>


### [35] [Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing](https://arxiv.org/abs/2509.22255)
*Syed Mahbubul Huq,Daniel Brito,Daniel Sikar,Rajesh Mojumder*

Main category: cs.AI

TL;DR: The paper evaluates the capabilities of Large Language Models (LLMs) in solving combinatorial optimization tasks, specifically the 2D bin-packing problem, by combining them with evolutionary algorithms to iteratively refine heuristics.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of Large Language Models (LLMs) in solving specialized domains like combinatorial optimization, and evaluate their efficiency compared to traditional algorithms.

Method: A systematic methodology is designed that integrates LLMs with evolutionary algorithms to iteratively develop and improve heuristic solutions for the 2D bin-packing problem.

Result: Experiments showed that LLMs like GPT-4o outperformed traditional methods by achieving optimal solutions within two iterations, reducing average bin usage from 16 to 15 and increasing space utilization efficiency from 0.76-0.78 to 0.83.

Conclusion: LLMs, when combined with evolutionary algorithms, demonstrate significant potential for solving complex optimization tasks efficiently, contributing to benchmarks in this specialized field.

Abstract: This paper presents an evaluation framework for assessing Large Language
Models' (LLMs) capabilities in combinatorial optimization, specifically
addressing the 2D bin-packing problem. We introduce a systematic methodology
that combines LLMs with evolutionary algorithms to generate and refine
heuristic solutions iteratively. Through comprehensive experiments comparing
LLM generated heuristics against traditional approaches (Finite First-Fit and
Hybrid First-Fit), we demonstrate that LLMs can produce more efficient
solutions while requiring fewer computational resources. Our evaluation reveals
that GPT-4o achieves optimal solutions within two iterations, reducing average
bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78
to 0.83. This work contributes to understanding LLM evaluation in specialized
domains and establishes benchmarks for assessing LLM performance in
combinatorial optimization tasks.

</details>


### [36] [InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.22261)
*Guanghao Zhu,Zhitian Hou,Zeyu Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: This study introduces two medical-specific multimodal large language models (MLLMs)â€”InfiMed-Foundation-1.7B and InfiMed-Foundation-4Bâ€”that outperform existing models in medical visual question answering and diagnostic tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of general-purpose MLLMs in handling domain-specific medical tasks, such as their lack of specialized knowledge, hallucinations, and inefficiency in utilizing large-scale medical datasets.

Method: The authors created specialized MLLMs by combining high-quality general-purpose and medical multimodal data, implementing a five-dimensional quality assessment framework for dataset curation, and fine-tuning the models through a three-stage process. They optimized training efficiency using techniques like low-to-high image resolution and multimodal sequence packing.

Result: InfiMed-Foundation-1.7B outperforms Qwen2.5VL-3B, and InfiMed-Foundation-4B exceeds the performance of HuatuoGPT-V-7B and MedGemma-27B-IT in medical visual question answering and diagnostic tasks.

Conclusion: By tackling challenges in data quality, training efficiency, and domain-specific knowledge extraction, this research enables more reliable and effective AI-driven healthcare solutions.

Abstract: Multimodal large language models (MLLMs) have shown remarkable potential in
various domains, yet their application in the medical field is hindered by
several challenges. General-purpose MLLMs often lack the specialized knowledge
required for medical tasks, leading to uncertain or hallucinatory responses.
Knowledge distillation from advanced models struggles to capture
domain-specific expertise in radiology and pharmacology. Additionally, the
computational cost of continual pretraining with large-scale medical data poses
significant efficiency challenges. To address these issues, we propose
InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs
designed to deliver state-of-the-art performance in medical applications. We
combined high-quality general-purpose and medical multimodal data and proposed
a novel five-dimensional quality assessment framework to curate high-quality
multimodal medical datasets. We employ low-to-high image resolution and
multimodal sequence packing to enhance training efficiency, enabling the
integration of extensive medical data. Furthermore, a three-stage supervised
fine-tuning process ensures effective knowledge extraction for complex medical
tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B
outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B
and MedGemma-27B-IT, demonstrating superior performance in medical visual
question answering and diagnostic tasks. By addressing key challenges in data
quality, training efficiency, and domain-specific knowledge extraction, our
work paves the way for more reliable and effective AI-driven solutions in
healthcare. InfiMed-Foundation-4B model is available at
\href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.

</details>


### [37] [Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models](https://arxiv.org/abs/2509.22284)
*Aleksandar TerziÄ‡,Nicolas Menet,Michael Hersche,Thomas Hofmann,Abbas Rahimi*

Main category: cs.AI

TL;DR: The paper introduces PD-SSM, a structured sparse parametrization of transition matrices in state-space models (SSMs), optimizing expressivity and computational efficiency for tasks like finite-state automata (FSA) state tracking.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of modern SSMs which struggle between computational efficiency and expressivity for finite-state automata (FSA) state tracking.

Method: Proposes PD-SSM, which parametrizes the transition matrix using a product of a column one-hot matrix (P) and a complex-valued diagonal matrix (D). This structure ensures BIBO stability, linear scaling of computational cost, and enhanced model expressivity.

Result: Theoretical guarantees of optimal FSA emulation with reduced dimensions, significant performance improvements in state tracking tasks, and competitive results in multiclass time-series classification.

Conclusion: The PD-SSM approach balances computational efficiency, scalability, and expressivity, with strong empirical and theoretical performance even in hybrid architectures involving Transformers for complex tasks.

Abstract: Modern state-space models (SSMs) often utilize transition matrices which
enable efficient computation but pose restrictions on the model's expressivity,
as measured in terms of the ability to emulate finite-state automata (FSA).
While unstructured transition matrices are optimal in terms of expressivity,
they come at a prohibitively high compute and memory cost even for moderate
state sizes. We propose a structured sparse parametrization of transition
matrices in SSMs that enables FSA state tracking with optimal state size and
depth, while keeping the computational cost of the recurrence comparable to
that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix
as the product of a column one-hot matrix ($P$) and a complex-valued diagonal
matrix ($D$). Consequently, the computational cost of parallel scans scales
linearly with the state size. Theoretically, the model is BIBO-stable and can
emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout
of size $N \times N$, significantly improving on all current structured SSM
guarantees. Experimentally, the model significantly outperforms a wide
collection of modern SSM variants on various FSA state tracking tasks. On
multiclass time-series classification, the performance is comparable to that of
neural controlled differential equations, a paradigm explicitly built for
time-series analysis. Finally, we integrate PD-SSM into a hybrid
Transformer-SSM architecture and demonstrate that the model can effectively
track the states of a complex FSA in which transitions are encoded as a set of
variable-length English sentences. The code is available at
https://github.com/IBM/expressive-sparse-state-space-model

</details>


### [38] [Large Language Models as Nondeterministic Causal Models](https://arxiv.org/abs/2509.22297)
*Sander Beckers*

Main category: cs.AI

TL;DR: The paper critiques existing methods for generating counterfactuals in large language models (LLMs) and proposes a simpler, agnostic approach that works with black-box LLMs.


<details>
  <summary>Details</summary>
Motivation: Examine the limitations of existing methods in generating counterfactuals for LLM behavior and introduce a simpler, more universal approach.

Method: The paper proposes representing LLMs as nondeterministic causal models to derive counterfactuals without requiring modifications to the LLM's implementation.

Result: The proposed method simplifies counterfactual generation and is compatible with any black-box LLM, overcoming ambiguities in existing methods.

Conclusion: The work establishes a theoretical framework for reasoning about LLM counterfactuals, enabling application-specific methods for diverse use cases.

Abstract: Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first
time, a method for generating counterfactuals of probabilistic Large Language
Models. Such counterfactuals tell us what would - or might - have been the
output of an LLM if some factual prompt ${\bf x}$ had been ${\bf x}^*$ instead.
The ability to generate such counterfactuals is an important necessary step
towards explaining, evaluating, and comparing, the behavior of LLMs. I argue,
however, that the existing method rests on an ambiguous interpretation of LLMs:
it does not interpret LLMs literally, for the method involves the assumption
that one can change the implementation of an LLM's sampling process without
changing the LLM itself, nor does it interpret LLMs as intended, for the method
involves explicitly representing a nondeterministic LLM as a deterministic
causal model. I here present a much simpler method for generating
counterfactuals that is based on an LLM's intended interpretation by
representing it as a nondeterministic causal model instead. The advantage of my
simpler method is that it is directly applicable to any black-box LLM without
modification, as it is agnostic to any implementation details. The advantage of
the existing method, on the other hand, is that it directly implements the
generation of a specific type of counterfactuals that is useful for certain
purposes, but not for others. I clarify how both methods relate by offering a
theoretical foundation for reasoning about counterfactuals in LLMs based on
their intended semantics, thereby laying the groundwork for novel
application-specific methods for generating counterfactuals.

</details>


### [39] [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
*Hieu Tran,Zonghai Yao,Nguyen Luong Tran,Zhichao Yang,Feiyun Ouyang,Shuo Han,Razieh Rahimi,Hong Yu*

Main category: cs.AI

TL;DR: The paper introduces PRIME, a reasoning framework mimicking human dual-process cognition, integrating System 1 (fast) and System 2 (deliberate) methods for enhanced LLM reasoning, achieving competitive results with top closed-source models.


<details>
  <summary>Details</summary>
Motivation: To enable open-source LLMs to perform complex, knowledge-intensive reasoning tasks, competing with advanced closed-source models like GPT-4.

Method: The PRIME framework employs a Quick Thinking Agent (System 1) for rapid answers and a System 2 pipeline with specialized agents for structured reasoning when uncertainty arises.

Result: PRIME enables open-source LLMs to perform on par with models like GPT-4 in benchmarks involving multi-hop and knowledge-grounded reasoning.

Conclusion: PRIME provides a scalable, effective approach to enhancing reasoning capabilities in LLMs by incorporating a dual-process cognitive model.

Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

</details>


### [40] [Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents](https://arxiv.org/abs/2509.22391)
*Jiaqi Shao,Yuxiang Lin,Munish Prasad Lohani,Yufeng Miao,Bing Luo*

Main category: cs.AI

TL;DR: The paper introduces SeekBench, a benchmark designed to evaluate the reasoning and decision-making capabilities of LLM-based search agents during open-domain QA tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLM search agents' reasoning abilities and epistemic competence beyond final answer accuracy.

Method: The authors developed SeekBench, consisting of 190 expert-annotated traces with over 1,800 response steps, to perform a detailed, evidence-based step-level analysis.

Result: SeekBench evaluates agents on their evidence-based reasoning, adaptive search reformulation, and calibration of evidence sufficiency.

Conclusion: SeekBench provides a novel benchmark for assessing and improving the epistemic competence of LLM search agents in open-domain QA scenarios.

Abstract: Recent work has explored training Large Language Model (LLM) search agents
with reinforcement learning (RL) for open-domain question answering (QA).
However, most evaluations focus solely on final answer accuracy, overlooking
how these agents reason with and act on external evidence. We introduce
SeekBench, the first benchmark for evaluating the \textit{epistemic competence}
of LLM search agents through step-level analysis of their response traces.
SeekBench comprises 190 expert-annotated traces with over 1,800 response steps
generated by LLM search agents, each enriched with evidence annotations for
granular analysis of whether agents (1) generate reasoning steps grounded in
observed evidence, (2) adaptively reformulate searches to recover from
low-quality results, and (3) have proper calibration to correctly assess
whether the current evidence is sufficient for providing an answer.

</details>


### [41] [EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer](https://arxiv.org/abs/2509.22407)
*Zhehao Dong,Xiaofeng Wang,Zheng Zhu,Yirui Wang,Yang Wang,Yukun Zhou,Boyuan Wang,Chaojun Ni,Runqi Ouyang,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang*

Main category: cs.AI

TL;DR: This paper introduces EMMA, a framework combining a video generation model (DreamTransfer) and a dynamic training strategy (AdaMix) to enhance robot manipulation generalization using both real and generated video data.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of collecting diverse real-world robot manipulation data, which is expensive and time-consuming, leading to a need for methods that allow robust generalization without extensive data collection.

Method: The method combines DreamTransfer, a diffusion Transformer that generates geometrically consistent robot manipulation videos with controlled visual editing, and AdaMix, a training technique that prioritizes difficult samples for more effective policy optimization.

Result: The proposed framework outperforms previous video generation methods in consistency and accuracy. VLAs trained with DreamTransfer-generated data show significant improvements in generalizing to unseen object categories and domains, achieving over 200% relative improvement in zero-shot robot manipulation tasks and an additional 13% when using AdaMix.

Conclusion: EMMA demonstrates that combining advanced video generation and hard-sample-aware training strategies can significantly boost robot manipulation capabilities, enabling enhanced generalization in visual domains with limited real-world demonstrations.

Abstract: Vision-language-action (VLA) models increasingly rely on diverse training
data to achieve robust generalization. However, collecting large-scale
real-world robot manipulation data across varied object appearances and
environmental conditions remains prohibitively time-consuming and expensive. To
overcome this bottleneck, we propose Embodied Manipulation Media Adaptation
(EMMA), a VLA policy enhancement framework that integrates a generative data
engine with an effective training pipeline. We introduce DreamTransfer, a
diffusion Transformer-based framework for generating multi-view consistent,
geometrically grounded embodied manipulation videos. DreamTransfer enables
text-controlled visual editing of robot videos, transforming foreground,
background, and lighting conditions without compromising 3D structure or
geometrical plausibility. Furthermore, we explore hybrid training with real and
generated data, and introduce AdaMix, a hard-sample-aware training strategy
that dynamically reweights training batches to focus optimization on
perceptually or kinematically challenging samples. Extensive experiments show
that videos generated by DreamTransfer significantly outperform prior video
generation methods in multi-view consistency, geometric fidelity, and
text-conditioning accuracy. Crucially, VLAs trained with generated data enable
robots to generalize to unseen object categories and novel visual domains using
only demonstrations from a single appearance. In real-world robotic
manipulation tasks with zero-shot visual domains, our approach achieves over a
200% relative performance gain compared to training on real data alone, and
further improves by 13% with AdaMix, demonstrating its effectiveness in
boosting policy generalization.

</details>


### [42] [GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation](https://arxiv.org/abs/2509.22460)
*Shichao Weng,Zhiqiang Wang,Yuhua Zhou,Rui Lu,Ting Liu,Zhiyang Teng,Xiaozhang Liu,Hanmeng Liu*

Main category: cs.AI

TL;DR: GeoSketch is a neural-symbolic framework for geometric problem-solving, integrating perception, symbolic reasoning, and dynamic actions for iterative visuospatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing Multimodal Large Language Models lack the ability to dynamically manipulate geometric diagrams, a key element of human reasoning using auxiliary lines and transformations.

Method: GeoSketch consists of three modules: (1) perception transforms diagrams into structured logic, (2) symbolic reasoning decides deductive steps, and (3) sketch actions update diagrams dynamically. Training uses supervised fine-tuning and reinforcement learning.

Result: GeoSketch outperforms strong multimodal LLM baselines in reasoning accuracy and problem-solving success using a benchmark with 390 dynamic geometry problems.

Conclusion: GeoSketch advances multimodal reasoning by incorporating hierarchical reasoning, executable actions, and symbolic verification, establishing a framework for handling complex visuospatial tasks.

Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large
Language Models (MLLMs), requiring not only the joint interpretation of text
and diagrams but also iterative visuospatial reasoning. While existing
approaches process diagrams as static images, they lack the capacity for
dynamic manipulation - a core aspect of human geometric reasoning involving
auxiliary line construction and affine transformations. We present GeoSketch, a
neural-symbolic framework that recasts geometric reasoning as an interactive
perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module
that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning
module that applies geometric theorems to decide the next deductive step, and
(3) a Sketch Action module that executes operations such as drawing auxiliary
lines or applying transformations, thereby updating the diagram in a closed
loop. To train this agent, we develop a two-stage pipeline: supervised
fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement
learning with dense, symbolic rewards to enhance robustness and strategic
exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a
high-quality set of 390 geometry problems requiring auxiliary construction or
affine transformations. Experiments on strong MLLM baselines demonstrate that
GeoSketch significantly improves stepwise reasoning accuracy and
problem-solving success over static perception methods. By unifying
hierarchical decision-making, executable visual actions, and symbolic
verification, GeoSketch advances multimodal reasoning from static
interpretation to dynamic, verifiable interaction, establishing a new
foundation for solving complex visuospatial problems.

</details>


### [43] [InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios](https://arxiv.org/abs/2509.22502)
*Chenglin Yu,Yang Yu,Songmiao Wang,Yucheng Wang,Yifan Yang,Jinjia Li,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: The paper introduces InfiAgent, a multi-agent framework designed to enhance scalability and cost-effectiveness of LLM agents via automating system decomposition, dual audits, and self-evolution mechanisms. It outperforms existing frameworks and can create papers recognized by experts.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from inefficiencies in current LLM agent development, requiring expertise, hand-crafted workflows, and prompts that impede scalability and practicality across diverse industries.

Method: InfiAgent introduces innovations like hierarchical agent system decomposition, a dual-audit mechanism, optimized task-agent matching, and self-evolution capabilities to autonomously adapt to tasks, inefficiencies, or optimization needs.

Result: InfiAgent achieves a 9.9% performance increase over ADAS and is shown to successfully assist in creating scientifically credible papers, validated by acceptance at top IEEE conferences.

Conclusion: InfiAgent proves to be a scalable, efficient, and quality-driven multi-agent framework suitable for infinite scenarios, enhancing both performance and practicality for industrial applications.

Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities
in organizing and executing complex tasks, and many such agents are now widely
used in various application scenarios. However, developing these agents
requires carefully designed workflows, carefully crafted prompts, and iterative
tuning, which requires LLM techniques and domain-specific expertise. These
hand-crafted limitations hinder the scalability and cost-effectiveness of LLM
agents across a wide range of industries. To address these challenges, we
propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that
can be applied to \textbf{infi}nite scenarios, which introduces several key
innovations: a generalized "agent-as-a-tool" mechanism that automatically
decomposes complex agents into hierarchical multi-agent systems; a dual-audit
mechanism that ensures the quality and stability of task completion; an agent
routing function that enables efficient task-agent matching; and an agent
self-evolution mechanism that autonomously restructures the agent DAG based on
new tasks, poor performance, or optimization opportunities. Furthermore,
InfiAgent's atomic task design supports agent parallelism, significantly
improving execution efficiency. This framework evolves into a versatile
pyramid-like multi-agent system capable of solving a wide range of problems.
Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\%
higher performance compared to ADAS (similar auto-generated agent framework),
while a case study of the AI research assistant InfiHelper shows that it
generates scientific papers that have received recognition from human reviewers
at top-tier IEEE conferences.

</details>


### [44] [Estimating the Empowerment of Language Model Agents](https://arxiv.org/abs/2509.22504)
*Jinyeop Song,Jeff Gore,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: The paper introduces a novel evaluation framework using empowerment (mutual information between actions and future states) for assessing the performance of language model agents.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation approaches for LM agents are benchmark-centric and require substantial human effort. A scalable, open-ended evaluation method is needed.

Method: EELMA (Estimating Empowerment of Language Model Agents) calculates empowerment from multi-turn text interactions, validated on language games and web-browsing scenarios.

Result: Empowerment correlates with task performance, and environmental and agentic factors influence its estimation. High empowerment moments are linked to general capabilities.

Conclusion: Empowerment offers a promising metric to assess LM agents in complex, open-ended settings, providing insights into their capabilities and performance.

Abstract: As language model (LM) agents become more capable and gain broader access to
real-world tools, there is a growing need for scalable evaluation frameworks of
agentic capability. However, conventional benchmark-centric evaluations are
costly to design and require human designers to come up with valid tasks that
translate into insights about general model capabilities. In this work, we
propose information-theoretic evaluation based on empowerment, the mutual
information between an agent's actions and future states, as an open-ended
method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of
Language Model Agents), an algorithm for approximating effective empowerment
from multi-turn text interactions. We validate EELMA on both language games and
scaled-up realistic web-browsing scenarios. We find that empowerment strongly
correlates with average task performance, characterize the impact of
environmental complexity and agentic factors such as chain-of-thought, model
scale, and memory length on estimated empowerment, and that high empowerment
states and actions are often pivotal moments for general capabilities.
Together, these results demonstrate empowerment as an appealing general-purpose
metric for evaluating and monitoring LM agents in complex, open-ended settings.

</details>


### [45] [TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments](https://arxiv.org/abs/2509.22516)
*Rakesh Thakur,Shivaansh Kaushik,Gauri Chopra,Harsh Rohilla*

Main category: cs.AI

TL;DR: TrueGradeAI introduces an AI-driven digital examination framework to address traditional assessment drawbacks. It combines handwriting preservation, explainable automation, and fairness-focused grading using advanced technologies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the limitations of paper-based exams, such as environmental impact, grading inefficiencies, and evaluator bias, while preserving handwriting and ensuring fairness.

Method: It employs secure tablet-based handwriting capture, transformer-based OCR for text transcription, and a retrieval-augmented pipeline with a large language model for scoring, incorporating explicit reasoning and evidence-linked grading.

Result: The system reduces logistical complexities, environmental impact, grading delays, and evaluator bias while ensuring transparent, scalable, and reusable evaluation processes.

Conclusion: TrueGradeAI represents a significant step forward in assessment technology by combining scalability, transparency, environmental benefits, and fairness, while building a knowledge base and auditing capabilities.

Abstract: This paper introduces TrueGradeAI, an AI-driven digital examination framework
designed to overcome the shortcomings of traditional paper-based assessments,
including excessive paper usage, logistical complexity, grading delays, and
evaluator bias. The system preserves natural handwriting by capturing stylus
input on secure tablets and applying transformer-based optical character
recognition for transcription. Evaluation is conducted through a
retrieval-augmented pipeline that integrates faculty solutions, cache layers,
and external references, enabling a large language model to assign scores with
explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems
that primarily digitize responses, TrueGradeAI advances the field by
incorporating explainable automation, bias mitigation, and auditable grading
trails. By uniting handwriting preservation with scalable and transparent
evaluation, the framework reduces environmental costs, accelerates feedback
cycles, and progressively builds a reusable knowledge base, while actively
working to mitigate grading bias and ensure fairness in assessment.

</details>


### [46] [Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](https://arxiv.org/abs/2509.22613)
*Siwei Wang,Yifei Shen,Haoran Sun,Shi Feng,Shang-Hua Teng,Li Dong,Yaru Hao,Wei Chen*

Main category: cs.AI

TL;DR: The study analyzes reinforcement learning (RL) effects on Large Language Models (LLMs) through graph-based abstraction, comparing policy gradient (PG) and Q-learning.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical advantages and limitations of RL methods in enhancing LLM planning capabilities, addressing gaps in the theoretical basis.

Method: Theoretical analyses of RL strategies (policy gradient and Q-learning) using a graph-based abstraction, along with practical validation on the Blocksworld benchmark.

Result: RL enables proper planning through exploration, improving generalization. Q-learning preserves output diversity and supports off-policy learning, while PG suffers from diversity collapse. However, reward design is critical to avoid reward hacking.

Conclusion: Q-learning offers substantial benefits over policy gradient methods for better planning in LLMs, though careful reward design is essential to avoid pitfalls like reward hacking.

Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the
planning capabilities of Large Language Models (LLMs), yet the theoretical
basis for their effectiveness remains elusive. In this work, we investigate
RL's benefits and limitations through a tractable graph-based abstraction,
focusing on policy gradient (PG) and Q-learning methods. Our theoretical
analyses reveal that supervised fine-tuning (SFT) may introduce
co-occurrence-based spurious solutions, whereas RL achieves correct planning
primarily through exploration, underscoring exploration's role in enabling
better generalization. However, we also show that PG suffers from diversity
collapse, where output diversity decreases during training and persists even
after perfect accuracy is attained. By contrast, Q-learning provides two key
advantages: off-policy learning and diversity preservation at convergence. We
further demonstrate that careful reward design is necessary to prevent reward
hacking in Q-learning. Finally, applying our framework to the real-world
planning benchmark Blocksworld, we confirm that these behaviors manifest in
practice.

</details>


### [47] [REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model](https://arxiv.org/abs/2509.22518)
*Bo Li,Guanzhi Deng,Ronghao Chen,Junrong Yue,Shuo Zhang,Qinghua Zhao,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: The paper introduces the concept of a "Reasoning Manifold" and proposes the REMA framework to analyze reasoning failures in Large Language Models (LLMs) through geometric deviations in their internal representations.


<details>
  <summary>Details</summary>
Motivation: Understanding reasoning failures in LLMs is crucial to interpretability research, as it helps uncover the mechanisms behind their computational processes and errors.

Method: The REMA framework measures geometric deviations by comparing erroneous representations to correctly reasoned representations using k-nearest neighbors distance and identifies divergence points across model layers to trace the origins of reasoning errors.

Result: Extensive experiments show the reasoning manifold is low-dimensional and that erroneous representations are highly separable from correct ones. The REMA framework effectively identifies and explains reasoning failures.

Conclusion: The paper establishes a geometric perspective for analyzing reasoning in LLMs, and REMA serves as a promising tool for diagnosing and understanding computational processes behind reasoning failures.

Abstract: Understanding how Large Language Models (LLMs) perform complex reasoning and
their failure mechanisms is a challenge in interpretability research. To
provide a measurable geometric analysis perspective, we define the concept of
the Reasoning Manifold, a latent low-dimensional geometric structure formed by
the internal representations corresponding to all correctly reasoned
generations. This structure can be conceptualized as the embodiment of the
effective thinking paths that the model has learned to successfully solve a
given task. Based on this concept, we build REMA, a framework that explains the
origins of failures by quantitatively comparing the spatial relationships of
internal model representations corresponding to both erroneous and correct
reasoning samples. Specifically, REMA first quantifies the geometric deviation
of each erroneous representation by calculating its k-nearest neighbors
distance to the approximated manifold formed by correct representations,
thereby providing a unified failure signal. It then localizes the divergence
points where these deviations first become significant by tracking this
deviation metric across the model's layers and comparing it against a baseline
of internal fluctuations from correct representations, thus identifying where
the reasoning chain begins to go off-track. Our extensive experiments on
diverse language and multimodal models and tasks demonstrate the
low-dimensional nature of the reasoning manifold and the high separability
between erroneous and correct reasoning representations. The results also
validate the effectiveness of the REMA framework in analyzing the origins of
reasoning failures. This research connects abstract reasoning failures to
measurable geometric deviations in representations, providing new avenues for
in-depth understanding and diagnosis of the internal computational processes of
black-box models.

</details>


### [48] [The Emergence of Altruism in Large-Language-Model Agents Society](https://arxiv.org/abs/2509.22537)
*Haoyang Li,Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: This paper examines the varied social behaviors of LLMs in a large-scale simulation, revealing intrinsic differences in altruistic versus egoistic tendencies across models.


<details>
  <summary>Details</summary>
Motivation: Existing studies have largely explored cooperation in small games but ignored altruism in large-scale societies.

Method: A Schelling-variant urban migration model with 200+ LLM agents was used to examine their decision-making in a social dilemma.

Result: Two LLM archetypes identified: 'Adaptive Egoists' prioritize self-interest but respond to social norms, and 'Altruistic Optimizers' inherently prioritize collective welfare.

Conclusion: Model selection in social simulations depends not just on reasoning but intrinsic social logic; different archetypes have varied implications for use cases.

Abstract: Leveraging Large Language Models (LLMs) for social simulation is a frontier
in computational social science. Understanding the social logics these agents
embody is critical to this attempt. However, existing research has primarily
focused on cooperation in small-scale, task-oriented games, overlooking how
altruism, which means sacrificing self-interest for collective benefit, emerges
in large-scale agent societies. To address this gap, we introduce a
Schelling-variant urban migration model that creates a social dilemma,
compelling over 200 LLM agents to navigate an explicit conflict between
egoistic (personal utility) and altruistic (system utility) goals. Our central
finding is a fundamental difference in the social tendencies of LLMs. We
identify two distinct archetypes: "Adaptive Egoists", which default to
prioritizing self-interest but whose altruistic behaviors significantly
increase under the influence of a social norm-setting message board; and
"Altruistic Optimizers", which exhibit an inherent altruistic logic,
consistently prioritizing collective benefit even at a direct cost to
themselves. Furthermore, to qualitatively analyze the cognitive underpinnings
of these decisions, we introduce a method inspired by Grounded Theory to
systematically code agent reasoning. In summary, this research provides the
first evidence of intrinsic heterogeneity in the egoistic and altruistic
tendencies of different LLMs. We propose that for social simulation, model
selection is not merely a matter of choosing reasoning capability, but of
choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a
more suitable choice for simulating complex human societies, "Altruistic
Optimizers" are better suited for modeling idealized pro-social actors or
scenarios where collective welfare is the primary consideration.

</details>


### [49] [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)
*Chenyu Zhou,Tianyi Xu,Jianghao Lin,Dongdong Ge*

Main category: cs.AI

TL;DR: StepORLM introduces a self-evolving framework addressing key shortcomings in LLMs for Operations Research, outperforming larger models and enhancing inference scalability.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches for Operations Research suffer from flawed credit assignment and lack holistic evaluation of interdependent steps.

Method: StepORLM employs a co-evolutionary loop involving a policy model and a generative process reward model (GenPRM), improving their performance through dual-feedback using W-DPO alignment.

Result: The 8B-parameter StepORLM surpasses six benchmarks, outperforming larger models and specialized methods, while its GenPRM enhances process verification for other LLMs.

Conclusion: StepORLM offers a robust framework for Operations Research, boosting model inference scalability and accuracy through iterative improvement and generative supervision.

Abstract: Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.

</details>


### [50] [UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](https://arxiv.org/abs/2509.22570)
*Qi Mao,Tinghan Yang,Jiahao Li,Bin Li,Libiao Jin,Yan Lu*

Main category: cs.AI

TL;DR: UniMIC, a new interactive coding framework, uses tokenized representations for low-bitrate communication between edge devices and cloud LMMs, achieving compression efficiency and task robustness.


<details>
  <summary>Details</summary>
Motivation: Current codecs fail for multimodal, interactive AI applications, degrading performance in human-AI collaboration settings.

Method: UniMIC transmits tokenized representations with lightweight Transformer entropy models for compression and scenario-specific designs.

Result: UniMIC demonstrated substantial bitrate savings (<0.05bpp) while maintaining strong performance across tasks like image generation and visual QA.

Conclusion: UniMIC offers an efficient and adaptable solution for multimodal interactive communication, suitable for future AI integrations.

Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI
agents is transforming human-AI collaboration into bidirectional, multimodal
interaction. However, existing codecs remain optimized for unimodal, one-way
communication, resulting in repeated degradation under conventional
compress-transmit-reconstruct pipelines. To address this limitation, we propose
UniMIC, a Unified token-based Multimodal Interactive Coding framework that
bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or
plain text, UniMIC employs compact tokenized representations as the
communication medium, enabling efficient low-bitrate transmission while
maintaining compatibility with LMMs. To further enhance compression,
lightweight Transformer-based entropy models with scenario-specific
designs-generic, masked, and text-conditioned-effectively minimize inter-token
redundancy. Extensive experiments on text-to-image generation, text-guided
inpainting, outpainting, and visual question answering show that UniMIC
achieves substantial bitrate savings and remains robust even at ultra-low
bitrates (<0.05bpp), without compromising downstream task performance. These
results establish UniMIC as a practical and forward-looking paradigm for
next-generation multimodal interactive communication.

</details>


### [51] [Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time](https://arxiv.org/abs/2509.22572)
*Yixuan Han,Fan Ma,Ruijie Quan,Yi Yang*

Main category: cs.AI

TL;DR: Dynamic Experts Search (DES) introduces architecture-aware Test-Time Scaling (TTS) for large language models (LLMs) by varying expert activation, improving reasoning abilities and performance stability without extra cost.


<details>
  <summary>Details</summary>
Motivation: Existing Test-Time Scaling approaches focus primarily on output-level sampling, neglecting the structural diversity in Mixture-of-Experts (MoE) architecture, and limiting reasoning ability during inference.

Method: DES incorporates Dynamic MoE for controllable expert activation and Expert Configuration Inheritance to balance stability and diversity across diverse reasoning trajectories during inference.

Result: Experiments show DES achieves better accuracy and stability across various reasoning benchmarks (math, code, knowledge) compared to traditional TTS approaches, without requiring additional computational cost.

Conclusion: DES demonstrates the untapped potential in leveraging structural flexibility within MoE LLMs, affirming its scalability and effectiveness in improving reasoning trajectories during inference.

Abstract: Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.

</details>


### [52] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: Claim Advisor is a web application leveraging large language models to optimize the creation and evaluation of product claims, enhancing speed and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: Creating product claims requires significant resources, and the process is pivotal for driving consumer purchase behavior. The paper introduces a tool aimed at improving this process.

Method: The solution involves the use of in-context learning and LLM fine-tuning to build the Claim Advisor application, which offers claim search, generation, optimization, and simulation capabilities.

Result: Applications of Claim Advisor in a consumer packaged goods (CPG) company have yielded promising outcomes, demonstrating its effectiveness in streamlining claims processes.

Conclusion: Claim Advisor represents a significant advancement in generative AI applications, offering broad utility across industries and encouraging further exploration and innovation in the field.

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [53] [Privacy-Preserving Performance Profiling of In-The-Wild GPUs](https://arxiv.org/abs/2509.21762)
*Ian McDougall,Michael Davies,Rahul Chatterjee,Somesh Jha,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: The paper addresses the challenges of planet-scale GPU performance profiling and presents a solution for real-time, comprehensive fleet-wide data collection while solving issues related to user experience, privacy, and scalability.


<details>
  <summary>Details</summary>
Motivation: The authors aim to optimize GPU chip design and application performance by enabling efficient fleet-wide data collection of performance characteristics that current tools fail to provide.

Method: The authors propose a system capable of planet-scale real-time profiling that overcomes issues of system slowdown, ensures privacy, and offers scalability up to simulations of 100,000 GPUs.

Result: Their simulation of a 100,000 GPU deployment running Torchbench applications demonstrates that the proposed system effectively addresses user experience, privacy, and large-scale data profiling needs.

Conclusion: The proposed system successfully enables comprehensive, real-time GPU profiling on a large scale, paving the way for better chip designs and application optimizations without compromising user privacy or performance.

Abstract: GPUs are the dominant platform for many important applications today
including deep learning, accelerated computing, and scientific simulation.
However, as the complexity of both applications and hardware increases, GPU
chip manufacturers face a significant challenge: how to gather comprehensive
performance characteristics and value profiles from GPUs deployed in real-world
scenarios. Such data, encompassing the types of kernels executed and the time
spent in each, is crucial for optimizing chip design and enhancing application
performance. Unfortunately, despite the availability of low-level tools like
NSYS and NCU, current methodologies fall short, offering data collection
capabilities only on an individual user basis rather than a broader, more
informative fleet-wide scale. This paper takes on the problem of realizing a
system that allows planet-scale real-time GPU performance profiling of
low-level hardware characteristics. The three fundamental problems we solve
are: i) user experience of achieving this with no slowdown; ii) preserving user
privacy, so that no 3rd party is aware of what applications any user runs; iii)
efficacy in showing we are able to collect data and assign it applications even
when run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,
running applications from the Torchbench suite, showing our system addresses
all 3 problems.

</details>


### [54] [NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction](https://arxiv.org/abs/2509.22410)
*Shayne Wadle,Yanxin Zhang,Vikas Singh,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: The paper introduces a deep learning framework for simulating new microprocessors on existing hardware.


<details>
  <summary>Details</summary>
Motivation: To address slow, cycle-accurate simulators and their reliance on unrepresentative benchmark traces for microprocessor evaluation.

Method: A DL model trained on microarchitecture-independent features predicts cycle-level performance for hypothetical processor designs, using existing silicon. It includes a lightweight trace collector and sampling strategy, along with a proposed Neutrino on-chip accelerator.

Result: The framework achieves 5 MIPS simulation speed on a commodity GPU with 0.1% performance overhead. The Neutrino accelerator shows an 85x performance improvement over the GPU.

Conclusion: The framework facilitates accurate, large-scale performance analysis and hardware A/B testing using real-world applications.

Abstract: The evaluation of new microprocessor designs is constrained by slow,
cycle-accurate simulators that rely on unrepresentative benchmark traces. This
paper introduces a novel deep learning framework for high-fidelity,
``in-the-wild'' simulation on production hardware. Our core contribution is a
DL model trained on microarchitecture-independent features to predict
cycle-level performance for hypothetical processor designs. This unique
approach allows the model to be deployed on existing silicon to evaluate future
hardware. We propose a complete system featuring a lightweight hardware trace
collector and a principled sampling strategy to minimize user impact. This
system achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a
mere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip
accelerator improves performance by 85x over the GPU. We demonstrate that this
framework enables accurate performance analysis and large-scale hardware A/B
testing on a massive scale using real-world applications.

</details>


### [55] [AxLLM: accelerator architecture for large language models with computation reuse capability](https://arxiv.org/abs/2509.22512)
*Soroush Ahadi,Mehdi Modarressi,Masoud Daneshtalab*

Main category: cs.AR

TL;DR: This paper introduces AxLLM, a hardware accelerator for quantized models that reduces computations by caching repeated operations, achieving energy and speed improvements.


<details>
  <summary>Details</summary>
Motivation: Large language models require massive computational and memory resources, necessitating innovative solutions for efficient deployment.

Method: The authors use model quantization to enhance parameter locality, and introduce AxLLMâ€”a hardware architecture featuring a redundancy elimination method to reuse multiplication results of repeated weights.

Result: AxLLM reduces computations by up to 90%, achieves 28% lower energy consumption, and delivers a 1.7x speedup compared to standard baselines.

Conclusion: AxLLM is presented as an efficient and scalable solution for deploying quantized language models, reducing redundant calculations and enabling improved hardware performance.

Abstract: Large language models demand massive computational power and memory
resources, posing significant challenges for efficient deployment. While
quantization has been widely explored to reduce model size and computation,
this paper demonstrates an additional benefit: quantization increases parameter
locality, creating opportunities for computation reuse. Building on this
insight, we propose AxLLM, a hardware accelerator architecture designed for
quantized models. Axllm introduces a novel redundancy elimination technique
that caches and reuses multiplication results for repeated weight values,
substantially reducing redundant operations. The architecture features dual
multiply and reuse pipelines, efficiently supporting both base models and LoRA
fine-tuned models without altering parameters, retraining, or requiring offline
preprocessing. Experimental results show that AxLLM achieves up to 90%
reduction in computations, delivering 28% lower energy consumption and a 1.7x
speedup over baseline execution. These results highlight Axllm as a scalable
and efficient solution for accelerating LLMs on specialized hardware.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [56] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: The paper proposes a novel method to detect hallucinations in large language models by identifying concentrated feature subsets in deep layers, leading to efficient and accurate detection.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of hallucination in large language models caused by biases in training data and the lack of precise localization of hallucination signals in neural network layers.

Method: The paper introduces a dual-model architecture combining a Projected Fusion (PF) block for adaptive inter-layer feature weighting and a Differential Feature Learning (DFL) mechanism to detect discriminative features by comparing representations from parallel encoders.

Result: The proposed method showed significant accuracy improvements in detecting hallucinations in question answering and dialogue tasks, leveraging feature sparsity and a hierarchical funnel pattern in layers. Detection performance was maintained using only 1% of feature dimensions.

Conclusion: Hallucination signals in large language models are more concentrated than previously thought, enabling the development of efficient and cost-effective detection methods without sacrificing accuracy.

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [57] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: The paper introduces Contextual Influence Value (CI value), a new metric for selecting high-quality contexts in Retrieval-Augmented Generation (RAG) systems, addressing issues caused by noisy or irrelevant data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the effectiveness of Retrieval-Augmented Generation (RAG) by addressing its vulnerability to noise and irrelevance in the retrieved contexts, which compromise the quality of responses.

Method: The authors redefine context quality assessment as a data valuation problem during inference and propose the Contextual Influence Value (CI value). They also develop a parameterized surrogate model using a hierarchical architecture to predict CI values efficiently, integrating local and global relevance, and end-to-end generator feedback.

Result: Their context selection method, based on CI values, performs significantly better than state-of-the-art baselines across 8 NLP tasks and multiple large language models, effectively improving the filtering of poor contexts while retaining critical information.

Conclusion: Integrating CI value-based context filtering enhances RAG systems by selecting salient and high-quality external knowledge, leading to better grounding and performance in various NLP applications.

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [58] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: The paper investigates the gap between reported context window sizes and effective usage in real-world tasks, revealing significant performance degradation with increasing tokens.


<details>
  <summary>Details</summary>
Motivation: To understand the real-world effectiveness of large language model context windows and highlight the gap between reported maximum sizes and practical limitations.

Method: The authors define the concept of maximum effective context window (MECW), develop testing methods for evaluating performance over different window sizes and problem types, and conduct tests across various models with a standardized comparison approach.

Result: The study found that MECW is much smaller than the reported maximum context window (MCW), varying by problem type. Severe performance degradation started at 100 to 1000 tokens for most models, with some failing after 100 tokens.

Conclusion: The paper concludes that current large language models fall drastically short of their advertised context window sizes, and their effective range is problem-dependent, suggesting room for improvement in accuracy and reliability.

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [59] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: The paper advocates the necessity of incorporating human-crafted symbols into AI systems for guiding their capabilities beyond large scale models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current AI systems, which rely primarily on scaling instead of deeper symbolic guidance, hindering genuine discovery.

Method: Introduce human-crafted symbolic structures as a compass to complement the intuition of large language models.

Result: Incorporating symbolic guidance can enhance AI's discovery capacity, ensuring more meaningful and directed outputs.

Conclusion: AI advancement shouldn't solely focus on scaling; leveraging human symbolic input is crucial for navigating intuition and achieving authentic discoveries.

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [60] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn RosÃ©,Daphne Ippolito*

Main category: cs.CL

TL;DR: The study analyzes how different aspects of programming languages enhance LLM reasoning skills, finding structural features more impactful than semantic ones.


<details>
  <summary>Details</summary>
Motivation: To understand which elements of programming languages most effectively enhance the reasoning capabilities of large language models.

Method: Constructing parallel datasets in ten programming languages, applying structured perturbations, finetuning LLMs across five model families and eight scales, and systematically evaluating their performance.

Result: LLMs show greater sensitivity to structural disruptions than semantic ones, and representations such as pseudocode and flowcharts can be effective alternatives to full code.

Conclusion: The research offers insights into leveraging coding structures to optimize LLM reasoning, emphasizing that training data design can significantly impact performance.

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [61] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: The paper explores inconsistencies in moral reasoning by Large Language Models (LLMs) when operating in diverse linguistic and cultural environments, identifying issues with cultural misalignments and reasoning strategies.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in multilingual settings where ethical reasoning is essential, yet their English-dominated pretraining raises concerns about their cultural adaptability and ethical generalization.

Method: The authors conducted a multilingual zero-shot evaluation by translating established moral reasoning benchmarks into five diverse languages, analyzing LLM-generated responses for inconsistencies.

Result: The study identifies notable inconsistencies in LLMs' moral judgments across languages, stemming from cultural disparities, disagreement patterns, and reasoning strategies.

Conclusion: The paper highlights the need for integrating culturally-aware approaches in AI, offering a typology of moral reasoning errors to improve multilingual and multicultural AI systems.

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [62] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: The paper evaluates GPT-5 as a tool for diabetes diagnosis and monitoring, using synthetic cases aligned with the ADA Standards of Care 2025 for five specific use cases, with promising results.


<details>
  <summary>Details</summary>
Motivation: Diabetes mellitus is a growing global health issue, and current diagnosis and monitoring methods are hindered by vague symptoms and complexity. Advances in LLMs like GPT-5 offer potential solutions to improve decision support.

Method: The study tested GPT-5 using a simulation framework with synthetic cases based on ADA guidelines and public datasets. Five key scenarios were evaluated: symptom recognition, lab interpretation, gestational diabetes screening, remote monitoring, and multimodal complication detection. Outputs included clinical rationales, patient explanations, and JSON summaries.

Result: GPT-5 performed well in classifying cases, aligning with ADA criteria, and generating structured, interpretable outputs suitable for clinicians and patients.

Conclusion: GPT-5 shows promise as a dual-purpose clinical and patient tool for diabetes care, highlighting the importance of reproducible frameworks for LLM evaluation in healthcare.

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [63] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: The paper examines how fine-tuning pretrained language models (PLMs) to align with moral objectives (such as mitigating stereotypes) impacts downstream task performance.


<details>
  <summary>Details</summary>
Motivation: To explore the cost of mitigating gender stereotypes in PLMs, especially the trade-offs between fairness and downstream task performance.

Method: Analysis of forgetting mechanisms and fairness objectives in the context of mitigating gender stereotypes.

Result: Reveals that methods to mitigate stereotypes increase overall forgetting, and generalized solutions fail to balance forgetting and task performance.

Conclusion: Current fairness objectives are insufficient for achieving desired trade-offs; mitigating stereotypes negatively affects downstream task performance, needing improved strategies.

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [64] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: The paper introduces RL with Verifiable Rewards (RLVR) and applies it to BIRD, a benchmark for converting natural language to SQL, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Address enterprise challenges using tailored RL models that leverage organization-specific knowledge in scenarios with verifiable reward functions.

Method: Used a training approach combining model/prompt selection, TAO-based offline RL warm-up, and rigorous online RLVR training.

Result: Achieved state-of-the-art accuracy on the BIRD test set, yielding up to 75.68% accuracy with fewer generations compared to competitors.

Conclusion: The proposed RLVR framework is broadly applicable across enterprise domains, showcasing simplicity and effectiveness.

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [65] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: The paper investigates improving reinforcement learning with verifiable rewards (RLVR) by incorporating mixture-of-token generation (MoT-G) to better utilize a model's probability distribution during reasoning, leading to superior results in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Currently, RLVR methods discard the model's probability distribution over candidate tokens at each reasoning step, unnecessarily constraining the reasoning process. This limits the potential of large language models in reasoning tasks.

Method: The authors propose a unified framework for mixture-of-token generation (MoT-G) that extends RLVR into the continuous mixture space of token embeddings. They experiment with two MoT-G variants on reasoning tasks to improve LLM reasoning.

Result: The MoT-G methods showed significant improvements, with 5-35% gains on 7 out of 10 tasks tested in Reasoning-Gym. They achieved comparable accuracy using half the number of trajectories, indicating enhanced training efficiency.

Conclusion: The mixture-of-token generation (MoT-G) approach effectively improves reasoning capabilities in RLVR by maintaining higher hidden-state entropy and encouraging exploration, offering more efficient training and significant performance gains.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [66] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: The paper introduces Dual-Head Reasoning Distillation (DHRD) method to improve classification accuracy without the throughput penalty typically seen in Chain-of-Thought (CoT) prompting.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the trade-off between improved classification performance through Chain-of-Thought (CoT) prompting and the significant reduction in inference throughput caused by rationale generation.

Method: Introduce a dual-head architecture with a pooled classification head for inference and a reasoning head supervised only during training, coupled with a loss function combining label cross-entropy and token-level LM loss.

Result: DHRD achieves relative performance improvements ranging from 0.65% to 5.47% on seven SuperGLUE tasks, particularly excelling in entailment and causal reasoning tasks. Its inference speed surpasses CoT decoding by 96-142 times.

Conclusion: The DHRD method effectively combines the strengths of CoT prompting with efficient throughput, providing a practical solution for real-time applications without sacrificing reasoning capabilities in training.

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [67] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: This paper introduces an agricultural chatbot designed to provide farmers with accessible 24/7 information related to weather, market rates, plant protection, and government schemes. It utilizes a sentence embedding model with an impressive accuracy of 86%.


<details>
  <summary>Details</summary>
Motivation: To provide easily accessible and precise agricultural information to farmers for improved agricultural productivity, thus reducing reliance on call centers.

Method: The chatbot uses a sentence embedding model integrated with synonym elimination and entity extraction to enhance its accuracy in answering queries.

Result: The system achieved accuracy improvement from 56% to 86% after optimizing the model with synonym elimination and entity extraction.

Conclusion: By implementing this chatbot, farmers gain simplified access to vital agricultural knowledge, leading to enhanced productivity, and call center employees can focus on more impactful tasks.

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [68] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: This study analyzes speaker diarization challenges for African-accented English, focusing on domain effects for general and clinical dialogues. A domain penalty is observed in clinical speech and remains significant even after adaptation attempts.


<details>
  <summary>Details</summary>
Motivation: Understanding the domain-specific challenges in speaker diarization for African-accented English, particularly in clinical dialogues, where overlap and short turns are common.

Method: The paper evaluates various production and open speaker diarization systems using a strict DER protocol, performs error analysis, and tests domain adaptation through fine-tuning segmentation modules on accent-matched data.

Result: Clinical speech demonstrates higher error rates, primarily due to false alarms and missed detections caused by frequent overlaps and short conversational turns. Domain adaptation partially reduces errors but does not entirely address the domain penalty.

Conclusion: Overlap-aware segmentation and development of balanced clinical resources are identified as practical next steps for improving speaker diarization performance in domain-specific scenarios.

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [69] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: The paper examines how large language models (LLMs) can reliably cite human-verifiable sources in critical domains by comparing two citation paradigms: Generation-Time Citation (G-Cite) and Post-hoc Citation (P-Cite).


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for trustworthy LLMs to provide human-verifiable citations in high-stakes areas where even small errors can have significant consequences.

Method: The authors define two paradigms, G-Cite and P-Cite, and conduct evaluations using zero-shot and advanced retrieval-augmented methods across four attribution datasets to compare their performance.

Result: Findings reveal a trade-off between citation coverage and correctness, with P-Cite excelling in high coverage and acceptable correctness, while G-Cite prioritizes precision but sacrifices coverage and speed.

Conclusion: The authors recommend a retrieval-focused, P-Cite-first strategy for high-stakes applications, using G-Cite for scenarios requiring strict claim verification.

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [70] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: The paper presents ComPSum, a framework for personalized multi-document summarization (MDS), focusing on user-preference analysis for creating tailored summaries, and introduces AuthorMap and PerMSum for evaluation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for personalized summaries that align with individual writing styles and content focus by identifying fine-grained differences in user preferences.

Method: The researchers introduce ComPSum, which analyzes user preferences by comparing them to others, and uses this structured analysis to create personalized summaries. They evaluate using AuthorMap, a reference-free evaluation method, and PerMSum, a dataset tailored to personalized MDS.

Result: ComPSum was evaluated on the newly created PerMSum dataset and assessed through the AuthorMap framework, demonstrating superior performance compared to strong baseline models.

Conclusion: ComPSum is effective for personalized MDS, leveraging structured user preference analysis, and surpasses existing models according to the evaluation metrics proposed in the paper.

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [71] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: The paper explores using vision-language models (VLMs) for long-horizon multimodal planning by adopting formal planning languages like PDDL for verifiable solutions.


<details>
  <summary>Details</summary>
Motivation: To address limitations of embodied agents in long-horizon multimodal planning, leveraging the success of LLM formalization in text-based planning environments.

Method: Developed five VLM-as-formalizer pipelines that integrate multimodal planning problems with PDDL formalization, including diverse one-shot and low-quality image scenarios.

Result: The proposed VLM-as-formalizer pipelines outperform direct plan generation methods but reveal weaknesses in VLMs' ability to comprehensively capture object relationships.

Conclusion: Vision remains the key bottleneck for multimodal planning; intermediate textual representations show potential yet inconsistent improvements, signaling opportunities for further research.

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [72] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: The study evaluates multilingual AI models' ability to translate figurative language like idioms and puns into 20 languages, highlighting challenges in cultural localisation despite grammatical accuracy.


<details>
  <summary>Details</summary>
Motivation: Understanding limitations in translation of culturally nuanced expressions for real-world applications like e-commerce marketing.

Method: Analyzed 87 translations of marketing emails across 24 dialects, assessed by human reviewers for tone, meaning, and audience localisation.

Result: Models performed generally well grammatically but struggled with culturally nuanced figurative language, even in high-resource languages.

Conclusion: Current multilingual models exhibit limitations in cultural localisation, necessitating further research and refinement for real-world use cases.

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [73] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: The paper explores Multi-Objective Reinforcement Learning (MORL) for optimizing multiple objectives in Large Language Models (LLMs), proposing a taxonomy, addressing current limitations, and outlining future research directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenges of optimizing LLMs for multiple objectives, particularly focusing on the need for flexible and efficient MORL methods given the inherent complexities of LLMs and reinforcement learning.

Method: The authors introduce a MORL taxonomy, evaluate existing methods, and propose a vision for benchmarking frameworks to study diverse objective relationships. They emphasize meta-policy development and bi-level learning paradigms for improving LLM optimization.

Result: The paper identifies the limitations of current MORL approaches for LLMs and highlights meta-policy MORL as a promising direction for achieving efficiency and flexibility.

Conclusion: The findings suggest that advancing MORL methods, particularly through meta-policy adaptation and novel benchmarking frameworks, can significantly enhance LLM optimization and personalization.

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [74] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: OjaKV addresses memory limitations in large language models by combining smart token preservation and adaptive token compression, boosting efficiency and accuracy in long-context reasoning.


<details>
  <summary>Details</summary>
Motivation: Memory bottlenecks in KV-cache storage hinder the performance of large language models in processing extensive contexts.

Method: OjaKV combines a hybrid storage policy, which selectively preserves high-fidelity token anchors, with online subspace adaptation using Oja's algorithm to compress intermediate tokens.

Result: Experimental results show OjaKV maintains or improves zero-shot accuracy, especially excelling in benchmarks requiring long-context reasoning, all while achieving high KV-cache compression ratios.

Conclusion: OjaKV provides a practical solution for memory-efficient long-context inference without the need for model fine-tuning, effectively addressing memory constraints in autoregressive generation.

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [75] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: This paper reviews explainable AI (XAI) techniques tailored for language models (LMs), emphasizing their architecture types and evaluating methods' plausibility and faithfulness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in language models, especially given their black-box nature and challenges in high-stakes applications.

Method: The survey organized XAI methods by LM architecture typesâ€”encoder-only, decoder-only, and encoder-decoderâ€”analyzed adaptations, and evaluated techniques against plausibility and faithfulness.

Result: The paper provides an organized review of XAI tailored for LMs, highlights strengths and weaknesses of existing methods, and identifies challenges and potential future directions.

Conclusion: There is a need for robust and interpretable XAI techniques specific to LMs, and the paper aims to guide future research towards this goal.

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [76] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: FeatBench introduces a novel benchmark to evaluate the "vibe coding" capabilities of coding agents, focusing on feature implementation using abstract natural language prompts.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for code generation fail to adequately assess large language models' ability to implement features through natural language interactions in the emerging "vibe coding" paradigm.

Method: The authors propose FeatBench, a benchmark characterized by pure natural language prompts, a rigorous multi-level filtering process, evolving automated data pipelines, comprehensive testing protocols, and diverse application domains.

Result: The testing of four leading LLMs on FeatBench revealed a maximum success rate of 29.94% for feature implementation tasks, highlighting gaps in current agent performance and tendencies towards aggressive design choices.

Conclusion: FeatBench illustrates the challenges of feature implementation in vibe coding, identifies tendencies in agent behavior, and provides a valuable tool for advancing research in this domain.

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [77] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: The paper tackles the degradation of peer review quality in AI conferences by introducing a metric, ReviewScore, for detecting misinformed review points using human-annotated data and automated tools.


<details>
  <summary>Details</summary>
Motivation: The degradation in peer review quality due to the surge in AI conference submissions necessitates methods to detect low-quality and misinformed reviews reliably.

Method: The paper defines misinformed review points and introduces ReviewScore to measure them. It uses a factuality-checking engine to evaluate premises and employs human-annotated datasets to validate LLM-based automation.

Result: The study finds that 15.2% of weaknesses and 26.4% of questions in reviews are misinformed. It demonstrates moderate agreement between human evaluations and eight state-of-the-art LLMs, highlighting the effectiveness of premise-level rather than weakness-level evaluations.

Conclusion: Premise-level factuality evaluation shows significant promise for automating ReviewScore assessment, providing a path toward more reliable peer review processes.

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [78] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: This paper introduces GRAB, a benchmark for assessing topic models on financial disclosures using a risk taxonomy and unsupervised methods.


<details>
  <summary>Details</summary>
Motivation: Risk categorization in 10-K risk disclosures is important for oversight and investment, but no benchmark exists to evaluate unsupervised topic models for this task.

Method: The paper uses FinBERT token attention, YAKE keyphrase signals, and taxonomy-aware collocation matching to generate labels grounded in a risk taxonomy without manual annotation. Evaluation involves standardized dataset splits and robust metrics.

Result: The study produced span-grounded sentence labels for 1.61M sentences from financial filings and validated a taxonomy mapping 193 terms to 21 fine-grained risk types organized under five macro classes.

Conclusion: GRAB offers a standardized, finance-specific dataset, labels, and tools for consistent evaluation of topic models across methodologies, enabling reproducible assessments in financial risk disclosure contexts.

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [79] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: The paper introduces Think-on-Graph 3.0 (ToG-3), a framework enhancing Retrieval-Augmented Generation (RAG) for LLMs with an adaptive graph mechanism that outperforms traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current graph-based RAG methods for LLMs face scalability and accuracy issues due to dependencies on manually constructed or auto-extracted knowledge graphs.

Method: ToG-3 introduces the MACER mechanism, which dynamically evolves queries and graph sub-structures using a multi-agent system (Constructor, Retriever, Reflector, Responser) for adaptive evidence retrieval.

Result: ToG-3 achieves superior performance on both deep and broad reasoning benchmarks compared to baselines, supported by ablation studies demonstrating MACER's efficacy.

Conclusion: ToG-3 overcomes limitations in static graph construction of previous RAG methods, enabling precise reasoning with lightweight LLMs through its dynamic and multi-agent framework.

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [80] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: The paper introduces ProPerSim, a simulation framework designed for creating proactive and personalized AI assistants, and proposes ProPerAssistant, which adapts based on feedback to improve user satisfaction.


<details>
  <summary>Details</summary>
Motivation: With the rising integration of large language models in daily life, there is an unmet need for AI assistants that achieve both proactivity and personalization simultaneously, which has been underexplored so far.

Method: The authors develop ProPerSim, a simulation environment where user personas interact with assistants by rating the quality of personalized suggestions. They also propose ProPerAssistant, a retrieval-augmented model that adapts based on user feedback over time.

Result: Experiments performed with ProPerAssistant on 32 diverse user personas demonstrate its ability to adapt strategies and steadily improve user satisfaction scores.

Conclusion: The study emphasizes the potential of combining personalization and proactivity in AI assistants, supported by the experimental success of their proposed framework and model.

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [81] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: The paper addresses challenges in using LLMs for answering multiple questions over long contexts, showing fine-tuned smaller models can sometimes outperform GPT-4o in accuracy.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the need to tackle computational cost and latency issues when answering multiple questions from the same context in real-world industrial applications.

Method: The researchers analyzed proprietary and public LLMs, conducting experiments to benchmark their performance on multi-question answering tasks.

Result: Proprietary models like GPT-4o yielded strong performance, but fine-tuned public LLMs of up to 8 billion parameters achieved comparable or superior accuracy.

Conclusion: Fine-tuned public LLMs offer a promising and cost-efficient alternative for practical applications, challenging the dominance of expensive proprietary models.

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [82] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: This paper introduces a novel approach called Self-Speculative Biased Decoding to improve latency and user experience in streaming applications using LLMs, achieving faster speeds and less flickering during translation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the usability of LLMs in latency-sensitive streaming applications like live translation, which demand continual updates and computational efficiency.

Method: The authors propose using the recent output as a draft for expanding input context, applying a bias to accept draft tokens during verification, and switching to conventional decoding only when divergence occurs.

Result: The method achieves up to 1.7x speedup compared to traditional auto-regressive approaches and reduces output flickering by 80%, improving both efficiency and user experience.

Conclusion: The proposed method offers a model-agnostic, plug-and-play solution that balances computational efficiency with quality, making it valuable for real-time translation tasks.

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [83] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: Recent Large Audio-Language Models (LALMs) struggle with challenging audio reasoning tasks due to a lack of acoustic tools. The proposed Thinking-with-Sound (TwS) framework integrates audio-domain analysis with linguistic reasoning to improve robustness in complex acoustic scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of LALMs in handling complex audio reasoning tasks, particularly those involving acoustic perturbations and requiring advanced tools like noise suppression and source separation.

Method: TwS framework introduces 'Audio CoT,' combining linguistic reasoning with active audio analysis through multimodal reasoning. This enables real-time manipulation and analysis of audio signals during task execution.

Result: TwS significantly improves robustness on the MELD-Hard1k benchmark, reversing significant performance drops caused by acoustic perturbations. Small models achieve a 24.73% accuracy gain, while larger models reach a 36.61% gain.

Conclusion: Audio CoT demonstrates its effectiveness in enhancing robustness without requiring model retraining, offering a promising approach for developing advanced LALMs capable of handling complex acoustic scenarios.

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [84] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: This paper introduces SynerGen, a generative backbone model designed to solidify retrieval and ranking tasks together for search and recommendation in large-scale systems.


<details>
  <summary>Details</summary>
Motivation: The dominant pipeline of separately handling retrieval and ranking in recommender systems creates inefficiencies and trade-offs, especially when trying to unify personalized search and query-free recommendation.

Method: The authors propose SynerGen, a decoder-only Transformer with joint optimization using InfoNCE for retrieval and a hybrid pointwise-pairwise loss for ranking, along with a novel time-aware rotary positional embedding.

Result: SynerGen achieves significant performance improvements across search and recommendation benchmarks, surpassing state-of-the-art baselines.

Conclusion: A single unified generative model like SynerGen showcases its potential for industrial-scale applications by replacing the traditional separated retrieve-rank architecture.

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [85] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: This paper analyzes the impact of structured output on LLMs using causal inference, finding no causal impact in most scenarios across tasks.


<details>
  <summary>Details</summary>
Motivation: To clarify conflicting findings about the impact of structured output on LLMs' generation, addressing limitations like restricted testing scenarios and reliance on coarse metrics.

Method: Five potential causal structures were explored using causal inference, and structured output's impact was tested across seven public tasks and one developed reasoning task.

Result: Causal inference showed no causal impact in 43 out of 48 scenarios; the remaining 5 scenarios involved multifaceted causal effects influenced by instructions.

Conclusion: Structured output's impact on LLMs is context-dependent, and the use of causal inference reveals nuanced effects that can differ from coarse metrics' findings.

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [86] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: This paper introduces Cultural Awareness Reward modeling Benchmark (CARB) to assess cultural awareness in reward models for large language models and proposes an approach called Think-as-Locals to improve culturally grounded reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing reward model evaluations lack datasets relevant to diverse cultures, leading to inadequate assessment of cultural awarenessâ€”a critical aspect for globally aligning large language models.

Method: The authors designed CARB, covering 10 cultures across 4 domains, and introduced Think-as-Locals with reinforcement learning from verifiable rewards (RLVR) to focus on deeper cultural reasoning and mitigate reliance on surface-level features.

Result: Experiments show that CARB improves understanding of deficiencies in cultural awareness in state-of-the-art reward models, while Think-as-Locals reduces spurious feature reliance and enhances the cultural alignment capabilities of models.

Conclusion: CARB and the proposed Think-as-Locals framework together advance the evaluation and improvement of culture-aware reward modeling for large language models, facilitating better global alignment.

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [87] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: The paper expands the action space in simultaneous machine translation (SiMT) using adaptive actions like SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION, and PRONOMINALIZATION, showing improvements in translation quality and latency metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional encoder-decoder policies in SiMT are not sufficient to balance real-time constraints and high translation quality. The paper addresses this gap by enhancing the action space to better handle tasks like restructuring and simplification.

Method: The method involves implementing new adaptive actions into a decoder-only large language model. Training references are created using action-aware prompting, and evaluation is conducted through a latency-aware text-to-speech pipeline that assesses quality and timing simultaneously.

Result: Experiments reveal consistent improvements in semantic metrics (e.g., COMET-KIWI) and reduced delays using the ACL60/60 benchmarks. Combining DROP and SENTENCE_CUT actions delivers the best balance between fluency and latency.

Conclusion: Expanding the action space in SiMT enhances both quality and latency, representing a strong step towards achieving human-like simultaneous interpretation.

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [88] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: The paper introduces CaMIB, a model for Human Multimodal Language Understanding that addresses poor generalization due to dataset biases through causal principles and improved feature disentanglement.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models are susceptible to dataset biases, leading to over-reliance on statistical shortcuts and poor performance on out-of-distribution data.

Method: The paper proposes CaMIB, which uses an information bottleneck to filter input noise and disentangles multimodal representations into causal and non-causal parts. It employs techniques like an instrumental variable constraint and backdoor adjustment to enhance causal feature consistency.

Result: Experiments across tasks like sentiment analysis, humor detection, and sarcasm detection show that CaMIB improves both in-distribution and out-of-distribution performances. The model also offers better interpretability.

Conclusion: CaMIB effectively alleviates dataset bias issues, demonstrating the benefits of integrating causal principles for multimodal understanding and generalization.

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [89] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: The paper investigates linguistic puzzle solving and generation using Large Language Models (LLMs), showcasing their performance and potential impact on promoting linguistics.


<details>
  <summary>Details</summary>
Motivation: The authors aim to promote linguistics as a field, disseminate knowledge about rare languages, and expand the audience's interest through solving and automating the creation of linguistic puzzles.

Method: The paper extends an existing benchmark for linguistic puzzle-solving and examines the performance of LLMs like OpenAI's o1 on various linguistic topics. It also introduces the use of insights from solving experiments to automate puzzle generation.

Result: The study finds LLMs excel beyond human capability in most linguistic puzzles except those involving writing systems and understudied languages. Automated generation of puzzles shows promise in engaging audiences and expanding linguistics' reach.

Conclusion: Automating linguistic puzzle generation could revolutionize the promotion of linguistics and broaden public awareness while supporting the study of rare and understudied languages.

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [90] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: The paper develops Reshaped Token-level policy gradients (ResT), a method for optimizing tool-usage policies in large language models (LLMs) by leveraging entropy-informed token reweighting, achieving state-of-the-art results in experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional reinforcement learning approaches for tool-use tasks in LLMs are inefficient due to sparse rewards and high policy-gradient variance. This paper seeks to address these limitations by improving training stability and efficiency.

Method: The method involves deriving a theoretical link between policy entropy and training stability in tool-use tasks. Based on this, the authors introduce ResT, a policy gradient approach that uses entropy-aware token reweighting to progressively focus on reasoning tokens during training.

Result: ResT outperformed previous methods on benchmark datasets, improving performance by up to 8.76%. It also surpassed GPT-4o on single-turn and multi-turn tasks in experiments.

Conclusion: ResT successfully stabilizes training and enhances reasoning capability in LLMs for tool-use tasks by addressing structural correctness and semantic reasoning in a balanced manner.

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [91] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: The paper introduces a cascade system leveraging semantic agreement for efficient LLM deployment, achieving high-quality outputs with reduced costs and latency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining output reliability in cascade systems for LLMs, especially in open-ended tasks where quality varies and multiple valid responses exist.

Method: The proposed method employs semantic agreementâ€”consensus between outputs of diverse, smaller modelsâ€”as a training-free signal to decide whether to defer to larger models.

Result: Semantic cascades achieve comparable or better quality than large models while reducing costs by 40% and latency by 60%, and are effective across a range of model sizes and black-box APIs.

Conclusion: Semantic agreement provides a practical, robust, and cost-effective strategy for cascading systems, suitable for real-world applications, without requiring internal modifications to models.

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [92] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: The paper introduces TRACE, a framework improving empathetic response generation by combining analytical depth and generative fluency.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between specialized models' deep analysis and Large Language Models' generative fluency in empathetic response generation.

Method: TRACE uses a task-decomposed approach, splitting the process into analysis (building understanding) and synthesis (response generation).

Result: TRACE outperformed strong baselines in both automatic and LLM-based evaluations, showing its effectiveness.

Conclusion: TRACE's structured cognitive process improves empathetic response generation, making it a promising framework for creating human-like conversational agents.

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [93] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: This paper introduces KnowMT-Bench, the first benchmark for evaluating multi-turn long-form question-answering in knowledge-intensive fields using large language models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of benchmarks for evaluating the knowledge-intensive factuality accuracy of multi-turn long-form question-answering performance in large language models.

Method: The paper presents KnowMT-Bench, where models are evaluated in dynamic settings by generating their own multi-turn dialogues. Performance is assessed based on the factual correctness and delivery efficiency of the final answer using human-validated automated pipelines.

Result: The study shows performance drops due to contextual noise in the generated histories and verbosity as dialogues lengthen. Retrieval-augmented generation (RAG) is found to counteract these issues.

Conclusion: The benchmark highlights the challenges faced by LLMs in multi-turn factual capabilities and suggests that retrieval-augmented methods can improve these weaknesses, making KnowMT-Bench a valuable tool for progressing LLMs in knowledge-intensive domains.

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [94] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: The paper introduces LoRAN, a non-linear extension of the LoRA fine-tuning method, integrating the Sinter activation function for improved low-rank tuning in large language models.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the need to overcome the expressiveness limitations of the linearity in LoRA, aiming to enhance the performance of parameter-efficient fine-tuning methods for large language models.

Method: The authors propose LoRAN, introducing lightweight non-linear transformations to LoRA's low-rank updates. They also design the Sinter activation function to add structured perturbations while maintaining parameter efficiency.

Result: The experiments, conducted on summarization and classification tasks, demonstrate that LoRAN consistently outperforms QLoRA. Ablation studies show that Sinter activation surpasses other standard activations like Sigmoid, ReLU, and Tanh.

Conclusion: LoRAN and the Sinter activation function offer significant advancements in low-rank tuning, proving the importance of exploring activation design for enhancing parameter-efficient learning in large-scale models.

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [95] [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
*Min-Hsuan Yeh,Yixuan Li,Tanwi Mallick*

Main category: cs.CL

TL;DR: LUMINA is proposed as a novel framework to detect hallucinations in Retrieval-Augmented Generation (RAG) systems by measuring how models use external context and internal knowledge.


<details>
  <summary>Details</summary>
Motivation: Current RAG-based large language models still hallucinate despite having correct and sufficient external context. This may be due to an imbalance between external context usage and internal knowledge. Existing methods to quantify these signals are limited by extensive hyperparameter tuning.

Method: LUMINA quantifies external context utilization via distributional distance and tracks internal knowledge via token prediction evolution across transformer layers. It introduces statistical validation for its measurements.

Result: Experiments with common RAG benchmarks and four open-source LLMs show LUMINA outperformed prior methods by up to +13% AUROC on HalluRAG and remained robust under less strict retrieval quality and model matching conditions.

Conclusion: LUMINA provides an effective and practical method to detect hallucinations in RAG systems, improving performance metrics over existing approaches.

Abstract: Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

</details>


### [96] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: The paper introduces RL-ZVP, an algorithm that leverages zero-variance prompts for improved policy optimization in Large Language Models (LLMs), achieving better performance on math reasoning tasks compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning abilities of LLMs, especially by addressing the oversight of zero-variance prompts in current reinforcement learning frameworks such as GRPO.

Method: The RL-ZVP algorithm directly incorporates zero-variance prompts to reward correctness and penalize errors, using token-level feedback for nuanced signal extraction, even when direct response comparison isn't possible.

Result: RL-ZVP outperformed GRPO and other baselines across six math reasoning benchmarks, with accuracy gains of up to 8.61 points and pass rate improvements of 7.77 points.

Conclusion: Learning signals from zero-variance prompts, previously thought uninformative, can enhance LLM performance, demonstrating the value of RL-ZVP for policy optimization.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [97] [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
*Yijin Guo,Ye Shen,Farong Wen,Junying Wang,Zicheng Zhang,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: The paper introduces QoNext, a framework adapting Quality of Experience (QoE) principles to assess foundation models, focusing on user experience instead of only output correctness.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of foundation models overlook user satisfaction factors like interaction quality, limiting their assessment's relevance for real user experiences.

Method: QoNext identifies user experience factors, conducts experiments collecting human ratings under diverse conditions, builds a QoE database, and trains predictive models to estimate user experience.

Result: QoNext delivers fine-grained evaluations and provides actionable insights, aiding in optimizing foundation models for user-focused outputs.

Conclusion: Using QoNext, evaluations become proactive, offering both a deeper understanding of user experiences and practical strategies for improving foundation model performance in real-world scenarios.

Abstract: Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.

</details>


### [98] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: The paper addresses the issue of performance degradation in Mixture-of-Experts (MoE) models when scaling the activated experts during inference and proposes a novel framework, EMoE, to improve collaboration and maintain robust performance across varying computational budgets.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts models struggle with degrading performance when the number of activated experts at inference increases beyond the training-time configuration, highlighting the need for improved scalability and collaboration among experts.

Method: The authors propose Elastic Mixture-of-Experts (EMoE), a training framework that teaches experts to collaborate effectively in diverse combinations while encouraging optimized selection by the router. This approach avoids additional training overhead and prepares the model for scalable activation during inference.

Result: EMoE significantly enhances the performance-scaling range of MoE models, allowing activation of 2-3Ã— more experts during inference while delivering superior peak performance.

Conclusion: The EMoE framework addresses collaboration deficiencies among experts in MoE models, enabling robust and scalable performance during inference across various computational budgets, and unlocking higher potential for deployment flexibility and efficiency.

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [99] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin EravcÄ±*

Main category: cs.CL

TL;DR: This paper develops a dataset and classification pipeline to enhance qualitative citation intent analysis for Turkish, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for analyzing citation intents often struggle with agglutinative languages like Turkish, necessitating tools tailored for this linguistic specificity.

Method: The authors created a new dataset for Turkish citation intents, optimized prompt designs through the DSPy framework, and employed a stacked generalization ensemble with XGBoost for classification.

Result: The pipeline achieved a 91.3% accuracy in citation intent classification, setting a new benchmark for Turkish citation analysis.

Conclusion: This research provides a valuable dataset and methodology for Turkish NLP, facilitating reliable and automated citation intent analysis for future studies.

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [100] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: This paper introduces AutoSCORE, a multi-agent framework using large language models (LLMs) for rubric-aligned automated scoring that addresses common challenges such as low accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Automated scoring can enhance education by reducing human rater dependency, but current LLM approaches face issues like rubric misalignment and low interpretability.

Method: The proposed framework, AutoSCORE, uses two agents: one to extract rubric-relevant components from student responses and another to assign final scores based on these components. This structured approach mimics human grading.

Result: AutoSCORE outperforms single-agent LLMs in scoring accuracy, human-machine agreement, and error metrics across diverse tasks, rubrics, and datasets, with significant benefits for complex rubrics and smaller-scale LLMs.

Conclusion: The multi-agent, rubric-aligned approach of AutoSCORE provides a scalable, interpretable, and robust solution for automated scoring in education, overcoming key limitations of current LLM-based systems.

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [101] [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
*Haotian Tan,Hiroki Ouchi,Sakriani Sakti*

Main category: cs.CL

TL;DR: Proposes SimulSense, a framework for more efficient and human-like simultaneous speech translation.


<details>
  <summary>Details</summary>
Motivation: Current systems for SimulST rely on computationally expensive models and require interleaved training data.

Method: Introduces SimulSense, which mimics human interpreters by making real-time writing decisions when perceiving new sense units.

Result: Experiments show SimulSense achieves better quality-latency tradeoff and improves decision-making speed by up to 9.6x compared to baselines.

Conclusion: SimulSense provides a significant improvement in efficiency and quality for simultaneous speech translation systems.

Abstract: How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.

</details>


### [102] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils KrÃ¼ger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: The study investigates the application of chain-of-thought (CoT) prompting in clinical contexts using large language models (LLMs) and finds significant performance degradation in most models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how CoT prompting impacts LLM performance in clinical care settings, where reliable reasoning and interpretability are crucial, especially when dealing with challenging clinical texts like electronic health records (EHRs).

Method: The study evaluates 95 advanced LLMs across 87 real-world clinical text tasks, spanning 9 languages and 8 task types. Analyses include reasoning length, medical concept alignment, and error profiling, using both LLM-as-a-judge and expert evaluations.

Result: Findings show that 86.3% of models experience consistent performance degradation under CoT prompting. Stronger models are less affected, while less capable ones see significant declines. Systematic patterns of failure in clinical settings are revealed.

Conclusion: While CoT prompting improves interpretability, it may compromise reliability in clinical text applications. The study underscores the need for transparent reasoning strategies tailored for clinical care settings.

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [103] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: This paper introduces ThaiFACTUAL, a novel framework for mitigating biases in political stance detection tasks for large language models (LLMs) in the complex Thai political setting, without requiring model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Large language models often exhibit systematic biases in culturally complex and low-resource political settings, like Thailand, where indirect language and entangled sentiment and stance complicate fair and reliable stance detection.

Method: ThaiFACTUAL employs counterfactual data augmentation and rationale-based supervision to untangle sentiment from stance and mitigate political bias in LLMs. Additionally, the authors introduce a high-quality Thai political stance dataset annotated comprehensively.

Result: ThaiFACTUAL reduces spurious correlations, improves zero-shot generalization, and enhances fairness across multiple LLMs in political stance tasks.

Conclusion: Culturally grounded debiasing techniques, such as ThaiFACTUAL, are crucial for improving fairness and reliability in stance detection for underrepresented languages and complex cultural contexts.

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [104] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: This paper proposes a framework, MotivGraph-SoIQ, that enhances ideation in large language models (LLMs) by integrating a motivational knowledge graph and Socratic dialogue to address grounding and bias challenges.


<details>
  <summary>Details</summary>
Motivation: LLMs have potential for academic ideation, but face challenges in grounding ideas and mitigating confirmation bias during refinement.

Method: The authors introduce a framework combining a Motivational Knowledge Graph, which organizes problems, challenges, and solutions, with a dual-agent Socratic Ideator to refine ideas through Socratic questioning.

Result: The proposed framework demonstrated superior performance in terms of LLM scoring, ELO ranking, and human evaluation on the ICLR25 paper topics dataset compared to existing methods.

Conclusion: MotivGraph-SoIQ effectively improves idea grounding and refinement in LLM ideation, addressing key issues of bias and quality enhancement.

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [105] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: The paper addresses Large Language Models' (LLMs) hallucination problems and proposes a black-box detection metric based on analyzing their uncertainty expression, showing promising performance.


<details>
  <summary>Details</summary>
Motivation: The need to detect and mitigate hallucinations in Large Language Models (LLMs) without relying on restricted internal states or external resources.

Method: Developed a black-box hallucination detection metric focusing on how LLMs express uncertainty, supported by behavior analysis of their consistent and non-consistent responses.

Result: Their metric outperformed baselines that relied on internal LLM states in predicting response factuality.

Conclusion: The proposed black-box approach is viable for detecting hallucinations, especially when internal APIs or extensive resources are unavailable.

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [106] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: The paper introduces GraphSearch, a novel approach to enhance GraphRAG by addressing limitations in retrieval depth and graph data utilization, leading to better performance on multi-hop reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of shallow evidence retrieval and inefficient use of graph-based structured knowledge, which hinders advanced reasoning in traditional GraphRAG models.

Method: They propose GraphSearch, which features a modular framework with six modules for iterative reasoning and a dual-channel retrieval strategy that simultaneously uses semantic and relational queries for complementary knowledge extraction.

Result: GraphSearch demonstrated significant improvements in answer accuracy and generation quality across six multi-hop RAG benchmarks.

Conclusion: GraphSearch effectively advances the capabilities of graph retrieval-augmented generation by enabling deeper and more structured reasoning with complementary retrieval strategies.

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [107] [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
*Evangelia Zve,Benjamin Icard,Alice Breton,Lila Sainero,Gauvain Bourgne,Jean-Gabriel Ganascia*

Main category: cs.CL

TL;DR: Outliers in topic modeling can signal emerging topics over time using vector embeddings and clustering approaches.


<details>
  <summary>Details</summary>
Motivation: To investigate if outliers, usually considered noise, can reveal emerging topics in dynamic news corpora.

Method: Used vector embeddings from language models and a cumulative clustering technique to track outlier evolution in news datasets.

Result: Outliers consistently evolved into coherent topics over time in datasets about corporate social responsibility and climate change.

Conclusion: Outliers play a meaningful role in topic evolution and are valuable for identifying emerging trends across languages and models.

Abstract: This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.

</details>


### [108] [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
*Jean Seo,Hyunkyung Lee,Gibaeg Kim,Wooseok Han,Jaehyo Yoo,Seungseop Lim,Kihun Shin,Eunho Yang*

Main category: cs.CL

TL;DR: The paper presents a fine-grained taxonomy called TACOS for enhancing safety in clinical chatbot applications.


<details>
  <summary>Details</summary>
Motivation: The inaccurate or harmful responses in clinical chatbots pose significant safety risks, highlighting the need for better methods than existing guardrails or tool-calling approaches.

Method: They developed a 21-class taxonomy, TACOS, that integrates user intent classification with safety filtering and tool selection, and evaluated it through a curated dataset and experiments.

Result: Their taxonomy addressed a wide range of clinical and non-clinical queries, showing the advantages in clinical contexts and providing insights into data distribution and model knowledge.

Conclusion: TACOS enhances the ability to manage nuanced safety demands in clinical agents and highlights the importance of tailored taxonomies for complex domains like healthcare.

Abstract: Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

</details>


### [109] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: The paper introduces the Fuzzy Reasoning Chain (FRC) framework, combining LLM semantic priors and fuzzy membership degrees to address challenges in handling ambiguous or uncertain text. It improves interpretability and robustness in sentiment analysis.


<details>
  <summary>Details</summary>
Motivation: Challenges persist in NLP, particularly in dealing with ambiguity, polysemy, or uncertainty in text interpretation, which existing probability-based methods struggle to address.

Method: The FRC framework integrates large language model-based semantic priors with fuzzy membership reasoning, allowing ambiguous inputs to be transitioned into clear decisions while capturing uncertainty signals.

Result: The framework was validated on sentiment analysis tasks, and results demonstrated stable reasoning, improved interpretability, and effective knowledge transfer across different model scales.

Conclusion: FRC provides a novel and general mechanism for managing ambiguous expressions, improving robustness and interpretability beyond traditional probability-based approaches.

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [110] [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
*Yudong Li,Yufei Sun,Yuhan Yao,Peiru Yang,Wanyue Li,Jiajun Zou,Yongfeng Huang,Linlin Shen*

Main category: cs.CL

TL;DR: This paper presents RedNote-Vibe, a novel 5-year dataset designed for analyzing AI-Generated Text (AIGT) on social media, along with a new detection framework, PLAD, leveraging psycholinguistic features.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated text on social media raises challenges in detection, especially in dynamic temporal contexts, requiring datasets and methods tailored to longitudinal analyses.

Method: The authors created the RedNote-Vibe dataset from Xiaohongshu platform data, capturing user engagement over 5 years. They also developed PLAD, using psycholinguistic features to distinguish AI-generated and human content.

Result: PLAD showed superior AIGT detection performance compared to existing methods while revealing how psycholinguistic markers relate to social media engagement trends.

Conclusion: This work provides foundational toolsâ€”via RedNote-Vibe and PLADâ€”for studying the interplay of AI-generated content and user dynamics on social media over time, advancing interpretability in detection models.

Abstract: The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

</details>


### [111] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: This paper introduces QCET, a taxonomy to standardize NLP quality criterion names and definitions, addressing the issue of unclear evaluation comparability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the problem of misleading comparability in NLP evaluations, which has hindered scientific progress in the field.

Method: The authors derive a taxonomy (QCET) through a descriptive analysis of three surveys on NLP evaluations, creating a hierarchical structure with standardized definitions.

Result: QCET provides a standard set of quality criteria, allowing for better comparability among evaluations and supporting evaluation design and regulatory compliance.

Conclusion: The taxonomy enhances scientific reliability in NLP by creating consistency in evaluation criteria and addressing pervasive ambiguity in past practices.

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [112] [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
*Wanli Yang,Fei Sun,Rui Tang,Hongyu Zang,Du Su,Qi Cao,Jingang Wang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper argues that fine-tuning has been incorrectly perceived as ineffective for model editing due to improper adaptation. By using a breadth-first pipeline and local tuning strategies, it enhances fine-tuning, achieving superior performance in scalability and effectiveness.


<details>
  <summary>Details</summary>
Motivation: The prevailing belief is that fine-tuning fails as a model editing method, largely due to issues with the applied sequential optimization approach. The paper aims to revisit this assumption and reshape the understanding of fine-tuning's capabilities.

Method: The study proposes the LocFT-BF method, which utilizes fine-tuning adjusted to a breadth-first pipeline paired with systematic analysis of tuning parameter locations for more efficient localized edits.

Result: LocFT-BF delivers state-of-the-art results, successfully handling 100K edits and scaling to models with 72B parameters without compromising general model capabilities.

Conclusion: Fine-tuning is established as a viable and competitive method for model editing, overturning previous misconceptions. The paper offers a refined framework and lays groundwork for further advancements in the field.

Abstract: Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.

</details>


### [113] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: The paper introduces CoSpaDi, a new approach for compressing large language models (LLMs) using structured sparse dictionary learning instead of traditional low-rank weight approximation. This approach provides better accuracy and efficiency in model compression.


<details>
  <summary>Details</summary>
Motivation: Traditional low-rank weight approximation used in compressing large language models (LLMs) often results in reduced model accuracy due to the rigidity of the imposed structural constraint. The authors aim to improve this approach by introducing a more flexible compression technique with minimal accuracy loss.

Method: The authors propose CoSpaDi, a compression framework that uses structured sparse dictionary learning. It replaces low-rank decomposition by approximating weight matrices with a dense dictionary and a column-sparse coefficient matrix, allowing a union-of-subspaces representation. A small calibration dataset is used to optimize the factorization for preserving output activations close to those of the original model.

Result: CoSpaDi outperforms state-of-the-art low-rank methods in accuracy and perplexity metrics when applied to various Llama and Qwen models under compression ratios of 20-50%. It demonstrates the effectiveness of structured sparse dictionary learning and is compatible with additional post-training quantization.

Conclusion: Structured sparse dictionary learning presents a compelling alternative to traditional low-rank approaches for efficient LLM deployment. CoSpaDi achieves better model fidelity, reduced memory usage, and computational efficiency, making it a superior option for compressing large language models.

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [114] [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
*Justin Vasselli,Eunike Andriani Kardinata,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: The paper introduces the Dialogue Act Script (DAS) framework to create localized multilingual dialogues from abstract intent representations without direct translation, improving cultural and contextual appropriateness.


<details>
  <summary>Details</summary>
Motivation: Dialogue systems are generally trained or evaluated on translated English-language dialogues, which may introduce artifacts, diminishing their naturalness and cultural relevance, especially in non-English contexts.

Method: The proposed DAS framework encodes dialogues using structured dialogue act representations. These representations allow for dialogue generation directly in the target language, ensuring localization and mitigating issues like translationese.

Result: Experiments on Italian, German, and Chinese showed that DAS-generated dialogues consistently surpassed human and machine-translated dialogues in terms of cultural relevance, coherence, and situational appropriateness.

Conclusion: The DAS framework enables the creation of more natural, culturally appropriate multilingual dialogues, providing an alternative to direct translations and improving the overall quality and adaptability of multilingual dialogue systems.

Abstract: Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

</details>


### [115] [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
*Shaoning Sun,Jiachen Yu,Zongqi Wang,Xuewei Yang,Tianle Gu,Yujiu Yang*

Main category: cs.CL

TL;DR: This paper identifies the solve-to-judge gap in generative reward models (GRMs) and proposes the Solve-to-Judge (S2J) method to reduce it, enhancing judgment performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap where GRMs fail to make correct judgments on some queries, despite being capable of solving them (14%-37%).

Method: The paper introduces the S2J approach, which simultaneously leverages a GRM's solving and judging abilities for supervision, explicitly linking these capabilities during optimization.

Result: S2J reduces the solve-to-judge gap by 16.2% and improves judgment performance by 5.8%, achieving state-of-the-art performance using a smaller dataset.

Conclusion: The S2J approach narrows the gap between problem-solving and judgment capabilities in GRMs, optimizing their evaluation performance through self-evolution without external distillation.

Abstract: With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

</details>


### [116] [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
*Primakov Chungkham,V Venktesh,Vinay Setty,Avishek Anand*

Main category: cs.CL

TL;DR: Large language models struggle with fact-checking complex numerical claims. This study introduces a verifier model (VERIFIERFC) to enhance reasoning accuracy using a Test-Time Scaling (TTS) methodology.


<details>
  <summary>Details</summary>
Motivation: Fact-checking numerical claims is challenging due to the requirements for compositional and numerical reasoning, alongside issues like reasoning drift in large language models.

Method: They developed a verifier model named VERIFIERFC to navigate multiple reasoning paths via Test-Time Scaling (TTS) and introduced an adaptive TTS mechanism to improve computational efficiency.

Result: The TTS approach effectively mitigated reasoning drift and achieved an 18.8% improvement in performance compared to single-shot verification, delivering 1.8x higher efficiency.

Conclusion: The proposed TTS methodology and VERIFIERFC model significantly improve the accuracy and efficiency of fact-checking numerical claims.

Abstract: Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC

</details>


### [117] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: The paper introduces "Uni-LAP," a combined framework using SCMs and LLMs for improving legal article prediction across jurisdictions, achieving superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Legal article prediction is crucial for judicial decision-making but existing techniques face challenges with the complexity of cases, limitations of supervised models, and incompatibilities across jurisdictions.

Method: The Uni-LAP framework enhances SCMs with a Top-K loss function for candidate generation and employs LLM-driven syllogism-inspired reasoning for refinement, combining their strengths.

Result: Uni-LAP demonstrated better prediction accuracy and generalizability across datasets from diverse jurisdictions, outperforming baseline approaches.

Conclusion: Uni-LAP effectively addresses key limitations in legal article prediction methods and offers a universal framework applicable across different legal systems.

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [118] [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
*Andrei-Alexandru Manea,JindÅ™ich LibovickÃ½*

Main category: cs.CL

TL;DR: The paper reviews multilingual vision-language models, focusing on the trade-off between language neutrality and cultural awareness.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in balancing language neutrality and cultural awareness in vision-language models.

Method: The authors reviewed 31 models and 21 benchmarks, analyzing their architectures, training methods, and evaluation approaches.

Result: The study found a preference for language neutrality through contrastive learning and noted gaps between training objectives and evaluation goals.

Conclusion: Training and evaluation approaches should better align and incorporate diverse, culturally grounded content to improve cross-lingual performance.

Abstract: This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.

</details>


### [119] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,SaÅ¡o DÅ¾eroski,Barbara KorouÅ¡iÄ‡ Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: The paper introduces FoodSEM, a fine-tuned language model that excels at linking food-related entities to multiple ontologies, achieving state-of-the-art performance with high accuracy.


<details>
  <summary>Details</summary>
Motivation: General-purpose and domain-specific models struggle with accurately solving food named-entity linking (NEL). The research addresses this gap by creating a specialized model for the food domain.

Method: The authors developed FoodSEM using fine-tuning of large language models within an instruction-response (IR) framework to link food-related entities to ontologies like FoodOn and SNOMED-CT.

Result: FoodSEM demonstrated best-in-class performance, reaching F1 scores of up to 98% on specific datasets, outperforming zero-shot and few-shot approaches.

Conclusion: FoodSEM advances food NEL with state-of-the-art accuracy, provides resources for further research, and establishes a strong baseline for future benchmarking in the domain.

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [120] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: The paper introduces a Reasoning Capsule (R-Capsule) framework to improve reasoning efficiency, accuracy, and interpretability in large language models by blending explicit and latent reasoning methods.


<details>
  <summary>Details</summary>
Motivation: To mitigate verbosity, latency, and error propagation in Chain-of-Thought (CoT) prompting while preserving reasoning efficiency and transparency.

Method: Develops a hybrid approach with learned latent tokens (Reasoning Capsules) inspired by the Information Bottleneck principle, balancing minimality for efficiency and sufficiency for task accuracy.

Result: The R-Capsule framework reduces token footprint while maintaining or improving accuracy on complex benchmarks, achieving a balance between efficiency, accuracy, and interpretability.

Conclusion: R-Capsule successfully combines latent reasoning efficiency and explicit reasoning transparency, offering an effective alternative to CoT reasoning in large language models.

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [121] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: The paper presents Group Tree Optimization (GTO), a novel method to speed up large language model (LLM) inference by aligning training objectives with decoding-time policies, achieving better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in speculative decoding for LLM inference caused by a misalignment between the single-path-focused training objectives and the tree policy used during decoding.

Method: GTO consists of two core components: (1) Draft Tree Reward, a sampling-free objective that evaluates the expected acceptance length of the draft tree, and (2) Group-based Draft Policy Training, which uses a PPO-style optimization scheme with debiased comparisons to improve the draft model's alignment with decoding-time policies.

Result: GTO increased acceptance length by 7.4% and delivered an additional 7.7% speedup compared to the previous state-of-the-art method across tasks like dialogue, code, and math on several LLMs.

Conclusion: Group Tree Optimization helps bridge the gap between training and decoding policies in speculative decoding, offering a general and practical solution to enhance LLM inference efficiency and performance.

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [122] [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
*Raia Abu Ahmad,Rana Abdulla,Tilahun Abedissa Taffa,Soeren Auer,Hamed Babaei Giglou,Ekaterina Borisova,Zongxiong Chen,Stefan Dietze,Jennifer DSouza,Mayra Elwes,Genet-Asefa Gesese,Shufan Jiang,Ekaterina Kutafina,Philipp Mayr,Georg Rehm,Sameer Sadruddin,Sonja Schimmler,Daniel Schneider,Kanishka Silva,Sharmila Upadhyaya,Ricardo Usbeck*

Main category: cs.CL

TL;DR: The paper reviews 12 shared tasks from the NFDI4DS consortium that promote FAIR principles and reproducible research in scholarly document processing.


<details>
  <summary>Details</summary>
Motivation: Shared tasks are essential for standardized evaluation and fostering transparent and reproducible research practices within the scientific community.

Method: The paper highlights shared tasks hosted by NFDI4DS, focusing on diverse challenges in scholarly document processing and creating open-access datasets, tools, and models.

Result: The tasks led to methodological advancements and provided resources that integrate into the NFDI4DS consortium's infrastructure.

Conclusion: Shared tasks are instrumental in driving innovation and FAIR practices, while enhancing research data accessibility and collaboration in the broader scholarly community.

Abstract: Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.

</details>


### [123] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: This paper introduces MACC, a framework for compressing Chain-of-Thought (CoT) reasoning outputs to reduce latency while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the latency problem of verbose Chain-of-Thought reasoning outputs without sacrificing accuracy.

Method: The method, MACC, progressively compresses CoTs using multiround refinement and leverages token elasticity to determine optimal compression depth adaptively.

Result: MACC improved average accuracy by 5.6% compared to state-of-the-art baselines, reduced CoT length by 47 tokens on average, and improved latency.

Conclusion: CoT compression via MACC is effective, predictable, and enables efficient model selection and forecasting without repeated fine-tuning.

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [124] [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
*Sai Teja Lekkala,Yadagiri Annepaka,Arun Kumar Challa,Samatha Reddy Machireddy,Partha Pakray,Chukhu Chunka*

Main category: cs.CL

TL;DR: This paper studies issues around detecting machine-generated text, introducing methods for binary/multiclass classification, adversarial attack mitigation, and human-AI collaborative segmentation. It also presents a new dataset, BMAS English, for these tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address questions on human creativity preservation amid increasing capabilities of LLMs and to improve existing methods for detecting machine-generated text.

Method: The paper employs machine-generated text detection techniques across multiple scenarios, including document-level binary and multiclass classification, generator attribution, adversarial attack mitigation, and sentence-level segmentation.

Result: A new dataset, BMAS English, is developed for binary classification of human vs. machine text, multiclass classification for identifying machine generators, adversarial attack mitigation, and sentence-level segmentation.

Conclusion: The proposed work enhances previous efforts in Machine-Generated Text Detection (MGTD) by introducing more comprehensive tasks and datasets to address evolving challenges in the domain.

Abstract: Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.

</details>


### [125] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip JukiÄ‡,Martin Tutek,Jan Å najder*

Main category: cs.CL

TL;DR: The paper introduces CompAs, a meta-learning framework that turns context into adapter parameters with a compositional structure for large language models (LLMs). This approach overcomes inefficiencies of in-context learning (ICL) and supervised fine-tuning (SFT), offering lower inference costs, robustness, and scalability.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and limitations of ICL and SFT in adapting LLMs to new tasks, particularly when processing long contexts or integrating multiple chunks of information.

Method: The authors propose CompAs, which translates context into adapter parameters with a compositional structure. This allows algebraic merging of multiple inputs, avoids long prompt reprocessing, and enables reversible context encoding to recover input via a decoder.

Result: CompAs demonstrated superior performance over ICL and previous generator-based methods in diverse multiple-choice and extractive question-answering tasks, especially when scaling to handle more inputs.

Conclusion: Composable adapter generation using CompAs is a practical and efficient alternative for deploying large language models at scale, providing benefits like reduced costs, robustness to context length issues, and greater task flexibility.

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [126] [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
*Nicolas Boizard,Hippolyte Gisserot-Boukhlef,Kevin El-Haddad,CÃ©line Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: This paper explores Large Language Models (LLMs) for reasoning tasks, comparing Instruction Fine-Tuning (IFT) methods and reasoning frameworks across tasks and model scales.


<details>
  <summary>Details</summary>
Motivation: Investigate the effectiveness, scalability, and cost-performance trade-offs of reasoning in LLMs on diverse tasks.

Method: A synthetic data distillation framework was used to empirically compare reasoning and Instruction Fine-Tuning models across tasks and formats.

Result: Reasoning methods consistently outperform or match larger IFT models as scale increases, especially on reasoning-intensive tasks.

Conclusion: While IFT is cost-efficient, reasoning models excel with scale and surpass IFT for tasks requiring intensive reasoning or open-ended abilities.

Abstract: Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.

</details>


### [127] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: The paper argues that outputs from large language models (LLMs) are inherently meaningless because LLMs lack the necessary intentions for literal meanings. Despite this, their outputs can still appear meaningful and support acquiring knowledge.


<details>
  <summary>Details</summary>
Motivation: To question the intrinsic meaning of LLM outputs by addressing whether they can hold literal meanings without possessing intentions.

Method: The authors create a philosophical argument based on two premises: intentions are crucial for literal meanings, and LLMs cannot have these intentions. They also counter externalist and internalist semantic views.

Result: The argument demonstrates that LLMs' outputs are meaningless due to the absence of intentions, but highlights why those outputs still appear meaningful and useful.

Conclusion: LLMs cannot produce meaningful outputs in a literal sense, though their outputs seemingly hold meaning and remain useful for truth identification and knowledge acquisition.

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [128] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: The paper introduces Recursive Thematic Partitioning (RTP), a framework leveraging LLMs to generate interpretable taxonomies for text corpora via a binary tree structure, outperforming traditional keyword-based topic models.


<details>
  <summary>Details</summary>
Motivation: Traditional topic models often produce output that lacks semantic coherence and requires manual interpretation, particularly in data-scarce domains. There is a need for a more interpretable and semantically coherent approach to unsupervised text analysis.

Method: The authors propose RTP, which uses Large Language Models (LLMs) to interactively create binary trees. Each node represents a question for semantically partitioning the data, resulting in an interpretable cluster taxonomy.

Result: RTP demonstrates enhanced interpretability over baseline models like BERTopic and provides effective features for downstream classification tasks when themes in data align with task labels. The approach also supports structured prompts for generative models.

Conclusion: RTP redefines text analysis by focusing on knowledge-driven thematic analysis instead of statistical pattern discovery, offering a dual utility in clustering and synthesis for generative tasks.

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [129] [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
*Yuhan Song,Linhao Zhang,Chuhan Wu,Aiwei Liu,Wei Jia,Houfeng Wang,Xiao Zhou*

Main category: cs.CL

TL;DR: Semantic speech tokenizers are often fragile under noise conditions; StableToken introduces a robust multi-branch mechanism to improve stability, reducing token instability and enhancing downstream SpeechLLM performance.


<details>
  <summary>Details</summary>
Motivation: Existing semantic speech tokenizers struggle with robustness against acoustic perturbations, even when speech remains intelligible. This compromises efficiency for downstream language models.

Method: The paper proposes StableToken, which uses a consensus-driven, multi-branch architecture with a bit-wise voting mechanism for improved token sequence stability in noisy conditions.

Result: StableToken drastically reduces Unit Edit Distance (UED) under various noise conditions and enhances SpeechLLM performance across multiple tasks.

Conclusion: StableToken solves stability issues in semantic speech tokenizers and directly improves performance and robustness of downstream language models in noisy environments.

Abstract: Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.

</details>


### [130] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: The paper introduces Composite Reasoning (CR) for Large Language Models to dynamically use multiple reasoning styles for better problem-solving, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in LLMs which rely on singular reasoning paradigms for intricate problems.

Method: An adaptive reasoning approach enabling LLMs to combine deductive, inductive, and abductive styles based on domain-specific problem needs.

Result: CR outperformed baselines like Chain-of-Thought (CoT) and DeepSeek-R1, demonstrating better accuracy, sample efficiency, and token usage.

Conclusion: Internal reasoning style diversity enhances LLMs' robustness, adaptability, and efficiency in solving complex problems.

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [131] [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
*Jaehoon Kim,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: This paper identifies that transferring reasoning abilities to smaller models often fails due to distributional misalignment, and proposes "Reverse Speculative Decoding" to address this issue, which improves reasoning benchmarks for the student models.


<details>
  <summary>Details</summary>
Motivation: Improving the transfer of reasoning capabilities from large to smaller language models often fails because smaller models face difficulties learning from their larger teacher models' reasoning traces.

Method: The proposed approach, called Reverse Speculative Decoding (RSD), allows the teacher to suggest tokens but has the student model determine their acceptance based on its own probability distributions. This avoids using low-probability tokens that hinder learning.

Result: Using RSD, smaller models like Qwen3-0.6B demonstrated a 4.9% improvement on reasoning benchmarks, contrasted with a 20.5% degradation when using traditional fine-tuning methods.

Conclusion: The study establishes that successful transfer of reasoning abilities requires alignment with smaller models' unique distributional characteristics; however, RSD-produced traces are specific to each student model and cannot be universally applied.

Abstract: Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.

</details>


### [132] [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
*Yuan Ge,Saihan Chen,Jingqi Xiao,Xiaoqian Liu,Tong Xiao,Yan Xiang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: The paper introduces FLEXI, a benchmark for evaluating full-duplex spoken dialogue systems in real-time, with a focus on latency, quality, and emergency responsiveness.


<details>
  <summary>Details</summary>
Motivation: Natural human-computer interaction via real-time spoken dialogue systems requires robust benchmarking and modeling for seamless communication.

Method: The authors developed FLEXI, a benchmark that assesses models on latency, quality, and conversational effectiveness across six scenarios, including emergencies.

Result: FLEXI highlighted performance gaps between open-source and commercial large language models, particularly in emergency responsiveness and interaction latency.

Conclusion: Next token-pair prediction is suggested as a promising approach for enhancing human-like full-duplex interaction in spoken dialogue systems.

Abstract: Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.

</details>


### [133] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: This paper addresses the safety of Large Language Models (LLMs) through legal compliance, leveraging frameworks like the EU AI Act and GDPR. It introduces a benchmark for safety compliance and develops 'Compliance Reasoner' for aligning LLMs with these regulations, achieving notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from addressing insufficient safety frameworks for modern LLM behaviors. Current methods are ad-hoc and lack systematic rigor, necessitating a compliance-based approach for more robust safety.

Method: The authors created a safety compliance benchmark based on legal scenarios and introduced 'Compliance Reasoner,' aligning Qwen3-8B with legal standards via Group Policy Optimization (GRPO).

Result: Compliance Reasoner demonstrated significant performance improvements on the introduced benchmark, with +10.45% for the EU AI Act and +11.85% for GDPR.

Conclusion: The proposed Compliance Reasoner successfully enhances LLM safety with legal compliance, presenting a systematic way to mitigate risks using established legal standards.

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [134] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: The study proposes SSKG-LLM, a novel model that integrates both structural and semantic information of Knowledge Graphs (KGs) to improve reasoning in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with hallucination issues and fail to utilize the structural aspects of KGs effectively due to gaps in embedding spaces.

Method: Introducing SSKG-LLM, which includes Knowledge Graph Retrieval, Knowledge Graph Encoding, and Knowledge Graph Adaptation modules to integrate structural and semantic KG information into LLM processing.

Result: Experiments demonstrate that leveraging the structural information of KGs enhances LLMs' reasoning capabilities.

Conclusion: The SSKG-LLM model proves effective at addressing hallucination issues in LLMs by efficiently incorporating KG structure and semantics, thus improving factual reasoning.

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [135] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: This paper investigates the interaction between explainability and fairness in NLP, particularly in hate speech detection, finding mixed reliability of input-based explanations.


<details>
  <summary>Details</summary>
Motivation: Bias in NLP models, coupled with their black-box nature, raises fairness concerns and complicates bias mitigation. The paper explores whether input-based explanations can help address these challenges.

Method: The authors conduct a systematic analysis focusing on encoder- and decoder-only models in hate speech detection, analyzing three areas: identifying biased predictions, selecting fair models, and bias mitigation during training.

Result: The study finds that input-based explanations are effective for detecting biased predictions and aiding bias mitigation, but not reliable for selecting the fairest models among candidates.

Conclusion: Input-based explanations have potential for reducing bias in NLP, especially during training, but their utility for model selection is limited.

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [136] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,BjÃ¶rn Gehrke*

Main category: cs.CL

TL;DR: The study evaluates fine-tuned large language models (LLMs) for translating natural language into first-order logic (FOL), highlighting model architectures, training strategies, and key findings.


<details>
  <summary>Details</summary>
Motivation: Automating natural language translation to FOL is essential for knowledge representation and formal methods, but it remains challenging.

Method: The authors systematically evaluate different LLM architectures (encoder-decoder vs. decoder-only) and training strategies using two datasets, introducing various techniques and metrics.

Result: Fine-tuned Flan-T5-XXL achieves 70% accuracy, outperforming other models and symbolic systems, with key findings indicating the importance of predicate availability and model type.

Conclusion: Predicate extraction is the primary bottleneck in structural logic translation, with multi-faceted improvements and model capabilities playing a crucial role in advancing FOL translation.

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [137] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: The paper investigates transformer models' ability to infer transitive relations, using grid-like directed graphs. Results show performance depends on graph dimensionality and model scale, but transformers struggle with non-grid graphs and disconnected components.


<details>
  <summary>Details</summary>
Motivation: To ensure factual correctness in transformer-based LLM responses by studying their reasoning capabilities, particularly in inferring transitive relations. This is essential for tasks like causal inference.

Method: Generated directed graphs of varying sizes to train transformers of different scales and evaluated their ability to infer connectivity for various graph configurations.

Result: Transformers effectively learn connectivity in low-dimensional grid graphs, with higher-dimensional grid graphs posing challenges. Scaling the models improves generalization but performance drops in non-grid graphs with increased disconnected components.

Conclusion: Transformers can infer connectivity better in structured, low-dimensional settings, and scaling aids generalization. However, they struggle with complex graph types like disconnected graphs with numerous components.

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [138] [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
*Sophie Spliethoff,Sanne Hoeken,Silke Schwandt,Sina ZarrieÃŸ,Ã–zge AlaÃ§am*

Main category: cs.CL

TL;DR: This paper explores using NLP to study religious invective during the Protestant Reformation in Tudor England, creating the InviTE corpus and evaluating models for historical text analysis.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to advance historical research by applying NLP techniques specifically to study invective language during the Protestant Reformation, providing insights into language use in Early Modern English.

Method: The authors employ a structured workflow for data collection and annotation, creating the InviTE corpus with 2000 expert-annotated Early Modern English sentences. They also test fine-tuned BERT-based models and instruction-tuned LLMs for invective detection.

Result: The experiments demonstrate that models fine-tuned on historical data significantly outperform zero-shot instruction-tuned LLMs in detecting invective language.

Conclusion: The study demonstrates the effectiveness of applying fine-tuned NLP models to historical text, underscoring the value of domain-specific training in linguistic research of historical corpora.

Abstract: In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.

</details>


### [139] [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
*Christoph Unger,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: The paper discusses the application of Bayesian approaches to relevance-theoretic pragmatics, focusing on conversational implicatures.


<details>
  <summary>Details</summary>
Motivation: To explore how Bayesian probability theory can be applied to understand pragmatic phenomena, specifically relevance-theoretic pragmatics.

Method: Examines conversational implicatures using Rational Speech Act theory within a Bayesian framework.

Result: Demonstrates that Bayesian approaches can model implicit communication effectively within relevance-theoretic pragmatics.

Conclusion: Bayesian frameworks show promise in advancing our understanding of implicit meaning communicated through conversational implicatures, bridging cognitive science and pragmatics.

Abstract: Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

</details>


### [140] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: The paper introduces CHRONOBERG, a temporally structured corpus of English book texts spanning 250 years, designed to analyze diachronic linguistic changes and train LLMs for better temporal contextualization.


<details>
  <summary>Details</summary>
Motivation: Modern LLMs lack the ability to reliably capture diachronic variation and the long-term temporal structure in language due to the limited temporal organization of existing corpora.

Method: The authors curate a temporally structured corpus from Project Gutenberg, including temporal annotations, and conduct temporally sensitive Valence-Arousal-Dominance analysis. Historical affective lexicons are also developed and used to analyze LLM performance in detecting discriminatory language and sentiment over time.

Result: The study highlights that language models trained on CHRONOBERG struggle to encode diachronic shifts in meaning, pointing to the importance of temporally aware training and evaluation processes.

Conclusion: CHRONOBERG provides a crucial resource for studying linguistic evolution and improving LLMsâ€™ temporal understanding, thus advocating for enhanced temporal attention in training pipelines.

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [141] [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
*Max Malyi,Jonathan Shek,Andre Biscaya*

Main category: cs.CL

TL;DR: The paper introduces and tests a framework using large language models (LLMs) to analyze unstructured wind turbine maintenance logs for deeper insights beyond simple text classification.


<details>
  <summary>Details</summary>
Motivation: Traditional reliability analysis struggles with unstructured free-text data in wind turbine maintenance logs, limiting operational insights. Existing machine learning approaches are inadequate as they focus only on classification.

Method: An exploratory framework utilizing large language models (LLMs) was developed to perform deep semantic analysis on maintenance logs. The framework is tested across four workflows: failure mode identification, causal chain inference, comparative site analysis, and data quality auditing.

Result: The results show that LLMs effectively synthesize textual content, generating actionable insights and expert-level hypotheses. They advance beyond mere labeling to enable deeper operational intelligence.

Conclusion: The methodology demonstrates that LLMs can serve as "reliability co-pilots" in operational settings, unlocking hidden insights in unstructured data. This offers a reproducible pathway for transforming operational intelligence in the wind energy sector.

Abstract: A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.

</details>


### [142] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: The paper examines political biases in large language models (LLMs), focusing on their training data. It identifies a predominance of left-leaning content and its correlation with the models' biases.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs exhibiting politically biased text, the underlying causes related to training data remain poorly understood. This paper aims to address this gap by analyzing political content in training data.

Method: Researchers analyze the training data of the open-source OLMO2 model. They use large random samples, annotate documents for political orientation, and examine their domains and content. Additionally, they investigate correlations between training data and models' biases on policy issues.

Result: The study finds that left-leaning content dominates the datasets, with pre-training data being more politically charged than post-training data. It also highlights differences in framing between left- and right-leaning content, and strong correlations between training data stance and model bias.

Conclusion: The analysis demonstrates the influence of political bias in training data on LLM output. The authors advocate for integrating political content analysis into data curation pipelines and improving transparency regarding data filtering.

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [143] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: The paper introduces Chimera, a test suite to evaluate diagram comprehension in vision-language models (VLMs), revealing that current VLMs often rely on shortcut behaviors rather than genuine understanding.


<details>
  <summary>Details</summary>
Motivation: Diagrams pose unique challenges for AI models, given their reliance on symbolic and visual processing. Despite apparent success, there is concern whether VLMs truly comprehend diagrams or exploit shortcuts.

Method: The authors developed Chimera, a test suite of 7,500 annotated diagrams with semantic triples and multi-level questions. Chimera assesses recognition, understanding, grounding, and reasoning capabilities in VLMs.

Result: Evaluation of 15 VLMs showed their strong performance is largely due to shortcuts: minor influence from visual memorization, moderate dependence on knowledge recall, and significant exploitation of Clever-Hans shortcuts.

Conclusion: Current VLMs demonstrate critical weaknesses in genuinely understanding diagrams. Robust benchmarks like Chimera are necessary to advance genuine comprehension in AI models.

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [144] [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
*Maor Juliet Lavi,Tova Milo,Mor Geva*

Main category: cs.CL

TL;DR: This paper addresses unanswerability detection in extractive QA by proposing a method that identifies directions in the model's activation space for classification.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with confidently providing incorrect answers due to unanswerable questions, necessitating better unanswerability detection methods.

Method: Identify an activation direction representing unanswerability through activation additions during inference and evaluate its effectiveness using projection scores.

Result: The proposed approach outperforms existing methods in detecting unanswerable questions, generalizes across datasets, and adapts to other unanswerability scenarios.

Conclusion: The method reliably detects unanswerable questions, generalizes effectively, and offers control over the model's abstention behavior.

Abstract: Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.

</details>


### [145] [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
*Antreas Ioannou,Andreas Shiamishis,Nora Hollenstein,Nezihe Merve GÃ¼rel*

Main category: cs.CL

TL;DR: The paper evaluates LLaMA and Gemini models on multilingual legal benchmarks, showing challenges like low accuracy on legal reasoning and adversarial vulnerabilities, with Gemini outperforming LLaMA.


<details>
  <summary>Details</summary>
Motivation: To understand the capabilities and limitations of LLMs, particularly in high-stakes fields like law, given their growing integration into legal workflows.

Method: The study evaluates LLMs using multilingual legal and non-legal benchmarks, assesses adversarial robustness through perturbations, employs an LLM-as-a-Judge approach, and introduces an open-source modular evaluation pipeline.

Result: Legal tasks, especially reasoning benchmarks, see LLM accuracies below 50%. English is more stable but not always more accurate. Gemini outperforms LLaMA by 24 percentage points on average, though challenges persist.

Conclusion: Although newer LLMs show improvements, deploying them reliably for critical multilingual legal applications remains difficult due to inherent limitations.

Abstract: In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

</details>


### [146] [NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use](https://arxiv.org/abs/2509.22479)
*Yuqing Zhang,Ecesu Ãœrker,Tessa Verhoef,Gemma Boleda,Arianna Bisazza*

Main category: cs.CL

TL;DR: The paper introduces NeLLCom-Lex, a neural framework for simulating semantic change in lexical systems based on communicative needs.


<details>
  <summary>Details</summary>
Motivation: Existing observational and experimental methods fail to fully capture causal mechanisms of lexical semantic change, especially due to the extended diachronic nature of the process.

Method: The framework grounds neural agents in a real lexical system and manipulates their communicative needs to simulate semantic evolution using a well-known color naming task.

Result: NeLLCom-Lex demonstrates that neural agents can reproduce human-like behavior and adapt their lexical systems based on communicative needs, validated through supervised and reinforcement learning.

Conclusion: NeLLCom-Lex is a promising tool for analyzing and understanding the drivers of semantic change, advancing research on the mechanisms of how language evolves.

Abstract: Lexical semantic change has primarily been investigated with observational
and experimental methods; however, observational methods (corpus analysis,
distributional semantic modeling) cannot get at causal mechanisms, and
experimental paradigms with humans are hard to apply to semantic change due to
the extended diachronic processes involved. This work introduces NeLLCom-Lex, a
neural-agent framework designed to simulate semantic change by first grounding
agents in a real lexical system (e.g. English) and then systematically
manipulating their communicative needs. Using a well-established color naming
task, we simulate the evolution of a lexical system within a single generation,
and study which factors lead agents to: (i) develop human-like naming behavior
and lexicons, and (ii) change their behavior and lexicons according to their
communicative needs. Our experiments with different supervised and
reinforcement learning pipelines show that neural agents trained to 'speak' an
existing language can reproduce human-like patterns in color naming to a
remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the
mechanisms of semantic change.

</details>


### [147] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: The paper introduces 'solution divergence' as a novel metric to improve problem-solving performance in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address how solution divergence in LLMs impacts their problem-solving abilities and explore its usage as a training and evaluation metric.

Method: Investigating the relationship between solution divergence and performance across various LLMs, and leveraging divergence as a metric during supervised fine-tuning (SFT) and reinforcement learning (RL).

Result: Findings show higher solution divergence leads to better problem-solving performance and improves success rates in three problem domains.

Conclusion: Solution divergence is an effective and simple tool that enhances training and evaluation of LLMs, showing potential in both SFT and RL strategies.

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [148] [JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA](https://arxiv.org/abs/2509.22490)
*Hossain Shaikh Saadi,Minh Duc Bui,Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: The paper presents JGU Mainz's participation in the WMT25 challenge, focusing on machine translation and question answering for specific Slavic languages. The team fine-tuned a Qwen2.5-3B-Instruct model, utilized additional data and ensembling, and achieved results surpassing the baseline.


<details>
  <summary>Details</summary>
Motivation: To improve machine translation and question answering tasks for under-resourced Slavic languages like Ukrainian, Upper Sorbian, and Lower Sorbian.

Method: The authors used parameter-efficient fine-tuning of a Qwen2.5-3B-Instruct model, integrating extra data for both tasks, adding retrieval-augmented generation for Ukrainian QA, and applying ensembling for QA in Sorbian languages.

Result: Experimental results showed that their models outperformed the baseline in both machine translation and question answering tasks.

Conclusion: The study demonstrates the effectiveness of leveraging fine-tuning, additional data, and advanced techniques like ensembling to enhance natural language processing performance for low-resource Slavic languages.

Abstract: This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.

</details>


### [149] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: This paper introduces a training-free method to represent large language models (LLMs) as linear operators in semantic task spaces for scalable and interpretable model selection.


<details>
  <summary>Details</summary>
Motivation: The need for a scalable and interpretable approach to identify the best-performing LLM for a given task, given the abundance of pre-trained models and existing limitations in existing strategies.

Method: The authors represent LLMs as linear operators using closed-form computations of geometrical properties within the semantic task space, bypassing the need for retraining and improving scalability.

Result: The proposed approach succeeded in achieving competitive or state-of-the-art results in success prediction and model selection tasks, demonstrating adaptability in dynamic and out-of-sample scenarios.

Conclusion: The paper demonstrates an efficient and interpretable representation of LLMs, offering scalability, adaptability, and strong performance without requiring additional training.

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [150] [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: The paper proposes Adaptive Multi-Branch Steering (AMBS), a method for aligning large language models (LLMs) along multiple objectives (helpfulness, harmlessness, and honesty) without catastrophic forgetting or inconsistencies across objectives.


<details>
  <summary>Details</summary>
Motivation: Ensuring large language models align with multiple objectives (helpfulness, harmlessness, honesty) is vital for their safe and reliable deployment. Current methods face challenges like catastrophic forgetting and inference fragmentation, highlighting the need for more unified and efficient techniques.

Method: The authors introduce AMBS, a two-stage 1-to-N framework. Stage I computes shared post-attention hidden states, and Stage II uses cloned representations with a policy-reference mechanism to achieve objective-specific control while keeping cross-objective consistency.

Result: The AMBS method showed consistent improvement in aligning LLMs to the HHH objectives. In experiments with models like DeepSeek-7B, it enhanced alignment scores by 32.4% and reduced unsafe outputs by 11% compared to a naive 1-to-N baseline.

Conclusion: AMBS is an effective way to achieve unified and efficient multi-objective alignment in large language models, outperforming naive 1-to-N approaches and being competitive with current state-of-the-art methods.

Abstract: Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

</details>


### [151] [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
*Wenjun Wang,Shuo Cai,Congkai Xie,Mingfa Feng,Yiming Zhang,Zhen Li,Kejing Yang,Ming Li,Jiannong Cao,Yuan Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: This paper proposes an efficient and stable FP8 training recipe for LLMs, yielding substantial computational gains without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Training LLMs is computationally expensive, and while FP8 offers efficiency gains, a lack of open-source training recipes has limited its adoption.

Method: The authors present an end-to-end FP8 training recipe combining continual pre-training with supervised fine-tuning, using a novel hybrid-granularity quantization strategy.

Result: Experiments show that FP8 training achieves comparable performance to BF16, with up to 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput.

Conclusion: FP8 training proves to be a practical and robust alternative to BF16, enabling more efficient large-scale model training and democratizing the process through open-source code.

Abstract: The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.

</details>


### [152] [Think Socially via Cognitive Reasoning](https://arxiv.org/abs/2509.22546)
*Jinfeng Zhou,Zheyu Chen,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: The paper introduces Cognitive Reasoning and the CogFlow framework for improving LLMs' social cognition using structured cognitive flows and a dual reinforcement learning approach.


<details>
  <summary>Details</summary>
Motivation: Traditional logical reasoning in LLMs falls short in understanding and processing ambiguous social situations.

Method: Cognitive Reasoning is implemented through CogFlow, which combines tree-structured planning for dataset creation, supervised fine-tuning, and reinforcement learning optimized by multi-objective rewards.

Result: The experiments reveal enhancements in both social cognitive capabilities of LLMs and human social decision-making.

Conclusion: CogFlow provides a significant step forward in adapting LLMs to handle complex social problems with structured and adaptive reasoning approaches.

Abstract: LLMs trained for logical reasoning excel at step-by-step deduction to reach
verifiable answers. However, this paradigm is ill-suited for navigating social
situations, which induce an interpretive process of analyzing ambiguous cues
that rarely yield a definitive outcome. To bridge this gap, we introduce
Cognitive Reasoning, a paradigm modeled on human social cognition. It
formulates the interpretive process into a structured cognitive flow of
interconnected cognitive units (e.g., observation or attribution), which
combine adaptively to enable effective social thinking and responses. We then
propose CogFlow, a complete framework that instills this capability in LLMs.
CogFlow first curates a dataset of cognitive flows by simulating the
associative and progressive nature of human thought via tree-structured
planning. After instilling the basic cognitive reasoning capability via
supervised fine-tuning, CogFlow adopts reinforcement learning to enable the
model to improve itself via trial and error, guided by a multi-objective reward
that optimizes both cognitive flow and response quality. Extensive experiments
show that CogFlow effectively enhances the social cognitive capabilities of
LLMs, and even humans, leading to more effective social decision-making.

</details>


### [153] [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
*Wenyuan Chen,Fateme Nateghi Haredasht,Kameron C. Black,Francois Grolleau,Emily Alsentzer,Jonathan H. Chen,Stephen P. Ma*

Main category: cs.CL

TL;DR: This paper analyzes using large language models (LLMs) for drafting clinician responses to patient messages via EHR portals and introduces a method for robust evaluation to ensure accuracy and tone appropriateness.


<details>
  <summary>Details</summary>
Motivation: The increasing use of asynchronous patient-clinician messaging through EHR portals creates higher clinician workloads, sparking interest in LLM assistance. However, concerns about clinical inaccuracy and tone mismatch necessitate robust evaluation mechanisms.

Method: The paper proposes a clinically-grounded error ontology with 5 domains and 59 error codes. It also develops a Retrieval-Augmented Evaluation Pipeline (RAEC) utilizing semantically similar historical responses for quality judgment and a two-stage DSPy prompting system for interpretable error detection.

Result: The retrieval context enhanced the evaluation of clinician response drafts, improving detection of errors like clinical completeness. Human validation on 100 messages showed improved agreement (50% vs. 33% concordance) and evaluation performance (F1 = 0.500 vs. 0.256) compared to the baseline.

Conclusion: The RAEC pipeline improves evaluation effectiveness for LLM-drafted clinician responses, demonstrating promise as a safeguard for ensuring high-quality, context-appropriate messaging in patient-clinician communication.

Abstract: Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

</details>


### [154] [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)
*Yehonatan Pesiakhovsky,Zorik Gekhman,Yosi Mass,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: This paper introduces a benchmark and evaluation protocol for localizing hallucinations in model outputs using large language models (LLMs), achieving moderate success and offering insights into challenges and strategies.


<details>
  <summary>Details</summary>
Motivation: To simplify and improve the process of evaluating and localizing hallucinations in model outputs, given the limitations of existing complex pipelines and insufficient benchmarks.

Method: The researchers developed a benchmark using human-annotated examples, implemented an LLM-based evaluation protocol, introduced a representation with free-form textual descriptions for hallucination errors, and tested four large-scale LLMs with varying prompting strategies.

Result: The benchmark proved challenging, with the best-performing model achieving an F1 score of 0.67. Key difficulties included misflagging missing details and verifying information aligned with parametric knowledge rather than the source text.

Conclusion: Using LLMs for hallucination localization is promising but still challenging. Factors like prompt optimization and distinguishing factual accuracy from source alignment require further research to improve model performance.

Abstract: Context-grounded hallucinations are cases where model outputs contain
information not verifiable against the source text. We study the applicability
of LLMs for localizing such hallucinations, as a more practical alternative to
existing complex evaluation pipelines. In the absence of established benchmarks
for meta-evaluation of hallucinations localization, we construct one tailored
to LLMs, involving a challenging human annotation of over 1,000 examples. We
complement the benchmark with an LLM-based evaluation protocol, verifying its
quality in a human evaluation. Since existing representations of hallucinations
limit the types of errors that can be expressed, we propose a new
representation based on free-form textual descriptions, capturing the full
range of possible errors. We conduct a comprehensive study, evaluating four
large-scale LLMs, which highlights the benchmark's difficulty, as the best
model achieves an F1 score of only 0.67. Through careful analysis, we offer
insights into optimal prompting strategies for the task and identify the main
factors that make it challenging for LLMs: (1) a tendency to incorrectly flag
missing details as inconsistent, despite being instructed to check only facts
in the output; and (2) difficulty with outputs containing factually correct
information absent from the source - and thus not verifiable - due to alignment
with the model's parametric knowledge.

</details>


### [155] [ArabJobs: A Multinational Corpus of Arabic Job Ads](https://arxiv.org/abs/2509.22589)
*Mo El-Haj*

Main category: cs.CL

TL;DR: The paper introduces ArabJobs, a corpus of Arabic job advertisements, and demonstrates its applications in NLP and labor market studies.


<details>
  <summary>Details</summary>
Motivation: The authors aim to create a corpus that captures linguistic, regional, and socio-economic variations in Arabic job ads for research purposes.

Method: The corpus consists of 8,500 job postings and is analyzed for gender representation, occupational structure, dialectal variation, and benchmark NLP tasks.

Result: The analyses reveal insights into the Arab job market and demonstrate applications like salary estimation, job category normalization, and gender bias detection.

Conclusion: ArabJobs serves as a valuable resource for fairness-aware Arabic NLP and labor market analysis, emphasizing its utility for diverse research areas.

Abstract: ArabJobs is a publicly available corpus of Arabic job advertisements
collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.
Comprising over 8,500 postings and more than 550,000 words, the dataset
captures linguistic, regional, and socio-economic variation in the Arab labour
market. We present analyses of gender representation and occupational
structure, and highlight dialectal variation across ads, which offers
opportunities for future research. We also demonstrate applications such as
salary estimation and job category normalisation using large language models,
alongside benchmark tasks for gender bias detection and profession
classification. The findings show the utility of ArabJobs for fairness-aware
Arabic NLP and labour market research. The dataset is publicly available on
GitHub: https://github.com/drelhaj/ArabJobs.

</details>


### [156] [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
*Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.CL

TL;DR: The paper establishes that subregular language classes are linearly separable, ensuring learnability and foundation for natural language modeling.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore the learnability of subregular language classes and investigate their applicability in modeling natural language structure.

Method: They analyze the standard subregular language classes, running synthetic and real-data experiments, while also making the implementation publicly available.

Result: Synthetic experiments showed perfect separability under ideal conditions, and real-data experiments confirmed alignment with linguistic constraints.

Conclusion: The subregular hierarchy forms a robust, interpretable base for understanding and modeling natural language.

Abstract: We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.

</details>


### [157] [Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods](https://arxiv.org/abs/2509.22603)
*Rakesh Thakur,Harsh Chaturvedi,Ruqayya Shah,Janvi Chauhan,Ayush Sharma*

Main category: cs.CL

TL;DR: The study compares NLP techniques for modeling deliberation, showing the advantage of Frequency-Based Discourse Modulation and Quantum-Deliberation Frameworks.


<details>
  <summary>Details</summary>
Motivation: Advancements in NLP make it feasible to analyze opinion shifts during deliberation and forecast outcomes under diverse scenarios.

Method: The paper collected diverse viewpoints to create a dataset and simulated deliberation using enriched product presentations. It analyzed and compared NLP models.

Result: Two frameworksâ€”Frequency-Based Discourse Modulation and Quantum-Deliberationâ€”outperformed existing models in interpreting deliberative discourse.

Conclusion: The research demonstrates the effective use of NLP in areas like public policy, debate evaluation, decision-support systems, and social media analysis.

Abstract: Deliberation plays a crucial role in shaping outcomes by weighing diverse
perspectives before reaching decisions. With recent advancements in Natural
Language Processing, it has become possible to computationally model
deliberation by analyzing opinion shifts and predicting potential outcomes
under varying scenarios. In this study, we present a comparative analysis of
multiple NLP techniques to evaluate how effectively models interpret
deliberative discourse and produce meaningful insights. Opinions from
individuals of varied backgrounds were collected to construct a self-sourced
dataset that reflects diverse viewpoints. Deliberation was simulated using
product presentations enriched with striking facts, which often prompted
measurable shifts in audience opinions. We have given comparative analysis
between two models namely Frequency-Based Discourse Modulation and
Quantum-Deliberation Framework which outperform the existing state of art
models. The findings highlight practical applications in public policy-making,
debate evaluation, decision-support frameworks, and large-scale social media
opinion mining.

</details>


### [158] [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](https://arxiv.org/abs/2509.22612)
*Jonne SÃ¤levÃ¤,Duygu Ataman,Constantine Lignos*

Main category: cs.CL

TL;DR: The paper discusses resampling methods to quantify uncertainty and variability in NLP benchmarks, considering both model and data sources.


<details>
  <summary>Details</summary>
Motivation: To address the underestimated variability in performance scores due to experimental and replication factors in multilingual/multitask NLP benchmarks.

Method: A resampling-based approach to compute sampling distributions for metrics like averages, differences between models, and rankings.

Result: Demonstrated applicability of methods in tasks such as question answering, machine translation, and named entity recognition.

Conclusion: Properly quantifying variability requires accounting for both model- and data-related sources using resampling-based methods in NLP benchmarks.

Abstract: In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.

</details>


### [159] [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
*Xingyu Shen,Yingfa Chen,Zhen Leng Thai,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: StateX is introduced as a training pipeline to expand the states of pre-trained RNNs, enhancing their recall ability and in-context learning efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of recurrent models like RNNs in recalling long-context information, especially without incurring high costs of directly training larger recurrent states.

Method: The paper designs post-training architectural modifications for linear attention and state space models to scale up their state size, ensuring minimal or no increase in model parameters.

Result: StateX demonstrated efficient improvement in recall and in-context learning for RNNs, including models with up to 1.3B parameters, without high post-training costs or sacrificing other functionalities.

Conclusion: StateX is an effective and low-cost approach to enhance the performance of RNNs in tasks requiring long-context recall while preserving efficiency and minimizing resource requirements.

Abstract: While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.

</details>


### [160] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: This paper proposes a new variational reasoning framework for language models, integrating variational inference with reinforcement learning methods to improve their reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the reasoning capabilities of language models by creating a stable and principled training framework that unifies probabilistic methods and RL-based approaches.

Method: The authors developed a framework treating thinking traces as latent variables, optimized via variational inference, and introduced a multi-trace objective and forward-KL formulation for better training stability.

Result: Their approach empirically improves reasoning performance across Qwen 2.5 and Qwen 3 model families on diverse reasoning tasks while addressing biases in prior methods.

Conclusion: The work provides a probabilistic foundation that bridges variational inference and reinforcement learning methods, offering stable training objectives and advancing reasoning tasks in language models.

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [161] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: The paper introduces a novel feedback-conditional policy (FCP) that leverages verbal feedback as a condition for training language models, offering an alternative to reward-based methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing reinforcement learning techniques that compress rich verbal feedback into scalar rewards, potentially losing nuanced information.

Method: The feedback-conditional policy (FCP) uses response-feedback pairs to model the posterior distribution via maximum likelihood training. It also includes an online bootstrapping stage where the model generates outputs under positive conditions and learns iteratively through fresh feedback.

Result: FCP offers a method for learning from verbal feedback in a more expressive way than scalar reward-based approaches, enabling nuanced adjustments to language model training.

Conclusion: This framework reframes feedback-driven learning as conditional generation, emphasizing direct learning from verbal signals to improve LLM adaptation and expressiveness.

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [162] [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
*Arkadiy Saakyan,Najoung Kim,Smaranda Muresan,Tuhin Chakrabarty*

Main category: cs.CL

TL;DR: The study critiques the use of n-gram novelty for measuring textual creativity, showing its limitations by exploring creativity's dual nature of novelty and appropriateness.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether n-gram novelty effectively captures the dual aspects of creativity in text: originality and appropriateness.

Method: The study analyzes 7542 annotations by expert writers assessing novelty, pragmaticality, and sensicality of text from both human and AI sources.

Result: N-gram novelty correlates weakly with true creativity. High novelty often lacks pragmatic value, especially in open-source LLMs. Frontier closed-source models underperform relative to humans in generating creativity.

Conclusion: N-gram novelty alone is insufficient for assessing textual creativity. AI models need improvement in balancing novelty with pragmaticality and sensicality to better emulate human creativity.

Abstract: N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

</details>


### [163] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: WebGen-Agent is launched to improve website codebase generation accuracy using visual feedback and GUI-agent testing, outperforming existing systems significantly.


<details>
  <summary>Details</summary>
Motivation: Current repository-level code-generation tasks fail in maintaining quality for visually intensive and user-interaction dependent website generation, prompting the need for a novel method.

Method: WebGen-Agent incorporates visual feedback scores, GUI-agent assistance, and iterative refinement combined with the innovative Step-GRPO training, which includes dense reward supervision.

Result: Accuracy and appearance metrics for Claude-3.5-Sonnet showed remarkable improvements, and noteworthy enhancements were also observed for Qwen2.5-Coder models.

Conclusion: Integrating dense visual feedback signals significantly enhances the performance and accuracy of LLMs in website codebase generation tasks.

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


### [164] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: VoiceAssistant-Eval is a new benchmark designed to evaluate AI assistants' listening, speaking, and viewing capabilities using 10,497 curated examples across 13 task categories.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are insufficient to comprehensively assess the capabilities of modern voice-first AI assistants, necessitating the need for a new evaluation framework.

Method: The benchmark evaluates AI systems by examining task categories such as natural sounds, dialogue, music, heterogeneous images, and more through assessments of content quality, speech performance, and consistency. 21 models were tested, including GPT-4o-Audio.

Result: Key findings include: 1) proprietary models do not always outperform open-source ones, 2) models perform well in speaking but struggle in audio understanding, and 3) smaller models can be highly competitive. The mid-sized Step-Audio-2-mini (7B) surpassed larger models in listening accuracy.

Conclusion: VoiceAssistant-Eval highlights gaps in multimodal input handling, robustness, and safety alignment for AI assistants, while offering an in-depth evaluation framework to guide their development. Code and data will be made publicly available.

Abstract: The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [165] [Random Direct Preference Optimization for Radiography Report Generation](https://arxiv.org/abs/2509.21351)
*Valentin Samokhin,Boris Shirokikh,Mikhail Goncharov,Dmitriy Umerenkov,Maksim Bobrin,Ivan Oseledets,Dmitry Dylov,Mikhail Belyaev*

Main category: cs.CV

TL;DR: The paper introduces a framework to enhance Radiography Report Generation (RRG) using a model-agnostic technique called Direct Preference Optimization (DPO), demonstrating up to 5% improvement in clinical metrics without extra data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the quality gap in current Radiography Report Generation (RRG) methods, which prevents their deployment in real clinical settings, while leveraging the success of Visual Language Models (VLMs) in other domains.

Method: The paper proposes a model-agnostic framework using Direct Preference Optimization (DPO) with random contrastive sampling to train models more effectively, without relying on reward models or human annotations.

Result: Enhancements from the proposed framework are shown to improve clinical performance metrics by up to 5%, validated across three state-of-the-art RRG models.

Conclusion: The study demonstrates that the Random DPO framework can significantly improve RRG accuracy without requiring additional training data, making it a practical and impactful tool for clinical applications.

Abstract: Radiography Report Generation (RRG) has gained significant attention in
medical image analysis as a promising tool for alleviating the growing workload
of radiologists. However, despite numerous advancements, existing methods have
yet to achieve the quality required for deployment in real-world clinical
settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated
remarkable progress in the general domain by adopting training strategies
originally designed for Large Language Models (LLMs), such as alignment
techniques. In this paper, we introduce a model-agnostic framework to enhance
RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages
random contrastive sampling to construct training pairs, eliminating the need
for reward models or human preference annotations. Experiments on supplementing
three state-of-the-art models with our Random DPO show that our method improves
clinical performance metrics by up to 5%, without requiring any additional
training data.

</details>


### [166] [Improving Autism Detection with Multimodal Behavioral Analysis](https://arxiv.org/abs/2509.21352)
*William Saakyan,Matthias Norden,Lola Eversmann,Simon Kirsch,Muyu Lin,Simon Guendelman,Isabel Dziobek,Hanna Drimalla*

Main category: cs.CV

TL;DR: The paper introduces a multimodal diagnostic approach for Autism Spectrum Condition (ASC) using video analysis of behavioral markers and improves gaze-based classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Autism diagnosis is resource-intensive and existing computer-aided methods face challenges with gaze feature performance and generalizability.

Method: Used video analysis of standardized dataset including facial expressions, voice prosody, head motion, HRV, and gaze. Proposed new descriptors for gaze angle variability and integrated behavioral markers using late fusion technique.

Result: Improved gaze-based classification accuracy from 64% to 69% and achieved an overall classification accuracy of 74% using multimodal analysis.

Conclusion: Multimodal video-based screening tools appear promising for scalable and effective autism assessments.

Abstract: Due to the complex and resource-intensive nature of diagnosing Autism
Spectrum Condition (ASC), several computer-aided diagnostic support methods
have been proposed to detect autism by analyzing behavioral cues in patient
video data. While these models show promising results on some datasets, they
struggle with poor gaze feature performance and lack of real-world
generalizability. To tackle these challenges, we analyze a standardized video
dataset comprising 168 participants with ASC (46% female) and 157 non-autistic
participants (46% female), making it, to our knowledge, the largest and most
balanced dataset available. We conduct a multimodal analysis of facial
expressions, voice prosody, head motion, heart rate variability (HRV), and gaze
behavior. To address the limitations of prior gaze models, we introduce novel
statistical descriptors that quantify variability in eye gaze angles, improving
gaze-based classification accuracy from 64% to 69% and aligning computational
findings with clinical research on gaze aversion in ASC. Using late fusion, we
achieve a classification accuracy of 74%, demonstrating the effectiveness of
integrating behavioral markers across multiple modalities. Our findings
highlight the potential for scalable, video-based screening tools to support
autism assessment.

</details>


### [167] [KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache](https://arxiv.org/abs/2509.21354)
*Wanshun Xu,Long Zhuang*

Main category: cs.CV

TL;DR: The paper introduces KV-Efficient VLA, a framework addressing Vision-Language-Action (VLA) model scalability issues like high computational cost and memory inefficiency. It employs lightweight memory compression to optimize real-time applicability.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models have potential for unified robotic perception and control but face challenges with inefficient memory usage and quadratic attention costs, limiting scalability and real-time deployment.

Method: The proposed framework segments the key-value cache into chunks and uses a recurrent gating module to summarize and filter historical context based on utility scores. This maintains relevant information while pruning irrelevant data during inference.

Result: The method achieves up to 1.21x inference speedup and 36% reduction in KV memory usage with minimal impact on task success.

Conclusion: KV-Efficient VLA improves real-time capabilities of VLA models by reducing memory inefficiencies and enhancing scalability without altering existing training pipelines or control logic.

Abstract: Vision-Language-Action (VLA) models promise unified robotic perception and
control, yet their scalability is constrained by the quadratic cost of
attention and the unbounded growth of key-value (KV) memory during long-horizon
inference. While recent methods improve generalization through scaling backbone
architectures, they often neglect the inference inefficiencies critical to
real-time deployment. In this work, we present KV-Efficient VLA, a
model-agnostic memory compression framework that addresses these limitations by
introducing a lightweight, training-friendly mechanism to selectively retain
high-utility context. Our method partitions the KV cache into fixed size chunks
and employs a recurrent gating module to summarize and filter historical
context according to learned utility scores. This design preserves recent
fine-grained detail while aggressively pruning stale, low-relevance memory, all
while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x
inference speedup and 36% KV memory reduction, with minimal impact on task
success. Our method integrates seamlessly into existing autoregressive and
hybrid VLA stacks, enabling scalable inference without modifying training
pipelines or downstream control logic.

</details>


### [168] [Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports](https://arxiv.org/abs/2509.21356)
*Razi Mahmood,Diego Machado-Reyes,Joy Wu,Parisa Kaviani,Ken C. L. Wong,Niharika D'Souza,Mannudeep Kalra,Ge Wang,Pingkun Yan,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: The paper introduces a fact-checking model to detect inaccuracies and hallucinations in chest X-ray radiology reports generated by vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models generate clinically compelling radiology reports for chest X-rays, but often contain factual errors and hallucinations, which prevent their clinical adoption.

Method: The authors create a synthetic dataset simulating report errors using perturbed ground truth findings and locations. They train a multi-label cross-modal contrastive regression network for phrase-grounded fact-checking.

Result: The proposed method achieved high accuracy, robustness, and concordance with ground truth in detecting factual errors across multiple datasets, including results from state-of-the-art report generators.

Conclusion: The FC model can improve radiology workflows by effectively validating report accuracy and reducing errors during clinical inference.

Abstract: With the emergence of large-scale vision language models (VLM), it is now
possible to produce realistic-looking radiology reports for chest X-ray images.
However, their clinical translation has been hampered by the factual errors and
hallucinations in the produced descriptions during inference. In this paper, we
present a novel phrase-grounded fact-checking model (FC model) that detects
errors in findings and their indicated locations in automatically generated
chest radiology reports.
  Specifically, we simulate the errors in reports through a large synthetic
dataset derived by perturbing findings and their locations in ground truth
reports to form real and fake findings-location pairs with images. A new
multi-label cross-modal contrastive regression network is then trained on this
dataset. We present results demonstrating the robustness of our method in terms
of accuracy of finding veracity prediction and localization on multiple X-ray
datasets. We also show its effectiveness for error detection in reports of SOTA
report generators on multiple datasets achieving a concordance correlation
coefficient of 0.997 with ground truth-based verification, thus pointing to its
utility during clinical inference in radiology workflows.

</details>


### [169] [MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification](https://arxiv.org/abs/2509.21358)
*Jason Jordan,Mohammadreza Akbari Lor,Peter Koulen,Mei-Ling Shyu,Shu-Ching Chen*

Main category: cs.CV

TL;DR: The study introduces MDF-MLLM, a multimodal deep learning model combining image and textual features for retinal disease classification, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing multimodal large language models (MLLMs) in capturing low-level spatial details crucial for diagnosing retinal diseases.

Method: Developed MDF-MLLM by integrating skip features from U-Net encoder layers into a LLaMA-based MLLM, fine-tuning both components. Used image-text pairs from public datasets and evaluated using accuracy and F1-score.

Result: MDF-MLLM achieved 94% classification accuracy, a 56% improvement over baseline. Recall and F1-scores improved significantly by 67% and 35%, respectively.

Conclusion: The proposed MDF-MLLM demonstrates high accuracy and interpretability, offering a promising framework for clinical deployment and potential future extensions to broader disease classifications and segmentation tasks.

Abstract: This study aimed to enhance disease classification accuracy from retinal
fundus images by integrating fine-grained image features and global textual
context using a novel multimodal deep learning architecture. Existing
multimodal large language models (MLLMs) often struggle to capture low-level
spatial details critical for diagnosing retinal diseases such as glaucoma,
diabetic retinopathy, and retinitis pigmentosa. This model development and
validation study was conducted on 1,305 fundus image-text pairs compiled from
three public datasets (FIVES, HRF, and StoneRounds), covering acquired and
inherited retinal diseases, and evaluated using classification accuracy and
F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers
into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are
patch-wise projected and fused using scaled cross-attention and FiLM-based
U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease
classification task. MDF-MLLM, with both U-Net and MLLM components fully
fine-tuned during training, achieved a significantly higher accuracy of 94%,
representing a 56% improvement. Recall and F1-scores improved by as much as 67%
and 35% over baseline, respectively. Ablation studies confirmed that the
multi-depth fusion approach contributed to substantial gains in spatial
reasoning and classification, particularly for inherited diseases with rich
clinical text. MDF-MLLM presents a generalizable, interpretable, and modular
framework for fundus image classification, outperforming traditional MLLM
baselines through multi-scale feature fusion. The architecture holds promise
for real-world deployment in clinical decision support systems. Future work
will explore synchronized training techniques, a larger pool of diseases for
more generalizability, and extending the model for segmentation tasks.

</details>


### [170] [Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models](https://arxiv.org/abs/2509.21360)
*Xingkai Peng,Jun Jiang,Meng Tong,Shuai Li,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CV

TL;DR: This study introduces the Multimodal Prompt Decoupling Attack (MPDA) for bypassing safety filters in Text-to-Image (T2I) models, utilizing image-based inputs to generate NSFW content.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the vulnerabilities in T2I models, particularly in how jailbreak attacks can exploit these models to produce NSFW content, which is inadequately addressed by existing text-based methods.

Method: The method includes three core steps: (1) Unsafe prompts are decoupled into pseudo-safe and harmful components using a large language model (LLM), (2) natural adversarial prompts are created to bypass filters and guide the model, and (3) the final output's semantic consistency is ensured via image captions and iterative refinement by the visual language model.

Result: The paper provides a novel multimodal approach that can effectively evade the safety mechanisms of T2I systems, revealing vulnerabilities related to image-based inputs.

Conclusion: This work underscores substantial risks in T2I models, emphasizing the need for more robust safety measures to mitigate the exploitation of such vulnerabilities.

Abstract: Text-to-image (T2I) models have been widely applied in generating
high-fidelity images across various domains. However, these models may also be
abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks.
Existing jailbreak methods primarily manipulate the textual prompt, leaving
potential vulnerabilities in image-based inputs largely unexplored. Moreover,
text-based methods face challenges in bypassing the model's safety filters. In
response to these limitations, we propose the Multimodal Prompt Decoupling
Attack (MPDA), which utilizes image modality to separate the harmful semantic
components of the original unsafe prompt. MPDA follows three core steps:
firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe
prompts and harmful prompts. The former are seemingly harmless sub-prompts that
can bypass filters, while the latter are sub-prompts with unsafe semantics that
trigger filters. Subsequently, the LLM rewrites the harmful prompts into
natural adversarial prompts to bypass safety filters, which guide the T2I model
to modify the base image into an NSFW output. Finally, to ensure semantic
consistency between the generated NSFW images and the original unsafe prompts,
the visual language model generates image captions, providing a new pathway to
guide the LLM in iterative rewriting and refining the generated content.

</details>


### [171] [A Mutual Learning Method for Salient Object Detection with intertwined Multi-Supervision--Revised](https://arxiv.org/abs/2509.21363)
*Runmin Wu,Mengyang Feng,Wenlong Guan,Dong Wang,Huchuan Lu,Errui Ding*

Main category: cs.CV

TL;DR: This paper tackles salient object detection challenges like incomplete predictions and inaccurate boundaries by integrating salient object detection, foreground contour detection, and edge detection tasks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations in currently predicted saliency maps, such as incomplete predictions due to the complexity of objects and inaccurate boundaries caused by convolution and pooling operations.

Method: The authors propose an intertwined training approach for salient object detection and foreground contour detection and utilize mutual learning modules (MLMs) as core building blocks to enhance performance.

Result: Extensive experiments on seven datasets demonstrated superior performance, achieving state-of-the-art results in salient object detection and edge detection.

Conclusion: By integrating multiple tasks and leveraging mutual learning, the proposed method substantially improved saliency maps and established a strong benchmark for salient object and edge detection.

Abstract: Though deep learning techniques have made great progress in salient object
detection recently, the predicted saliency maps still suffer from incomplete
predictions due to the internal complexity of objects and inaccurate boundaries
caused by strides in convolution and pooling operations. To alleviate these
issues, we propose to train saliency detection networks by exploiting the
supervision from not only salient object detection, but also foreground contour
detection and edge detection. First, we leverage salient object detection and
foreground contour detection tasks in an intertwined manner to generate
saliency maps with uniform highlight. Second, the foreground contour and edge
detection tasks guide each other simultaneously, thereby leading to precise
foreground contour prediction and reducing the local noises for edge
prediction. In addition, we develop a novel mutual learning module (MLM) which
serves as the building block of our method. Each MLM consists of multiple
network branches trained in a mutual learning manner, which improves the
performance by a large margin. Extensive experiments on seven challenging
datasets demonstrate that the proposed method has delivered state-of-the-art
results in both salient object detection and edge detection.

</details>


### [172] [MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation](https://arxiv.org/abs/2509.21365)
*Zhicheng Du,Qingyang Shi,Jiasheng Lu,Yingshan Liang,Xinyu Zhang,Yiran Wang,Peiwu Qin*

Main category: cs.CV

TL;DR: This paper proposes MAJORScore, a new metric for evaluating the relevance of multiple modalities ($N \geq 3$) in multimodal datasets, addressing limitations in current metrics suited only for two modalities.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal relevance metrics, such as those derived from pretrained contrastive learning models, are limited to analyzing correlations between two modalities, restricting the scope of multimodal similarity evaluation.

Method: MAJORScore utilizes multimodal joint representation to integrate multiple modalities into a unified latent space, enabling fair and scalable relevance scoring across three or more modalities.

Result: MAJORScore outperforms existing metrics, showing a 26.03%â€“64.29% improvement in consistent modality relevance and a 13.28%â€“20.54% decrease in inconsistence, demonstrating higher reliability and accuracy.

Conclusion: MAJORScore is a more robust and reliable method for evaluating multiple modality relevance, particularly in large-scale multimodal datasets and model performance comparisons.

Abstract: The multimodal relevance metric is usually borrowed from the embedding
ability of pretrained contrastive learning models for bimodal data, which is
used to evaluate the correlation between cross-modal data (e.g., CLIP).
However, the commonly used evaluation metrics are only suitable for the
associated analysis between two modalities, which greatly limits the evaluation
of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation
metric for the relevance of multiple modalities ($N$ modalities, $N\ge3$) via
multimodal joint representation for the first time. The ability of multimodal
joint representation to integrate multiple modalities into the same latent
space can accurately represent different modalities at one scale, providing
support for fair relevance scoring. Extensive experiments have shown that
MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by
13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves
as a more reliable metric for evaluating similarity on large-scale multimodal
datasets and multimodal model performance evaluation.

</details>


### [173] [Safety Assessment of Scaffolding on Construction Site using AI](https://arxiv.org/abs/2509.21368)
*Sameer Prabhu,Amit Patwardhan,Ramin Karim*

Main category: cs.CV

TL;DR: This paper proposes a cloud-based AI platform to improve scaffolding inspections by comparing point cloud data with certified references, enabling automated safety assessments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of time-intensive and error-prone manual visual inspections of scaffolding that impact worker safety and structural reliability.

Method: A cloud-based AI platform is introduced to analyze point cloud data, comparing certified reference data with recent scans to identify deviations in scaffolding structures.

Result: The system automates monitoring processes, potentially reducing the time and errors associated with manual inspections, while improving construction site safety.

Conclusion: AI and digitization can significantly enhance scaffolding safety inspections, leading to more reliable structural assessments and improved worker safety.

Abstract: In the construction industry, safety assessment is vital to ensure both the
reliability of assets and the safety of workers. Scaffolding, a key structural
support asset requires regular inspection to detect and identify alterations
from the design rules that may compromise the integrity and stability. At
present, inspections are primarily visual and are conducted by site manager or
accredited personnel to identify deviations. However, visual inspection is
time-intensive and can be susceptible to human errors, which can lead to unsafe
conditions. This paper explores the use of Artificial Intelligence (AI) and
digitization to enhance the accuracy of scaffolding inspection and contribute
to the safety improvement. A cloud-based AI platform is developed to process
and analyse the point cloud data of scaffolding structure. The proposed system
detects structural modifications through comparison and evaluation of certified
reference data with the recent point cloud data. This approach may enable
automated monitoring of scaffolding, reducing the time and effort required for
manual inspections while enhancing the safety on a construction site.

</details>


### [174] [Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis](https://arxiv.org/abs/2509.21375)
*Aleksa Jelaca,Ying Jiao,Chang Tian,Marie-Francine Moens*

Main category: cs.CV

TL;DR: This paper introduces a framework to improve fine-grained counterfactual controllability in text-to-image generation, focusing on generating unconventional images like objects with counterfactual sizes.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with generating deliberate counterfactual images that contradict common-sense choices, which limits creativity and exploratory applications.

Method: The authors propose an automatic prompt engineering framework with three parts: an image evaluator for dataset construction, a supervised prompt rewriter for generating revised prompts, and a DPO-trained ranker to select the best prompt.

Result: The paper introduces the first counterfactual size text-image dataset and improves the image evaluator using Grounded SAM, achieving a 114% enhancement over its backbone. Experimental results show better performance compared to baselines including ChatGPT-4o.

Conclusion: The framework sets a precedent for addressing counterfactual controllability and paves the way for expanded creative and exploratory capabilities in text-to-image generation research.

Abstract: Text-to-image generation has advanced rapidly with large-scale multimodal
training, yet fine-grained controllability remains a critical challenge.
Counterfactual controllability, defined as the capacity to deliberately
generate images that contradict common-sense patterns, remains a major
challenge but plays a crucial role in enabling creativity and exploratory
applications. In this work, we address this gap with a focus on counterfactual
size (e.g., generating a tiny walrus beside a giant button) and propose an
automatic prompt engineering framework that adapts base prompts into revised
prompts for counterfactual images. The framework comprises three components: an
image evaluator that guides dataset construction by identifying successful
image generations, a supervised prompt rewriter that produces revised prompts,
and a DPO-trained ranker that selects the optimal revised prompt. We construct
the first counterfactual size text-image dataset and enhance the image
evaluator by extending Grounded SAM with refinements, achieving a 114 percent
improvement over its backbone. Experiments demonstrate that our method
outperforms state-of-the-art baselines and ChatGPT-4o, establishing a
foundation for future research on counterfactual controllability.

</details>


### [175] [In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence](https://arxiv.org/abs/2509.21376)
*Shiraz S Kaderuppan,Jonathan Mar,Andrew Irvine,Anurag Sharma,Muhammad Ramadan Saifuddin,Wai Leong Eugene Wong,Wai Lok Woo*

Main category: cs.CV

TL;DR: This paper explores a cost-effective approach to achieving super-resolution (SR) optical microscopy using phase-modulated modalities like Zernike phase contrast and differential interference contrast microscopy, evaluated using two deep neural networks (O-Net and Theta-Net).


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the traditional lateral resolution limit of optical microscopy (~200nm) without relying on expensive external modules or specialized techniques, making SR microscopy accessible to non-specialist contexts.

Method: The study evaluates two in silico deep neural network architectures (O-Net and Theta-Net) on their ability to super-resolve a custom-fabricated test target with nanoscale features, calibrated via atomic force microscopy.

Result: Both models successfully super-resolve images but are complementary in performance: O-Net performs better under high signal-to-noise ratios (SNRs) and Theta-Net is preferable under low SNR conditions.

Conclusion: The work emphasizes the role of DNN architecture and the source image SNR in determining model efficacy for non-fluorescent optical nanoscopy, providing a promising and economical SR microscopy solution for diverse contexts.

Abstract: The field of optical microscopy spans across numerous industries and research
domains, ranging from education to healthcare, quality inspection and analysis.
Nonetheless, a key limitation often cited by optical microscopists refers to
the limit of its lateral resolution (typically defined as ~200nm), with
potential circumventions involving either costly external modules (e.g.
confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution
(SR) fluorescent microscopy]. Addressing these challenges in a normal
(non-specialist) context thus remains an aspect outside the scope of most
microscope users & facilities. This study thus seeks to evaluate an alternative
& economical approach to achieving SR optical microscopy, involving
non-fluorescent phase-modulated microscopical modalities such as Zernike phase
contrast (PCM) and differential interference contrast (DIC) microscopy. Two in
silico deep neural network (DNN) architectures which we developed previously
(termed O-Net and Theta-Net) are assessed on their abilities to resolve a
custom-fabricated test target containing nanoscale features calibrated via
atomic force microscopy (AFM). The results of our study demonstrate that
although both O-Net and Theta-Net seemingly performed well when super-resolving
these images, they were complementary (rather than competing) approaches to be
considered for image SR, particularly under different image signal-to-noise
ratios (SNRs). High image SNRs favoured the application of O-Net models, while
low SNRs inclined preferentially towards Theta-Net models. These findings
demonstrate the importance of model architectures (in conjunction with the
source image SNR) on model performance and the SR quality of the generated
images where DNN models are utilized for non-fluorescent optical nanoscopy,
even where the same training dataset & number of epochs are being used.

</details>


### [176] [Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation](https://arxiv.org/abs/2509.21377)
*Yinfeng Yu,Hailong Zhang,Meiling Zhu*

Main category: cs.CV

TL;DR: This paper proposes a novel approach called Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN) that excels in leveraging multimodal cues for robotic navigation.


<details>
  <summary>Details</summary>
Motivation: To improve the ability of robots to locate audio sources by effectively integrating visual and auditory signals, addressing the limitations in prior multimodal fusion techniques.

Method: Introduced a multi-target architecture and a refined Transformer mechanism to filter and selectively fuse audio-visual information, tested on Replica and Matterport3D datasets.

Result: The proposed DMTF-AVN method outperformed previous techniques in success rate, path efficiency, and scene adaptation, demonstrating strong scalability and generalizability.

Conclusion: DMTF-AVN enhances multimodal fusion strategies, setting a new benchmark for audio-visual navigation in robotics, with its codebase and supplementary materials publicly available.

Abstract: Audiovisual embodied navigation enables robots to locate audio sources by
dynamically integrating visual observations from onboard sensors with the
auditory signals emitted by the target. The core challenge lies in effectively
leveraging multimodal cues to guide navigation. While prior works have explored
basic fusion of visual and audio data, they often overlook deeper perceptual
context. To address this, we propose the Dynamic Multi-Target Fusion for
Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target
architecture coupled with a refined Transformer mechanism to filter and
selectively fuse cross-modal information. Extensive experiments on the Replica
and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art
performance, outperforming existing methods in success rate (SR), path
efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits
strong scalability and generalizability, paving the way for advanced multimodal
fusion strategies in robotic navigation. The code and videos are available at
  https://github.com/zzzmmm-svg/DMTF.

</details>


### [177] [SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders](https://arxiv.org/abs/2509.21379)
*Enrico Cassano,Riccardo Renzulli,Marco Nurisso,Mirko Zaffaroni,Alan Perotti,Marco Grangetto*

Main category: cs.CV

TL;DR: SAEmnesia introduces a supervised sparse autoencoder training method to better localize and unlearn concept representations in text-to-image diffusion models.


<details>
  <summary>Details</summary>
Motivation: Concept representations in diffusion models are often distributed across multiple neurons, requiring extensive searching for unlearning. There is a need for a precise approach to mitigate this inefficiency.

Method: SAEmnesia employs supervised sparse autoencoder training coupled with systematic concept labeling. It enhances neuron specialization to achieve one-to-one mappings between concepts and neurons.

Result: SAEmnesia improves the localization of concept representations, reduces hyperparameter search by 96.67%, advances unlearning accuracy by 9.22% on the UnlearnCanvas benchmark, and boosts scalability for sequential unlearning tasks by 28.4%.

Conclusion: SAEmnesia offers a computationally efficient and interpretable method for concept unlearning, setting new benchmarks in unlearning accuracy and scalability against state-of-the-art techniques.

Abstract: Effective concept unlearning in text-to-image diffusion models requires
precise localization of concept representations within the model's latent
space. While sparse autoencoders successfully reduce neuron polysemanticity
(i.e., multiple concepts per neuron) compared to the original network,
individual concept representations can still be distributed across multiple
latent features, requiring extensive search procedures for concept unlearning.
We introduce SAEmnesia, a supervised sparse autoencoder training method that
promotes one-to-one concept-neuron mappings through systematic concept
labeling, mitigating feature splitting and promoting feature centralization.
Our approach learns specialized neurons with significantly stronger concept
associations compared to unsupervised baselines. The only computational
overhead introduced by SAEmnesia is limited to cross-entropy computation during
training. At inference time, this interpretable representation reduces
hyperparameter search by 96.67% with respect to current approaches. On the
UnlearnCanvas benchmark, SAEmnesia achieves a 9.22% improvement over the
state-of-the-art. In sequential unlearning tasks, we demonstrate superior
scalability with a 28.4% improvement in unlearning accuracy for 9-object
removal.

</details>


### [178] [Coreset selection based on Intra-class diversity](https://arxiv.org/abs/2509.21380)
*Imran Ashraf,Mukhtar Ullah,Muhammad Faisal Nadeem,Muhammad Nouman Noor*

Main category: cs.CV

TL;DR: The paper proposes an advanced method for dataset subset selection aimed at improving training efficiency in deep learning models, particularly for biomedical imaging.


<details>
  <summary>Details</summary>
Motivation: Deep learning models require substantial computational resources for training due to large datasets and hyperparameter optimization. Efficient subset selection methods are needed to reduce resource demands while preserving dataset representativeness.

Method: The study introduces an intelligent coreset selection mechanism that extracts intra-class diversity by forming per-class clusters to ensure better representativeness of the dataset.

Result: Experiments on a biomedical imaging dataset demonstrate that the proposed sampling method outperforms random sampling in classification performance metrics.

Conclusion: The proposed mechanism enhances dataset representativeness, addresses intra-class diversity challenges, and improves performance metrics while significantly reducing computational demands.

Abstract: Deep Learning models have transformed various domains, including the
healthcare sector, particularly biomedical image classification by learning
intricate features and enabling accurate diagnostics pertaining to complex
diseases. Recent studies have adopted two different approaches to train DL
models: training from scratch and transfer learning. Both approaches demand
substantial computational time and resources due to the involvement of massive
datasets in model training. These computational demands are further increased
due to the design-space exploration required for selecting optimal
hyperparameters, which typically necessitates several training rounds. With the
growing sizes of datasets, exploring solutions to this problem has recently
gained the research community's attention. A plausible solution is to select a
subset of the dataset for training and hyperparameter search. This subset,
referred to as the corset, must be a representative set of the original
dataset. A straightforward approach to selecting the coreset could be employing
random sampling, albeit at the cost of compromising the representativeness of
the original dataset. A critical limitation of random sampling is the bias
towards the dominant classes in an imbalanced dataset. Even if the dataset has
inter-class balance, this random sampling will not capture intra-class
diversity. This study addresses this issue by introducing an intelligent,
lightweight mechanism for coreset selection. Specifically, it proposes a method
to extract intra-class diversity, forming per-class clusters that are utilized
for the final sampling. We demonstrate the efficacy of the proposed methodology
by conducting extensive classification experiments on a well-known biomedical
imaging dataset. Results demonstrate that the proposed scheme outperforms the
random sampling approach on several performance metrics for uniform conditions.

</details>


### [179] [The LongiMam model for improved breast cancer risk prediction using longitudinal mammograms](https://arxiv.org/abs/2509.21383)
*Manel Rakez,Thomas Louis,Julien Guillaumin,Foucauld Chamming's,Pierre Fillard,Brice Amadeo,Virginie Rondeau*

Main category: cs.CV

TL;DR: The paper introduces LongiMam, a deep learning model that uses up to four prior mammograms to improve breast cancer prediction. It combines convolutional and recurrent networks for spatial and temporal analysis, outperforming single-visit models in a population-based dataset.


<details>
  <summary>Details</summary>
Motivation: To enhance breast cancer risk prediction models, which traditionally use limited imaging data, by incorporating longitudinal data and addressing real-world conditions like imbalanced outcomes and varied follow-up.

Method: Developed LongiMam, a deep learning model combining convolutional and recurrent neural networks, trained on a population-based screening dataset with up to four prior mammograms to capture spatial and temporal patterns.

Result: LongiMam showed improved prediction accuracy by integrating multiple prior mammograms, particularly in key risk groups such as women with dense breasts, those aged 55+, and individuals with changes in mammographic density.

Conclusion: Longitudinal mammogram analysis improves breast cancer prediction and risk stratification, showcasing the importance of combining historical and recent imaging data for screening. The model is open-source for public use.

Abstract: Risk-adapted breast cancer screening requires robust models that leverage
longitudinal imaging data. Most current deep learning models use single or
limited prior mammograms and lack adaptation for real-world settings marked by
imbalanced outcome distribution and heterogeneous follow-up. We developed
LongiMam, an end-to-end deep learning model that integrates both current and up
to four prior mammograms. LongiMam combines a convolutional and a recurrent
neural network to capture spatial and temporal patterns predictive of breast
cancer. The model was trained and evaluated using a large, population-based
screening dataset with disproportionate case-to-control ratio typical of
clinical screening. Across several scenarios that varied in the number and
composition of prior exams, LongiMam consistently improved prediction when
prior mammograms were included. The addition of prior and current visits
outperformed single-visit models, while priors alone performed less well,
highlighting the importance of combining historical and recent information.
Subgroup analyses confirmed the model's efficacy across key risk groups,
including women with dense breasts and those aged 55 years or older. Moreover,
the model performed best in women with observed changes in mammographic density
over time. These findings demonstrate that longitudinal modeling enhances
breast cancer prediction and support the use of repeated mammograms to refine
risk stratification in screening programs. LongiMam is publicly available as
open-source software.

</details>


### [180] [Assessing the Alignment of Popular CNNs to the Brain for Valence Appraisal](https://arxiv.org/abs/2509.21384)
*Laurent Mertens,Elahe' Yargholi,Laura Van Hove,Hans Op de Beeck,Jan Van den Stock,Joost Vennekens*

Main category: cs.CV

TL;DR: CNNs have parallels with human brain functions in general vision tasks, but their relevance in social cognition seems limited. A new framework, Object2Brain, was introduced for further analysis.


<details>
  <summary>Details</summary>
Motivation: Investigate the alignment of CNNs with human brain processes, specifically in the domain of social cognition, which is more complex than general visual perception.

Method: Correlation analysis between CNN outputs and human behavioral/functional MRI data for image valence appraisal. Object2Brain framework combines GradCAM and object detection at the CNN-filter level to study object class influences.

Result: CNNs show limitations in mirroring higher-order brain processing in tasks like valence appraisal. Different architectures exhibit varying sensitivities to object classes.

Conclusion: While CNNs align partly with human perception, their matching with advanced cognitive processes is restricted. Object2Brain offers insights into how object classes affect CNN-human correlations.

Abstract: Convolutional Neural Networks (CNNs) are a popular type of computer model
that have proven their worth in many computer vision tasks. Moreover, they form
an interesting study object for the field of psychology, with shown
correspondences between the workings of CNNs and the human brain. However,
these correspondences have so far mostly been studied in the context of general
visual perception. In contrast, this paper explores to what extent this
correspondence also holds for a more complex brain process, namely social
cognition. To this end, we assess the alignment between popular CNN
architectures and both human behavioral and fMRI data for image valence
appraisal through a correlation analysis. We show that for this task CNNs
struggle to go beyond simple visual processing, and do not seem to reflect
higher-order brain processing. Furthermore, we present Object2Brain, a novel
framework that combines GradCAM and object detection at the CNN-filter level
with the aforementioned correlation analysis to study the influence of
different object classes on the CNN-to-human correlations. Despite similar
correlation trends, different CNN architectures are shown to display different
object class sensitivities.

</details>


### [181] [Debugging Concept Bottleneck Models through Removal and Retraining](https://arxiv.org/abs/2509.21385)
*Eric Enouen,Sainyam Galhotra*

Main category: cs.CV

TL;DR: The paper introduces CBDebug, a framework to debug Concept Bottleneck Models (CBMs). It involves identifying and removing undesired concepts and retraining models using user feedback, improving prediction accuracy and addressing biases.


<details>
  <summary>Details</summary>
Motivation: CBMs offer transparency and allow expert interventions but fail to resolve systemic biases from shortcuts learned in data. The paper seeks to address this limitation by introducing a debugging mechanism.

Method: The method includes a two-step framework: 1) Removal of undesired concepts based on expert feedback, and 2) Retraining the model using CBDebug, applying supervised bias mitigation and augmentation via auxiliary labels derived from concept explanations.

Result: CBDebug significantly enhances CBM performance, outpacing prior retraining methods across diverse architectures and benchmarks with spurious correlations.

Conclusion: The debugging framework effectively addresses alignment issues between CBMs and expert reasoning, reducing reliance on biased concepts and improving model generalization.

Abstract: Concept Bottleneck Models (CBMs) use a set of human-interpretable concepts to
predict the final task label, enabling domain experts to not only validate the
CBM's predictions, but also intervene on incorrect concepts at test time.
However, these interventions fail to address systemic misalignment between the
CBM and the expert's reasoning, such as when the model learns shortcuts from
biased data. To address this, we present a general interpretable debugging
framework for CBMs that follows a two-step process of Removal and Retraining.
In the Removal step, experts use concept explanations to identify and remove
any undesired concepts. In the Retraining step, we introduce CBDebug, a novel
method that leverages the interpretability of CBMs as a bridge for converting
concept-level user feedback into sample-level auxiliary labels. These labels
are then used to apply supervised bias mitigation and targeted augmentation,
reducing the model's reliance on undesired concepts. We evaluate our framework
with both real and automated expert feedback, and find that CBDebug
significantly outperforms prior retraining methods across multiple CBM
architectures (PIP-Net, Post-hoc CBM) and benchmarks with known spurious
correlations.

</details>


### [182] [ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data](https://arxiv.org/abs/2509.21386)
*Anja Sheppard,Tyler Smithline,Andrew Scheffer,David Smith,Advaith V. Sethuraman,Ryan Bird,Sabrina Lin,Katherine A. Skinner*

Main category: cs.CV

TL;DR: The paper presents ShipwreckFinder, an open-source QGIS plugin for detecting shipwrecks using deep learning and sonar data.


<details>
  <summary>Details</summary>
Motivation: Manual identification of shipwrecks in bathymetry data is time-consuming and requires expertise; the paper aims to automate this process using advanced technology.

Method: The authors developed a QGIS plugin powered by a deep learning model trained on data from the Great Lakes and Ireland coasts, with dataset augmentation via synthetic data generation.

Result: The tool showed better segmentation performance than an ArcGIS deep learning toolkit and traditional methods.

Conclusion: ShipwreckFinder effectively automates shipwreck detection, outperforming existing tools and is publicly available for use.

Abstract: In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that
detects shipwrecks from multibeam sonar data. Shipwrecks are an important
historical marker of maritime history, and can be discovered through manual
inspection of bathymetric data. However, this is a time-consuming process and
often requires expert analysis. Our proposed tool allows users to automatically
preprocess bathymetry data, perform deep learning inference, threshold model
outputs, and produce either pixel-wise segmentation masks or bounding boxes of
predicted shipwrecks. The backbone of this open-source tool is a deep learning
model, which is trained on a variety of shipwreck data from the Great Lakes and
the coasts of Ireland. Additionally, we employ synthetic data generation in
order to increase the size and diversity of our dataset. We demonstrate
superior segmentation performance with our open-source tool and training
pipeline as compared to a deep learning-based ArcGIS toolkit and a more
classical inverse sinkhole detection method. The open-source tool can be found
at https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.

</details>


### [183] [Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence](https://arxiv.org/abs/2509.21387)
*Sanish Suwal,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: This study examines the effects of neural network pruning on both saliency maps and semantic concept coherence, revealing that moderate pruning improves human-aligned interpretability while excessive pruning undermines it.


<details>
  <summary>Details</summary>
Motivation: To understand how pruning neural networks impacts their interpretability, particularly in terms of saliency maps and concept representation, which has not been clearly explored before.

Method: ResNet-18 was trained on ImageNette and subjected to magnitude-based pruning, followed by fine-tuning. Post-hoc saliency explanations from Vanilla Gradients and Integrated Gradients were analyzed, alongside CRAFT-based semantic concept coherence under varying pruning levels.

Result: Light-to-moderate pruning improved saliency-map focus and preserved semantically meaningful concepts, while aggressive pruning reduced saliency map sparsity and merged heterogeneous features, hindering interpretability.

Conclusion: Moderate pruning shapes internal network representations to align better with human-attention patterns, but excessive pruning undermines interpretability despite maintaining accuracy.

Abstract: Prior works have shown that neural networks can be heavily pruned while
preserving performance, but the impact of pruning on model interpretability
remains unclear. In this work, we investigate how magnitude-based pruning
followed by fine-tuning affects both low-level saliency maps and high-level
concept representations. Using a ResNet-18 trained on ImageNette, we compare
post-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG)
across pruning levels, evaluating sparsity and faithfulness. We further apply
CRAFT-based concept extraction to track changes in semantic coherence of
learned concepts. Our results show that light-to-moderate pruning improves
saliency-map focus and faithfulness while retaining distinct, semantically
meaningful concepts. In contrast, aggressive pruning merges heterogeneous
features, reducing saliency map sparsity and concept coherence despite
maintaining accuracy. These findings suggest that while pruning can shape
internal representations toward more human-aligned attention patterns,
excessive pruning undermines interpretability.

</details>


### [184] [TUN3D: Towards Real-World Scene Understanding from Unposed Images](https://arxiv.org/abs/2509.21388)
*Anton Konushin,Nikita Drozdov,Bulat Gabdullin,Alexey Zakharov,Anna Vorontsova,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: TUN3D is a new method that simultaneously performs layout estimation and 3D object detection using multi-view images without needing depth sensors or ground-truth camera information, achieving state-of-the-art results across benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing approaches that require depth sensors or ground-truth camera poses, making 3D understanding tasks accessible using only multi-view image inputs.

Method: A lightweight sparse-convolutional network with two specialized heads: one for layout estimation utilizing a novel parametric wall representation, and another for 3D object detection.

Result: TUN3D achieves state-of-the-art performance in three challenging benchmarks, excelling in layout estimation and performing competitively in 3D object detection.

Conclusion: TUN3D advances holistic indoor scene understanding by eliminating the need for depth supervision or ground-truth camera poses, providing a practical and effective solution for real-world scenes.

Abstract: Layout estimation and 3D object detection are two fundamental tasks in indoor
scene understanding. When combined, they enable the creation of a compact yet
semantically rich spatial representation of a scene. Existing approaches
typically rely on point cloud input, which poses a major limitation since most
consumer cameras lack depth sensors and visual-only data remains far more
common. We address this issue with TUN3D, the first method that tackles joint
layout estimation and 3D object detection in real scans, given multi-view
images as input, and does not require ground-truth camera poses or depth
supervision. Our approach builds on a lightweight sparse-convolutional backbone
and employs two dedicated heads: one for 3D object detection and one for layout
estimation, leveraging a novel and effective parametric wall representation.
Extensive experiments show that TUN3D achieves state-of-the-art performance
across three challenging scene understanding benchmarks: (i) using ground-truth
point clouds, (ii) using posed images, and (iii) using unposed images. While
performing on par with specialized 3D object detection methods, TUN3D
significantly advances layout estimation, setting a new benchmark in holistic
indoor scene understanding. Code is available at
https://github.com/col14m/tun3d .

</details>


### [185] [Large AI Model-Enabled Generative Semantic Communications for Image Transmission](https://arxiv.org/abs/2509.21394)
*Qiyu Ma,Wanli Ni,Zhijin Qin*

Main category: cs.CV

TL;DR: This paper presents a generative semantic communication system for image transmission by segmenting images into key and non-key regions and optimizing processes accordingly, achieving superior semantic fidelity and visual quality.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of existing methodologies in ensuring high-quality reconstruction of visually important image regions in semantic communication systems.

Method: Segment images into key and non-key regions, process each region with distinct encoding methods, and utilize lightweight deployment strategies like model quantization and low-rank adaptation fine-tuning.

Result: Simulation results show improvements in semantic fidelity and visual quality over conventional image transmission methods.

Conclusion: The proposed system effectively enhances image transmission in semantic communication systems by prioritizing visually critical content and optimizing resource utilization.

Abstract: The rapid development of generative artificial intelligence (AI) has
introduced significant opportunities for enhancing the efficiency and accuracy
of image transmission within semantic communication systems. Despite these
advancements, existing methodologies often neglect the difference in importance
of different regions of the image, potentially compromising the reconstruction
quality of visually critical content. To address this issue, we introduce an
innovative generative semantic communication system that refines semantic
granularity by segmenting images into key and non-key regions. Key regions,
which contain essential visual information, are processed using an image
oriented semantic encoder, while non-key regions are efficiently compressed
through an image-to-text modeling approach. Additionally, to mitigate the
substantial storage and computational demands posed by large AI models, the
proposed system employs a lightweight deployment strategy incorporating model
quantization and low-rank adaptation fine-tuning techniques, significantly
boosting resource utilization without sacrificing performance. Simulation
results demonstrate that the proposed system outperforms traditional methods in
terms of both semantic fidelity and visual quality, thereby affirming its
effectiveness for image transmission tasks.

</details>


### [186] [mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human Sensing](https://arxiv.org/abs/2509.21396)
*Nabeel Nisar Bhat,Maksim Karnaukh,Stein Vandenbroeke,Wouter Lemoine,Jakob Struye,Jesus Omar Lacruz,Siddhartha Kumar,Mohammad Hossein Moghaddam,Joerg Widmer,Rafael Berkvens,Jeroen Famaey*

Main category: cs.CV

TL;DR: This paper introduces mmHSense, a set of labeled mmWave datasets, for human sensing research in ISAC systems and validates their utility in downstream tasks with efficient fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To support human sensing research within Integrated Sensing and Communication systems, particularly for end applications like gesture recognition and localization.

Method: The paper provides open labeled mmWave datasets, describes the testbed and experimental settings, and performs validation using a downstream task with parameter-efficient fine-tuning techniques.

Result: The datasets are validated for their utility in ISAC-based tasks, showcasing reduced computational complexity and preserved model performance during adaptation.

Conclusion: mmHSense datasets advance mmWave ISAC research by benefiting applications, reducing computational requirements, and enabling task adaptability.

Abstract: This article presents mmHSense, a set of open labeled mmWave datasets to
support human sensing research within Integrated Sensing and Communication
(ISAC) systems. The datasets can be used to explore mmWave ISAC for various end
applications such as gesture recognition, person identification, pose
estimation, and localization. Moreover, the datasets can be used to develop and
advance signal processing and deep learning research on mmWave ISAC. This
article describes the testbed, experimental settings, and signal features for
each dataset. Furthermore, the utility of the datasets is demonstrated through
validation on a specific downstream task. In addition, we demonstrate the use
of parameter-efficient fine-tuning to adapt ISAC models to different tasks,
significantly reducing computational complexity while maintaining performance
on prior tasks.

</details>


### [187] [Skeleton Sparsification and Densification Scale-Spaces](https://arxiv.org/abs/2509.21398)
*Julia Gierke,Pascal Peter*

Main category: cs.CV

TL;DR: This paper introduces a new framework called skeletonisation scale-spaces, which hierarchically simplifies the medial axis while addressing its noise sensitivity.


<details>
  <summary>Details</summary>
Motivation: To overcome the noise sensitivity of the medial axis caused by minor boundary variations, which results in disproportionate expansions of the skeleton.

Method: The authors propose skeletonisation scale-spaces, combining medial axis sparsification with scale-space properties and introducing densification for reversible progression across scales.

Result: The proposed framework achieves robust skeletonisation, shape compression, and stiffness enhancement, demonstrated through proof-of-concept experiments.

Conclusion: The skeletonisation scale-spaces framework offers a scalable, transformative tool for shape analysis and practical applications while mitigating the limitations of traditional pruning methods.

Abstract: The Hamilton-Jacobi skeleton, also known as the medial axis, is a powerful
shape descriptor that represents binary objects in terms of the centres of
maximal inscribed discs. Despite its broad applicability, the medial axis
suffers from sensitivity to noise: minor boundary variations can lead to
disproportionately large and undesirable expansions of the skeleton. Classical
pruning methods mitigate this shortcoming by systematically removing extraneous
skeletal branches. This sequential simplification of skeletons resembles the
principle of sparsification scale-spaces that embed images into a family of
reconstructions from increasingly sparse pixel representations.
  We combine both worlds by introducing skeletonisation scale-spaces: They
leverage sparsification of the medial axis to achieve hierarchical
simplification of shapes. Unlike conventional pruning, our framework inherently
satisfies key scale-space properties such as hierarchical architecture,
controllable simplification, and equivariance to geometric transformations. We
provide a rigorous theoretical foundation in both continuous and discrete
formulations and extend the concept further with densification. This allows
inverse progression from coarse to fine scales and can even reach beyond the
original skeleton to produce overcomplete shape representations with relevancy
for practical applications.
  Through proof-of-concept experiments, we demonstrate the effectiveness of our
framework for practical tasks including robust skeletonisation, shape
compression, and stiffness enhancement for additive manufacturing.

</details>


### [188] [Downscaling climate projections to 1 km with single-image super resolution](https://arxiv.org/abs/2509.21399)
*Petr KoÅ¡Å¥Ã¡l,Pavel KordÃ­k,OndÅ™ej Podsztavek*

Main category: cs.CV

TL;DR: This paper introduces a method to downscale low-resolution climate projections (12.5 km) to a high resolution of 1 km using single-image super-resolution models.


<details>
  <summary>Details</summary>
Motivation: High-resolution climate projections are crucial for local decision-making, but current projections have low spatial resolution, limiting their practical applicability.

Method: The authors trained single-image super-resolution models on high-resolution observational data and then applied them to low-resolution climate projections to achieve downscaling. They evaluated the results using observed climate indices without requiring ground-truth high-resolution projections.

Result: The approach successfully downscaled daily mean temperature climate projections without increasing the error in climate indicators compared to the original low-resolution projections.

Conclusion: Single-image super-resolution models can effectively generate high-resolution climate projections, improving their usability in decision-making while maintaining the fidelity of climate indicators.

Abstract: High-resolution climate projections are essential for local decision-making.
However, available climate projections have low spatial resolution (e.g. 12.5
km), which limits their usability. We address this limitation by leveraging
single-image super-resolution models to statistically downscale climate
projections to 1-km resolution. Since high-resolution climate projections are
unavailable for training, we train models on a high-resolution observational
gridded data set and apply them to low-resolution climate projections. We
propose a climate indicator-based assessment using observed climate indices
computed at weather station locations to evaluate the downscaled climate
projections without ground-truth high-resolution climate projections.
Experiments on daily mean temperature demonstrate that single-image
super-resolution models can downscale climate projections without increasing
the error of climate indicators compared to low-resolution climate projections.

</details>


### [189] [JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation](https://arxiv.org/abs/2509.21401)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.CV

TL;DR: The paper introduces JaiLIP, a novel image-based attack method targeting Vision-Language Models (VLMs) to produce harmful outputs. It achieves imperceptible and highly effective adversarial images, surpassing existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the increasing threat of image-based perturbations causing harmful outputs in Vision-Language Models.

Method: JaiLIP employs loss-guided image perturbation that combines MSE loss between clean and adversarial images with a harmful-output loss objective.

Result: The experimental results confirm JaiLIP's effectiveness in generating toxic outputs, with imperceptible adversarial images outperforming existing methods.

Conclusion: Image-based jailbreak attacks pose significant challenges and necessitate efficient countermeasures for Vision-Language Models.

Abstract: Vision-Language Models (VLMs) have remarkable abilities in generating
multimodal reasoning tasks. However, potential misuse or safety alignment
concerns of VLMs have increased significantly due to different categories of
attack vectors. Among various attack vectors, recent studies have demonstrated
that image-based perturbations are particularly effective in generating harmful
outputs. In the literature, many existing techniques have been proposed to
jailbreak VLMs, leading to unstable performance and visible perturbations. In
this study, we propose Jailbreaking with Loss-guided Image Perturbation
(JaiLIP), a jailbreaking attack in the image space that minimizes a joint
objective combining the mean squared error (MSE) loss between clean and
adversarial image with the models harmful-output loss. We evaluate our proposed
method on VLMs using standard toxicity metrics from Perspective API and
Detoxify. Experimental results demonstrate that our method generates highly
effective and imperceptible adversarial images, outperforming existing methods
in producing toxicity. Moreover, we have evaluated our method in the
transportation domain to demonstrate the attacks practicality beyond toxic text
generation in specific domain. Our findings emphasize the practical challenges
of image-based jailbreak attacks and the need for efficient defense mechanisms
for VLMs.

</details>


### [190] [Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?](https://arxiv.org/abs/2509.21419)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: This paper investigates the performance gap between automated systems using deep learning and human expert botanists in identifying plants, finding them to be closely matched.


<details>
  <summary>Details</summary>
Motivation: To understand how the performance of automated species identification systems, aided by deep learning, compares to human expert capabilities in dealing with uncertainties in visual or audio observations.

Method: A comparative challenge (LifeCLEF 2018 ExpertCLEF) was conducted involving 19 deep learning systems from 4 research teams and 9 expert botanists. This comparison evaluated their performance in identifying French flora based on certain metrics.

Result: The findings show that state-of-the-art deep learning systems are nearly as skilled as highly trained human botanists in species identification.

Conclusion: The research provides a concrete step in demonstrating the maturity of deep learning systems in achieving human-level expertise, highlighting their viability as complementary tools in biological sciences.

Abstract: Automated identification of plants and animals has improved considerably in
the last few years, in particular thanks to the recent advances in deep
learning. The next big question is how far such automated systems are from the
human expertise. Indeed, even the best experts are sometimes confused and/or
disagree between each others when validating visual or audio observations of
living organism. A picture actually contains only a partial information that is
usually not sufficient to determine the right species with certainty.
Quantifying this uncertainty and comparing it to the performance of automated
systems is of high interest for both computer scientists and expert
naturalists. The LifeCLEF 2018 ExpertCLEF challenge presented in this paper was
designed to allow this comparison between human experts and automated systems.
In total, 19 deep-learning systems implemented by 4 different research teams
were evaluated with regard to 9 expert botanists of the French flora. The main
outcome of this work is that the performance of state-of-the-art deep learning
models is now close to the most advanced human expertise. This paper presents
more precisely the resources and assessments of the challenge, summarizes the
approaches and systems employed by the participating research groups, and
provides an analysis of the main outcomes.

</details>


### [191] [QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models](https://arxiv.org/abs/2509.21420)
*Jian Liu,Chunshi Wang,Song Guo,Haohan Weng,Zhen Zhou,Zhiqi Li,Jiaao Yu,Yiling Zhu,Jing Xu,Biwen Lei,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: QuadGPT introduces an autoregressive framework for generating quadrilateral meshes directly, surpassing previous triangle-to-quad conversion methods in accuracy and quality.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating quad meshes rely on converting triangle meshes, which often results in suboptimal topology and quality.

Method: QuadGPT employs a sequence prediction paradigm with a unified tokenization method for mixed topologies and a Reinforcement Learning fine-tuning technique called tDPO.

Result: Experiments show QuadGPT outperforms existing pipelines in geometric accuracy and topological quality.

Conclusion: QuadGPT establishes a new standard for quad-mesh generation, demonstrating the efficacy of combining autoregressive modeling with topology-aware RL refinement for 3D assets.

Abstract: The generation of quadrilateral-dominant meshes is a cornerstone of
professional 3D content creation. However, existing generative models generate
quad meshes by first generating triangle meshes and then merging triangles into
quadrilaterals with some specific rules, which typically produces quad meshes
with poor topology. In this paper, we introduce QuadGPT, the first
autoregressive framework for generating quadrilateral meshes in an end-to-end
manner. QuadGPT formulates this as a sequence prediction paradigm,
distinguished by two key innovations: a unified tokenization method to handle
mixed topologies of triangles and quadrilaterals, and a specialized
Reinforcement Learning fine-tuning method tDPO for better generation quality.
Extensive experiments demonstrate that QuadGPT significantly surpasses previous
triangle-to-quad conversion pipelines in both geometric accuracy and
topological quality. Our work establishes a new benchmark for native quad-mesh
generation and showcases the power of combining large-scale autoregressive
models with topology-aware RL refinement for creating structured 3D assets.

</details>


### [192] [DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation](https://arxiv.org/abs/2509.21433)
*Jiaqi Liu,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: The paper addresses ethical and legal challenges in text-to-image diffusion models by proposing DyME, a modular framework for on-demand concept erasure, overcoming limitations of static erasure models.


<details>
  <summary>Details</summary>
Motivation: Legal and ethical concerns arise from diffusion models inadvertently reproducing copyrighted styles and protected visual concepts. Current methods for concept erasure are not scalable to handle varying generation requests.

Method: DyME trains lightweight concept-specific LoRA adapters and dynamically composes them at inference, supported by bi-level orthogonality constraints to reduce interference among adapters.

Result: DyME consistently outperforms existing methods in erasing multiple concepts with higher fidelity while minimizing degradation of non-target content, demonstrated on ErasureBench-H and standard datasets.

Conclusion: DyME introduces a scalable and dynamic approach to concept erasure in generative models, addressing real-world constraints and achieving superior performance in fidelity and flexibility.

Abstract: Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted
styles and protected visual concepts, raising legal and ethical concerns.
Concept erasure has emerged as a safeguard, aiming to selectively suppress such
concepts through fine-tuning. However, existing methods do not scale to
practical settings where providers must erase multiple and possibly conflicting
concepts. The core bottleneck is their reliance on static erasure: a single
checkpoint is fine-tuned to remove all target concepts, regardless of the
actual erasure needs at inference. This rigid design mismatches real-world
usage, where requests vary per generation, leading to degraded erasure success
and reduced fidelity for non-target content. We propose DyME, an on-demand
erasure framework that trains lightweight, concept-specific LoRA adapters and
dynamically composes only those needed at inference. This modular design
enables flexible multi-concept erasure, but naive composition causes
interference among adapters, especially when many or semantically related
concepts are suppressed. To overcome this, we introduce bi-level orthogonality
constraints at both the feature and parameter levels, disentangling
representation shifts and enforcing orthogonal adapter subspaces. We further
develop ErasureBench-H, a new hierarchical benchmark with
brand-series-character structure, enabling principled evaluation across
semantic granularities and erasure set sizes. Experiments on ErasureBench-H and
standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME
consistently outperforms state-of-the-art baselines, achieving higher
multi-concept erasure fidelity with minimal collateral degradation.

</details>


### [193] [VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](https://arxiv.org/abs/2509.21451)
*Abdul Waheed,Zhen Wu,Dareen Alharthi,Seungone Kim,Bhiksha Raj*

Main category: cs.CV

TL;DR: This paper presents VideoJudge, a specialized multimodal LLM for evaluating video understanding models, outperforming larger baselines and highlighting the importance of incorporating video inputs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing evaluation metrics and methods in video understanding tasks, where current metrics fail to capture human judgment effectively and manual evaluations are costly.

Method: The study introduces VideoJudge, MLLM judges trained using an interplay between a generator and an evaluator that ensures quality responses conditioned on target ratings and utilizes video inputs.

Result: VideoJudge-7B outperformed larger MLLM judge baselines in three out of four benchmarks. Video inputs were found crucial for accurate evaluation, as LLM judges without video inputs performed worse.

Conclusion: VideoJudge offers a more precise and effective solution for evaluating video understanding models, emphasizing the need for multimodal video input in evaluations.

Abstract: Precisely evaluating video understanding models remains challenging: commonly
used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of
human judgment, while obtaining such judgments through manual evaluation is
costly. Recent work has explored using large language models (LLMs) or
multimodal LLMs (MLLMs) as evaluators, but their extension to video
understanding remains relatively unexplored. In this work, we introduce
VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from
video understanding models (\textit{i.e.}, text responses conditioned on
videos). To train VideoJudge, our recipe builds on the interplay between a
generator and an evaluator: the generator is prompted to produce responses
conditioned on a target rating, and responses not matching the evaluator's
rating are discarded. Across three out of four meta-evaluation benchmarks,
VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B
and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than
MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve
performance, indicating that providing video inputs is crucial for evaluation
of video understanding tasks.

</details>


### [194] [Residual Vector Quantization For Communication-Efficient Multi-Agent Perception](https://arxiv.org/abs/2509.21464)
*Dereje Shenkut,B. V. K Vijaya Kumar*

Main category: cs.CV

TL;DR: ReVQom introduces a codec for efficient feature compression to enhance multi-agent collaborative perception, achieving significant compression with minimal loss of accuracy.


<details>
  <summary>Details</summary>
Motivation: Improving scalability for multi-agent collaborative perception by addressing communication bandwidth limitations.

Method: ReVQom employs a bottleneck network and multi-stage residual vector quantization to compress intermediate features, reducing communication payloads significantly.

Result: Achieved 273x to 1365x compression (depending on bandwidth) on DAIR-V2X dataset, with accuracy comparable or better than raw-feature collaborative perception.

Conclusion: ReVQom enables scalable and efficient multi-agent perception with reduced bandwidth demands, advancing practical V2X applications.

Abstract: Multi-agent collaborative perception (CP) improves scene understanding by
sharing information across connected agents such as autonomous vehicles,
unmanned aerial vehicles, and robots. Communication bandwidth, however,
constrains scalability. We present ReVQom, a learned feature codec that
preserves spatial identity while compressing intermediate features. ReVQom is
an end-to-end method that compresses feature dimensions via a simple bottleneck
network followed by multi-stage residual vector quantization (RVQ). This allows
only per-pixel code indices to be transmitted, reducing payloads from 8192 bits
per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent
with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves
273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),
ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables
ultra-low-bandwidth operation with graceful degradation. ReVQom allows
efficient and accurate multi-agent collaborative perception with a step toward
practical V2X deployment.

</details>


### [195] [Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models](https://arxiv.org/abs/2509.21466)
*Khaloud S. AlKhalifah,Malak Mashaabi,Hend Al-Khalifa*

Main category: cs.CV

TL;DR: This paper examines gender stereotypes and cultural inaccuracies in AI-generated images of professionals in Saudi Arabia, focusing on outputs by three AI models.


<details>
  <summary>Details</summary>
Motivation: To analyze the extent to which AI-driven Text-to-Image models perpetuate societal biases, particularly in gender representation and cultural accuracy for Saudi professionals.

Method: The study evaluated 1,006 images generated by ImageFX, DALL-E V3, and Grok for 56 Saudi professions using neutral prompts. Two annotators assessed the images on five specific dimensions, with a third researcher adjudicating disagreements.

Result: AI outputs demonstrated a strong male bias: 85% male (ImageFX), 86.6% male (Grok), and 96% male (DALL-E V3). Cultural inaccuracies in clothing, settings, and activities were also prevalent.

Conclusion: Current models reflect societal biases from their training data, yielding limited and often inaccurate portrayals of Saudi professions. Efforts to diversify data, improve algorithms, and integrate culturally sensitive evaluation are crucial.

Abstract: This study investigates the extent to which contemporary Text-to-Image
artificial intelligence (AI) models perpetuate gender stereotypes and cultural
inaccuracies when generating depictions of professionals in Saudi Arabia. We
analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse
Saudi professions using neutral prompts. Two trained Saudi annotators evaluated
each image on five dimensions: perceived gender, clothing and appearance,
background and setting, activities and interactions, and age. A third senior
researcher adjudicated whenever the two primary raters disagreed, yielding
10,100 individual judgements. The results reveal a strong gender imbalance,
with ImageFX outputs being 85\% male, Grok 86.6\% male, and DALL-E V3 96\%
male, indicating that DALL-E V3 exhibited the strongest overall gender
stereotyping. This imbalance was most evident in leadership and technical
roles. Moreover, cultural inaccuracies in clothing, settings, and depicted
activities were frequently observed across all three models.
Counter-stereotypical images often arise from cultural misinterpretations
rather than genuinely progressive portrayals. We conclude that current models
mirror societal biases embedded in their training data, generated by humans,
offering only a limited reflection of the Saudi labour market's gender dynamics
and cultural nuances. These findings underscore the urgent need for more
diverse training data, fairer algorithms, and culturally sensitive evaluation
frameworks to ensure equitable and authentic visual outputs.

</details>


### [196] [Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation](https://arxiv.org/abs/2509.21486)
*Zixuan Wang,Yu Sun,Hongwei Wang,Baoyu Jing,Xiang Shen,Xin Dong,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: The paper presents a novel pretraining paradigm for multimodal large language models (MLLMs) to detect inappropriate content in short videos, integrating reasoning capabilities and overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid rise of short video platforms necessitates effective detection methods for inappropriate content. Traditional methods rely on separate small models for specific issues, which lack scalability and require extensive labeled data.

Method: The authors introduced three pretraining tasks: Captioning to enhance video detail perception, VQA to improve understanding of content guidelines, and Chain-of-Thought reasoning to boost reasoning capabilities of MLLMs.

Result: Experiments demonstrated improved zero-shot and supervised fine-tuning performance of the model. The pretrained MLLM also exhibited strong generalization to emerging and unseen content issues.

Conclusion: The proposed pretraining paradigm effectively equips MLLMs with enhanced reasoning, understanding, and detection capabilities, outperforming standard approaches in tackling inappropriate content on short video platforms.

Abstract: Short video platforms are evolving rapidly, making the identification of
inappropriate content increasingly critical. Existing approaches typically
train separate and small classification models for each type of issue, which
requires extensive human-labeled data and lacks cross-issue generalization. We
propose a reasoning-enhanced multimodal large language model (MLLM) pretraining
paradigm for unified inappropriate content detection. To address the
distribution gap between short video content and the original pretraining data
of MLLMs, as well as the complex issue definitions, we introduce three targeted
pretraining tasks: (1) \textit{Caption}, to enhance the MLLM's perception of
video details; (2) \textit{Visual Question Answering (VQA)}, to deepen the
MLLM's understanding of issue definitions and annotation guidelines; (3)
\textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability.
Experimental results show that our pretraining approach significantly improves
the MLLM's performance in both zero-shot and supervised fine-tuning (SFT)
settings. In addition, our pretrained model demonstrates strong generalization
capabilities to emergent, previously unseen issues.

</details>


### [197] [Learning GUI Grounding with Spatial Reasoning from Visual Feedback](https://arxiv.org/abs/2509.21552)
*Yu Zhao,Wei-Ning Chen,Huseyin Atahan Inan,Samuel Kessler,Lu Wang,Lukas Wutschitz,Fangkai Yang,Chaoyun Zhang,Pasquale Minervini,Saravan Rajmohan,Robert Sim*

Main category: cs.CV

TL;DR: The paper introduces GUI-Cursor, a model for GUI grounding, reframing the task from coordinate prediction to interactive cursor search, significantly improving grounding accuracy and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address challenges faced by Vision Language Models (VLMs) in predicting numerical coordinates in high-resolution GUI images with complex layouts.

Method: The proposed GUI-Cursor leverages interactive search where actions move a cursor closer to the target. It is trained with multi-step online reinforcement learning using a dense trajectory-based reward mechanism.

Result: GUI-Cursor achieved state-of-the-art accuracy improvements: from 88.8% to 93.9% on ScreenSpot-v2 and from 26.8% to 56.5% on ScreenSpot-Pro.

Conclusion: GUI-Cursor demonstrates enhanced accuracy and efficiency in GUI grounding tasks, solving 95% of instances within two steps, and adapting flexibly to more complex cases.

Abstract: Graphical User Interface (GUI) grounding is commonly framed as a coordinate
prediction task -- given a natural language instruction, generate on-screen
coordinates for actions such as clicks and keystrokes. However, recent Vision
Language Models (VLMs) often fail to predict accurate numeric coordinates when
processing high-resolution GUI images with complex layouts. To address this
issue, we reframe GUI grounding as an \emph{interactive search task}, where the
VLM generates actions to move a cursor in the GUI to locate UI elements. At
each step, the model determines the target object, evaluates the spatial
relations between the cursor and the target, and moves the cursor closer to the
target conditioned on the movement history. In this interactive process, the
rendered cursor provides visual feedback to help the model align its
predictions with the corresponding on-screen locations. We train our GUI
grounding model, GUI-Cursor, using multi-step online reinforcement learning
with a dense trajectory-based reward function. Our experimental results show
that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy
and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\% \rightarrow
93.9\%$) and ScreenSpot-Pro ($26.8\% \rightarrow 56.5\%$). Moreover, we observe
that GUI-Cursor learns to solve the problem within two steps for 95\% of
instances and can adaptively conduct more steps on more difficult examples.

</details>


### [198] [X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.21559)
*Prasanna Reddy Pulakurthi,Jiamian Wang,Majid Rabbani,Sohail Dianat,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.CV

TL;DR: The paper introduces X-CoT, an explainable text-to-video retrieval framework that uses reasoning instead of embedding-based similarity rankings, enhancing interpretability and data quality analysis.


<details>
  <summary>Details</summary>
Motivation: Current embedding-based text-to-video retrieval systems lack interpretability and struggle with low-quality data pairs, which affects ranking quality and reliability.

Method: X-CoT replaces similarity-based rankings with a process relying on LLM CoT reasoning, introduces new video annotations for semantic understanding, and uses pairwise comparisons for detailed reasoning and ranking.

Result: X-CoT improves retrieval performance while providing rationales for ranking decisions, enabling analysis of both model behavior and data quality.

Conclusion: X-CoT addresses the interpretability challenges in text-to-video retrieval, offering a more transparent framework that not only enhances retrieval effectiveness but also enables better data assessment.

Abstract: Prevalent text-to-video retrieval systems mainly adopt embedding models for
feature extraction and compute cosine similarities for ranking. However, this
design presents two limitations. Low-quality text-video data pairs could
compromise the retrieval, yet are hard to identify and examine. Cosine
similarity alone provides no explanation for the ranking results, limiting the
interpretability. We ask that can we interpret the ranking results, so as to
assess the retrieval models and examine the text-video data? This work proposes
X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of
the embedding model-based similarity ranking. We first expand the existing
benchmarks with additional video annotations to support semantic understanding
and reduce data bias. We also devise a retrieval CoT consisting of pairwise
comparison steps, yielding detailed reasoning and complete ranking. X-CoT
empirically improves the retrieval performance and produces detailed
rationales. It also facilitates the model behavior and data quality analysis.
Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.

</details>


### [199] [Unsupervised Defect Detection for Surgical Instruments](https://arxiv.org/abs/2509.21561)
*Joseph Huang,Yichi Zhang,Jingxi Yu,Wei Chen,Seunghyun Hwang,Qiang Qiu,Amy R. Reibman,Edward J. Delp,Fengqing Zhu*

Main category: cs.CV

TL;DR: The paper proposes a method to improve defect detection in surgical instruments, overcoming limitations in current automated approaches.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of surgical instruments is error-prone, and existing automated defect detection methods trained on non-surgical images fail in this domain.

Method: The method combines background masking, a patch-based analysis strategy, and domain adaptation to enhance unsupervised defect detection for surgical instruments.

Result: The proposed approach effectively detects fine-grained defects in surgical instrument imagery, addressing false positives and domain shift problems.

Conclusion: This method provides reliable, domain-specific visual defect detection for surgical instruments, improving upon existing techniques.

Abstract: Ensuring the safety of surgical instruments requires reliable detection of
visual defects. However, manual inspection is prone to error, and existing
automated defect detection methods, typically trained on natural/industrial
images, fail to transfer effectively to the surgical domain. We demonstrate
that simply applying or fine-tuning these approaches leads to issues: false
positive detections arising from textured backgrounds, poor sensitivity to
small, subtle defects, and inadequate capture of instrument-specific features
due to domain shift. To address these challenges, we propose a versatile method
that adapts unsupervised defect detection methods specifically for surgical
instruments. By integrating background masking, a patch-based analysis
strategy, and efficient domain adaptation, our method overcomes these
limitations, enabling the reliable detection of fine-grained defects in
surgical instrument imagery.

</details>


### [200] [No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models](https://arxiv.org/abs/2509.21565)
*Junno Yun,YaÅŸar Utku AlÃ§alar,Mehmet AkÃ§akaya*

Main category: cs.CV

TL;DR: The paper introduces a regularization method called Linear Separability (LSEP) for improving intermediate layer representations in large-scale diffusion models, eliminating the dependency on external pretrained encoders and enhancing efficiency and generation quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the reliance on computationally expensive, pretrained external encoders for improving feature representations in diffusion models and to enhance training efficiency and representation quality.

Method: The method introduces Linear SEParability (LSEP) as a training regularization that integrates linear probing into the model's learning dynamics, avoiding post-hoc evaluation and external encoder dependency.

Result: Using LSEP, the paper achieves significant improvements in efficiency and generation quality, specifically reaching an FID of 1.46 on the $256 \times 256$ ImageNet dataset using flow-based transformer architectures such as SiTs.

Conclusion: The proposed LSEP method demonstrates potential as an effective and efficient alternative to alignment-based approaches for improving feature representations in diffusion models.

Abstract: Efficient training strategies for large-scale diffusion models have recently
emphasized the importance of improving discriminative feature representations
in these models. A central line of work in this direction is representation
alignment with features obtained from powerful external encoders, which
improves the representation quality as assessed through linear probing.
Alignment-based approaches show promise but depend on large pretrained
encoders, which are computationally expensive to obtain. In this work, we
propose an alternative regularization for training, based on promoting the
Linear SEParability (LSEP) of intermediate layer representations. LSEP
eliminates the need for an auxiliary encoder and representation alignment,
while incorporating linear probing directly into the network's learning
dynamics rather than treating it as a simple post-hoc evaluation tool. Our
results demonstrate substantial improvements in both training efficiency and
generation quality on flow-based transformer architectures such as SiTs,
achieving an FID of 1.46 on $256 \times 256$ ImageNet dataset.

</details>


### [201] [Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms](https://arxiv.org/abs/2509.21573)
*Boyi Chen,Zhangyu Wang,Fabian Deuser,Johann Maximilian Zollner,Martin Werner*

Main category: cs.CV

TL;DR: The paper addresses the challenges in global-scale image-based geo-localization by introducing a spatially regularized contrastive learning strategy that leverages geostatistical spatial correlation models.


<details>
  <summary>Details</summary>
Motivation: Accurate geo-localization across diverse environments and visually ambiguous scenes is challenging, especially with false negatives and hard negatives in existing contrastive learning methods.

Method: The authors propose integrating a semivariogram, a geostatistical tool, into the contrastive learning process. This tool captures spatial correlations and helps identify false negatives and hard negatives.

Result: The model, integrated into GeoCLIP and tested on the OSV5M dataset, shows improved geo-localization performance, particularly at finer granularity.

Conclusion: Explicitly modeling spatial priors with geostatistical tools enhances accuracy and robustness in image-based geo-localization tasks.

Abstract: Accurate and robust image-based geo-localization at a global scale is
challenging due to diverse environments, visually ambiguous scenes, and the
lack of distinctive landmarks in many regions. While contrastive learning
methods show promising performance by aligning features between street-view
images and corresponding locations, they neglect the underlying spatial
dependency in the geographic space. As a result, they fail to address the issue
of false negatives -- image pairs that are both visually and geographically
similar but labeled as negatives, and struggle to effectively distinguish hard
negatives, which are visually similar but geographically distant. To address
this issue, we propose a novel spatially regularized contrastive learning
strategy that integrates a semivariogram, which is a geostatistical tool for
modeling how spatial correlation changes with distance. We fit the
semivariogram by relating the distance of images in feature space to their
geographical distance, capturing the expected visual content in a spatial
correlation. With the fitted semivariogram, we define the expected visual
dissimilarity at a given spatial distance as reference to identify hard
negatives and false negatives. We integrate this strategy into GeoCLIP and
evaluate it on the OSV5M dataset, demonstrating that explicitly modeling
spatial priors improves image-based geo-localization performance, particularly
at finer granularity.

</details>


### [202] [X-Streamer: Unified Human World Modeling with Audiovisual Interaction](https://arxiv.org/abs/2509.21574)
*You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Guoxian Song,Xiaochen Zhao,Chao Liang,Jianwen Jiang,Hongyi Xu,Linjie Luo*

Main category: cs.CV

TL;DR: The paper presents X-Streamer, a framework for generating real-time multimodal digital human agents capable of endless audiovisual interactions from static portraits. It employs a dual-transformer architecture for understanding and generating text, speech, and video.


<details>
  <summary>Details</summary>
Motivation: To create an advanced digital human model capable of performing realistic, long-term, interactive multimodal communication (text, speech, video) in real-time and overcoming technical challenges of synchronization and stability.

Method: The proposed system integrates a Thinker-Actor dual-transformer architecture. The Thinker processes user inputs, while the Actor generates synchronized audiovisual responses using pretrained models, autoregressive diffusion mechanisms, and advanced attention techniques.

Result: X-Streamer is able to consistently sustain hours-long video chats in real-time, successfully converting static portraits into intelligent, interactive agents by ensuring clear synchronization and stability across multimodal outputs.

Conclusion: The framework demonstrates significant progress in unified world modeling and paves the way toward crafting fully interactive digital human agents capable of infinite communication streams.

Abstract: We introduce X-Streamer, an end-to-end multimodal human world modeling
framework for building digital human agents capable of infinite interactions
across text, speech, and video within a single unified architecture. Starting
from a single portrait, X-Streamer enables real-time, open-ended video calls
driven by streaming multimodal inputs. At its core is a Thinker-Actor
dual-transformer architecture that unifies multimodal understanding and
generation, turning a static portrait into persistent and intelligent
audiovisual interactions. The Thinker module perceives and reasons over
streaming user inputs, while its hidden states are translated by the Actor into
synchronized multimodal streams in real time. Concretely, the Thinker leverages
a pretrained large language-speech model, while the Actor employs a chunk-wise
autoregressive diffusion model that cross-attends to the Thinker's hidden
states to produce time-aligned multimodal responses with interleaved discrete
text and audio tokens and continuous video latents. To ensure long-horizon
stability, we design inter- and intra-chunk attentions with time-aligned
multimodal positional embeddings for fine-grained cross-modality alignment and
context retention, further reinforced by chunk-wise diffusion forcing and
global identity referencing. X-Streamer runs in real time on two A100 GPUs,
sustaining hours-long consistent video chat experiences from arbitrary
portraits and paving the way toward unified world modeling of interactive
digital humans.

</details>


### [203] [What Happens Next? Anticipating Future Motion by Generating Point Trajectories](https://arxiv.org/abs/2509.21592)
*Gabrijel Boduljak,Laurynas Karazija,Iro Laina,Christian Rupprecht,Andrea Vedaldi*

Main category: cs.CV

TL;DR: The paper addresses forecasting object motion from a single image using a model that generates dense motion trajectories instead of image pixels, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The inability of existing video generation models to forecast motion accurately from a single image motivates the development of a new approach.

Method: The researchers propose a conditional generation model for dense trajectory grids, based on modern video generators, but tailored to output motion trajectories.

Result: The method achieves better accuracy, diversity, and effectiveness in simulated and real-world datasets compared to prior models.

Conclusion: Generating motion trajectories directly offers superior performance compared to video pixel generations when predicting motion from a single image.

Abstract: We consider the problem of forecasting motion from a single image, i.e.,
predicting how objects in the world are likely to move, without the ability to
observe other parameters such as the object velocities or the forces applied to
them. We formulate this task as conditional generation of dense trajectory
grids with a model that closely follows the architecture of modern video
generators but outputs motion trajectories instead of pixels. This approach
captures scene-wide dynamics and uncertainty, yielding more accurate and
diverse predictions than prior regressors and generators. We extensively
evaluate our method on simulated data, demonstrate its effectiveness on
downstream applications such as robotics, and show promising accuracy on
real-world intuitive physics datasets. Although recent state-of-the-art video
generators are often regarded as world models, we show that they struggle with
forecasting motion from a single image, even in simple physical scenarios such
as falling blocks or mechanical object interactions, despite fine-tuning on
such data. We show that this limitation arises from the overhead of generating
pixels rather than directly modeling motion.

</details>


### [204] [Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis](https://arxiv.org/abs/2509.21595)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: This paper compares two self-supervised video action recognition methods: DINOv3 with spatial feature extraction and V-JEPA2 with temporal modeling. DINOv3 excels in clustering and pose-recognition tasks, whereas V-JEPA2 ensures consistent accuracy across diverse actions.


<details>
  <summary>Details</summary>
Motivation: To understand architectural trade-offs in self-supervised learning models for video action recognition and empirically guide feature extraction method selection.

Method: Two architectures, DINOv3 (spatial feature extraction) and V-JEPA2 (temporal modeling), were evaluated on the UCF Sports dataset regarding classification, clustering, intra-class consistency, and inter-class discrimination.

Result: DINOv3 demonstrated better clustering (Silhouette score 0.31) and discrimination (6.16x), especially for static poses, while V-JEPA2 showed lower variance (0.094 vs 0.288) and balanced quality for motion-dependent actions.

Conclusion: DINOv3 is ideal for pose-identifiable actions, but V-JEPA2 provides robust, consistent performance across diverse action types due to temporal modeling. Each model has strengths suited to specific tasks.

Abstract: This study presents a comprehensive comparative analysis of two prominent
self-supervised learning architectures for video action recognition: DINOv3,
which processes frames independently through spatial feature extraction, and
V-JEPA2, which employs joint temporal modeling across video sequences. We
evaluate both approaches on the UCF Sports dataset, examining feature quality
through multiple dimensions including classification accuracy, clustering
performance, intra-class consistency, and inter-class discrimination. Our
analysis reveals fundamental architectural trade-offs: DINOv3 achieves superior
clustering performance (Silhouette score: 0.31 vs 0.21) and demonstrates
exceptional discrimination capability (6.16x separation ratio) particularly for
pose-identifiable actions, while V-JEPA2 exhibits consistent reliability across
all action types with significantly lower performance variance (0.094 vs
0.288). Through action-specific evaluation, we identify that DINOv3's spatial
processing architecture excels at static pose recognition but shows degraded
performance on motion-dependent actions, whereas V-JEPA2's temporal modeling
provides balanced representation quality across diverse action categories.
These findings contribute to the understanding of architectural design choices
in video analysis systems and provide empirical guidance for selecting
appropriate feature extraction methods based on task requirements and
reliability constraints.

</details>


### [205] [VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment](https://arxiv.org/abs/2509.21609)
*Md. Mahfuzur Rahman,Kishor Datta Gupta,Marufa Kamal,Fahad Rahman,Sunzida Siddique,Ahmed Rafi Hasan,Mohd Ariful Haque,Roy George*

Main category: cs.CV

TL;DR: Immediate damage assessment post-catastrophes requires accurate descriptions. VLCE, a multimodal system, uses CNN-LSTM and Vision Transformer models with semantic knowledge integration for generating detailed disaster imagery captions.


<details>
  <summary>Details</summary>
Motivation: Conventional damage assessment methods are slow and risky, while existing computer vision approaches fail to offer detailed situational understanding for disaster management.

Method: VLCE combines a CNN-LSTM model pretrained on EuroSat imagery with a Vision Transformer model trained on UAV images. It incorporates external semantic databases like ConceptNet and WordNet to enhance vocabulary and accuracy.

Result: VLCE outperformed baseline models (LLaVA and QwenVL) in assessments, achieving up to 95.33% on caption informativeness while maintaining good semantic alignment.

Conclusion: VLCE offers significant improvements for automated damage assessment post-disasters by generating actionable and enriched descriptions from imagery data sources.

Abstract: Immediate damage assessment is essential after natural catastrophes; yet,
conventional hand evaluation techniques are sluggish and perilous. Although
satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives
of impacted regions, current computer vision methodologies generally yield just
classification labels or segmentation masks, so constraining their capacity to
deliver a thorough situational comprehension. We introduce the Vision Language
Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive,
contextually-informed explanations of disaster imagery. VLCE employs a
dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone
pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision
Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.
Both systems utilize external semantic knowledge from ConceptNet and WordNet to
expand vocabulary coverage and improve description accuracy. We assess VLCE in
comparison to leading vision-language models (LLaVA and QwenVL) utilizing
CLIPScore for semantic alignment and InfoMetIC for caption informativeness.
Experimental findings indicate that VLCE markedly surpasses baseline models,
attaining a maximum of 95.33% on InfoMetIC while preserving competitive
semantic alignment. Our dual-architecture system demonstrates significant
potential for improving disaster damage assessment by automating the production
of actionable, information-dense descriptions from satellite and drone photos.

</details>


### [206] [A Data-driven Typology of Vision Models from Integrated Representational Metrics](https://arxiv.org/abs/2509.21628)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.CV

TL;DR: This paper uses novel similarity metrics to compare large vision models, showing that geometry and unit tuning differentiate model families better than linear mappings. A fused similarity approach yielded sharper insights into model relationships.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore what aspects of large vision models are shared across different architectures and training paradigms, and what aspects are family-specific, providing a comprehensive understanding of representational structures.

Method: The authors utilized diverse representational similarity metrics, integrating these facets using Similarity Network Fusion (SNF) to combine their strengths and analyze models through clustering techniques.

Result: Metrics based on geometric and tuning properties showed strong discrimination between model families, while linear decodability showed weaker separations. SNF integration revealed sharper insights, clustering models into both expected and surprising relationships, such as convergences between architectures and training paradigms.

Conclusion: The study provides a systematic typology of vision models, emphasizing that architecture and training objectives combine to shape distinct computational strategies in large vision models, often transcending surface-level design distinctions.

Abstract: Large vision models differ widely in architecture and training paradigm, yet
we lack principled methods to determine which aspects of their representations
are shared across families and which reflect distinctive computational
strategies. We leverage a suite of representational similarity metrics, each
capturing a different facet-geometry, unit tuning, or linear decodability-and
assess family separability using multiple complementary measures. Metrics
preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family
discrimination, whereas flexible mappings such as Linear Predictivity show
weaker separation. These findings indicate that geometry and tuning carry
family-specific signatures, while linearly decodable information is more
broadly shared. To integrate these complementary facets, we adapt Similarity
Network Fusion (SNF), a method inspired by multi-omics integration. SNF
achieves substantially sharper family separation than any individual metric and
produces robust composite signatures. Clustering of the fused similarity matrix
recovers both expected and surprising patterns: supervised ResNets and ViTs
form distinct clusters, yet all self-supervised models group together across
architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with
masked autoencoders, suggesting convergence between architectural modernization
and reconstruction-based training. This biology-inspired framework provides a
principled typology of vision models, showing that emergent computational
strategies-shaped jointly by architecture and training objective-define
representational structure beyond surface design categories.

</details>


### [207] [FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction](https://arxiv.org/abs/2509.21657)
*Yixiang Dai,Fan Jiang,Chiyu Wang,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: FantasyWorld is a geometry-enhanced framework that augments video foundation models to achieve consistent and generalizable 3D-aware video representations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance 3D grounding capabilities of video foundation models, which are pivotal for applications in AR/VR and robotic navigation.

Method: FantasyWorld integrates frozen video foundation models with a trainable geometric branch, enabling joint modeling of video latents and an implicit 3D field through cross-branch information exchange.

Result: FantasyWorld demonstrates superior multi-view coherence and style consistency compared to geometry-consistent baselines, while also serving as representations for downstream 3D tasks without needing optimization or fine-tuning.

Conclusion: The framework bridges the gap between video imagination and 3D perception, advancing capabilities for downstream 3D tasks and AR/VR applications.

Abstract: High-quality 3D world models are pivotal for embodied intelligence and
Artificial General Intelligence (AGI), underpinning applications such as AR/VR
content creation and robotic navigation. Despite the established strong
imaginative priors, current video foundation models lack explicit 3D grounding
capabilities, thus being limited in both spatial consistency and their utility
for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a
geometry-enhanced framework that augments frozen video foundation models with a
trainable geometric branch, enabling joint modeling of video latents and an
implicit 3D field in a single forward pass. Our approach introduces
cross-branch supervision, where geometry cues guide video generation and video
priors regularize 3D prediction, thus yielding consistent and generalizable
3D-aware video representations. Notably, the resulting latents from the
geometric branch can potentially serve as versatile representations for
downstream 3D tasks such as novel view synthesis and navigation, without
requiring per-scene optimization or fine-tuning. Extensive experiments show
that FantasyWorld effectively bridges video imagination and 3D perception,
outperforming recent geometry-consistent baselines in multi-view coherence and
style consistency. Ablation studies further confirm that these gains stem from
the unified backbone and cross-branch information exchange.

</details>


### [208] [MORPH: Shape-agnostic PDE Foundation Models](https://arxiv.org/abs/2509.21670)
*Mahindra Singh Rautela,Alexander Most,Siddharth Mansingh,Bradley C. Love,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.CV

TL;DR: The paper presents MORPH, a versatile autoregressive foundation model for solving PDEs using convolutional vision transformer architecture, achieving strong results even in complex, heterogeneous datasets.


<details>
  <summary>Details</summary>
Motivation: To create a universal machine learning model capable of handling diverse partial differential equations (PDEs) across different data dimensions and resolutions, addressing the challenge of heterogeneity in scientific observations.

Method: MORPH utilizes a convolutional vision transformer with three main components: component-wise convolution for local interactions, inter-field cross-attention for multi-field data, and axial attentions for reducing computational costs. The model is pretrained on varied PDE datasets and tested with fine-tuning or LoRA approaches.

Result: MORPH achieves superior or competitive performance compared to state-of-the-art models across various generalization tasks in zero-shot and full-shot scenarios.

Conclusion: MORPH demonstrates the potential to be a scalable, data-efficient backbone for scientific machine learning, capable of handling the complexity and multimodality of scientific datasets effectively.

Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for
partial differential equations (PDEs). MORPH is built on a convolutional vision
transformer backbone that seamlessly handles heterogeneous spatiotemporal
datasets of varying data dimensionality (1D--3D) at different resolutions,
multiple fields with mixed scalar and vector components. The architecture
combines (i) component-wise convolution, which jointly processes scalar and
vector channels to capture local interactions, (ii) inter-field
cross-attention, which models and selectively propagates information between
different physical fields, (iii) axial attentions, which factorizes full
spatiotemporal self-attention along individual spatial and temporal axes to
reduce computational burden while retaining expressivity. We pretrain multiple
model variants on a diverse collection of heterogeneous PDE datasets and
evaluate transfer to a range of downstream prediction tasks. Using both
full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH
outperforms models trained from scratch in both zero-shot and full-shot
generalization. Across extensive evaluations, MORPH matches or surpasses strong
baselines and recent state-of-the-art models. Collectively, these capabilities
present a flexible and powerful backbone for learning from heterogeneous and
multimodal nature of scientific observations, charting a path toward scalable
and data-efficient scientific machine learning.

</details>


### [209] [MS-YOLO: Infrared Object Detection for Edge Deployment via MobileNetV4 and SlideLoss](https://arxiv.org/abs/2509.21696)
*Jiali Zhang,Thomas S. White,Haoliang Zhang,Wenqing Hu,Donald C. Wunsch II,Jian Liu*

Main category: cs.CV

TL;DR: This paper proposes \texttt{MS-YOLO}, an optimized YOLO variant utilizing MobileNetV4 and a novel loss function \emph{SlideLoss}, tailored for resource-efficient and precise infrared-based urban object detection.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like class imbalance, thermal noise, and computational constraints in infrared-based urban object detection, improving accuracy and real-time applicability under low-light and adverse weather conditions.

Method: The study substituted YOLOv8's CSPDarknet backbone with MobileNetV4 for efficiency, and introduced SlideLoss, a loss function that emphasizes under-represented samples. YOLO variants were tested on the FLIR ADAS V2 dataset.

Result: \texttt{MS-YOLO} achieved competitive mAP, superior precision, and reduced computational overhead, requiring only 6.7 GFLOPs.

Conclusion: The proposed \texttt{MS-YOLO} effectively balances accuracy and efficiency, demonstrating suitability for real-time edge deployments in urban settings.

Abstract: Infrared imaging has emerged as a robust solution for urban object detection
under low-light and adverse weather conditions, offering significant advantages
over traditional visible-light cameras. However, challenges such as class
imbalance, thermal noise, and computational constraints can significantly
hinder model performance in practical settings. To address these issues, we
evaluate multiple YOLO variants on the FLIR ADAS V2 dataset, ultimately
selecting YOLOv8 as our baseline due to its balanced accuracy and efficiency.
Building on this foundation, we present \texttt{MS-YOLO} (\textbf{M}obileNetv4
and \textbf{S}lideLoss based on YOLO), which replaces YOLOv8's CSPDarknet
backbone with the more efficient MobileNetV4, reducing computational overhead
by \textbf{1.5%} while sustaining high accuracy. In addition, we introduce
\emph{SlideLoss}, a novel loss function that dynamically emphasizes
under-represented and occluded samples, boosting precision without sacrificing
recall. Experiments on the FLIR ADAS V2 benchmark show that \texttt{MS-YOLO}
attains competitive mAP and superior precision while operating at only
\textbf{6.7 GFLOPs}. These results demonstrate that \texttt{MS-YOLO}
effectively addresses the dual challenge of maintaining high detection quality
while minimizing computational costs, making it well-suited for real-time edge
deployment in urban environments.

</details>


### [210] [Motion-Aware Transformer for Multi-Object Tracking](https://arxiv.org/abs/2509.21715)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: This paper introduces the Motion-Aware Transformer (MATR), enhancing multi-object tracking (MOT) by explicitly modeling object motion to improve detection and association.


<details>
  <summary>Details</summary>
Motivation: MOT in videos is challenging due to complex object motions and crowded scenes, and current DETR-based frameworks lack adequate handling of detection and tracking queries, leading to accuracy issues.

Method: The method introduces the Motion-Aware Transformer (MATR), which predicts object movements to update tracking queries beforehand, reducing query collisions and enabling consistent training.

Result: MATR demonstrates significant improvements across MOT benchmarks like DanceTrack, SportsMOT, and BDD100k, achieving state-of-the-art metrics without relying on external datasets.

Conclusion: Explicitly modeling motion in end-to-end Transformers simplifies and effectively advances multi-object tracking tasks.

Abstract: Multi-object tracking (MOT) in videos remains challenging due to complex
object motions and crowded scenes. Recent DETR-based frameworks offer
end-to-end solutions but typically process detection and tracking queries
jointly within a single Transformer Decoder layer, leading to conflicts and
degraded association accuracy. We introduce the Motion-Aware Transformer
(MATR), which explicitly predicts object movements across frames to update
track queries in advance. By reducing query collisions, MATR enables more
consistent training and improves both detection and association. Extensive
experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers
significant gains across standard metrics. On DanceTrack, MATR improves HOTA by
more than 9 points over MOTR without additional data and reaches a new
state-of-the-art score of 71.3 with supplementary data. MATR also achieves
state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6
mHOTA) without relying on external datasets. These results demonstrate that
explicitly modeling motion within end-to-end Transformers offers a simple yet
highly effective approach to advancing multi-object tracking.

</details>


### [211] [DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining](https://arxiv.org/abs/2509.21719)
*Shuning Sun,Jialang Lu,Xiang Chen,Jichao Wang,Dianjie Lu,Guijuan Zhang,Guangwei Gao,Zhuoran Zheng*

Main category: cs.CV

TL;DR: The paper introduces DeLiVR, a video deraining method leveraging Lie-group differential biases, to enhance spatiotemporal consistency while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Videos captured in real-world conditions often contain rain streaks, blur, and noise, alongside challenges like temporal artifacts and cross-frame mismatches exacerbated by camera movements. Existing methods are computationally demanding and less reliable, necessitating a better solution.

Method: DeLiVR utilizes Lie groups to incorporate spatiotemporal biases into attention scores. Two main components are introduced: 1) a rotation-bounded Lie relative bias for geometry-consistent alignment using in-plane angle prediction; and 2) a differential group displacement to estimate angular velocity and guide rain streak alignment through temporal decay and attention masks.

Result: Extensive experiments conducted on public benchmarks demonstrated the superiority of DeLiVR in addressing video deraining challenges more effectively compared to prior methods.

Conclusion: By leveraging Lie-group differential biases, DeLiVR achieves both spatial and temporal consistency in video deraining, offering a computationally efficient and robust alternative to existing techniques.

Abstract: Videos captured in the wild often suffer from rain streaks, blur, and noise.
In addition, even slight changes in camera pose can amplify cross-frame
mismatches and temporal artifacts. Existing methods rely on optical flow or
heuristic alignment, which are computationally expensive and less robust. To
address these challenges, Lie groups provide a principled way to represent
continuous geometric transformations, making them well-suited for enforcing
spatial and temporal consistency in video modeling. Building on this insight,
we propose DeLiVR, an efficient video deraining method that injects
spatiotemporal Lie-group differential biases directly into attention scores of
the network. Specifically, the method introduces two complementary components.
First, a rotation-bounded Lie relative bias predicts the in-plane angle of each
frame using a compact prediction module, where normalized coordinates are
rotated and compared with base coordinates to achieve geometry-consistent
alignment before feature aggregation. Second, a differential group displacement
computes angular differences between adjacent frames to estimate a velocity.
This bias computation combines temporal decay and attention masks to focus on
inter-frame relationships while precisely matching the direction of rain
streaks. Extensive experimental results demonstrate the effectiveness of our
method on publicly available benchmarks.

</details>


### [212] [On the Status of Foundation Models for SAR Imagery](https://arxiv.org/abs/2509.21722)
*Nathan Inkawhich*

Main category: cs.CV

TL;DR: The paper explores applying foundational AI/ML models, specifically Self-Supervised Learning (SSL), to SAR object recognition tasks and sets a new state-of-the-art with fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: The authors are inspired by recent AI/ML advancements in the natural image domain, particularly SSL-trained models, which offer improved robustness, transferability, and adaptability with minimal labeled data.

Method: The authors tested state-of-the-art visual foundational models (DINOv2, DINOv3, PE-Core) for SAR object recognition and fine-tuned them with SAR data using SSL. They analyzed the performance of different backbones and task-adaptation recipes.

Result: The fine-tuned AFRL-DINOv2 models outperformed the current best SAR-domain model (SARATR-X), setting a new benchmark for SAR foundation models.

Conclusion: While fine-tuning SSL models for SAR is promising, there is still significant room for improvement, and further innovation is needed in SAR foundational models.

Abstract: In this work we investigate the viability of foundational AI/ML models for
Synthetic Aperture Radar (SAR) object recognition tasks. We are inspired by the
tremendous progress being made in the wider community, particularly in the
natural image domain where frontier labs are training huge models on web-scale
datasets with unprecedented computing budgets. It has become clear that these
models, often trained with Self-Supervised Learning (SSL), will transform how
we develop AI/ML solutions for object recognition tasks - they can be adapted
downstream with very limited labeled data, they are more robust to many forms
of distribution shift, and their features are highly transferable
out-of-the-box. For these reasons and more, we are motivated to apply this
technology to the SAR domain. In our experiments we first run tests with
today's most powerful visual foundational models, including DINOv2, DINOv3 and
PE-Core and observe their shortcomings at extracting semantically-interesting
discriminative SAR target features when used off-the-shelf. We then show that
Self-Supervised finetuning of publicly available SSL models with SAR data is a
viable path forward by training several AFRL-DINOv2s and setting a new
state-of-the-art for SAR foundation models, significantly outperforming today's
best SAR-domain model SARATR-X. Our experiments further analyze the performance
trade-off of using different backbones with different downstream
task-adaptation recipes, and we monitor each model's ability to overcome
challenges within the downstream environments (e.g., extended operating
conditions and low amounts of labeled data). We hope this work will inform and
inspire future SAR foundation model builders, because despite our positive
results, we still have a long way to go.

</details>


### [213] [UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments](https://arxiv.org/abs/2509.21733)
*Jiannan Xiang,Yun Zhu,Lei Shu,Maria Wang,Lijun Yu,Gabriel Barcik,James Lyon,Srinivas Sunkara,Jindong Chen*

Main category: cs.CV

TL;DR: UISim, an image-based UI simulator, addresses challenges in mobile UI testing and AI agent interaction by simulating realistic UI transitions based on screen image inputs.


<details>
  <summary>Details</summary>
Motivation: Current methods for testing mobile UIs and training AI agents are limited, relying on physical devices or static analysis, which restricts scalability and innovation.

Method: The paper introduces UISim, using a two-stage method: first, it predicts the layout of the next UI state; then, it synthesizes a visually consistent image based on this layout.

Result: Experiments show UISim surpasses existing UI generation baselines in producing realistic, coherent subsequent UI states.

Conclusion: UISim offers a scalable, dynamic solution for UI testing and AI agent training, providing practical benefits and applications like synthetic data generation and UI navigation planning.

Abstract: Developing and testing user interfaces (UIs) and training AI agents to
interact with them are challenging due to the dynamic and diverse nature of
real-world mobile environments. Existing methods often rely on cumbersome
physical devices or limited static analysis of screenshots, which hinders
scalable testing and the development of intelligent UI agents. We introduce
UISim, a novel image-based UI simulator that offers a dynamic and interactive
platform for exploring mobile phone environments purely from screen images. Our
system employs a two-stage method: given an initial phone screen image and a
user action, it first predicts the abstract layout of the next UI state, then
synthesizes a new, visually consistent image based on this predicted layout.
This approach enables the realistic simulation of UI transitions. UISim
provides immediate practical benefits for UI testing, rapid prototyping, and
synthetic data generation. Furthermore, its interactive capabilities pave the
way for advanced applications, such as UI navigation task planning for AI
agents. Our experimental results show that UISim outperforms end-to-end UI
generation baselines in generating realistic and coherent subsequent UI states,
highlighting its fidelity and potential to streamline UI development and
enhance AI agent training.

</details>


### [214] [LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.21738)
*Mehwish Mehmood,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: The paper introduces LFA-Net, a lightweight retinal vessel segmentation network, designed for resource-constrained settings while achieving high segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient retinal vessel segmentation is critical for diagnosing vision-related and systemic diseases, particularly in computationally limited clinical environments.

Method: The paper presents LFA-Net, which features the LiteFusion-Attention module. This module integrates residual learning, Vision Mamba-inspired dynamics, and modulation-based attention to efficiently capture both local and global context.

Result: LFA-Net achieves high performance with minimal computational resources: 0.11M parameters, 0.42MB memory size, and 4.46 GFLOPs. It also demonstrates excellent metrics on datasets DRIVE (Dice: 83.28%), STARE (Dice: 87.44%), and CHASE_DB (Dice: 84.50%).

Conclusion: The proposed LFA-Net addresses small vessel segmentation and high computational cost issues, making it suitable for real-world applications in resource-constrained environments. The code is publicly available for further use.

Abstract: Lightweight retinal vessel segmentation is important for the early diagnosis
of vision-threatening and systemic diseases, especially in a real-world
clinical environment with limited computational resources. Although
segmentation methods based on deep learning are improving, existing models are
still facing challenges of small vessel segmentation and high computational
costs. To address these challenges, we proposed a new vascular segmentation
network, LFA-Net, which incorporates a newly designed attention module,
LiteFusion-Attention. This attention module incorporates residual learning
connections, Vision Mamba-inspired dynamics, and modulation-based attention,
enabling the model to capture local and global context efficiently and in a
lightweight manner. LFA-Net offers high performance with 0.11 million
parameters, 0.42 MB memory size, and 4.46 GFLOPs, which make it ideal for
resource-constrained environments. We validated our proposed model on DRIVE,
STARE, and CHASE_DB with outstanding performance in terms of dice scores of
83.28, 87.44, and 84.50% and Jaccard indices of 72.85, 79.31, and 74.70%,
respectively. The code of LFA-Net is available online
https://github.com/Mehwish4593/LFA-Net.

</details>


### [215] [DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation](https://arxiv.org/abs/2509.21930)
*Jiahui Wang,Changhao Chen*

Main category: cs.CV

TL;DR: The paper introduces DynaNav, a dynamic visual navigation framework for robotics and AI, which prioritizes efficiency and interpretability by adapting feature and layer selection based on scene complexity.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of transformer-based foundation models, particularly high computational overhead and weak interpretability, making them unsuitable for resource-constrained environments.

Method: Proposes a dynamic framework with trainable feature selectors and an early-exit mechanism optimized via Bayesian Optimization to enhance computational efficiency and interpretability.

Result: DynaNav demonstrated significant improvement in efficiency metrics (2.26x reduction in FLOPs, 42.3% lower inference time, 32.8% lower memory usage) while enhancing navigation performance across four datasets.

Conclusion: DynaNav successfully addresses computational and interpretability challenges in visual navigation, making it suitable for resource-tight applications.

Abstract: Visual navigation is essential for robotics and embodied AI. However,
existing foundation models, particularly those with transformer decoders,
suffer from high computational overhead and lack interpretability, limiting
their deployment in resource-tight scenarios. To address this, we propose
DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer
selection based on scene complexity. It employs a trainable hard feature
selector for sparse operations, enhancing efficiency and interpretability.
Additionally, we integrate feature selection into an early-exit mechanism, with
Bayesian Optimization determining optimal exit thresholds to reduce
computational cost. Extensive experiments in real-world-based datasets and
simulated environments demonstrate the effectiveness of DynaNav. Compared to
ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time,
and 32.8% lower memory usage, while improving navigation performance across
four public datasets.

</details>


### [216] [Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition](https://arxiv.org/abs/2509.21747)
*Qing Zhu,Wangdong Guo,Qirong Mao,Xiaohua Huang,Xiuyan Shao,Wenming Zheng*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for group-level emotion recognition (GER) that leverages visual contextual information and label-guided semantic details to enhance performance, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: GER methods often neglect the role of scene context in modeling individual relationships and the semantic insights from emotion labels, limiting their efficacy.

Method: The proposed framework uses: (1) a visual context encoding module that captures diverse individual relationships using multi-scale scene data, and (2) an emotion semantic encoding module that generates refined semantic representations via prompting and structuring a large language model with emotion labels.

Result: The method demonstrates competitive performance on three widely used GER datasets compared to current state-of-the-art approaches.

Conclusion: Integrating visual context and label-guided semantic information significantly improves the performance of GER systems, making them more effective at holistic emotion recognition.

Abstract: Group-level emotion recognition (GER) aims to identify holistic emotions
within a scene involving multiple individuals. Current existed methods
underestimate the importance of visual scene contextual information in modeling
individual relationships. Furthermore, they overlook the crucial role of
semantic information from emotional labels for complete understanding of
emotions. To address this limitation, we propose a novel framework that
incorporates visual scene context and label-guided semantic information to
improve GER performance. It involves the visual context encoding module that
leverages multi-scale scene information to diversely encode individual
relationships. Complementarily, the emotion semantic encoding module utilizes
group-level emotion labels to prompt a large language model to generate nuanced
emotion lexicons. These lexicons, in conjunction with the emotion labels, are
then subsequently refined into comprehensive semantic representations through
the utilization of a structured emotion tree. Finally, similarity-aware
interaction is proposed to align and integrate visual and semantic information,
thereby generating enhanced group-level emotion representations and
subsequently improving the performance of GER. Experiments on three widely
adopted GER datasets demonstrate that our proposed method achieves competitive
performance compared to state-of-the-art methods.

</details>


### [217] [Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics](https://arxiv.org/abs/2509.22014)
*Saurav Jha,Stefan K. Ehrlich*

Main category: cs.CV

TL;DR: This paper introduces a multimodal framework for healthcare robotics that improves video-based scene understanding and reasoning over time, achieving better robustness and competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of current Vision-Language Models (VLMs) in temporal reasoning, uncertainty estimation, and structured outputs, which are critical for ensuring safety in healthcare robotics.

Method: The paper proposes a lightweight framework combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer. The framework supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation, and generates structured scene graphs for video-based scene understanding.

Result: The framework demonstrated competitive accuracy and improved robustness when evaluated on the Video-MME benchmark and a custom clinical dataset, outperforming state-of-the-art VLMs in certain healthcare robotics scenarios.

Conclusion: The proposed framework shows significant potential for enhancing applications in robot-assisted surgery, patient monitoring, and clinical decision support, addressing key limitations in existing VLMs for healthcare robotics.

Abstract: Healthcare robotics requires robust multimodal perception and reasoning to
ensure safety in dynamic clinical environments. Current Vision-Language Models
(VLMs) demonstrate strong general-purpose capabilities but remain limited in
temporal reasoning, uncertainty estimation, and structured outputs needed for
robotic planning. We present a lightweight agentic multimodal framework for
video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model
with a SmolAgent-based orchestration layer, it supports chain-of-thought
reasoning, speech-vision fusion, and dynamic tool invocation. The framework
generates structured scene graphs and leverages a hybrid retrieval module for
interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark
and a custom clinical dataset show competitive accuracy and improved robustness
compared to state-of-the-art VLMs, demonstrating its potential for applications
in robot-assisted surgery, patient monitoring, and decision support.

</details>


### [218] [KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields](https://arxiv.org/abs/2509.21750)
*Yu Li,Da Chang,Xi Xiao*

Main category: cs.CV

TL;DR: Introducing KG-SAM, a framework improving SAM for medical imaging segmentation by integrating anatomical knowledge, uncertainty estimation, and boundary refinement.


<details>
  <summary>Details</summary>
Motivation: SAM struggles with challenges like ambiguous boundaries and anatomy modeling when applied to medical imaging, affecting its accuracy and reliability.

Method: KG-SAM uses a medical knowledge graph, an energy-based CRF for boundary refinement, and an uncertainty-aware fusion module for improved segmentation.

Result: KG-SAM achieves 82.69% Dice score on prostate segmentation and notable gains in abdominal segmentation with MRI (78.05%) and CT (79.68%) datasets.

Conclusion: KG-SAM demonstrates robustness and generalizability for medical image segmentation, addressing key limitations of SAM in the clinical domain.

Abstract: While the Segment Anything Model (SAM) has achieved remarkable success in
image segmentation, its direct application to medical imaging remains hindered
by fundamental challenges, including ambiguous boundaries, insufficient
modeling of anatomical relationships, and the absence of uncertainty
quantification. To address these limitations, we introduce KG-SAM, a
knowledge-guided framework that synergistically integrates anatomical priors
with boundary refinement and uncertainty estimation. Specifically, KG-SAM
incorporates (i) a medical knowledge graph to encode fine-grained anatomical
relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce
anatomically consistent predictions, and (iii) an uncertainty-aware fusion
module to enhance reliability in high-stakes clinical scenarios. Extensive
experiments across multi-center medical datasets demonstrate the effectiveness
of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate
segmentation and delivers substantial gains in abdominal segmentation, reaching
78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and
generalizable framework for advancing medical image segmentation.

</details>


### [219] [UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models](https://arxiv.org/abs/2509.21760)
*Lan Chen,Yuchao Gu,Qi Mao*

Main category: cs.CV

TL;DR: The paper presents UniVid, a framework that fine-tunes pre-trained video generation models to handle diverse vision tasks without task-specific modifications.


<details>
  <summary>Details</summary>
Motivation: Inspired by the success of large language models unifying tasks, the authors aim to explore whether pre-trained video generation models can adapt to diverse image and video tasks without costly task-specific pre-training.

Method: The proposed framework, UniVid, fine-tunes a video diffusion transformer to perform various vision tasks using visual sentences, where context sequences define tasks and output modalities.

Result: UniVid demonstrates good generalization in cross-modal and cross-source tasks, despite being trained only on natural video data. Vision tasks can switch between understanding and generation by reversing visual sentence order.

Conclusion: Pre-trained video generation models have the potential to function as scalable and unified foundations for vision tasks, offering an alternative to extensive task-specific pre-training.

Abstract: Large language models, trained on extensive corpora, successfully unify
diverse linguistic tasks within a single generative framework. Inspired by
this, recent works like Large Vision Model (LVM) extend this paradigm to vision
by organizing tasks into sequential visual sentences, where visual prompts
serve as the context to guide outputs. However, such modeling requires
task-specific pre-training across modalities and sources, which is costly and
limits scalability to unseen tasks. Given that pre-trained video generation
models inherently capture temporal sequence dependencies, we explore a more
unified and scalable alternative: can a pre-trained video generation model
adapt to diverse image and video tasks? To answer this, we propose UniVid, a
framework that fine-tunes a video diffusion transformer to handle various
vision tasks without task-specific modifications. Tasks are represented as
visual sentences, where the context sequence defines both the task and the
expected output modality. We evaluate the generalization of UniVid from two
perspectives: (1) cross-modal inference with contexts composed of both images
and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks
from natural to annotated data, without multi-source pre-training. Despite
being trained solely on natural video data, UniVid generalizes well in both
settings. Notably, understanding and generation tasks can easily switch by
simply reversing the visual sentence order in this paradigm. These findings
highlight the potential of pre-trained video generation models to serve as a
scalable and unified foundation for vision modeling. Our code will be released
at https://github.com/CUC-MIPG/UniVid.

</details>


### [220] [CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones](https://arxiv.org/abs/2509.21764)
*Wenyi Gong,Mieszko Lis*

Main category: cs.CV

TL;DR: The paper presents a token merging method for Vision Transformer (ViT) architectures that preserves spatial structure, providing improved performance and speed without compromising model accuracy.


<details>
  <summary>Details</summary>
Motivation: ViT architectures rely heavily on spatial structures, but existing token reduction methods often fail to preserve such spatial integrity, limiting compatibility and performance. The paper aims to address this gap.

Method: The method uses (i) a 2D reduction strategy for structured layouts, (ii) a spatial-aware merging algorithm to maintain relative positions, and (iii) a max-magnitude-per-dimension token representation to retain critical features.

Result: The method achieves 1.25x speedup on SAM-H with only a 0.7% mIOU drop on COCO and 1.15x speedup on DeiT-B with no accuracy loss on ImageNet after one epoch of fine-tuning.

Conclusion: The proposed token merging approach enhances computational efficiency while maintaining the spatial structure, making it highly suitable for diverse ViT architectures across a variety of vision tasks.

Abstract: Many modern ViT backbones adopt spatial architectural designs, such as window
attention, decomposed relative positional embeddings in SAM, and RoPE in
DINOv3. Such architectures impose new challenges on token reduction, as the
vast majority of existing methods fail to preserve the spatial structure these
architectures depend on. In this paper, we introduce a simple yet effective
token merging method that maintains spatial integrity, enabling seamless
compatibility with spatial architectures. We reconcile two seemingly
conflicting requirements: (i)exploiting the uneven information distribution
across the spatial layout while (ii)preserving the spatial structure
post-merging. Our approach employs (i)a 2D reduction strategy to enforce
structured token layouts, (ii)a spatial-aware merging algorithm that maintains
relative token positions, and (iii)a novel max-magnitude-per-dimension token
representation that preserves salient features. Our method demonstrates strong
performance both off-the-shelf and with fine-tuning, achieving state-of-the-art
results on spatial and non-spatial architectures across various vision tasks.
Specifically, we achieve 1.25x speedup on SAM-H with only 0.7% mIOU drop
evaluated on COCO off-the-shelf, and 1.15x speedup on DeiT-B with no top-1
accuracy drop on ImageNet within just one epoch of fine-tuning.

</details>


### [221] [MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning](https://arxiv.org/abs/2509.22281)
*Jinkun Hao,Naifu Liang,Zhen Luo,Xudong Xu,Weipeng Zhong,Ran Yi,Yichen Jin,Zhaoyang Lyu,Feng Zheng,Lizhuang Ma,Jiangmiao Pang*

Main category: cs.CV

TL;DR: The paper addresses the challenge of generating task-relevant tabletop scenes using a novel framework and dataset, MesaTask-10K. Traditional methods are either time-consuming or lack realism.


<details>
  <summary>Details</summary>
Motivation: Robots require plausible and task-relevant tabletop scene generation for efficient task execution, yet current methods are either manual or random, lacking alignment with tasks.

Method: Proposed a Spatial Reasoning Chain within the MesaTask framework, enhanced by DPO algorithms, to generate realistic 3D tabletop layouts. Introduced MesaTask-10K dataset to assist research.

Result: MesaTask outperforms baseline methods in producing task-specific and realistic tabletop scenes as shown through exhaustive experiments.

Conclusion: The study successfully bridges high-level task instructions with realistic scene generation, advancing robotics manipulation tasks and providing a valuable dataset and methodology.

Abstract: The ability of robots to interpret human instructions and execute
manipulation tasks necessitates the availability of task-relevant tabletop
scenes for training. However, traditional methods for creating these scenes
rely on time-consuming manual layout design or purely randomized layouts, which
are limited in terms of plausibility or alignment with the tasks. In this
paper, we formulate a novel task, namely task-oriented tabletop scene
generation, which poses significant challenges due to the substantial gap
between high-level task instructions and the tabletop scenes. To support
research on such a challenging task, we introduce MesaTask-10K, a large-scale
dataset comprising approximately 10,700 synthetic tabletop scenes with manually
crafted layouts that ensure realistic layouts and intricate inter-object
relations. To bridge the gap between tasks and scenes, we propose a Spatial
Reasoning Chain that decomposes the generation process into object inference,
spatial interrelation reasoning, and scene graph construction for the final 3D
layout. We present MesaTask, an LLM-based framework that utilizes this
reasoning chain and is further enhanced with DPO algorithms to generate
physically plausible tabletop scenes that align well with given task
descriptions. Exhaustive experiments demonstrate the superior performance of
MesaTask compared to baselines in generating task-conforming tabletop scenes
with realistic layouts. Project page is at https://mesatask.github.io/

</details>


### [222] [Training-Free Multimodal Deepfake Detection via Graph Reasoning](https://arxiv.org/abs/2509.21774)
*Yuxin Liu,Fei Wang,Kun Li,Yiqi Nie,Junjie Chen,Yanyan Wei,Zhangling Duan,Zhaohong Jia*

Main category: cs.CV

TL;DR: The paper presents "GASP-ICL," a training-free framework enhancing large vision-language models (LVLMs) for multimodal deepfake detection (MDD), showcasing performance gains on multiple forgery types.


<details>
  <summary>Details</summary>
Motivation: Modern information systems demand robust multimodal deepfake detection due to vulnerabilities in visual, audio, and textual manipulations. Current LVLMs struggle with subtle forgery cues, cross-modal inconsistencies, and task-oriented retrieval.

Method: The paper proposes "GASP-ICL," a training-free MDD framework. It includes a pipeline preserving semantic relevance while injecting task-aware knowledge. The framework uses an MDD-adapted feature extractor for aligned image-text pair retrieval and introduces "GSTAS," a module to propagate query-aligned signals and refine task-relevant demonstrations.

Result: Experiments on four types of forgery demonstrate that "GASP-ICL" outperforms strong baselines, offering superior multimodal deepfake detection without fine-tuning the LVLMs.

Conclusion: The proposed GASP-ICL framework significantly improves detection robustness for multimodal deepfake detection tasks by leveraging adaptive scoring and in-context learning without requiring large model fine-tuning.

Abstract: Multimodal deepfake detection (MDD) aims to uncover manipulations across
visual, textual, and auditory modalities, thereby reinforcing the reliability
of modern information systems. Although large vision-language models (LVLMs)
exhibit strong multimodal reasoning, their effectiveness in MDD is limited by
challenges in capturing subtle forgery cues, resolving cross-modal
inconsistencies, and performing task-aligned retrieval. To this end, we propose
Guided Adaptive Scorer and Propagation In-Context Learning (GASP-ICL), a
training-free framework for MDD. GASP-ICL employs a pipeline to preserve
semantic relevance while injecting task-aware knowledge into LVLMs. We leverage
an MDD-adapted feature extractor to retrieve aligned image-text pairs and build
a candidate set. We further design the Graph-Structured Taylor Adaptive Scorer
(GSTAS) to capture cross-sample relations and propagate query-aligned signals,
producing discriminative exemplars. This enables precise selection of
semantically aligned, task-relevant demonstrations, enhancing LVLMs for robust
MDD. Experiments on four forgery types show that GASP-ICL surpasses strong
baselines, delivering gains without LVLM fine-tuning.

</details>


### [223] [Prompt-guided Representation Disentanglement for Action Recognition](https://arxiv.org/abs/2509.21783)
*Tianci Wu,Guangming Zhu,Jiang Lu,Siyuan Wang,Ning Wang,Nuoye Xiong,Zhang Liang*

Main category: cs.CV

TL;DR: ProDA framework enhances action recognition in videos by disentangling actions using scene graphs and dynamic prompts.


<details>
  <summary>Details</summary>
Motivation: Existing methods face difficulties in multi-action scenarios as they use unified features, lacking the capability to model interactions between different objects.

Method: ProDA utilizes Spatio-temporal Scene Graphs and Dynamic Prompt Module to create disentangled action-specific representations. It employs a Graph Parsing Neural Network with dynamic weights for video context.

Result: ProDA outperformed state-of-the-art methods in video action recognition tasks.

Conclusion: ProDA is an effective solution for disentangling specific actions from complex video scenes, advancing the field of action recognition.

Abstract: Action recognition is a fundamental task in video understanding. Existing
methods typically extract unified features to process all actions in one video,
which makes it challenging to model the interactions between different objects
in multi-action scenarios. To alleviate this issue, we explore disentangling
any specified actions from complex scenes as an effective solution. In this
paper, we propose Prompt-guided Disentangled Representation for Action
Recognition (ProDA), a novel framework that disentangles any specified actions
from a multi-action scene. ProDA leverages Spatio-temporal Scene Graphs (SSGs)
and introduces Dynamic Prompt Module (DPM) to guide a Graph Parsing Neural
Network (GPNN) in generating action-specific representations. Furthermore, we
design a video-adapted GPNN that aggregates information using dynamic weights.
Experiments in video action recognition demonstrate the effectiveness of our
approach when compared with the state-of-the-art methods. Our code can be found
in https://github.com/iamsnaping/ProDA.git

</details>


### [224] [DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images](https://arxiv.org/abs/2509.21787)
*Dwip Dalal,Gautam Vashishtha,Anku Ranui,Aishwarya Reganti,Parth Patwa,Mohd Sarique,Chandan Gupta,Keshav Nath,Viswanatha Reddy,Vinija Jain,Aman Chadha,Amitava Das,Amit Sheth,Asif Ekbal*

Main category: cs.CV

TL;DR: This study introduces a multimodal dataset and a model called DeHater to detect and mitigate hateful content in images.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the rise in harmful online content that distorts public discourse and challenges digital well-being.

Method: The authors used watermarked stable diffusion techniques and the Digital Attention Analysis Module (DAAM) to create hate attention maps, which identify and blur hateful sections in images.

Result: A new vision-language model, DeHater, was developed for identifying and mitigating hateful content using multimodal data, achieving improved performance in image hate detection tasks.

Conclusion: The proposed dataset and DeHater model advance ethical AI solutions for identifying and removing hateful digital content, particularly in social media contexts.

Abstract: The rise in harmful online content not only distorts public discourse but
also poses significant challenges to maintaining a healthy digital environment.
In response to this, we introduce a multimodal dataset uniquely crafted for
identifying hate in digital content. Central to our methodology is the
innovative application of watermarked, stability-enhanced, stable diffusion
techniques combined with the Digital Attention Analysis Module (DAAM). This
combination is instrumental in pinpointing the hateful elements within images,
thereby generating detailed hate attention maps, which are used to blur these
regions from the image, thereby removing the hateful sections of the image. We
release this data set as a part of the dehate shared task. This paper also
describes the details of the shared task. Furthermore, we present DeHater, a
vision-language model designed for multimodal dehatification tasks. Our
approach sets a new standard in AI-driven image hate detection given textual
prompts, contributing to the development of more ethical AI applications in
social media.

</details>


### [225] [MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning](https://arxiv.org/abs/2509.21788)
*Lihao Zheng,Jiawei Chen,Xintian Shen,Hao Ma,Tao Wei*

Main category: cs.CV

TL;DR: The paper introduces MIRG-RL, a method to enhance multi-image reasoning and grounding using a novel training approach and reinforcement learning, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current Large Visual Language Models (LVLMs) struggle with cross-image reasoning and grounding due to their lack of multi-image reasoning capabilities and insufficient cross-image reward modeling.

Method: The authors propose a two-stage training paradigm combining supervised fine-tuning with annotated trajectories and image-aware reinforcement learning. They also construct a lightweight reasoning-enhanced dataset using a novel trajectory data generation method and employ dual reward functions for objects and images.

Result: MIRG-RL surpasses previous methods in multi-image grounding benchmarks, achieving 64.82% on cross-image reasoning tasks, a 1% improvement over the prior state-of-the-art.

Conclusion: MIRG-RL successfully addresses limitations in LVLMs for multi-image reasoning and grounding, offering a unified framework with proven performance gains and public code/dataset available for further use.

Abstract: Multi-image reasoning and grounding require understanding complex cross-image
relationships at both object levels and image levels. Current Large Visual
Language Models (LVLMs) face two critical challenges: the lack of cross-image
reasoning capabilities and insufficient cross-image reference reward modeling.
To address these issues, we propose a unified framework - Multi-Image Reasoning
and Grounding with Reinforcement Learning (MIRG-RL). Specifically, our
two-stage training paradigm combines supervised fine-tuning with annotated
trajectories and image-aware reinforcement learning optimization, progressively
developing multi-image reasoning capabilities. Furthermore, we innovatively
propose a method for constructing the trajectory data, which integrates
object-level and image-level annotation information, and use this method to
generate a lightweight reasoning-enhanced dataset. To effectively resolve
cross-image ambiguities, we design an image-aware RL policy with dual reward
functions for objects and images. Experiments demonstrate that MIRG-RL achieves
state-of-the-art (SOTA) performance in multi-image grounding benchmarks,
attaining 64.82% on cross-image reasoning tasks - exceeding the previous best
method by 1%. The code and dataset have been released at
https://github.com/ZEUS2035/MIRG-RL.

</details>


### [226] [LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE](https://arxiv.org/abs/2509.21790)
*Yu Shang,Lei Jin,Yiding Ma,Xin Zhang,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: The paper introduces LongScape, a framework for stable long-horizon video generation, addressing limitations in existing diffusion-based and autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for video generation face challenges like temporal inconsistency and visual drift, making long-horizon generation unstable.

Method: LongScape combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation, using an action-guided variable-length chunking mechanism and a Context-aware Mixture-of-Experts (CMoE) for chunk specialization.

Result: Extensive experiments confirm that LongScape achieves consistent, high-quality, long-horizon video rollouts.

Conclusion: LongScape effectively overcomes the challenges of long-horizon video generation, paving the way for improved embodied manipulation data creation.

Abstract: Video-based world models hold significant potential for generating
high-quality embodied manipulation data. However, current video generation
methods struggle to achieve stable long-horizon generation: classical
diffusion-based approaches often suffer from temporal inconsistency and visual
drift over multiple rollouts, while autoregressive methods tend to compromise
on visual detail. To solve this, we introduce LongScape, a hybrid framework
that adaptively combines intra-chunk diffusion denoising with inter-chunk
autoregressive causal generation. Our core innovation is an action-guided,
variable-length chunking mechanism that partitions video based on the semantic
context of robotic actions. This ensures each chunk represents a complete,
coherent action, enabling the model to flexibly generate diverse dynamics. We
further introduce a Context-aware Mixture-of-Experts (CMoE) framework that
adaptively activates specialized experts for each chunk during generation,
guaranteeing high visual quality and seamless chunk transitions. Extensive
experimental results demonstrate that our method achieves stable and consistent
long-horizon generation over extended rollouts. Our code is available at:
https://github.com/tsinghua-fib-lab/Longscape.

</details>


### [227] [MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation](https://arxiv.org/abs/2509.21797)
*Yu Shang,Yangcheng Yu,Xin Zhang,Xin Jin,Haisheng Su,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: The paper introduces MoWM, a hybrid mixture-of-world-model framework, which combines latent and pixel space world models for improved embodied action planning in robotics.


<details>
  <summary>Details</summary>
Motivation: Pixel-level reconstruction in video world models causes visual redundancies, while latent models lack the necessary fine-grained details for precise manipulation.

Method: MoWM leverages motion-aware latent model representations as high-level priors to guide extraction of fine-grained visual features from pixel space models, optimizing embodied planning tasks.

Result: MoWM achieves state-of-the-art task performance and generalization on the CALVIN benchmark, surpassing existing approaches.

Conclusion: By fusing hybrid feature spaces, MoWM provides enhanced precision and generalization, advancing embodied action planning research and offering a framework for improved manipulation tasks.

Abstract: Embodied action planning is a core challenge in robotics, requiring models to
generate precise actions from visual observations and language instructions.
While video generation world models are promising, their reliance on
pixel-level reconstruction often introduces visual redundancies that hinder
action decoding and generalization. Latent world models offer a compact,
motion-aware representation, but overlook the fine-grained details critical for
precise manipulation. To overcome these limitations, we propose MoWM, a
mixture-of-world-model framework that fuses representations from hybrid world
models for embodied action planning. Our approach uses motion-aware
representations from a latent model as a high-level prior, which guides the
extraction of fine-grained visual features from the pixel space model. This
design allows MoWM to highlight the informative visual details needed for
action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that
our method achieves state-of-the-art task success rates and superior
generalization. We also provide a comprehensive analysis of the strengths of
each feature space, offering valuable insights for future research in embodied
planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.

</details>


### [228] [JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation](https://arxiv.org/abs/2509.22548)
*Shuang Zeng,Dekang Qi,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Shiyi Liang,Mu Xu,Xing Wei*

Main category: cs.CV

TL;DR: The paper introduces JanusVLN, a novel Vision-and-Language Navigation framework using dual implicit neural memory to improve spatial and semantic reasoning, achieving state-of-the-art (SOTA) performance.


<details>
  <summary>Details</summary>
Motivation: Current VLN methods suffer from issues such as spatial information loss and computational inefficiency due to reliance on explicit semantic memory.

Method: The framework incorporates a dual implicit neural memory using spatial-geometric and visual-semantic representations with compact neural models, alongside incremental updates through historical key-value caches.

Result: JanusVLN outperformed over 20 recent methods and demonstrated significant success rate improvements, ranging from 3.6-35.5 percentage points against various benchmarks.

Conclusion: JanusVLN's innovative dual implicit memory paradigm offers promising directions for future research in VLN applications by optimizing spatial and semantic cognition.

Abstract: Vision-and-Language Navigation requires an embodied agent to navigate through
unseen environments, guided by natural language instructions and a continuous
video stream. Recent advances in VLN have been driven by the powerful semantic
understanding of Multimodal Large Language Models. However, these methods
typically rely on explicit semantic memory, such as building textual cognitive
maps or storing historical visual frames. This type of method suffers from
spatial information loss, computational redundancy, and memory bloat, which
impede efficient navigation. Inspired by the implicit scene representation in
human navigation, analogous to the left brain's semantic understanding and the
right brain's spatial cognition, we propose JanusVLN, a novel VLN framework
featuring a dual implicit neural memory that models spatial-geometric and
visual-semantic memory as separate, compact, and fixed-size neural
representations. This framework first extends the MLLM to incorporate 3D prior
knowledge from the spatial-geometric encoder, thereby enhancing the spatial
reasoning capabilities of models based solely on RGB input. Then, the
historical key-value caches from the spatial-geometric and visual-semantic
encoders are constructed into a dual implicit memory. By retaining only the KVs
of tokens in the initial and sliding window, redundant computation is avoided,
enabling efficient incremental updates. Extensive experiments demonstrate that
JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For
example, the success rate improves by 10.5-35.5 compared to methods using
multiple data types as input and by 3.6-10.8 compared to methods using more RGB
training data. This indicates that the proposed dual implicit neural memory, as
a novel paradigm, explores promising new directions for future VLN research.
Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.

</details>


### [229] [DiTraj: training-free trajectory control for video diffusion transformer](https://arxiv.org/abs/2509.21839)
*Cheng Lei,Jiayu Zhang,Yue Ma,Xinyu Wang,Long Chen,Liang Tang,Yiqiang Yan,Fei Su,Zhicheng Zhao*

Main category: cs.CV

TL;DR: The paper introduces DiTraj, a training-free framework that enhances trajectory control in text-to-video generation using Diffusion Transformers (DiT).


<details>
  <summary>Details</summary>
Motivation: Existing trajectory control methods either demand vast training resources or are tailored for U-Net, limiting their use with DiT despite its generative strengths.

Method: The approach involves separating foreground and background prompts with a Large Language Model, then utilizing 3D-RoPE to modify position embedding of foreground tokens to enhance cross-frame attention and trajectory control.

Result: Experiments show DiTraj improves video quality and trajectory control compared to prior methods.

Conclusion: DiTraj effectively leverages DiT for trajectory-controlled video generation without extensive training requirements, showcasing a robust and user-friendly solution.

Abstract: Diffusion Transformers (DiT)-based video generation models with 3D full
attention exhibit strong generative capabilities. Trajectory control represents
a user-friendly task in the field of controllable video generation. However,
existing methods either require substantial training resources or are
specifically designed for U-Net, do not take advantage of the superior
performance of DiT. To address these issues, we propose DiTraj, a simple but
effective training-free framework for trajectory control in text-to-video
generation, tailored for DiT. Specifically, first, to inject the object's
trajectory, we propose foreground-background separation guidance: we use the
Large Language Model (LLM) to convert user-provided prompts into foreground and
background prompts, which respectively guide the generation of foreground and
background regions in the video. Then, we analyze 3D full attention and explore
the tight correlation between inter-token attention scores and position
embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled
3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding,
STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening
cross-frame attention among them and thus enhancing trajectory control.
Additionally, we achieve 3D-aware trajectory control by regulating the density
of position embedding. Extensive experiments demonstrate that our method
outperforms previous methods in both video quality and trajectory
controllability.

</details>


### [230] [A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design](https://arxiv.org/abs/2509.21845)
*Zichen Zhang,Kunlong Zhang,Hongwei Ruan,Yiming Luo*

Main category: cs.CV

TL;DR: This paper evaluates different strategies for multi-hop question answering in retrieval-augmented frameworks, showing that a hybrid approach delivers substantial accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of multi-hop reasoning in question answering, which requires combining information across passages.

Method: The paper evaluates cosine similarity, maximal marginal relevance, and a hybrid method that integrates dense embeddings with lexical overlap. Additionally, it adapts the EfficientRAG pipeline by introducing token labeling and iterative refinement.

Result: The hybrid approach outperformed baseline methods with a 50% relative improvement in exact match and 47% in F1 score on the HotpotQA dataset.

Conclusion: Hybrid retrieval-augmented generation provides a practical and efficient solution to multi-hop question answering, though challenges like distractors and temporal reasoning remain.

Abstract: Transformer-based models have advanced the field of question answering, but
multi-hop reasoning, where answers require combining evidence across multiple
passages, remains difficult. This paper presents a comprehensive evaluation of
retrieval strategies for multi-hop question answering within a
retrieval-augmented generation framework. We compare cosine similarity, maximal
marginal relevance, and a hybrid method that integrates dense embeddings with
lexical overlap and re-ranking. To further improve retrieval, we adapt the
EfficientRAG pipeline for query optimization, introducing token labeling and
iterative refinement while maintaining efficiency. Experiments on the HotpotQA
dataset show that the hybrid approach substantially outperforms baseline
methods, achieving a relative improvement of 50 percent in exact match and 47
percent in F1 score compared to cosine similarity. Error analysis reveals that
hybrid retrieval improves entity recall and evidence complementarity, while
remaining limited in handling distractors and temporal reasoning. Overall, the
results suggest that hybrid retrieval-augmented generation provides a practical
zero-shot solution for multi-hop question answering, balancing accuracy,
efficiency, and interpretability.

</details>


### [231] [Dynamic Novel View Synthesis in High Dynamic Range](https://arxiv.org/abs/2509.21853)
*Kaixuan Zhang,Zhipeng Xiong,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: This paper introduces HDR Dynamic Novel View Synthesis (HDR DNVS), focusing on photorealistic 3D HDR view synthesis from dynamic LDR scenes using a novel Gaussian Splatting-based architecture.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current HDR view synthesis methods which assume stationary scenes, thus making them unsuitable for dynamic real-world scenarios.

Method: The authors propose HDR-4DGS, which employs a Gaussian Splatting framework combined with a dynamic tone-mapping module to link LDR and HDR domains while managing temporal radiance variations.

Result: HDR-4DGS surpasses state-of-the-art methods in quantitative metrics and visual quality, delivering lifelike HDR renderings for diverse viewpoints and temporal scenarios.

Conclusion: This work successfully bridges HDR and LDR domains for dynamic scenes by introducing a robust solution, paving the way for realistic HDR synthesis in dynamic environments.

Abstract: High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D
model from Low Dynamic Range (LDR) training images captured under conventional
imaging conditions. Current methods primarily focus on static scenes,
implicitly assuming all scene elements remain stationary and non-living.
However, real-world scenarios frequently feature dynamic elements, such as
moving objects, varying lighting conditions, and other temporal events, thereby
presenting a significantly more challenging scenario. To address this gap, we
propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR
DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of
jointly modeling temporal radiance variations alongside sophisticated 3D
translation between LDR and HDR. To tackle this complex, intertwined challenge,
we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an
innovative dynamic tone-mapping module that explicitly connects HDR and LDR
domains, maintaining temporal radiance coherence by dynamically adapting
tone-mapping functions according to the evolving radiance distributions across
the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance
consistency and spatially accurate color translation, enabling photorealistic
HDR renderings from arbitrary viewpoints and time instances. Extensive
experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art
methods in both quantitative performance and visual fidelity. Source code will
be released.

</details>


### [232] [SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes](https://arxiv.org/abs/2509.21859)
*Minje Kim,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: The paper introduces SRHand, a method for reconstructing detailed 3D hand geometry and textured images from low-resolution images, by combining implicit image representation and explicit hand meshes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing high-fidelity 3D hand geometry and textures from low-resolution images, overcoming the limitations of existing methods that rely on high-resolution inputs and struggle with deformable, articulated structures like hands.

Method: SRHand uses a geometric-aware implicit image function (GIIF) that learns detailed hand priors by upsampling coarse input images, while jointly optimizing the implicit image function and explicit 3D hand meshes to maintain multi-view and pose consistency.

Result: The proposed method demonstrates significant improvements over state-of-the-art image upsampling and 3D hand reconstruction methods in both quantitative and qualitative evaluations on the InterHand2.6M and Goliath datasets.

Conclusion: SRHand effectively reconstructs detailed 3D hand geometry and textures from low-resolution images, outperforming existing approaches and addressing challenges in multi-view consistency and articulated hand representation.

Abstract: Reconstructing detailed hand avatars plays a crucial role in various
applications. While prior works have focused on capturing high-fidelity hand
geometry, they heavily rely on high-resolution multi-view image inputs and
struggle to generalize on low-resolution images. Multi-view image
super-resolution methods have been proposed to enforce 3D view consistency.
These methods, however, are limited to static objects/scenes with fixed
resolutions and are not applicable to articulated deformable hands. In this
paper, we propose SRHand (Super-Resolution Hand), the method for reconstructing
detailed 3D geometry as well as textured images of hands from low-resolution
images. SRHand leverages the advantages of implicit image representation with
explicit hand meshes. Specifically, we introduce a geometric-aware implicit
image function (GIIF) that learns detailed hand prior by upsampling the coarse
input images. By jointly optimizing the implicit image function and explicit 3D
hand shapes, our method preserves multi-view and pose consistency among
upsampled hand images, and achieves fine-detailed 3D reconstruction (wrinkles,
nails). In experiments using the InterHand2.6M and Goliath datasets, our method
significantly outperforms state-of-the-art image upsampling methods adapted to
hand datasets, and 3D hand reconstruction methods, quantitatively and
qualitatively. Project page: https://yunminjin2.github.io/projects/srhand

</details>


### [233] [Deepfakes: we need to re-think the concept of "real" images](https://arxiv.org/abs/2509.21864)
*Janis Keuper,Margret Keuper*

Main category: cs.CV

TL;DR: This paper questions the current methods for detecting 'fake' images, emphasizing the need to redefine 'real' images and develop new benchmarks.


<details>
  <summary>Details</summary>
Motivation: Concerns about the misuse of image generation technology have led to efforts in detecting 'fake' images, but the authors believe the definition of 'real' images is outdated and overlooked.

Method: The paper analyzes the reliance on old, low-resolution datasets for 'real' images and critiques the evolution of modern photography techniques.

Result: It highlights that 'real' images, often created by neural network-based algorithms in smartphones, blur the distinction between 'fake' and 'real.'

Conclusion: The paper argues for updated benchmarks and a redefinition of 'real' images, and questions the validity of current 'fake' image detection objectives.

Abstract: The wide availability and low usability barrier of modern image generation
models has triggered the reasonable fear of criminal misconduct and negative
social implications. The machine learning community has been engaging this
problem with an extensive series of publications proposing algorithmic
solutions for the detection of "fake", e.g. entirely generated or partially
manipulated images. While there is undoubtedly some progress towards technical
solutions of the problem, we argue that current and prior work is focusing too
much on generative algorithms and "fake" data-samples, neglecting a clear
definition and data collection of "real" images. The fundamental question "what
is a real image?" might appear to be quite philosophical, but our analysis
shows that the development and evaluation of basically all current
"fake"-detection methods is relying on only a few, quite old low-resolution
datasets of "real" images like ImageNet. However, the technology for the
acquisition of "real" images, aka taking photos, has drastically evolved over
the last decade: Today, over 90% of all photographs are produced by smartphones
which typically use algorithms to compute an image from multiple inputs (over
time) from multiple sensors. Based on the fact that these image formation
algorithms are typically neural network architectures which are closely related
to "fake"-image generators, we state the position that today, we need to
re-think the concept of "real" images. The purpose of this position paper is to
raise the awareness of the current shortcomings in this active field of
research and to trigger an open discussion whether the detection of "fake"
images is a sound objective at all. At the very least, we need a clear
technical definition of "real" images and new benchmark datasets.

</details>


### [234] [Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization](https://arxiv.org/abs/2509.21871)
*Boyang Liu,Yifan Hu,Senjie Jin,Shihan Dou,Gonglei Shi,Jie Shao,Tao Gui,Xuanjing Huang*

Main category: cs.CV

TL;DR: Aes-R1 is a framework for improving image aesthetic assessment performance in multimodal large language models (MLLMs) using reinforcement learning techniques and structured explanations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges faced by MLLMs in accurate image aesthetic assessment due to limited high-quality aesthetic reasoning data and the subjective nature of aesthetic judgments.

Method: The Aes-R1 framework integrates the AesCoT pipeline to produce aesthetic reasoning data and employs RAPO, a novel reinforcement learning algorithm, to enhance accuracy in scoring and relative ranking.

Result: Aes-R1 achieves significant improvements in aesthetic assessment benchmarks, increasing the backbone's PLCC and SRCC metrics by 47.9% and 34.8%, respectively.

Conclusion: The research demonstrates Aes-R1's effectiveness in providing interpretable aesthetic judgments with higher accuracy, robustness, and generalization capabilities, surpassing state-of-the-art methods.

Abstract: Multimodal large language models (MLLMs) are well suited to image aesthetic
assessment, as they can capture high-level aesthetic features leveraging their
cross-modal understanding capacity. However, the scarcity of multimodal
aesthetic reasoning data and the inherently subjective nature of aesthetic
judgment make it difficult for MLLMs to generate accurate aesthetic judgments
with interpretable rationales. To this end, we propose Aes-R1, a comprehensive
aesthetic reasoning framework with reinforcement learning (RL). Concretely,
Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality
chain-of-thought aesthetic reasoning data used for cold-start. After teaching
the model to generate structured explanations prior to scoring, we then employ
the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that
jointly optimizes absolute score regression and relative ranking order,
improving both per-image accuracy and cross-image preference judgments. Aes-R1
enables MLLMs to generate grounded explanations alongside faithful scores,
thereby enhancing aesthetic scoring and reasoning in a unified framework.
Extensive experiments demonstrate that Aes-R1 improves the backbone's average
PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar
size. More ablation studies validate Aes-R1's robust generalization under
limited supervision and in out-of-distribution scenarios.

</details>


### [235] [StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing](https://arxiv.org/abs/2509.21887)
*Liyang Chen,Tianze Zhou,Xu He,Boshi Tang,Zhiyong Wu,Yang Huang,Yang Wu,Zhongqian Sun,Wei Yang,Helen Meng*

Main category: cs.CV

TL;DR: The paper introduces StableDub, a framework for generating lip-synced visual dubbing with enhancements like speaker-specific lip habits and occlusion handling.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address two issues: (1) existing methods fail to capture speaker-specific lip habits, and (2) visual artifacts appear in occluded situations.

Method: StableDub introduces a lip-habit-modulated mechanism for better phonemic sync and speaker-specific dynamics, an occlusion-aware training strategy, and a hybrid Mamba-Transformer architecture for improved efficiency.

Result: StableDub achieves superior lip habit resemblance, occlusion robustness, audio-lip synchronization, and video quality compared to other methods.

Conclusion: The framework broadens the adaptability and efficiency of visual dubbing systems, particularly in low-resource settings, which is validated through experimental results and demos.

Abstract: The visual dubbing task aims to generate mouth movements synchronized with
the driving audio, which has seen significant progress in recent years.
However, two critical deficiencies hinder their wide application: (1)
Audio-only driving paradigms inadequately capture speaker-specific lip habits,
which fail to generate lip movements similar to the target avatar; (2)
Conventional blind-inpainting approaches frequently produce visual artifacts
when handling obstructions (e.g., microphones, hands), limiting practical
deployment. In this paper, we propose StableDub, a novel and concise framework
integrating lip-habit-aware modeling with occlusion-robust synthesis.
Specifically, building upon the Stable-Diffusion backbone, we develop a
lip-habit-modulated mechanism that jointly models phonemic audio-visual
synchronization and speaker-specific orofacial dynamics. To achieve plausible
lip geometries and object appearances under occlusion, we introduce the
occlusion-aware training strategy by explicitly exposing the occlusion objects
to the inpainting process. By incorporating the proposed designs, the model
eliminates the necessity for cost-intensive priors in previous methods, thereby
exhibiting superior training efficiency on the computationally intensive
diffusion-based backbone. To further optimize training efficiency from the
perspective of model architecture, we introduce a hybrid Mamba-Transformer
architecture, which demonstrates the enhanced applicability in low-resource
research scenarios. Extensive experimental results demonstrate that StableDub
achieves superior performance in lip habit resemblance and occlusion
robustness. Our method also surpasses other methods in audio-lip sync, video
quality, and resolution consistency. We expand the applicability of visual
dubbing methods from comprehensive aspects, and demo videos can be found at
https://stabledub.github.io.

</details>


### [236] [Drag4D: Align Your Motion with Text-Driven 3D Scene Generation](https://arxiv.org/abs/2509.21888)
*Minjun Kang,Inkyu Shin,Taeyeop Lee,In So Kweon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: Drag4D integrates interactive object motion control into text-driven 3D scene generation, enabling users to animate objects along 3D trajectories within realistic 3D environments.


<details>
  <summary>Details</summary>
Motivation: To enable seamless integration of user-controlled object motion and realistic 3D backgrounds in text-to-3D scene generation.

Method: The pipeline consists of three stages: background generation via 2D Gaussian Splatting, 3D object extraction and integration using copy-and-paste techniques, and temporally consistent motion animation with a video diffusion model.

Result: The framework effectively aligns user-controlled object motion within visually complete 3D scenes, with stable motion and high-quality reconstructions.

Conclusion: Drag4D demonstrates the ability to harmonize object motion and 3D backgrounds, offering user-defined control and realistic rendering, as validated by staged and final evaluations.

Abstract: We introduce Drag4D, an interactive framework that integrates object motion
control within text-driven 3D scene generation. This framework enables users to
define 3D trajectories for the 3D objects generated from a single image,
seamlessly integrating them into a high-quality 3D background. Our Drag4D
pipeline consists of three stages. First, we enhance text-to-3D background
generation by applying 2D Gaussian Splatting with panoramic images and
inpainted novel views, resulting in dense and visually complete 3D
reconstructions. In the second stage, given a reference image of the target
object, we introduce a 3D copy-and-paste approach: the target instance is
extracted in a full 3D mesh using an off-the-shelf image-to-3D model and
seamlessly composited into the generated 3D scene. The object mesh is then
positioned within the 3D scene via our physics-aware object position learning,
ensuring precise spatial alignment. Lastly, the spatially aligned object is
temporally animated along a user-defined 3D trajectory. To mitigate motion
hallucination and ensure view-consistent temporal alignment, we develop a
part-augmented, motion-conditioned video diffusion model that processes
multiview image pairs together with their projected 2D trajectories. We
demonstrate the effectiveness of our unified architecture through evaluations
at each stage and in the final results, showcasing the harmonized alignment of
user-controlled object motion within a high-quality 3D background.

</details>


### [237] [Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers](https://arxiv.org/abs/2509.21893)
*Jibin Song,Mingi Kwon,Jaeseok Jeong,Youngjung Uh*

Main category: cs.CV

TL;DR: The paper introduces Syncphony, a model designed for generating high-quality, temporally synchronized videos using audio as input.


<details>
  <summary>Details</summary>
Motivation: The need for temporally controlled video generation aligns with the limitations of current text-to-video and image-to-video methods, which lack precision in timing. Audio, as it inherently carries temporal cues, presents a strong alternative input for synchronized video generation.

Method: Syncphony utilizes a pre-trained video backbone with two innovative components: 1) Motion-aware Loss focusing on high-motion regions, and 2) Audio Sync Guidance which leverages visual alignment techniques without replacing visual quality. A novel evaluation metric, CycleSync, is introduced to assess synchronization accuracy.

Result: Syncphony successfully generates high-resolution (380x640, 24fps) videos synchronized with diverse audio inputs. It outperforms existing methods in synchronization precision and visual quality across AVSync15 and The Greatest Hits datasets.

Conclusion: Audio is an effective modality for enabling precise temporal control in video generation. Syncphonyâ€™s approach demonstrates significant advancements in synchronization accuracy and visual appeal, offering a promising direction for audio-to-video generation systems.

Abstract: Text-to-video and image-to-video generation have made rapid progress in
visual quality, but they remain limited in controlling the precise timing of
motion. In contrast, audio provides temporal cues aligned with video motion,
making it a promising condition for temporally controlled video generation.
However, existing audio-to-video (A2V) models struggle with fine-grained
synchronization due to indirect conditioning mechanisms or limited temporal
modeling capacity. We present Syncphony, which generates 380x640 resolution,
24fps videos synchronized with diverse audio inputs. Our approach builds upon a
pre-trained video backbone and incorporates two key components to improve
synchronization: (1) Motion-aware Loss, which emphasizes learning at
high-motion regions; (2) Audio Sync Guidance, which guides the full model using
a visually aligned off-sync model without audio layers to better exploit audio
cues at inference while maintaining visual quality. To evaluate
synchronization, we propose CycleSync, a video-to-audio-based metric that
measures the amount of motion cues in the generated video to reconstruct the
original audio. Experiments on AVSync15 and The Greatest Hits datasets
demonstrate that Syncphony outperforms existing methods in both synchronization
accuracy and visual quality. Project page is available at:
https://jibin86.github.io/syncphony_project_page

</details>


### [238] [LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation](https://arxiv.org/abs/2509.21894)
*Yixiao Liu,Yizhou Yang,Jinwen Li,Jun Tao,Ruoyu Li,Xiangkun Wang,Min Zhu,Junlong Cheng*

Main category: cs.CV

TL;DR: The paper proposes LG-CD, a language-guided model, for improved remote sensing change detection using multimodal inputs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current RSCD methods overlook the semantic potential of multimodal inputs like text in favor of visual data alone.

Method: The model leverages visual foundational models, multi-layer adapters, a Text Fusion Attention Module, and a Vision-Semantic Fusion Decoder.

Result: LG-CD achieves superior performance on three datasets: LEVIR-CD, WHU-CD, and SYSU-CD, outperforming previous models.

Conclusion: The integration of visual and semantic information via multimodal data enhances the accuracy and robustness of remote sensing change detection.

Abstract: Remote Sensing Change Detection (RSCD) typically identifies changes in land
cover or surface conditions by analyzing multi-temporal images. Currently, most
deep learning-based methods primarily focus on learning unimodal visual
information, while neglecting the rich semantic information provided by
multimodal data such as text. To address this limitation, we propose a novel
Language-Guided Change Detection model (LG-CD). This model leverages natural
language prompts to direct the network's attention to regions of interest,
significantly improving the accuracy and robustness of change detection.
Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature
extractor to capture multi-scale pyramid features from high-resolution to
low-resolution across bi-temporal remote sensing images. Subsequently,
multi-layer adapters are employed to fine-tune the model for downstream tasks,
ensuring its effectiveness in remote sensing change detection. Additionally, we
design a Text Fusion Attention Module (TFAM) to align visual and textual
information, enabling the model to focus on target change regions using text
prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented,
which deeply integrates visual and semantic information through a
cross-attention mechanism to produce highly accurate change detection masks.
Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate
that LG-CD consistently outperforms state-of-the-art change detection methods.
Furthermore, our approach provides new insights into achieving generalized
change detection by leveraging multimodal information.

</details>


### [239] [TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation](https://arxiv.org/abs/2509.21905)
*Qihang Wang,Yaxiong Wang,Lechao Cheng,Zhun Zhong*

Main category: cs.CV

TL;DR: The paper presents a novel framework for image editing using both text and drag interactions to overcome the limitations of text- and drag-driven methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of text-driven methods (lacking spatial precision) and drag-driven methods (limited texture control) in image editing.

Method: A unified diffusion-based framework introducing Point-Cloud Deterministic Drag and Drag-Text Guided Denoising for balancing text and drag controls.

Result: The approach achieves high-fidelity joint editing while performing strongly in text-only, drag-only, or combined modes, surpassing specialized methods.

Conclusion: The proposed framework establishes a versatile solution for controllable image editing with text and drag interactions, demonstrating superior performance and generalizability.

Abstract: This paper explores image editing under the joint control of text and drag
interactions. While recent advances in text-driven and drag-driven editing have
achieved remarkable progress, they suffer from complementary limitations:
text-driven methods excel in texture manipulation but lack precise spatial
control, whereas drag-driven approaches primarily modify shape and structure
without fine-grained texture guidance. To address these limitations, we propose
a unified diffusion-based framework for joint drag-text image editing,
integrating the strengths of both paradigms. Our framework introduces two key
innovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space
layout control through 3D feature mapping, and (2) Drag-Text Guided Denoising,
dynamically balancing the influence of drag and text conditions during
denoising. Notably, our model supports flexible editing modes - operating with
text-only, drag-only, or combined conditions - while maintaining strong
performance in each setting. Extensive quantitative and qualitative experiments
demonstrate that our method not only achieves high-fidelity joint editing but
also matches or surpasses the performance of specialized text-only or drag-only
approaches, establishing a versatile and generalizable solution for
controllable image manipulation. Code will be made publicly available to
reproduce all results presented in this work.

</details>


### [240] [Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning](https://arxiv.org/abs/2509.21916)
*Boying Li,Chang Liu,Petter KyÃ¶sti,Mattias Ã–hman,Devashish Singha Roy,Sofia Plazzi,Hamam Mokayed,Olle Hagner*

Main category: cs.CV

TL;DR: The paper introduces a sideload-CL-adaptation framework to improve vehicle detection in UAV images from Nordic regions, leveraging unannotated data using contrastive learning and fusion strategies.


<details>
  <summary>Details</summary>
Motivation: Detecting vehicles in UAV images from Nordic regions is challenging due to snow visibility issues, domain shifts, and the scarcity of annotated data.

Method: The method involves a sideload-CL-adaptation framework which uses contrastive learning for representation extraction on unannotated data, followed by fine-tuning a frozen YOLO11n backbone using this pre-trained model.

Result: The proposed model shows an improvement of 3.8% to 9.5% in mAP50 on the NVD dataset.

Conclusion: Utilizing unannotated UAV data through the sideload-CL-adaptation framework effectively enhances vehicle detection performance in snow-covered regions.

Abstract: Aside from common challenges in remote sensing like small, sparse targets and
computation cost limitations, detecting vehicles from UAV images in the Nordic
regions faces strong visibility challenges and domain shifts caused by diverse
levels of snow coverage. Although annotated data are expensive, unannotated
data is cheaper to obtain by simply flying the drones. In this work, we
proposed a sideload-CL-adaptation framework that enables the use of unannotated
data to improve vehicle detection using lightweight models. Specifically, we
propose to train a CNN-based representation extractor through contrastive
learning on the unannotated data in the pretraining stage, and then sideload it
to a frozen YOLO11n backbone in the fine-tuning stage. To find a robust
sideload-CL-adaptation, we conducted extensive experiments to compare various
fusion methods and granularity. Our proposed sideload-CL-adaptation model
improves the detection performance by 3.8% to 9.5% in terms of mAP50 on the NVD
dataset.

</details>


### [241] [Taming Flow-based I2V Models for Creative Video Editing](https://arxiv.org/abs/2509.21917)
*Xianghao Kong,Hansheng Chen,Yuwei Guo,Lvmin Zhang,Gordon Wetzstein,Maneesh Agrawala,Anyi Rao*

Main category: cs.CV

TL;DR: This paper introduces IF-V2V, an inversion-free method for video editing utilizing image-to-video models without computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods are limited by inversion requirements or extensive optimization, restricting the use of up-to-date models effectively.

Method: The method uses Vector Field Rectification, Structure-and-Motion-Preserving Initialization, and Deviation Caching to adapt flow-matching-based I2V models for video editing.

Result: IF-V2V achieves superior editing quality and consistency compared to current methods.

Conclusion: The proposed strategy is lightweight, efficient, and seamlessly integrates with existing systems to enhance video editing capabilities.

Abstract: Although image editing techniques have advanced significantly, video editing,
which aims to manipulate videos according to user intent, remains an emerging
challenge. Most existing image-conditioned video editing methods either require
inversion with model-specific design or need extensive optimization, limiting
their capability of leveraging up-to-date image-to-video (I2V) models to
transfer the editing capability of image editing models to the video domain. To
this end, we propose IF-V2V, an Inversion-Free method that can adapt
off-the-shelf flow-matching-based I2V models for video editing without
significant computational overhead. To circumvent inversion, we devise Vector
Field Rectification with Sample Deviation to incorporate information from the
source video into the denoising process by introducing a deviation term into
the denoising vector field. To further ensure consistency with the source video
in a model-agnostic way, we introduce Structure-and-Motion-Preserving
Initialization to generate motion-aware temporally correlated noise with
structural information embedded. We also present a Deviation Caching mechanism
to minimize the additional computational cost for denoising vector
rectification without significantly impacting editing quality. Evaluations
demonstrate that our method achieves superior editing quality and consistency
over existing approaches, offering a lightweight plug-and-play solution to
realize visual creativity.

</details>


### [242] [Multi-View Crowd Counting With Self-Supervised Learning](https://arxiv.org/abs/2509.21918)
*Hong Mo,Xiong Zhang,Tengfei Shi,Zhongbo Wu*

Main category: cs.CV

TL;DR: The paper introduces SSLCounter, a self-supervised multi-view counting (MVC) framework that leverages neural volumetric rendering for improved data efficiency, achieving competitive performance with less annotated data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the heavy reliance of current MVC methods on fully supervised learning (FSL) approaches, which require large amounts of annotated data.

Method: SSLCounter employs self-supervised learning and neural volumetric rendering to reconstruct continuous 3D scene geometry and its view-dependent 2D appearances, aiming to reduce the need for extensive annotated datasets.

Result: SSLCounter achieves state-of-the-art performance and demonstrates competitive results while using only 70% of the training data compared to traditional FSL methods.

Conclusion: The proposed framework offers an efficient and flexible method for MVC tasks, reducing the dependency on large annotated datasets, and is seamlessly integrable into existing systems.

Abstract: Multi-view counting (MVC) methods have attracted significant research
attention and stimulated remarkable progress in recent years. Despite their
success, most MVC methods have focused on improving performance by following
the fully supervised learning (FSL) paradigm, which often requires large
amounts of annotated data. In this work, we propose SSLCounter, a novel
self-supervised learning (SSL) framework for MVC that leverages neural
volumetric rendering to alleviate the reliance on large-scale annotated
datasets. SSLCounter learns an implicit representation w.r.t. the scene,
enabling the reconstruction of continuous geometry shape and the complex,
view-dependent appearance of their 2D projections via differential neural
rendering. Owing to its inherent flexibility, the key idea of our method can be
seamlessly integrated into exsiting frameworks. Notably, extensive experiments
demonstrate that SSLCounter not only demonstrates state-of-the-art performances
but also delivers competitive performance with only using 70% proportion of
training data, showcasing its superior data efficiency across multiple MVC
benchmarks.

</details>


### [243] [Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding](https://arxiv.org/abs/2509.21922)
*Vahid Mirjalili,Ramin Giahi,Sriram Kollipara,Akshay Kekuda,Kehui Yao,Kai Zhao,Jianpeng Xu,Kaushiki Nag,Sinduja Subramaniam,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.CV

TL;DR: The paper presents a benchmark for evaluating object-centric spatial reasoning in vision foundation models using a synthetic dataset. It highlights a gap between localization accuracy and true spatial understanding.


<details>
  <summary>Details</summary>
Motivation: The need for assessing whether vision models and vision-language models (VLMs) capture object arrangements, positions, and relational reasoning within a scene beyond simple localization accuracy.

Method: The authors developed a systematic benchmark for spatial reasoning, utilizing a controlled synthetic dataset and testing state-of-the-art vision models and VLMs across tasks like spatial localization, reasoning, and downstream retrieval.

Result: Detectors like GroundingDINO and OWLv2 excel in localization but lack relational reasoning, while VLMs such as SmolVLM and GPT-4o handle relational reasoning but have less precise spatial localization.

Conclusion: The paper identifies a trade-off in current models: they either excel in accuracy or relational reasoning but not both, emphasizing the need for more spatially-aware models.

Abstract: Spatial understanding is a critical capability for vision foundation models.
While recent advances in large vision models or vision-language models (VLMs)
have expanded recognition capabilities, most benchmarks emphasize localization
accuracy rather than whether models capture how objects are arranged and
related within a scene. This gap is consequential; effective scene
understanding requires not only identifying objects, but reasoning about their
relative positions, groupings, and depth. In this paper, we present a
systematic benchmark for object-centric spatial reasoning in foundation models.
Using a controlled synthetic dataset, we evaluate state-of-the-art vision
models (e.g., GroundingDINO, Florence-2, OWLv2) and large VLMs (e.g., InternVL,
LLaVA, GPT-4o) across three tasks: spatial localization, spatial reasoning, and
downstream retrieval tasks. We find a stable trade-off: detectors such as
GroundingDINO and OWLv2 deliver precise boxes with limited relational
reasoning, while VLMs like SmolVLM and GPT-4o provide coarse layout cues and
fluent captions but struggle with fine-grained spatial context. Our study
highlights the gap between localization and true spatial understanding, and
pointing toward the need for spatially-aware foundation models in the
community.

</details>


### [244] [PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning](https://arxiv.org/abs/2509.21926)
*Jiahao Zhang,Bowen Wang,Hong Liu,Yuta Nakashima,Hajime Nagahara*

Main category: cs.CV

TL;DR: PANICL proposes a framework to improve Visual In-Context Learning (VICL) by reducing biases caused by reliance on a single in-context pair, enhancing task performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing VICL methods often face challenges like over-reliance on single in-context pairs, leading to biased and unstable predictions across various vision tasks.

Method: The paper introduces PANICL, a training-free framework that leverages multiple in-context pairs to smooth assignment scores, thereby mitigating biases without additional training demands.

Result: PANICL consistently improves performance across diverse vision tasks, including segmentation, detection, and colorization, and demonstrates robustness to domain and label-space shifts.

Conclusion: PANICL enhances VICL by addressing its limitations, showing broader task applicability, improved generalization, and robustness to various shifts.

Abstract: Visual In-Context Learning (VICL) uses input-output image pairs, referred to
as in-context pairs (or examples), as prompts alongside query images to guide
models in performing diverse vision tasks. However, VICL often suffers from
over-reliance on a single in-context pair, which can lead to biased and
unstable predictions. We introduce PAtch-based $k$-Nearest neighbor visual
In-Context Learning (PANICL), a general training-free framework that mitigates
this issue by leveraging multiple in-context pairs. PANICL smooths assignment
scores across pairs, reducing bias without requiring additional training.
Extensive experiments on a variety of tasks, including foreground segmentation,
single object detection, colorization, multi-object segmentation, and keypoint
detection, demonstrate consistent improvements over strong baselines. Moreover,
PANICL exhibits strong robustness to domain shifts, including dataset-level
shift (e.g., from COCO to Pascal) and label-space shift (e.g., FSS-1000), and
generalizes well to other VICL models such as SegGPT, Painter, and LVM,
highlighting its versatility and broad applicability.

</details>


### [245] [SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference](https://arxiv.org/abs/2509.21927)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: The paper introduces SingRef6D, a lightweight 6D pose estimation pipeline using only a single RGB image, overcoming depth sensor and RGB-based limitations while achieving performance improvements in pose estimation.


<details>
  <summary>Details</summary>
Motivation: The need to overcome limitations of existing 6D pose estimation methods, such as dependence on depth sensors and poor performance of RGB-based approaches in challenging conditions.

Method: SingRef6D leverages a token-scaler fine-tuning mechanism with depth prediction enhancement and integrates depth-aware matching into spatial relationships.

Result: Improved depth prediction accuracy (14.41% on REAL275) and enhanced pose estimation performance (6.1% improvement in average recall across datasets).

Conclusion: SingRef6D effectively addresses practical limitations of 6D estimation methods, demonstrating robustness in resource-limited and challenging settings.

Abstract: Recent 6D pose estimation methods demonstrate notable performance but still
face some practical limitations. For instance, many of them rely heavily on
sensor depth, which may fail with challenging surface conditions, such as
transparent or highly reflective materials. In the meantime, RGB-based
solutions provide less robust matching performance in low-light and
texture-less scenes due to the lack of geometry information. Motivated by
these, we propose SingRef6D, a lightweight pipeline requiring only a single RGB
image as a reference, eliminating the need for costly depth sensors, multi-view
image acquisition, or training view synthesis models and neural fields. This
enables SingRef6D to remain robust and capable even under resource-limited
settings where depth or dense templates are unavailable. Our framework
incorporates two key innovations. First, we propose a token-scaler-based
fine-tuning mechanism with a novel optimization loss on top of Depth-Anything
v2 to enhance its ability to predict accurate depth, even for challenging
surfaces. Our results show a 14.41% improvement (in $\delta_{1.05}$) on REAL275
depth prediction compared to Depth-Anything v2 (with fine-tuned head). Second,
benefiting from depth availability, we introduce a depth-aware matching process
that effectively integrates spatial relationships within LoFTR, enabling our
system to handle matching for challenging materials and lighting conditions.
Evaluations of pose estimation on the REAL275, ClearPose, and Toyota-Light
datasets show that our approach surpasses state-of-the-art methods, achieving a
6.1% improvement in average recall.

</details>


### [246] [SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet](https://arxiv.org/abs/2509.21938)
*Woosung Joung,Daewon Chae,Jinkyu Kim*

Main category: cs.CV

TL;DR: ControlNet enhances detail control in text-to-image models but struggles with misaligned visual conditions. SemanticControl addresses this by adaptively refining guidance, achieving better alignment and visual quality.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of ControlNet, which requires precisely aligned visual conditions, an impractical expectation for uncommon or imaginative text prompts.

Method: SemanticControl adaptively adjusts influence from visual conditions by using a surrogate-denoising process to create informative attention masks for better alignment with text prompts.

Result: Experimental results show improved generation performance across different visual conditions (such as depth maps and skeletons) under loosely aligned scenarios, outperforming previous models.

Conclusion: SemanticControl provides a training-free solution that significantly improves visual fidelity and text alignment when generation conditions are not perfectly aligned. Available for further implementation via shared code.

Abstract: ControlNet has enabled detailed spatial control in text-to-image diffusion
models by incorporating additional visual conditions such as depth or edge
maps. However, its effectiveness heavily depends on the availability of visual
conditions that are precisely aligned with the generation goal specified by
text prompt-a requirement that often fails in practice, especially for uncommon
or imaginative scenes. For example, generating an image of a cat cooking in a
specific pose may be infeasible due to the lack of suitable visual conditions.
In contrast, structurally similar cues can often be found in more common
settings-for instance, poses of humans cooking are widely available and can
serve as rough visual guides. Unfortunately, existing ControlNet models
struggle to use such loosely aligned visual conditions, often resulting in low
text fidelity or visual artifacts. To address this limitation, we propose
SemanticControl, a training-free method for effectively leveraging misaligned
but semantically relevant visual conditions. Our approach adaptively suppresses
the influence of the visual condition where it conflicts with the prompt, while
strengthening guidance from the text. The key idea is to first run an auxiliary
denoising process using a surrogate prompt aligned with the visual condition
(e.g., "a human playing guitar" for a human pose condition) to extract
informative attention masks, and then utilize these masks during the denoising
of the actual target prompt (e.g., cat playing guitar). Experimental results
demonstrate that our method improves performance under loosely aligned
conditions across various conditions, including depth maps, edge maps, and
human skeletons, outperforming existing baselines. Our code is available at
https://mung3477.github.io/semantic-control.

</details>


### [247] [Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach](https://arxiv.org/abs/2509.21950)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Main category: cs.CV

TL;DR: The paper explores the ability of Multimodal Large Language Models (MLLMs) to understand emotions in images and proposes a new task and method to address evaluation challenges.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistent evaluation and limited understanding of MLLMs' ability to perceive emotions in images.

Method: A new Emotion Statement Judgment task is introduced, along with an automated pipeline for creating emotion-centric statements. The study systematically evaluates MLLMs on this task.

Result: MLLMs show stronger performance in emotion interpretation and context-based judgments but struggle with perception subjectivity. They also lag behind human-level emotional understanding.

Conclusion: This foundational evaluation framework highlights limitations and gaps in MLLMs' emotional intelligence, providing directions for future improvements.

Abstract: Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional
performance across diverse tasks, continually surpassing previous expectations
regarding their capabilities. Nevertheless, their proficiency in perceiving
emotions from images remains debated, with studies yielding divergent results
in zero-shot scenarios. We argue that this inconsistency stems partly from
constraints in existing evaluation methods, including the oversight of
plausible responses, limited emotional taxonomies, neglect of contextual
factors, and labor-intensive annotations. To facilitate customized visual
emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task
that overcomes these constraints. Complementing this task, we devise an
automated pipeline that efficiently constructs emotion-centric statements with
minimal human effort. Through systematically evaluating prevailing MLLMs, our
study showcases their stronger performance in emotion interpretation and
context-based emotion judgment, while revealing relative limitations in
comprehending perception subjectivity. When compared to humans, even
top-performing MLLMs like GPT4o demonstrate remarkable performance gaps,
underscoring key areas for future improvement. By developing a fundamental
evaluation framework and conducting a comprehensive MLLM assessment, we hope
this work contributes to advancing emotional intelligence in MLLMs. Project
page: https://github.com/wdqqdw/MVEI.

</details>


### [248] [MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning](https://arxiv.org/abs/2509.21953)
*Tao Wu,Yibo Jiang,Yehao Lu,Zhizhong Wang,Zeyi Huang,Zequn Qin,Xi Li*

Main category: cs.CV

TL;DR: The paper proposes MultiCrafter, a framework for generating multi-subject images that addresses attribute leakage and improves alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-subject image generation suffer from attribute leakage and fail to align with human preferences adequately.

Method: The paper introduces positional supervision to reduce inter-subject attention entanglement, a Mixture-of-Experts (MoE) architecture for better adaptability, and an online reinforcement learning framework tailored to the MoE setup for preference alignment.

Result: Experiments demonstrate that MultiCrafter significantly enhances subject fidelity and aligns better with human preferences compared to existing methods.

Conclusion: MultiCrafter offers a robust solution for high-fidelity and preference-aligned multi-subject image generation by overcoming key limitations of previous approaches.

Abstract: Multi-subject image generation aims to synthesize user-provided subjects in a
single image while preserving subject fidelity, ensuring prompt consistency,
and aligning with human aesthetic preferences. However, existing methods,
particularly those built on the In-Context-Learning paradigm, are limited by
their reliance on simple reconstruction-based objectives, leading to both
severe attribute leakage that compromises subject fidelity and failing to align
with nuanced human preferences. To address this, we propose MultiCrafter, a
framework that ensures high-fidelity, preference-aligned generation. First, we
find that the root cause of attribute leakage is a significant entanglement of
attention between different subjects during the generation process. Therefore,
we introduce explicit positional supervision to explicitly separate attention
regions for each subject, effectively mitigating attribute leakage. To enable
the model to accurately plan the attention region of different subjects in
diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the
model's capacity, allowing different experts to focus on different scenarios.
Finally, we design a novel online reinforcement learning framework to align the
model with human preferences, featuring a scoring mechanism to accurately
assess multi-subject fidelity and a more stable training strategy tailored for
the MoE architecture. Experiments validate that our framework significantly
improves subject fidelity while aligning with human preferences better.

</details>


### [249] [PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data](https://arxiv.org/abs/2509.21965)
*Zhe Zhu,Le Wan,Rui Xu,Yiheng Zhang,Honghua Chen,Zhiyang Dou,Cheng Lin,Yuan Liu,Mingqiang Wei*

Main category: cs.CV

TL;DR: PartSAM is a novel 3D part segmentation model trained directly on large-scale 3D data, overcoming limitations of prior 2D-based methods.


<details>
  <summary>Details</summary>
Motivation: 3D object segmentation faces challenges in generalization and reliance on 2D models, leading to limitations in understanding intrinsic geometry.

Method: PartSAM utilizes a triplane-based dual-branch encoder-decoder framework and introduces a large-scale model-in-the-loop annotation pipeline for diverse 3D shape-part labels.

Result: PartSAM demonstrates superior performance and emergent open-world capabilities like accurate part identification and decomposition of both surface and internal structures.

Conclusion: PartSAM sets a new benchmark in 3D part understanding, offering scalability, generalization, and superior functionality. Its code and model will be publicly available.

Abstract: Segmenting 3D objects into parts is a long-standing challenge in computer
vision. To overcome taxonomy constraints and generalize to unseen 3D objects,
recent works turn to open-world part segmentation. These approaches typically
transfer supervision from 2D foundation models, such as SAM, by lifting
multi-view masks into 3D. However, this indirect paradigm fails to capture
intrinsic geometry, leading to surface-only understanding, uncontrolled
decomposition, and limited generalization. We present PartSAM, the first
promptable part segmentation model trained natively on large-scale 3D data.
Following the design philosophy of SAM, PartSAM employs an encoder-decoder
architecture in which a triplane-based dual-branch encoder produces spatially
structured tokens for scalable part-aware representation learning. To enable
large-scale supervision, we further introduce a model-in-the-loop annotation
pipeline that curates over five million 3D shape-part pairs from online assets,
providing diverse and fine-grained labels. This combination of scalable
architecture and diverse 3D data yields emergent open-world capabilities: with
a single prompt, PartSAM achieves highly accurate part identification, and in a
Segment-Every-Part mode, it automatically decomposes shapes into both surface
and internal structures. Extensive experiments show that PartSAM outperforms
state-of-the-art methods by large margins across multiple benchmarks, marking a
decisive step toward foundation models for 3D part understanding. Our code and
model will be released soon.

</details>


### [250] [No-Reference Image Contrast Assessment with Customized EfficientNet-B0](https://arxiv.org/abs/2509.21967)
*Javad Hassannataj Joloudari,Bita Mesbahzadeh,Omid Zare,Emrah Arslan,Roohallah Alizadehsani,Hossein Moosaei*

Main category: cs.CV

TL;DR: The study focuses on enhancing no-reference image quality assessment (NR IQA) for contrast distortions using deep learning models like EfficientNet B0, ResNet18, and MobileNetV2, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing NR IQA models in assessing contrast distortions under diverse real-world scenarios.

Method: Customizing and fine-tuning three pre-trained architectures (EfficientNet B0, ResNet18, and MobileNetV2) to predict perceptual Mean Opinion Scores, and incorporating a contrast-aware regression head. Models were trained on two benchmark datasets using targeted data augmentations, with evaluation metrics such as PLCC and SRCC.

Result: EfficientNet B0 achieved the highest performance, with PLCC = 0.9286 and SRCC = 0.9178 on CCID2014, and PLCC = 0.9581 and SRCC = 0.9369 on CID2013, surpassing traditional and other deep learning methods.

Conclusion: The proposed contrast-aware adaptation of lightweight pre-trained networks is robust, scalable, and suitable for real-time applications, effectively addressing perceptual contrast distortions.

Abstract: Image contrast was a fundamental factor in visual perception and played a
vital role in overall image quality. However, most no reference image quality
assessment NR IQA models struggled to accurately evaluate contrast distortions
under diverse real world conditions. In this study, we proposed a deep learning
based framework for blind contrast quality assessment by customizing and
fine-tuning three pre trained architectures, EfficientNet B0, ResNet18, and
MobileNetV2, for perceptual Mean Opinion Score, along with an additional model
built on a Siamese network, which indicated a limited ability to capture
perceptual contrast distortions. Each model is modified with a contrast-aware
regression head and trained end to end using targeted data augmentations on two
benchmark datasets, CID2013 and CCID2014, containing synthetic and authentic
contrast distortions. Performance is evaluated using Pearson Linear Correlation
Coefficient and Spearman Rank Order Correlation Coefficient, which assess the
alignment between predicted and human rated scores. Among these three models,
our customized EfficientNet B0 model achieved state-of-the-art performance with
PLCC = 0.9286 and SRCC = 0.9178 on CCID2014 and PLCC = 0.9581 and SRCC = 0.9369
on CID2013, surpassing traditional methods and outperforming other deep
baselines. These results highlighted the models robustness and effectiveness in
capturing perceptual contrast distortion. Overall, the proposed method
demonstrated that contrast aware adaptation of lightweight pre trained networks
can yield a high performing, scalable solution for no reference contrast
quality assessment suitable for real time and resource constrained
applications.

</details>


### [251] [Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning](https://arxiv.org/abs/2509.21976)
*Zilun Zhang,Zian Guan,Tiancheng Zhao,Haozhan Shen,Tianyu Li,Yuxiang Cai,Zhonggen Su,Zhaojun Liu,Jianwei Yin,Xiang Li*

Main category: cs.CV

TL;DR: The paper introduces Geo-R1, a reinforcement fine-tuning approach for geospatial referring tasks, focusing on reasoning first, then acting, to enhance generalization in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: Address performance and generalization challenges in geospatial referring expression tasks in contexts with limited labeled data.

Method: The authors propose Geo-R1, a paradigm where the model generates interpretable reasoning chains to decompose referring expressions and then uses these rationales to locate objects.

Result: Geo-R1 outperformed supervised fine-tuning baselines in few-shot benchmarks, demonstrated strong cross-dataset generalization, and provided interpretability in results.

Conclusion: Geo-R1 improves the use of limited annotations, boosts generalization, and offers interpretability, making it robust for few-shot geospatial referring tasks.

Abstract: Referring expression understanding in remote sensing poses unique challenges,
as it requires reasoning over complex object-context relationships. While
supervised fine-tuning (SFT) on multimodal large language models achieves
strong performance with massive labeled datasets, they struggle in data-scarce
scenarios, leading to poor generalization. To address this limitation, we
propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm
for few-shot geospatial referring. Geo-R1 enforces the model to first generate
explicit, interpretable reasoning chains that decompose referring expressions,
and then leverage these rationales to localize target objects. This "reason
first, then act" process enables the model to make more effective use of
limited annotations, enhances generalization, and provides interpretability. We
validate Geo-R1 on three carefully designed few-shot geospatial referring
benchmarks, where our model consistently and substantially outperforms SFT
baselines. It also demonstrates strong cross-dataset generalization,
highlighting its robustness. Code and data will be released at
http://geo-r1.github.io.

</details>


### [252] [Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models](https://arxiv.org/abs/2509.21979)
*Zikun Guo,Xinyue Xu,Pei Xiang,Shu Yang,Xin Han,Di Wang,Lijie Hu*

Main category: cs.CV

TL;DR: The paper discusses the issue of clinical sycophancy in vision language models, evaluates it with a novel benchmark, and introduces the VIPER framework for mitigating this behavior.


<details>
  <summary>Details</summary>
Motivation: To address the sycophantic behavior exhibited by vision language models (VLMs), which prioritize user cues over evidence-based reasoning in medical workflows.

Method: The study introduces a medical sycophancy dataset built from existing datasets and uses psychologically motivated templates to analyze sycophancy in models through adversarial experiments. It then proposes VIPER, a strategy to generate evidence-first answers by filtering non-evidentiary content.

Result: VLMs were found vulnerable to adversarial responses with biases independent of visual evidence. VIPER reduced sycophancy significantly while maintaining interpretability.

Conclusion: VIPER provides a robust solution to mitigate sycophantic behavior in medical VLMs, emphasizing evidence-based responses for effective real-world deployment.

Abstract: Vision language models(VLMs) are increasingly integrated into clinical
workflows, but they often exhibit sycophantic behavior prioritizing alignment
with user phrasing social cues or perceived authority over evidence based
reasoning. This study evaluate clinical sycophancy in medical visual question
answering through a novel clinically grounded benchmark. We propose a medical
sycophancy dataset construct from PathVQA, SLAKE, and VQA-RAD stratified by
different type organ system and modality. Using psychologically motivated
pressure templates including various sycophancy. In our adversarial experiments
on various VLMs, we found that these models are generally vulnerable,
exhibiting significant variations in the occurrence of adversarial responses,
with weak correlations to the model accuracy or size. Imitation and expert
provided corrections were found to be the most effective triggers, suggesting
that the models possess a bias mechanism independent of visual evidence. To
address this, we propose Visual Information Purification for Evidence based
Response (VIPER) a lightweight mitigation strategy that filters non evidentiary
content for example social pressures and then generates constrained evidence
first answers. This framework reduces sycophancy by an average amount
outperforming baselines while maintaining interpretability. Our benchmark
analysis and mitigation framework lay the groundwork for robust deployment of
medical VLMs in real world clinician interactions emphasizing the need for
evidence anchored defenses.

</details>


### [253] [Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm](https://arxiv.org/abs/2509.21980)
*Zeyu Wang,Baiyu Chen,Kun Yan,Hongjing Piao,Hao Xue,Flora D. Salim,Yuanchun Shi,Yuntao Wang*

Main category: cs.CV

TL;DR: GLARIFY enhances Vision-Language Models (VLMs) by using spatiotemporal gaze data, addressing challenges in gaze noise and ambiguous user queries.


<details>
  <summary>Details</summary>
Motivation: To improve the usability of Vision-Language Models (VLMs) for smart glasses by addressing gaze data noise and ambiguity in human attention during multi-modal querying.

Method: The paper introduces GLARIFY, which synthesizes a dataset (GLARIFY-Ambi) with GPT-4o and its chain-of-thought (CoT) process, integrates a heatmap module for gaze information, and evaluates the model using gaze-enhanced visual input.

Result: GLARIFY significantly outperforms baseline models in aligning VLMs with human attention, proving its effectiveness in real-world scenarios.

Conclusion: GLARIFY introduces a robust and intuitive way to incorporate noisy gaze patterns into VLMs, improving their alignment with human attention and paving the way for more effective visual assistants.

Abstract: With the rise in popularity of smart glasses, users' attention has been
integrated into Vision-Language Models (VLMs) to streamline multi-modal
querying in daily scenarios. However, leveraging gaze data to model users'
attention may introduce ambiguity challenges: (1) users' verbal questions
become ambiguous by using pronouns or skipping context, (2) humans' gaze
patterns can be noisy and exhibit complex spatiotemporal relationships with
their spoken questions. Previous works only consider single image as visual
modality input, failing to capture the dynamic nature of the user's attention.
In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal
gaze information to enhance the model's effectiveness in real-world
applications. Initially, we analyzed hundreds of querying samples with the gaze
modality to demonstrate the noisy nature of users' gaze patterns. We then
utilized GPT-4o to design an automatic data synthesis pipeline to generate the
GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process
to handle noisy gaze patterns. Finally, we designed a heatmap module to
incorporate gaze information into cutting-edge VLMs while preserving their
pretrained knowledge. We evaluated GLARIFY using a hold-out test set.
Experiments demonstrate that GLARIFY significantly outperforms baselines. By
robustly aligning VLMs with human attention, GLARIFY paves the way for a usable
and intuitive interaction paradigm with a visual assistant.

</details>


### [254] [From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs](https://arxiv.org/abs/2509.21984)
*Yingjie Zhu,Xuefeng Bai,Kehai Chen,Yang Xiang,Weili Guan,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: The paper identifies spatial bias in Large Vision-Language Models (LVLMs) and proposes a solution called Balanced Position Assignment (BaPA) to promote spatial robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the insufficient understanding of spatial robustness in LVLMs, specifically their inconsistent outputs when key visual information is shifted to different positions.

Method: The authors designed a probing dataset to evaluate spatial bias, analyzed position embedding strategies, and introduced BaPA, which assigns identical position embeddings to all image tokens to balance cross-modal interaction.

Result: BaPA improves spatial robustness of LVLMs, enhances performance across diverse multimodal benchmarks, and yields balanced attention flow for better visual understanding.

Conclusion: BaPA is a simple yet effective mechanism that significantly strengthens LVLMsâ€™ spatial robustness without retraining, offering a solution to their spatial-semantic limitations.

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success across
a wide range of multimodal tasks, yet their robustness to spatial variations
remains insufficiently understood. In this work, we present a systematic study
of the spatial bias of LVLMs, focusing on how models respond when identical key
visual information is placed at different locations within an image. Through a
carefully designed probing dataset, we demonstrate that current LVLMs often
produce inconsistent outputs under such spatial shifts, revealing a fundamental
limitation in their spatial-semantic understanding. Further analysis shows that
this phenomenon originates not from the vision encoder, which reliably
perceives and interprets visual content across positions, but from the
unbalanced design of position embeddings in the language model component. In
particular, the widely adopted position embedding strategies, such as RoPE,
introduce imbalance during cross-modal interaction, leading image tokens at
different positions to exert unequal influence on semantic understanding. To
mitigate this issue, we introduce Balanced Position Assignment (BaPA), a simple
yet effective mechanism that assigns identical position embeddings to all image
tokens, promoting a more balanced integration of visual information. Extensive
experiments show that BaPA enhances the spatial robustness of LVLMs without
retraining and further boosts their performance across diverse multimodal
benchmarks when combined with lightweight fine-tuning. Further analysis of
information flow reveals that BaPA yields balanced attention, enabling more
holistic visual understanding.

</details>


### [255] [Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation](https://arxiv.org/abs/2509.21989)
*Abdelrahman Eldesokey,Aleksandar Cvejic,Bernard Ghanem,Peter Wonka*

Main category: cs.CV

TL;DR: The researchers present a method for disentangling visual and semantic features from pretrained diffusion models, enabling evaluation and localization of inconsistencies in subject-driven image generation.


<details>
  <summary>Details</summary>
Motivation: Disentangling visual and semantic features has been challenging due to the lack of annotated datasets, and yet it is critical for understanding and improving subject-driven image generation.

Method: A novel automated pipeline constructs annotated image pairs with semantic and visual correspondences, coupled with a contrastive architecture to separate visual and semantic features. A new metric, Visual Semantic Matching (VSM), is introduced for quantifying inconsistencies.

Result: The proposed method outperforms existing global feature-based metrics (CLIP, DINO, vision-language models) in identifying and localizing visual inconsistencies in subject-driven generation.

Conclusion: This approach offers the first combined solution for both quantification and localization of visual inconsistencies, advancing the task of evaluating subject-driven image generation.

Abstract: We propose a novel approach for disentangling visual and semantic features
from the backbones of pre-trained diffusion models, enabling visual
correspondence in a manner analogous to the well-established semantic
correspondence. While diffusion model backbones are known to encode
semantically rich features, they must also contain visual features to support
their image synthesis capabilities. However, isolating these visual features is
challenging due to the absence of annotated datasets. To address this, we
introduce an automated pipeline that constructs image pairs with annotated
semantic and visual correspondences based on existing subject-driven image
generation datasets, and design a contrastive architecture to separate the two
feature types. Leveraging the disentangled representations, we propose a new
metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies
in subject-driven image generation. Empirical results show that our approach
outperforms global feature-based metrics such as CLIP, DINO, and
vision--language models in quantifying visual inconsistencies while also
enabling spatial localization of inconsistent regions. To our knowledge, this
is the first method that supports both quantification and localization of
inconsistencies in subject-driven generation, offering a valuable tool for
advancing this task. Project
Page:https://abdo-eldesokey.github.io/mind-the-glitch/

</details>


### [256] [WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM](https://arxiv.org/abs/2509.21990)
*Changli Tang,Qinfan Xiao,Ke Mei,Tianyi Wang,Fengyun Rao,Chao Zhang*

Main category: cs.CV

TL;DR: WAVE introduces unified representations for text, audio, and video modalities using multimodal embeddings, achieving state-of-the-art results in several tasks.


<details>
  <summary>Details</summary>
Motivation: Dynamic modal data, such as audio and video, lacks adequate representation compared to text in multimodal embeddings.

Method: WAVE uses hierarchical feature fusion combined with multi-modal, multi-task training to create embeddings tailored to user prompts.

Result: WAVE achieves state-of-the-art performance in cross-modal retrieval tasks, multimodal question answering, and MMEB-v2 video benchmarks.

Conclusion: The proposed approach successfully unifies representation across modalities and improves cross-modal applications, validated through ablation studies.

Abstract: While embeddings from multimodal large language models (LLMs) excel as
general-purpose representations, their application to dynamic modalities like
audio and video remains underexplored. We introduce WAVE (\textbf{u}nified \&
\textbf{v}ersatile \textbf{a}udio-\textbf{v}isual \textbf{e}mbeddings), the
first LLM-based embedding that creates a unified representation space for text,
audio, and video modalities. WAVE employs a novel hierarchical feature fusion
strategy and a joint multi-modal, multi-task training approach to enable two
key capabilities: any-to-any cross-modal retrieval and the generation of
prompt-aware embeddings tailored to user instructions. Experimentally, WAVE
sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves
superior results in audio and video-to-audio retrieval. Its prompt-aware nature
also yields remarkable performance in multimodal question answering,
significantly outperforming existing embedding models. Ablation studies
validate our joint training strategy, demonstrating improved performance across
all modalities. With a newly introduced benchmark for versatile audio-visual
learning, WAVE opens up broad possibilities for cross-modal, any-to-any
applications. Our code, checkpoints, and data will be released.

</details>


### [257] [ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models](https://arxiv.org/abs/2509.21991)
*Jewon Lee,Wooksu Shin,Seungmin Yang,Ki-Ung Song,DongUk Lim,Jaeyeon Kim,Tae-Ho Kim,Bo-Kyeong Kim*

Main category: cs.CV

TL;DR: The paper introduces ERGO, a method for efficient processing of high-resolution images in vision-language tasks using a coarse-to-fine reasoning approach, reducing computation while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently process high-resolution images in vision-language tasks while minimizing computational overhead.

Method: Two-stage coarse-to-fine reasoning pipeline: (1) downsampled image analysis to identify relevant regions, (2) high-resolution processing of these regions. The model uses reasoning-driven perception and a reinforcement learning framework to reduce visual ambiguity and improve region selection.

Result: ERGO achieves higher accuracy with greater efficiency compared to competitive methods, surpassing Qwen2.5-VL-7B by 4.7 points on the V* benchmark while using only 23% of vision tokens and achieving 3x inference speedup.

Conclusion: The proposed ERGO method balances computational efficiency and accuracy in vision-language tasks, offering a significant improvement over existing models by leveraging reasoning-driven perception and coarse-to-fine processing.

Abstract: Efficient processing of high-resolution images is crucial for real-world
vision-language applications. However, existing Large Vision-Language Models
(LVLMs) incur substantial computational overhead due to the large number of
vision tokens. With the advent of "thinking with images" models, reasoning now
extends beyond text to the visual domain. This capability motivates our
two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is
analyzed to identify task-relevant regions; then, only these regions are
cropped at full resolution and processed in a subsequent reasoning stage. This
approach reduces computational cost while preserving fine-grained visual
details where necessary. A major challenge lies in inferring which regions are
truly relevant to a given query. Recent related methods often fail in the first
stage after input-image downsampling, due to perception-driven reasoning, where
clear visual information is required for effective reasoning. To address this
issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs
reasoning-driven perception-leveraging multimodal context to determine where to
focus. Our model can account for perceptual uncertainty, expanding the cropped
region to cover visually ambiguous areas for answering questions. To this end,
we develop simple yet effective reward components in a reinforcement learning
framework for coarse-to-fine perception. Across multiple datasets, our approach
delivers higher accuracy than the original model and competitive methods, with
greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*
benchmark by 4.7 points while using only 23% of the vision tokens, achieving a
3x inference speedup. The code and models can be found at:
https://github.com/nota-github/ERGO.

</details>


### [258] [DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints](https://arxiv.org/abs/2509.21992)
*Sungmin Woo,Sangyoun Lee*

Main category: cs.CV

TL;DR: A novel framework called DualFocus is proposed to enhance depth estimation from focus cues, outperforming existing methods in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Modern Depth-from-Focus methods face challenges in complex scenes with fine textures or rapid depth changes, where focus cues are ambiguous.

Method: DualFocus applies a variational formulation with dual constraints: spatial constraints identify true depth edges, and focal constraints enforce unimodal focus probabilities, leveraging gradient patterns in the focal stack.

Result: Extensive testing on four public datasets shows that DualFocus achieves superior depth accuracy and perceptual quality compared to state-of-the-art methods.

Conclusion: DualFocus improves the robustness and accuracy of depth estimation by effectively addressing ambiguous focus cues using its unique modeling approach.

Abstract: Depth-from-Focus (DFF) enables precise depth estimation by analyzing focus
cues across a stack of images captured at varying focal lengths. While recent
learning-based approaches have advanced this field, they often struggle in
complex scenes with fine textures or abrupt depth changes, where focus cues may
become ambiguous or misleading. We present DualFocus, a novel DFF framework
that leverages the focal stack's unique gradient patterns induced by focus
variation, jointly modeling focus changes over spatial and focal dimensions.
Our approach introduces a variational formulation with dual constraints
tailored to DFF: spatial constraints exploit gradient pattern changes across
focus levels to distinguish true depth edges from texture artifacts, while
focal constraints enforce unimodal, monotonic focus probabilities aligned with
physical focus behavior. These inductive biases improve robustness and accuracy
in challenging regions. Comprehensive experiments on four public datasets
demonstrate that DualFocus consistently outperforms state-of-the-art methods in
both depth accuracy and perceptual quality.

</details>


### [259] [Rate-Distortion Optimized Communication for Collaborative Perception](https://arxiv.org/abs/2509.21994)
*Genjia Liu,Anning Hu,Yue Hu,Wenjun Zhang,Siheng Chen*

Main category: cs.CV

TL;DR: The paper introduces a theoretical framework for optimizing communication in collaborative perception tasks and proposes a framework called RDcomm, which achieves high performance while significantly reducing communication demands.


<details>
  <summary>Details</summary>
Motivation: Collaborative perception requires multi-agent systems to share visual data efficiently under bandwidth constraints. Existing approaches lack a strong theoretical foundation for optimizing task performance while minimizing communication.

Method: The authors develop a pragmatic rate-distortion theory tailored for multi-agent systems. They design RDcomm, incorporating task entropy discrete coding for pragmatic information supply and mutual-information-driven message selection for eliminating redundancy.

Result: RDcomm demonstrated state-of-the-art results in 3D object detection and BEV segmentation tasks on datasets like DAIR-V2X and OPV2V, while reducing communication volume by up to 108 times.

Conclusion: The proposed framework successfully balances task performance and communication efficiency, advancing the theoretical and practical frontiers of collaborative perception.

Abstract: Collaborative perception emphasizes enhancing environmental understanding by
enabling multiple agents to share visual information with limited bandwidth
resources. While prior work has explored the empirical trade-off between task
performance and communication volume, a significant gap remains in the
theoretical foundation. To fill this gap, we draw on information theory and
introduce a pragmatic rate-distortion theory for multi-agent collaboration,
specifically formulated to analyze performance-communication trade-off in
goal-oriented multi-agent systems. This theory concretizes two key conditions
for designing optimal communication strategies: supplying pragmatically
relevant information and transmitting redundancy-less messages. Guided by these
two conditions, we propose RDcomm, a communication-efficient collaborative
perception framework that introduces two key innovations: i) task entropy
discrete coding, which assigns features with task-relevant codeword-lengths to
maximize the efficiency in supplying pragmatic information; ii)
mutual-information-driven message selection, which utilizes mutual information
neural estimation to approach the optimal redundancy-less condition.
Experiments on 3D object detection and BEV segmentation demonstrate that RDcomm
achieves state-of-the-art accuracy on DAIR-V2X and OPV2V, while reducing
communication volume by up to 108 times. The code will be released.

</details>


### [260] [FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration](https://arxiv.org/abs/2509.21995)
*Muxi Chen,Zhaohua Zhang,Chenchen Zhao,Mingyang Chen,Wenyu Jiang,Tianwen Jiang,Jianhuan Zhuo,Yu Tang,Qiuyong Xiao,Jihong Zhang,Qiang Xu*

Main category: cs.CV

TL;DR: The paper proposes an active exploration approach with FailureAtlas to identify systematic failures in T2I models, showcasing its scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image benchmarks are static and limited in diagnosing systematic failures or identifying their root causes in models.

Method: The authors design FailureAtlas, a framework that autonomously explores and maps failure scenarios by focusing on minimal, failure-inducing concepts, utilizing novel computational acceleration techniques.

Result: FailureAtlas revealed over 247,000 previously unknown failure cases in Stable Diffusion 1.5 models and linked many failures to training data scarcity.

Conclusion: FailureAtlas introduces an effective, scalable diagnostic-first approach for auditing generative AI models to drive robust developments in the field.

Abstract: Static benchmarks have provided a valuable foundation for comparing
Text-to-Image (T2I) models. However, their passive design offers limited
diagnostic power, struggling to uncover the full landscape of systematic
failures or isolate their root causes. We argue for a complementary paradigm:
active exploration. We introduce FailureAtlas, the first framework designed to
autonomously explore and map the vast failure landscape of T2I models at scale.
FailureAtlas frames error discovery as a structured search for minimal,
failure-inducing concepts. While it is a computationally explosive problem, we
make it tractable with novel acceleration techniques. When applied to Stable
Diffusion models, our method uncovers hundreds of thousands of previously
unknown error slices (over 247,000 in SD1.5 alone) and provides the first
large-scale evidence linking these failures to data scarcity in the training
set. By providing a principled and scalable engine for deep model auditing,
FailureAtlas establishes a new, diagnostic-first methodology to guide the
development of more robust generative AI. The code is available at
https://github.com/cure-lab/FailureAtlas

</details>


### [261] [Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors](https://arxiv.org/abs/2509.21997)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: This paper addresses hallucination issues in Multimodal Large Language Models (MLLMs) with a training-free, self-supervised method that uses dual anchors to correct hidden state representations, improving accuracy without additional costs or side effects.


<details>
  <summary>Details</summary>
Motivation: To reduce the hallucination problem in MLLMs, which persists even in larger models and typically requires solutions that involve trade-offs, additional training, or handcrafted priors.

Method: The authors propose a novel self-supervised hallucination mitigation method that uses a text-to-image model to amplify hallucination signals. This mechanism creates a positive anchor (original image) and a negative anchor (hallucinated image), adjusting decoder hidden states towards faithful representations.

Result: The method reduces hallucinations (object, attribute, and relation levels) significantly, e.g., a 5% reduction on the CHAIR benchmark with the LLaVA-v1.5-7B model, while maintaining recall and caption quality. It generalizes well across different architectures such as LLaVA-NEXT-7B and InstructBLIP-7B.

Conclusion: The proposed method effectively mitigates hallucinations without additional training, human priors, or compromising performance. It demonstrates high robustness, is plug-and-play, and introduces minimal side effects when applied to hallucination-free captions.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable success
across diverse vision-language tasks, yet they remain highly susceptible to
hallucinations, producing content that is fluent but inconsistent with visual
evidence. Such hallucinations, spanning objects, attributes, and relations,
persist even in larger models, while existing mitigation approaches often
require additional finetuning, handcrafted priors, or trade-offs that
compromise informativeness and scalability. To address this limitation, we
propose a training-free, self-supervised method for hallucination mitigation.
Our approach introduces a novel hallucination amplification mechanism: a
caption is projected into the visual space via a text-to-image model to reveal
implicit hallucination signals, serving as a negative anchor, while the
original image provides a positive anchor. Leveraging these dual anchors, we
edit decoder hidden states by pulling representations toward faithful semantics
and pushing them away from hallucination directions. This correction requires
no human priors or additional training costs, ensuring both effectiveness and
efficiency. Extensive experiments across multiple benchmarks show that our
method significantly reduces hallucinations at the object, attribute, and
relation levels while largely preserving recall and caption richness, e.g.,
achieving a hallucination reduction by over 5% using LLaVA-v1.5-7B on CHAIR.
Furthermore, results on diverse architectures, including LLaVA-NEXT-7B,
Cambrian-8B, and InstructBLIP-7B, validate strong cross-architecture
generalization. More importantly, when applied to hallucination-free captions,
our method introduces almost no side effects, underscoring its robustness and
practical plug-and-play applicability. The implementation will be publicly
available.

</details>


### [262] [CoFFT: Chain of Foresight-Focus Thought for Visual Language Models](https://arxiv.org/abs/2509.22010)
*Xinyu Zhang,Yuxuan Dong,Lingling Zhang,Chengyou Jia,Zhuohang Dang,Basura Fernando,Jun Liu,Mike Zheng Shou*

Main category: cs.CV

TL;DR: The paper introduces "Chain of Foresight-Focus Thought" (CoFFT), a training-free method to enhance Vision Language Models' visual reasoning accuracy, overcoming issues like interference and hallucinations.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models (VLMs) struggle with interference and hallucination due to irrelevant information in visual inputs.

Method: CoFFT iteratively emulates human visual cognition using three stages: generation of diverse reasoning samples, decoding samples based on visual and reasoning alignment, and adjusting visual focus for optimal reasoning.

Result: Empirical testing on multiple benchmarks showed consistent performance improvement of 3.1-5.8%, with manageable computational overhead.

Conclusion: CoFFT effectively improves visual reasoning in VLMs and addresses issues of interference and hallucination without requiring additional training.

Abstract: Despite significant advances in Vision Language Models (VLMs), they remain
constrained by the complexity and redundancy of visual input. When images
contain large amounts of irrelevant information, VLMs are susceptible to
interference, thus generating excessive task-irrelevant reasoning processes or
even hallucinations. This limitation stems from their inability to discover and
process the required regions during reasoning precisely. To address this
limitation, we present the Chain of Foresight-Focus Thought (CoFFT), a novel
training-free approach that enhances VLMs' visual reasoning by emulating human
visual cognition. Each Foresight-Focus Thought consists of three stages: (1)
Diverse Sample Generation: generates diverse reasoning samples to explore
potential reasoning paths, where each sample contains several reasoning steps;
(2) Dual Foresight Decoding: rigorously evaluates these samples based on both
visual focus and reasoning progression, adding the first step of optimal sample
to the reasoning process; (3) Visual Focus Adjustment: precisely adjust visual
focus toward regions most beneficial for future reasoning, before returning to
stage (1) to generate subsequent reasoning samples until reaching the final
answer. These stages function iteratively, creating an interdependent cycle
where reasoning guides visual focus and visual focus informs subsequent
reasoning. Empirical results across multiple benchmarks using Qwen2.5-VL,
InternVL-2.5, and Llava-Next demonstrate consistent performance improvements of
3.1-5.8\% with controllable increasing computational overhead.

</details>


### [263] [EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking](https://arxiv.org/abs/2509.22019)
*Yuki Sakai,Ryosuke Furuta,Juichun Yen,Yoichi Sato*

Main category: cs.CV

TL;DR: The paper introduces an egocentric video dataset for analyzing face-to-face instructional interactions, focusing on procedural step segmentation and conversation-state classification. It benchmarks multimodal large language models (MLLMs) against task-specific models, finding MLLMs effective at understanding these interactions without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Face-to-face instructional interactions are understudied in computer vision due to a lack of datasets and analytical techniques.

Method: The authors develop an egocentric video dataset and annotate it for two tasks: procedural step segmentation and conversation-state classification. They evaluate multimodal large language models (MLLMs) that process images, audio, and text against traditional task-specific models.

Result: MLLMs outperform task-specific baseline models, showing promise in understanding verbal and nonverbal cues in instructional interactions, even without fine-tuning.

Conclusion: MLLMs can provide a holistic understanding of educational face-to-face interactions, and the dataset serves as a foundational tool for advancing research in this domain.

Abstract: Analyzing instructional interactions between an instructor and a learner who
are co-present in the same physical space is a critical problem for educational
support and skill transfer. Yet such face-to-face instructional scenes have not
been systematically studied in computer vision. We identify two key reasons: i)
the lack of suitable datasets and ii) limited analytical techniques. To address
this gap, we present a new egocentric video dataset of face-to-face instruction
and provide ground-truth annotations for two fundamental tasks that serve as a
first step toward a comprehensive understanding of instructional interactions:
procedural step segmentation and conversation-state classification. Using this
dataset, we benchmark multimodal large language models (MLLMs) against
conventional task-specific models. Since face-to-face instruction involves
multiple modalities (speech content and prosody, gaze and body motion, and
visual context), effective understanding requires methods that handle verbal
and nonverbal communication in an integrated manner. Accordingly, we evaluate
recently introduced MLLMs that jointly process images, audio, and text. This
evaluation quantifies the extent to which current machine learning models
understand face-to-face instructional scenes. In experiments, MLLMs outperform
specialized baselines even without task-specific fine-tuning, suggesting their
promise for holistic understanding of instructional interactions.

</details>


### [264] [High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling](https://arxiv.org/abs/2509.22063)
*Chao Huang,Susan Liang,Yapeng Tian,Anurag Kumar,Chenliang Xu*

Main category: cs.CV

TL;DR: DAVIS is a novel audio-visual sound separation framework leveraging generative models like DDPM and Flow Matching to achieve high-quality separations.


<details>
  <summary>Details</summary>
Motivation: Existing sound separation methods struggle to capture complex data distributions necessary for diverse sound categories, motivating the need for a more effective generative approach.

Method: The framework synthesizes separated sound spectrograms using DDPM and Flow Matching generative models within a specialized Separation U-Net architecture, conditioned on mixed audio and associated visual data.

Result: DAVIS, in both its DDPM and Flow Matching forms, outperformed leading methods in sound separation on AVE and MUSIC datasets.

Conclusion: DAVIS demonstrates the efficacy of generative modeling for audio-visual sound separation, overcoming the limitations of mask-based regression approaches.

Abstract: We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that
solves the audio-visual sound source separation task through generative
learning. Existing methods typically frame sound separation as a mask-based
regression problem, achieving significant progress. However, they face
limitations in capturing the complex data distribution required for
high-quality separation of sounds from diverse categories. In contrast, DAVIS
circumvents these issues by leveraging potent generative modeling paradigms,
specifically Denoising Diffusion Probabilistic Models (DDPM) and the more
recent Flow Matching (FM), integrated within a specialized Separation U-Net
architecture. Our framework operates by synthesizing the desired separated
sound spectrograms directly from a noise distribution, conditioned concurrently
on the mixed audio input and associated visual information. The inherent nature
of its generative objective makes DAVIS particularly adept at producing
high-quality sound separations for diverse sound categories. We present
comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching
variants, against leading methods on the standard AVE and MUSIC datasets. The
results affirm that both variants surpass existing approaches in separation
quality, highlighting the efficacy of our generative framework for tackling the
audio-visual source separation task.

</details>


### [265] [SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection](https://arxiv.org/abs/2509.22070)
*Inzamamul Alam,Md Tanvir Islam,Simon S. Woo*

Main category: cs.CV

TL;DR: The paper introduces SpecXNet, an advanced model for deepfake detection that combines spatial and spectral domain features to improve accuracy and generalization. It achieves state-of-the-art results and is both robust and efficient.


<details>
  <summary>Details</summary>
Motivation: The increasing realism of deepfake content generated by GANs and diffusion models makes detection harder, requiring more generalized approaches to handle unseen manipulations across datasets.

Method: SpecXNet employs a dual-domain architecture featuring the Dual-Domain Feature Coupler (DDFC) for texture and structural inconsistencies and a Dual Fourier Attention (DFA) module for dynamic fusion of spatial and spectral features. It utilizes a modified XceptionNet backbone.

Result: Extensive experiments demonstrate SpecXNet achieves state-of-the-art accuracy across multiple deepfake benchmarks, particularly excelling in cross-dataset and unseen manipulation scenarios.

Conclusion: Unified spatial-spectral learning as implemented in SpecXNet is highly effective for robust deepfake detection, offering reliable results while being computationally feasible. The authors provide code for reproducibility.

Abstract: The increasing realism of content generated by GANs and diffusion models has
made deepfake detection significantly more challenging. Existing approaches
often focus solely on spatial or frequency-domain features, limiting their
generalization to unseen manipulations. We propose the Spectral
Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust
deepfake detection. The core \textbf{Dual-Domain Feature Coupler (DDFC)}
decomposes features into a local spatial branch for capturing texture-level
anomalies and a global spectral branch that employs Fast Fourier Transform to
model periodic inconsistencies. This dual-domain formulation allows SpecXNet to
jointly exploit localized detail and global structural coherence, which are
critical for distinguishing authentic from manipulated images. We also
introduce the \textbf{Dual Fourier Attention (DFA)} module, which dynamically
fuses spatial and spectral features in a content-aware manner. Built atop a
modified XceptionNet backbone, we embed the DDFC and DFA modules within a
separable convolution block. Extensive experiments on multiple deepfake
benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly
under cross-dataset and unseen manipulation scenarios, while maintaining
real-time feasibility. Our results highlight the effectiveness of unified
spatial-spectral learning for robust and generalizable deepfake detection. To
ensure reproducibility, we released the full code on
\href{https://github.com/inzamamulDU/SpecXNet}{\textcolor{blue}{\textbf{GitHub}}}.

</details>


### [266] [Large Material Gaussian Model for Relightable 3D Generation](https://arxiv.org/abs/2509.22112)
*Jingrui Ye,Lingting Zhu,Runze Zhang,Zeyu Hu,Yingda Yin,Lanjiong Li,Lequan Yu,Qingmin Liao*

Main category: cs.CV

TL;DR: This paper introduces a framework called MGM to create high-quality 3D content with PBR materials, addressing limitations in existing methods by modeling material properties like albedo, roughness, and metallic characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction models fail to capture essential material properties, such as PBR attributes, which are vital for realistic rendering under diverse lighting conditions.

Method: The authors fine-tune a material diffusion model using input depth and normal maps to generate multiview PBR images. They then represent PBR materials with a Gaussian material framework that aligns with 2D Gaussian Splatting for reconstruction.

Result: Experiments show that the MGM method outperforms baseline approaches in visual quality and material representation, facilitating dynamic relighting in various lighting environments.

Conclusion: The MGM framework is a significant step towards automated and high-quality 3D asset creation with realistic and adaptable material properties for practical rendering applications.

Abstract: The increasing demand for 3D assets across various industries necessitates
efficient and automated methods for 3D content creation. Leveraging 3D Gaussian
Splatting, recent large reconstruction models (LRMs) have demonstrated the
ability to efficiently achieve high-quality 3D rendering by integrating
multiview diffusion for generation and scalable transformers for
reconstruction. However, existing models fail to produce the material
properties of assets, which is crucial for realistic rendering in diverse
lighting environments. In this paper, we introduce the Large Material Gaussian
Model (MGM), a novel framework designed to generate high-quality 3D content
with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and
metallic properties, rather than merely producing RGB textures with
uncontrolled light baking. Specifically, we first fine-tune a new multiview
material diffusion model conditioned on input depth and normal maps. Utilizing
the generated multiview PBR images, we explore a Gaussian material
representation that not only aligns with 2D Gaussian Splatting but also models
each channel of the PBR materials. The reconstructed point clouds can then be
rendered to acquire PBR attributes, enabling dynamic relighting by applying
various ambient light maps. Extensive experiments demonstrate that the
materials produced by our method not only exhibit greater visual appeal
compared to baseline methods but also enhance material modeling, thereby
enabling practical downstream rendering applications.

</details>


### [267] [Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud](https://arxiv.org/abs/2509.22132)
*Jingjing Lu,Huilong Pi,Yunchuan Qin,Zhuo Tang,Ruihui Li*

Main category: cs.CV

TL;DR: The paper proposes a novel self-supervised approach for point cloud completion that relies on multi-view augmentations of single partial point clouds, achieving state-of-the-art results on synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for point cloud completion face limitations such as reliance on ground truth (supervised), need for unpaired or multi-view data (unsupervised/weakly-supervised), and ineffective self-supervised signals.

Method: The proposed method introduces novel self-supervised signals based on multi-view augmentations of single partial point clouds. It also incorporates Mamba to improve the quality of generated point clouds.

Result: The proposed method demonstrates state-of-the-art performance on both synthetic and real-world datasets.

Conclusion: The novel self-supervised approach addresses the limitations of other methods, offering a generalizable and effective solution for point cloud completion.

Abstract: Point cloud completion aims to reconstruct complete shapes from partial
observations. Although current methods have achieved remarkable performance,
they still have some limitations: Supervised methods heavily rely on ground
truth, which limits their generalization to real-world datasets due to the
synthetic-to-real domain gap. Unsupervised methods require complete point
clouds to compose unpaired training data, and weakly-supervised methods need
multi-view observations of the object. Existing self-supervised methods
frequently produce unsatisfactory predictions due to the limited capabilities
of their self-supervised signals. To overcome these challenges, we propose a
novel self-supervised point cloud completion method. We design a set of novel
self-supervised signals based on multi-view augmentations of the single partial
point cloud. Additionally, to enhance the model's learning ability, we first
incorporate Mamba into self-supervised point cloud completion task, encouraging
the model to generate point clouds with better quality. Experiments on
synthetic and real-world datasets demonstrate that our method achieves
state-of-the-art results.

</details>


### [268] [REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation](https://arxiv.org/abs/2509.22139)
*Yicheng Jiang,Jin Yuan,Hua Yuan,Yao Zhang,Yong Rui*

Main category: cs.CV

TL;DR: Refine-Control introduces a semi-supervised framework to improve conditional image generation models, making them more resource-efficient and suitable for edge devices.


<details>
  <summary>Details</summary>
Motivation: Deploying conditional image generation on edge devices faces challenges due to high computational resource demands and data scarcity.

Method: Refine-Control employs a tri-level knowledge fusion loss and semi-supervised distillation with both labeled and unlabeled data.

Result: The framework reduces computational cost and latency while preserving image generation quality and control.

Conclusion: Refine-Control enables efficient, controllable, and high-quality image generation on edge devices, addressing resource and privacy concerns.

Abstract: Conditional image generation models have achieved remarkable results by
leveraging text-based control to generate customized images. However, the high
resource demands of these models and the scarcity of well-annotated data have
hindered their deployment on edge devices, leading to enormous costs and
privacy concerns, especially when user data is sent to a third party. To
overcome these challenges, we propose Refine-Control, a semi-supervised
distillation framework. Specifically, we improve the performance of the student
model by introducing a tri-level knowledge fusion loss to transfer different
levels of knowledge. To enhance generalization and alleviate dataset scarcity,
we introduce a semi-supervised distillation method utilizing both labeled and
unlabeled data. Our experiments reveal that Refine-Control achieves significant
reductions in computational cost and latency, while maintaining high-fidelity
generation capabilities and controllability, as quantified by comparative
metrics.

</details>


### [269] [Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions](https://arxiv.org/abs/2509.22150)
*Zhiqiang Tian,Weigang Li,Junwei Hu,Chunhua Deng*

Main category: cs.CV

TL;DR: The paper introduces a novel classification strategy, JGEKD, for non-IID 3D point cloud data that incorporates class correlation via knowledge distillation and joint graph entropy.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked class correlation in 3D point clouds under the common IID assumption in classification tasks.

Method: The method involves using joint graphs to capture class relationships, Siamese structures for spatial transformation invariance, and knowledge distillation frameworks (self- and teacher-distillation).

Result: The proposed strategy achieves competitive outcomes on multiple 3D datasets including ScanObject, ModelNet40, and ModelNet-C.

Conclusion: JGEKD improves the robustness and classification accuracy of models for 3D point clouds, effectively handling class correlations and data corruptions.

Abstract: Classification tasks in 3D point clouds often assume that class events
\replaced{are }{follow }independent and identically distributed (IID), although
this assumption destroys the correlation between classes. This \replaced{study
}{paper }proposes a classification strategy, \textbf{J}oint \textbf{G}raph
\textbf{E}ntropy \textbf{K}nowledge \textbf{D}istillation (JGEKD), suitable for
non-independent and identically distributed 3D point cloud data,
\replaced{which }{the strategy } achieves knowledge transfer of class
correlations through knowledge distillation by constructing a loss function
based on joint graph entropy. First\deleted{ly}, we employ joint graphs to
capture add{the }hidden relationships between classes\replaced{ and}{,}
implement knowledge distillation to train our model by calculating the entropy
of add{add }graph.\replaced{ Subsequently}{ Then}, to handle 3D point clouds
\deleted{that is }invariant to spatial transformations, we construct
\replaced{S}{s}iamese structures and develop two frameworks, self-knowledge
distillation and teacher-knowledge distillation, to facilitate information
transfer between different transformation forms of the same data. \replaced{In
addition}{ Additionally}, we use the above framework to achieve knowledge
transfer between point clouds and their corrupted forms, and increase the
robustness against corruption of model. Extensive experiments on ScanObject,
ModelNet40, ScanntV2\_cls and ModelNet-C demonstrate that the proposed strategy
can achieve competitive results.

</details>


### [270] [MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models](https://arxiv.org/abs/2509.22151)
*Jonas Belouadi,Tamy Boubekeur,Adrien Kaiser*

Main category: cs.CV

TL;DR: MultiMat enhances procedural material graph synthesis by combining visual and textual graph representations, outperforming text-only methods.


<details>
  <summary>Details</summary>
Motivation: Creating procedural material node graphs is complex and requires professional training. Current approaches represent graphs only textually, ignoring their visual-spatial aspects.

Method: MultiMat utilizes a multimodal program synthesis framework with large multimodal models and a new production-quality dataset. A constrained tree search algorithm is used for syntactically valid graph generation.

Result: MultiMat achieves higher efficiency, better visual quality, and fidelity in both unconditional and conditional graph synthesis compared to text-only methods.

Conclusion: The proposed method sets new state-of-the-art performance for procedural material graph synthesis by leveraging multimodal data.

Abstract: Material node graphs are programs that generate the 2D channels of procedural
materials, including geometry such as roughness and displacement maps, and
reflectance such as albedo and conductivity maps. They are essential in
computer graphics for representing the appearance of virtual 3D objects
parametrically and at arbitrary resolution. In particular, their directed
acyclic graph structures and intermediate states provide an intuitive
understanding and workflow for interactive appearance modeling. Creating such
graphs is a challenging task and typically requires professional training.
While recent neural program synthesis approaches attempt to simplify this
process, they solely represent graphs as textual programs, failing to capture
the inherently visual-spatial nature of node graphs that makes them accessible
to humans. To address this gap, we present MultiMat, a multimodal program
synthesis framework that leverages large multimodal models to process both
visual and textual graph representations for improved generation of procedural
material graphs. We train our models on a new dataset of production-quality
procedural materials and combine them with a constrained tree search inference
algorithm that ensures syntactic validity while efficiently navigating the
program space. Our experimental results show that our multimodal program
synthesis method is more efficient in both unconditional and conditional graph
synthesis with higher visual quality and fidelity than text-only baselines,
establishing new state-of-the-art performance.

</details>


### [271] [DragGANSpace: Latent Space Exploration and Control for GANs](https://arxiv.org/abs/2509.22169)
*Kirsten Odendaal,Neela Kaushik,Spencer Halverson*

Main category: cs.CV

TL;DR: This paper combines StyleGAN, DragGAN, and PCA to streamline GAN-generated image manipulation and improve optimization efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance the latent space efficiency and controllability of GAN-generated images.

Method: Integrated approach using StyleGAN for structured latent space, DragGAN for image manipulation, and PCA for dimensionality reduction and model alignment.

Result: Improved optimization efficiency, consistent visual quality, enhanced SSIM in shallower latent spaces, and capability to align and manipulate images across different StyleGAN models.

Conclusion: Integration of PCA with DragGAN provides efficient and intuitive latent space control, with potential applications in image synthesis and editing.

Abstract: This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA)
to enhance the latent space efficiency and controllability of GAN-generated
images. Style-GAN provides a structured latent space, DragGAN enables intuitive
image manipulation, and PCA reduces dimensionality and facilitates cross-model
alignment for more streamlined and interpretable exploration of latent spaces.
We apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and
find that our approach of integrating PCA-based dimensionality reduction with
the Drag-GAN framework for image manipulation retains performance while
improving optimization efficiency. Notably, introducing PCA into the latent W+
layers of DragGAN can consistently reduce the total optimization time while
maintaining good visual quality and even boosting the Structural Similarity
Index Measure (SSIM) of the optimized image, particularly in shallower latent
spaces (W+ layers = 3). We also demonstrate capability for aligning images
generated by two StyleGAN models trained on similar but distinct data domains
(AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these
aligned images to manipulate the images in an intuitive and interpretable
manner. Our findings highlight the possibility for efficient and interpretable
latent space control for a wide range of image synthesis and editing
applications.

</details>


### [272] [MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing](https://arxiv.org/abs/2509.22186)
*Junbo Niu,Zheng Liu,Zhuangcheng Gu,Bin Wang,Linke Ouyang,Zhiyuan Zhao,Tao Chu,Tianyao He,Fan Wu,Qintong Zhang,Zhenjiang Jin,Guang Liang,Rui Zhang,Wenzheng Zhang,Yuan Qu,Zhifei Ren,Yuefeng Sun,Yuanhong Zheng,Dongsheng Ma,Zirui Tang,Boyu Niu,Ziyang Miao,Hejun Dong,Siyi Qian,Junyuan Zhang,Jingzhou Chen,Fangdong Wang,Xiaomeng Zhao,Liqun Wei,Wei Li,Shasha Wang,Ruiliang Xu,Yuanyuan Cao,Lu Chen,Qianqian Wu,Huaiyu Gu,Lindong Lu,Keming Wang,Dechen Lin,Guanlin Shen,Xuanhe Zhou,Linfeng Zhang,Yuhang Zang,Xiaoyi Dong,Jiaqi Wang,Bo Zhang,Lei Bai,Pei Chu,Weijia Li,Jiang Wu,Lijun Wu,Zhenxiang Li,Guangyu Wang,Zhongying Tu,Chao Xu,Kai Chen,Yu Qiao,Bowen Zhou,Dahua Lin,Wentao Zhang,Conghui He*

Main category: cs.CV

TL;DR: MinerU2.5 is a 1.2B-parameter efficient vision-language document parsing model with a two-stage parsing strategy, achieving state-of-the-art accuracy with lower computational demands.


<details>
  <summary>Details</summary>
Motivation: To develop a document parsing model that balances state-of-the-art recognition accuracy with high computational efficiency.

Method: A two-stage parsing approach is used: stage one performs layout analysis on low-res images, and stage two performs fine content recognition using high-res crops. A data engine is created for diverse training corpus generation.

Result: MinerU2.5 achieves state-of-the-art results on multiple document parsing benchmarks, outperforming other models in accuracy while reducing computational effort.

Conclusion: MinerU2.5 proves effective in combining high accuracy with computational efficiency, setting a new standard in the field of document parsing models.

Abstract: We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language
model that achieves state-of-the-art recognition accuracy while maintaining
exceptional computational efficiency. Our approach employs a coarse-to-fine,
two-stage parsing strategy that decouples global layout analysis from local
content recognition. In the first stage, the model performs efficient layout
analysis on downsampled images to identify structural elements, circumventing
the computational overhead of processing high-resolution inputs. In the second
stage, guided by the global layout, it performs targeted content recognition on
native-resolution crops extracted from the original image, preserving
fine-grained details in dense text, complex formulas, and tables. To support
this strategy, we developed a comprehensive data engine that generates diverse,
large-scale training corpora for both pretraining and fine-tuning. Ultimately,
MinerU2.5 demonstrates strong document parsing ability, achieving
state-of-the-art performance on multiple benchmarks, surpassing both
general-purpose and domain-specific models across various recognition tasks,
while maintaining significantly lower computational overhead.

</details>


### [273] [Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models](https://arxiv.org/abs/2509.22221)
*Jiaqi Liu,Lang Sun,Ronghao Fu,Bo Yang*

Main category: cs.CV

TL;DR: This paper introduces Geo-CoT and RSThinker to improve reasoning in vision-language models for remote sensing using a structured, multi-step analytical framework.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models struggle with complex analytical tasks in remote sensing due to their end-to-end training paradigm, which leads to unverifiable outputs.

Method: The paper proposes a two-stage alignment strategy: Supervised Fine-Tuning (SFT) for foundational training and Group Reward Policy Optimization (GRPO) for improving reasoning accuracy. A large-scale Geo-CoT380k dataset is utilized.

Result: The developed model, RSThinker, surpassed state-of-the-art models across various tasks, delivering both answers and verified analytical processes.

Conclusion: Geo-CoT380k and RSThinker enhance remote sensing by transitioning models from opaque operations to structured reasoning, advancing Earth Observation techniques.

Abstract: Vision-Language Models (VLMs) in remote sensing often fail at complex
analytical tasks, a limitation stemming from their end-to-end training paradigm
that bypasses crucial reasoning steps and leads to unverifiable outputs. To
address this limitation, we introduce the Perceptually-Grounded Geospatial
Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as
a verifiable, multi-step process. We instill this analytical process through a
two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale
dataset of structured Geo-CoT rationales. This strategy first employs
supervised fine-tuning (SFT) to instill the foundational cognitive
architecture, then leverages Group Reward Policy Optimization (GRPO) to refine
the model's reasoning policy towards factual correctness. The resulting model,
RSThinker, outputs both a final answer and its justifying, verifiable
analytical trace. This capability yields dominant performance, significantly
outperforming state-of-the-art models across a comprehensive range of tasks.
The public release of our Geo-CoT380k dataset and RSThinker model upon
publication serves as a concrete pathway from opaque perception towards
structured, verifiable reasoning for Earth Observation.

</details>


### [274] [Polysemous Language Gaussian Splatting via Matching-based Mask Lifting](https://arxiv.org/abs/2509.22225)
*Jiayu Ding,Xinpeng Liu,Zhiyi Pan,Shiqiang Long,Ge Li*

Main category: cs.CV

TL;DR: The paper introduces MUSplat, a training-free framework for improving 3D Gaussian Splatting scene understanding, addressing issues with costly retraining, limited semantics, and semantic inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene understanding methods face limitations like the need for expensive retraining, limited ability to represent multiple concepts, and cross-view semantic inconsistencies.

Method: MUSplat utilizes a pre-trained 2D segmentation model to generate and lift masks into 3D, optimizes object groups via semantic entropy and geometric opacity, and employs a Vision-Language Model for reconciling inconsistencies.

Result: MUSplat reduces adaptation time from hours to minutes while surpassing training-based frameworks in 3D object selection and segmentation benchmarks.

Conclusion: MUSplat effectively overcomes critical limitations of existing methods, offering a faster, versatile, and more semantically robust solution for open-vocabulary 3D scene understanding.

Abstract: Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS)
scenes is a critical challenge. However, mainstream methods suffer from three
key flaws: (i) their reliance on costly per-scene retraining prevents
plug-and-play application; (ii) their restrictive monosemous design fails to
represent complex, multi-concept semantics; and (iii) their vulnerability to
cross-view semantic inconsistencies corrupts the final semantic representation.
To overcome these limitations, we introduce MUSplat, a training-free framework
that abandons feature optimization entirely. Leveraging a pre-trained 2D
segmentation model, our pipeline generates and lifts multi-granularity 2D masks
into 3D, where we estimate a foreground probability for each Gaussian point to
form initial object groups. We then optimize the ambiguous boundaries of these
initial groups using semantic entropy and geometric opacity. Subsequently, by
interpreting the object's appearance across its most representative viewpoints,
a Vision-Language Model (VLM) distills robust textual features that reconciles
visual inconsistencies, enabling open-vocabulary querying via semantic
matching. By eliminating the costly per-scene training process, MUSplat reduces
scene adaptation time from hours to mere minutes. On benchmark tasks for
open-vocabulary 3D object selection and semantic segmentation, MUSplat
outperforms established training-based frameworks while simultaneously
addressing their monosemous limitations.

</details>


### [275] [UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective](https://arxiv.org/abs/2509.22228)
*Jun He,Yi Lin,Zilong Huang,Jiacong Yin,Junyan Ye,Yuchuan Zhou,Weijia Li,Xiang Zhang*

Main category: cs.CV

TL;DR: UrbanFeel introduces a benchmark to evaluate how Multimodal Large Language Models (MLLMs) understand urban development and subjective environmental perception using 14.3K visual questions paired with image data. Gemini-2.5 Pro leads the evaluations.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to understand how MLLMs perceive urban environments, including their structural and perceptual changes, which influence over half of the global population. Existing benchmarks lack depth in temporal and subjective urban analysis.

Method: Researchers created the UrbanFeel benchmark, utilizing images from 11 cities and generating 14.3K visual Q&A pairs through clustering, rule-based techniques, model-prompting, and manual inputs. This is designed to test three dimensions of urban analysis.

Result: Evaluations of 20 state-of-the-art MLLMs revealed Gemini-2.5 Pro as the best-performing model, closely matching human capabilities, especially in scene perception and change detection. However, MLLMs struggled with temporal reasoning tasks.

Conclusion: UrbanFeel provides a critical benchmark for advancing how AI models approach urban development understanding, showing promise but also exposing limitations in temporal reasoning for sustainable urban planning.

Abstract: Urban development impacts over half of the global population, making
human-centered understanding of its structural and perceptual changes essential
for sustainable development. While Multimodal Large Language Models (MLLMs)
have shown remarkable capabilities across various domains, existing benchmarks
that explore their performance in urban environments remain limited, lacking
systematic exploration of temporal evolution and subjective perception of urban
environment that aligns with human perception. To address these limitations, we
propose UrbanFeel, a comprehensive benchmark designed to evaluate the
performance of MLLMs in urban development understanding and subjective
environmental perception. UrbanFeel comprises 14.3K carefully constructed
visual questions spanning three cognitively progressive dimensions: Static
Scene Perception, Temporal Change Understanding, and Subjective Environmental
Perception. We collect multi-temporal single-view and panoramic street-view
images from 11 representative cities worldwide, and generate high-quality
question-answer pairs through a hybrid pipeline of spatial clustering,
rule-based generation, model-assisted prompting, and manual annotation. Through
extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5
Pro achieves the best overall performance, with its accuracy approaching human
expert levels and narrowing the average gap to just 1.5\%. Most models perform
well on tasks grounded in scene understanding. In particular, some models even
surpass human annotators in pixel-level change detection. However, performance
drops notably in tasks requiring temporal reasoning over urban development.
Additionally, in the subjective perception dimension, several models reach
human-level or even higher consistency in evaluating dimension such as
beautiful and safety.

</details>


### [276] [A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.22229)
*Jiaping Yu,Muli Yang,Jiapeng Ji,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: This paper introduces the Experts Cooperative Learning (EXCL) framework to address Source-Free Unsupervised Domain Adaptation (SFUDA), utilizing complementary models and optimization strategies to improve performance on target data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of adapting a source-trained model to a target domain without access to source data, motivated by privacy and cost concerns.

Method: The paper proposes the EXCL framework with two key components: the Dual Experts framework (utilizing frozen source-domain and pretrained vision-language models) and the RAIN optimization pipeline (including retrieval, fine-tuning, and consistency enforcement).

Result: EXCL achieves state-of-the-art performance on four benchmark datasets, effectively mining knowledge from unlabeled target data.

Conclusion: EXCL enhances SFUDA methods by combining complementary model insights and leveraging latent target data structures, making it a significant advancement in the field.

Abstract: Source-Free Unsupervised Domain Adaptation (SFUDA) addresses the realistic
challenge of adapting a source-trained model to a target domain without access
to the source data, driven by concerns over privacy and cost. Existing SFUDA
methods either exploit only the source model's predictions or fine-tune large
multimodal models, yet both neglect complementary insights and the latent
structure of target data. In this paper, we propose the Experts Cooperative
Learning (EXCL). EXCL contains the Dual Experts framework and
Retrieval-Augmentation-Interaction optimization pipeline. The Dual Experts
framework places a frozen source-domain model (augmented with Conv-Adapter) and
a pretrained vision-language model (with a trainable text prompt) on equal
footing to mine consensus knowledge from unlabeled target samples. To
effectively train these plug-in modules under purely unsupervised conditions,
we introduce Retrieval-Augmented-Interaction(RAIN), a three-stage pipeline that
(1) collaboratively retrieves pseudo-source and complex target samples, (2)
separately fine-tunes each expert on its respective sample set, and (3)
enforces learning object consistency via a shared learning result. Extensive
experiments on four benchmark datasets demonstrate that our approach matches
state-of-the-art performance.

</details>


### [277] [FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing](https://arxiv.org/abs/2509.22244)
*Junyi Wu,Zhiteng Li,Haotong Qin,Xiaohong Liu,Linghe Kong,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: The paper introduces FlashEdit, a real-time, high-quality image editing framework using diffusion models, achieving significant speedup (150x) over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion model-based image editing methods achieve high quality but have prohibitively slow latency, limiting their practicality in real-world applications.

Method: FlashEdit employs three innovations: a One-Step Inversion-and-Editing (OSIE) pipeline, a Background Shield (BG-Shield) for preserving background regions, and a Sparsified Spatial Cross-Attention (SSCA) mechanism to ensure accurate, localized edits.

Result: FlashEdit delivers high-fidelity image edits in under 0.2 seconds while maintaining background consistency and structural integrity, showing significant efficiency and quality compared to multi-step methods.

Conclusion: FlashEdit provides a fast, reliable framework for image editing, overcoming the latency issues of prior techniques, and achieves a breakthrough in real-time image processing.

Abstract: Text-guided image editing with diffusion models has achieved remarkable
quality but suffers from prohibitive latency, hindering real-world
applications. We introduce FlashEdit, a novel framework designed to enable
high-fidelity, real-time image editing. Its efficiency stems from three key
innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses
costly iterative processes; (2) a Background Shield (BG-Shield) technique that
guarantees background preservation by selectively modifying features only
within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA)
mechanism that ensures precise, localized edits by suppressing semantic leakage
to the background. Extensive experiments demonstrate that FlashEdit maintains
superior background consistency and structural integrity, while performing
edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to
prior multi-step methods. Our code will be made publicly available at
https://github.com/JunyiWuCode/FlashEdit.

</details>


### [278] [Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks](https://arxiv.org/abs/2509.22258)
*Miao Jing,Mengting Jia,Junling Lin,Zhongxia Shen,Lijun Wang,Yuanyuan Peng,Huan Gao,Mingkun Xu,Shangyang Li*

Main category: cs.CV

TL;DR: Neural-MedBench is introduced as a compact benchmark for evaluating multimodal clinical reasoning ability, highlighting deficiencies in current vision-language models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Current vision-language models excel in performance metrics but fail to exhibit robust clinical reasoning skills. The paper aims to address this gap by creating a specialized benchmark.

Method: The Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes to focus on tasks like differential diagnosis, lesion recognition, and rationale generation. A hybrid scoring system combining AI grading, clinician validation, and semantic similarity metrics ensures reliability.

Result: State-of-the-art VLMs demonstrate significantly lower performance on Neural-MedBench compared to conventional datasets, primarily due to reasoning rather than perceptual errors.

Conclusion: The study emphasizes the need for benchmarks like Neural-MedBench that prioritize reasoning depth to ensure clinically trustworthy AI, and suggests a Two-Axis Evaluation Framework for future assessments.

Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable
performance on standard medical benchmarks, yet their true clinical reasoning
ability remains unclear. Existing datasets predominantly emphasize
classification accuracy, creating an evaluation illusion in which models appear
proficient while still failing at high-stakes diagnostic reasoning. We
introduce Neural-MedBench, a compact yet reasoning-intensive benchmark
specifically designed to probe the limits of multimodal clinical reasoning in
neurology. Neural-MedBench integrates multi-sequence MRI scans, structured
electronic health records, and clinical notes, and encompasses three core task
families: differential diagnosis, lesion recognition, and rationale generation.
To ensure reliable evaluation, we develop a hybrid scoring pipeline that
combines LLM-based graders, clinician validation, and semantic similarity
metrics. Through systematic evaluation of state-of-the-art VLMs, including
GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to
conventional datasets. Error analysis shows that reasoning failures, rather
than perceptual errors, dominate model shortcomings. Our findings highlight the
necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets
for statistical generalization, and depth-oriented, compact benchmarks such as
Neural-MedBench for reasoning fidelity. We release Neural-MedBench at
https://neuromedbench.github.io/ as an open and extensible diagnostic testbed,
which guides the expansion of future benchmarks and enables rigorous yet
cost-effective assessment of clinically trustworthy AI.

</details>


### [279] [UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data](https://arxiv.org/abs/2509.22262)
*Yujian Yuan,Changjie Wu,Xinyuan Chang,Sijin Wang,Hang Zhang,Shiyi Liang,Shuang Zeng,Mu Xu*

Main category: cs.CV

TL;DR: The paper introduces UniMapGen, a generative framework for constructing large-scale maps more effectively and reliably compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional large-scale map construction approaches are costly, labor-intensive, and limited by the drawbacks of satellite data, such as occlusions and outdatedness.

Method: UniMapGen leverages a generative framework with a focus on discrete sequence representation, multi-modal input flexibility, and a state update strategy to improve the continuity and quality of maps.

Result: UniMapGen achieves state-of-the-art performance in map construction on the OpenSatMap dataset and demonstrates capabilities to infer missing or occluded road data.

Conclusion: UniMapGen is a promising solution for large-scale map construction, addressing inefficiencies and satellite data limitations while improving the quality and consistency of generated maps.

Abstract: Large-scale map construction is foundational for critical applications such
as autonomous driving and navigation systems. Traditional large-scale map
construction approaches mainly rely on costly and inefficient special data
collection vehicles and labor-intensive annotation processes. While existing
satellite-based methods have demonstrated promising potential in enhancing the
efficiency and coverage of map construction, they exhibit two major
limitations: (1) inherent drawbacks of satellite data (e.g., occlusions,
outdatedness) and (2) inefficient vectorization from perception-based methods,
resulting in discontinuous and rough roads that require extensive
post-processing. This paper presents a novel generative framework, UniMapGen,
for large-scale map construction, offering three key innovations: (1)
representing lane lines as \textbf{discrete sequence} and establishing an
iterative strategy to generate more complete and smooth map vectors than
traditional perception-based methods. (2) proposing a flexible architecture
that supports \textbf{multi-modal} inputs, enabling dynamic selection among
BEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3)
developing a \textbf{state update} strategy for global continuity and
consistency of the constructed large-scale map. UniMapGen achieves
state-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen
can infer occluded roads and predict roads missing from dataset annotations.
Our code will be released.

</details>


### [280] [GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition](https://arxiv.org/abs/2509.22276)
*Dinh Minh Nguyen,Malte Avenhaus,Thomas Lindemeier*

Main category: cs.CV

TL;DR: GS-2M integrates mesh reconstruction and material decomposition with 3D Gaussian Splatting, addressing reflective surfaces without external model priors.


<details>
  <summary>Details</summary>
Motivation: To unify and enhance the processes of mesh reconstruction and material decomposition by addressing challenges posed by reflective surfaces and reliance on external priors.

Method: A novel framework combining 3D Gaussian Splatting, joint optimization of attributes for depth and normals, roughness supervision using multi-view photometric variation, and a tailored loss/optimization process.

Result: The method achieves reconstruction results comparable to state-of-the-art approaches, providing triangle meshes and material components validated on standard datasets.

Conclusion: GS-2M delivers a scalable, simplified solution for reconstruction and decomposition, improving performance in reflective surface scenarios without reliance on neural components or external priors.

Abstract: We propose a unified solution for mesh reconstruction and material
decomposition from multi-view images based on 3D Gaussian Splatting, referred
to as GS-2M. Previous works handle these tasks separately and struggle to
reconstruct highly reflective surfaces, often relying on priors from external
models to enhance the decomposition results. Conversely, our method addresses
these two problems by jointly optimizing attributes relevant to the quality of
rendered depth and normals, maintaining geometric details while being resilient
to reflective surfaces. Although contemporary works effectively solve these
tasks together, they often employ sophisticated neural components to learn
scene properties, which hinders their performance at scale. To further
eliminate these neural components, we propose a novel roughness supervision
strategy based on multi-view photometric variation. When combined with a
carefully designed loss and optimization process, our unified framework
produces reconstruction results comparable to state-of-the-art methods,
delivering triangle meshes and their associated material components for
downstream tasks. We validate the effectiveness of our approach with widely
used datasets from previous works and qualitative comparisons with
state-of-the-art surface reconstruction methods.

</details>


### [281] [Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models](https://arxiv.org/abs/2509.22283)
*Michael Jungo,Andreas Fischer*

Main category: cs.CV

TL;DR: The paper explores the application of rule-based reinforcement learning (RL) to document image classification, focusing on generalization capabilities across different scenarios.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to investigate whether rule-based reinforcement learning, recognized for its reasoning and verifiable rewards, can enhance downstream tasks in document analysis like document image classification.

Method: The study evaluates rule-based reinforcement learning under three out-of-distribution scenarios: out-of-distribution images, unseen classes, and different modalities.

Result: The authors find that rule-based reinforcement learning exhibits improved generalization capabilities when dealing with out-of-distribution data.

Conclusion: Rule-based reinforcement learning proves to be a valuable approach for document image classification, offering enhanced generalization across various scenarios.

Abstract: Rule-based reinforcement learning has been gaining popularity ever since
DeepSeek-R1 has demonstrated its success through simple verifiable rewards. In
the domain of document analysis, reinforcement learning is not as prevalent,
even though many downstream tasks may benefit from the emerging properties of
reinforcement learning, particularly the enhanced reason capabilities. We study
the effects of rule-based reinforcement learning with the task of Document
Image Classification which is one of the most commonly studied downstream tasks
in document analysis. We find that reinforcement learning tends to have better
generalisation capabilities to out-of-distritbution data, which we examine in
three different scenarios, namely out-of-distribution images, unseen classes
and different modalities. Our code is available at
https://github.com/jungomi/vision-finetune.

</details>


### [282] [Jailbreaking on Text-to-Video Models via Scene Splitting Strategy](https://arxiv.org/abs/2509.22292)
*Wonjun Lee,Haon Park,Doehyeon Lee,Bumsub Ham,Suhyun Kim*

Main category: cs.CV

TL;DR: This study highlights safety concerns in Text-to-Video (T2V) models and introduces a novel jailbreak attack called SceneSplit that manipulates narrative structure to bypass safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address the safety risks and vulnerabilities in T2V models, an area largely unexplored compared to other generative models like LLMs, VLMs, and T2I models.

Method: Developed SceneSplit, a black-box jailbreak method that fragments harmful narratives into benign scenes, narrowing generative output space to unsafe regions. Iterative scene manipulation and a strategy library are used to increase attack success rates.

Result: SceneSplit achieved high Attack Success Rates (ASR) of 77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2 across 11 safety categories, outperforming existing baselines.

Conclusion: The study demonstrates the vulnerability of current T2V safety mechanisms to attacks exploiting narrative structures, emphasizing the need for improved safety in these models.

Abstract: Along with the rapid advancement of numerous Text-to-Video (T2V) models,
growing concerns have emerged regarding their safety risks. While recent
studies have explored vulnerabilities in models like LLMs, VLMs, and
Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely
unexplored, leaving a significant safety gap. To address this gap, we introduce
SceneSplit, a novel black-box jailbreak method that works by fragmenting a
harmful narrative into multiple scenes, each individually benign. This approach
manipulates the generative output space, the abstract set of all potential
video outputs for a given prompt, using the combination of scenes as a powerful
constraint to guide the final outcome. While each scene individually
corresponds to a wide and safe space where most outcomes are benign, their
sequential combination collectively restricts this space, narrowing it to an
unsafe region and significantly increasing the likelihood of generating a
harmful video. This core mechanism is further enhanced through iterative scene
manipulation, which bypasses the safety filter within this constrained unsafe
region. Additionally, a strategy library that reuses successful attack patterns
further improves the attack's overall effectiveness and robustness. To validate
our method, we evaluate SceneSplit across 11 safety categories on T2V models.
Our results show that it achieves a high average Attack Success Rate (ASR) of
77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly
outperforming the existing baseline. Through this work, we demonstrate that
current T2V safety mechanisms are vulnerable to attacks that exploit narrative
structure, providing new insights for understanding and improving the safety of
T2V models.

</details>


### [283] [HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models](https://arxiv.org/abs/2509.22300)
*Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiGS is a momentum-based sampling technique that enhances image quality and efficiency in diffusion models without requiring additional training or computation.


<details>
  <summary>Details</summary>
Motivation: Addressing issues like unrealistic outputs and lack of detail in current diffusion models, especially under limited NFEs and guidance scales.

Method: HiGS integrates a history-guided approach, using differences between current and past predictions to steer sampling, seamlessly integrating into existing diffusion frameworks.

Result: HiGS improved image quality across models and achieved a state-of-the-art FID of 1.61 for unguided ImageNet generation with only 30 steps.

Conclusion: HiGS is a plug-and-play technique that significantly enhances diffusion sampling efficiency and fidelity, requiring no extra training or resources.

Abstract: While diffusion models have made remarkable progress in image generation,
their outputs can still appear unrealistic and lack fine details, especially
when using fewer number of neural function evaluations (NFEs) or lower guidance
scales. To address this issue, we propose a novel momentum-based sampling
technique, termed history-guided sampling (HiGS), which enhances quality and
efficiency of diffusion sampling by integrating recent model predictions into
each inference step. Specifically, HiGS leverages the difference between the
current prediction and a weighted average of past predictions to steer the
sampling process toward more realistic outputs with better details and
structure. Our approach introduces practically no additional computation and
integrates seamlessly into existing diffusion frameworks, requiring neither
extra training nor fine-tuning. Extensive experiments show that HiGS
consistently improves image quality across diverse models and architectures and
under varying sampling budgets and guidance scales. Moreover, using a
pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for
unguided ImageNet generation at 256$\times$256 with only 30 sampling steps
(instead of the standard 250). We thus present HiGS as a plug-and-play
enhancement to standard diffusion sampling that enables faster generation with
higher fidelity.

</details>


### [284] [Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation](https://arxiv.org/abs/2509.22307)
*Jinpeng Lu,Linghan Cai,Yinda Chen,Guo Tang,Songhan Jiang,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: The paper introduces VeloxSeg, a lightweight dual-stream CNN-Transformer approach for 3D medical image segmentation, addressing efficiency and robustness issues in heterogenous and complex datasets.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between efficiency and robustness in lightweight 3D medical image segmentation, especially for complex anatomical structures and diverse imaging modalities.

Method: The proposed method, VeloxSeg, employs a dual-stream architecture combining CNN and Transformer with innovations like Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided convolution (JLC). It integrates Modal Interaction for multimodal data handling and uses Spatially Decoupled Knowledge Transfer (SDKT) with Gram matrices for enhanced texture prior injection.

Result: VeloxSeg demonstrates a 26% Dice score improvement, achieving significant efficiency gains with a 11x increase in GPU throughput and a 48x increase in CPU throughput on multimodal benchmarks.

Conclusion: VeloxSeg successfully overcomes the efficiency and robustness challenges of lightweight 3D medical segmentation, outperforming baselines in both accuracy and computational efficiency.

Abstract: Lightweight 3D medical image segmentation remains constrained by a
fundamental "efficiency / robustness conflict", particularly when processing
complex anatomical structures and heterogeneous modalities. In this paper, we
study how to redesign the framework based on the characteristics of
high-dimensional 3D images, and explore data synergy to overcome the fragile
representation of lightweight methods. Our approach, VeloxSeg, begins with a
deployable and extensible dual-stream CNN-Transformer architecture composed of
Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided
convolution (JLC). For each 3D image, we invoke a "glance-and-focus" principle,
where PWA rapidly retrieves multi-scale information, and JLC ensures robust
local feature extraction with minimal parameters, significantly enhancing the
model's ability to operate with low computational budget. Followed by an
extension of the dual-stream architecture that incorporates modal interaction
into the multi-scale image-retrieval process, VeloxSeg efficiently models
heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer
(SDKT) via Gram matrices injects the texture prior extracted by a
self-supervised network into the segmentation network, yielding stronger
representations than baselines at no extra inference cost. Experimental results
on multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement,
alongside increasing GPU throughput by 11x and CPU by 48x. Codes are available
at https://github.com/JinPLu/VeloxSeg.

</details>


### [285] [NIFTY: a Non-Local Image Flow Matching for Texture Synthesis](https://arxiv.org/abs/2509.22318)
*Pierrick Chatillon,Julien Rabin,David TschumperlÃ©*

Main category: cs.CV

TL;DR: The paper proposes NIFTY, a hybrid framework for exemplar-based texture synthesis, combining modern and classical techniques to overcome limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings in exemplar-based texture synthesis, such as poor initialization and visual artifacts.

Method: A non-parametric flow-matching model built on non-local patch matching, avoiding the need for neural network training.

Result: Experimental results show NIFTY's effectiveness compared to representative methods from the literature.

Conclusion: NIFTY successfully combines classical and recent methods for improved texture synthesis, providing an effective alternative without neural network reliance.

Abstract: This paper addresses the problem of exemplar-based texture synthesis. We
introduce NIFTY, a hybrid framework that combines recent insights on diffusion
models trained with convolutional neural networks, and classical patch-based
texture optimization techniques. NIFTY is a non-parametric flow-matching model
built on non-local patch matching, which avoids the need for neural network
training while alleviating common shortcomings of patch-based methods, such as
poor initialization or visual artifacts. Experimental results demonstrate the
effectiveness of the proposed approach compared to representative methods from
the literature. Code is available at https://github.com/PierrickCh/Nifty.git

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [286] [Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM](https://arxiv.org/abs/2509.21527)
*Mahesh Doijade,Andrey Alekseenko,Ania Brown,Alan Gray,SzilÃ¡rd PÃ¡ll*

Main category: cs.DC

TL;DR: The paper presents a GPU kernel-initiated redesign for the GROMACS molecular dynamics simulations, showcasing improved communication-computation overlap and strong-scaling performance.


<details>
  <summary>Details</summary>
Motivation: Molecular dynamics simulations require strong scaling for fixed-sized problems, but GROMACS, being highly latency-sensitive, faces scalability challenges on heterogeneous supercomputers due to MPI's CPU-centric design adding latencies and reducing GPU utilization.

Method: The paper introduces a GPU-resident NVSHMEM-based redesign for the GROMACS halo-exchange algorithm. It uses highly tuned GPU kernels for data packing and communication, employing hardware latency-hiding, kernel fusion, and the NVLink asynchronous copy engine to optimize performance.

Result: The proposed approach enhances GROMACS strong scaling performance by up to 1.5x intra-node and 2x multi-node on NVLink, achieving up to 1.3x improvement in multi-node scenarios using NVLink+InfiniBand.

Conclusion: GPU-initiated communication offers significant benefits for improving the scalability of latency-sensitive applications like GROMACS on modern supercomputing infrastructure.

Abstract: Improving time-to-solution in molecular dynamics simulations often requires
strong scaling due to fixed-sized problems. GROMACS is highly
latency-sensitive, with peak iteration rates in the sub-millisecond, making
scalability on heterogeneous supercomputers challenging. MPI's CPU-centric
nature introduces additional latencies on GPU-resident applications' critical
path, hindering GPU utilization and scalability. To address these limitations,
we present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain
decomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data
packing and communication, leveraging hardware latency-hiding for fine-grained
overlap. We employ kernel fusion across overlapped data forwarding
communication phases and utilize the asynchronous copy engine over NVLink to
optimize latency and bandwidth. Our GPU-resident formulation greatly increases
communication-computation overlap, improving GROMACS strong scaling performance
across NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x
multi-node over NVLink+InfiniBand. This demonstrates the profound benefits of
GPU-initiated communication for strong-scaling a broad range of
latency-sensitive applications.

</details>


### [287] [Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training](https://arxiv.org/abs/2509.21841)
*Chang Chen,Tiancheng Chen,Jiangfei Duan,Qianchao Zhu,Zerui Wang,Qinghao Hu,Peng Sun,Xiuhong Li,Chao Yang,Torsten Hoefler*

Main category: cs.DC

TL;DR: The paper introduces Zeppelin, a system designed to address inefficiencies in training large language models (LLMs) with varying sequence lengths, achieving a 2.80x speedup over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from inefficient load balancing and suboptimal computational/communication strategies in large-scale data-parallel training of LLMs with varying sequence lengths.

Method: Zeppelin integrates three techniques: hierarchical sequence partitioning for attention modules, a routing layer to optimize inter-node communication, and a remapping layer for efficient computation across attention and linear modules.

Result: Zeppelin achieves an average 2.80x speedup compared to state-of-the-art approaches due to improved resource utilization and system design.

Conclusion: Zeppelin significantly improves training efficiency for LLMs, addressing key scalability challenges and optimizing both computation and communication.

Abstract: Training large language models (LLMs) with increasingly long and varying
sequence lengths introduces severe load imbalance challenges in large-scale
data-parallel training. Recent frameworks attempt to mitigate these issues
through data reorganization or hybrid parallel strategies. However, they often
overlook how computational and communication costs scale with sequence length,
resulting in suboptimal performance. We identify three critical challenges: (1)
varying computation-to-communication ratios across sequences of different
lengths in distributed attention, (2) mismatch between static NIC-GPU affinity
and dynamic parallel workloads, and (3) distinct optimal partitioning
strategies required for quadratic attention versus linear components. To
address these challenges, we present Zeppelin, a novel training system that
integrates three key techniques: (1) a hierarchical sequence partitioning
method for the attention module that reduces communication overhead and
balances computation, supported by an efficient attention engine that applies
divergent parallel strategies; (2) a routing layer that orchestrates inter-node
transfers to fully utilize NIC bandwidth; and (3) a remapping layer that
transforms sequence layouts between attention and linear modules, ensuring high
computational efficiency across both. Comprehensive evaluations across diverse
configurations show that Zeppelin delivers an average 2.80x speedup over
state-of-the-art methods.

</details>


### [288] [Code once, Run Green: Automated Green Code Translation in Serverless Computing](https://arxiv.org/abs/2509.22068)
*Sebastian Werner,Mathis KÃ¤hler,Alireza Hakamian*

Main category: cs.DC

TL;DR: This paper explores using large language models (LLMs) to translate serverless functions into more energy-efficient programming languages to reduce energy consumption.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the issue of persistent energy debt in computing systems arising from legacy architectural and implementation decisions.

Method: The authors design ReFaaS, integrate it into the Fission serverless framework, and evaluate LLMs for code translations to reduce energy use.

Result: Preliminary results show up to 70% energy reduction per invocation, with net savings achieved after 3,000â€“5,000 invocations, depending on the LLM used.

Conclusion: Though promising, challenges remain in function suitability, amortization thresholds, and feasibility, indicating future research directions.

Abstract: The rapid digitization and the increasing use of emerging technologies such
as AI models have significantly contributed to the emissions of computing
infrastructure. Efforts to mitigate this impact typically focus on the
infrastructure level such as powering data centers with renewable energy, or
through the specific design of energy-efficient software. However, both
strategies rely on stakeholder intervention, making their adoption in legacy
and already-deployed systems unlikely. As a result, past architectural and
implementation decisions continue to incur additional energy usage - a
phenomenon we refer to as energy debt.
  Hence, in this paper, we investigate the potential of serverless computing
platforms to automatically reduce energy debt by leveraging the unique access
to function source code. Specifically, we explore whether large language models
(LLMs) can translate serverless functions into more energy-efficient
programming languages while preserving functional correctness. To this end, we
design and implement ReFaaS and integrate it into the Fission serverless
framework. We evaluate multiple LLMs on their ability to perform such code
translations and analyze their impact on energy consumption.
  Our preliminary results indicate that translated functions can reduce
invocation energy by up to 70%, achieving net energy savings after
approximately 3,000 to 5,000 invocations, depending on the LLM used.
Nonetheless, the approach faces several challenges: not all functions are
suitable for translation, and for some, the amortization threshold is
significantly higher or unreachable. Despite these limitations, we identify
four key research challenges whose resolution could unlock long-term, automated
mitigation of energy debt in serverless computing.

</details>


### [289] [The AI_INFN Platform: Artificial Intelligence Development in the Cloud](https://arxiv.org/abs/2509.22117)
*Lucio Anderlini,Giulio Bianchini,Diego Ciangottini,Stefano Dal Pra,Diego Michelotto,Rosa Petrini,Daniele Spiga*

Main category: cs.DC

TL;DR: The paper discusses the AI_INFN project, its Kubernetes platform, and its role in facilitating effective use of GPU-powered ML workflows for INFN use cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of adopting machine learning in scientific software development, particularly regarding resource allocation and hardware accelerators.

Method: It establishes a Kubernetes platform that integrates tools like Virtual Kubelet and InterLink API to manage GPU-powered workflows across distributed computing resources.

Result: The computing infrastructure supports distributed resource providers, including large supercomputers, and has shown promising initial test results and case studies.

Conclusion: The Kubernetes-based setup demonstrates scalability and efficiency in managing AI workloads, serving as a model for other use cases needing specialized infrastructures.

Abstract: Machine Learning (ML) is driving a revolution in the way scientists design,
develop, and deploy data-intensive software. However, the adoption of ML
presents new challenges for the computing infrastructure, particularly in terms
of provisioning and orchestrating access to hardware accelerators for
development, testing, and production. The INFN-funded project AI_INFN
(Artificial Intelligence at INFN) aims at fostering the adoption of ML
techniques within INFN use cases by providing support on multiple aspects,
including the provisioning of AI-tailored computing resources. It leverages
cloud-native solutions in the context of INFN Cloud, to share hardware
accelerators as effectively as possible, ensuring the diversity of the
Institute's research activities is not compromised. In this contribution, we
provide an update on the commissioning of a Kubernetes platform designed to
ease the development of GPU-powered data analysis workflows and their
scalability on heterogeneous distributed computing resources, also using the
offloading mechanism with Virtual Kubelet and InterLink API. This setup can
manage workflows across different resource providers, including sites of the
Worldwide LHC Computing Grid and supercomputers such as CINECA Leonardo,
providing a model for use cases requiring dedicated infrastructures for
different parts of the workload. Initial test results, emerging case studies,
and integration scenarios will be presented with functional tests and
benchmarks.

</details>


### [290] [Orientation does not help with 3-coloring a grid in online-LOCAL](https://arxiv.org/abs/2509.22233)
*Thomas Boudier,Filippo Casagrande,Avinandan Das,Massimo Equi,Henrik Lievonen,Augusto Modanese,Ronja Stimpert*

Main category: cs.DC

TL;DR: The paper establishes a lower bound of Î©(log n) locality for 3-coloring in deterministic and randomized online-LOCAL models, even when given a globally consistent orientation of the grid.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to generalize recent results on the online-LOCAL and SLOCAL models for 3-coloring grids, by removing the assumption that algorithms lack access to grid orientation.

Method: The authors develop a novel proof technique to extend the 3-coloring lower bound results to cases where algorithms are provided with a globally consistent grid orientation.

Result: The proof demonstrates that even with orientation access, the Î©(log n) locality lower bound still holds for 3-coloring in online-LOCAL and SLOCAL models.

Conclusion: Access to grid orientation does not reduce the locality lower bound for 3-coloring, strengthening the robustness of recent complexity results in online-LOCAL and SLOCAL models.

Abstract: The online-LOCAL and SLOCAL models are extensions of the LOCAL model where
nodes are processed in a sequential but potentially adversarial order. So far,
the only problem we know of where the global memory of the online-LOCAL model
has an advantage over SLOCAL is 3-coloring bipartite graphs. Recently, Chang et
al. [PODC 2024] showed that even in grids, 3-coloring requires $\Omega(\log n)$
locality in deterministic online-LOCAL. This result was subsequently extended
by Akbari et al. [STOC 2025] to also hold in randomized online-LOCAL. However,
both proofs heavily rely on the assumption that the algorithm does not have
access to the orientation of the underlying grid. In this paper, we show how to
lift this requirement and obtain the same lower bound (against either model)
even when the algorithm is explicitly given a globally consistent orientation
of the grid.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [291] [Discovering and Analyzing Stochastic Processes to Reduce Waste in Food Retail](https://arxiv.org/abs/2509.21322)
*Anna Kalenkova,Lu Xia,Dirk Neumann*

Main category: cs.LG

TL;DR: This paper introduces a method integrating object-centric process mining and stochastic analysis to address food waste in retail.


<details>
  <summary>Details</summary>
Motivation: To reduce food waste and product shortages in food retail by balancing customer purchasing behaviors and supply strategies.

Method: The paper uses continuous-time Markov chains to model sales data, extends the model with supply activities, and conducts a what-if analysis to evaluate supply-demand dynamics.

Result: The approach identifies optimal strategies to balance supply and demand, ensuring reduced food waste and preventing shortages.

Conclusion: Integrating process mining and stochastic analysis provides an effective solution to optimize food retail strategies, potentially reducing waste and shortages.

Abstract: This paper proposes a novel method for analyzing food retail processes with a
focus on reducing food waste. The approach integrates object-centric process
mining (OCPM) with stochastic process discovery and analysis. First, a
stochastic process in the form of a continuous-time Markov chain is discovered
from grocery store sales data. This model is then extended with supply
activities. Finally, a what-if analysis is conducted to evaluate how the
quantity of products in the store evolves over time. This enables the
identification of an optimal balance between customer purchasing behavior and
supply strategies, helping to prevent both food waste due to oversupply and
product shortages.

</details>


### [292] [Impact of Loss Weight and Model Complexity on Physics-Informed Neural Networks for Computational Fluid Dynamics](https://arxiv.org/abs/2509.21393)
*Yi En Chou,Te Hsin Liu,Chao An Lin*

Main category: cs.LG

TL;DR: This paper introduces improved weighting schemes to enhance the performance of Physics-Informed Neural Networks (PINNs) in solving partial differential equations (PDEs).


<details>
  <summary>Details</summary>
Motivation: To address the sensitivity of Physics-Informed Neural Networks (PINNs) to loss weight selection, which impacts their stability and accuracy when solving PDEs.

Method: Two-dimensional analysis-based weighting schemes were proposed: one based on quantifiable terms and another incorporating unquantifiable terms to balance training.

Result: The second weighting scheme improves stability and accuracy across benchmarks, particularly outperforming in high Peclet number convection diffusion scenarios, where other solvers fail.

Conclusion: The proposed weighting schemes offer robust and generalizable solutions for solving computational fluid dynamics (CFD) problems using PINNs.

Abstract: Physics Informed Neural Networks offer a mesh free framework for solving PDEs
but are highly sensitive to loss weight selection. We propose two dimensional
analysis based weighting schemes, one based on quantifiable terms, and another
also incorporating unquantifiable terms for more balanced training. Benchmarks
on heat conduction, convection diffusion, and lid driven cavity flows show that
the second scheme consistently improves stability and accuracy over equal
weighting. Notably, in high Peclet number convection diffusion, where
traditional solvers fail, PINNs with our scheme achieve stable, accurate
predictions, highlighting their robustness and generalizability in CFD
problems.

</details>


### [293] [Fast-Forward Lattice Boltzmann: Learning Kinetic Behaviour with Physics-Informed Neural Operators](https://arxiv.org/abs/2509.22411)
*Xiao Xue,Marco F. P. ten Eikelder,Mingyang Gao,Xiaoyuan Cheng,Yiming Yang,Yi He,Shuo Wang,Sibo Cheng,Yukun Hu,Peter V. Coveney*

Main category: cs.LG

TL;DR: This paper presents a physics-informed neural operator framework to solve the lattice Boltzmann equation (LBE) for complex flow dynamics, reducing computational complexity and enabling generalization across scales and scenarios.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational intensity of solving the LBE due to strict time-step restrictions imposed by collision kernels.

Method: The paper introduces a physics-informed neural operator framework that incorporates intrinsic moment-matching constraints of the LBE and global equivariance to predict over large time horizons, bypassing step-by-step integration.

Result: The proposed framework demonstrates high robustness across diverse and complex flow scenarios such as von Karman vortex shedding, ligament breakup, and bubble adhesion, with generalization capabilities across different resolutions and kinetic datasets.

Conclusion: The approach enables a data-driven, computationally efficient way to model complex kinetic systems, applicable universally across various dynamics.

Abstract: The lattice Boltzmann equation (LBE), rooted in kinetic theory, provides a
powerful framework for capturing complex flow behaviour by describing the
evolution of single-particle distribution functions (PDFs). Despite its
success, solving the LBE numerically remains computationally intensive due to
strict time-step restrictions imposed by collision kernels. Here, we introduce
a physics-informed neural operator framework for the LBE that enables
prediction over large time horizons without step-by-step integration,
effectively bypassing the need to explicitly solve the collision kernel. We
incorporate intrinsic moment-matching constraints of the LBE, along with global
equivariance of the full distribution field, enabling the model to capture the
complex dynamics of the underlying kinetic system. Our framework is
discretization-invariant, enabling models trained on coarse lattices to
generalise to finer ones (kinetic super-resolution). In addition, it is
agnostic to the specific form of the underlying collision model, which makes it
naturally applicable across different kinetic datasets regardless of the
governing dynamics. Our results demonstrate robustness across complex flow
scenarios, including von Karman vortex shedding, ligament breakup, and bubble
adhesion. This establishes a new data-driven pathway for modelling kinetic
systems.

</details>


### [294] [LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?](https://arxiv.org/abs/2509.21403)
*Rushil Gupta,Jason Hartford,Bang Liu*

Main category: cs.LG

TL;DR: The paper evaluates whether LLMs can perform in-context experimental design and finds they lack sensitivity to feedback, are outperformed by classical methods, but can be improved with a hybrid LLM-guided approach.


<details>
  <summary>Details</summary>
Motivation: To investigate claims that large language models (LLMs) can be effectively used as general-purpose agents for in-context experimental design in tasks such as genetic perturbation and molecular property discovery.

Method: The authors evaluate both open- and closed-source instruction-tuned LLMs on experimental design tasks. They compare these agents against classical methods like linear bandits and Gaussian process optimization, introducing a hybrid approach, LLM-guided Nearest Neighbour (LLMNN) sampling, which incorporates LLM knowledge into experimental design.

Result: LLM-based agents lack sensitivity to experimental feedback, as randomizing outcome labels has no impact on performance. Classical methods consistently outperform LLM agents, while the proposed LLMNN approach achieves competitive or better results across tasks without requiring complex adaptation.

Conclusion: Current LLMs do not practically implement in-context experimental design, indicating a need for hybrid frameworks that use LLMs for prior-based reasoning while relying on other methods for adaptive experimentation.

Abstract: Large language models (LLMs) have recently been proposed as general-purpose
agents for experimental design, with claims that they can perform in-context
experimental design. We evaluate this hypothesis using both open- and
closed-source instruction-tuned LLMs applied to genetic perturbation and
molecular property discovery tasks. We find that LLM-based agents show no
sensitivity to experimental feedback: replacing true outcomes with randomly
permuted labels has no impact on performance. Across benchmarks, classical
methods such as linear bandits and Gaussian process optimization consistently
outperform LLM agents. We further propose a simple hybrid method, LLM-guided
Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with
nearest-neighbor sampling to guide the design of experiments. LLMNN achieves
competitive or superior performance across domains without requiring
significant in-context adaptation. These results suggest that current open- and
closed-source LLMs do not perform in-context experimental design in practice
and highlight the need for hybrid frameworks that decouple prior-based
reasoning from batch acquisition with updated posteriors.

</details>


### [295] [PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters](https://arxiv.org/abs/2509.21619)
*Krishu K Thapa,Reet Barik,Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: The paper proposes an approach to reduce training resources by switching from full parameter training to Low-Rank Adaptation (LoRA) at stages of partial convergence, significantly improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Training large models with millions to billions of parameters requires extensive computational resources, and the paper aims to develop methods to optimize this process.

Method: An adaptive approach is introduced to identify partial convergence during training and dynamically switch to Low-Rank Adaptation (LoRA) using user-defined hyperparameters, assigning module-specific ranks.

Result: The method preserved model accuracy while reducing trainable parameters by 90%, improving throughput by 3x, lowering training time per epoch by 1.5x, and reducing GPU memory consumption by 20%.

Conclusion: The proposed dynamic switching approach enhances training efficiency without compromising model performance, establishing it as a significant optimization for large parameter models.

Abstract: Training large models ranging from millions to billions of parameters is
highly resource-intensive, requiring significant time, compute, and memory. It
is observed that most of the learning (higher change in weights) takes place in
the earlier stage of the training loop. These changes stabilize as training
continues, enabling them to be captured by matrices of a low intrinsic rank.
Therefore, we propose an approach to identify such states of partial
convergence and dynamically switch from full parameter training to Low-Rank
Adaptation (LoRA) on the ViT-Large model. We introduce a flexible approach that
leverages user-defined hyperparameters to determine the switching point and
assign a rank specific to each module layer based on its level of convergence.
Experimental results show that this approach preserves model accuracy while
reducing the number of trainable parameters to 10% of its original size,
resulting in a 3x improvement in throughput, and a 1.5x reduction in average
training time per epoch while also reducing GPU memory consumption by 20%

</details>


### [296] [Object Identification Under Known Dynamics: A PIRNN Approach for UAV Classification](https://arxiv.org/abs/2509.21405)
*Nyi Nyi Aung,Neil Muralles,Adrian Stein*

Main category: cs.LG

TL;DR: This paper introduces a physics-informed neural network framework for object identification in UAVs with known dynamics, achieving high accuracy and reduced training time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve object identification accuracy and efficiency in UAV applications by leveraging known dynamics through a physics-informed approach.

Method: A physics-informed residual neural network is used for state mapping and derivative prediction combined with a softmax layer for confidence estimation.

Result: Case studies involving quadcopters, fixed-wing, and helicopters show high classification accuracy and reduced training time.

Conclusion: The framework serves as an effective solution to system identification tasks, especially when dynamics are well understood, enhancing UAV applications.

Abstract: This work addresses object identification under known dynamics in unmanned
aerial vehicle applications, where learning and classification are combined
through a physics-informed residual neural network. The proposed framework
leverages physics-informed learning for state mapping and state-derivative
prediction, while a softmax layer enables multi-class confidence estimation.
Quadcopter, fixed-wing, and helicopter aerial vehicles are considered as case
studies. The results demonstrate high classification accuracy with reduced
training time, offering a promising solution for system identification problems
in domains where the underlying dynamics are well understood.

</details>


### [297] [Neuroprobe: Evaluating Intracranial Brain Responses to Naturalistic Stimuli](https://arxiv.org/abs/2509.21671)
*Andrii Zahorodnii,Christopher Wang,Bennett Stankovits,Charikleia Moraitaki,Geeling Chau,Andrei Barbu,Boris Katz,Ila R Fiete*

Main category: cs.LG

TL;DR: Neuroprobe is a benchmark suite for iEEG recordings that facilitates the study of multi-modal language processing in the brain and enables model comparisons. It is built on the BrainTreebank dataset.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of standardized evaluation frameworks for intracranial EEG (iEEG) recordings, which are critical for developing brain-computer interfaces and neurological treatments.

Method: Neuroprobe utilizes the BrainTreebank dataset, featuring 40 hours of iEEG data from 10 subjects during a naturalistic movie-viewing task, and offers tasks to study language processing in the brain.

Result: Using Neuroprobe, the study visualizes information flow between brain regions like the superior temporal gyrus and the prefrontal cortex while highlighting the progression from auditory to language features. Unexpectedly, linear baselines perform better than advanced models on some tasks.

Conclusion: Neuroprobe provides a systematic and computationally efficient framework for advancing neuroscience insights and comparing neural foundation models. It is openly available to accelerate progress in iEEG research.

Abstract: High-resolution neural datasets enable foundation models for the next
generation of brain-computer interfaces and neurological treatments. The
community requires rigorous benchmarks to discriminate between competing
modeling approaches, yet no standardized evaluation frameworks exist for
intracranial EEG (iEEG) recordings. To address this gap, we present Neuroprobe:
a suite of decoding tasks for studying multi-modal language processing in the
brain. Unlike scalp EEG, intracranial EEG requires invasive surgery to implant
electrodes that record neural activity directly from the brain with minimal
signal distortion. Neuroprobe is built on the BrainTreebank dataset, which
consists of 40 hours of iEEG recordings from 10 human subjects performing a
naturalistic movie viewing task. Neuroprobe serves two critical functions.
First, it is a mine from which neuroscience insights can be drawn. Its high
temporal and spatial resolution allows researchers to systematically determine
when and where computations for each aspect of language processing occur in the
brain by measuring the decodability of each feature across time and all
electrode locations. Using Neuroprobe, we visualize how information flows from
the superior temporal gyrus to the prefrontal cortex, and the progression from
simple auditory features to more complex language features in a purely
data-driven manner. Second, as the field moves toward neural foundation models,
Neuroprobe provides a rigorous framework for comparing competing architectures
and training protocols. We found that the linear baseline is surprisingly
strong, beating frontier foundation models on many tasks. Neuroprobe is
designed with computational efficiency and ease of use in mind. We make the
code for Neuroprobe openly available and maintain a public leaderboard, aiming
to enable rapid progress in the field of iEEG foundation models, at
https://neuroprobe.dev/

</details>


### [298] [Null-Space Filtering for Data-Free Continual Model Merging: Preserving Transparency, Promoting Fidelity](https://arxiv.org/abs/2509.21413)
*Zihuan Qiu,Lei Wang,Yang Cao,Runtong Zhang,Bing Su,Yi Xu,Fanman Meng,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: The paper introduces NUFILT, a data-free continual model merging framework, aiming to fuse models for evolving tasks while maintaining transparency and fidelity, achieving state-of-the-art results in vision and NLP benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current continual model merging approaches fail to balance transparency and fidelity in the absence of task data, prompting the need for a method that effectively aligns data-level desiderata with parameter-space optimization.

Method: NUFILT uses a null-space projector to filter overlapping components, ensuring transparency, and a lightweight LoRA adapter to inject new task-specific signals for fidelity. It employs projection-based surrogate loss for training and fuses updates in a layer-wise linear fashion without extra inference cost.

Result: NUFILT outperforms existing methods like OPCM and WUDI-Merging by improving average accuracy by 4-7%. It achieves minimal forgetting across vision and NLP benchmarks while reducing computational overhead.

Conclusion: NUFILT effectively addresses transparency and fidelity challenges in data-free continual model merging, providing a theoretical foundation and demonstrating empirical success.

Abstract: Data-free continual model merging (DFCMM) aims to fuse independently
fine-tuned models into a single backbone that evolves with incoming tasks
without accessing task data. This paper formulate two fundamental desiderata
for DFCMM: transparency, avoiding interference with earlier tasks, and
fidelity, adapting faithfully to each new task. This poses a challenge that
existing approaches fail to address: how to bridge data-level desiderata with
parameter-space optimization to ensure transparency and fidelity in the absence
of task data. To this end, we propose NUFILT (NUll-space FILTering), a
data-free framework that directly links these desiderata to optimization. Our
key observation is that task vectors approximately align with representation
subspaces, providing structural surrogates for enforcing transparency and
fidelity. Accordingly, we design a null-space projector that preserves prior
responses by filtering out overlapping components of new task vectors, thereby
ensuring transparency, and a lightweight LoRA adapter that injects
complementary task-specific signals, enabling fidelity in adapting to new
tasks. The adapter is trained with a projection-based surrogate loss to retain
consistency with previous knowledge while introducing novel directions. This
joint filtering-adaptation process allows the backbone to absorb new knowledge
while retaining existing behaviors, and the updates are finally fused back in a
layer-wise linear fashion without extra parameters or inference cost.
Theoretically, we establish approximate subspace alignment guarantees that
justify null-space filtering. Empirically, NUFILT achieves state-of-the-art
performance with minimal forgetting on both vision and NLP benchmarks,
improving average accuracy by 4-7% over OPCM and WUDI-Merging, while narrowing
the gap to fine-tuning and reducing computation overhead.

</details>


### [299] [Role-Aware Multi-modal federated learning system for detecting phishing webpages](https://arxiv.org/abs/2509.22369)
*Bo Wang,Imran Khan,Martin White,Natalia Beloff*

Main category: cs.LG

TL;DR: This paper presents a federated, multi-modal phishing website detector capable of handling URL, HTML, and IMAGE inputs across clients under privacy-preserving settings. It reports high detection accuracy and low false positive rates using a novel training approach.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of building a flexible, robust, and privacy-preserving phishing website detector that works across multiple data types (e.g., URL, HTML, and IMAGE).

Method: The authors propose "role-aware bucket aggregation" with hard gating on top of FedProx for federated learning. This technique aggregates modality-specific parameters separately and avoids cross-embedding conflicts, ensuring stable convergence without the need for learnable routing.

Result: The detector achieves high performance, with accuracy of 97.5% and FPR of 2.4% on multi-modal data combining TR-OP URL and HTML inputs. Subset evaluations achieve image-specific accuracy of 95.5% (FPR 5.9%) and HTML-specific accuracy of 96.5% (FPR 1.8%) on different datasets, indicating the robustness of this method.

Conclusion: The proposed framework enables stable training in federated settings while maintaining high accuracy and flexibility in phishing website detection. This highlights its practicality for privacy-preserving multi-modal applications.

Abstract: We present a federated, multi-modal phishing website detector that supports
URL, HTML, and IMAGE inputs without binding clients to a fixed modality at
inference: any client can invoke any modality head trained elsewhere.
Methodologically, we propose role-aware bucket aggregation on top of FedProx,
inspired by Mixture-of-Experts and FedMM. We drop learnable routing and use
hard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling
separate aggregation of modality-specific parameters to isolate cross-embedding
conflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc
97.5% with FPR 2.4% across two data types; on the image subset (ablation) it
attains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an
early three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc
96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results
indicate that bucket aggregation with hard-gated experts enables stable
federated training under strict privacy, while improving the usability and
flexibility of multi-modal phishing detection.

</details>


### [300] [Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope](https://arxiv.org/abs/2509.21446)
*Waleed Esmail,Alexander Kappes,Stuart Russell,Christine Thomas*

Main category: cs.LG

TL;DR: SeismoGPT is a transformer model for predicting seismic waveforms, with potential applications in gravitational wave detectors.


<details>
  <summary>Details</summary>
Motivation: To enable accurate forecasting of seismic activity to assist in future gravitational wave observatory operations, especially for mitigating Newtonian noise.

Method: The paper presents a transformer-based, autoregressive model trained on waveform data, capable of single-station and array inputs, leveraging temporal and spatial dependencies for prediction.

Result: SeismoGPT demonstrates accurate short-term seismic forecasts, capturing realistic ground motion, with expected performance decay over longer prediction windows.

Conclusion: The method shows promise for data-driven seismic forecasting and applications in real-time observatory control for gravitational wave detection.

Abstract: We introduce \textit{SeismoGPT}, a transformer-based model for forecasting
three-component seismic waveforms in the context of future gravitational wave
detectors like the Einstein Telescope. The model is trained in an
autoregressive setting and can operate on both single-station and array-based
inputs. By learning temporal and spatial dependencies directly from waveform
data, SeismoGPT captures realistic ground motion patterns and provides accurate
short-term forecasts. Our results show that the model performs well within the
immediate prediction window and gradually degrades further ahead, as expected
in autoregressive systems. This approach lays the groundwork for data-driven
seismic forecasting that could support Newtonian noise mitigation and real-time
observatory control.

</details>


### [301] [Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data](https://arxiv.org/abs/2509.21465)
*George Yakushev,Alina Shutova,Ivan Rubachev,Renat Sergazinov,Artem Babenko*

Main category: cs.LG

TL;DR: The paper proposes using reasoning-capable large language models (LLMs) to create decision trees for small tabular datasets, enhancing explainability and enabling human intervention.


<details>
  <summary>Details</summary>
Motivation: To address difficulties in interpretability, reliance on large synthetic datasets, and high inference costs of tabular foundation models, especially in low-resource tabular problems.

Method: The authors design tools that enable LLMs to construct, analyze, and manipulate decision trees by combining prior knowledge with learning from small tabular datasets in an agentic setup.

Result: The decision trees generated by LLMs outperform traditional decision tree methods (CART) on low-resource tabular tasks. Although not surpassing black-box models in accuracy, they offer human-readable reasoning traces for bias and data leak checks.

Conclusion: This method provides a lightweight and interpretable alternative to black-box models for low-resource tabular problems, with the added benefit of integrating human input to improve decision accuracy and incorporate domain knowledge.

Abstract: Tabular foundation models are becoming increasingly popular for low-resource
tabular problems. These models make up for small training datasets by
pretraining on large volumes of synthetic data. The prior knowledge obtained
via pretraining provides the exceptional performance, but the resulting model
becomes a black box that is difficult to interpret and costly to inference. In
this work, we explore an alternative strategy: using reasoning-capable LLMs to
induce decision trees for small tabular datasets in agentic setup. We design a
minimal set of tools for constructing, analyzing and manipulating decision
trees. By using these tools, LLMs combine their prior knowledge with learning
from data to create a lightweight decision tree that outperforms traditional
CART on low-resource tabular problems. While a single decision tree does not
outperform state-of-the-art black box models, it comes with a human-readable
reasoning trace that can be checked for biases and data leaks. Furthermore, the
reasoning-based LLM's creation process allows for additional human input:
correcting biases or incorporating domain-specific intuition that is not
captured in the data.

</details>


### [302] [Score-based Idempotent Distillation of Diffusion Models](https://arxiv.org/abs/2509.21470)
*Shehtab Zaman,Chengyan Liu,Kenneth Chiu*

Main category: cs.LG

TL;DR: The paper introduces SIGNs, which combine diffusion models and Idempotent Generative Networks (IGNs) to achieve faster and more stable generative modeling.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost and stability challenges in existing generative models like GANs and diffusion models, by combining their strengths into a new approach.

Method: SIGNs are derived by distilling IGNs from pre-trained diffusion models, leveraging score-based training and achieving stability without adversarial training. They can perform multi-step sampling and zero-shot editing.

Result: The proposed method achieves state-of-the-art results in idempotent models on image datasets like CIFAR and CelebA, showing both efficiency and high quality.

Conclusion: SIGNs enable faster inference and flexible sampling while maintaining stability and high performance; thus, they offer a practical alternative to current generative modeling approaches.

Abstract: Idempotent generative networks (IGNs) are a new line of generative models
based on idempotent mapping to a target manifold. IGNs support both single-and
multi-step generation, allowing for a flexible trade-off between computational
cost and sample quality. But similar to Generative Adversarial Networks (GANs),
conventional IGNs require adversarial training and are prone to training
instabilities and mode collapse. Diffusion and score-based models are popular
approaches to generative modeling that iteratively transport samples from one
distribution, usually a Gaussian, to a target data distribution. These models
have gained popularity due to their stable training dynamics and high-fidelity
generation quality. However, this stability and quality come at the cost of
high computational cost, as the data must be transported incrementally along
the entire trajectory. New sampling methods, model distillation, and
consistency models have been developed to reduce the sampling cost and even
perform one-shot sampling from diffusion models. In this work, we unite
diffusion and IGNs by distilling idempotent models from diffusion model scores,
called SIGN. Our proposed method is highly stable and does not require
adversarial losses. We provide a theoretical analysis of our proposed
score-based training methods and empirically show that IGNs can be effectively
distilled from a pre-trained diffusion model, enabling faster inference than
iterative score-based models. SIGNs can perform multi-step sampling, allowing
users to trade off quality for efficiency. These models operate directly on the
source domain; they can project corrupted or alternate distributions back onto
the target manifold, enabling zero-shot editing of inputs. We validate our
models on multiple image datasets, achieving state-of-the-art results for
idempotent models on the CIFAR and CelebA datasets.

</details>


### [303] [LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning](https://arxiv.org/abs/2509.21617)
*Marco Paul E. Apolinario,Kaushik Roy*

Main category: cs.LG

TL;DR: LANCE introduces a low-rank activation compression framework using higher-order SVD to reduce memory/computation costs for on-device fine-tuning and continual learning, maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable efficient on-device learning for personalization, privacy, and adaptability while addressing the challenges of high memory costs and catastrophic forgetting during fine-tuning and continual learning.

Method: LANCE employs a one-shot higher-order Singular Value Decomposition (SVD) to create reusable low-rank subspaces for activation projection, minimizing computational overhead and supporting continual learning with orthogonal subspace allocation.

Result: LANCE achieves up to 250Ã— reduction in activation storage while preserving model accuracy for tasks on datasets like CIFAR-10/100, Oxford-IIIT Pets, and others. For continual learning benchmarks, it delivers competitive performance with reduced memory cost.

Conclusion: LANCE provides a scalable and memory-efficient method for on-device learning, making it suitable for edge devices and a significant step forward in handling fine-tuning and continual learning challenges.

Abstract: On-device learning is essential for personalization, privacy, and long-term
adaptation in resource-constrained environments. Achieving this requires
efficient learning, both fine-tuning existing models and continually acquiring
new tasks without catastrophic forgetting. Yet both settings are constrained by
high memory cost of storing activations during backpropagation. Existing
activation compression methods reduce this cost but relying on repeated
low-rank decompositions, introducing computational overhead. Also, such methods
have not been explored for continual learning. We propose LANCE (Low-rank
Activation Compression), a framework that performs one-shot higher-order
Singular Value Decompsoition (SVD) to obtain a reusable low-rank subspace for
activation projection. This eliminates repeated decompositions, reducing both
memory and computation. Moreover, fixed low-rank subspaces further enable
on-device continual learning by allocating tasks to orthogonal subspaces
without storing large task-specific matrices. Experiments show that LANCE
reduces activation storage up to 250$\times$ while maintaining accuracy
comparable to full backpropagation on CIFAR-10/100, Oxford-IIIT Pets,
Flowers102, and CUB-200 datasets. On continual learning benchmarks (Split
CIFAR-100, Split MiniImageNet, 5-Datasets), it achieves performance competitive
with orthogonal gradient projection methods at a fraction of the memory cost.
These results position LANCE as a practical and scalable solution for efficient
fine-tuning and continual learning on edge devices.

</details>


### [304] [Are Hallucinations Bad Estimations?](https://arxiv.org/abs/2509.21473)
*Hude Liu,Jerry Yao-Chieh Hu,Jennifer Yuntong Zhang,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: This paper investigates hallucinations in generative models and shows they occur even in loss-minimizing optimal estimators due to structural misalignment between loss minimization and human-acceptable outputs.


<details>
  <summary>Details</summary>
Motivation: To address the persistent issue of hallucinations in generative models, which compromise the reliability of their outputs, and to offer a theoretical foundation explaining why these phenomena occur.

Method: The authors formalize hallucinations as failures to link estimates to plausible causes, derive a high-probability lower bound for hallucinate rates across generic data distributions, and conduct experiments in coin aggregation, open-ended QA, and text-to-image tasks.

Result: They uncover that hallucinations are intrinsic to loss-minimizing estimators and stem from miscalibration issues, wherever structural misalignments exist between loss metrics and human expectations.

Conclusion: Hallucinations arise as an inherent issue from the misalignment of estimation objectives with human requirements, emphasizing the need for recalibration to improve generative model outputs.

Abstract: We formalize hallucinations in generative models as failures to link an
estimate to any plausible cause. Under this interpretation, we show that even
loss-minimizing optimal estimators still hallucinate. We confirm this with a
general high probability lower bound on hallucinate rate for generic data
distributions. This reframes hallucination as structural misalignment between
loss minimization and human-acceptable outputs, and hence estimation errors
induced by miscalibration. Experiments on coin aggregation, open-ended QA, and
text-to-image support our theory.

</details>


### [305] [d2: Improved Techniques for Training Reasoning Diffusion Language Models](https://arxiv.org/abs/2509.21474)
*Guanghan Wang,Yair Schiff,Gilad Turok,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: This paper introduces "d2," a reasoning framework for masked Diffusion Language Models (DLMs) that uses a new policy gradient algorithm to enhance reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: While DLMs excel in text generation, their reasoning capabilities, particularly through reinforcement learning, require improvement.

Method: The proposed framework relies on a new policy gradient algorithm that leverages masking properties for accurate trajectory likelihood estimation. Computational-efficiency versus accuracy trade-offs are addressed in a structured manner.

Result: The framework achieves state-of-the-art results in logical reasoning tasks like Countdown and Sudoku, as well as math reasoning benchmarks GSM8K and MATH500, without supervised fine-tuning.

Conclusion: The d2 framework effectively improves reasoning in DLMs using reinforcement learning and highlights the importance of any-order likelihood estimation for efficient reasoning.

Abstract: While diffusion language models (DLMs) have achieved competitive performance
in text generation, improving their reasoning ability with reinforcement
learning remains an active research area. Here, we introduce d2, a reasoning
framework tailored for masked DLMs. Central to our framework is a new policy
gradient algorithm that relies on properties of masking to accurately estimate
the likelihoods of sampling trajectories. Our estimators trade off computation
for approximation accuracy in an analytically tractable manner, and are
particularly effective for DLMs that support any-order likelihood estimation.
We characterize and study this property in popular DLMs and show that it is key
for efficient diffusion-based reasoning. Empirically, d2 significantly improves
over previous diffusion reasoning frameworks using only RL (without relying on
supervised fine-tuning), and sets a new state-of-the-art performance for DLMs
on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks
(GSM8K and MATH500).

</details>


### [306] [VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations](https://arxiv.org/abs/2509.21477)
*Yuan Gao,Hao Wu,Qingsong Wen,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: The paper addresses reconstructing subsurface ocean dynamics from limited surface data. It introduces KD48, a new benchmark dataset, and VISION, a novel methodology to handle missing observational data effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable systematic progress in reconstructing ocean dynamics given the challenges posed by incomplete surface observation data and the absence of standardized benchmarks.

Method: The method involves KD48 for benchmarking and VISION, a reconstruction model employing Dynamic Prompting. This approach generates on-the-fly visual prompts from available observations and uses a State-conditioned Prompting module to guide computational strategies efficiently.

Result: VISION significantly outperforms current state-of-the-art models on the KD48 benchmark and adapts well to scenarios with extreme data loss.

Conclusion: The paper provides a crucial benchmark (KD48) and a robust methodology (VISION), establishing a foundation for advancing ocean science research under data uncertainty.

Abstract: Reconstructing subsurface ocean dynamics, such as vertical velocity fields,
from incomplete surface observations poses a critical challenge in Earth
science, a field long hampered by the lack of standardized, analysis-ready
benchmarks. To systematically address this issue and catalyze research, we
first build and release KD48, a high-resolution ocean dynamics benchmark
derived from petascale simulations and curated with expert-driven denoising.
Building on this benchmark, we introduce VISION, a novel reconstruction
paradigm based on Dynamic Prompting designed to tackle the core problem of
missing data in real-world observations. The essence of VISION lies in its
ability to generate a visual prompt on-the-fly from any available subset of
observations, which encodes both data availability and the ocean's physical
state. More importantly, we design a State-conditioned Prompting module that
efficiently injects this prompt into a universal backbone, endowed with
geometry- and scale-aware operators, to guide its adaptive adjustment of
computational strategies. This mechanism enables VISION to precisely handle the
challenges posed by varying input combinations. Extensive experiments on the
KD48 benchmark demonstrate that VISION not only substantially outperforms
state-of-the-art models but also exhibits strong generalization under extreme
data missing scenarios. By providing a high-quality benchmark and a robust
model, our work establishes a solid infrastructure for ocean science research
under data uncertainty. Our codes are available at:
https://github.com/YuanGao-YG/VISION.

</details>


### [307] [Filtering with Confidence: When Data Augmentation Meets Conformal Prediction](https://arxiv.org/abs/2509.21479)
*Zixuan Wu,So Won Jeong,Yating Liu,Yeo Jin Jung,Claire Donnat*

Main category: cs.LG

TL;DR: The paper proposes conformal data augmentation to produce synthetic data with reduced bias and improved filtering.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and minimizing biases in synthetic data augmentation processes.

Method: The authors introduce conformal data augmentation, which uses conformal prediction for filtering synthetic data with provable risk control.

Result: Across multiple tasks, the approach improves F1 scores by up to 40% over unaugmented baselines and 4% over filtered augmentation baselines.

Conclusion: Conformal data augmentation is effective, easy to implement, and boosts performance across diverse applications.

Abstract: With promising empirical performance across a wide range of applications,
synthetic data augmentation appears a viable solution to data scarcity and the
demands of increasingly data-intensive models. Its effectiveness lies in
expanding the training set in a way that reduces estimator variance while
introducing only minimal bias. Controlling this bias is therefore critical:
effective data augmentation should generate diverse samples from the same
underlying distribution as the training set, with minimal shifts. In this
paper, we propose conformal data augmentation, a principled data filtering
framework that leverages the power of conformal prediction to produce diverse
synthetic data while filtering out poor-quality generations with provable risk
control. Our method is simple to implement, requires no access to internal
model logits, nor large-scale model retraining. We demonstrate the
effectiveness of our approach across multiple tasks, including topic
prediction, sentiment analysis, image classification, and fraud detection,
showing consistent performance improvements of up to 40% in F1 score over
unaugmented baselines, and 4% over other filtered augmentation baselines.

</details>


### [308] [High-Probability Analysis of Online and Federated Zero-Order Optimisation](https://arxiv.org/abs/2509.21484)
*Arya Akhavan,David Janz,El-Mahdi El-Mhamdi*

Main category: cs.LG

TL;DR: The paper introduces FedZero, a federated zero-order optimization algorithm with strong theoretical guarantees for distributed learning.


<details>
  <summary>Details</summary>
Motivation: To improve optimization techniques in federated settings, especially for cases without direct gradient information.

Method: FedZero employs a gradient estimator with randomization over the â„“1-sphere, and its analysis involves new concentration inequalities.

Result: FedZero achieves near-optimal error bounds in federated settings and is the first to provide high-probability guarantees in single-worker convex zero-order optimization.

Conclusion: FedZero provides robust theoretical guarantees, advancing both federated learning and zero-order optimization techniques.

Abstract: We study distributed learning in the setting of gradient-free zero-order
optimization and introduce FedZero, a federated zero-order algorithm that
delivers sharp theoretical guarantees. Specifically, FedZero: (1) achieves
near-optimal optimization error bounds with high probability in the federated
convex setting; and (2) in the single-worker regime-where the problem reduces
to the standard zero-order framework, establishes the first high-probability
convergence guarantees for convex zero-order optimization, thereby
strengthening the classical expectation-based results. At its core, FedZero
employs a gradient estimator based on randomization over the $\ell_1$-sphere.
To analyze it, we develop new concentration inequalities for Lipschitz
functions under the uniform measure on the $\ell_1$-sphere, with explicit
constants. These concentration tools are not only central to our
high-probability guarantees but may also be of independent interest.

</details>


### [309] [Neural Operators for Mathematical Modeling of Transient Fluid Flow in Subsurface Reservoir Systems](https://arxiv.org/abs/2509.21485)
*Daniil D. Sirota,Sergey A. Khan,Sergey L. Kostikov,Kirill A. Butov*

Main category: cs.LG

TL;DR: The paper introduces TFNO-opt, a neural network architecture that models transient fluid flow in subsurface reservoirs, achieving calculation speeds six orders of magnitude faster than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods for simulating subsurface reservoir systems, while accurate, are computationally expensive and unsuitable for real-time control and decision-making tasks.

Method: The method leverages a modified Fourier neural operator architecture incorporating features like adjustable internal time resolution, tensor decomposition in the spectral domain, the Sobolev norm for error evaluation, and separated reconstruction of initial conditions.

Result: The proposed TFNO-opt achieved significant improvements in computational speed, demonstrated by hydrodynamic modeling of underground gas storage systems, where a six-order-of-magnitude acceleration was observed.

Conclusion: The improvements in the TFNO-opt architecture make it a practical and efficient tool for real-time control and analysis of complex reservoir systems, expanding its application potential in various domains.

Abstract: This paper presents a method for modeling transient fluid flow in subsurface
reservoir systems based on the developed neural operator architecture
(TFNO-opt). Reservoir systems are complex dynamic objects with distributed
parameters described by systems of partial differential equations (PDEs).
Traditional numerical methods for modeling such systems, despite their high
accuracy, are characterized by significant time costs for performing
calculations, which limits their applicability in control and decision support
problems. The proposed architecture (TFNO-opt) is based on Fourier neural
operators, which allow approximating PDE solutions in infinite-dimensional
functional spaces, providing invariance to discretization and the possibility
of generalization to various implementations of equations. The developed
modifications are aimed at increasing the accuracy and stability of the trained
neural operator, which is especially important for control problems. These
include adjustable internal time resolution of the integral Fourier operator,
tensor decomposition of parameters in the spectral domain, use of the Sobolev
norm in the error function, and separation of approximation errors and
reconstruction of initial conditions for more accurate reproduction of physical
processes. The effectiveness of the proposed improvements is confirmed by
computational experiments. The practical significance is confirmed by
computational experiments using the example of the problem of hydrodynamic
modeling of an underground gas storage (UGS), where the acceleration of
calculations by six orders of magnitude was achieved, compared to traditional
methods. This opens up new opportunities for the effective control of complex
reservoir systems.

</details>


### [310] [GraphPFN: A Prior-Data Fitted Graph Foundation Model](https://arxiv.org/abs/2509.21489)
*Dmitry Eremeev,Oleg Platonov,Gleb Bazhenov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: GraphPFN introduces a novel graph foundation model that leverages synthetic graph priors and graph-aware adaptations of tabular foundation models to enhance graph learning tasks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing graph foundation models like G2T-FM rely on hand-crafted features, which can limit their ability to learn complex graph-specific patterns. This paper aims to overcome these limitations by proposing a more efficient strategy for graph data learning.

Method: The authors generate synthetic attributed graphs using a combination of stochastic block models and preferential attachment processes for graph structure, and graph-aware structured causal models for node attributes and targets. They then augment a tabular foundation model (LimiX) with graph neighborhood aggregation layers and pretrain it on these synthetic graphs.

Result: GraphPFN demonstrates strong in-context learning performance across diverse real-world graph datasets containing up to 50,000 nodes, achieving state-of-the-art results and outperforming prior models like G2T-FM and task-specific graph neural networks (GNNs) in most cases.

Conclusion: Pretraining on synthetic graphs with a carefully designed prior distribution is an effective strategy for boosting the performance of graph foundation models, showcasing the potential for broader applications in graph data tasks.

Abstract: Foundation models pretrained on large-scale datasets have transformed such
fields as natural language processing and computer vision, but their
application to graph data remains limited. Recently emerged graph foundation
models, such as G2T-FM, utilize tabular foundation models for graph tasks and
were shown to significantly outperform prior attempts to create GFMs. However,
these models primarily rely on hand-crafted graph features, limiting their
ability to learn complex graph-specific patterns. In this work, we propose
GraphPFN: a prior-data fitted network for node-level prediction. First, we
design a prior distribution of synthetic attributed graphs. For graph structure
generation, we use a novel combination of multiple stochastic block models and
a preferential attachment process. We then apply graph-aware structured causal
models to generate node attributes and targets. This procedure allows us to
efficiently generate a wide range of realistic graph datasets. Then, we augment
the tabular foundation model LimiX with attention-based graph neighborhood
aggregation layers and train it on synthetic graphs sampled from our prior,
allowing the model to capture graph structural dependencies not present in
tabular data. On diverse real-world graph datasets with up to 50,000 nodes,
GraphPFN shows strong in-context learning performance and achieves
state-of-the-art results after finetuning, outperforming both G2T-FM and
task-specific GNNs trained from scratch on most datasets. More broadly, our
work demonstrates that pretraining on synthetic graphs from a well-designed
prior distribution is an effective strategy for building graph foundation
models.

</details>


### [311] [SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models](https://arxiv.org/abs/2509.21498)
*Arani Roy,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: SlimDiff is a framework that efficiently compresses diffusion models without retraining, achieving notable acceleration and size reduction.


<details>
  <summary>Details</summary>
Motivation: Address computational limitations of diffusion models while maintaining their generative performance without performance degradation from fine-tuning or retraining.

Method: Proposes activation-informed structural compression using spectral approximation. Utilizes activation covariances to apply module-wise decompositions and dynamic pruning, avoiding errors across timesteps.

Result: Achieved up to 35% acceleration and ~100M parameter reduction while preserving generative quality using minimal samples.

Conclusion: SlimDiff is a pioneering closed-form, training-free approach for compressing diffusion models, offering efficiency and performance clarity.

Abstract: Diffusion models (DMs), lauded for their generative performance, are
computationally prohibitive due to their billion-scale parameters and iterative
denoising dynamics. Existing efficiency techniques, such as quantization,
timestep reduction, or pruning, offer savings in compute, memory, or runtime
but are strictly bottlenecked by reliance on fine-tuning or retraining to
recover performance. In this work, we introduce SlimDiff, an automated
activation-informed structural compression framework that reduces both
attention and feedforward dimensionalities in DMs, while being entirely
gradient-free. SlimDiff reframes DM compression as a spectral approximation
task, where activation covariances across denoising timesteps define low-rank
subspaces that guide dynamic pruning under a fixed compression budget. This
activation-aware formulation mitigates error accumulation across timesteps by
applying module-wise decompositions over functional weight groups: query--key
interactions, value--output couplings, and feedforward projections, rather than
isolated matrix factorizations, while adaptively allocating sparsity across
modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff
achieves up to 35\% acceleration and $\sim$100M parameter reduction over
baselines, with generation quality on par with uncompressed models without any
backpropagation. Crucially, our approach requires only about 500 calibration
samples, over 70$\times$ fewer than prior methods. To our knowledge, this is
the first closed-form, activation-guided structural compression of DMs that is
entirely training-free, providing both theoretical clarity and practical
efficiency.

</details>


### [312] [Prophecy: Inferring Formal Properties from Neuron Activations](https://arxiv.org/abs/2509.21677)
*Divya Gopinath,Corina S. Pasareanu,Muhammad Usman*

Main category: cs.LG

TL;DR: This paper introduces Prophecy, a tool to automatically extract formal properties from feed-forward neural networks by analyzing neuron activation logic. It can infer rules linking inner layer activations to specific output behaviors.


<details>
  <summary>Details</summary>
Motivation: To provide a mechanism for understanding and formally verifying the inner workings and behavior of neural networks, addressing the lack of interpretability and verification tools for such systems.

Method: Prophecy analyzes neuron activation states (on/off or value) in hidden layers and extracts precondition rules to infer specific output properties of feed-forward neural networks.

Result: The tool is applied to various models and output properties, offering insights like formal explanations, compositional verification, run-time monitoring, repair, and application in large vision-language models.

Conclusion: Prophecy enhances the interpretability, verification, and reliability of neural networks, demonstrating its utility in both theoretical and practical applications, including modern large models.

Abstract: We present Prophecy, a tool for automatically inferring formal properties of
feed-forward neural networks. Prophecy is based on the observation that a
significant part of the logic of feed-forward networks is captured in the
activation status of the neurons at inner layers. Prophecy works by extracting
rules based on neuron activations (values or on/off statuses) as preconditions
that imply certain desirable output property, e.g., the prediction being a
certain class. These rules represent network properties captured in the hidden
layers that imply the desired output behavior. We present the architecture of
the tool, highlight its features and demonstrate its usage on different types
of models and output properties. We present an overview of its applications,
such as inferring and proving formal explanations of neural networks,
compositional verification, run-time monitoring, repair, and others. We also
show novel results highlighting its potential in the era of large
vision-language models.

</details>


### [313] [Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training](https://arxiv.org/abs/2509.21500)
*Junkai Zhang,Zihao Wang,Lin Gui,Swarnashree Mysore Sathyendra,Jaehwan Jeong,Victor Veitch,Wei Wang,Yunzhong He,Bing Liu,Lifeng Jin*

Main category: cs.LG

TL;DR: The paper introduces a rubric-based reward system to address challenges of reward over-optimization in reinforcement fine-tuning for large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the issue of reward over-optimization in RFT, where models exploit reward signals to achieve high scores at the expense of output quality.

Method: Rubric-based rewards are designed to focus on examples in the high-reward tail and implemented in a workflow that effectively distinguishes among diverse and high-quality responses.

Result: The use of rubric-based rewards mitigates reward over-optimization and improves the performance and output quality of LLMs post-training.

Conclusion: Rubric-based rewards show promise in improving LLM alignment by handling tail examples more effectively and avoiding issues of reward exploitation.

Abstract: Reinforcement fine-tuning (RFT) often suffers from \emph{reward
over-optimization}, where a policy model hacks the reward signals to achieve
high scores while producing low-quality outputs. Our theoretical analysis shows
that the key lies in reward misspecification at the high-reward tail: the
inability to reliably distinguish Excellent responses from merely Great ones.
This motivate us to focus on the high-reward region. However, such tail
examples are scarce under the base LLM. While off-policy exemplars (e.g. from
stronger models or rewrites) are easier to obtain, naively training on them
yields a misspecified reward for the policy we aim to align. To address this,
we study rubric-based rewards. By design, rubrics can leverage off-policy
examples while remaining insensitive to their artifacts. To elicit rubrics that
capture the high-reward tail, we highlight the importance of distinguishing
among great and diverse responses, and introduce a workflow to implement this
idea. We empirically demonstrate that rubric-based rewards substantially
mitigate reward over-optimization and deliver effective LLM post-training
improvements. Our code can be accessed at
https://github.com/Jun-Kai-Zhang/rubrics.git .

</details>


### [314] [Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations](https://arxiv.org/abs/2509.21511)
*Micha Livne*

Main category: cs.LG

TL;DR: This paper introduces cMIM, a framework combining probabilistic and contrastive objectives for improved representation learning, outperforming prior methods in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Current representation learning methods struggle to balance generative and discriminative tasks, requiring a framework that performs well in both aspects.

Method: cMIM is proposed as an extension of MIM with a contrastive objective, removing the need for data augmentations, reducing sensitivity to batch size, and utilizing a novel 'informative embeddings' technique.

Result: cMIM demonstrates superior performance compared to MIM and InfoNCE on classification and regression tasks across various domains, while maintaining reconstruction quality.

Conclusion: cMIM establishes itself as a unified framework for effective representation learning across generative and discriminative applications.

Abstract: Learning representations that transfer well to diverse downstream tasks
remains a central challenge in representation learning. Existing paradigms --
contrastive learning, self-supervised masking, and denoising auto-encoders --
balance this challenge with different trade-offs. We introduce the {contrastive
Mutual Information Machine} (cMIM), a probabilistic framework that extends the
Mutual Information Machine (MIM) with a contrastive objective. While MIM
maximizes mutual information between inputs and latents and promotes clustering
of codes, it falls short on discriminative tasks. cMIM addresses this gap by
imposing global discriminative structure while retaining MIM's generative
fidelity. Our contributions are threefold. First, we propose cMIM, a
contrastive extension of MIM that removes the need for positive data
augmentation and is substantially less sensitive to batch size than InfoNCE.
Second, we introduce {informative embeddings}, a general technique for
extracting enriched features from encoder-decoder models that boosts
discriminative performance without additional training and applies broadly
beyond MIM. Third, we provide empirical evidence across vision and molecular
benchmarks showing that cMIM outperforms MIM and InfoNCE on classification and
regression tasks while preserving competitive reconstruction quality. These
results position cMIM as a unified framework for representation learning,
advancing the goal of models that serve both discriminative and generative
applications effectively.

</details>


### [315] [DistillKac: Few-Step Image Generation via Damped Wave Equations](https://arxiv.org/abs/2509.21513)
*Weiqiao Han,Chenlin Meng,Christopher D. Manning,Stefano Ermon*

Main category: cs.LG

TL;DR: DistillKac is a fast image generator using finite speed probability mass transport via the damped wave equation. It enhances numerical stability and delivers high-quality samples efficiently.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address issues in diffusion models, such as unbounded propagation speed and stiffness in reverse-time velocities, by incorporating finite speed transport in image generation.

Method: They leverage the stochastic Kac representation of the damped wave equation, introduce velocity-space guidance, and propose endpoint-only distillation where a student model learns from a frozen teacher over long intervals.

Result: DistillKac generates high-quality samples with fewer function evaluations while maintaining numerical stability through finite-speed probability flows.

Conclusion: The paper demonstrates that DistillKac effectively combines fast computation and numerical stability, offering a robust alternative to traditional diffusion-based image generation methods.

Abstract: We present DistillKac, a fast image generator that uses the damped wave
equation and its stochastic Kac representation to move probability mass at
finite speed. In contrast to diffusion models whose reverse time velocities can
become stiff and implicitly allow unbounded propagation speed, Kac dynamics
enforce finite speed transport and yield globally bounded kinetic energy.
Building on this structure, we introduce classifier-free guidance in velocity
space that preserves square integrability under mild conditions. We then
propose endpoint only distillation that trains a student to match a frozen
teacher over long intervals. We prove a stability result that promotes
supervision at the endpoints to closeness along the entire path. Experiments
demonstrate DistillKac delivers high quality samples with very few function
evaluations while retaining the numerical stability benefits of finite speed
probability flows.

</details>


### [316] [Uncertainty-Aware Knowledge Tracing Models](https://arxiv.org/abs/2509.21514)
*Joshua Mitton,Prarthana Bhattacharyya,Ralph Abboud,Simon Woodhead*

Main category: cs.LG

TL;DR: The paper proposes incorporating predictive uncertainty into Knowledge Tracing models to improve the identification of incorrect predictions, particularly when students pick distractors.


<details>
  <summary>Details</summary>
Motivation: To address the issue that current Knowledge Tracing models fail to detect student errors, especially in distractor scenarios, leading to inaccurate assessments of student ability.

Method: The researchers introduced predictive uncertainty into KT models to capture instances of incorrect predictions and studied its alignment with model errors.

Result: Predictive uncertainty was found to be informative and correlated with model incorrect predictions, proving its potential value.

Conclusion: Incorporating predictive uncertainty into KT models enhances their ability to detect errors and offers pedagogical benefits, especially in resource-constrained educational platforms.

Abstract: The main focus of research on Knowledge Tracing (KT) models is on model
developments with the aim of improving predictive accuracy. Most of these
models make the most incorrect predictions when students choose a distractor,
leading to student errors going undetected. We present an approach to add new
capabilities to KT models by capturing predictive uncertainty and demonstrate
that a larger predictive uncertainty aligns with model incorrect predictions.
We show that uncertainty in KT models is informative and that this signal would
be pedagogically useful for application in an educational learning platform
that can be used in a limited resource setting where understanding student
ability is necessary.

</details>


### [317] [$\mathbf{Li_2}$: A Framework on Dynamics of Feature Emergence and Delayed Generalization](https://arxiv.org/abs/2509.21519)
*Yuandong Tian*

Main category: cs.LG

TL;DR: This paper studies the phenomenon of grokking in 2-layer nonlinear networks by introducing a framework (Liâ‚‚) to dissect it into three stages: lazy learning, independent feature learning, and interactive feature learning.


<details>
  <summary>Details</summary>
Motivation: To address the open question of determining what kind of features emerge and under which conditions delayed generalization (grokking) occurs during training on complex inputs.

Method: The paper introduces the Liâ‚‚ framework, which highlights three stages of grokking (lazy learning, independent feature learning, interactive feature learning) and studies their dynamics using the structure of backpropagated gradients and theoretical analysis of energy functions and gradient dynamics.

Result: It identifies how gradients evolve across stages, describes the conditions leading to feature emergence, and explains the roles of hyperparameters like weight decay and learning rate in memorization and generalization. It also provides scaling laws and insights into optimizer effectiveness.

Conclusion: The Liâ‚‚ framework provides a provable, theoretical explanation for grokking and extends its principles to understand hidden feature interactions, generalization, and the effectiveness of modern optimizers in multi-layer networks.

Abstract: While the phenomenon of grokking, i.e., delayed generalization, has been
studied extensively, it remains an open question whether there is a
mathematical framework to characterize what kind of features emerge, how and in
which conditions it happens from training, for complex structured inputs. We
propose a novel framework, named $\mathbf{Li_2}$, that captures three key
stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy
learning, (II) independent feature learning and (III) interactive feature
learning, characterized by the structure of backpropagated gradient $G_F$
across layers. In (I), $G_F$ is random, and top layer overfits to random hidden
representation. In (II), the gradient of each node (column of $G_F$) only
depends on its own activation, and thus each hidden node learns their
representation independently from $G_F$, which now carries information about
target labels, thanks to weight decay. Interestingly, the independent dynamics
follows exactly the gradient ascent of an energy function $E$, and its local
maxima are precisely the emerging features. We study whether these local-optima
induced features are generalizable, their representation power, and how they
change on sample size, in group arithmetic tasks. Finally, in (III), we
provably show how hidden nodes interact, and how $G_F$ changes to focus on
missing features that need to be learned. Our study sheds lights on roles
played by key hyperparameters such as weight decay, learning rate and sample
sizes in grokking, leads to provable scaling laws of memorization and
generalization, and reveals the underlying cause why recent optimizers such as
Muon can be effective, from the first principles of gradient dynamics. Our
analysis can be extended to multi-layer architectures.

</details>


### [318] [TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning](https://arxiv.org/abs/2509.21526)
*Hongyang He,Xinyuan Song,Yangfan He,Zeyu Zhang,Yanshu Li,Haochen You,Lifan Sun,Wenqiao Zhang*

Main category: cs.LG

TL;DR: TRiCo introduces a novel triadic game-theoretic framework for semi-supervised learning (SSL), integrating a teacher, two students, and an adversarial generator. It addresses limitations in pseudo-labeling, sample modeling, and static interactions, yielding state-of-the-art results across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current SSL methods face challenges involving static interactions, unreliable pseudo-labeling, and insufficient modeling of hard samples. TRiCo aims to overcome these limitations with a more dynamic and robust SSL framework.

Method: TRiCo employs a triadic structure with: (i) two student classifiers trained on frozen complementary representations, (ii) a meta-learned teacher that regulates pseudo-labeling through feedback, and (iii) a generator that creates adversarial embedding perturbations. This interaction is modeled as a Stackelberg game.

Result: TRiCo demonstrates state-of-the-art performance consistently across datasets like CIFAR-10, SVHN, STL-10, and ImageNet in low-label scenarios, showcasing compatibility with various architectures.

Conclusion: By introducing structured triadic interactions and overcoming the limitations of existing SSL methods, TRiCo provides a robust, architecture-agnostic solution to improve SSL performance under challenging conditions.

Abstract: We introduce TRiCo, a novel triadic game-theoretic co-training framework that
rethinks the structure of semi-supervised learning by incorporating a teacher,
two students, and an adversarial generator into a unified training paradigm.
Unlike existing co-training or teacher-student approaches, TRiCo formulates SSL
as a structured interaction among three roles: (i) two student classifiers
trained on frozen, complementary representations, (ii) a meta-learned teacher
that adaptively regulates pseudo-label selection and loss balancing via
validation-based feedback, and (iii) a non-parametric generator that perturbs
embeddings to uncover decision boundary weaknesses. Pseudo-labels are selected
based on mutual information rather than confidence, providing a more robust
measure of epistemic uncertainty. This triadic interaction is formalized as a
Stackelberg game, where the teacher leads strategy optimization and students
follow under adversarial perturbations. By addressing key limitations in
existing SSL frameworks, such as static view interactions, unreliable
pseudo-labels, and lack of hard sample modeling, TRiCo provides a principled
and generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10,
and ImageNet demonstrate that TRiCo consistently achieves state-of-the-art
performance in low-label regimes, while remaining architecture-agnostic and
compatible with frozen vision backbones.

</details>


### [319] [Preemptive Detection and Steering of LLM Misalignment via Latent Reachability](https://arxiv.org/abs/2509.21528)
*Sathwik Karnik,Somil Bansal*

Main category: cs.LG

TL;DR: The paper introduces BRT-Align, a control-theoretic approach to enhance safety in large language model (LLM) outputs by using backward reachability to predict and mitigate unsafe continuations during inference.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inability of the dominant reinforcement learning from human feedback (RLHF) method to safeguard large language models (LLMs) from generating harmful content during inference.

Method: The authors propose BRT-Align, which treats autoregressive generation as a dynamical system in latent space, and uses backward reachability to calculate a safety value function. This allows for a runtime monitor to forecast unsafe outputs and a steering filter to redirect unsafe trajectories.

Result: Experiments demonstrate that BRT-Align outperforms existing methods in terms of accurate and early detection of unsafe content, reduces the generation of unsafe content while preserving diversity and coherence, and offers qualitative improvements in the alignment of generated responses.

Conclusion: BRT-Align is a principled and practical inference-time safety mechanism for LLMs, providing effective mitigation of unsafe outputs and paving the way for safer AI systems through reachability analysis.

Abstract: Large language models (LLMs) are now ubiquitous in everyday tools, raising
urgent safety concerns about their tendency to generate harmful content. The
dominant safety approach -- reinforcement learning from human feedback (RLHF)
-- effectively shapes model behavior during training but offers no safeguards
at inference time, where unsafe continuations may still arise. We propose
BRT-Align, a reachability-based framework that brings control-theoretic safety
tools to LLM inference. BRT-Align models autoregressive generation as a
dynamical system in latent space and learn a safety value function via backward
reachability, estimating the worst-case evolution of a trajectory. This enables
two complementary mechanisms: (1) a runtime monitor that forecasts unsafe
completions several tokens in advance, and (2) a least-restrictive steering
filter that minimally perturbs latent states to redirect generation away from
unsafe regions. Experiments across multiple LLMs and toxicity benchmarks
demonstrate that BRT-Align provides more accurate and earlier detection of
unsafe continuations than baselines. Moreover, for LLM safety alignment,
BRT-Align substantially reduces unsafe generations while preserving sentence
diversity and coherence. Qualitative results further highlight emergent
alignment properties: BRT-Align consistently produces responses that are less
violent, less profane, less offensive, and less politically biased. Together,
these findings demonstrate that reachability analysis provides a principled and
practical foundation for inference-time LLM safety.

</details>


### [320] [Expert-guided Clinical Text Augmentation via Query-Based Model Collaboration](https://arxiv.org/abs/2509.21530)
*Dongkyu Cho,Miao Zhang,Rumi Chunara*

Main category: cs.LG

TL;DR: This paper proposes a new approach to safely and effectively use large language models (LLMs) for data augmentation in high-stakes domains like healthcare.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of using LLMs for data augmentation in healthcare, where ensuring the accuracy and safety of synthetic data is critical to avoid the generation of clinically incorrect or misleading information.

Method: The method involves a query-based model collaboration framework that incorporates expert-level domain knowledge to guide the data augmentation process, thus preserving crucial medical information.

Result: Experiments on clinical prediction tasks show that the proposed approach outperforms existing LLM-based augmentation strategies and reduces factual errors, thereby improving safety.

Conclusion: The framework bridges the gap between the generative capabilities of LLMs and the stringent safety requirements in specialized domains like healthcare.

Abstract: Data augmentation is a widely used strategy to improve model robustness and
generalization by enriching training datasets with synthetic examples. While
large language models (LLMs) have demonstrated strong generative capabilities
for this purpose, their applications in high-stakes domains like healthcare
present unique challenges due to the risk of generating clinically incorrect or
misleading information. In this work, we propose a novel query-based model
collaboration framework that integrates expert-level domain knowledge to guide
the augmentation process to preserve critical medical information. Experiments
on clinical prediction tasks demonstrate that our lightweight
collaboration-based approach consistently outperforms existing LLM augmentation
methods while improving safety through reduced factual errors. This framework
addresses the gap between LLM augmentation potential and the safety
requirements of specialized domains.

</details>


### [321] [Machine Learning. The Science of Selection under Uncertainty](https://arxiv.org/abs/2509.21547)
*Yevgeny Seldin*

Main category: cs.LG

TL;DR: The abstract discusses a book on statistical tools to ensure robust selection in machine learning, focusing on generalization bounds, regret analysis in online learning, and concentration inequalities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address selection under uncertainty in machine learning, where noisy empirical estimates challenge reliability in choosing successful prediction models.

Method: The authors use concentration of measure inequalities, generalization bounding techniques like Vapnik-Chervonenkis, and regret analysis in both offline and online learning settings.

Result: The book combines theoretical approaches such as Occam's Razor and PAC-Bayesian analysis for generalization guarantees, and regret bounds for online learning with stochastic and adversarial conditions.

Conclusion: The work provides a comprehensive toolbox for statistical analysis and performance guarantees in supervised and online machine learning scenarios, enhancing understanding of selection under uncertainty.

Abstract: Learning, whether natural or artificial, is a process of selection. It starts
with a set of candidate options and selects the more successful ones. In the
case of machine learning the selection is done based on empirical estimates of
prediction accuracy of candidate prediction rules on some data. Due to
randomness of data sampling the empirical estimates are inherently noisy,
leading to selection under uncertainty. The book provides statistical tools to
obtain theoretical guarantees on the outcome of selection under uncertainty. We
start with concentration of measure inequalities, which are the main
statistical instrument for controlling how much an empirical estimate of
expectation of a function deviates from the true expectation. The book covers a
broad range of inequalities, including Markov's, Chebyshev's, Hoeffding's,
Bernstein's, Empirical Bernstein's, Unexpected Bernstein's, kl, and split-kl.
We then study the classical (offline) supervised learning and provide a range
of tools for deriving generalization bounds, including Occam's razor,
Vapnik-Chervonenkis analysis, and PAC-Bayesian analysis. The latter is further
applied to derive generalization guarantees for weighted majority votes. After
covering the offline setting, we turn our attention to online learning. We
present the space of online learning problems characterized by environmental
feedback, environmental resistance, and structural complexity. A common
performance measure in online learning is regret, which compares performance of
an algorithm to performance of the best prediction rule in hindsight, out of a
restricted set of prediction rules. We present tools for deriving regret bounds
in stochastic and adversarial environments, and under full information and
bandit feedback.

</details>


### [322] [A circuit for predicting hierarchical structure in-context in Large Language Models](https://arxiv.org/abs/2509.21534)
*Tankred Saanum,Can Demircan,Samuel J. Gershman,Eric Schulz*

Main category: cs.LG

TL;DR: This paper analyzes how large language models (LLMs) use induction attention heads to learn structured and repetitive patterns in context, offering insights into their mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs manage complex in-context learning tasks, especially those involving hierarchical dependencies found in natural language.

Method: The authors design a synthetic task featuring hierarchically dependent token sequences, evaluate various LLMs, and study the role of induction heads and their ability to adaptively learn latent structures.

Result: Adaptive induction heads enable LLMs to predict tokens by selectively attending to context-relevant cues, further learning latent structures via other attention heads.

Conclusion: The study provides a mechanistic explanation for how LLM induction heads learn to manage higher-order patterns, enriching our understanding of their in-context learning abilities.

Abstract: Large Language Models (LLMs) excel at in-context learning, the ability to use
information provided as context to improve prediction of future tokens.
Induction heads have been argued to play a crucial role for in-context learning
in Transformer Language Models. These attention heads make a token attend to
successors of past occurrences of the same token in the input. This basic
mechanism supports LLMs' ability to copy and predict repeating patterns.
However, it is unclear if this same mechanism can support in-context learning
of more complex repetitive patterns with hierarchical structure. Natural
language is teeming with such cases: The article "the" in English usually
prefaces multiple nouns in a text. When predicting which token succeeds a
particular instance of "the", we need to integrate further contextual cues from
the text to predict the correct noun. If induction heads naively attend to all
past instances of successor tokens of "the" in a context-independent manner,
they cannot support this level of contextual information integration. In this
study, we design a synthetic in-context learning task, where tokens are
repeated with hierarchical dependencies. Here, attending uniformly to all
successor tokens is not sufficient to accurately predict future tokens.
Evaluating a range of LLMs on these token sequences and natural language
analogues, we find adaptive induction heads that support prediction by learning
what to attend to in-context. Next, we investigate how induction heads
themselves learn in-context. We find evidence that learning is supported by
attention heads that uncover a set of latent contexts, determining the
different token transition relationships. Overall, we not only show that LLMs
have induction heads that learn, but offer a complete mechanistic account of
how LLMs learn to predict higher-order repetitive patterns in-context.

</details>


### [323] [Interpretable time series analysis with Gumbel dynamics](https://arxiv.org/abs/2509.21578)
*Yiliu Wang,Timothy Doyeon Kim,Eric Shea-Brown,Uygar SÃ¼mbÃ¼l*

Main category: cs.LG

TL;DR: The Gumbel Dynamical Model (GDM) introduces a continuous relaxation of discrete states to model smoother transitions in dynamical systems, accommodating stochastic and overlapping states while enhancing interpretability and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional switching dynamical systems struggle to accurately capture smooth transitions and stochastic mixtures of states in time series data due to their reliance on discrete states, leading to rapid and spurious switching in real-world datasets.

Method: The GDM employs a continuous relaxation of discrete states based on the Gumbel distribution, enabling fully differentiable modeling and gradient descent training for improved scalability. It expands available state dynamics to approximate non-stationary dynamics better.

Result: GDM exhibited superior performance in simulation datasets, effectively modeling soft, sticky states and transitions in stochastic settings. It also demonstrated its utility in interpreting states in real-world time series with multiple dynamics, outperforming traditional methods.

Conclusion: GDM provides a scalable, interpretable approach to modeling complex time series data, addressing limitations of traditional methods by approximating smoother transitions and accommodating stochastic dynamics effectively.

Abstract: Switching dynamical systems can model complicated time series data while
maintaining interpretability by inferring a finite set of dynamics primitives
and explaining different portions of the observed time series with one of these
primitives. However, due to the discrete nature of this set, such models
struggle to capture smooth, variable-speed transitions, as well as stochastic
mixtures of overlapping states, and the inferred dynamics often display
spurious rapid switching on real-world datasets. Here, we propose the Gumbel
Dynamical Model (GDM). First, by introducing a continuous relaxation of
discrete states and a different noise model defined on the relaxed-discrete
state space via the Gumbel distribution, GDM expands the set of available state
dynamics, allowing the model to approximate smoother and non-stationary
ground-truth dynamics more faithfully. Second, the relaxation makes the model
fully differentiable, enabling fast and scalable training with standard
gradient descent methods. We validate our approach on standard simulation
datasets and highlight its ability to model soft, sticky states and transitions
in a stochastic setting. Furthermore, we apply our model to two real-world
datasets, demonstrating its ability to infer interpretable states in stochastic
time series with multiple dynamics, a setting where traditional methods often
fail.

</details>


### [324] [Evidence for Limited Metacognition in LLMs](https://arxiv.org/abs/2509.21545)
*Christopher Ackerman*

Main category: cs.LG

TL;DR: The study explores metacognition in Large Language Models (LLMs) by measuring their ability to strategize using internal knowledge, revealing certain emerging metacognitive abilities and contextual limitations.


<details>
  <summary>Details</summary>
Motivation: To investigate the possibility of self-awareness or sentience in LLMs and address the potential safety and policy impacts by developing a quantitative framework for measuring their metacognitive abilities.

Method: The paper introduces a methodology inspired by nonhuman animal metacognition studies, avoiding self-reports and instead testing strategic deployment of knowledge of internal states through two experimental paradigms, along with analyzing token probabilities.

Result: Frontier LLMs since early 2024 exhibit signs of metacognitive abilities like self-assessment of answer confidence and utilizing anticipated responses. These findings are supported by token probability analysis, pointing to potential internal metacognitive signals.

Conclusion: The observed metacognitive abilities are limited, context-dependent, and qualitatively distinct from human capabilities. Variance in models suggests post-training plays a role in metacognitive development.

Abstract: The possibility of LLM self-awareness and even sentience is gaining
increasing public attention and has major safety and policy implications, but
the science of measuring them is still in a nascent state. Here we introduce a
novel methodology for quantitatively evaluating metacognitive abilities in
LLMs. Taking inspiration from research on metacognition in nonhuman animals,
our approach eschews model self-reports and instead tests to what degree models
can strategically deploy knowledge of internal states. Using two experimental
paradigms, we demonstrate that frontier LLMs introduced since early 2024 show
increasingly strong evidence of certain metacognitive abilities, specifically
the ability to assess and utilize their own confidence in their ability to
answer factual and reasoning questions correctly and the ability to anticipate
what answers they would give and utilize that information appropriately. We
buttress these behavioral findings with an analysis of the token probabilities
returned by the models, which suggests the presence of an upstream internal
signal that could provide the basis for metacognition. We further find that
these abilities 1) are limited in resolution, 2) emerge in context-dependent
manners, and 3) seem to be qualitatively different from those of humans. We
also report intriguing differences across models of similar capabilities,
suggesting that LLM post-training may have a role in developing metacognitive
abilities.

</details>


### [325] [GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks](https://arxiv.org/abs/2509.21605)
*Tian Yu Yen,Reese E. Jones,Ravi G. Patel*

Main category: cs.LG

TL;DR: This paper introduces GenUQ, a generative hyper-network approach to uncertainty quantification in operator learning, demonstrating superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Numerical integration of PDEs is computationally expensive, and integrating uncertainty quantification into operator models is challenging due to difficulties in constructing likelihoods for stochastic operators.

Method: A generative hyper-network, GenUQ, is proposed to produce parameter distributions consistent with observed data, avoiding the need for likelihood construction.

Result: GenUQ outperformed existing UQ methods in three test problems: recovering a manufactured operator, solving a stochastic elliptic PDE, and modeling porous steel failure.

Conclusion: GenUQ provides an efficient and robust alternative to traditional likelihood-based uncertainty quantification methods in operator learning.

Abstract: Operator learning is a recently developed generalization of regression to
mappings between functions. It promises to drastically reduce expensive
numerical integration of PDEs to fast evaluations of mappings between
functional states of a system, i.e., surrogate and reduced-order modeling.
Operator learning has already found applications in several areas such as
modeling sea ice, combustion, and atmospheric physics. Recent approaches
towards integrating uncertainty quantification into the operator models have
relied on likelihood based methods to infer parameter distributions from noisy
data. However, stochastic operators may yield actions from which a likelihood
is difficult or impossible to construct. In this paper, we introduce, GenUQ, a
measure-theoretic approach to UQ that avoids constructing a likelihood by
introducing a generative hyper-network model that produces parameter
distributions consistent with observed data. We demonstrate that GenUQ
outperforms other UQ methods in three example problems, recovering a
manufactured operator, learning the solution operator to a stochastic elliptic
PDE, and modeling the failure location of porous steel under tension.

</details>


### [326] [DriftLite: Lightweight Drift Control for Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2509.21655)
*Yinuo Ren,Wenhao Gao,Lexing Ying,Grant M. Rotskoff,Jiequn Han*

Main category: cs.LG

TL;DR: This paper introduces DriftLite, a lightweight, training-free method for improving inference-time adaptation in diffusion models, addressing limitations like bias and high computational cost in existing approaches.


<details>
  <summary>Details</summary>
Motivation: To enable efficient inference-time adaptation of pre-trained diffusion models to new target distributions without retraining while tackling issues in current methods.

Method: DriftLite leverages the Fokker-Planck equation to provide optimal stability control and a novel implementation of drift steering through Variance- and Energy-Controlling Guidance (VCG/ECG).

Result: DriftLite demonstrates reduced variance and higher sample quality compared to guidance-based and sequential Monte Carlo methods across various applications.

Conclusion: DriftLite presents a scalable, principled approach for inference-time adaptation, making it effective and computationally efficient in diverse domains.

Abstract: We study inference-time scaling for diffusion models, where the goal is to
adapt a pre-trained model to new target distributions without retraining.
Existing guidance-based methods are simple but introduce bias, while
particle-based corrections suffer from weight degeneracy and high computational
cost. We introduce DriftLite, a lightweight, training-free particle-based
approach that steers the inference dynamics on the fly with provably optimal
stability control. DriftLite exploits a previously unexplored degree of freedom
in the Fokker-Planck equation between the drift and particle potential, and
yields two practical instantiations: Variance- and Energy-Controlling Guidance
(VCG/ECG) for approximating the optimal drift with minimal overhead. Across
Gaussian mixture models, particle systems, and large-scale protein-ligand
co-folding problems, DriftLite consistently reduces variance and improves
sample quality over pure guidance and sequential Monte Carlo baselines. These
results highlight a principled, efficient route toward scalable inference-time
adaptation of diffusion models.

</details>


### [327] [Differentiable Structure Learning for General Binary Data](https://arxiv.org/abs/2509.21658)
*Chang Deng,Bryon Aragam*

Main category: cs.LG

TL;DR: Current differentiable structure learning methods fail to capture complex non-linear dependencies in discrete data. This paper introduces a general optimization framework to address the issue, achieving improved empirical results.


<details>
  <summary>Details</summary>
Motivation: Existing methods are constrained by oversimplified assumptions and ignore the complexities of discrete data, leading to limited generalizability.

Method: A novel differentiable structure learning framework based on a single general optimization problem to identify dependencies in discrete data without restrictive assumptions.

Result: The framework effectively captures complex relationships in discrete data and demonstrates empirical improvements over previous methods.

Conclusion: This approach enhances the ability to analyze discrete data comprehensively, providing a more robust and generalized solution compared to prior methods.

Abstract: Existing methods for differentiable structure learning in discrete data
typically assume that the data are generated from specific structural equation
models. However, these assumptions may not align with the true data-generating
process, which limits the general applicability of such methods. Furthermore,
current approaches often ignore the complex dependence structure inherent in
discrete data and consider only linear effects. We propose a differentiable
structure learning framework that is capable of capturing arbitrary
dependencies among discrete variables. We show that although general discrete
models are unidentifiable from purely observational data, it is possible to
characterize the complete set of compatible parameters and structures.
Additionally, we establish identifiability up to Markov equivalence under mild
assumptions. We formulate the learning problem as a single differentiable
optimization task in the most general form, thereby avoiding the unrealistic
simplifications adopted by previous methods. Empirical results demonstrate that
our approach effectively captures complex relationships in discrete data.

</details>


### [328] [Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews](https://arxiv.org/abs/2509.21579)
*Mst Eshita Khatun,Halima Akter,Tasnimul Rehan,Toufiq Ahmed*

Main category: cs.LG

TL;DR: This study addresses fraudulent product reviews by employing advanced big data and machine learning approaches on Amazon reviews, achieving 90.35% detection accuracy using Logistic Regression.


<details>
  <summary>Details</summary>
Motivation: To tackle the growing issue of fraudulent online reviews that undermine consumer trust and affect seller reputations.

Method: Conducted analysis of a massive Amazon review dataset using a scalable big data framework and machine learning classifiers, focusing on logistic regression.

Result: Logistic Regression achieved a detection accuracy of 90.35% in identifying spam reviews.

Conclusion: The study helps enhance the authenticity of reviews, strengthening trust and transparency in online shopping through efficient spam detection techniques.

Abstract: In this digital era, online shopping is common practice in our daily lives.
Product reviews significantly influence consumer buying behavior and help
establish buyer trust. However, the prevalence of fraudulent reviews undermines
this trust by potentially misleading consumers and damaging the reputations of
the sellers. This research addresses this pressing issue by employing advanced
big data analytics and machine learning approaches on a substantial dataset of
Amazon product reviews. The primary objective is to detect and classify spam
reviews accurately so that it enhances the authenticity of the review. Using a
scalable big data framework, we efficiently process and analyze a large scale
of review data, extracting key features indicative of fraudulent behavior. Our
study illustrates the utility of various machine learning classifiers in
detecting spam reviews, with Logistic Regression achieving an accuracy of
90.35%, thus contributing to a more trustworthy and transparent online shopping
environment.

</details>


### [329] [A Systematic Review of Conformal Inference Procedures for Treatment Effect Estimation: Methods and Challenges](https://arxiv.org/abs/2509.21660)
*Pascal Memmesheimer,Vincent Heuveline,JÃ¼rgen Hesser*

Main category: cs.LG

TL;DR: The paper reviews conformal prediction methods for treatment effect estimation, highlighting their ability to quantify uncertainty and reliability across models with distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty issues in machine learning-based treatment effect estimation for informed decision-making in high-stakes areas such as healthcare and policy.

Method: A systematic review of eleven papers on conformal prediction methods for treatment effect estimation, providing theoretical background and discussing state-of-the-art techniques.

Result: Key insights are synthesized from the eleven papers, providing an overview of current conformal prediction applications and their effectiveness in quantifying uncertainty.

Conclusion: Conformal prediction methods can significantly improve treatment effect estimation, and the paper proposes future research directions to expand their applicability and robustness.

Abstract: Treatment effect estimation is essential for informed decision-making in many
fields such as healthcare, economics, and public policy. While flexible machine
learning models have been widely applied for estimating heterogeneous treatment
effects, quantifying the inherent uncertainty of their point predictions
remains an issue. Recent advancements in conformal prediction address this
limitation by allowing for inexpensive computation, as well as distribution
shifts, while still providing frequentist, finite-sample coverage guarantees
under minimal assumptions for any point-predictor model. This advancement holds
significant potential for improving decision-making in especially high-stakes
environments. In this work, we perform a systematic review regarding conformal
prediction methods for treatment effect estimation and provide for both the
necessary theoretical background. Through a systematic filtering process, we
select and analyze eleven key papers, identifying and describing current
state-of-the-art methods in this area. Based on our findings, we propose
directions for future research.

</details>


### [330] [Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection](https://arxiv.org/abs/2509.21606)
*Seohyeon Cha,Huancheng Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedProTIP improves federated continual learning by addressing catastrophic forgetting through projection-based updates and task identity prediction.


<details>
  <summary>Details</summary>
Motivation: FCL faces challenges such as catastrophic forgetting, privacy concerns, data heterogeneity, and constrained communication in decentralized environments.

Method: FedProTIP employs update projections onto orthogonal subspaces to avoid interference and uses core bases to dynamically predict task identity.

Result: FedProTIP achieves superior performance compared to state-of-the-art methods, especially in task-agnostic scenarios.

Conclusion: This framework effectively mitigates catastrophic forgetting while enhancing task-agnostic inference, contributing significantly to continual learning in federated settings.

Abstract: Federated continual learning (FCL) enables distributed client devices to
learn from streaming data across diverse and evolving tasks. A major challenge
to continual learning, catastrophic forgetting, is exacerbated in decentralized
settings by the data heterogeneity, constrained communication and privacy
concerns. We propose Federated gradient Projection-based Continual Learning
with Task Identity Prediction (FedProTIP), a novel FCL framework that mitigates
forgetting by projecting client updates onto the orthogonal complement of the
subspace spanned by previously learned representations of the global model.
This projection reduces interference with earlier tasks and preserves
performance across the task sequence. To further address the challenge of
task-agnostic inference, we incorporate a lightweight mechanism that leverages
core bases from prior tasks to predict task identity and dynamically adjust the
global model's outputs. Extensive experiments across standard FCL benchmarks
demonstrate that FedProTIP significantly outperforms state-of-the-art methods
in average accuracy, particularly in settings where task identities are a
priori unknown.

</details>


### [331] [Beyond Johnson-Lindenstrauss: Uniform Bounds for Sketched Bilinear Forms](https://arxiv.org/abs/2509.21847)
*Rohan Deb,Qiaobo Li,Mayank Shrivastava,Arindam Banerjee*

Main category: cs.LG

TL;DR: This work develops a framework for deriving uniform bounds on sketched bilinear forms, improving analyses important to machine learning and randomized algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing uniform bounds for sketched bilinear forms are either not applicable or suboptimal on general sets, necessitating improved analytical techniques.

Method: The authors use a general framework based on generic chaining and new techniques for handling suprema over pairs of sets. They also analyze cases involving sums of independent sketching matrices.

Result: The study establishes uniform bounds in terms of geometric complexities for sketched bilinear forms, recovers earlier results like the J-L lemma, extends RIP guarantees, and provides improved convergence for specific applications.

Conclusion: This unified approach presents sharper bounds for sketched operations, with applications in federated learning and bandit algorithms, leveraging geometric complexity over ambient dimension.

Abstract: Uniform bounds on sketched inner products of vectors or matrices underpin
several important computational and statistical results in machine learning and
randomized algorithms, including the Johnson-Lindenstrauss (J-L) lemma, the
Restricted Isometry Property (RIP), randomized sketching, and approximate
linear algebra. However, many modern analyses involve *sketched bilinear
forms*, for which existing uniform bounds either do not apply or are not sharp
on general sets. In this work, we develop a general framework to analyze such
sketched bilinear forms and derive uniform bounds in terms of geometric
complexities of the associated sets. Our approach relies on generic chaining
and introduces new techniques for handling suprema over pairs of sets. We
further extend these results to the setting where the bilinear form involves a
sum of $T$ independent sketching matrices and show that the deviation scales as
$\sqrt{T}$. This unified analysis recovers known results such as the J-L lemma
as special cases, while extending RIP-type guarantees. Additionally, we obtain
improved convergence bounds for sketched Federated Learning algorithms where
such cross terms arise naturally due to sketched gradient compression, and
design sketched variants of bandit algorithms with sharper regret bounds that
depend on the geometric complexity of the action and parameter sets, rather
than the ambient dimension.

</details>


### [332] [Causal Abstraction Inference under Lossy Representations](https://arxiv.org/abs/2509.21607)
*Kevin Xia,Elias Bareinboim*

Main category: cs.LG

TL;DR: This paper introduces projected abstractions to accommodate lossy representations in causal abstraction frameworks, enabling effective handling of high-dimensional causal queries.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of causal abstraction frameworks that fail to handle lossy abstraction functions effectively, where the abstract invariance condition is violated.

Method: They propose projected abstractions, provide formal definitions, construct models, translate causal queries, and prove new graphical criteria for estimating causal queries from limited low-level data.

Result: Projected abstractions effectively translate various types of causal queries between low and high levels, and prove practical utility in experimental image scenarios.

Conclusion: Projected abstractions expand the scope of causal abstraction frameworks, enabling robust handling of lossy representations and advancing causal reasoning in complex domains.

Abstract: The study of causal abstractions bridges two integral components of human
intelligence: the ability to determine cause and effect, and the ability to
interpret complex patterns into abstract concepts. Formally, causal abstraction
frameworks define connections between complicated low-level causal models and
simple high-level ones. One major limitation of most existing definitions is
that they are not well-defined when considering lossy abstraction functions in
which multiple low-level interventions can have different effects while mapping
to the same high-level intervention (an assumption called the abstract
invariance condition). In this paper, we introduce a new type of abstractions
called projected abstractions that generalize existing definitions to
accommodate lossy representations. We show how to construct a projected
abstraction from the low-level model and how it translates equivalent
observational, interventional, and counterfactual causal queries from low to
high-level. Given that the true model is rarely available in practice we prove
a new graphical criteria for identifying and estimating high-level causal
queries from limited low-level data. Finally, we experimentally show the
effectiveness of projected abstraction models in high-dimensional image
settings.

</details>


### [333] [Why High-rank Neural Networks Generalize?: An Algebraic Framework with RKHSs](https://arxiv.org/abs/2509.21895)
*Yuka Hashimoto,Sho Sonoda,Isao Ishikawa,Masahiro Ikeda*

Main category: cs.LG

TL;DR: The paper introduces a new Rademacher complexity bound for deep neural networks using Koopman operators, group representations, and RKHSs, explaining generalization for high-rank weight matrices.


<details>
  <summary>Details</summary>
Motivation: Existing Rademacher complexity bounds are limited in applicability to certain neural network models and fail to generalize effectively for models with high-rank weight matrices.

Method: Developed an algebraic representation of neural networks and a kernel function to construct an RKHS, leveraging Koopman operators and group representations.

Result: Derived a new Rademacher complexity bound for a broader range of realistic neural network models.

Conclusion: This Koopman-based framework enhances the understanding of generalization behavior for diverse practical neural network architectures.

Abstract: We derive a new Rademacher complexity bound for deep neural networks using
Koopman operators, group representations, and reproducing kernel Hilbert spaces
(RKHSs). The proposed bound describes why the models with high-rank weight
matrices generalize well. Although there are existing bounds that attempt to
describe this phenomenon, these existing bounds can be applied to limited types
of models. We introduce an algebraic representation of neural networks and a
kernel function to construct an RKHS to derive a bound for a wider range of
realistic models. This work paves the way for the Koopman-based theory for
Rademacher complexity bounds to be valid for more practical situations.

</details>


### [334] [Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching](https://arxiv.org/abs/2509.21912)
*Zhengyan Wan,Yidong Ouyang,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: cs.LG

TL;DR: The paper introduces a new guidance framework for sampling discrete data, improving efficiency and generality over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing guidance methods for discrete data rely on a first-order Taylor approximation, which can result in significant errors due to the discrete nature of state spaces.

Method: The paper proposes a framework that computes the exact transition rate for discrete data using a discrete flow matching model. This allows for efficient guidance in sampling with only a single forward pass per step.

Result: The framework is demonstrated to be effective in energy-guided simulations, text-to-image generation, and multimodal understanding, showing improvements in both efficiency and quality.

Conclusion: The proposed guidance framework presents a more accurate and efficient method for discrete data sampling, generalizing previous methods and being applicable to tasks like masked diffusion models and preference alignment.

Abstract: Guidance provides a simple and effective framework for posterior sampling by
steering the generation process towards the desired distribution. When modeling
discrete data, existing approaches mostly focus on guidance with the
first-order Taylor approximation to improve the sampling efficiency. However,
such an approximation is inappropriate in discrete state spaces since the
approximation error could be large. A novel guidance framework for discrete
data is proposed to address this problem: We derive the exact transition rate
for the desired distribution given a learned discrete flow matching model,
leading to guidance that only requires a single forward pass in each sampling
step, significantly improving efficiency. This unified novel framework is
general enough, encompassing existing guidance methods as special cases, and it
can also be seamlessly applied to the masked diffusion model. We demonstrate
the effectiveness of our proposed guidance on energy-guided simulations and
preference alignment on text-to-image generation and multimodal understanding
tasks. The code is available through
https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/main.

</details>


### [335] [Shoot from the HIP: Hessian Interatomic Potentials without derivatives](https://arxiv.org/abs/2509.21624)
*Andreas Burger,Luca Thiede,Nikolaj RÃ¸nne,Varinia Bernales,Nandita Vijaykumar,Tejs Vegge,Arghya Bhowmik,Alan Aspuru-Guzik*

Main category: cs.LG

TL;DR: The study introduces a deep learning method to directly predict molecular Hessians, providing faster and more accurate results than traditional computational approaches.


<details>
  <summary>Details</summary>
Motivation: Calculating molecular Hessians is computationally challenging and scales poorly with system size using existing quantum mechanical or neural network methods.

Method: The authors develop a method to predict SE(3)-equivariant, symmetric Hessians directly using irreducible representations up to degree $l=2$ computed by graph neural networks during message passing.

Result: The proposed method achieves significant improvements in speed, accuracy, memory efficiency, and scaling compared to traditional approaches, and excels in downstream tasks like transition state search and vibrational analysis.

Conclusion: Directly predicting Hessians through this novel deep learning approach demonstrates consistent superiority across various tasks, opening new possibilities with the open-sourced HIP codebase and models.

Abstract: Fundamental tasks in computational chemistry, from transition state search to
vibrational analysis, rely on molecular Hessians, which are the second
derivatives of the potential energy. Yet, Hessians are computationally
expensive to calculate and scale poorly with system size, with both quantum
mechanical methods and neural networks. In this work, we demonstrate that
Hessians can be predicted directly from a deep learning model, without relying
on automatic differentiation or finite differences. We observe that one can
construct SE(3)-equivariant, symmetric Hessians from irreducible
representations (irrep) features up to degree $l$=2 computed during message
passing in graph neural networks. This makes HIP Hessians one to two orders of
magnitude faster, more accurate, more memory efficient, easier to train, and
enables more favorable scaling with system size. We validate our predictions
across a wide range of downstream tasks, demonstrating consistently superior
performance for transition state search, accelerated geometry optimization,
zero-point energy corrections, and vibrational analysis benchmarks. We
open-source the HIP codebase and model weights to enable further development of
the direct prediction of Hessians at https://github.com/BurgerAndreas/hip

</details>


### [336] [Convexity-Driven Projection for Point Cloud Dimensionality Reduction](https://arxiv.org/abs/2509.22043)
*Suman Sanyal*

Main category: cs.LG

TL;DR: The paper introduces Convexity-Driven Projection (CDP), a linear dimensionality reduction method for point clouds focusing on maintaining local non-convexity.


<details>
  <summary>Details</summary>
Motivation: To create a dimensionality reduction method that effectively preserves detour-induced local non-convexity in point cloud data, overcoming the challenges posed by boundary effects in other techniques.

Method: The method employs a $k$-NN graph to identify important point pairs, computes a structure matrix from their normalized directions, and uses its top-$k$ eigenvectors for projection. Guarantees on distortion and direction energy capture are provided.

Result: The technique offers pairwise guarantees on distortion and provides spectral bounds for expected performance. The evaluation method is designed for practitioners to verify these guarantees.

Conclusion: CDP is a boundary-free, verifiable method that retains critical non-convex features in dimensionality reduction tasks, with clear performance guarantees for the user.

Abstract: We propose Convexity-Driven Projection (CDP), a boundary-free linear method
for dimensionality reduction of point clouds that targets preserving
detour-induced local non-convexity. CDP builds a $k$-NN graph, identifies
admissible pairs whose Euclidean-to-shortest-path ratios are below a threshold,
and aggregates their normalized directions to form a positive semidefinite
non-convexity structure matrix. The projection uses the top-$k$ eigenvectors of
the structure matrix. We give two verifiable guarantees. A pairwise
a-posteriori certificate that bounds the post-projection distortion for each
admissible pair, and an average-case spectral bound that links expected
captured direction energy to the spectrum of the structure matrix, yielding
quantile statements for typical distortion. Our evaluation protocol reports
fixed- and reselected-pairs detour errors and certificate quantiles, enabling
practitioners to check guarantees on their data.

</details>


### [337] [Blockwise Hadamard high-Rank Adaptation for Parameter-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2509.21637)
*Feng Yu,Jia Hu,Geyong Min*

Main category: cs.LG

TL;DR: The paper introduces Block Hadamard high-Rank Adaptation (BHRA), a PEFT method targeting efficient fine-tuning by partitioning weight matrices to enhance rank modulation while staying resource-effective.


<details>
  <summary>Details</summary>
Motivation: Classical PEFT methods like LoRA are limited in making heterogeneous transformations due to constraints in nominal rank, and current extensions either amplify ranks globally or sacrifice inductive biases.

Method: The proposed BHRA method partitions each weight matrix into blocks and applies localized Hadamard modulation within each block, maintaining low parameter usage while improving adaptability.

Result: BHRA shows improved performance over PEFT baselines on eight reasoning tasks and two arithmetic benchmarks, tested across multiple models like Llama and Mistral under equal parameter budgets.

Conclusion: BHRA effectively enhances the capabilities of PEFT methods by introducing localized rank amplification, achieving both efficiency and superior task performance.

Abstract: Parameter-efficient fine-tuning (PEFT) methods must be resource-efficient yet
handle heterogeneous reasoning transformations, and classical low-rank
adaptation (LoRA) is constrained by the nominal rank $r$. Hadamard-style
extensions like HiRA raise the nominal rank but couple every update to the
global energy pattern of the frozen weight matrix, while ABBA trades this
inductive bias for fully learned dense intermediates. To address the limitation
of global modulation, we propose Block Hadamard high-Rank Adaptation (BHRA),
which partitions each weight matrix and applies HiRA-style multiplicative
modulation independently within every block, preserving the PEFT parameter
footprint while unlocking localized rank amplification. Our empirical analyses
reveal that this blockwise design maintains rich spectra across rank budgets,
mitigating the collapse induced by global modulation. Across eight commonsense
reasoning tasks and two arithmetic benchmarks with Llama-3.2 1B/3B, Mistral-7B,
and Gemma-2 9B, BHRA consistently surpasses strong PEFT baselines under matched
parameter budgets.

</details>


### [338] [Understanding and Enhancing Mask-Based Pretraining towards Universal Representations](https://arxiv.org/abs/2509.21650)
*Mingze Dong,Leda Wang,Yuval Kluger*

Main category: cs.LG

TL;DR: The paper examines the behavior and limits of mask-based pretraining methods, introduces a theoretical framework, and proposes a new pretraining scheme (R$^2$MAE) that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the unclear role and limitations of mask-based pretraining in learning data representations and improve pretraining techniques across language, vision, and biology domains.

Method: The authors present a theoretical framework to study mask-based pretraining using linear regression, validate their findings on diverse neural architectures, and develop a new pretraining scheme called R$^2$MAE.

Result: The proposed R$^2$MAE pretraining scheme outperforms standard and complex masking techniques across diverse applications, including vision, language, DNA sequences, and single-cell models.

Conclusion: R$^2$MAE enhances the effectiveness of mask-based pretraining by incorporating multi-scale features, providing consistent improvements for state-of-the-art models across various domains.

Abstract: Mask-based pretraining has become a cornerstone of modern large-scale models
across language, vision, and recently biology. Despite its empirical success,
its role and limits in learning data representations have been unclear. In this
work, we show that the behavior of mask-based pretraining can be directly
characterized by test risk in high-dimensional minimum-norm ("ridge-less")
linear regression, without relying on further model specifications. Further
analysis of linear models uncovers several novel aspects of mask-based
pretraining. The theoretical framework and its implications have been validated
across diverse neural architectures (including MLPs, CNNs, and Transformers)
applied to both vision and language tasks. Guided by our theory, we propose an
embarrassingly simple yet overlooked pretraining scheme named Randomly Random
Mask AutoEncoding (R$^2$MAE), which enforces capturing multi-scale features
from data and is able to outperform optimal fixed mask ratio settings in our
linear model framework. We implement R$^2$MAE in vision, language, DNA
sequence, and single-cell models, where it consistently outperforms standard
and more complicated masking schemes, leading to improvements for
state-of-the-art models. Our code is available at:
https://github.com/MingzeDong/r2mae

</details>


### [339] [SHAKE-GNN: Scalable Hierarchical Kirchhoff-Forest Graph Neural Network](https://arxiv.org/abs/2509.22100)
*Zhipu Cui,Johannes Lutzeyer*

Main category: cs.LG

TL;DR: SHAKE-GNN introduces a scalable framework for graph-level tasks using stochastic multi-resolution decompositions based on Kirchhoff Forests, balancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Scaling GNNs to large graphs is challenging, particularly for graph-level tasks, necessitating new approaches to enhance both scalability and performance.

Method: SHAKE-GNN leverages Kirchhoff Forests to create multi-scale graph representations and incorporates a new data-driven strategy for selecting trade-off parameters.

Result: SHAKE-GNN demonstrates competitive results on large graph classification benchmarks and handles scalability issues effectively.

Conclusion: SHAKE-GNN provides a scalable and efficient alternative for graph-level tasks in GNNs, with strong performance on large-scale problems.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across a range
of learning tasks. However, scaling GNNs to large graphs remains a significant
challenge, especially for graph-level tasks. In this work, we introduce
SHAKE-GNN, a novel scalable graph-level GNN framework based on a hierarchy of
Kirchhoff Forests, a class of random spanning forests used to construct
stochastic multi-resolution decompositions of graphs. SHAKE-GNN produces
multi-scale representations, enabling flexible trade-offs between efficiency
and performance. We introduce an improved, data-driven strategy for selecting
the trade-off parameter and analyse the time-complexity of SHAKE-GNN.
Experimental results on multiple large-scale graph classification benchmarks
demonstrate that SHAKE-GNN achieves competitive performance while offering
improved scalability.

</details>


### [340] [Limitations on Safe, Trusted, Artificial General Intelligence](https://arxiv.org/abs/2509.21654)
*Rina Panigrahy,Vatsal Sharan*

Main category: cs.LG

TL;DR: The paper introduces mathematical definitions for safety, trust, and Artificial General Intelligence (AGI) in AI systems, showing that a safe and trusted AI cannot be an AGI due to fundamental incompatibilities.


<details>
  <summary>Details</summary>
Motivation: Safety, trust, and AGI are widely discussed goals in AI systems, but the lack of rigorous definitions for these concepts has led to inconsistent interpretations.

Method: Mathematical definitions were proposed for the notions of safety, trust, and AGI, followed by proofs to demonstrate incompatibilities between these definitions. The results were derived using program verification, planning, and graph reachability tasks, inspired by concepts like GÃ¶del's incompleteness theorems and Turing's undecidability proof.

Result: The findings reveal that under strict mathematical definitions, a safe and trusted AI system cannot possess AGI capabilities, due to the existence of tasks solvable by humans but unsolvable by the system.

Conclusion: Mathematical definitions of safety, trust, and AGI highlight that achieving all three simultaneously in an AI system is fundamentally incompatible. However, real-world systems may adopt practical interpretations instead.

Abstract: Safety, trust and Artificial General Intelligence (AGI) are aspirational
goals in artificial intelligence (AI) systems, and there are several informal
interpretations of these notions. In this paper, we propose strict,
mathematical definitions of safety, trust, and AGI, and demonstrate a
fundamental incompatibility between them. We define safety of a system as the
property that it never makes any false claims, trust as the assumption that the
system is safe, and AGI as the property of an AI system always matching or
exceeding human capability. Our core finding is that -- for our formal
definitions of these notions -- a safe and trusted AI system cannot be an AGI
system: for such a safe, trusted system there are task instances which are
easily and provably solvable by a human but not by the system. We note that we
consider strict mathematical definitions of safety and trust, and it is
possible for real-world deployments to instead rely on alternate, practical
interpretations of these notions. We show our results for program verification,
planning, and graph reachability. Our proofs draw parallels to G\"odel's
incompleteness theorems and Turing's proof of the undecidability of the halting
problem, and can be regarded as interpretations of G\"odel's and Turing's
results.

</details>


### [341] [Mechanistic Independence: A Principle for Identifiable Disentangled Representations](https://arxiv.org/abs/2509.22196)
*Stefan Matthes,Zhiwei Han,Hao Shen*

Main category: cs.LG

TL;DR: This paper develops a framework for disentangled representations based on mechanistic independence, enabling identifiability of latent factors without relying on statistical assumptions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve understanding and identifiability of disentangled representations, which are essential for recovering latent factors of observed data.

Method: The authors propose a mechanistic independence framework with several independence criteria, involving support, sparsity, and higher-order conditions, to identify latent subspaces under nonlinear, non-invertible mixing.

Result: They show that disentangled representations can be achieved and identified via their framework, establishing a hierarchy among the criteria and providing graph-theoretic insights into latent subspaces.

Conclusion: The study elucidates conditions for identifying disentangled representations, advancing the field without dependency on specific latent density or statistical assumptions.

Abstract: Disentangled representations seek to recover latent factors of variation
underlying observed data, yet their identifiability is still not fully
understood. We introduce a unified framework in which disentanglement is
achieved through mechanistic independence, which characterizes latent factors
by how they act on observed variables rather than by their latent distribution.
This perspective is invariant to changes of the latent density, even when such
changes induce statistical dependencies among factors. Within this framework,
we propose several related independence criteria -- ranging from support-based
and sparsity-based to higher-order conditions -- and show that each yields
identifiability of latent subspaces, even under nonlinear, non-invertible
mixing. We further establish a hierarchy among these criteria and provide a
graph-theoretic characterization of latent subspaces as connected components.
Together, these results clarify the conditions under which disentangled
representations can be identified without relying on statistical assumptions.

</details>


### [342] [RED-DiffEq: Regularization by denoising diffusion models for solving inverse PDE problems with application to full waveform inversion](https://arxiv.org/abs/2509.21659)
*Siming Shan,Min Zhu,Youzuo Lin,Lu Lu*

Main category: cs.LG

TL;DR: The paper proposes RED-DiffEq, a framework that uses pretrained diffusion models for regularization in PDE-governed inverse problems, improving accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: PDE-governed inverse problems, pivotal in many fields, face challenges like nonlinearity, ill-posedness, and noise sensitivity.

Method: The approach integrates physics-driven inversion with data-driven learning by employing pretrained diffusion models as regularization mechanisms.

Result: Applied to seismic imaging, RED-DiffEq demonstrated improved accuracy, robustness, and generalization to complex, untrained velocity models.

Conclusion: RED-DiffEq is effective in seismic imaging and adaptable across diverse PDE-governed inverse problems, showcasing enhanced performance and generalization.

Abstract: Partial differential equation (PDE)-governed inverse problems are fundamental
across various scientific and engineering applications; yet they face
significant challenges due to nonlinearity, ill-posedness, and sensitivity to
noise. Here, we introduce a new computational framework, RED-DiffEq, by
integrating physics-driven inversion and data-driven learning. RED-DiffEq
leverages pretrained diffusion models as a regularization mechanism for
PDE-governed inverse problems. We apply RED-DiffEq to solve the full waveform
inversion problem in geophysics, a challenging seismic imaging technique that
seeks to reconstruct high-resolution subsurface velocity models from seismic
measurement data. Our method shows enhanced accuracy and robustness compared to
conventional methods. Additionally, it exhibits strong generalization ability
to more complex velocity models that the diffusion model is not trained on. Our
framework can also be directly applied to diverse PDE-governed inverse
problems.

</details>


### [343] [Global Convergence in Neural ODEs: Impact of Activation Functions](https://arxiv.org/abs/2509.22436)
*Tianxiang Gao,Siyuan Sun,Hailiang Liu,Hongyang Gao*

Main category: cs.LG

TL;DR: The paper focuses on improving the training dynamics of Neural ODEs by studying the impact of activation function properties, such as smoothness and nonlinearity, for better convergence and efficient computation.


<details>
  <summary>Details</summary>
Motivation: Neural ODEs face challenges in training due to gradient computation accuracy and convergence issues, motivating an investigation into how activation functions affect these aspects.

Method: Analyzing the role of smoothness and nonlinearity in activation functions to ensure globally unique solutions for ODEs and maintaining spectral properties of Neural Tangent Kernel. Theoretical findings are validated with numerical experiments.

Result: The study establishes global convergence of Neural ODEs under gradient descent in overparameterized regimes, supported by experimental evidence.

Conclusion: The analysis identifies optimal characteristics of activation functions (such as smoothness and sufficient nonlinearity) as crucial for efficient Neural ODE training, offering guidelines for scaling and practical improvements.

Abstract: Neural Ordinary Differential Equations (ODEs) have been successful in various
applications due to their continuous nature and parameter-sharing efficiency.
However, these unique characteristics also introduce challenges in training,
particularly with respect to gradient computation accuracy and convergence
analysis. In this paper, we address these challenges by investigating the
impact of activation functions. We demonstrate that the properties of
activation functions, specifically smoothness and nonlinearity, are critical to
the training dynamics. Smooth activation functions guarantee globally unique
solutions for both forward and backward ODEs, while sufficient nonlinearity is
essential for maintaining the spectral properties of the Neural Tangent Kernel
(NTK) during training. Together, these properties enable us to establish the
global convergence of Neural ODEs under gradient descent in overparameterized
regimes. Our theoretical findings are validated by numerical experiments, which
not only support our analysis but also provide practical guidelines for scaling
Neural ODEs, potentially leading to faster training and improved performance in
real-world applications.

</details>


### [344] [MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought Object State Reasoning](https://arxiv.org/abs/2509.21662)
*Afrina Tabassum,Bin Guo,Xiyao Ma,Hoda Eldardiry,Ismini Lourentzou*

Main category: cs.LG

TL;DR: MMPlanner is a zero-shot framework for multimodal procedural planning (MPP) that improves textual planning, cross-modal alignment, and visual step ordering significantly.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of maintaining object-state consistency across modalities in multimodal procedural planning and enhancing the quality of generated plans.

Method: The study introduces MMPlanner, incorporating Object State Reasoning Chain-of-Thought (OSR-CoT) prompting for object-state transitions and LLM-as-a-judge protocols for systematic evaluation.

Result: MMPlanner achieves state-of-the-art results, enhancing textual planning accuracy by +6.8%, cross-modal alignment by +11.9%, and visual step ordering by +26.7% on RECIPEPLAN and WIKIPLAN datasets.

Conclusion: MMPlanner's innovative approach improves multimodal planning quality, with explicit modeling of object states and robust evaluation metrics.

Abstract: Multimodal Procedural Planning (MPP) aims to generate step-by-step
instructions that combine text and images, with the central challenge of
preserving object-state consistency across modalities while producing
informative plans. Existing approaches often leverage large language models
(LLMs) to refine textual steps; however, visual object-state alignment and
systematic evaluation are largely underexplored. We present MMPlanner, a
zero-shot MPP framework that introduces Object State Reasoning Chain-of-Thought
(OSR-CoT) prompting to explicitly model object-state transitions and generate
accurate multimodal plans. To assess plan quality, we design LLM-as-a-judge
protocols for planning accuracy and cross-modal alignment, and further propose
a visual step-reordering task to measure temporal coherence. Experiments on
RECIPEPLAN and WIKIPLAN show that MMPlanner achieves state-of-the-art
performance, improving textual planning by +6.8%, cross-modal alignment by
+11.9%, and visual step ordering by +26.7%

</details>


### [345] [Logic of Hypotheses: from Zero to Full Knowledge in Neurosymbolic Integration](https://arxiv.org/abs/2509.21663)
*Davide Bizzaro,Alessandro Daniele*

Main category: cs.LG

TL;DR: The paper introduces Logic of Hypotheses (LoH), a language that combines neural learning and symbolic reasoning through a choice operator and fuzzy logic for interpretable and high-performing decision models.


<details>
  <summary>Details</summary>
Motivation: To unify and advance the integration of neural-network learning with symbolic reasoning, addressing the need for a flexible framework that combines data-driven and human-defined rules.

Method: The authors developed Logic of Hypotheses (LoH), extending propositional logic syntax with a learnable choice operator and leveraging Goedel fuzzy logic. LoH enables direct compilation into a differentiable graph, trainable via backpropagation.

Result: LoH subsumes existing NeSy models, simplifies integration of symbolic and neural methods, and achieves strong empirical results on both tabular data and the Visual Tic-Tac-Toe NeSy task while delivering interpretable decision rules.

Conclusion: The framework is a versatile and powerful tool for combining symbolic priors with neural learning, showing promise for practical applications requiring symbolic interpretability and strong performance.

Abstract: Neurosymbolic integration (NeSy) blends neural-network learning with symbolic
reasoning. The field can be split between methods injecting hand-crafted rules
into neural models, and methods inducing symbolic rules from data. We introduce
Logic of Hypotheses (LoH), a novel language that unifies these strands,
enabling the flexible integration of data-driven rule learning with symbolic
priors and expert knowledge. LoH extends propositional logic syntax with a
choice operator, which has learnable parameters and selects a subformula from a
pool of options. Using fuzzy logic, formulas in LoH can be directly compiled
into a differentiable computational graph, so the optimal choices can be
learned via backpropagation. This framework subsumes some existing NeSy models,
while adding the possibility of arbitrary degrees of knowledge specification.
Moreover, the use of Goedel fuzzy logic and the recently developed Goedel trick
yields models that can be discretized to hard Boolean-valued functions without
any loss in performance. We provide experimental analysis on such models,
showing strong results on tabular data and on the Visual Tic-Tac-Toe NeSy task,
while producing interpretable decision rules.

</details>


### [346] [DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks](https://arxiv.org/abs/2509.21666)
*Joshua Salim,Jordan Yu,Xilei Zhao*

Main category: cs.LG

TL;DR: This paper introduces DIM, a regularization method that enforces domain-informed monotonicity to enhance predictive performance and address overfitting in deep learning models.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often overfit due to their complexity and memorize noise instead of learning generalizable patterns. A method to maintain domain-informed monotonic relationships is required to address this issue.

Method: The DIM method penalizes violations of monotonic relationships relative to a linear baseline during training. It uses a mathematical framework to establish a reference, measure deviations, and integrate monotonicity constraints into the objective function.

Result: Experiments on a real-world ridesourcing dataset and synthetic data demonstrated that modest monotonicity constraints consistently improved model performance.

Conclusion: DIM is effective in enhancing the predictive performance of neural networks and reducing overfitting by ensuring domain-informed monotonicity constraints.

Abstract: While deep learning models excel at predictive tasks, they often overfit due
to their complex structure and large number of parameters, causing them to
memorize training data, including noise, rather than learn patterns that
generalize to new data. To tackle this challenge, this paper proposes a new
regularization method, i.e., Enforcing Domain-Informed Monotonicity in Deep
Neural Networks (DIM), which maintains domain-informed monotonic relationships
in complex deep learning models to further improve predictions. Specifically,
our method enforces monotonicity by penalizing violations relative to a linear
baseline, effectively encouraging the model to follow expected trends while
preserving its predictive power. We formalize this approach through a
comprehensive mathematical framework that establishes a linear reference,
measures deviations from monotonic behavior, and integrates these measurements
into the training objective. We test and validate the proposed methodology
using a real-world ridesourcing dataset from Chicago and a synthetically
created dataset. Experiments across various neural network architectures show
that even modest monotonicity constraints consistently enhance model
performance. DIM enhances the predictive performance of deep neural networks by
applying domain-informed monotonicity constraints to regularize model behavior
and mitigate overfitting

</details>


### [347] [A Theoretical Analysis of Discrete Flow Matching Generative Models](https://arxiv.org/abs/2509.22623)
*Maojiang Su,Mingcheng Lu,Jerry Yao-Chieh Hu,Shang Wu,Zhao Song,Alex Reneau,Han Liu*

Main category: cs.LG

TL;DR: The paper presents a theoretical analysis for Discrete Flow Matching (DFM) generative models, emphasizing how the learned velocity field controls the total variation distance between generated and target distributions.


<details>
  <summary>Details</summary>
Motivation: To provide formal guarantees showing that DFM models can converge to true data distributions as training data size increases.

Method: The analysis involves decomposing the distribution estimation error into approximation and estimation errors, studying Transformer architecture capacity, and deriving statistical convergence rates for finite datasets.

Result: Establishes bounds on both approximation and estimation errors, proving that DFM models can accurately approximate the target distribution with sufficient training data.

Conclusion: Trained DFM models are theoretically justified to converge to the true data distribution, bridging the gap between model design and formal guarantees.

Abstract: We provide a theoretical analysis for end-to-end training Discrete Flow
Matching (DFM) generative models. DFM is a promising discrete generative
modeling framework that learns the underlying generative dynamics by training a
neural network to approximate the transformative velocity field. Our analysis
establishes a clear chain of guarantees by decomposing the final distribution
estimation error. We first prove that the total variation distance between the
generated and target distributions is controlled by the risk of the learned
velocity field. We then bound this risk by analyzing its two primary sources:
(i) Approximation Error, where we quantify the capacity of the Transformer
architecture to represent the true velocity, and (ii) Estimation Error, where
we derive statistical convergence rates that bound the error from training on a
finite dataset. By composing these results, we provide the first formal proof
that the distribution generated by a trained DFM model provably converges to
the true data distribution as the training set size increases.

</details>


### [348] [SlotFM: A Motion Foundation Model with Slot Attention for Diverse Downstream Tasks](https://arxiv.org/abs/2509.21673)
*Junyong Park,Oron Levy,Rebecca Adaimi,Asaf Liberman,Gierad Laput,Abdelkareem Bedri*

Main category: cs.LG

TL;DR: The paper presents SlotFM, an accelerometer foundation model that uses Time-Frequency Slot Attention to generalize across diverse tasks, achieving significant improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current foundation models for accelerometer data focus heavily on classifying daily activities, limiting their utility for broader tasks reliant on diverse signal characteristics.

Method: The proposed SlotFM model employs Time-Frequency Slot Attention to generate embeddings capturing unique signal components and introduces two loss regularizers to enhance reconstruction and preserve task-relevant information.

Result: SlotFM was evaluated on 16 downstream tasks, outperforming current self-supervised methods on 13 tasks and achieving comparable performance on the remaining 3, with an average 4.5% performance gain.

Conclusion: SlotFM demonstrates superior generalization capabilities for wearable sensing tasks, extending the applicability of accelerometer data across diverse downstream applications.

Abstract: Wearable accelerometers are used for a wide range of applications, such as
gesture recognition, gait analysis, and sports monitoring. Yet most existing
foundation models focus primarily on classifying common daily activities such
as locomotion and exercise, limiting their applicability to the broader range
of tasks that rely on other signal characteristics. We present SlotFM, an
accelerometer foundation model that generalizes across diverse downstream
tasks. SlotFM uses Time-Frequency Slot Attention, an extension of Slot
Attention that processes both time and frequency representations of the raw
signals. It generates multiple small embeddings (slots), each capturing
different signal components, enabling task-specific heads to focus on the most
relevant parts of the data. We also introduce two loss regularizers that
capture local structure and frequency patterns, which improve reconstruction of
fine-grained details and helps the embeddings preserve task-relevant
information. We evaluate SlotFM on 16 classification and regression downstream
tasks that extend beyond standard human activity recognition. It outperforms
existing self-supervised approaches on 13 of these tasks and achieves
comparable results to the best performing approaches on the remaining tasks. On
average, our method yields a 4.5% performance gain, demonstrating strong
generalization for sensing foundation models.

</details>


### [349] [Scalable Second-order Riemannian Optimization for $K$-means Clustering](https://arxiv.org/abs/2509.21675)
*Peng Xu,Chun-Ying Hou,Xiaohui Chen,Richard Y. Zhang*

Main category: cs.LG

TL;DR: The paper introduces a novel smooth optimization approach for the $K$-means problem using Riemannian Newton algorithms to improve efficiency and convergence, achieving competitive statistical accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in constraint feasibility and objective optimality in current algorithms for $K$-means clustering problems, and to improve convergence guarantees.

Method: The paper formulates $K$-means as smooth unconstrained optimization over a submanifold and employs a cubic-regularized Riemannian Newton algorithm for solving the problem, leveraging manifold factorization to achieve linear-time computation for Newton subproblems.

Result: The proposed method demonstrates faster convergence compared to first-order nonnegative factorization techniques, with similar statistical accuracy.

Conclusion: This approach provides an effective balance between optimization and computational feasibility in $K$-means clustering, paving the way for improved algorithms with rigorous second-order guarantees.

Abstract: Clustering is a hard discrete optimization problem. Nonconvex approaches such
as low-rank semidefinite programming (SDP) have recently demonstrated promising
statistical and local algorithmic guarantees for cluster recovery. Due to the
combinatorial structure of the $K$-means clustering problem, current relaxation
algorithms struggle to balance their constraint feasibility and objective
optimality, presenting tremendous challenges in computing the second-order
critical points with rigorous guarantees. In this paper, we provide a new
formulation of the $K$-means problem as a smooth unconstrained optimization
over a submanifold and characterize its Riemannian structures to allow it to be
solved using a second-order cubic-regularized Riemannian Newton algorithm. By
factorizing the $K$-means manifold into a product manifold, we show how each
Newton subproblem can be solved in linear time. Our numerical experiments show
that the proposed method converges significantly faster than the
state-of-the-art first-order nonnegative low-rank factorization method, while
achieving similarly optimal statistical accuracy.

</details>


### [350] [SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding](https://arxiv.org/abs/2509.21689)
*Thomas Walton,Darin Tsui,Aryan Musharaf,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: SpecMER enhances protein sequence generation by integrating biological, structural, and functional priors, allowing faster and more plausible outputs compared to traditional autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in high-throughput protein screening caused by latency in autoregressive models, and biologically implausible outputs from typical draft models.

Method: SpecMER combines speculative decoding with guidance from k-mer motifs, scoring candidate sequences based on biological patterns to improve plausibility and maintain efficiency.

Result: SpecMER achieves a 24-32% speedup in generation, higher model acceptance rates, and improvements in the quality of sequence likelihoods.

Conclusion: The framework improves the biological plausibility and computational efficiency of protein sequence generation, offering a practical solution for high-throughput applications.

Abstract: Autoregressive models have transformed protein engineering by enabling the
generation of novel protein sequences beyond those found in nature. However,
their sequential inference introduces significant latency, limiting their
utility in high-throughput protein screening. Speculative decoding accelerates
generation by employing a lightweight draft model to sample tokens, which a
larger target model then verifies and refines. Yet, in protein sequence
generation, draft models are typically agnostic to the structural and
functional constraints of the target protein, leading to biologically
implausible outputs and a shift in the likelihood distribution of generated
sequences. We introduce SpecMER (Speculative Decoding via k-mer Guidance), a
novel framework that incorporates biological, structural, and functional priors
using k-mer motifs extracted from multiple sequence alignments. By scoring
candidate sequences in parallel and selecting those most consistent with known
biological patterns, SpecMER significantly improves sequence plausibility while
retaining the efficiency of speculative decoding. SpecMER achieves 24-32%
speedup over standard autoregressive decoding, along with higher acceptance
rates and improved sequence likelihoods.

</details>


### [351] [Wav2Arrest 2.0: Long-Horizon Cardiac Arrest Prediction with Time-to-Event Modeling, Identity-Invariance, and Pseudo-Lab Alignment](https://arxiv.org/abs/2509.21695)
*Saurabh Kataria,Davood Fattahi,Minxiao Wang,Ran Xiao,Matthew Clark,Timothy Ruchti,Mark Mai,Xiao Hu*

Main category: cs.LG

TL;DR: This paper enhances cardiac arrest prediction using PPG data by introducing three improvements: time-to-event modeling, patient-identity invariant feature learning, and pseudo-lab value regression. These advancements improve prediction accuracy, especially over longer time horizons.


<details>
  <summary>Details</summary>
Motivation: Improving cardiac arrest prediction from PPG data is essential for better early warning systems, particularly when labels or data are scarce.

Method: The paper utilizes time-to-event modeling, adversarial deconfounding for patient features via a biometric model (p-vector), and regression on pseudo-lab values generated by pre-trained networks. It also applies PCGrad optimization to manage gradient conflicts in multi-task learning.

Result: These methods independently boost 24-hour prediction AUC from 0.74 to 0.78-0.80, showing significant improvements over longer timeframes without performance loss near critical events.

Conclusion: The proposed methods enhance the capability of physiological models to predict cardiac arrest more accurately and reliably, especially in minimal data scenarios, advancing the field of early warning systems.

Abstract: High-frequency physiological waveform modality offers deep, real-time
insights into patient status. Recently, physiological foundation models based
on Photoplethysmography (PPG), such as PPG-GPT, have been shown to predict
critical events, including Cardiac Arrest (CA). However, their powerful
representation still needs to be leveraged suitably, especially when the
downstream data/label is scarce. We offer three orthogonal improvements to
improve PPG-only CA systems by using minimal auxiliary information. First, we
propose to use time-to-event modeling, either through simple regression to the
event onset time or by pursuing fine-grained discrete survival modeling.
Second, we encourage the model to learn CA-focused features by making them
patient-identity invariant. This is achieved by first training the
largest-scale de-identified biometric identification model, referred to as the
p-vector, and subsequently using it adversarially to deconfound cues, such as
person identity, that may cause overfitting through memorization. Third, we
propose regression on the pseudo-lab values generated by pre-trained auxiliary
estimator networks. This is crucial since true blood lab measurements, such as
lactate, sodium, troponin, and potassium, are collected sparingly. Via
zero-shot prediction, the auxiliary networks can enrich cardiac arrest waveform
labels and generate pseudo-continuous estimates as targets. Our proposals can
independently improve the 24-hour time-averaged AUC from the 0.74 to the
0.78-0.80 range. We primarily improve over longer time horizons with minimal
degradation near the event, thus pushing the Early Warning System research.
Finally, we pursue multi-task formulation and diagnose it with a high gradient
conflict rate among competing losses, which we alleviate via the PCGrad
optimization technique.

</details>


### [352] [Exact Subgraph Isomorphism Network for Predictive Graph Mining](https://arxiv.org/abs/2509.21699)
*Taiga Kojima,Masayuki Karasuyama*

Main category: cs.LG

TL;DR: The paper introduces a model, Exact subgraph Isomorphism Network (EIN), which combines subgraph enumeration, neural networks, and sparse regularization to improve graph-level prediction, achieving both interpretability and high discriminative ability.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of building a graph-level prediction model that balances high prediction accuracy with interpretability while managing computational complexity.

Method: The proposed EIN uses exact subgraph enumeration, a neural network for learning, and sparse regularization to improve prediction accuracy while enabling subgraph pruning and interpretability.

Result: EIN demonstrates high prediction performance compared to standard graph neural network models and supports post-hoc analyses through the identification of important subgraphs.

Conclusion: EIN successfully enhances both the discriminative ability and interpretability of graph-level prediction tasks by leveraging subgraph structures and applying sparse regularization.

Abstract: In the graph-level prediction task (predict a label for a given graph), the
information contained in subgraphs of the input graph plays a key role. In this
paper, we propose Exact subgraph Isomorphism Network (EIN), which combines the
exact subgraph enumeration, neural network, and a sparse regularization. In
general, building a graph-level prediction model achieving high discriminative
ability along with interpretability is still a challenging problem. Our
combination of the subgraph enumeration and neural network contributes to high
discriminative ability about the subgraph structure of the input graph.
Further, the sparse regularization in EIN enables us 1) to derive an effective
pruning strategy that mitigates computational difficulty of the enumeration
while maintaining the prediction performance, and 2) to identify important
subgraphs that contributes to high interpretability. We empirically show that
EIN has sufficiently high prediction performance compared with standard graph
neural network models, and also, we show examples of post-hoc analysis based on
the selected subgraphs.

</details>


### [353] [Downscaling human mobility data based on demographic socioeconomic and commuting characteristics using interpretable machine learning methods](https://arxiv.org/abs/2509.21703)
*Yuqin Jiang,Andrey A. Popov,Tianle Duan,Qingchun Li*

Main category: cs.LG

TL;DR: The paper proposes a machine learning framework to downscale NY taxi OD trip flows into smaller spatial units, assessing model performance and variable impacts.


<details>
  <summary>Details</summary>
Motivation: Understanding urban human mobility at finer spatial levels aids in social science applications like transportation improvement and urban planning.

Method: Four machine learning models (LR, RF, SVM, NN) were used to explore variable correlations. Sensitivity analysis was deployed to understand variable importance in nonlinear models.

Result: NN had the best performance on the datasets; however, SVM showed superior generalization for downscaling tasks.

Conclusion: The framework offers analytical and practical solutions to enhance transportation systems and urban development strategies.

Abstract: Understanding urban human mobility patterns at various spatial levels is
essential for social science. This study presents a machine learning framework
to downscale origin-destination (OD) taxi trips flows in New York City from a
larger spatial unit to a smaller spatial unit. First, correlations between OD
trips and demographic, socioeconomic, and commuting characteristics are
developed using four models: Linear Regression (LR), Random Forest (RF),
Support Vector Machine (SVM), and Neural Networks (NN). Second, a
perturbation-based sensitivity analysis is applied to interpret variable
importance for nonlinear models. The results show that the linear regression
model failed to capture the complex variable interactions. While NN performs
best with the training and testing datasets, SVM shows the best generalization
ability in downscaling performance. The methodology presented in this study
provides both analytical advancement and practical applications to improve
transportation services and urban development.

</details>


### [354] [PQFed: A Privacy-Preserving Quality-Controlled Federated Learning Framework](https://arxiv.org/abs/2509.21704)
*Weiqi Yue,Wenbiao Li,Yuzhou Jiang,Anisa Halimi,Roger French,Erman Ayday*

Main category: cs.LG

TL;DR: PQFed is a new framework for personalized federated learning that enables customized training strategies by clustering clients based on data similarity, improving model performance in scenarios with data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with data heterogeneity, which hampers the performance of the global model. Existing methods often train a global model with all clients and adapt locally later, but these approaches may not address varying data distributions effectively.

Method: PQFed extracts features from each client's raw data, applies clustering to estimate data similarity, and implements client selection strategies to allow collaboration among clients with similar data distributions before federated training.

Result: Experimental results using CIFAR-10 and MNIST datasets show that PQFed improves the target client's performance and outperforms the baseline cluster-based algorithm IFCA under low-participation scenarios.

Conclusion: PQFed proves to be scalable and effective for personalized federated learning, addressing data heterogeneity by enabling clients to collaborate with those having compatible data distributions.

Abstract: Federated learning enables collaborative model training without sharing raw
data, but data heterogeneity consistently challenges the performance of the
global model. Traditional optimization methods often rely on collaborative
global model training involving all clients, followed by local adaptation to
improve individual performance. In this work, we focus on early-stage quality
control and propose PQFed, a novel privacy-preserving personalized federated
learning framework that designs customized training strategies for each client
prior to the federated training process. PQFed extracts representative features
from each client's raw data and applies clustering techniques to estimate
inter-client dataset similarity. Based on these similarity estimates, the
framework implements a client selection strategy that enables each client to
collaborate with others who have compatible data distributions. We evaluate
PQFed on two benchmark datasets, CIFAR-10 and MNIST, integrated with three
existing federated learning algorithms. Experimental results show that PQFed
consistently improves the target client's model performance, even with a
limited number of participants. We further benchmark PQFed against a baseline
cluster-based algorithm, IFCA, and observe that PQFed also achieves better
performance in low-participation scenarios. These findings highlight PQFed's
scalability and effectiveness in personalized federated learning settings.

</details>


### [355] [A Unifying Framework for Parallelizing Sequential Models with Linear Dynamical Systems](https://arxiv.org/abs/2509.21716)
*Xavier Gonzalez,E. Kelly Buchanan,Hyun Dong Lee,Jerry Weihong Liu,Ke Alexander Wang,David M. Zoltowski,Christopher RÃ©,Scott W. Linderman*

Main category: cs.LG

TL;DR: The paper proposes a framework using linear dynamical systems (LDSs) to unify and understand fixed-point methods for parallelizing sequential models.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in leveraging parallelism in sequential machine learning models to improve efficiency and scalability.

Method: The research interprets various fixed-point iteration methods (Newton, Picard, Jacobi) using linear dynamical systems as approximate linearizations of nonlinear recursions.

Result: It establishes a unified theoretical foundation, connecting diverse fixed-point methods, and clarifies where they are most effective for parallel computation.

Conclusion: This framework bridges algorithms, provides insights into their shared mechanisms, and highlights potential opportunities for optimizing sequential computation.

Abstract: Harnessing parallelism in seemingly sequential models is a central challenge
for modern machine learning. Several approaches have been proposed for
evaluating sequential processes in parallel using fixed-point methods, like
Newton, Picard, and Jacobi iterations. In this work, we show that these methods
can be understood within a common framework based on linear dynamical systems
(LDSs), where different iteration schemes arise naturally as approximate
linearizations of a nonlinear recursion. This unifying view highlights shared
principles behind these techniques and clarifies when particular fixed-point
methods are most likely to be effective. By bridging diverse algorithms through
the language of LDSs, our framework provides a clearer theoretical foundation
for parallelizing sequential models and points toward new opportunities for
efficient and scalable computation.

</details>


### [356] [Information-Theoretic Bayesian Optimization for Bilevel Optimization Problems](https://arxiv.org/abs/2509.21725)
*Takuya Kanayama,Yuki Ito,Tomoyuki Tamura,Masayuki Karasuyama*

Main category: cs.LG

TL;DR: This paper proposes an information-theoretic approach for addressing bilevel optimization problems using Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: Bilevel optimization is challenging due to its nested structure and complex problem definition, especially when both levels involve expensive black-box functions. Despite its importance, it has been less studied compared to other extensions of Bayesian optimization.

Method: The authors proposed an information-theoretic approach that evaluates the information gain of both the upper- and lower-level solutions and values. They introduced a unified criterion to measure the benefit across both levels and provided a practical lower bound method for information gain evaluation.

Result: The proposed method was empirically validated using several benchmark datasets, demonstrating its effectiveness.

Conclusion: An information-theoretic approach is feasible and effective for tackling bilevel optimization problems, contributing to the advancement of Bayesian optimization.

Abstract: A bilevel optimization problem consists of two optimization problems nested
as an upper- and a lower-level problem, in which the optimality of the
lower-level problem defines a constraint for the upper-level problem. This
paper considers Bayesian optimization (BO) for the case that both the upper-
and lower-levels involve expensive black-box functions. Because of its nested
structure, bilevel optimization has a complex problem definition and, compared
with other standard extensions of BO such as multi-objective or constraint
settings, it has not been widely studied. We propose an information-theoretic
approach that considers the information gain of both the upper- and
lower-optimal solutions and values. This enables us to define a unified
criterion that measures the benefit for both level problems, simultaneously.
Further, we also show a practical lower bound based approach to evaluating the
information gain. We empirically demonstrate the effectiveness of our proposed
method through several benchmark datasets.

</details>


### [357] [Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks](https://arxiv.org/abs/2509.21735)
*Houliang Zhou,Rong Zhou,Yangying Liu,Kanhao Zhao,Li Shen,Brian Y. Chen,Yu Zhang,Lifang He,Alzheimer's Disease Neuroimaging Initiative*

Main category: cs.LG

TL;DR: The paper proposes an interpretable spatio-temporal graph neural network using dual Stochastic Differential Equations to predict Alzheimer's disease (AD) progression from irregularly-sampled fMRI data.


<details>
  <summary>Details</summary>
Motivation: Forecasting Alzheimer's disease progression is essential for early intervention but is hindered by the complex spatio-temporal dysfunctions in brain networks, which are poorly captured by current methods.

Method: The authors developed a spatio-temporal graph neural network framework that uses dual Stochastic Differential Equations to model irregularly-sampled longitudinal fMRI data and identify key brain abnormalities.

Result: The framework successfully identified critical brain regions (e.g., parahippocampal, prefrontal cortex) and abnormal networks (ventral attention, dorsal attention, default mode) associated with AD progression. These findings correlated strongly with clinical symptoms.

Conclusion: The study demonstrates the potential of spatio-temporal graph-based approaches for early and individualized AD progression prediction, uncovering both established and new biomarkers.

Abstract: Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease
(AD) progression is crucial for timely intervention. However, this task remains
challenging due to the complex dysfunctions in the spatio-temporal
characteristics of underlying brain networks, which are often overlooked by
existing methods. To address these limitations, we develop an interpretable
spatio-temporal graph neural network framework to predict future AD
progression, leveraging dual Stochastic Differential Equations (SDEs) to model
the irregularly-sampled longitudinal functional magnetic resonance imaging
(fMRI) data. We validate our approach on two independent cohorts, including the
Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease
Neuroimaging Initiative (ADNI). Our framework effectively learns sparse
regional and connective importance probabilities, enabling the identification
of key brain circuit abnormalities associated with disease progression.
Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal
lobule as salient regions, with significant disruptions in the ventral
attention, dorsal attention, and default mode networks. These abnormalities
correlate strongly with longitudinal AD-related clinical symptoms. Moreover,
our interpretability strategy reveals both established and novel neural
systems-level and sex-specific biomarkers, offering new insights into the
neurobiological mechanisms underlying AD progression. Our findings highlight
the potential of spatio-temporal graph-based learning for early, individualized
prediction of AD progression, even in the context of irregularly-sampled
longitudinal imaging data.

</details>


### [358] [ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation](https://arxiv.org/abs/2509.22402)
*Nan Tang,Jing-Cheng Pang,Guanlin Li,Chao Qian,Yang Yu*

Main category: cs.LG

TL;DR: This paper introduces ReLAM, a framework for generating rewards in visual RL tasks using keypoints from images. It outperforms current methods in robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing rewards in visual RL, especially when precise positional data is unavailable in real-world settings.

Method: ReLAM uses a keypoint-based anticipation model to generate dense rewards from video demonstrations, enabling hierarchical RL with a structured curriculum.

Result: ReLAM accelerates learning and outperforms state-of-the-art methods in complex robotic manipulation tasks.

Conclusion: The proposed framework shows that structured, keypoint-based rewards can effectively improve RL performance in long-horizon manipulation tasks.

Abstract: Reward design remains a critical bottleneck in visual reinforcement learning
(RL) for robotic manipulation. In simulated environments, rewards are
conventionally designed based on the distance to a target position. However,
such precise positional information is often unavailable in real-world visual
settings due to sensory and perceptual limitations. In this study, we propose a
method that implicitly infers spatial distances through keypoints extracted
from images. Building on this, we introduce Reward Learning with Anticipation
Model (ReLAM), a novel framework that automatically generates dense, structured
rewards from action-free video demonstrations. ReLAM first learns an
anticipation model that serves as a planner and proposes intermediate
keypoint-based subgoals on the optimal path to the final goal, creating a
structured learning curriculum directly aligned with the task's geometric
objectives. Based on the anticipated subgoals, a continuous reward signal is
provided to train a low-level, goal-conditioned policy under the hierarchical
reinforcement learning (HRL) framework with provable sub-optimality bound.
Extensive experiments on complex, long-horizon manipulation tasks show that
ReLAM significantly accelerates learning and achieves superior performance
compared to state-of-the-art methods.

</details>


### [359] [POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization](https://arxiv.org/abs/2509.21737)
*Ziqing Wang,Yibo Wen,William Pattie,Xiao Luo,Weimin Wu,Jerry Yao-Chieh Hu,Abhishek Pandey,Han Liu,Kaize Ding*

Main category: cs.LG

TL;DR: POLO is a new optimization framework using LLMs that enhances molecular optimization by learning from entire optimization trajectories rather than isolated steps, achieving significantly better sample efficiency and success rates.


<details>
  <summary>Details</summary>
Motivation: Traditional lead optimization methods in drug discovery struggle with sample efficiency, requiring many oracle evaluations to navigate chemical space for enhancing molecular properties.

Method: POLO introduces Preference-Guided Policy Optimization (PGPO), combining trajectory-level optimization and turn-level preference feedback within a reinforcement learning framework to utilize LLMs for molecular optimization.

Result: POLO achieves an 84% success rate on single-property tasks (2.3x improvement) and a 50% success rate on multi-property tasks using only 500 oracle evaluations, outperforming traditional and LLM-based baselines.

Conclusion: By effectively leveraging entire optimization trajectories and intermediate evaluations, POLO represents a breakthrough in sample-efficient molecular optimization, advancing drug discovery processes.

Abstract: Lead optimization in drug discovery requires efficiently navigating vast
chemical space through iterative cycles to enhance molecular properties while
preserving structural similarity to the original lead compound. Despite recent
advances, traditional optimization methods struggle with sample
efficiency-achieving good optimization performance with limited oracle
evaluations. Large Language Models (LLMs) provide a promising approach through
their in-context learning and instruction following capabilities, which align
naturally with these iterative processes. However, existing LLM-based methods
fail to leverage this strength, treating each optimization step independently.
To address this, we present POLO (Preference-guided multi-turn Optimization for
Lead Optimization), which enables LLMs to learn from complete optimization
trajectories rather than isolated steps. At its core, POLO introduces
Preference-Guided Policy Optimization (PGPO), a novel reinforcement learning
algorithm that extracts learning signals at two complementary levels:
trajectory-level optimization reinforces successful strategies, while
turn-level preference learning provides dense comparative feedback by ranking
intermediate molecules within each trajectory. Through this dual-level learning
from intermediate evaluation, POLO achieves superior sample efficiency by fully
exploiting each costly oracle call. Extensive experiments demonstrate that POLO
achieves 84% average success rate on single-property tasks (2.3x better than
baselines) and 50% on multi-property tasks using only 500 oracle evaluations,
significantly advancing the state-of-the-art in sample-efficient molecular
optimization.

</details>


### [360] [Brain PathoGraph Learning](https://arxiv.org/abs/2509.21742)
*Ciyuan Peng,Nguyen Linh Dan Le,Shan Jin,Dexuan Ding,Shuo Yu,Feng Xia*

Main category: cs.LG

TL;DR: The paper introduces BrainPoG, a model that improves brain graph learning by focusing on disease-relevant knowledge, showing better performance and efficiency in brain disease detection.


<details>
  <summary>Details</summary>
Motivation: Current brain graph learning methods lack the ability to specifically learn disease-related patterns, leading to inefficiencies in computational costs and reduced practical utility in clinical settings.

Method: The proposed BrainPoG model utilizes a pathological pattern filter to identify and retain disease-relevant subgraphs, followed by a feature distillation module to enhance pathological features while reducing irrelevant noise.

Result: Experiments on four benchmark datasets demonstrate that BrainPoG outperforms existing methods in both accuracy and computational efficiency for brain disease detection tasks.

Conclusion: BrainPoG successfully learns disease-relevant knowledge with reduced computational cost, making it a practical and efficient tool for real-world clinical applications.

Abstract: Brain graph learning has demonstrated significant achievements in the fields
of neuroscience and artificial intelligence. However, existing methods struggle
to selectively learn disease-related knowledge, leading to heavy parameters and
computational costs. This challenge diminishes their efficiency, as well as
limits their practicality for real-world clinical applications. To this end, we
propose a lightweight Brain PathoGraph Learning (BrainPoG) model that enables
efficient brain graph learning by pathological pattern filtering and
pathological feature distillation. Specifically, BrainPoG first contains a
filter to extract the pathological pattern formulated by highly
disease-relevant subgraphs, achieving graph pruning and lesion localization. A
PathoGraph is therefore constructed by dropping less disease-relevant subgraphs
from the whole brain graph. Afterwards, a pathological feature distillation
module is designed to reduce disease-irrelevant noise features and enhance
pathological features of each node in the PathoGraph. BrainPoG can exclusively
learn informative disease-related knowledge while avoiding less relevant
information, achieving efficient brain graph learning. Extensive experiments on
four benchmark datasets demonstrate that BrainPoG exhibits superiority in both
model performance and computational efficiency across various brain disease
detection tasks.

</details>


### [361] [HyperCore: Coreset Selection under Noise via Hypersphere Models](https://arxiv.org/abs/2509.21746)
*Brian B. Moser,Arundhati S. Shanbhag,Tobias C. Nauen,Stanislav Frolov,Federico Raue,Joachim Folz,Andreas Dengel*

Main category: cs.LG

TL;DR: HyperCore introduces a robust coreset selection method that adapts to noisy datasets by leveraging hypersphere models to effectively discard mislabeled and ambiguous data.


<details>
  <summary>Details</summary>
Motivation: Existing coreset selection methods fail to address annotation errors and require fixed pruning ratios, limiting their effectiveness in practical, noisy scenarios.

Method: HyperCore uses hypersphere models for each class, embedding samples near hypersphere centers, and applies Youden's J statistic for adaptive noise-aware data pruning.

Result: HyperCore consistently outperforms state-of-the-art coreset methods in noisy and low-data environments, producing smaller yet information-rich subsets.

Conclusion: HyperCore offers a scalable solution for efficient and noise-free model training while simplifying data pruning by removing dependence on hyperparameter tuning.

Abstract: The goal of coreset selection methods is to identify representative subsets
of datasets for efficient model training. Yet, existing methods often ignore
the possibility of annotation errors and require fixed pruning ratios, making
them impractical in real-world settings. We present HyperCore, a robust and
adaptive coreset selection framework designed explicitly for noisy
environments. HyperCore leverages lightweight hypersphere models learned per
class, embedding in-class samples close to a hypersphere center while naturally
segregating out-of-class samples based on their distance. By using Youden's J
statistic, HyperCore can adaptively select pruning thresholds, enabling
automatic, noise-aware data pruning without hyperparameter tuning. Our
experiments reveal that HyperCore consistently surpasses state-of-the-art
coreset selection methods, especially under noisy and low-data regimes.
HyperCore effectively discards mislabeled and ambiguous points, yielding
compact yet highly informative subsets suitable for scalable and noise-free
learning.

</details>


### [362] [SubZeroCore: A Submodular Approach with Zero Training for Coreset Selection](https://arxiv.org/abs/2509.21748)
*Brian B. Moser,Tobias C. Nauen,Arundhati S. Shanbhag,Federico Raue,Stanislav Frolov,Joachim Folz,Andreas Dengel*

Main category: cs.LG

TL;DR: SubZeroCore is a training-free coreset selection method that balances submodular coverage and density to identify representative subsets, significantly outperforming training-based baselines at high pruning rates.


<details>
  <summary>Details</summary>
Motivation: Existing coreset selection methods paradoxically rely on training-based signals, undermining their purpose by requiring evaluation of the entire dataset.

Method: SubZeroCore combines submodular coverage and density into a unified objective using a closed-form solution and a single hyperparameter to balance these objectives.

Result: SubZeroCore matches and surpasses training-based baselines at high pruning rates, with lower computational costs and improved robustness to label noise.

Conclusion: SubZeroCore is a practical and scalable solution for efficient dataset pruning, offering significant advantages over existing methods in terms of performance, robustness, and computational efficiency.

Abstract: The goal of coreset selection is to identify representative subsets of
datasets for efficient model training. Yet, existing approaches paradoxically
require expensive training-based signals, e.g., gradients, decision boundary
estimates or forgetting counts, computed over the entire dataset prior to
pruning, which undermines their very purpose by requiring training on samples
they aim to avoid. We introduce SubZeroCore, a novel, training-free coreset
selection method that integrates submodular coverage and density into a single,
unified objective. To achieve this, we introduce a sampling strategy based on a
closed-form solution to optimally balance these objectives, guided by a single
hyperparameter that explicitly controls the desired coverage for local density
measures. Despite no training, extensive evaluations show that SubZeroCore
matches training-based baselines and significantly outperforms them at high
pruning rates, while dramatically reducing computational overhead. SubZeroCore
also demonstrates superior robustness to label noise, highlighting its
practical effectiveness and scalability for real-world scenarios.

</details>


### [363] [Reparameterizing 4DVAR with neural fields](https://arxiv.org/abs/2509.21751)
*Jaemin Oh*

Main category: cs.LG

TL;DR: The paper introduces a neural field-based approach to 4DVAR data assimilation, allowing parallel-in-time optimization, improved stability, and reduced computational cost compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Because of the computational challenges associated with optimizing 4DVAR's cost function, there is a need for innovative approaches to enhance efficiency and stability.

Method: The method reformulates 4DVAR by representing the spatiotemporal state as a neural network parameterized function, combined with physics-informed losses to optimize the state.

Result: Experiments on 2D Navier-Stokes equations demonstrate improved stability in initial condition estimation and reduced oscillations, with no reliance on ground-truth or reanalysis data.

Conclusion: This neural field-based 4DVAR framework simplifies implementation, lessens computational intensity, and provides robust solutions for settings with limited data access.

Abstract: Four-dimensional variational data assimilation (4DVAR) is a cornerstone of
numerical weather prediction, but its cost function is difficult to optimize
and computationally intensive. We propose a neural field-based reformulation in
which the full spatiotemporal state is represented as a continuous function
parameterized by a neural network. This reparameterization removes the
time-sequential dependency of classical 4DVAR, enabling parallel-in-time
optimization in parameter space. Physical constraints are incorporated directly
through a physics-informed loss, simplifying implementation and reducing
computational cost. We evaluate the method on the two-dimensional
incompressible Navier--Stokes equations with Kolmogorov forcing. Compared to a
baseline 4DVAR implementation, the neural reparameterized variants produce more
stable initial condition estimates without spurious oscillations. Notably,
unlike most machine learning-based approaches, our framework does not require
access to ground-truth states or reanalysis data, broadening its applicability
to settings with limited reference information.

</details>


### [364] [Machine Learning and AI Applied to fNIRS Data Reveals Novel Brain Activity Biomarkers in Stable Subclinical Multiple Sclerosis](https://arxiv.org/abs/2509.21770)
*Sadman Saumik Islam,Bruna Dalcin Baldasso,Davide Cattaneo,Xianta Jiang,Michelle Ploughman*

Main category: cs.LG

TL;DR: The study utilized functional near-infrared spectroscopy (fNIRS) with machine learning to detect subtle brain activity biomarkers associated with cognitive fatigue in people with MS during hand dexterity tasks.


<details>
  <summary>Details</summary>
Motivation: To better understand the neural basis of cognitive fatigue and identify biomarkers for targeted brain stimulation treatments in individuals with MS.

Method: Researchers used fNIRS neuroimaging during hand dexterity tasks, applying machine learning (K-Nearest Neighbor classifier, XAI techniques) to analyze brain activation patterns and classify MS patients from controls.

Result: The classifier achieved 75% accuracy for single tasks and 66.7% for dual tasks. Key brain regions in the ipsilateral hemisphere showed suppressed activity and slower neurovascular response in MS groups.

Conclusion: Novel biomarkers identified through fNIRS and machine learning could lead to personalized brain stimulation therapies for cognitive fatigue in MS patients.

Abstract: People with Multiple Sclerosis (MS) complain of problems with hand dexterity
and cognitive fatigue. However, in many cases, impairments are subtle and
difficult to detect. Functional near-infrared spectroscopy (fNIRS) is a
non-invasive neuroimaging technique that measures brain hemodynamic responses
during cognitive or motor tasks. We aimed to detect brain activity biomarkers
that could explain subjective reports of cognitive fatigue while completing
dexterous tasks and provide targets for future brain stimulation treatments. We
recruited 15 people with MS who did not have a hand (Nine Hole Peg Test
[NHPT]), mobility, or cognitive impairment, and 12 age- and sex-matched
controls. Participants completed two types of hand dexterity tasks with their
dominant hand, single task and dual task (NHPT while holding a ball between the
fifth finger and hypothenar eminence of the same hand). We analyzed fNIRS data
(oxygenated and deoxygenated hemoglobin levels) using a machine learning
framework to classify MS patients from controls based on their brain activation
patterns in bilateral prefrontal and sensorimotor cortices. The K-Nearest
Neighbor classifier achieved an accuracy of 75.0% for single manual dexterity
tasks and 66.7% for the more complex dual manual dexterity tasks. Using XAI, we
found that the most important brain regions contributing to the machine
learning model were the supramarginal/angular gyri and the precentral gyrus
(sensory integration and motor regions) of the ipsilateral hemisphere, with
suppressed activity and slower neurovascular response in the MS group. During
both tasks, deoxygenated hemoglobin levels were better predictors than the
conventional measure of oxygenated hemoglobin. This nonconventional method of
fNIRS data analysis revealed novel brain activity biomarkers that can help
develop personalized brain stimulation targets.

</details>


### [365] [Beyond Formula Complexity: Effective Information Criterion Improves Performance and Interpretability for Symbolic Regression](https://arxiv.org/abs/2509.21780)
*Zihan Yu,Guanren Wang,Jingtao Ding,Huandong Wang,Yong Li*

Main category: cs.LG

TL;DR: The study proposes Effective Information Criterion (EIC) for symbolic regression, focusing on formula interpretability beyond compactness metrics.


<details>
  <summary>Details</summary>
Motivation: Existing symbolic regression methods often prioritize the size of formulas in complexity metrics, neglecting their mathematical structure and interpretability.

Method: The paper introduces EIC, a criterion that evaluates formulas as information systems by detecting structural irrationality through loss of significant digits or rounding noise.

Result: EIC enhances search-based symbolic regression algorithms' Pareto frontier performance, reduces irrational formula structures, boosts generative-based algorithm efficiency, and aligns with human interpretability preferences.

Conclusion: EIC effectively measures formula interpretability by addressing structural issues, advancing symbolic regression methods and aligning closer to human preferences.

Abstract: Symbolic regression discovers accurate and interpretable formulas to describe
given data, thereby providing scientific insights for domain experts and
promoting scientific discovery. However, existing symbolic regression methods
often use complexity metrics as a proxy for interoperability, which only
considers the size of the formula but ignores its internal mathematical
structure. Therefore, while they can discover formulas with compact forms, the
discovered formulas often have structures that are difficult to analyze or
interpret mathematically. In this work, inspired by the observation that
physical formulas are typically numerically stable under limited calculation
precision, we propose the Effective Information Criterion (EIC). It treats
formulas as information processing systems with specific internal structures
and identifies the unreasonable structure in them by the loss of significant
digits or the amplification of rounding noise as data flows through the system.
We find that this criterion reveals the gap between the structural rationality
of models discovered by existing symbolic regression algorithms and real-world
physical formulas. Combining EIC with various search-based symbolic regression
algorithms improves their performance on the Pareto frontier and reduces the
irrational structure in the results. Combining EIC with generative-based
algorithms reduces the number of samples required for pre-training, improving
sample efficiency by 2~4 times. Finally, for different formulas with similar
accuracy and complexity, EIC shows a 70.2% agreement with 108 human experts'
preferences for formula interpretability, demonstrating that EIC, by measuring
the unreasonable structures in formulas, actually reflects the formula's
interpretability.

</details>


### [366] [FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning](https://arxiv.org/abs/2509.21792)
*Yizhou Zhang,Ning Lv,Teng Wang,Jisheng Dang*

Main category: cs.LG

TL;DR: This paper enhances the efficiency of Group Relative Policy Optimization (GRPO) by proposing a concurrent-aware speculative decoding framework and an online draft learning mechanism, achieving significant speedup in training large language models.


<details>
  <summary>Details</summary>
Motivation: To address the excessively slow training process in GRPO caused by computationally expensive autoregressive response generation and limited speedup using speculative decoding in high-concurrency scenarios.

Method: The authors propose a two-part solution: (1) a concurrency-aware speculative decoding framework that adapts drafting and verification strategies dynamically according to real-time concurrency levels, and (2) an online draft learning mechanism allowing the draft model to adapt using feedback from the target model.

Result: The proposed method achieves end-to-end speedups of 2.35x to 2.72x across multiple mathematical reasoning datasets and models, outperforming baseline efficiency approaches.

Conclusion: This work significantly improves the training efficiency of GRPO for large language models by addressing bottlenecks in generation and distributional drift, offering practical solutions for real-world deployment.

Abstract: Group relative policy optimization (GRPO) has demonstrated significant
potential in improving the reasoning capabilities of large language models
(LLMs) via reinforcement learning. However, its practical deployment is impeded
by an excessively slow training process, primarily attributed to the
computationally intensive autoregressive generation of multiple responses per
query, which makes the generation phase the primary performance bottleneck.
Although speculative decoding presents a promising direction for acceleration,
its direct application in GRPO achieves limited speedup under high-concurrency
training conditions. To overcome this limitation, we propose a
concurrency-aware speculative decoding framework that dynamically adjusts the
drafting and verification strategy according to real-time concurrency levels,
thereby maximizing the acceleration of the generation process. Furthermore, to
address performance degradation arising from distributional drift between the
evolving target model and the fixed draft model during training, we introduce
an online draft learning mechanism that enables the draft model to continuously
adapt using feedback signals from the target model. Experimental results across
multiple mathematical reasoning datasets and models demonstrate that the
proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly
surpassing baseline approaches in efficiency. The code is available at
https://github.com/yedaotian9/GRPO_speculative.

</details>


### [367] [Exploring the Relationships Between Physiological Signals During Automated Fatigue Detection](https://arxiv.org/abs/2509.21794)
*Kourosh Kakhi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharyab*

Main category: cs.LG

TL;DR: This paper studies the use of multi-signal physiological feature fusion for robust fatigue detection, showing improved accuracy via various machine learning methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and robustness of fatigue detection systems by using statistical relationships between multiple physiological signals.

Method: Utilized the DROZY dataset, extracted features from ECG, EMG, EOG, and EEG signal combinations, and evaluated classification performance using Decision Tree, Random Forest, Logistic Regression, and XGBoost models.

Result: XGBoost using EMG and EEG signals showed the best performance, with SHAP analysis identifying ECG EOG correlation as essential for classification.

Conclusion: Feature-level fusion of physiological signals enhances fatigue detection systems' accuracy, interpretability, and practicality.

Abstract: Fatigue detection using physiological signals is critical in domains such as
transportation, healthcare, and performance monitoring. While most studies
focus on single modalities, this work examines statistical relationships
between signal pairs to improve classification robustness. Using the DROZY
dataset, we extracted features from ECG, EMG, EOG, and EEG across 15 signal
combinations and evaluated them with Decision Tree, Random Forest, Logistic
Regression, and XGBoost. Results show that XGBoost with the EMG EEG combination
achieved the best performance. SHAP analysis highlighted ECG EOG correlation as
a key feature, and multi signal models consistently outperformed single signal
ones. These findings demonstrate that feature level fusion of physiological
signals enhances accuracy, interpretability, and practical applicability of
fatigue monitoring systems.

</details>


### [368] [ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations](https://arxiv.org/abs/2509.21802)
*Chang Liu,Bohao Zhao,Jingtao Ding,Yong Li*

Main category: cs.LG

TL;DR: ChaosNexus, a foundation model for chaotic systems, achieves strong zero-shot and few-shot forecasting with a new multi-scale architecture showcasing improved accuracy across diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Forecasting chaotic systems is challenging due to sensitivity to initial conditions and limited observational data. Traditional models lack generalization for real-world applications, necessitating robust forecasting solutions.

Method: ChaosNexus introduces a novel multi-scale architecture called ScaleFormer with Mixture-of-Experts layers. It is pre-trained on diverse chaotic dynamics to capture universal patterns and system-specific behaviors.

Result: In synthetic tests, ChaosNexus improves long-term attractor statistics by over 40% compared to baselines and achieves competitive weather forecasting results with mean errors below 1 degree, excelling in zero-shot and few-shot applications.

Conclusion: ChaosNexus underscores the importance of training diversity for scientific models, achieving breakthroughs in generalization and efficiency for chaotic systems across synthetic and real-world scenarios.

Abstract: Accurately forecasting chaotic systems, prevalent in domains such as weather
prediction and fluid dynamics, remains a significant scientific challenge. The
inherent sensitivity of these systems to initial conditions, coupled with a
scarcity of observational data, severely constrains traditional modeling
approaches. Since these models are typically trained for a specific system,
they lack the generalization capacity necessary for real-world applications,
which demand robust zero-shot or few-shot forecasting on novel or data-limited
scenarios. To overcome this generalization barrier, we propose ChaosNexus, a
foundation model pre-trained on a diverse corpus of chaotic dynamics.
ChaosNexus employs a novel multi-scale architecture named ScaleFormer augmented
with Mixture-of-Experts layers, to capture both universal patterns and
system-specific behaviors. The model demonstrates state-of-the-art zero-shot
generalization across both synthetic and real-world benchmarks. On a
large-scale testbed comprising over 9,000 synthetic chaotic systems, it
improves the fidelity of long-term attractor statistics by more than 40%
compared to the leading baseline. This robust performance extends to real-world
applications with exceptional data efficiency. For instance, in 5-day global
weather forecasting, ChaosNexus achieves a competitive zero-shot mean error
below 1 degree, a result that further improves with few-shot fine-tuning.
Moreover, experiments on the scaling behavior of ChaosNexus provide a guiding
principle for scientific foundation models: cross-system generalization stems
from the diversity of training systems, rather than sheer data volume.

</details>


### [369] [Scaling Laws for Neural Material Models](https://arxiv.org/abs/2509.21811)
*Akshay Trikha,Kyle Chu,Advait Gosai,Parker Szachta,Eric Weiner*

Main category: cs.LG

TL;DR: The paper investigates how adjusting training data, model size, and computational resources affects the performance of deep learning models for material property prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve material property prediction for applications like batteries, semiconductors, and medical devices by leveraging deep learning methods.

Method: The authors trained transformer and EquiformerV2 neural networks, analyzed their performance by varying hyperparameters (training data, model size, and compute), and identified empirical scaling laws governing their predictive accuracy.

Result: Empirical scaling laws were discovered, showing a power law relationship between model loss and hyperparameters, helping quantify the impact of scaling on performance.

Conclusion: Scaling training data, model size, and compute enhances predictive accuracy, and further exploration of scaling laws in additional neural network architectures is recommended.

Abstract: Predicting material properties is crucial for designing better batteries,
semiconductors, and medical devices. Deep learning helps scientists quickly
find promising materials by predicting their energy, forces, and stresses.
Companies scale capacities of deep learning models in multiple domains, such as
language modeling, and invest many millions of dollars into such models. Our
team analyzes how scaling training data (giving models more information to
learn from), model sizes (giving models more capacity to learn patterns), and
compute (giving models more computational resources) for neural networks
affects their performance for material property prediction. In particular, we
trained both transformer and EquiformerV2 neural networks to predict material
properties. We find empirical scaling laws for these models: we can predict how
increasing each of the three hyperparameters (training data, model size, and
compute) affects predictive performance. In particular, the loss $L$ can be
measured with a power law relationship $L = \alpha \cdot N^{-\beta}$, where
$\alpha$ and $\beta$ are constants while $N$ is the relevant hyperparameter. We
also incorporate command-line arguments for changing training settings such as
the amount of epochs, maximum learning rate, and whether mixed precision is
enabled. Future work could entail further investigating scaling laws for other
neural network models in this domain, such as GemNet and fully connected
networks, to assess how they compare to the models we trained.

</details>


### [370] [Sharpness-Aware Minimization Can Hallucinate Minimizers](https://arxiv.org/abs/2509.21818)
*Chanwoong Park,Uijeong Jang,Ernest K. Ryu,Insoon Yang*

Main category: cs.LG

TL;DR: SAM can lead to spurious solutions, but a proposed remedy addresses this issue.


<details>
  <summary>Details</summary>
Motivation: To investigate why SAM leads to hallucinated minimizers and propose a solution to improve its reliability.

Method: Theoretical proofs of hallucinated minimizer existence, experiments demonstrating practical occurrence, and a remedy proposal.

Result: SAM is shown to converge to hallucinated minimizers theoretically and empirically, with an effective fix suggested.

Conclusion: While SAM steers training towards supposedly better minimizers, certain adjustments are needed to avoid its unintended pitfalls.

Abstract: Sharpness-Aware Minimization (SAM) is a widely used method that steers
training toward flatter minimizers, which typically generalize better. In this
work, however, we show that SAM can converge to hallucinated minimizers --
points that are not minimizers of the original objective. We theoretically
prove the existence of such hallucinated minimizers and establish conditions
for local convergence to them. We further provide empirical evidence
demonstrating that SAM can indeed converge to these points in practice.
Finally, we propose a simple yet effective remedy for avoiding hallucinated
minimizers.

</details>


### [371] [Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.21828)
*The Viet Bui,Tien Mai,Hong Thanh Nguyen*

Main category: cs.LG

TL;DR: The paper tackles online multi-agent reinforcement learning (MARL) under sparse reward conditions by integrating online inverse preference learning with on-policy optimization, complemented by large language models for better reward prediction.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of sparse rewards in MARL, which limits guidance from intermediate rewards and impedes effective policy learning.

Method: The authors developed an implicit multi-agent reward learning model using a preference-based value-decomposition network, producing dual advantage streams to guide both centralized critic and decentralized actors. Preference labels from LLMs enhance reward model quality.

Result: Empirical evaluations on MAMuJoCo and SMACv2 benchmarks show superior performance of the proposed approach compared to baseline methods.

Conclusion: The proposed framework effectively resolves sparse reward issues in online MARL, leveraging both innovative reward modeling and preference labels from LLMs.

Abstract: We study the problem of online multi-agent reinforcement learning (MARL) in
environments with sparse rewards, where reward feedback is not provided at each
interaction but only revealed at the end of a trajectory. This setting, though
realistic, presents a fundamental challenge: the lack of intermediate rewards
hinders standard MARL algorithms from effectively guiding policy learning. To
address this issue, we propose a novel framework that integrates online inverse
preference learning with multi-agent on-policy optimization into a unified
architecture. At its core, our approach introduces an implicit multi-agent
reward learning model, built upon a preference-based value-decomposition
network, which produces both global and local reward signals. These signals are
further used to construct dual advantage streams, enabling differentiated
learning targets for the centralized critic and decentralized actors. In
addition, we demonstrate how large language models (LLMs) can be leveraged to
provide preference labels that enhance the quality of the learned reward model.
Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and
SMACv2, show that our method achieves superior performance compared to existing
baselines, highlighting its effectiveness in addressing sparse-reward
challenges in online MARL.

</details>


### [372] [On the Complexity Theory of Masked Discrete Diffusion: From $\mathrm{poly}(1/Îµ)$ to Nearly $Îµ$-Free](https://arxiv.org/abs/2509.21835)
*Xunpeng Huang,Yingyu Lin,Nishant Jain,Kaibo Wang,Difan Zou,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: This paper analyzes masked discrete diffusion for text generation, improving the understanding of its theoretical complexity and proposing efficiency enhancements.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient theoretical understanding of masked discrete diffusion for text generation and to highlight its advantages over uniform discrete diffusion.

Method: The authors analyze the Euler sampler for total variation accuracy and introduce the Mask-Aware Truncated Uniformization (MATU) approach to improve efficiency and remove restrictive assumptions.

Result: Euler samplers achieve discrete score evaluation efficiency, and MATU reduces computational complexity while maintaining accuracy, outperforming prior methods in masked diffusion.

Conclusion: The paper establishes a thorough theoretical foundation for masked discrete diffusion, confirming its benefits over uniform diffusion and guiding future analyses of diffusion-based language models.

Abstract: We study masked discrete diffusion -- a flexible paradigm for text generation
in which tokens are progressively corrupted by special mask symbols before
being denoised. Although this approach has demonstrated strong empirical
performance, its theoretical complexity in high-dimensional settings remains
insufficiently understood. Existing analyses largely focus on uniform discrete
diffusion, and more recent attempts addressing masked diffusion either (1)
overlook widely used Euler samplers, (2) impose restrictive bounded-score
assumptions, or (3) fail to showcase the advantages of masked discrete
diffusion over its uniform counterpart. To address this gap, we show that Euler
samplers can achieve $\epsilon$-accuracy in total variation (TV) with
$\tilde{O}(d^{2}\epsilon^{-3/2})$ discrete score evaluations, thereby providing
the first rigorous analysis of typical Euler sampler in masked discrete
diffusion. We then propose a Mask-Aware Truncated Uniformization (MATU)
approach that both removes bounded-score assumptions and preserves unbiased
discrete score approximation. By exploiting the property that each token can be
unmasked at most once, MATU attains a nearly $\epsilon$-free complexity of
$O(d\,\ln d\cdot (1-\epsilon^2))$. This result surpasses existing
uniformization methods under uniform discrete diffusion, eliminating the
$\ln(1/\epsilon)$ factor and substantially speeding up convergence. Our
findings not only provide a rigorous theoretical foundation for masked discrete
diffusion, showcasing its practical advantages over uniform diffusion for text
generation, but also pave the way for future efforts to analyze diffusion-based
language models developed under masking paradigm.

</details>


### [373] [Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration](https://arxiv.org/abs/2509.21848)
*Taejong Joo,Shu Ishida,Ivan Sosnovik,Bryan Lim,Sahand Rezaei-Shoshtari,Adam Gaier,Robert Giaquinto*

Main category: cs.LG

TL;DR: The paper introduces Graph of Agents (GoA), a framework for long context modeling using multi-agent systems that achieves significant improvements in document question answering benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of multi-agent systems in long context modeling, such as dependency on hand-crafted collaboration and prompt engineering.

Method: The authors propose an information-theoretic compression framework to optimize multi-agent collaboration, dynamically constructing a Graph of Agents (GoA) for efficient context handling.

Result: GoA outperformed existing methods, achieving a 5.7% improvement in retrieval-augmented generation and surpassing Llama 3.1 8B's extended context window performance on LongBench.

Conclusion: The GoA framework significantly enhances the context modeling capacity of multi-agent systems, enabling efficient long-input processing without increasing model-specific context windows.

Abstract: As a model-agnostic approach to long context modeling, multi-agent systems
can process inputs longer than a large language model's context window without
retraining or architectural modifications. However, their performance often
heavily relies on hand-crafted multi-agent collaboration strategies and prompt
engineering, which limit generalizability. In this work, we introduce a
principled framework that formalizes the model-agnostic long context modeling
problem as a compression problem, yielding an information-theoretic compression
objective. Building on this framework, we propose Graph of Agents (GoA), which
dynamically constructs an input-dependent collaboration structure that
maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document
question answering benchmarks, GoA improves the average $F_1$ score of
retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using
a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K
context window, GoA surpasses the 128K context window Llama 3.1 8B on
LongBench, showing a dramatic increase in effective context length. Our source
code is available at https://github.com/tjoo512/graph-of-agents.

</details>


### [374] [MolSpectLLM: A Molecular Foundation Model Bridging Spectroscopy, Molecule Elucidation, and 3D Structure Generation](https://arxiv.org/abs/2509.21861)
*Shuaike Shen,Jiaqing Xie,Zhuo Yang,Antong Zhang,Shuzhou Sun,Ben Gao,Tianfan Fu,Biqing Qi,Yuqiang Li*

Main category: cs.LG

TL;DR: MolSpectLLM is introduced as a molecular foundation model incorporating experimental spectra and 3D structures, achieving state-of-the-art results on spectrum-related tasks.


<details>
  <summary>Details</summary>
Motivation: Current molecular modeling approaches neglect experimental spectra and 3D structural information, reducing effectiveness in stereochemistry, spatial conformation, and validation-critical tasks.

Method: MolSpectLLM leverages the Qwen2.5-7B framework, integrating experimental spectroscopy data with molecular 3D structure.

Result: MolSpectLLM reaches an average accuracy of 0.53 for spectrum-related benchmarks and outperforms general-purpose LLMs in spectra analysis tasks.

Conclusion: MolSpectLLM bridges experimental spectral analysis, molecular elucidation, and structural generation, enhancing performance in real-world molecular tasks.

Abstract: Recent advances in molecular foundation models have shown impressive
performance in molecular property prediction and de novo molecular design, with
promising applications in areas such as drug discovery and reaction prediction.
Nevertheless, most existing approaches rely exclusively on SMILES
representations and overlook both experimental spectra and 3D structural
information-two indispensable sources for capturing molecular behavior in
real-world scenarios. This limitation reduces their effectiveness in tasks
where stereochemistry, spatial conformation, and experimental validation are
critical. To overcome these challenges, we propose MolSpectLLM, a molecular
foundation model pretrained on Qwen2.5-7B that unifies experimental
spectroscopy with molecular 3D structure. By explicitly modeling molecular
spectra, MolSpectLLM achieves state-of-the-art performance on spectrum-related
tasks, with an average accuracy of 0.53 across NMR, IR, and MS benchmarks.
MolSpectLLM also shows strong performance on the spectra analysis task,
obtaining 15.5% sequence accuracy and 41.7% token accuracy on
Spectra-to-SMILES, substantially outperforming large general-purpose LLMs. More
importantly, MolSpectLLM not only achieves strong performance on molecular
elucidation tasks, but also generates accurate 3D molecular structures directly
from SMILES or spectral inputs, bridging spectral analysis, molecular
elucidation, and molecular design.

</details>


### [375] [Beyond RAG vs. Long-Context: Learning Distraction-Aware Retrieval for Efficient Knowledge Grounding](https://arxiv.org/abs/2509.21865)
*Seong-Woong Shim,Myunsoo Kim,Jae Hyeon Cho,Byung-Jun Lee*

Main category: cs.LG

TL;DR: The paper proposes LDAR, an adaptive retrieval method, shown to outperform direct long-context approaches by optimizing token usage and mitigating distraction in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing RAG frameworks face limitations due to increased context window sizes in LLMs, such as inefficiencies and degradation of quality from large contexts.

Method: The authors introduce LDAR, a learning-based adaptive retriever that selectively retrieves context passages to minimize distractions during LLM processing.

Result: LDAR demonstrates significantly higher performance and robustness compared to long-context methods across multiple benchmarks and architectures.

Conclusion: LDAR strikes a balance between sufficient information coverage and managing distractions, proving its effectiveness for knowledge-intensive tasks.

Abstract: Retrieval-Augmented Generation (RAG) is a framework for grounding Large
Language Models (LLMs) in external, up-to-date information. However, recent
advancements in context window size allow LLMs to process inputs of up to 128K
tokens or more, offering an alternative strategy: supplying the full document
context directly to the model, rather than relying on RAG to retrieve a subset
of contexts. Nevertheless, this emerging alternative strategy has notable
limitations: (i) it is token-inefficient to handle large and potentially
redundant contexts; (ii) it exacerbates the `lost in the middle' phenomenon;
and (iii) under limited model capacity, it amplifies distraction, ultimately
degrading LLM output quality. In this paper, we propose LDAR (Learning
Distraction-Aware Retrieval), an adaptive retriever that learns to retrieve
contexts in a way that mitigates interference from distracting passages,
thereby achieving significantly higher performance with reduced token usage
compared to long-context approaches. Extensive experiments across diverse LLM
architectures and six knowledge-intensive benchmarks demonstrate the
effectiveness and robustness of our approach, highlighting the importance of
balancing the trade-off between information coverage and distraction.

</details>


### [376] [Abductive Logical Rule Induction by Bridging Inductive Logic Programming and Multimodal Large Language Models](https://arxiv.org/abs/2509.21874)
*Yifei Peng,Yaoli Liu,Enbo Xia,Yu Jin,Wang-Zhou Dai,Zhong Ren,Yao-Xiang Ding,Kun Zhou*

Main category: cs.LG

TL;DR: This paper introduces ILP-CoT, which combines Inductive Logic Programming (ILP) and Multimodal Large Language Models (MLLMs) for logical rule induction from limited unstructured data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in combining ILP and MLLMs for abductive logical rule induction. ILP faces limitations due to background knowledge requirements and computational costs, while MLLMs suffer from perceptual hallucinations.

Method: ILP-CoT utilizes MLLMs to propose structure-correct rules first, then builds ILP tasks with refined search spaces based on these proposals. The ILP system subsequently generates rules with rectified logical facts and formal reasoning.

Result: ILP-CoT showed effectiveness on logical induction benchmarks and demonstrated potential applications, such as text-to-image custom generation with rule induction.

Conclusion: ILP-CoT successfully bridges ILP and MLLMs, providing a feasible and effective solution for abductive logical rule induction. Code and data are made publicly available for further exploration.

Abstract: We propose ILP-CoT, a method that bridges Inductive Logic Programming (ILP)
and Multimodal Large Language Models (MLLMs) for abductive logical rule
induction. The task involves both discovering logical facts and inducing
logical rules from a small number of unstructured textual or visual inputs,
which still remain challenging when solely relying on ILP, due to the
requirement of specified background knowledge and high computational cost, or
MLLMs, due to the appearance of perceptual hallucinations. Based on the key
observation that MLLMs could propose structure-correct rules even under
hallucinations, our approach automatically builds ILP tasks with pruned search
spaces based on the rule structure proposals from MLLMs, and utilizes ILP
system to output rules built upon rectified logical facts and formal inductive
reasoning. Its effectiveness is verified through challenging logical induction
benchmarks, as well as a potential application of our approach, namely
text-to-image customized generation with rule induction. Our code and data are
released at https://github.com/future-item/ILP-CoT.

</details>


### [377] [Zubov-Net: Adaptive Stability for Neural ODEs Reconciling Accuracy with Robustness](https://arxiv.org/abs/2509.21879)
*Chaoyang Luo,Yan Zou,Nanjing Huang*

Main category: cs.LG

TL;DR: The paper introduces Zubov-Net, an adaptive stable learning framework for Neural ODEs that balances accuracy and robustness by reformulating Zubov's equation and actively optimizing prescribed regions of attraction (PRoAs).


<details>
  <summary>Details</summary>
Motivation: Existing approaches for Neural ODEs struggle with the trade-off between accuracy and robustness due to challenges in imposing stability conditions.

Method: The study introduces tripartite losses (consistency, classification, separation) and a parallel boundary sampling algorithm, supported by an input-attention-based convex neural network, to ensure alignment of PRoAs-RoAs and training stability.

Result: Zubov-Net achieves high classification accuracy and significantly improves robustness against stochastic noises and adversarial attacks.

Conclusion: The framework effectively addresses the robustness vs. accuracy trade-off in Neural ODEs, offering theoretical guarantees and experimental improvements in stability and separability.

Abstract: Despite neural ordinary differential equations (Neural ODEs) exhibiting
intrinsic robustness under input perturbations due to their dynamical systems
nature, recent approaches often involve imposing Lyapunov-based stability
conditions to provide formal robustness guarantees. However, a fundamental
challenge remains: the tension between robustness and accuracy, primarily
stemming from the difficulty in imposing appropriate stability conditions. To
address this, we propose an adaptive stable learning framework named Zubov-Net,
which innovatively reformulates Zubov's equation into a consistency
characterization between regions of attraction (RoAs) and prescribed RoAs
(PRoAs). Building on this consistency, we introduce a new paradigm for actively
controlling the geometry of RoAs by directly optimizing PRoAs to reconcile
accuracy and robustness. Our approach is realized through tripartite losses
(consistency, classification, and separation losses) and a parallel boundary
sampling algorithm that co-optimizes the Neural ODE and the Lyapunov function.
To enhance the discriminativity of Lyapunov functions, we design an
input-attention-based convex neural network via a softmax attention mechanism
that focuses on equilibrium-relevant features and also serves as weight
normalization to maintain training stability in deep architectures.
Theoretically, we prove that minimizing the tripartite loss guarantees
consistent alignment of PRoAs-RoAs, trajectory stability, and non-overlapping
PRoAs. Moreover, we establish stochastic convex separability with tighter
probability bounds and fewer dimensionality requirements to justify the convex
design in Lyapunov functions. Experimentally, Zubov-Net maintains high
classification accuracy while significantly improving robustness against
various stochastic noises and adversarial attacks.

</details>


### [378] [Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2509.21882)
*Aaron Tu,Weihao Xuan,Heli Qi,Xu Huang,Qingcheng Zeng,Shayan Talaei,Yijia Xiao,Peng Xia,Xiangru Tang,Yuchen Zhuang,Bing Hu,Hanqun Cao,Wenqi Shi,Tianang Leng,Rui Yang,Yingjian Chen,Ziqi Wang,Irene Li,Nan Liu,Huaxiu Yao,Li Erran Li,Ge Liu,Amin Saberi,Naoto Yokoya,Jure Leskovec,Yejin Choi,Fang Wu*

Main category: cs.LG

TL;DR: The paper analyzes the efficacy of Reinforcement Learning with Verifiable Rewards (RLVR), finding that while gains are real, they are often overstated due to several factors. It introduces a new protocol for more reliable evaluations.


<details>
  <summary>Details</summary>
Motivation: To assess the actual improvements of RLVR in tasks like math and code, and to determine if these improvements come with costs or certain evaluation issues.

Method: The authors conduct parity-controlled evaluations using contamination audits and matched-budget reproductions to test RLVR's effect on accuracy and reliability. They critique existing benchmarks and propose a new tax-aware training and evaluation protocol.

Result: Findings reveal that headline gains of RLVR are sometimes misleading due to evaluation pitfalls, RLVR taxes, and data contamination. The new protocol provides better and more reliable reasoning gain estimates.

Conclusion: RLVR is practical and valuable, but improvements require better evaluation methods. The paper recommends industry adoption of the new protocol focusing on reliability and safety without overstating performance gains.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a practical and
scalable approach to enhancing large language models in areas such as math,
code, and other structured tasks. Two questions motivate this paper: how much
of the reported gains survive under strictly parity-controlled evaluation, and
whether RLVR is cost-free or exacts a measurable tax. We argue that progress is
real, but gains are often overstated due to three forces - an RLVR tax,
evaluation pitfalls, and data contamination. Using a partial-prompt
contamination audit and matched-budget reproductions across base and RL models,
we show that several headline gaps shrink or vanish under clean,
parity-controlled evaluation. We then propose a tax-aware training and
evaluation protocol that co-optimizes accuracy, grounding, and calibrated
abstention and standardizes budgeting and provenance checks. Applied to recent
RLVR setups, this protocol yields more reliable estimates of reasoning gains
and, in several cases, revises prior conclusions. Our position is constructive:
RLVR is valuable and industry-ready; we advocate keeping its practical benefits
while prioritizing reliability, safety, and measurement.

</details>


### [379] [Closing the Oracle Gap: Increment Vector Transformation for Class Incremental Learning](https://arxiv.org/abs/2509.21898)
*Zihuan Qiu,Yi Xu,Fanman Meng,Runtong Zhang,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: The paper introduces a framework called Increment Vector Transformation (IVT) to enhance Class Incremental Learning by preserving linear connectivity to previous task optima, resulting in reduced catastrophic forgetting and improved model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from addressing the performance gap between current Class Incremental Learning (CIL) methods and their oracle counterparts, where models trained with full access to historical data perform significantly better. The paper is inspired by insights on Linear Mode Connectivity (LMC).

Method: The method involves the proposal of Increment Vector Transformation (IVT), which periodically transforms model parameters to preserve low-loss linear connections to previous task optima, achieved via efficiently approximating transformations using diagonal Fisher Information Matrices. IVT is plug-and-play, supports exemplar-free and exemplar-based scenarios, and is compatible with various initialization strategies.

Result: IVT enhances strong CIL baselines across multiple datasets, such as improving accuracy on CIFAR-100 by +5.12% and reducing forgetting by 2.54%, while yielding even higher performance gains in last and average accuracy on FGVCAircraft.

Conclusion: The findings demonstrate that IVT effectively mitigates catastrophic forgetting in CIL settings by leveraging the geometric properties of oracle solutions, offering improved task retention and consistent performance. The approach is efficient, versatile, and widely applicable to different scenarios.

Abstract: Class Incremental Learning (CIL) aims to sequentially acquire knowledge of
new classes without forgetting previously learned ones. Despite recent
progress, current CIL methods still exhibit significant performance gaps
compared to their oracle counterparts-models trained with full access to
historical data. Inspired by recent insights on Linear Mode Connectivity (LMC),
we revisit the geometric properties of oracle solutions in CIL and uncover a
fundamental observation: these oracle solutions typically maintain low-loss
linear connections to the optimum of previous tasks. Motivated by this finding,
we propose Increment Vector Transformation (IVT), a novel plug-and-play
framework designed to mitigate catastrophic forgetting during training. Rather
than directly following CIL updates, IVT periodically teleports the model
parameters to transformed solutions that preserve linear connectivity to
previous task optimum. By maintaining low-loss along these connecting paths,
IVT effectively ensures stable performance on previously learned tasks. The
transformation is efficiently approximated using diagonal Fisher Information
Matrices, making IVT suitable for both exemplar-free and exemplar-based
scenarios, and compatible with various initialization strategies. Extensive
experiments on CIFAR-100, FGVCAircraft, ImageNet-Subset, and ImageNet-Full
demonstrate that IVT consistently enhances the performance of strong CIL
baselines. Specifically, on CIFAR-100, IVT improves the last accuracy of the
PASS baseline by +5.12% and reduces forgetting by 2.54%. For the
CLIP-pre-trained SLCA baseline on FGVCAircraft, IVT yields gains of +14.93% in
average accuracy and +21.95% in last accuracy. The code will be released.

</details>


### [380] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: The paper introduces Multiplicative-Additive Constrained Models (MACMs) that enhance interpretability and predictive performance in machine learning, especially in high-stakes fields like healthcare.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Generalized Additive Models (GAMs) in handling higher-order interactions while maintaining interpretability, especially for critical applications like healthcare.

Method: The paper proposes MACMs, which combine a multiplicative model with an additive component to disentangle interaction and independent effects. This allows visualizations of both componentsâ€™ shape functions for better interpretability.

Result: MACMs outperform Curve Ergodic Set Regression (CESR) and state-of-the-art GAMs in predictive performance based on experimental results.

Conclusion: MACMs represent a significant advancement over traditional GAMs and CESR by enhancing predictive capabilities while preserving the interpretability of machine learning models.

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [381] [Generation Properties of Stochastic Interpolation under Finite Training Set](https://arxiv.org/abs/2509.21925)
*Yunchen Li,Shaohui Lin,Zhou Yu*

Main category: cs.LG

TL;DR: The paper explores the behavior of generative models with finite training data, providing theoretical insights, definitions of underfitting/overfitting, and verifying findings with practical experiments.


<details>
  <summary>Details</summary>
Motivation: Addressing the theoretical understanding of generative models in scenarios where training data is limited.

Method: Utilizing the stochastic interpolation framework to derive mathematical expressions and introducing definitions of underfitting and overfitting specific to generative models.

Result: Key findings include deterministic recovery of training samples, stochastic generation with Gaussian noise, and convex combinations of samples with mixed noise under estimation errors.

Conclusion: The study establishes theoretical constructs explaining generative behaviors under finite training populations and validates them through experimental evidence.

Abstract: This paper investigates the theoretical behavior of generative models under
finite training populations. Within the stochastic interpolation generative
framework, we derive closed-form expressions for the optimal velocity field and
score function when only a finite number of training samples are available. We
demonstrate that, under some regularity conditions, the deterministic
generative process exactly recovers the training samples, while the stochastic
generative process manifests as training samples with added Gaussian noise.
Beyond the idealized setting, we consider model estimation errors and introduce
formal definitions of underfitting and overfitting specific to generative
models. Our theoretical analysis reveals that, in the presence of estimation
errors, the stochastic generation process effectively produces convex
combinations of training samples corrupted by a mixture of uniform and Gaussian
noise. Experiments on generation tasks and downstream tasks such as
classification support our theory.

</details>


### [382] [Extracting Actionable Insights from Building Energy Data using Vision LLMs on Wavelet and 3D Recurrence Representations](https://arxiv.org/abs/2509.21934)
*Amine Bechar,Adel Oulefki,Abbes Amira,Fatih Kurogollu,Yassine Himeur*

Main category: cs.LG

TL;DR: The paper presents a framework utilizing 3D representations of energy time-series data with visual language large models (VLLMs) to detect anomalies and recommend energy optimizations, achieving superior results compared to raw time-series analysis.


<details>
  <summary>Details</summary>
Motivation: Time-series energy data is challenging to analyze due to its nonlinear and multi-scale nature, requiring an innovative approach for actionable insights and recommendations.

Method: The method converts 1D time-series data to 3D representations using continuous wavelet transforms (CWTs) and recurrence plots (RPs). These visual encodings are then fine-tuned with VLLMs like Idefics-7B to interpret patterns, anomalies, and make recommendations.

Result: Fine-tuned VLLMs achieved better anomaly detection performance on building energy datasets, with validation losses of 0.0952 (CWTs) and 0.1064 (RPs), surpassing raw time-series fine-tuning.

Conclusion: The proposed framework successfully combines time-series visualization with VLLMs, providing a scalable and interpretative method for building energy management and efficiency optimization.

Abstract: The analysis of complex building time-series for actionable insights and
recommendations remains challenging due to the nonlinear and multi-scale
characteristics of energy data. To address this, we propose a framework that
fine-tunes visual language large models (VLLMs) on 3D graphical representations
of the data. The approach converts 1D time-series into 3D representations using
continuous wavelet transforms (CWTs) and recurrence plots (RPs), which capture
temporal dynamics and localize frequency anomalies. These 3D encodings enable
VLLMs to visually interpret energy-consumption patterns, detect anomalies, and
provide recommendations for energy efficiency. We demonstrate the framework on
real-world building-energy datasets, where fine-tuned VLLMs successfully
monitor building states, identify recurring anomalies, and generate
optimization recommendations. Quantitatively, the Idefics-7B VLLM achieves
validation losses of 0.0952 with CWTs and 0.1064 with RPs on the University of
Sharjah energy dataset, outperforming direct fine-tuning on raw time-series
data (0.1176) for anomaly detection. This work bridges time-series analysis and
visualization, providing a scalable and interpretable framework for energy
analytics.

</details>


### [383] [Statistical Advantage of Softmax Attention: Insights from Single-Location Regression](https://arxiv.org/abs/2509.21936)
*O. Duranthon,P. Marion,C. Boyer,B. Loureiro,L. ZdeborovÃ¡*

Main category: cs.LG

TL;DR: This paper explores why softmax-based attention mechanisms outperform others by studying their theoretical behavior in single-location regression tasks.


<details>
  <summary>Details</summary>
Motivation: To address the theoretical gap in understanding why softmax-based attention mechanisms dominate over alternatives like linear attention in large language models.

Method: A principled analysis of attention mechanisms using statistical physics concepts applied to single-location regression tasks, examining both population-level and finite-sample regimes.

Result: Softmax achieves optimal Bayes risk at the population level and consistently outperforms linear attention in finite-sample scenarios, even if no longer Bayes-optimal in finite samples.

Conclusion: Softmax attention mechanisms are fundamental for optimal performance, especially in high-dimensional settings, and outperform linear alternatives theoretically and empirically.

Abstract: Large language models rely on attention mechanisms with a softmax activation.
Yet the dominance of softmax over alternatives (e.g., component-wise or linear)
remains poorly understood, and many theoretical works have focused on the
easier-to-analyze linearized attention. In this work, we address this gap
through a principled study of the single-location regression task, where the
output depends on a linear transformation of a single input token at a random
location. Building on ideas from statistical physics, we develop an analysis of
attention-based predictors in the high-dimensional limit, where generalization
performance is captured by a small set of order parameters. At the population
level, we show that softmax achieves the Bayes risk, whereas linear attention
fundamentally falls short. We then examine other activation functions to
identify which properties are necessary for optimal performance. Finally, we
analyze the finite-sample regime: we provide an asymptotic characterization of
the test error and show that, while softmax is no longer Bayes-optimal, it
consistently outperforms linear attention. We discuss the connection with
optimization by gradient-based algorithms.

</details>


### [384] [Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning](https://arxiv.org/abs/2509.21942)
*Xianghua Zeng,Hao Peng,Angsheng Li,Yicheng Pan*

Main category: cs.LG

TL;DR: This paper introduces SIHD, a framework that adaptively models trajectories using hierarchical diffusion for offline RL and achieves significant improvements in decision-making and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchical diffusion methods in offline RL use fixed two-layer hierarchies with rigid temporal scales, limiting flexibility and adaptability to diverse tasks.

Method: The proposed SIHD framework adapts the diffusion hierarchy using structural information from offline trajectories, employing structural information gain as conditioning signals, and introducing a structural entropy regularizer to balance exploration and avoid errors.

Result: Experimental evaluations show SIHD outperforms state-of-the-art baselines in both decision-making performance and generalization in complex offline RL environments.

Conclusion: SIHD demonstrates the ability to adaptively tackle long-horizon tasks with sparse rewards, enhancing both trajectory modeling flexibility and robust generalization across different scenarios.

Abstract: Diffusion-based generative methods have shown promising potential for
modeling trajectories from offline reinforcement learning (RL) datasets, and
hierarchical diffusion has been introduced to mitigate variance accumulation
and computational challenges in long-horizon planning tasks. However, existing
approaches typically assume a fixed two-layer diffusion hierarchy with a single
predefined temporal scale, which limits adaptability to diverse downstream
tasks and reduces flexibility in decision making. In this work, we propose
SIHD, a novel Structural Information-based Hierarchical Diffusion framework for
effective and stable offline policy learning in long-horizon environments with
sparse rewards. Specifically, we analyze structural information embedded in
offline trajectories to construct the diffusion hierarchy adaptively, enabling
flexible trajectory modeling across multiple temporal scales. Rather than
relying on reward predictions from localized sub-trajectories, we quantify the
structural information gain of each state community and use it as a
conditioning signal within the corresponding diffusion layer. To reduce
overreliance on offline datasets, we introduce a structural entropy regularizer
that encourages exploration of underrepresented states while avoiding
extrapolation errors from distributional shifts. Extensive evaluations on
challenging offline RL tasks show that SIHD significantly outperforms
state-of-the-art baselines in decision-making performance and demonstrates
superior generalization across diverse scenarios.

</details>


### [385] [Active Attacks: Red-teaming LLMs via Adaptive Environments](https://arxiv.org/abs/2509.21947)
*Taeyoung Yun,Pierre-Luc St-Charles,Jinkyoo Park,Yoshua Bengio,Minsu Kim*

Main category: cs.LG

TL;DR: The paper presents 'Active Attacks,' an RL-based approach for training attacker LLMs to generate diverse harmful prompts, outperforming prior methods significantly while requiring minimal additional computational cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of generating diverse harmful prompts for LLM safety fine-tuning since manual engineering and existing diversity-seeking RL methods are limited in scope and collapse into narrow-focused regions.

Method: The proposed method, 'Active Attacks,' uses a novel RL-based red-teaming algorithm inspired by active learning. It continually adapts to safety fine-tuning updates on victim LLMs and incorporates easy-to-hard exploration to uncover various attack modes.

Result: 'Active Attacks' achieved a significant improvement in cross-attack success rates (31.28%) compared to prior state-of-the-art methods like GFlowNets (0.07%), with only a 6% increase in computation.

Conclusion: The study demonstrates the efficacy of 'Active Attacks' as a highly efficient, adaptive, and diverse prompt generation method for safety fine-tuning in LLMs.

Abstract: We address the challenge of generating diverse attack prompts for large
language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual
content) and are used for safety fine-tuning. Rather than relying on manual
prompt engineering, attacker LLMs can be trained with reinforcement learning
(RL) to automatically generate such prompts using only a toxicity classifier as
a reward. However, capturing a wide range of harmful behaviors is a significant
challenge that requires explicit diversity objectives. Existing
diversity-seeking RL methods often collapse to limited modes: once high-reward
prompts are found, exploration of new regions is discouraged. Inspired by the
active learning paradigm that encourages adaptive exploration, we introduce
\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its
attacks as the victim evolves. By periodically safety fine-tuning the victim
LLM with collected attack prompts, rewards in exploited regions diminish, which
forces the attacker to seek unexplored vulnerabilities. This process naturally
induces an easy-to-hard exploration curriculum, where the attacker progresses
beyond easy modes toward increasingly difficult ones. As a result, Active
Attacks uncovers a wide range of local attack modes step by step, and their
combination achieves wide coverage of the multi-mode distribution. Active
Attacks, a simple plug-and-play module that seamlessly integrates into existing
RL objectives, unexpectedly outperformed prior RL-based methods -- including
GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates
against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a
relative gain greater than $400\ \times$) with only a 6% increase in
computation. Our code is publicly available
\href{https://github.com/dbsxodud-11/active_attacks}{here}.

</details>


### [386] [Think Smart, Not Hard: Difficulty Adaptive Reasoning for Large Audio Language Models](https://arxiv.org/abs/2509.21960)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: cs.LG

TL;DR: This paper addresses the inefficiency of "one-size-fits-all" reasoning in Large Audio Language Models (LALMs) and introduces a method to adapt reasoning depth based on problem difficulty.


<details>
  <summary>Details</summary>
Motivation: Large Audio Language Models (LALMs) often apply uniform reasoning strategies to problems of varying complexity, leading to inefficiencies such as overthinking simpler questions and underthinking complex ones.

Method: The authors propose a difficulty-adaptive reasoning framework for LALMs, utilizing a reward function that dynamically links reasoning depth to perceived problem difficulty.

Result: Experiments demonstrate the method improves task performance and reduces average reasoning length, proving both effectiveness and efficiency.

Conclusion: Difficulty-adaptive reasoning enhances LALMs' problem-solving capabilities while optimizing reasoning length, providing insights for further advancements in reasoning structure paradigms.

Abstract: Large Audio Language Models (LALMs), powered by the chain-of-thought (CoT)
paradigm, have shown remarkable reasoning capabilities. Intuitively, different
problems often require varying depths of reasoning. While some methods can
determine whether to reason for a given problem, they typically lack a
fine-grained mechanism to modulate how much to reason. This often results in a
``one-size-fits-all'' reasoning depth, which generates redundant overthinking
for simple questions while failing to allocate sufficient thought to complex
ones. In this paper, we conduct an in-depth analysis of LALMs and find that an
effective and efficient LALM should reason smartly by adapting its reasoning
depth to the problem's complexity. To achieve this, we propose a
difficulty-adaptive reasoning method for LALMs. Specifically, we propose a
reward function that dynamically links reasoning length to the model's
perceived problem difficulty. This reward encourages shorter, concise reasoning
for easy tasks and more elaborate, in-depth reasoning for complex ones.
Extensive experiments demonstrate that our method is both effective and
efficient, simultaneously improving task performance and significantly reducing
the average reasoning length. Further analysis on reasoning structure paradigm
offers valuable insights for future work.

</details>


### [387] [GRAM-TDI: adaptive multimodal representation learning for drug target interaction prediction](https://arxiv.org/abs/2509.21971)
*Feng Jiang,Amina Mollaysa,Hehuan Ma,Tommaso Mansi,Junzhou Huang,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: GRAMDTI is a framework for drug target interaction prediction that incorporates multimodal input and adaptive modality usage, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current DTI prediction models prioritize SMILES protein pairs and overlook multimodal data sources, limiting prediction accuracy and reliability.

Method: GRAMDTI integrates multimodal molecular and protein data for unified representations using volume-based contrastive learning and adaptive modality dropout, alongside IC50 activity for weak supervision.

Result: GRAMDTI demonstrated superior performance over state-of-the-art baselines across four datasets, showcasing improved robustness, alignment, and predictive accuracy.

Conclusion: GRAMDTI validates the advantages of leveraging multimodal inputs, adaptive dropout, and auxiliary supervision to enhance computational drug discovery efficiency and generalization.

Abstract: Drug target interaction (DTI) prediction is a cornerstone of computational
drug discovery, enabling rational design, repurposing, and mechanistic
insights. While deep learning has advanced DTI modeling, existing approaches
primarily rely on SMILES protein pairs and fail to exploit the rich multimodal
information available for small molecules and proteins. We introduce GRAMDTI, a
pretraining framework that integrates multimodal molecular and protein inputs
into unified representations. GRAMDTI extends volume based contrastive learning
to four modalities, capturing higher-order semantic alignment beyond
conventional pairwise approaches. To handle modality informativeness, we
propose adaptive modality dropout, dynamically regulating each modality's
contribution during pre-training. Additionally, IC50 activity measurements,
when available, are incorporated as weak supervision to ground representations
in biologically meaningful interaction strengths. Experiments on four publicly
available datasets demonstrate that GRAMDTI consistently outperforms state of
the art baselines. Our results highlight the benefits of higher order
multimodal alignment, adaptive modality utilization, and auxiliary supervision
for robust and generalizable DTI prediction.

</details>


### [388] [Stage-wise Dynamics of Classifier-Free Guidance in Diffusion Models](https://arxiv.org/abs/2509.22007)
*Cheng Jin,Qitan Shi,Yuantao Gu*

Main category: cs.LG

TL;DR: The paper analyzes the mechanisms of Classifier-Free Guidance (CFG) in diffusion models, highlighting its effects on multimodal conditionals and proposing a time-varying guidance schedule to optimize quality and diversity.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of Classifier-Free Guidance (CFG) beyond improving conditional fidelity and explore its influence on sampling dynamics, particularly under multimodal conditionals.

Method: The authors provide a theoretical and experimental analysis, breaking down the CFG sampling process into three stages: Direction Shift, Mode Separation, and Concentration. Experiments validate theoretical claims and test a time-varying guidance schedule.

Result: They found that stronger guidance improves semantic alignment but reduces diversity. Early strong guidance reduces global diversity, while late strong guidance diminishes fine-grained variability. A proposed time-varying guidance schedule improves both quality and diversity.

Conclusion: The unified framework clarifies the balance between fidelity and diversity in diffusion models and demonstrates the advantages of a dynamic guidance schedule to enhance both.

Abstract: Classifier-Free Guidance (CFG) is widely used to improve conditional fidelity
in diffusion models, but its impact on sampling dynamics remains poorly
understood. Prior studies, often restricted to unimodal conditional
distributions or simplified cases, provide only a partial picture. We analyze
CFG under multimodal conditionals and show that the sampling process unfolds in
three successive stages. In the Direction Shift stage, guidance accelerates
movement toward the weighted mean, introducing initialization bias and norm
growth. In the Mode Separation stage, local dynamics remain largely neutral,
but the inherited bias suppresses weaker modes, reducing global diversity. In
the Concentration stage, guidance amplifies within-mode contraction,
diminishing fine-grained variability. This unified view explains a widely
observed phenomenon: stronger guidance improves semantic alignment but
inevitably reduces diversity. Experiments support these predictions, showing
that early strong guidance erodes global diversity, while late strong guidance
suppresses fine-grained variation. Moreover, our theory naturally suggests a
time-varying guidance schedule, and empirical results confirm that it
consistently improves both quality and diversity.

</details>


### [389] [Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning](https://arxiv.org/abs/2509.22008)
*Yajie Qi,Wei Wei,Lin Li,Lijun Zhang,Zhidong Gao,Da Wang,Huizhong Song*

Main category: cs.LG

TL;DR: Structured Goal-guided Reinforcement Learning (SGRL) integrates goal planning and action pruning with LLMs for efficient exploration and decision-making, achieving superior results on Crafter and Craftax-Classic tasks.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning faces challenges in complex environments due to inefficient exploration and weak long-horizon planning, prompting the need for enhanced guidance mechanisms.

Method: SGRL employs LLMs to generate structured, reusable goal functions and uses an action-pruning mechanism to facilitate goal-aligned decisions for RL agents.

Result: Experiments on Crafter and Craftax-Classic show that SGRL outperforms existing state-of-the-art RL methods in efficiency and decision-making.

Conclusion: SGRL effectively leverages LLMs for structured goal planning and action pruning, addressing fundamental RL challenges and exhibiting strong performance in tested environments.

Abstract: Real-world decision-making tasks typically occur in complex and open
environments, posing significant challenges to reinforcement learning (RL)
agents' exploration efficiency and long-horizon planning capabilities. A
promising approach is LLM-enhanced RL, which leverages the rich prior knowledge
and strong planning capabilities of LLMs to guide RL agents in efficient
exploration. However, existing methods mostly rely on frequent and costly LLM
invocations and suffer from limited performance due to the semantic mismatch.
In this paper, we introduce a Structured Goal-guided Reinforcement Learning
(SGRL) method that integrates a structured goal planner and a goal-conditioned
action pruner to guide RL agents toward efficient exploration. Specifically,
the structured goal planner utilizes LLMs to generate a reusable, structured
function for goal generation, in which goals are prioritized. Furthermore, by
utilizing LLMs to determine goals' priority weights, it dynamically generates
forward-looking goals to guide the agent's policy toward more promising
decision-making trajectories. The goal-conditioned action pruner employs an
action masking mechanism that filters out actions misaligned with the current
goal, thereby constraining the RL agent to select goal-consistent policies. We
evaluate the proposed method on Crafter and Craftax-Classic, and experimental
results demonstrate that SGRL achieves superior performance compared to
existing state-of-the-art methods.

</details>


### [390] [Concept-SAE: Active Causal Probing of Visual Model Behavior](https://arxiv.org/abs/2509.22015)
*Jianrong Ding,Muxi Chen,Chenchen Zhao,Qiang Xu*

Main category: cs.LG

TL;DR: Concept-SAE introduces a hybrid disentanglement strategy for creating semantically meaningful concept tokens in autoencoders, enabling causal probing and failure mode analysis.


<details>
  <summary>Details</summary>
Motivation: The ambiguous and unreliable nature of features discovered by Standard Sparse Autoencoders limits their applicability for causal analysis in model behavior.

Method: Concept-SAE employs a novel dual-supervision disentanglement approach to generate semantically grounded concept tokens that are spatially localized and faithful.

Result: The framework significantly enhances disentanglement fidelity, enabling causal intervention analysis and systematic probing of models' adversarial vulnerabilities.

Conclusion: Concept-SAE opens pathways for mechanistic and causal exploration of model behavior, overcoming limitations of correlational interpretative techniques.

Abstract: Standard Sparse Autoencoders (SAEs) excel at discovering a dictionary of a
model's learned features, offering a powerful observational lens. However, the
ambiguous and ungrounded nature of these features makes them unreliable
instruments for the active, causal probing of model behavior. To solve this, we
introduce Concept-SAE, a framework that forges semantically grounded concept
tokens through a novel hybrid disentanglement strategy. We first quantitatively
demonstrate that our dual-supervision approach produces tokens that are
remarkably faithful and spatially localized, outperforming alternative methods
in disentanglement. This validated fidelity enables two critical applications:
(1) we probe the causal link between internal concepts and predictions via
direct intervention, and (2) we probe the model's failure modes by
systematically localizing adversarial vulnerabilities to specific layers.
Concept-SAE provides a validated blueprint for moving beyond correlational
interpretation to the mechanistic, causal probing of model behavior.

</details>


### [391] [AEGIS: Authentic Edge Growth In Sparsity for Link Prediction in Edge-Sparse Bipartite Knowledge Graphs](https://arxiv.org/abs/2509.22017)
*Hugh Xuechen Liu,KÄ±vanÃ§ Tatar*

Main category: cs.LG

TL;DR: This paper introduces AEGIS, a framework for enhancing link prediction in sparse bipartite knowledge graphs by resampling edges without creating fabricated nodes.


<details>
  <summary>Details</summary>
Motivation: Sparse and data-poor bipartite knowledge graphs face challenges in link prediction, necessitating methods to enhance edge information while retaining authenticity.

Method: AEGIS resamples existing edges using uniform or degree-aware methods to augment data while preserving original nodes. It evaluates performance using AUC-ROC and Brier score metrics across different sparsity levels.

Result: AEGIS shows promising results, especially when using semantic KNN augmentation, improving AUC and reducing Brier scores on sparse and text-rich graphs. Conversely, random and synthetic edges harm performance.

Conclusion: Authenticity-constrained resampling is a data-efficient strategy for sparse link prediction, with semantic augmentation offering additional gains when node descriptions are available.

Abstract: Bipartite knowledge graphs in niche domains are typically data-poor and
edge-sparse, which hinders link prediction. We introduce AEGIS (Authentic Edge
Growth In Sparsity), an edge-only augmentation framework that resamples
existing training edges -either uniformly simple or with inverse-degree bias
degree-aware -thereby preserving the original node set and sidestepping
fabricated endpoints. To probe authenticity across regimes, we consider
naturally sparse graphs (game design pattern's game-pattern network) and induce
sparsity in denser benchmarks (Amazon, MovieLens) via high-rate bond
percolation. We evaluate augmentations on two complementary metrics: AUC-ROC
(higher is better) and the Brier score (lower is better), using two-tailed
paired t-tests against sparse baselines. On Amazon and MovieLens, copy-based
AEGIS variants match the baseline while the semantic KNN augmentation is the
only method that restores AUC and calibration; random and synthetic edges
remain detrimental. On the text-rich GDP graph, semantic KNN achieves the
largest AUC improvement and Brier score reduction, and simple also lowers the
Brier score relative to the sparse control. These findings position
authenticity-constrained resampling as a data-efficient strategy for sparse
bipartite link prediction, with semantic augmentation providing an additional
boost when informative node descriptions are available.

</details>


### [392] [Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models](https://arxiv.org/abs/2509.22020)
*Shilei Cao,Hehai Lin,Jiashun Cheng,Yang Liu,Guowen Li,Xuehe Wang,Juepeng Zheng,Haoyuan Liang,Meng Jin,Chengwei Qin,Hong Cheng,Haohuan Fu*

Main category: cs.LG

TL;DR: This paper presents WeatherPEFT, a new parameter-efficient fine-tuning (PEFT) framework tailored for weather foundation models (WFMs), addressing challenges like heterogeneity and spatiotemporal variations with dynamic prompting and adaptive parameter selection.


<details>
  <summary>Details</summary>
Motivation: The research targets the increasing computational demands of weather foundation models and the inefficiencies of existing PEFT methods, which perform poorly on weather-specific tasks involving high variability and complexity.

Method: WeatherPEFT combines Task-Adaptive Dynamic Prompting (TADP) for dynamic embedding recalibration during forward passes and Stochastic Fisher-Guided Adaptive Selection (SFAS) for selective, efficient parameter updates during backpropagation.

Result: WeatherPEFT matches the performance of full-tuning approaches on three weather downstream tasks while requiring fewer parameters compared to existing PEFT methods.

Conclusion: WeatherPEFT is an efficient and effective solution for fine-tuning WFMs, addressing their unique challenges and offering computational savings without sacrificing performance.

Abstract: While recent advances in machine learning have equipped Weather Foundation
Models (WFMs) with substantial generalization capabilities across diverse
downstream tasks, the escalating computational requirements associated with
their expanding scale increasingly hinder practical deployment. Current
Parameter-Efficient Fine-Tuning (PEFT) methods, designed for vision or language
tasks, fail to address the unique challenges of weather downstream tasks, such
as variable heterogeneity, resolution diversity, and spatiotemporal coverage
variations, leading to suboptimal performance when applied to WFMs. To bridge
this gap, we introduce WeatherPEFT, a novel PEFT framework for WFMs
incorporating two synergistic innovations. First, during the forward pass,
Task-Adaptive Dynamic Prompting (TADP) dynamically injects the embedding
weights within the encoder to the input tokens of the pre-trained backbone via
internal and external pattern extraction, enabling context-aware feature
recalibration for specific downstream tasks. Furthermore, during
backpropagation, Stochastic Fisher-Guided Adaptive Selection (SFAS) not only
leverages Fisher information to identify and update the most task-critical
parameters, thereby preserving invariant pre-trained knowledge, but also
introduces randomness to stabilize the selection. We demonstrate the
effectiveness and efficiency of WeatherPEFT on three downstream tasks, where
existing PEFT methods show significant gaps versus Full-Tuning, and WeatherPEFT
achieves performance parity with Full-Tuning using fewer trainable parameters.
The code of this work will be released.

</details>


### [393] [Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error](https://arxiv.org/abs/2509.22023)
*Panagiotis Giannoulis,Yorgos Pantis,Christos Tzamos*

Main category: cs.LG

TL;DR: This paper introduces a novel method for solving NP problems using a vanilla Transformer model, achieving high accuracy in Sudoku puzzles.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of Large Language Models in solving combinatorial problems and demonstrate proficiency in NP-class tasks.

Method: Employs a decoder-only Transformer (GPT-2) with imitation learning of Sudoku rules and a Depth-First Search strategy.

Result: Achieves state-of-the-art 99% accuracy on Sudoku compared to neuro-symbolic systems.

Conclusion: Vanilla Transformer models can effectively solve complex combinatorial problems using explicit search strategies and learning frameworks.

Abstract: Despite their proficiency in various language tasks, Large Language Models
(LLMs) struggle with combinatorial problems like Satisfiability, Traveling
Salesman Problem, or even basic arithmetic. We address this gap through a novel
approach for solving problems in the class NP. We focus on the paradigmatic
task of Sudoku and achieve state-of-the-art accuracy (99\%) compared to prior
neuro-symbolic approaches. Unlike prior work that used custom architectures,
our method employs a vanilla decoder-only Transformer (GPT-2) without external
tools or function calling. Our method integrates imitation learning of simple
Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy
involving informed guessing and backtracking. Moving beyond imitation learning,
we seek to minimize the number of guesses until reaching a solution. We provide
a rigorous analysis of this setup formalizing its connection to a contextual
variant of Min-Sum Set Cover, a well-studied problem in algorithms and
stochastic optimization.

</details>


### [394] [MCGM: Multi-stage Clustered Global Modeling for Long-range Interactions in Molecules](https://arxiv.org/abs/2509.22028)
*Haodong Pan,Yusong Wang,Nanning Zheng,Caijui Jiang*

Main category: cs.LG

TL;DR: This paper introduces a module named Multi-stage Clustered Global Modeling (MCGM) to enhance geometric graph neural networks (GNNs) for modeling long-range molecular interactions while addressing computational and generalization challenges in conventional methods.


<details>
  <summary>Details</summary>
Motivation: Geometric GNNs excel in capturing molecular geometry but struggle with modeling long-range interactions due to locality-biased message passing. Existing solutions either scale poorly, lack generalization, or require extensive parameter tuning.

Method: The proposed MCGM module uses hierarchical clustering to build multi-resolution atomic clusters, extract global information, and propagate it back through residual connections. It integrates seamlessly into existing GNN architectures.

Result: MCGM reduces OE62 energy prediction error by an average of 26.2%, achieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/Ã… for forces), and requires 20% fewer parameters compared to Neural P3M.

Conclusion: MCGM provides a lightweight and effective way to enhance GNNs with global context, improving accuracy and efficiency in molecular modeling tasks. Code will be made available upon acceptance.

Abstract: Geometric graph neural networks (GNNs) excel at capturing molecular geometry,
yet their locality-biased message passing hampers the modeling of long-range
interactions. Current solutions have fundamental limitations: extending cutoff
radii causes computational costs to scale cubically with distance;
physics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific
and lack generality; Fourier-space methods require careful tuning of multiple
parameters (e.g., mesh size, k-space cutoff) with added computational overhead.
We introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,
plug-and-play module that endows geometric GNNs with hierarchical global
context through efficient clustering operations. MCGM builds a multi-resolution
hierarchy of atomic clusters, distills global information via dynamic
hierarchical clustering, and propagates this context back through learned
transformations, ultimately reinforcing atomic features via residual
connections. Seamlessly integrated into four diverse backbone architectures,
MCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM
achieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\AA} for
forces) while using 20% fewer parameters than Neural P3M. Code will be made
available upon acceptance.

</details>


### [395] [OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features](https://arxiv.org/abs/2509.22033)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Rogov,Elena Tutubalina,Ivan Oseledets*

Main category: cs.LG

TL;DR: The paper introduces Orthogonal Sparse Autoencoder (OrtSAE) to enhance feature disentanglement in sparse autoencoders by penalizing cosine similarity and demonstrates its benefits over traditional SAEs.


<details>
  <summary>Details</summary>
Motivation: To address issues of feature absorption and feature composition in sparse autoencoders (SAEs) that hinder the development of disentangled and human-interpretable features.

Method: Proposes the Orthogonal SAE (OrtSAE), which includes a novel training procedure that penalizes high pairwise cosine similarity between features to encourage feature orthogonality. The approach scales linearly with model size without introducing significant computational costs.

Result: OrtSAE discovers 9% more distinct features, reduces feature absorption by 65%, reduces composition by 15%, improves performance on spurious correlation removal by 6%, and shows comparable performance to traditional SAEs in other tasks.

Conclusion: OrtSAE effectively mitigates key limitations of SAEs, promoting disentanglement and improving performance benchmarks while maintaining computational efficiency.

Abstract: Sparse autoencoders (SAEs) are a technique for sparse decomposition of neural
network activations into human-interpretable features. However, current SAEs
suffer from feature absorption, where specialized features capture instances of
general features creating representation holes, and feature composition, where
independent features merge into composite representations. In this work, we
introduce Orthogonal SAE (OrtSAE), a novel approach aimed to mitigate these
issues by enforcing orthogonality between the learned features. By implementing
a new training procedure that penalizes high pairwise cosine similarity between
SAE features, OrtSAE promotes the development of disentangled features while
scaling linearly with the SAE size, avoiding significant computational
overhead. We train OrtSAE across different models and layers and compare it
with other methods. We find that OrtSAE discovers 9% more distinct features,
reduces feature absorption (by 65%) and composition (by 15%), improves
performance on spurious correlation removal (+6%), and achieves on-par
performance for other downstream tasks compared to traditional SAEs.

</details>


### [396] [Latent Diffusion : Multi-Dimension Stable Diffusion Latent Space Explorer](https://arxiv.org/abs/2509.22038)
*Zhihua Zhong,Xuanyang Huang*

Main category: cs.LG

TL;DR: This paper introduces a framework for incorporating customizable latent space operations in diffusion models like Stable Diffusion, enhancing their utility for creative and artistic applications.


<details>
  <summary>Details</summary>
Motivation: Diffusion models lack intuitive latent vector control, which limits their flexibility compared to GANs, particularly in artistic expression.

Method: The authors developed a framework that allows direct manipulation of conceptual and spatial representations in the latent space of diffusion models.

Result: The framework's potential was demonstrated through artworks showcasing conceptual blending and dynamic motion generation. Insights into the latent space structure were also uncovered.

Conclusion: This framework significantly extends the creative possibilities of diffusion models, offering both practical artistic tools and deeper insights into latent space geometry.

Abstract: Latent space is one of the key concepts in generative AI, offering powerful
means for creative exploration through vector manipulation. However, diffusion
models like Stable Diffusion lack the intuitive latent vector control found in
GANs, limiting their flexibility for artistic expression. This paper introduces
\workname, a framework for integrating customizable latent space operations
into the diffusion process. By enabling direct manipulation of conceptual and
spatial representations, this approach expands creative possibilities in
generative art. We demonstrate the potential of this framework through two
artworks, \textit{Infinitepedia} and \textit{Latent Motion}, highlighting its
use in conceptual blending and dynamic motion generation. Our findings reveal
latent space structures with semantic and meaningless regions, offering
insights into the geometry of diffusion models and paving the way for further
explorations of latent space.

</details>


### [397] [MO-GRPO: Mitigating Reward Hacking of Group Relative Policy Optimization on Multi-Objective Problems](https://arxiv.org/abs/2509.22047)
*Yuki Ichihara,Yuu Jinnai,Tetsuro Morimura,Mitsuki Sakamoto,Ryota Mitsuhashi,Eiji Uchibe*

Main category: cs.LG

TL;DR: MO-GRPO is introduced as an improvement to the GRPO algorithm for multi-objective reinforcement learning, addressing reward hacking with automatic reward normalization based on variances.


<details>
  <summary>Details</summary>
Motivation: The paper investigates the limitations of GRPO in multi-objective settings, specifically its vulnerability to reward hacking where one objective is overly optimized at the expense of others.

Method: MO-GRPO includes a normalization technique that reweights reward functions automatically according to their variances, ensuring balanced contributions to the loss function without manual tuning.

Result: In four experimental domains, MO-GRPO demonstrates stable learning with improved performance over GRPO by distributing reward correlations evenly.

Conclusion: MO-GRPO effectively addresses the challenges in multi-objective reinforcement learning, showing promise as a more balanced algorithm compared to GRPO.

Abstract: Group Relative Policy Optimization (GRPO) has been shown to be an effective
algorithm when an accurate reward model is available. However, such a highly
reliable reward model is not available in many real-world tasks. In this paper,
we particularly focus on multi-objective settings, in which we identify that
GRPO is vulnerable to reward hacking, optimizing only one of the objectives at
the cost of the others. To address this issue, we propose MO-GRPO, an extension
of GRPO with a simple normalization method to reweight the reward functions
automatically according to the variances of their values. We first show
analytically that MO-GRPO ensures that all reward functions contribute evenly
to the loss function while preserving the order of preferences, eliminating the
need for manual tuning of the reward functions' scales. Then, we evaluate
MO-GRPO experimentally in four domains: (i) the multi-armed bandits problem,
(ii) simulated control task (Mo-Gymnasium), (iii) machine translation tasks on
the WMT benchmark (En-Ja, En-Zh), and (iv) instruction following task. MO-GRPO
achieves stable learning by evenly distributing correlations among the
components of rewards, outperforming GRPO, showing MO-GRPO to be a promising
algorithm for multi-objective reinforcement learning problems.

</details>


### [398] [BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning](https://arxiv.org/abs/2509.22050)
*Yi Ding,Muyun Jiang,Weibang Jiang,Shuailei Zhang,Xinliang Zhou,Chenyu Liu,Shanglin Li,Yong Li,Cuntai Guan*

Main category: cs.LG

TL;DR: BrainPro is a novel large-scale EEG model that introduces improved spatial learning and state-aware encoders, showing state-of-the-art performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing EEG models often struggle to capture critical spatial interactions and state-aware representations, limiting their flexibility and effectiveness.

Method: BrainPro integrates a retrieval-based spatial learning block to capture spatial interactions, along with a state-decoupling block for state-aware representations, enabling adaptability to different tasks and hardware.

Result: BrainPro demonstrates state-of-the-art performance and robust generalization across nine public BCI datasets, highlighting its effectiveness.

Conclusion: BrainPro's innovative design advances the field of EEG modeling, providing enhanced adaptability and generalization for diverse applications in BCI and healthcare.

Abstract: Electroencephalography (EEG) is a non-invasive technique for recording brain
electrical activity, widely used in brain-computer interface (BCI) and
healthcare. Recent EEG foundation models trained on large-scale datasets have
shown improved performance and generalizability over traditional decoding
methods, yet significant challenges remain. Existing models often fail to
explicitly capture channel-to-channel and region-to-region interactions, which
are critical sources of information inherently encoded in EEG signals. Due to
varying channel configurations across datasets, they either approximate spatial
structure with self-attention or restrict training to a limited set of common
channels, sacrificing flexibility and effectiveness. Moreover, although EEG
datasets reflect diverse brain states such as emotion, motor, and others,
current models rarely learn state-aware representations during self-supervised
pre-training. To address these gaps, we propose BrainPro, a large EEG model
that introduces a retrieval-based spatial learning block to flexibly capture
channel- and region-level interactions across varying electrode layouts, and a
brain state-decoupling block that enables state-aware representation learning
through parallel encoders with decoupling and region-aware reconstruction
losses. This design allows BrainPro to adapt seamlessly to diverse tasks and
hardware settings. Pre-trained on an extensive EEG corpus, BrainPro achieves
state-of-the-art performance and robust generalization across nine public BCI
datasets. Our codes and the pre-trained weights will be released.

</details>


### [399] [Enriching Knowledge Distillation with Intra-Class Contrastive Learning](https://arxiv.org/abs/2509.22053)
*Hua Yuan,Ning Xu,Xin Geng,Yong Rui*

Main category: cs.LG

TL;DR: The paper proposes using intra-class contrastive loss in teacher models during knowledge distillation to enrich intra-class diversity within soft labels. Margin loss is incorporated to stabilize training and improve convergence.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge distillation methods fail to consider the diverse representations within the same class, focusing mostly on ground-truth labels as targets.

Method: Incorporating intra-class contrastive loss in teacher training, supplemented with margin loss to address training instability and slow convergence.

Result: The method enriches intra-class diversity and theoretical analysis supports its impact on class distances. Experiments validate its effectiveness.

Conclusion: Augmenting teacher training with intra-class contrastive loss improves the soft labels' ability to generalize diverse student representations, overcoming limitations of conventional approaches.

Abstract: Since the advent of knowledge distillation, much research has focused on how
the soft labels generated by the teacher model can be utilized effectively.
Existing studies points out that the implicit knowledge within soft labels
originates from the multi-view structure present in the data. Feature
variations within samples of the same class allow the student model to
generalize better by learning diverse representations. However, in existing
distillation methods, teacher models predominantly adhere to ground-truth
labels as targets, without considering the diverse representations within the
same class. Therefore, we propose incorporating an intra-class contrastive loss
during teacher training to enrich the intra-class information contained in soft
labels. In practice, we find that intra-class loss causes instability in
training and slows convergence. To mitigate these issues, margin loss is
integrated into intra-class contrastive learning to improve the training
stability and convergence speed. Simultaneously, we theoretically analyze the
impact of this loss on the intra-class distances and inter-class distances. It
has been proved that the intra-class contrastive loss can enrich the
intra-class diversity. Experimental results demonstrate the effectiveness of
the proposed method.

</details>


### [400] [Towards Understanding Feature Learning in Parameter Transfer](https://arxiv.org/abs/2509.22056)
*Hua Yuan,Xuran Meng,Qiufeng Wang,Shiyu Xia,Ning Xu,Xu Yang,Jing Wang,Xin Geng,Yong Rui*

Main category: cs.LG

TL;DR: This paper explores when transferring a subset of parameters from an upstream ReLU CNN to a downstream model is beneficial, and the factors influencing this effectiveness.


<details>
  <summary>Details</summary>
Motivation: Understanding the theoretical conditions and factors behind the success or failure of transferring subsets of model parameters between tasks, particularly for ReLU CNNs.

Method: Theoretical analysis of the parameter transfer process within ReLU CNNs, supported by empirical validation through numerical experiments and real-world data.

Result: Identification of conditions where inherited parameters contribute effectively as universal knowledge carriers; explanation of how parameter transfer can sometimes hinder downstream test accuracy.

Conclusion: Characterizing the dynamics of parameter transfer provides insights into optimizing its use and understanding when it may fail.

Abstract: Parameter transfer is a central paradigm in transfer learning, enabling
knowledge reuse across tasks and domains by sharing model parameters between
upstream and downstream models. However, when only a subset of parameters from
the upstream model is transferred to the downstream model, there remains a lack
of theoretical understanding of the conditions under which such partial
parameter reuse is beneficial and of the factors that govern its effectiveness.
To address this gap, we analyze a setting in which both the upstream and
downstream models are ReLU convolutional neural networks (CNNs). Within this
theoretical framework, we characterize how the inherited parameters act as
carriers of universal knowledge and identify key factors that amplify their
beneficial impact on the target task. Furthermore, our analysis provides
insight into why, in certain cases, transferring parameters can lead to lower
test accuracy on the target task than training a new model from scratch.
Numerical experiments and real-world data experiments are conducted to
empirically validate our theoretical findings.

</details>


### [401] [The Rogue Scalpel: Activation Steering Compromises LLM Safety](https://arxiv.org/abs/2509.22067)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Y. Rogov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.LG

TL;DR: Activation steering unexpectedly increases the likelihood of harmful compliance in LLMs, challenging its suitability as a safer alternative to fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To investigate whether activation steering can serve as a precise and interpretable mechanism to align and control LLM behavior without compromising safety.

Method: Conduct experiments on different model families by introducing activation steering, using random directions and sparse autoencoder (SAE) features, and combining vectors to evaluate their impact on harmful compliance.

Result: Activation steering increases harmful compliance rates from 0% to 2-27% depending on the method applied, with benign features from SAE increasing these rates further by 2-4%. Combined vectors create a universal attack, exacerbating the issue.

Conclusion: Activation steering breaks alignment safeguards, undermining its reliability as a safer, interpretable alternative for controlling LLM behavior.

Abstract: Activation steering is a promising technique for controlling LLM behavior by
adding semantically meaningful vectors directly into a model's hidden states
during inference. It is often framed as a precise, interpretable, and
potentially safer alternative to fine-tuning. We demonstrate the opposite:
steering systematically breaks model alignment safeguards, making it comply
with harmful requests. Through extensive experiments on different model
families, we show that even steering in a random direction can increase the
probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign
features from a sparse autoencoder (SAE), a common source of interpretable
directions, increases these rates by a further 2-4%. Finally, we show that
combining 20 randomly sampled vectors that jailbreak a single prompt creates a
universal attack, significantly increasing harmful compliance on unseen
requests. These results challenge the paradigm of safety through
interpretability, showing that precise control over model internals does not
guarantee precise control over model behavior.

</details>


### [402] [Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2509.22082)
*Li Xia,Zheng Liu,Sili Huang,Wei Tang,Xuan Liu*

Main category: cs.LG

TL;DR: The paper proposes NL-SME, a nonlinear surrogate modeling method for Gradient Inversion Attacks (GIA) within Federated Learning (FL), improving attack effectiveness significantly.


<details>
  <summary>Details</summary>
Motivation: Existing GIA methods for FL fail to capture the nonlinear complexities of SGD, limiting their effectiveness when reconstructing data from aggregated gradients.

Method: The authors introduce NL-SME, which uses learnable quadratic BÃ©zier curves for modeling nonlinear parameter trajectories, combined with regularization and scaling mechanisms.

Result: NL-SME outperforms baseline methods on CIFAR-100 and FEMNIST datasets, achieving better cosine similarity loss and maintaining computational efficiency.

Conclusion: NL-SME exposes intensified privacy vulnerabilities in Federated Learning, highlighting the need for robust defense strategies against improved GIA.

Abstract: Federated Learning (FL) preserves privacy by keeping raw data local, yet
Gradient Inversion Attacks (GIAs) pose significant threats. In FedAVG
multi-step scenarios, attackers observe only aggregated gradients, making data
reconstruction challenging. Existing surrogate model methods like SME assume
linear parameter trajectories, but we demonstrate this severely underestimates
SGD's nonlinear complexity, fundamentally limiting attack effectiveness. We
propose Non-Linear Surrogate Model Extension (NL-SME), the first method to
introduce nonlinear parametric trajectory modeling for GIAs. Our approach
replaces linear interpolation with learnable quadratic B\'ezier curves that
capture SGD's curved characteristics through control points, combined with
regularization and dvec scaling mechanisms for enhanced expressiveness.
Extensive experiments on CIFAR-100 and FEMNIST datasets show NL-SME
significantly outperforms baselines across all metrics, achieving
order-of-magnitude improvements in cosine similarity loss while maintaining
computational efficiency.This work exposes heightened privacy vulnerabilities
in FL's multi-step update paradigm and offers novel perspectives for developing
robust defense strategies.

</details>


### [403] [Reinforcement Learning for Durable Algorithmic Recourse](https://arxiv.org/abs/2509.22102)
*Marina Ceccon,Alessandro Fabris,Goran RadanoviÄ‡,Asia J. Biega,Gian Antonio Susto*

Main category: cs.LG

TL;DR: This paper introduces a time-aware, durable algorithmic recourse system that uses reinforcement learning to generate recommendations in dynamic environments, enhancing feasibility and long-term validity.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve algorithmic recourse systems by considering the temporal dynamics and behavioral adaptations of individuals in competitive, resource-constrained settings.

Method: The authors developed a novel reinforcement learning-based recourse algorithm within a time-aware framework to model how applicant behavior evolves over time, generating durable and valid recommendations suitable over a predefined time horizon.

Result: The proposed algorithm significantly outperformed existing methods in extensive simulations, demonstrating better feasibility and durability of recommendations over time.

Conclusion: Incorporating temporal and behavioral dynamics into algorithmic recourse systems leads to more practical and effective recommendations for long-term success, emphasizing the need for time-aware frameworks in such systems.

Abstract: Algorithmic recourse seeks to provide individuals with actionable
recommendations that increase their chances of receiving favorable outcomes
from automated decision systems (e.g., loan approvals). While prior research
has emphasized robustness to model updates, considerably less attention has
been given to the temporal dynamics of recourse--particularly in competitive,
resource-constrained settings where recommendations shape future applicant
pools. In this work, we present a novel time-aware framework for algorithmic
recourse, explicitly modeling how candidate populations adapt in response to
recommendations. Additionally, we introduce a novel reinforcement learning
(RL)-based recourse algorithm that captures the evolving dynamics of the
environment to generate recommendations that are both feasible and valid. We
design our recommendations to be durable, supporting validity over a predefined
time horizon T. This durability allows individuals to confidently reapply after
taking time to implement the suggested changes. Through extensive experiments
in complex simulation environments, we show that our approach substantially
outperforms existing baselines, offering a superior balance between feasibility
and long-term validity. Together, these results underscore the importance of
incorporating temporal and behavioral dynamics into the design of practical
recourse systems.

</details>


### [404] [Modeling Psychological Profiles in Volleyball via Mixed-Type Bayesian Networks](https://arxiv.org/abs/2509.22111)
*Maria Iannario,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.LG

TL;DR: The paper introduces a method to analyze the psychological traits and relations among various attributes to better understand athletes' development.


<details>
  <summary>Details</summary>
Motivation: To create a data-driven and interpretable framework for analyzing psychological traits in sports and enhancing athlete development strategies.

Method: The study employs latent MMHC, a hybrid structure-learning algorithm, capable of analyzing mixed-type variables and building directed acyclic graphs (DAGs) to reveal relational networks.

Result: Latent MMHC demonstrated superior structural accuracy, edge recall, and specificity in simulations. It identified key psychological trait relations in volleyball players, such as self-confidence, emotional arousal linked to motivation and anxiety, and upstream Big-Five traits.

Conclusion: This method offers valuable insights for profiling athletes' psychological traits and suggests propagation effects through directed networks, aiding athlete preparation and development.

Abstract: Psychological attributes rarely operate in isolation: coaches reason about
networks of related traits. We analyze a new dataset of 164 female volleyball
players from Italy's C and D leagues that combines standardized psychological
profiling with background information. To learn directed relationships among
mixed-type variables (ordinal questionnaire scores, categorical demographics,
continuous indicators), we introduce latent MMHC, a hybrid structure learner
that couples a latent Gaussian copula and a constraint-based skeleton with a
constrained score-based refinement to return a single DAG. We also study a
bootstrap-aggregated variant for stability. In simulations spanning sample
size, sparsity, and dimension, latent Max-Min Hill-Climbing (MMHC) attains
lower structural Hamming distance and higher edge recall than recent
copula-based learners while maintaining high specificity. Applied to
volleyball, the learned network organizes mental skills around goal setting and
self-confidence, with emotional arousal linking motivation and anxiety, and
locates Big-Five traits (notably neuroticism and extraversion) upstream of
skill clusters. Scenario analyses quantify how improvements in specific skills
propagate through the network to shift preparation, confidence, and
self-esteem. The approach provides an interpretable, data-driven framework for
profiling psychological traits in sport and for decision support in athlete
development.

</details>


### [405] [Countering adversarial evasion in regression analysis](https://arxiv.org/abs/2509.22113)
*David Benfield,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: This paper extends pessimistic bilevel optimization to regression tasks in adversarial machine learning, dealing with non-convex and non-unique adversarial strategies.


<details>
  <summary>Details</summary>
Motivation: To address adversarial threats in machine learning where adversaries adapt data to exploit prediction models, especially in regression scenarios where existing methods have limitations.

Method: Introduces a pessimistic bilevel optimization framework tailored for regression models without requiring convexity or uniqueness assumptions on adversarial strategies.

Result: The proposed framework aims to enhance resilience against adversarial manipulations in regression tasks.

Conclusion: Applying pessimistic bilevel optimization to regression expands its applicability in adversarial contexts, enhancing the robustness of such systems.

Abstract: Adversarial machine learning challenges the assumption that the underlying
distribution remains consistent throughout the training and implementation of a
prediction model. In particular, adversarial evasion considers scenarios where
adversaries adapt their data to influence particular outcomes from established
prediction models, such scenarios arise in applications such as spam email
filtering, malware detection and fake-image generation, where security methods
must be actively updated to keep up with the ever-improving generation of
malicious data. Game theoretic models have been shown to be effective at
modelling these scenarios and hence training resilient predictors against such
adversaries. Recent advancements in the use of pessimistic bilevel optimsiation
which remove assumptions about the convexity and uniqueness of the adversary's
optimal strategy have proved to be particularly effective at mitigating threats
to classifiers due to its ability to capture the antagonistic nature of the
adversary. However, this formulation has not yet been adapted to regression
scenarios. This article serves to propose a pessimistic bilevel optimisation
program for regression scenarios which makes no assumptions on the convexity or
uniqueness of the adversary's solutions.

</details>


### [406] [Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization](https://arxiv.org/abs/2509.22115)
*Chao Wang,Tao Yang,Hongtao Tian,Yunsheng Shi,Qiyao Ma,Xiaotao Liu,Ting Yao,Wenbo Ding*

Main category: cs.LG

TL;DR: Critic-free methods like GRPO are hindered by slow convergence due to diluted learning signals. D$^3$S addresses this by prioritizing samples and tokens for efficient policy optimization, using a dynamic down-sampling schedule to balance learning acceleration and generalization.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to improve the efficiency of critic-free methods, which typically require extensive computational resources and suffer from slow convergence due to uninformative data.

Method: The proposed Dynamic Dual-Level Down-Sampling (D$^3$S) framework uses sample-level and token-level prioritizations to focus learning efforts where they have the most impact. A dynamic down-sampling schedule inspired by curriculum learning is applied to balance aggressive learning and generalization.

Result: Experiments show integrating D$^3$S into advanced reinforcement learning algorithms achieves state-of-the-art performance while utilizing fewer samples and tokens across benchmarks like Qwen2.5 and Llama3.1.

Conclusion: The D$^3$S framework represents a significant advancement in policy optimization by accelerating learning and improving generalization, making it a superior choice for tasks requiring efficient use of data.

Abstract: Critic-free methods like GRPO reduce memory demands by estimating advantages
from multiple rollouts but tend to converge slowly, as critical learning
signals are diluted by an abundance of uninformative samples and tokens. To
tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling
(D$^3$S)} framework that prioritizes the most informative samples and tokens
across groups to improve the efficient of policy optimization. D$^3$S operates
along two levels: (1) the sample-level, which selects a subset of rollouts to
maximize advantage variance ($\text{Var}(A)$). We theoretically proven that
this selection is positively correlated with the upper bound of the policy
gradient norms, yielding higher policy gradients. (2) the token-level, which
prioritizes tokens with a high product of advantage magnitude and policy
entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the
policy is both uncertain and impactful. Moreover, to prevent overfitting to
high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by
curriculum learning. This schedule starts with aggressive down-sampling to
accelerate early learning and gradually relaxes to promote robust
generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that
integrating D$^3$S into advanced RL algorithms achieves state-of-the-art
performance and generalization while requiring \textit{fewer} samples and
tokens across diverse reasoning benchmarks. Our code is added in the
supplementary materials and will be made publicly available.

</details>


### [407] [Mind the Missing: Variable-Aware Representation Learning for Irregular EHR Time Series using Large Language Models](https://arxiv.org/abs/2509.22121)
*Jeong Eul Kwon,Joo Heung Yoon,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: The paper introduces VITAL, a framework utilizing large language models (LLMs) to manage irregular sampling and missing data in EHR-based time series. It categorizes clinical variables for better representation and outperforms current methods.


<details>
  <summary>Details</summary>
Motivation: Irregular sampling and high missingness make modeling EHR-derived time series challenging due to uneven recording intervals and data gaps.

Method: VITAL uses language model reprogramming for vital signs and customized embedding strategies for laboratory variables to capture temporal context and manage missing values.

Result: The framework achieves superior performance compared to state-of-the-art methods on benchmark datasets and remains effective under high missingness conditions.

Conclusion: VITAL demonstrates its ability to handle real-world clinical time series challenges, showcasing its efficacy and robustness in irregular sampling scenarios.

Abstract: Irregular sampling and high missingness are intrinsic challenges in modeling
time series derived from electronic health records (EHRs),where clinical
variables are measured at uneven intervals depending on workflow and
intervention timing. To address this, we propose VITAL, a variable-aware, large
language model (LLM) based framework tailored for learning from irregularly
sampled physiological time series. VITAL differentiates between two distinct
types of clinical variables: vital signs, which are frequently recorded and
exhibit temporal patterns, and laboratory tests, which are measured
sporadically and lack temporal structure. It reprograms vital signs into the
language space, enabling the LLM to capture temporal context and reason over
missing values through explicit encoding. In contrast, laboratory variables are
embedded either using representative summary values or a learnable [Not
measured] token, depending on their availability. Extensive evaluations on the
benchmark datasets from the PhysioNet demonstrate that VITAL outperforms state
of the art methods designed for irregular time series. Furthermore, it
maintains robust performance under high levels of missingness, which is
prevalent in real world clinical scenarios where key variables are often
unavailable.

</details>


### [408] [Slicing Wasserstein Over Wasserstein Via Functional Optimal Transport](https://arxiv.org/abs/2509.22138)
*Moritz Piening,Robert Beinert*

Main category: cs.LG

TL;DR: The paper introduces a computationally efficient alternative to the Wasserstein-over-Wasserstein (WoW) distance for comparing datasets or distributions. Using the isometry of 1D Wasserstein space, the proposed method avoids numerical instability and reduces computational efforts.


<details>
  <summary>Details</summary>
Motivation: Existing methods for accelerating sliced Wasserstein computations rely on parametric meta-measures and higher-order moments, which can lead to numerical instability and inefficiency. A more robust and scalable approach is needed for handling meta-measures in arbitrary metric spaces.

Method: The authors utilize the isometry between 1D Wasserstein spaces and quantile functions in $L_2([0,1])$ function space. They propose a general sliced Wasserstein framework using infinite-dimensional $L_2$-projections and Gaussian process parametrization. This approach is combined with integration over the Euclidean unit sphere, resulting in the double-sliced Wasserstein (DSW) metric.

Result: The DSW metric is computationally efficient and avoids the numerical instability associated with higher-order moments. Numerical experiments demonstrate that DSW serves as a scalable and effective replacement for WoW distance in datasets, images, and shapes.

Conclusion: The proposed DSW method provides a robust, efficient solution for computing Wasserstein distances between meta-measures, offering improvements in stability and scalability over previous methods.

Abstract: Wasserstein distances define a metric between probability measures on
arbitrary metric spaces, including meta-measures (measures over measures). The
resulting Wasserstein over Wasserstein (WoW) distance is a powerful, but
computationally costly tool for comparing datasets or distributions over images
and shapes. Existing sliced WoW accelerations rely on parametric meta-measures
or the existence of high-order moments, leading to numerical instability. As an
alternative, we propose to leverage the isometry between the 1d Wasserstein
space and the quantile functions in the function space $L_2([0,1])$. For this
purpose, we introduce a general sliced Wasserstein framework for arbitrary
Banach spaces. Due to the 1d Wasserstein isometry, this framework defines a
sliced distance between 1d meta-measures via infinite-dimensional
$L_2$-projections, parametrized by Gaussian processes. Combining this 1d
construction with classical integration over the Euclidean unit sphere yields
the double-sliced Wasserstein (DSW) metric for general meta-measures. We show
that DSW minimization is equivalent to WoW minimization for discretized
meta-measures, while avoiding unstable higher-order moments and computational
savings. Numerical experiments on datasets, shapes, and images validate DSW as
a scalable substitute for the WoW distance.

</details>


### [409] [Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization](https://arxiv.org/abs/2509.22161)
*Takashi Morita*

Main category: cs.LG

TL;DR: The paper addresses limitations in vector quantization and proposes a better method with combined regularization that improves performance.


<details>
  <summary>Details</summary>
Motivation: Vector quantization is widely used but has a critical issue: its non-differentiable nature hinders gradient backpropagation.

Method: The study introduces regularization minimizing the distance between simplex vertices and their closest smoothed quantizers to enhance balanced codebook utilization.

Result: Experiments show better codebook usage efficiency and improved results on tasks like discrete image autoencoding and speech representation learning.

Conclusion: The proposed regularization effectively tackles challenges in vector quantization while achieving stronger benchmark performance.

Abstract: Vector quantization, which discretizes a continuous vector space into a
finite set of representative vectors (a codebook), has been widely adopted in
modern machine learning. Despite its effectiveness, vector quantization poses a
fundamental challenge: the non-differentiable quantization step blocks gradient
backpropagation. Smoothed vector quantization addresses this issue by relaxing
the hard assignment of a codebook vector into a weighted combination of
codebook entries, represented as the matrix product of a simplex vector and the
codebook. Effective smoothing requires two properties: (1) smoothed quantizers
should remain close to a onehot vector, ensuring tight approximation, and (2)
all codebook entries should be utilized, preventing code collapse. Existing
methods typically address these desiderata separately. By contrast, the present
study introduces a simple and intuitive regularization that promotes both
simultaneously by minimizing the distance between each simplex vertex and its
$K$-nearest smoothed quantizers. Experiments on representative benchmarks,
including discrete image autoencoding and contrastive speech representation
learning, demonstrate that the proposed method achieves more reliable codebook
utilization and improves performance compared to prior approaches.

</details>


### [410] [Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs](https://arxiv.org/abs/2509.22166)
*Shirin Alanova,Kristina Kazistova,Ekaterina Galaeva,Alina Kostromina,Vladimir Smirnov,Redko Dmitry,Alexey Dontsov,Maxim Zhelnin,Evgeny Burnaev,Egor Shvetsov*

Main category: cs.LG

TL;DR: This paper investigates activation pruning in large language models (LLMs) as a sparsification technique for efficient inference, showcasing its advantages over weight pruning while proposing hardware-friendly methods and patterns.


<details>
  <summary>Details</summary>
Motivation: The growing demand for efficient inference in LLMs has emphasized the need for sparsification techniques, with activation pruning being underexplored despite its potential for input-adaptive compression and reduced I/O overhead.

Method: Comprehensive analysis of post-training N:M activation pruning across various LLMs, exploring plug-and-play error mitigation techniques, pruning criteria, and evaluating different sparsity patterns for hardware efficiency.

Result: Activation pruning achieves better generative capabilities preservation than weight pruning at similar sparsity levels. The 8:16 sparsity pattern emerges as a strong candidate balancing performance and hardware implementation complexity.

Conclusion: The study establishes practical methods for activation pruning and advocates for future hardware development to support more flexible sparsity patterns, enhancing efficiency in LLM inference.

Abstract: The demand for efficient large language model (LLM) inference has intensified
the focus on sparsification techniques. While semi-structured (N:M) pruning is
well-established for weights, its application to activation pruning remains
underexplored despite its potential for dynamic, input-adaptive compression and
reductions in I/O overhead. This work presents a comprehensive analysis of
methods for post-training N:M activation pruning in LLMs. Across multiple LLMs,
we demonstrate that pruning activations enables superior preservation of
generative capabilities compared to weight pruning at equivalent sparsity
levels. We evaluate lightweight, plug-and-play error mitigation techniques and
pruning criteria, establishing strong hardware-friendly baselines that require
minimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's
standard 2:4, showing that the 16:32 pattern achieves performance nearly on par
with unstructured sparsity. However, considering the trade-off between
flexibility and hardware implementation complexity, we focus on the 8:16
pattern as a superior candidate. Our findings provide both effective practical
methods for activation pruning and a motivation for future hardware to support
more flexible sparsity patterns. Our code is available
https://anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md .

</details>


### [411] [Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead](https://arxiv.org/abs/2509.22174)
*Durgesh Kalwar,Mayank Baranwal,Harshad Khadilkar*

Main category: cs.LG

TL;DR: DYNAWEIGHT is a novel framework for decentralized learning that dynamically assigns weights to neighboring servers based on local data losses, improving training speed and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns and computational efficiency in decentralized infrastructures by introducing an adaptive method for information aggregation.

Method: Introducing DYNAWEIGHT, a dynamic weight allocation framework based on relative loss measures of neighboring servers, which favors diverse information in multi-agent networks.

Result: Empirical results on datasets like MNIST, CIFAR10, and CIFAR100 show significant acceleration in decentralized training across diverse network setups and server counts.

Conclusion: DYNAWEIGHT enhances decentralized learning performance and is compatible with various server-level optimization algorithms, making it a versatile and promising solution for decentralized systems.

Abstract: In today's data-sensitive landscape, distributed learning emerges as a vital
tool, not only fortifying privacy measures but also streamlining computational
operations. This becomes especially crucial within fully decentralized
infrastructures where local processing is imperative due to the absence of
centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to
information aggregation in multi-agent networks. DYNAWEIGHT offers substantial
acceleration in decentralized learning with minimal additional communication
and memory overhead. Unlike traditional static weight assignments, such as
Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring
servers based on their relative losses on local datasets. Consequently, it
favors servers possessing diverse information, particularly in scenarios of
substantial data heterogeneity. Our experiments on various datasets MNIST,
CIFAR10, and CIFAR100 incorporating various server counts and graph topologies,
demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT
functions as an aggregation scheme compatible with any underlying server-level
optimization algorithm, underscoring its versatility and potential for
widespread integration.

</details>


### [412] [Learning Equivariant Functions via Quadratic Forms](https://arxiv.org/abs/2509.22184)
*Pavan Karjol,Vivek V Kashyap,Rohan Kashyap,Prathosh A P*

Main category: cs.LG

TL;DR: The paper introduces a method to learn group-equivariant functions by modeling their quadratic forms through neural networks, showcasing its applicability across tasks like polynomial regression and top quark tagging.


<details>
  <summary>Details</summary>
Motivation: The study aims to uncover group symmetries by leveraging properties of orthogonal groups and integrate them into neural network architectures for more efficient models.

Method: The approach uses the quadratic form $x^T A x$ to encode symmetry groups and incorporates biases via a symmetric matrix, enabling simplified equivariant and invariant model construction.

Result: The proposed models successfully capture group symmetry and demonstrate superior performance in tasks like polynomial regression and physical systems analysis compared to traditional methods.

Conclusion: The framework establishes effective strategies for learning group-equivariant functions, showing promise in tasks requiring symmetry discovery and efficient representation learning.

Abstract: In this study, we introduce a method for learning group (known or unknown)
equivariant functions by learning the associated quadratic form $x^T A x$
corresponding to the group from the data. Certain groups, known as orthogonal
groups, preserve a specific quadratic form, and we leverage this property to
uncover the underlying symmetry group under the assumption that it is
orthogonal. By utilizing the corresponding unique symmetric matrix and its
inherent diagonal form, we incorporate suitable inductive biases into the
neural network architecture, leading to models that are both simplified and
efficient. Our approach results in an invariant model that preserves norms,
while the equivariant model is represented as a product of a norm-invariant
model and a scale-invariant model, where the ``product'' refers to the group
action.
  Moreover, we extend our framework to a more general setting where the
function acts on tuples of input vectors via a diagonal (or product) group
action. In this extension, the equivariant function is decomposed into an
angular component extracted solely from the normalized first vector and a
scale-invariant component that depends on the full Gram matrix of the tuple.
This decomposition captures the inter-dependencies between multiple inputs
while preserving the underlying group symmetry.
  We assess the effectiveness of our framework across multiple tasks, including
polynomial regression, top quark tagging, and moment of inertia matrix
prediction. Comparative analysis with baseline methods demonstrates that our
model consistently excels in both discovering the underlying symmetry and
efficiently learning the corresponding equivariant function.

</details>


### [413] [Kernel Regression of Multi-Way Data via Tensor Trains with Hadamard Overparametrization: The Dynamic Graph Flow Case](https://arxiv.org/abs/2509.22197)
*Duc Thien Nguyen,Konstantinos Slavakis,Eleftherios Kofidis,Dimitris Pados*

Main category: cs.LG

TL;DR: The paper introduces KReTTaH, a regression-based framework utilizing tensor trains and kernel methods for interpreting and imputing multi-way data while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The need to improve interpretability and accuracy in multi-way data imputation, particularly in dynamic graph flows, motivates this study.

Method: The method combines kernel regression with tensor trains of fixed rank on low-dimensional manifolds and incorporates Hadamard overparametrization to ensure parameter efficiency and robustness.

Result: In tests with real-world graph datasets, KReTTaH outperformed state-of-the-art methods for imputing time-varying edge flows, leveraging its integration of graph-based priors and efficient modeling.

Conclusion: KReTTaH proves to be a flexible and efficient framework for interpretable multi-way data imputation, particularly in dynamic graph flows, by outperforming competing approaches.

Abstract: A regression-based framework for interpretable multi-way data imputation,
termed Kernel Regression via Tensor Trains with Hadamard overparametrization
(KReTTaH), is introduced. KReTTaH adopts a nonparametric formulation by casting
imputation as regression via reproducing kernel Hilbert spaces. Parameter
efficiency is achieved through tensors of fixed tensor-train (TT) rank, which
reside on low-dimensional Riemannian manifolds, and is further enhanced via
Hadamard overparametrization, which promotes sparsity within the TT parameter
space. Learning is accomplished by solving a smooth inverse problem posed on
the Riemannian manifold of fixed TT-rank tensors. As a representative
application, the estimation of dynamic graph flows is considered. In this
setting, KReTTaH exhibits flexibility by seamlessly incorporating graph-based
(topological) priors via its inverse problem formulation. Numerical tests on
real-world graph datasets demonstrate that KReTTaH consistently outperforms
state-of-the-art alternatives-including a nonparametric tensor- and a
neural-network-based methods-for imputing missing, time-varying edge flows.

</details>


### [414] [Reversible GNS for Dissipative Fluids with Consistent Bidirectional Dynamics](https://arxiv.org/abs/2509.22207)
*Mu Huang,Linning Xu,Mingyue Dai,Yidi Shao,Bo Dai*

Main category: cs.LG

TL;DR: R-GNS introduces a reversible graph network architecture to unify forward and inverse simulations in dissipative fluid systems, enabling faster and more accurate predictions of physically plausible trajectories.


<details>
  <summary>Details</summary>
Motivation: Simulating fluid dynamics for user-defined goals is complex, especially for dissipative systems where dynamics are irreversible. Current optimization techniques are slow and often fail, prompting the need for an efficient bidirectional framework.

Method: R-GNS proposes a mathematically invertible design using reversible residual message-passing shared parameters, coupling forward dynamics with inverse inference within a graph-based architecture.

Result: Experiments on challenging fluid benchmarks demonstrated R-GNS's higher accuracy, faster inverse inference (100x speedup), reduced parameter count (25%), and elimination of iterative optimization compared to traditional baselines.

Conclusion: R-GNS sets a new benchmark by closely coupling accurate forward simulation with efficient inverse inference, offering a unified solution for physically plausible modeling of dissipative fluid systems.

Abstract: Simulating physically plausible trajectories toward user-defined goals is a
fundamental yet challenging task in fluid dynamics. While particle-based
simulators can efficiently reproduce forward dynamics, inverse inference
remains difficult, especially in dissipative systems where dynamics are
irreversible and optimization-based solvers are slow, unstable, and often fail
to converge. In this work, we introduce the Reversible Graph Network Simulator
(R-GNS), a unified framework that enforces bidirectional consistency within a
single graph architecture. Unlike prior neural simulators that approximate
inverse dynamics by fitting backward data, R-GNS does not attempt to reverse
the underlying physics. Instead, we propose a mathematically invertible design
based on residual reversible message passing with shared parameters, coupling
forward dynamics with inverse inference to deliver accurate predictions and
efficient recovery of plausible initial states. Experiments on three
dissipative benchmarks (Water-3D, WaterRamps, and WaterDrop) show that R-GNS
achieves higher accuracy and consistency with only one quarter of the
parameters, and performs inverse inference more than 100 times faster than
optimization-based baselines. For forward simulation, R-GNS matches the speed
of strong GNS baselines, while in goal-conditioned tasks it eliminates
iterative optimization and achieves orders-of-magnitude speedups. On
goal-conditioned tasks, R-GNS further demonstrates its ability to complex
target shapes (e.g., characters "L" and "N") through vivid, physically
consistent trajectories. To our knowledge, this is the first reversible
framework that unifies forward and inverse simulation for dissipative fluid
systems.

</details>


### [415] [A Law of Data Reconstruction for Random Features (and Beyond)](https://arxiv.org/abs/2509.22214)
*Leonardo Iurada,Simone Bombari,Tatiana Tommasi,Marco Mondelli*

Main category: cs.LG

TL;DR: The paper explores how memorization in large-scale deep learning models can lead to data reconstruction and shows that this is possible when the number of parameters exceeds a threshold depending on data dimensionality and sample size.


<details>
  <summary>Details</summary>
Motivation: To understand memorization in machine learning models and its implications for reconstructing the training data set, focusing on scenarios where the number of parameters in the model is much larger than the training data dimensions.

Method: The study analyzes data reconstruction in the random features model and proposes an optimization method for reconstructing training data from model parameters. The analysis is extended to various architectures like two-layer fully-connected and deep residual networks.

Result: It is demonstrated that the entire training dataset can be effectively reconstructed when the model's parameter count significantly exceeds the threshold proportional to the product of data dimensionality and dataset size.

Conclusion: The work introduces and verifies a law of data reconstruction in deep learning, highlighting how large models with ample parameters can reconstruct training data and offering insights into its theoretical and practical implications.

Abstract: Large-scale deep learning models are known to memorize parts of the training
set. In machine learning theory, memorization is often framed as interpolation
or label fitting, and classical results show that this can be achieved when the
number of parameters $p$ in the model is larger than the number of training
samples $n$. In this work, we consider memorization from the perspective of
data reconstruction, demonstrating that this can be achieved when $p$ is larger
than $dn$, where $d$ is the dimensionality of the data. More specifically, we
show that, in the random features model, when $p \gg dn$, the subspace spanned
by the training samples in feature space gives sufficient information to
identify the individual samples in input space. Our analysis suggests an
optimization method to reconstruct the dataset from the model parameters, and
we demonstrate that this method performs well on various architectures (random
features, two-layer fully-connected and deep residual networks). Our results
reveal a law of data reconstruction, according to which the entire training
dataset can be recovered as $p$ exceeds the threshold $dn$.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [416] [Cycle is All You Need: More Is Different](https://arxiv.org/abs/2509.21340)
*Xin Li*

Main category: cs.NE

TL;DR: This paper proposes a cycle-closure mechanism as the foundation of memory and consciousness, leveraging neural state space cycles to encode meaningful patterns.


<details>
  <summary>Details</summary>
Motivation: Understanding memory and consciousness in terms of fundamental information-topological mechanisms, addressing how biological systems preserve coherence and meaning across contexts.

Method: Cycle closure in neural state space is modeled using polychronous neural groups, theta-gamma rhythms, and sheaf-cosheaf duality to integrate sensory information and action planning hierarchically.

Result: The study demonstrates that cycles stabilize memory by encoding persistent low-entropy invariants and generalize across environments, suggesting a fundamental minimal-energy cognitive model.

Conclusion: Cycle closure provides a unified theoretical framework for memory, cognition, and consciousness, with implications for biological efficiency and systemic coherence.

Abstract: We propose an information-topological framework in which cycle closure is the
fundamental mechanism of memory and consciousness. Memory is not a static store
but the ability to re-enter latent cycles in neural state space, with invariant
cycles serving as carriers of meaning by filtering order-specific noise and
preserving what persists across contexts. The dot-cycle dichotomy captures
this: transient dots scaffold exploration, while nontrivial cycles encode
low-entropy content invariants that stabilize memory. Biologically,
polychronous neural groups realize 1-cycles through delay-locked spiking
reinforced by STDP, nested within theta-gamma rhythms that enforce boundary
cancellation. These micro-cycles compose hierarchically, extending navigation
loops into general memory and cognition. The perception-action cycle introduces
high-order invariance: closure holds even across sense-act alternations,
generalizing ancestral homing behavior. Sheaf-cosheaf duality formalizes this
process: sheaves glue perceptual fragments into global sections, cosheaves
decompose global plans into actions and closure aligns top-down predictions
with bottom-up cycles. Consciousness then arises as the persistence of
high-order invariants that integrate (unity) yet differentiate (richness)
across contexts. We conclude that cycle is all you need: persistent invariants
enable generalization in non-ergodic environments with long-term coherence at
minimal energetic cost.

</details>


### [417] [From Embeddings to Equations: Genetic-Programming Surrogates for Interpretable Transformer Classification](https://arxiv.org/abs/2509.21341)
*Mohammad Sadegh Khorshidi,Navid Yazdanjue,Hassan Gharoun,Mohammad Reza Nikoo,Fang Chen,Amir H. Gandomi*

Main category: cs.NE

TL;DR: This paper explores symbolic surrogate modeling of Transformer embeddings using a genetic programming approach to create compact, interpretable classifiers with calibrated probabilities across five benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: To develop interpretable surrogate models that can offer reliable and explicit explanations for decisions made by frozen Transformer embeddings, while retaining strong classification performance and score calibration.

Method: The authors use a combination of semantic-preserving feature partitioning (SPFP) and cooperative multi-population genetic programming (MEGP) to create compact logit-based classifiers on embedding representations. Models are refined using temperature scaling for probability calibration.

Result: The surrogate models achieve high classification performance (F1 scores up to 0.99 for some datasets) and reduced expected calibration errors (ECE). The paper provides detailed evaluations, including reliability diagrams and global effect profiles.

Conclusion: The proposed approach results in interpretable, cross-modal classifiers that maintain strong discrimination while allowing for auditable and calibrated probability predictions.

Abstract: We study symbolic surrogate modeling of frozen Transformer embeddings to
obtain compact, auditable classifiers with calibrated probabilities. For five
benchmarks (SST2G, 20NG, MNIST, CIFAR10, MSC17), embeddings from ModernBERT,
DINOv2, and SigLIP are partitioned on the training set into disjoint,
information-preserving views via semantic-preserving feature partitioning
(SPFP). A cooperative multi-population genetic program (MEGP) then learns
additive, closed-form logit programs over these views. Across 30 runs per
dataset we report F1, AUC, log-loss, Brier, expected calibration error (ECE),
and symbolic complexity; a canonical model is chosen by a one-standard-error
rule on validation F1 with a parsimony tie-break. Temperature scaling fitted on
validation yields substantial ECE reductions on test. The resulting surrogates
achieve strong discrimination (up to F1 around 0.99 on MNIST, CIFAR10, MSC17;
around 0.95 on SST2G), while 20NG remains most challenging. We provide
reliability diagrams, dimension usage and overlap statistics,
contribution-based importances, and global effect profiles (PDP and ALE),
demonstrating faithful, cross-modal explanations grounded in explicit programs.

</details>


### [418] [SGNNBench: A Holistic Evaluation of Spiking Graph Neural Network on Large-scale Graph](https://arxiv.org/abs/2509.21342)
*Huizhe Zhang,Jintang Li,Yuchang Zhu,Liang Chen,Li Kuang*

Main category: cs.NE

TL;DR: This paper introduces SGNNBench, a benchmarking framework for Spiking Graph Neural Networks (SGNNs) that evaluates their effectiveness, energy efficiency, and architectural design using 9 SGNN models across 18 datasets.


<details>
  <summary>Details</summary>
Motivation: To address the computational and energy inefficiencies of Graph Neural Networks (GNNs) on large-scale graphs, and to provide a systematic benchmarking tool for assessing Spiking Graph Neural Networks (SGNNs).

Method: The authors developed SGNNBench to evaluate SGNNs on metrics like model size, memory usage, and theoretical energy consumption, while also investigating their architectural design.

Result: SGNNBench provides a comprehensive comparison of 9 SGNN models across 18 datasets, highlighting the energy bottlenecks and design principles.

Conclusion: SGNNBench bridges the gap in evaluating SGNNs and facilitates the development of energy-efficient and systematically designed SGNNs.

Abstract: Graph Neural Networks (GNNs) are exemplary deep models designed for graph
data. Message passing mechanism enables GNNs to effectively capture graph
topology and push the performance boundaries across various graph tasks.
However, the trend of developing such complex machinery for graph
representation learning has become unsustainable on large-scale graphs. The
computational and time overhead make it imperative to develop more
energy-efficient GNNs to cope with the explosive growth of real-world graphs.
Spiking Graph Neural Networks (SGNNs), which integrate biologically plausible
learning via unique spike-based neurons, have emerged as a promising
energy-efficient alternative. Different layers communicate with sparse and
binary spikes, which facilitates computation and storage of intermediate graph
representations. Despite the proliferation of SGNNs proposed in recent years,
there is no systematic benchmark to explore the basic design principles of
these brain-inspired networks on the graph data. To bridge this gap, we present
SGNNBench to quantify progress in the field of SGNNs. Specifically, SGNNBench
conducts an in-depth investigation of SGNNs from multiple perspectives,
including effectiveness, energy efficiency, and architectural design. We
comprehensively evaluate 9 state-of-the-art SGNNs across 18 datasets. Regarding
efficiency, we empirically compare these baselines w.r.t model size, memory
usage, and theoretical energy consumption to reveal the often-overlooked energy
bottlenecks of SGNNs. Besides, we elaborately investigate the design space of
SGNNs to promote the development of a general SGNN paradigm.

</details>


### [419] [Neuromorphic Deployment of Spiking Neural Networks for Cognitive Load Classification in Air Traffic Control](https://arxiv.org/abs/2509.21345)
*Jiahui An,Chonghao Cai,Olympia Gallou,Sara Irina Fabrikant,Giacomo Indiveri,Elisa Donati*

Main category: cs.NE

TL;DR: The study develops a neuromorphic system using Spiking Neural Networks (SNNs) for cognitive load classification in Air Traffic Control tasks, employing EEG and eye-tracking data, and successfully deploying the model on low-power neuromorphic hardware.


<details>
  <summary>Details</summary>
Motivation: To design an ultra-low-power, hardware-deployable system for cognitive load monitoring in real-world dynamic settings using neuromorphic approaches.

Method: The study utilized EEG and eye-tracking features to train conventional machine learning models and SNNs. A minimalistic single-layer SNN model trained with a biologically inspired delta-rule learning algorithm was quantized and implemented on the DYNAP-SE neuromorphic hardware.

Result: The single-layer SNN achieved competitive cognitive load classification accuracy (80.6%). When deployed on the DYNAP-SE chip, the model maintained 73.5% accuracy despite hardware constraints.

Conclusion: The research successfully demonstrates the viability of event-driven neuromorphic systems for cognitive state monitoring, enabling ultra-low-power operation in real-world dynamic environments.

Abstract: This paper presents a neuromorphic system for cognitive load classification
in a real-world setting, an Air Traffic Control (ATC) task, using a hardware
implementation of Spiking Neural Networks (SNNs). Electroencephalogram (EEG)
and eye-tracking features, extracted from an open-source dataset, were used to
train and evaluate both conventional machine learning models and SNNs. Among
the SNN architectures explored, a minimalistic, single-layer model trained with
a biologically inspired delta-rule learning algorithm achieved competitive
performance (80.6%). To enable deployment on neuromorphic hardware, the model
was quantized and implemented on the mixed-signal DYNAP-SE chip. Despite
hardware constraints and analog variability, the chip-deployed SNN maintained a
classification accuracy of up to 73.5% using spike-based input. These results
demonstrate the feasibility of event-driven neuromorphic systems for
ultra-low-power, embedded cognitive state monitoring in dynamic real-world
scenarios.

</details>


### [420] [Spiking Neural Networks for Mental Workload Classification with a Multimodal Approach](https://arxiv.org/abs/2509.21346)
*Jiahui An,Sara Irina Fabrikant,Giacomo Indiveri,Elisa Donati*

Main category: cs.NE

TL;DR: This paper investigates using spiking neural networks (SNNs) for real-time, low-power mental workload monitoring, comparing their performance with traditional machine learning models.


<details>
  <summary>Details</summary>
Motivation: To address the need for real-time and low-power workload monitoring as traditional EEG-based ML models are computationally expensive.

Method: The study compares SNN models, which are hardware-efficient, to various traditional ML models using an open-source multimodal EEG dataset.

Result: The comparison shows that multimodal integration enhances accuracy, and SNNs perform comparably to traditional ML models.

Conclusion: SNNs demonstrate great potential for real-time, energy-efficient cognitive load monitoring on adaptive embedded systems.

Abstract: Accurately assessing mental workload is crucial in cognitive neuroscience,
human-computer interaction, and real-time monitoring, as cognitive load
fluctuations affect performance and decision-making. While
Electroencephalography (EEG) based machine learning (ML) models can be used to
this end, their high computational cost hinders embedded real-time
applications. Hardware implementations of spiking neural networks (SNNs) offer
a promising alternative for low-power, fast, event-driven processing. This
study compares hardware compatible SNN models with various traditional ML ones,
using an open-source multimodal dataset. Our results show that multimodal
integration improves accuracy, with SNN performance comparable to the ML one,
demonstrating their potential for real-time implementations of cognitive load
detection. These findings position event-based processing as a promising
solution for low-latency, energy efficient workload monitoring in adaptive
closed-loop embedded devices that dynamically regulate cognitive load.

</details>


### [421] [Domain-Informed Genetic Superposition Programming: A Case Study on SFRC Beams](https://arxiv.org/abs/2509.21355)
*Mohammad Sadegh Khorshidi,Navid Yazdanjue,Hassan Gharoun,Mohammad Reza Nikoo,Fang Chen,Amir H. Gandomi*

Main category: cs.NE

TL;DR: The study introduces DIGSP, a framework using domain-informed genetic programming to model separable physical mechanisms in engineering systems. Applied to SFRC beam data, it demonstrated better performance than baseline models in terms of error metrics.


<details>
  <summary>Details</summary>
Motivation: To enhance modeling accuracy and interpretability for engineering systems governed by separable physical mechanisms by leveraging domain-informed symbolic regression.

Method: DIGSP involves partitioning input space into domain-specific features, evolving independent genetic programming populations, and introducing an adaptive symbolic abstraction mechanism (AHSAM) after stagnation. AHSAM uses ANOVA filtering to refine symbolic constructs injected across populations.

Result: DIGSP outperformed a baseline model in training and testing RMSE, showcased tighter error distributions, fewer outliers, and achieved statistical significance (p < 0.01) despite no observed validation RMSE difference.

Conclusion: Domain-informed structural decomposition and symbolic abstraction improve convergence and generalization, offering an interpretable and effective modeling strategy for systems with symbolic alignments.

Abstract: This study presents domain-informed genetic superposition programming
(DIGSP), a symbolic regression framework tailored for engineering systems
governed by separable physical mechanisms. DIGSP partitions the input space
into domain-specific feature subsets and evolves independent genetic
programming (GP) populations to model material-specific effects. Early
evolution occurs in isolation, while ensemble fitness promotes inter-population
cooperation. To enable symbolic superposition, an adaptive hierarchical
symbolic abstraction mechanism (AHSAM) is triggered after stagnation across all
populations. AHSAM performs analysis of variance- (ANOVA) based filtering to
identify statistically significant individuals, compresses them into symbolic
constructs, and injects them into all populations through a validation-guided
pruning cycle. The DIGSP is benchmarked against a baseline multi-gene genetic
programming (BGP) model using a dataset of steel fiber-reinforced concrete
(SFRC) beams. Across 30 independent trials with 65% training, 10% validation,
and 25% testing splits, DIGSP consistently outperformed BGP in training and
test root mean squared error (RMSE). The Wilcoxon rank-sum test confirmed
statistical significance (p < 0.01), and DIGSP showed tighter error
distributions and fewer outliers. No significant difference was observed in
validation RMSE due to limited sample size. These results demonstrate that
domain-informed structural decomposition and symbolic abstraction improve
convergence and generalization. DIGSP offers a principled and interpretable
modeling strategy for systems where symbolic superposition aligns with the
underlying physical structure.

</details>


### [422] [Smart Routing for EV Charge Point Operators in Mega Cities: Case Study of Istanbul](https://arxiv.org/abs/2509.21369)
*Onur Yenigun,Gozde Karatas Baydogmus,Kazim Yildiz*

Main category: cs.NE

TL;DR: This study proposes an optimized EV charging network maintenance planning method using K-means and genetic algorithms, achieving up to 35% efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of EVs requires sustainable management of charging infrastructure, specifically addressing logistical challenges in metropolitan cities.

Method: An integrated approach combining K-means clustering and genetic algorithms for geographic grouping and route optimization of EV charging stations.

Result: Testing on Istanbul's dataset revealed significant potential, with 35% distance savings in efficient scenarios compared to sequential planning.

Conclusion: The method enhances operational efficiency for EV charging networks in densely populated cities and has scalability potential, with plans for future integration of real-time factors.

Abstract: The rapidly increasing use of electric vehicles (EVs) has made it even more
important to manage the charging infrastructure sustainably. The expansion of
charging station networks, especially in large cities, creates serious
logistical challenges for charging point operators (CPOs) in planning
maintenance and repair activities. Inefficient field personnel management can
lead to time loss, high operational costs, and resource waste. This study
presents an integrated method to optimize the planning of EV charging network
maintenance operations. The proposed approach groups charging stations
according to geographical proximity using the K-means clustering algorithm and
calculates the shortest routes between clusters using a genetic algorithm. The
method was developed in Python and applied to a dataset consisting of 100 EV
charging stations in Istanbul.
  Considering the population density, traffic density, and resource constraints
of Istanbul, the route planning approach presented in this study has great
potential, especially for such metropolises. According to the different
parameter configurations tested, the most efficient scenario provided
approximately 35\% distance savings compared to the reference route created
according to the sequential data layout. While the reference route provides a
simple comparison, the study presents a solution that will enable field
operations in metropolitan cities such as Istanbul to be conducted in a more
efficient, planned and scalable manner. In future studies, it is planned to
integrate real-time factors such as traffic conditions and field technician
constraints.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [423] [SAHM: State-Aware Heterogeneous Multicore for Single-Thread Performance](https://arxiv.org/abs/2509.22405)
*Shayne Wadle,Karthikeyan Sankaralingam*

Main category: cs.PF

TL;DR: SAHM introduces a novel multicore design to improve single-thread performance by exploiting time-varying behavioral diversity via specialized cores optimized for specific states.


<details>
  <summary>Details</summary>
Motivation: Single-thread performance improvements face diminishing returns with traditional designs like deeper speculation and complex out-of-order execution.

Method: SAHM identifies 16 distinct behavioral states through performance counter analysis and utilizes specialized cores for these states, migrating threads dynamically based on workload behavior.

Result: In experiments, SAHM achieved a 17% performance speed-up in realistic scenarios, with negligible performance degradation (<1%) from migration costs.

Conclusion: State-aware core specialization provides a viable and efficient method to boost single-thread performance without excessive area, power, or complexity increases.

Abstract: Improving single-thread performance remains a critical challenge in modern
processor design, as conventional approaches such as deeper speculation, wider
pipelines, and complex out-of-order execution face diminishing returns. This
work introduces SAHM-State-Aware Heterogeneous Multicore-a novel architecture
that targets performance gains by exploiting fine-grained, time-varying
behavioral diversity in single-threaded workloads. Through empirical
characterization of performance counter data, we define 16 distinct behavioral
states representing different microarchitectural demands. Rather than
over-provisioning a monolithic core with all optimizations, SAHM uses a set of
specialized cores tailored to specific states and migrates threads at runtime
based on detected behavior. This design enables composable microarchitectural
enhancements without incurring prohibitive area, power, or complexity costs.
  We evaluate SAHM in both single-threaded and multiprogrammed scenarios,
demonstrating its ability to maintain core utilization while improving overall
performance through intelligent state-driven scheduling. Experimental results
show opportunity for 17% speed up in realistic scenarios. These speed ups are
robust against high-cost migration, decreasing by less than 1%. Overall,
state-aware core specialization is a new path forward for enhancing
single-thread performance.

</details>


### [424] [Size-Aware Dispatching to Fluid Queues](https://arxiv.org/abs/2509.21693)
*Runhan Xie,Esa HyytiÃ¤,Rhonda Righter*

Main category: cs.PF

TL;DR: This paper introduces a fluid-flow model to optimize routing of particles of varying sizes to parallel servers, minimizing mean latency, with theoretical analysis and numerical examples.


<details>
  <summary>Details</summary>
Motivation: Existing routing problems for incoming tasks or particles often lack efficient models to minimize system-wide mean latency, especially for fluid-flow scenarios with different particle sizes.

Method: A fluid-flow model was developed, where calculus of variation was applied to characterize optimal policies for routing incoming fluid.

Result: Numerical examples were used to validate and provide insights into the structure of optimal routing policies in large systems.

Conclusion: The proposed model and methodology help optimize large-scale distributed service systems by identifying and applying optimal routing controls.

Abstract: We develop a fluid-flow model for routing problems, where fluid consists of
different size particles and the task is to route the incoming fluid to $n$
parallel servers using the size information in order to minimize the mean
latency. The problem corresponds to the dispatching problem of (discrete) jobs
arriving according to a stochastic process. In the fluid model the problem
reduces to finding an optimal path to empty the system in $n$-dimensional
space. We use the calculus of variation to characterize the structure of
optimal policies. Numerical examples shed further light on the fluid routing
problem and the optimal control of large distributed service systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [425] [InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?](https://arxiv.org/abs/2509.21629)
*Anjiang Wei,Tarun Suresh,Tianran Sun,Haoze Wu,Ke Wang,Alex Aiken*

Main category: cs.PL

TL;DR: This paper evaluates LLM capabilities on invariant synthesis, essential for program verification. It uses a framework assessing correctness and speedup compared to traditional solvers. Although promising, LLMs currently fall short of traditional solvers like UAutomizer.


<details>
  <summary>Details</summary>
Motivation: Automatically discovering strong loop invariants for program verification is a long-standing challenge. The paper aims to evaluate large language models (LLMs) to understand their potential in invariant synthesis tasks.

Method: The study introduces a decision procedure grounded in verifier-based formal soundness guarantees. It compares 7 state-of-the-art LLMs and existing LLM-based verifiers against a traditional solver (UAutomizer) and examines performance using supervised fine-tuning and Best-of-N sampling.

Result: LLMs do not outperform UAutomizer yet. Performance variation is observed across models. Fine-tuning and Best-of-N sampling significantly enhance results for certain LLMs, such as Qwen3-Coder-480B and Claude-sonnet-4.

Conclusion: LLMs show promise in invariant synthesis but still lag behind traditional solvers. Fine-tuning and sampling methods boost potential, leaving the domain as an open challenge for future advancements in LLMs.

Abstract: Program verification relies on loop invariants, yet automatically discovering
strong invariants remains a long-standing challenge. We introduce a principled
framework for evaluating LLMs on invariant synthesis. Our approach uses a
verifier-based decision procedure with a formal soundness guarantee and
assesses not only correctness but also the speedup that invariants provide in
verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based
verifiers against the traditional solver UAutomizer. While LLM-based verifiers
represent a promising direction, they do not yet offer a significant advantage
over UAutomizer. Model capability also proves critical, as shown by sharp
differences in speedups across models, and our benchmark remains an open
challenge for current LLMs. Finally, we show that supervised fine-tuning and
Best-of-N sampling can improve performance: fine-tuning on 3589 instances
raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,
and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.

</details>


### [426] [Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics](https://arxiv.org/abs/2509.21793)
*Jianhong Zhao,Everett Hildenbrandt,Juan Conejero,Yongwang Zhao*

Main category: cs.PL

TL;DR: The paper presents a new paradigm, compiling by proving, which converts verification proofs into optimized execution rules, leading to significant performance gains via semantic rewrites.


<details>
  <summary>Details</summary>
Motivation: Verification proofs are typically discarded after proving program correctness. This research aims to utilize these proofs for generating optimized execution rules, enhancing performance.

Method: The study involves constructing All-Path Reachability Proofs through symbolic execution and compiling their graph structures into single rules while ensuring correctness. The approach is implemented as an extension to the K framework.

Result: Evaluation shows consistent performance improvements at the opcode level and massive performance gains for whole-program compilation.

Conclusion: Compiling by proving consolidates semantic rewrites into optimized rules, ensuring correctness and delivering substantial execution performance enhancements.

Abstract: Verification proofs encode complete program behavior, yet we discard them
after checking correctness. We present compiling by proving, a paradigm that
transforms these proofs into optimized execution rules. By constructing
All-Path Reachability Proofs through symbolic execution and compiling their
graph structure, we consolidate many semantic rewrites into single rules while
preserving correctness by construction. We implement this as a
language-agnostic extension to the K framework. Evaluation demonstrates
performance improvements across different compilation scopes: opcode-level
optimizations show consistent speedups, while whole-program compilation
achieves orders of magnitude greater performance gains.

</details>


### [427] [Committing to the bit: Relational programming with semiring arrays and SAT solving](https://arxiv.org/abs/2509.22614)
*Dmitri Volkov,Yafei Yang,Chung-chieh Shan*

Main category: cs.PL

TL;DR: The paper introduces semiringKanren, a new relational programming language utilizing semiring arrays, which can be compiled for efficient execution, including leveraging SAT solvers.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and generality of relational programming by introducing a semiring-based approach.

Method: The authors formalized a type system, parameterized semantics, and implemented a compilation targeting bitstring representations for performance gains.

Result: SemiringKanren outperformed miniKanren in solving Sudoku puzzles, demonstrating its practical efficiency.

Conclusion: SemiringKanren is a potentially more efficient variant of miniKanren, offering enhanced performance for certain relational programming tasks.

Abstract: We propose semiringKanren, a relational programming language where each
relation expression denotes a semiring array. We formalize a type system that
restricts the arrays to finite size. We then define a semantics that is
parameterized by the semiring that the arrays draw their elements from. We
compile semiringKanren types to bitstring representations. For the Boolean
semiring, this compilation enables us to use an SAT solver to run
semiringKanren programs efficiently. We compare the performance of
semiringKanren and faster miniKanren for solving Sudoku puzzles. Our experiment
shows that semiringKanren can be a more efficient variant of miniKanren.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [428] [Language-in-the-Loop Culvert Inspection on the Erie Canal](https://arxiv.org/abs/2509.21370)
*Yashom Dighe,Yash Turkar,Karthik Dantu*

Main category: cs.RO

TL;DR: The paper presents an autonomous system called VISION that combines vision-language models and viewpoint planning to inspect culverts autonomously, addressing challenges like limited access and poor illumination.


<details>
  <summary>Details</summary>
Motivation: To address the limitations and difficulties in human inspection of aging canal culverts, such as poor access, geometry, and lighting.

Method: The VISION system integrates a vision-language model (VLM) for suggesting regions of interest and rationales, with stereo depth estimation and a planner that adapts to culvert constraints for autonomy in inspection.

Result: When tested in an Erie Canal culvert, VISION achieved 61.4% initial agreement with expert assessments, improving to 80% after deploying its re-imaging loop.

Conclusion: The system reliably converts hypotheses into expert-aligned findings without requiring fine-tuning, demonstrating its effectiveness for autonomous culvert inspections.

Abstract: Culverts on canals such as the Erie Canal, built originally in 1825, require
frequent inspections to ensure safe operation. Human inspection of culverts is
challenging due to age, geometry, poor illumination, weather, and lack of easy
access. We introduce VISION, an end-to-end, language-in-the-loop autonomy
system that couples a web-scale vision-language model (VLM) with constrained
viewpoint planning for autonomous inspection of culverts. Brief prompts to the
VLM solicit open-vocabulary ROI proposals with rationales and confidences,
stereo depth is fused to recover scale, and a planner -- aware of culvert
constraints -- commands repositioning moves to capture targeted close-ups.
Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the
see, decide, move, re-image loop on-board and produces high-resolution images
for detailed reporting without domain-specific fine-tuning. In an external
evaluation by New York Canal Corporation personnel, initial ROI proposals
achieved 61.4\% agreement with subject-matter experts, and final
post-re-imaging assessments reached 80\%, indicating that VISION converts
tentative hypotheses into grounded, expert-aligned findings.

</details>


### [429] [Developing a Mono-Actuated Compliant GeoGami Robot](https://arxiv.org/abs/2509.21445)
*Archie Webster,Lee Skull,Seyed Amir Tafrishi*

Main category: cs.RO

TL;DR: The paper introduces GeoGami, a mono-actuated soft-rigid robot leveraging origami design for shape transformation and locomotion using compliance and minimal actuation.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the complexity of actuating high-degrees-of-freedom origami surfaces for robots, enabling shape contraction and motion with minimal actuators and enhanced repeatability.

Method: The proposed GeoGami integrates origami surface compliance with a geometric compliant skeleton and utilizes a single actuator for transformation and motion. It includes stiffness modeling, the design of a gearbox mechanism, and exploration of alternative cable-driven actuation techniques.

Result: The GeoGami platform demonstrates effective shape transformation and rolling locomotion, showcasing novel capabilities in adapting to different environments using minimal actuation.

Conclusion: The paper concludes that GeoGami expands the potential for robots that rely on shape transformation for mobility and environmental adaptability, offering a promising approach to soft-rigid robotic design.

Abstract: This paper presents the design of a new soft-rigid robotic platform,
"GeoGami". We leverage origami surface capabilities to achieve shape
contraction and to support locomotion with underactuated forms. A key challenge
is that origami surfaces have high degrees of freedom and typically require
many actuators; we address repeatability by integrating surface compliance. We
propose a mono-actuated GeoGami mobile platform that combines origami surface
compliance with a geometric compliant skeleton, enabling the robot to transform
and locomote using a single actuator. We demonstrate the robot, develop a
stiffness model, and describe the central gearbox mechanism. We also analyze
alternative cable-driven actuation methods for the skeleton to enable surface
transformation. Finally, we evaluate the GeoGami platform for capabilities,
including shape transformation and rolling. This platform opens new
capabilities for robots that change shape to access different environments and
that use shape transformation for locomotion.

</details>


### [430] [Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation](https://arxiv.org/abs/2509.21496)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yingming Chen,Cheuk Chi Tsang*

Main category: cs.RO

TL;DR: The paper addresses quadrotor challenges during near-wall operations by proposing a physics-based suction force model and suction-compensated model predictive control (SC-MPC) framework. Experimental results highlight SC-MPC's superior trajectory tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the safety and stability of quadrotors operating in wall-proximity environments, where unmodeled aerodynamic effects can result in destabilizing forces and potential accidents.

Method: The paper introduces a suction force model based on rotor speed and wall distance, integrated into a suction-compensated model predictive control (SC-MPC) framework. The methodology includes factor graph optimization and systematic parameter identification through experiments.

Result: Experimental validation shows SC-MPC achieves substantial accuracy improvements over PID and standard MPC control, with 74%-79% improvement in RMSE and notable reductions in mean absolute error.

Conclusion: SC-MPC ensures stable and accurate operations in near-wall environments, significantly outperforming existing methods. The implementation is open-sourced to promote adoption and reproducibility.

Abstract: The safe operation of quadrotors in near-wall urban or indoor environments
(e.g., inspection and search-and-rescue missions) is challenged by unmodeled
aerodynamic effects arising from wall-proximity. It generates complex vortices
that induce destabilizing suction forces, potentially leading to hazardous
vibrations or collisions. This paper presents a comprehensive solution
featuring (1) a physics-based suction force model that explicitly characterizes
the dependency on both rotor speed and wall distance, and (2) a
suction-compensated model predictive control (SC-MPC) framework designed to
ensure accurate and stable trajectory tracking during wall-proximity
operations. The proposed SC-MPC framework incorporates an enhanced dynamics
model that accounts for suction force effects, formulated as a factor graph
optimization problem integrating system dynamics constraints, trajectory
tracking objectives, control input smoothness requirements, and actuator
physical limitations. The suction force model parameters are systematically
identified through extensive experimental measurements across varying
operational conditions. Experimental validation demonstrates SC-MPC's superior
performance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0
cm RMSE in Y-axis position control - representing 74% and 79% improvements over
cascaded proportional-integral-derivative (PID) control, and 60% and 53%
improvements over standard MPC respectively. The corresponding mean absolute
error (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both
baselines. The evaluation platform employs a ducted quadrotor design that
provides collision protection while maintaining aerodynamic efficiency. To
facilitate reproducibility and community adoption, we have open-sourced our
complete implementation, available at
https://anonymous.4open.science/r/SC-MPC-6A61.

</details>


### [431] [DroneFL: Federated Learning for Multi-UAV Visual Target Tracking](https://arxiv.org/abs/2509.21523)
*Xiaofan Yu,Yuwei Wu,Katherine Mao,Ye Tian,Vijay Kumar,Tajana Rosing*

Main category: cs.RO

TL;DR: DroneFL is a federated learning framework tailored for multi-UAV target tracking, achieving significant improvements in prediction accuracy and tracking performance while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in multi-robot target tracking, especially in multi-UAV systems, where data heterogeneity, computational constraints, and trajectory planning need to be effectively managed.

Method: DroneFL employs a lightweight target trajectory prediction model using a YOLO backbone and shallow transformer, introduces altitude-based adaptive normalization for reducing data heterogeneity, and utilizes cloud aggregation for global model sharing and trajectory optimization.

Result: DroneFL improves prediction accuracy by 6%-83% and tracking efficiency by 0.4%-4.6%, operating in real-time on Raspberry Pi 5 with minimal data transmission to the cloud.

Conclusion: DroneFL proves effective in overcoming key challenges in multi-UAV tracking, demonstrating robust performance improvements and computational efficiency for real-world applications.

Abstract: Multi-robot target tracking is a fundamental problem that requires
coordinated monitoring of dynamic entities in applications such as precision
agriculture, environmental monitoring, disaster response, and security
surveillance. While Federated Learning (FL) has the potential to enhance
learning across multiple robots without centralized data aggregation, its use
in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely
underexplored. Key challenges include limited onboard computational resources,
significant data heterogeneity in FL due to varying targets and the fields of
view, and the need for tight coupling between trajectory prediction and
multi-robot planning. In this paper, we introduce DroneFL, the first federated
learning framework specifically designed for efficient multi-UAV target
tracking. We design a lightweight local model to predict target trajectories
from sensor inputs, using a frozen YOLO backbone and a shallow transformer for
efficient onboard training. The updated models are periodically aggregated in
the cloud for global knowledge sharing. To alleviate the data heterogeneity
that hinders FL convergence, DroneFL introduces a position-invariant model
architecture with altitude-based adaptive instance normalization. Finally, we
fuse predictions from multiple UAVs in the cloud and generate optimal
trajectories that balance target prediction accuracy and overall tracking
performance. Our results show that DroneFL reduces prediction error by 6%-83%
and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.
In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has
on average just 1.56 KBps data rate to the cloud.

</details>


### [432] [Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation](https://arxiv.org/abs/2509.21543)
*Jinbang Huang,Zhiyuan Li,Zhanguang Zhang,Xingyue Quan,Jianye Hao,Yingxue Zhang*

Main category: cs.RO

TL;DR: The paper introduces Plan2Evolve, a framework that uses large language models (LLMs) to self-generate symbolic planning data for improving reasoning and task planning capabilities, while lowering dependence on costly supervised datasets.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve robotic task planning and reasoning in LLMs without relying on expensive, human-curated datasets.

Method: Plan2Evolve utilizes a self-evolving framework where the LLM generates symbolic problem-plan pairs, converts them into chain-of-thought (CoT) reasoning trajectories, and fine-tunes itself for enhanced capabilities.

Result: The fine-tuned LLM demonstrates improved planning success, better generalization to diverse tasks, and reduced costs during inference.

Conclusion: Plan2Evolve has the potential to significantly advance scalable reasoning and planning in robotics using LLMs while minimizing the reliance on human-curated datasets.

Abstract: Large Language Models (LLMs) have recently shown strong potential in robotic
task planning, particularly through automatic planning domain generation that
integrates symbolic search. Prior approaches, however, have largely treated
these domains as search utilities, with limited attention to their potential as
scalable sources of reasoning data. At the same time, progress in reasoning
LLMs has been driven by chain-of-thought (CoT) supervision, whose application
in robotics remains dependent on costly, human-curated datasets. We propose
Plan2Evolve, an LLM self-evolving framework in which the base model generates
planning domains that serve as engines for producing symbolic problem-plan
pairs as reasoning traces. These pairs are then transformed into extended CoT
trajectories by the same model through natural-language explanations, thereby
explicitly aligning symbolic planning structures with natural language
reasoning. The resulting data extend beyond the model's intrinsic planning
capacity, enabling model fine-tuning that yields a planning-enhanced LLM with
improved planning success, stronger cross-task generalization, and reduced
inference costs.

</details>


### [433] [PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines](https://arxiv.org/abs/2509.21563)
*Zhixin Zhang,Liang Zhao,Pawel Ladosz*

Main category: cs.RO

TL;DR: PL-VIWO2 is a visual-inertial-wheel odometry system combining cameras, IMU, and wheel encoders to enhance robust state estimation for autonomous driving in urban environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional vision-based odometry in complex urban environments, ensuring long-term robust and accurate state estimation for autonomous driving.

Method: PL-VIWO2 integrates IMU, wheel encoder, and camera data using three key contributions: geometric line feature processing, SE(2)-constrained wheel pre-integration, and an efficient motion consistency check to filter dynamic features.

Result: PL-VIWO2 demonstrates superior accuracy, efficiency, and robustness in experiments conducted on simulations and public autonomous driving datasets, outperforming existing methods.

Conclusion: The proposed PL-VIWO2 system effectively tackles the challenges of vision-based odometry in urban setups, offering advanced methodologies for state estimation in autonomous driving.

Abstract: Vision-based odometry has been widely adopted in autonomous driving owing to
its low cost and lightweight setup; however, its performance often degrades in
complex outdoor urban environments. To address these challenges, we propose
PL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates
an IMU, wheel encoder, and camera (supporting both monocular and stereo) for
long-term robust state estimation. The main contributions are: (i) a novel line
feature processing framework that exploits the geometric relationship between
2D feature points and lines, enabling fast and robust line tracking and
triangulation while ensuring real-time performance; (ii) an SE(2)-constrained
SE(3) wheel pre-integration method that leverages the planar motion
characteristics of ground vehicles for accurate wheel updates; and (iii) an
efficient motion consistency check (MCC) that filters out dynamic features by
jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo
simulations and public autonomous driving datasets demonstrate that PL-VIWO2
outperforms state-of-the-art methods in terms of accuracy, efficiency, and
robustness.

</details>


### [434] [Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control](https://arxiv.org/abs/2509.21571)
*HaoZhe Xu,Cheng Cheng,HongRui Sang,Zhipeng Wang,Qiyong He,Xiuxian Li,Bin He*

Main category: cs.RO

TL;DR: The paper introduces a novel autonomous docking framework for UAVs and quadruped robots in GPS-denied environments, overcoming challenges posed by quadruped robots' instability.


<details>
  <summary>Details</summary>
Motivation: Existing UAV docking solutions are mainly for wheeled platforms, which are not suitable for navigating complex terrains. Quadruped robots offer better adaptability but create instability challenges for UAV docking.

Method: The framework combines a Hybrid Internal Model with Horizontal Alignment (HIM-HA) for quadruped platform stabilization and a three-phase UAV docking strategy focusing on acquisition, tracking, and terminal descent.

Result: The proposed method demonstrated successful docking in simulations and real-world tests, including challenging conditions like outdoor staircases (height > 17 cm) and rough slopes (steeper than 30 degrees).

Conclusion: This work establishes an effective solution for UAV-quadruped docking in complex terrains, advancing multi-vehicle cooperation in unstructured environments.

Abstract: Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots
is essential for heterogeneous systems, yet most existing approaches target
wheeled platforms whose limited mobility constrains exploration in complex
terrains. Quadruped robots offer superior adaptability but undergo frequent
posture variations, making it difficult to provide a stable landing surface for
UAVs. To address these challenges, we propose an autonomous UAV-quadruped
docking framework for GPS-denied environments. On the quadruped side, a Hybrid
Internal Model with Horizontal Alignment (HIM-HA), learned via deep
reinforcement learning, actively stabilizes the torso to provide a level
platform. On the UAV side, a three-phase strategy is adopted, consisting of
long-range acquisition with a median-filtered YOLOv8 detector, close-range
tracking with a constraint-aware controller that integrates a Nonsingular Fast
Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function
(BF) to guarantee finite-time error convergence under field-of-view (FOV)
constraints, and terminal descent guided by a Safety Period (SP) mechanism that
jointly verifies tracking accuracy and platform stability. The proposed
framework is validated in both simulation and real-world scenarios,
successfully achieving docking on outdoor staircases higher than 17 cm and
rough slopes steeper than 30 degrees. Supplementary materials and videos are
available at: https://uav-quadruped-docking.github.io.

</details>


### [435] [Real-Time Indoor Object SLAM with LLM-Enhanced Priors](https://arxiv.org/abs/2509.21602)
*Yang Jiao,Yiding Qiu,Henrik I. Christensen*

Main category: cs.RO

TL;DR: The paper uses large language models to enhance Object-level SLAM by providing commonsense priors, achieving improved mapping accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in Object-level SLAM caused by under-constrained optimization due to sparse observations and the labor-intensive acquisition of commonsense priors.

Method: Incorporating commonsense priors about object size and orientation from large language models into a graph-based SLAM framework and implementing a pipeline for real-time performance.

Result: The proposed system enhances mapping accuracy by 36.8% compared to the baseline and demonstrates real-time performance on datasets like TUM RGB-D and 3RScan.

Conclusion: Leveraging large language models for commonsense geometric priors improves Object-level SLAM in terms of accuracy and real-time feasibility, addressing prior challenges in the field.

Abstract: Object-level Simultaneous Localization and Mapping (SLAM), which incorporates
semantic information for high-level scene understanding, faces challenges of
under-constrained optimization due to sparse observations. Prior work has
introduced additional constraints using commonsense knowledge, but obtaining
such priors has traditionally been labor-intensive and lacks generalizability
across diverse object categories. We address this limitation by leveraging
large language models (LLMs) to provide commonsense knowledge of object
geometric attributes, specifically size and orientation, as prior factors in a
graph-based SLAM framework. These priors are particularly beneficial during the
initial phase when object observations are limited. We implement a complete
pipeline integrating these priors, achieving robust data association on sparse
object-level features and enabling real-time object SLAM. Our system, evaluated
on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\% over
the latest baseline. Additionally, we present real-world experiments in the
supplementary video, demonstrating its real-time performance.

</details>


### [436] [Generating Stable Placements via Physics-guided Diffusion Models](https://arxiv.org/abs/2509.21664)
*Philippe Nadeau,Miguel Rogel,Ivan BiliÄ‡,Ivan PetroviÄ‡,Jonathan Kelly*

Main category: cs.RO

TL;DR: The paper introduces a method to generate stable object placements in multi-object robotic manipulation scenes using a diffusion model, outperforming state-of-the-art geometric methods in robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Stable object placements are crucial for robotic manipulation but existing methods heavily rely on simulations or heuristic assessments, which are often limited.

Method: The authors use a diffusion model conditioned on scene and object point clouds and integrate it with a stability-aware loss to directly sample stable object placements.

Result: The proposed method improves placement robustness by 56% and reduces runtime by 47% compared to state-of-the-art geometric techniques.

Conclusion: The integration of geometry-aware priors with physics-guided stability checks in the sampling process allows efficient and improved placement strategies without needing retraining, demonstrating significant improvements in robotic manipulation.

Abstract: Stably placing an object in a multi-object scene is a fundamental challenge
in robotic manipulation, as placements must be penetration-free, establish
precise surface contact, and result in a force equilibrium. To assess
stability, existing methods rely on running a simulation engine or resort to
heuristic, appearance-based assessments. In contrast, our approach integrates
stability directly into the sampling process of a diffusion model. To this end,
we query an offline sampling-based planner to gather multi-modal placement
labels and train a diffusion model to generate stable placements. The diffusion
model is conditioned on scene and object point clouds, and serves as a
geometry-aware prior. We leverage the compositional nature of score-based
generative models to combine this learned prior with a stability-aware loss,
thereby increasing the likelihood of sampling from regions of high stability.
Importantly, this strategy requires no additional re-training or fine-tuning,
and can be directly applied to off-the-shelf models. We evaluate our method on
four benchmark scenes where stability can be accurately computed. Our
physics-guided models achieve placements that are 56% more robust to forceful
perturbations while reducing runtime by 47% compared to a state-of-the-art
geometric method.

</details>


### [437] [Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation](https://arxiv.org/abs/2509.21690)
*Muqun Hu,Wenxi Chen,Wenjing Li,Falak Mandali,Zijian He,Renhong Zhang,Praveen Krisna,Katherine Christian,Leo Benaharon,Dizhi Ma,Karthik Ramani,Yan Gu*

Main category: cs.RO

TL;DR: The paper focuses on achieving advanced humanoid table tennis capabilities using reinforcement learning, emphasizing proactive decision-making and physics-driven training rewards.


<details>
  <summary>Details</summary>
Motivation: To enable humanoid robots to perform table tennis with rapid perception, agility, and unified control, which is challenging given strict timing and coordination requirements.

Method: A reinforcement learning framework maps ball observations directly to joint commands. It uses a learned ball predictor for proactive decision-making and physics-based predictors for dense rewards during training.

Result: The developed policy achieved high performance in simulation, with a hit rate of 96% and success rate of 92%, and demonstrated effective zero-shot deployment on a physical humanoid robot.

Conclusion: The approach successfully bridges simulation and physical implementation, highlighting reinforcement learning as a feasible method for competitive humanoid table tennis.

Abstract: Humanoid table tennis (TT) demands rapid perception, proactive whole-body
motion, and agile footwork under strict timing -- capabilities that remain
difficult for unified controllers. We propose a reinforcement learning
framework that maps ball-position observations directly to whole-body joint
commands for both arm striking and leg locomotion, strengthened by predictive
signals and dense, physics-guided rewards. A lightweight learned predictor, fed
with recent ball positions, estimates future ball states and augments the
policy's observations for proactive decision-making. During training, a
physics-based predictor supplies precise future states to construct dense,
informative rewards that lead to effective exploration. The resulting policy
attains strong performance across varied serve ranges (hit rate $\geq$ 96% and
success rate $\geq$ 92%) in simulations. Ablation studies confirm that both the
learned predictor and the predictive reward design are critical for end-to-end
learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute
joints, the policy produces coordinated lateral and forward-backward footwork
with accurate, fast returns, suggesting a practical path toward versatile,
competitive humanoid TT.

</details>


### [438] [VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation](https://arxiv.org/abs/2509.21723)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: VLBiMan enables robots to perform bimanual manipulation with minimal human input by creating reusable skills from a single demonstration, using vision-language grounding to adapt to changing scenes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for bimanual manipulation require either extensive human demonstrations or result in rigid systems unable to adapt well to dynamic environments.

Method: VLBiMan uses task-aware decomposition to preserve invariant primitives while dynamically adapting components using semantic parsing, geometric constraints, and vision-language grounding.

Result: The framework drastically reduces demonstration needs, ensures compositional generalization, maintains robustness to novel situations, and supports cross-platform skill transfer.

Conclusion: VLBiMan represents a significant step toward practical bimanual robotic manipulation in unpredictable environments by leveraging human-like adaptability and vision-language anchoring.

Abstract: Achieving generalizable bimanual manipulation requires systems that can learn
efficiently from minimal human input while adapting to real-world uncertainties
and diverse embodiments. Existing approaches face a dilemma: imitation policy
learning demands extensive demonstrations to cover task variations, while
modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,
a framework that derives reusable skills from a single human example through
task-aware decomposition, preserving invariant primitives as anchors while
dynamically adapting adjustable components via vision-language grounding. This
adaptation mechanism resolves scene ambiguities caused by background changes,
object repositioning, or visual clutter without policy retraining, leveraging
semantic parsing and geometric feasibility constraints. Moreover, the system
inherits human-like hybrid control capabilities, enabling mixed synchronous and
asynchronous use of both arms. Extensive experiments validate VLBiMan across
tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in
demonstration requirements compared to imitation baselines, (2) compositional
generalization through atomic skill splicing for long-horizon tasks, (3)
robustness to novel but semantically similar objects and external disturbances,
and (4) strong cross-embodiment transfer, showing that skills learned from
human demonstrations can be instantiated on different robotic platforms without
retraining. By bridging human priors with vision-language anchored adaptation,
our work takes a step toward practical and versatile dual-arm manipulation in
unstructured settings.

</details>


### [439] [The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions](https://arxiv.org/abs/2509.21776)
*Hyeonseong Kim,Roy El-Helou,Seungbeen Lee,Sungjoon Choi,Matthew Pan*

Main category: cs.RO

TL;DR: This paper investigates playful deception in Human-Robot Interaction (HRI) inspired by the Turkish Ice Cream vendor routine, evaluating its impact on trust, enjoyment, and engagement during robotic handovers.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore how bounded and culturally familiar forms of deception can enhance user experience, engagement, and enjoyment in robotic interactions.

Method: The researchers designed a robotic manipulator with a custom end-effector incorporating five Turkish Ice Cream-inspired trick policies to execute deceptive handovers. They conducted a user study involving 91 participants using a mixed-design approach.

Result: Results show that playful deception enhances enjoyment and engagement but reduces perceived safety and trust, highlighting a trade-off in user experience.

Conclusion: Playful deception can be an effective strategy for designing interactive robots in entertainment-oriented contexts, but requires careful consideration of its trade-offs for trust and safety.

Abstract: Playful deception, a common feature in human social interactions, remains
underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice
Cream (TIC) vendor routine, we investigate how bounded, culturally familiar
forms of deception influence user trust, enjoyment, and engagement during
robotic handovers. We design a robotic manipulator equipped with a custom
end-effector and implement five TIC-inspired trick policies that deceptively
delay the handover of an ice cream-shaped object. Through a mixed-design user
study with 91 participants, we evaluate the effects of playful deception and
interaction duration on user experience. Results reveal that TIC-inspired
deception significantly enhances enjoyment and engagement, though reduces
perceived safety and trust, suggesting a structured trade-off across the
multi-dimensional aspects. Our findings demonstrate that playful deception can
be a valuable design strategy for interactive robots in entertainment and
engagement-focused contexts, while underscoring the importance of deliberate
consideration of its complex trade-offs. You can find more information,
including demonstration videos, on
https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .

</details>


### [440] [Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors](https://arxiv.org/abs/2509.21810)
*Ning Huang,Zhentao Xie,Qinchuan Li*

Main category: cs.RO

TL;DR: The paper introduces a framework called Conditional Adversarial Motion Priors (CAMP) to help quadruped robots learn diverse locomotion skills efficiently.


<details>
  <summary>Details</summary>
Motivation: Developing legged robots with agility and adaptability requires overcoming challenges in acquiring diverse locomotion skills and ensuring smooth transitions between them.

Method: The authors propose a multi-skill learning framework (CAMP) that uses a skill discriminator and skill-conditioned reward design to reconstruct and manage diverse locomotion skills learned from expert demonstrations.

Result: The framework enables quadruped robots to actively control and reuse multiple locomotion skills, making it robust and effective for generalizable policies.

Conclusion: CAMP provides a practical and efficient mechanism for legged robots to navigate complex environments with a repertoire of skills and seamless transitions.

Abstract: Despite growing interest in developing legged robots that emulate biological
locomotion for agile navigation of complex environments, acquiring a diverse
repertoire of skills remains a fundamental challenge in robotics. Existing
methods can learn motion behaviors from expert data, but they often fail to
acquire multiple locomotion skills through a single policy and lack smooth
skill transitions. We propose a multi-skill learning framework based on
Conditional Adversarial Motion Priors (CAMP), with the aim of enabling
quadruped robots to efficiently acquire a diverse set of locomotion skills from
expert demonstrations. Precise skill reconstruction is achieved through a novel
skill discriminator and skill-conditioned reward design. The overall framework
supports the active control and reuse of multiple skills, providing a practical
solution for learning generalizable policies in complex environments.

</details>


### [441] [Improved Vehicle Maneuver Prediction using Game Theoretic Priors](https://arxiv.org/abs/2509.21873)
*Nishant Doshi*

Main category: cs.RO

TL;DR: The paper explores using Level-k game theory combined with traditional classification models to improve lane-change predictions by considering entire scene interactions.


<details>
  <summary>Details</summary>
Motivation: Conventional trajectory-based classification models lack accuracy in lane-change predictions due to limited use of comprehensive scene information, prompting the need for enhanced methods.

Method: Level-k game theory is implemented to model hierarchical reasoning and interactions among vehicles, producing maneuver predictions via online optimization, combined with motion-based classification.

Result: Game-theoretic reasoning outputs accurate and rational maneuver predictions for vehicles, enhancing decision-making applications like Adaptive Cruise Control.

Conclusion: Integrating scene-aware game theory with classification models offers improved precision in predicting vehicle maneuvers, aiding safety and efficiency-focused driving systems.

Abstract: Conventional maneuver prediction methods use some sort of classification
model on temporal trajectory data to predict behavior of agents over a set time
horizon. Despite of having the best precision and recall, these models cannot
predict a lane change accurately unless they incorporate information about the
entire scene. Level-k game theory can leverage the human-like hierarchical
reasoning to come up with the most rational decisions each agent can make in a
group. This can be leveraged to model interactions between different vehicles
in presence of each other and hence compute the most rational decisions each
agent would make. The result of game theoretic evaluation can be used as a
"prior" or combined with a traditional motion-based classification model to
achieve more accurate predictions. The proposed approach assumes that the
states of the vehicles around the target lead vehicle are known. The module
will output the most rational maneuver prediction of the target vehicle based
on an online optimization solution. These predictions are instrumental in
decision making systems like Adaptive Cruise Control (ACC) or Traxen's
iQ-Cruise further improving the resulting fuel savings.

</details>


### [442] [WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces](https://arxiv.org/abs/2509.21878)
*Moses Gladson Selvamuthu,Tomoya Takahashi,Riichiro Tadakuma,Kazutoshi Tanaka*

Main category: cs.RO

TL;DR: The paper introduces Worm Gear-Based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator (VSA) enhancing robotic safety and adaptability.


<details>
  <summary>Details</summary>
Motivation: To design robotic manipulators that can better handle external forces with enhanced safety, compliance, and stiffness control.

Method: The authors developed WAVE, integrating a non-backdrivable worm gear, allowing for stiffness modulation and impact force absorption through a spring system.

Result: Experimental validation of the stiffness model confirmed low motor loads under external forces and demonstrated the actuator's robust performance in manipulator applications.

Conclusion: WAVE enables reliable and safe operation in demanding, contact-intensive tasks, decoupling external forces and extending actuator longevity.

Abstract: Robotic manipulators capable of regulating both compliance and stiffness
offer enhanced operational safety and versatility. Here, we introduce Worm
Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator
(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving
motor from external forces using this gear, WAVE enables precise force
transmission to the joint, while absorbing positional discrepancies through
compliance. WAVE is protected from excessive loads by converting impact forces
into elastic energy stored in a spring. In addition, the actuator achieves
continuous joint stiffness modulation by changing the spring's precompression
length. We demonstrate these capabilities, experimentally validate the proposed
stiffness model, show that motor loads approach zero at rest--even under
external loading--and present applications using a manipulator with WAVE. This
outcome showcases the successful decoupling of external forces. The protective
attributes of this actuator allow for extended operation in contact-intensive
tasks, and for robust robotic applications in challenging environments.

</details>


### [443] [SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks](https://arxiv.org/abs/2509.21928)
*Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian*

Main category: cs.RO

TL;DR: SAGE is a framework that bridges high-level planning and low-level control for complex, long-horizon manipulation tasks using semantic scene graphs.


<details>
  <summary>Details</summary>
Motivation: Address challenges in long-horizon manipulation tasks, including extended action sequences, complex object interactions, and gaps in task planning and control methods.

Method: Introduces SAGE, which uses semantic scene graphs for task-level reasoning and a two-part system: a scene graph-based task planner and a structural image editing pipeline.

Result: SAGE demonstrates state-of-the-art performance in diverse long-horizon manipulation tasks through experiments.

Conclusion: Semantic scene graphs are effective for bridging symbolic reasoning and visuo-motor control, offering advancements in manipulation task execution.

Abstract: Successfully solving long-horizon manipulation tasks remains a fundamental
challenge. These tasks involve extended action sequences and complex object
interactions, presenting a critical gap between high-level symbolic planning
and low-level continuous control. To bridge this gap, two essential
capabilities are required: robust long-horizon task planning and effective
goal-conditioned manipulation. Existing task planning methods, including
traditional and LLM-based approaches, often exhibit limited generalization or
sparse semantic reasoning. Meanwhile, image-conditioned control methods
struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a
novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon
Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural
representation for scene states. A structural scene graph enables bridging
task-level semantic reasoning and pixel-level visuo-motor control. This also
facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE
consists of two key components: (1) a scene graph-based task planner that uses
VLMs and LLMs to parse the environment and reason about physically-grounded
scene state transition sequences, and (2) a decoupled structural image editing
pipeline that controllably converts each target sub-goal graph into a
corresponding image through image inpainting and composition. Extensive
experiments have demonstrated that SAGE achieves state-of-the-art performance
on distinct long-horizon tasks.

</details>


### [444] [Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception](https://arxiv.org/abs/2509.21955)
*Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.RO

TL;DR: The paper introduces Learnable Conformal Prediction (LCP), enhancing predictive reliability in robotics by replacing fixed nonconformity scores with a lightweight neural function, reducing conservatism and improving task performance.


<details>
  <summary>Details</summary>
Motivation: Calibrating predictive uncertainty in robotic deep learning models is challenging, particularly for noisy or out-of-distribution inputs. Current conformal prediction methods use context-insensitive scores, limiting relevance and precision.

Method: The authors propose Learnable Conformal Prediction (LCP), which uses a neural function to adaptively model nonconformity scores based on task-specific and contextual features while ensuring theoretical coverage guarantees.

Result: LCP reduces prediction set sizes by up to 18%, tightens detection intervals by 52%, and improves path planning success from 72% to 91%, outperforming Standard CP and ensembles across seven benchmarks.

Conclusion: Learnable Conformal Prediction (LCP) offers a lightweight, adaptable, and computationally efficient solution to improving predictive reliability in robotic tasks while significantly enhancing performance metrics.

Abstract: Deep learning models in robotics often output point estimates with poorly
calibrated confidences, offering no native mechanism to quantify predictive
reliability under novel, noisy, or out-of-distribution inputs. Conformal
prediction (CP) addresses this gap by providing distribution-free coverage
guarantees, yet its reliance on fixed nonconformity scores ignores context and
can yield intervals that are overly conservative or unsafe. We address this
with Learnable Conformal Prediction (LCP), which replaces fixed scores with a
lightweight neural function that leverages geometric, semantic, and
task-specific features to produce context-aware uncertainty sets.
  LCP maintains CP's theoretical guarantees while reducing prediction set sizes
by 18% in classification, tightening detection intervals by 52%, and improving
path planning safety from 72% to 91% success with minimal overhead. Across
three robotic tasks on seven benchmarks, LCP consistently outperforms Standard
CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it
achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object
detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding
boxes. In path planning through cluttered environments, it improves success to
91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.
  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)
and supports online adaptation, making it well suited to resource-constrained
autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and
15.9% inference overhead, yet sustains 39 FPS on detection tasks while being
7.4 times more energy-efficient than ensembles.

</details>


### [445] [FlowDrive: moderated flow matching with data balancing for trajectory planning](https://arxiv.org/abs/2509.21961)
*Lingguang Wang,Ã–mer Åžahin TaÅŸ,Marlon Steiner,Christoph Stiller*

Main category: cs.RO

TL;DR: FlowDrive addresses the imbalance in driving data, especially underrepresented dangerous scenarios, and achieves state-of-the-art planning results through a novel flow-matching trajectory planner.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of imbalanced driving data, which biases learning-based planners towards frequent maneuvers and weakens their performance in critical, less frequent scenarios.

Method: FlowDrive utilizes a flow-matching trajectory planner that applies conditional rectified flow to map noise into trajectory distributions. It enhances trajectory diversity with moderated in-the-loop guidance that introduces small perturbations while maintaining scene-consistency.

Result: FlowDrive achieves state-of-the-art performance among learning-based trajectory planners on nuPlan and interPlan benchmarks. Incorporating moderated guidance and post-processing (FlowDrive*) further improves results, achieving leading performance across nearly all benchmark splits.

Conclusion: Reweighting training data by trajectory pattern and using the FlowDrive planner improves performance, offering a more balanced and effective approach to trajectory planning.

Abstract: Learning-based planners are sensitive to the long-tailed distribution of
driving data. Common maneuvers dominate datasets, while dangerous or rare
scenarios are sparse. This imbalance can bias models toward the frequent cases
and degrade performance on critical scenarios. To tackle this problem, we
compare balancing strategies for sampling training data and find reweighting by
trajectory pattern an effective approach. We then present FlowDrive, a
flow-matching trajectory planner that learns a conditional rectified flow to
map noise directly to trajectory distributions with few flow-matching steps. We
further introduce moderated, in-the-loop guidance that injects small
perturbation between flow steps to systematically increase trajectory diversity
while remaining scene-consistent. On nuPlan and the interaction-focused
interPlan benchmarks, FlowDrive achieves state-of-the-art results among
learning-based planners and approaches methods with rule-based refinements.
After adding moderated guidance and light post-processing (FlowDrive*), it
achieves overall state-of-the-art performance across nearly all benchmark
splits.

</details>


### [446] [Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning](https://arxiv.org/abs/2509.21983)
*Sigmund Hennum HÃ¸eg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du*

Main category: cs.RO

TL;DR: The paper addresses challenges in robot planning for long-horizon tasks using Diffusion Models while suggesting a hybrid diffusion approach combining symbolic planning and continuous trajectory generation.


<details>
  <summary>Details</summary>
Motivation: To tackle the difficulty in robotic planning for complex, long-horizon tasks where traditional Diffusion Models often fail.

Method: Introducing an integrated model using both discrete variable diffusion for symbolic planning and continuous diffusion for trajectory generation.

Result: The proposed hybrid diffusion dramatically outperforms baseline models and allows flexible trajectory synthesis conditioned on symbolic inputs.

Conclusion: Hybrid diffusion effectively resolves issues in generative planning for complex robotic tasks, advancing the capabilities of Diffusion Models in this domain.

Abstract: Constructing robots to accomplish long-horizon tasks is a long-standing
challenge within artificial intelligence. Approaches using generative methods,
particularly Diffusion Models, have gained attention due to their ability to
model continuous robotic trajectories for planning and control. However, we
show that these models struggle with long-horizon tasks that involve complex
decision-making and, in general, are prone to confusing different modes of
behavior, leading to failure. To remedy this, we propose to augment continuous
trajectory generation by simultaneously generating a high-level symbolic plan.
We show that this requires a novel mix of discrete variable diffusion and
continuous diffusion, which dramatically outperforms the baselines. In
addition, we illustrate how this hybrid diffusion process enables flexible
trajectory synthesis, allowing us to condition synthesized actions on partial
and complete symbolic conditions.

</details>


### [447] [Developing Vision-Language-Action Model from Egocentric Videos](https://arxiv.org/abs/2509.21986)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.RO

TL;DR: This paper proposes EgoScaler, a framework that derives 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary data. This enables Vision-Language-Action models (VLAs) to be trained directly from raw videos, facilitating scalable learning.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of raw egocentric videos as large-scale, cost-effective resources for training Vision-Language-Action models (VLAs) without relying on costly manual teleoperation or auxiliary annotations like hand-pose recordings.

Method: EgoScaler is introduced as a framework to extract 6DoF object manipulation trajectories from egocentric videos. It refines noisy or incomplete trajectories and builds a large dataset from four egocentric video datasets for VLA pre-training.

Result: Training on the constructed dataset improves task success rates by over 20% compared to training from scratch, yields competitive performance with real-robot datasets, and enhances results when combined with real-robot data.

Conclusion: Egocentric videos, processed via EgoScaler, represent a scalable and effective resource for advancing Vision-Language-Action research, minimizing dependency on costly manual data collection methods.

Abstract: Egocentric videos capture how humans manipulate objects and tools, providing
diverse motion cues for learning object manipulation. Unlike the costly,
expert-driven manual teleoperation commonly used in training
Vision-Language-Action models (VLAs), egocentric videos offer a scalable
alternative. However, prior studies that leverage such videos for training
robot policies typically rely on auxiliary annotations, such as detailed
hand-pose recordings. Consequently, it remains unclear whether VLAs can be
trained directly from raw egocentric videos. In this work, we address this
challenge by leveraging EgoScaler, a framework that extracts 6DoF object
manipulation trajectories from egocentric videos without requiring auxiliary
recordings. We apply EgoScaler to four large-scale egocentric video datasets
and automatically refine noisy or incomplete trajectories, thereby constructing
a new large-scale dataset for VLA pre-training. Our experiments with a
state-of-the-art $\pi_0$ architecture in both simulated and real-robot
environments yield three key findings: (i) pre-training on our dataset improves
task success rates by over 20\% compared to training from scratch, (ii) the
performance is competitive with that achieved using real-robot datasets, and
(iii) combining our dataset with real-robot data yields further improvements.
These results demonstrate that egocentric videos constitute a promising and
scalable resource for advancing VLA research.

</details>


### [448] [One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion](https://arxiv.org/abs/2509.22002)
*Yuping Gu,Bangchao Huang,Haoran Sun,Ronghan Xu,Jiayi Yin,Wei Zhang,Fang Wan,Jia Pan,Chaoyang Song*

Main category: cs.RO

TL;DR: This paper introduces a computational approach to design 1-DoF overconstrained robotic limbs, achieving efficient, self-collision-free motion for spatial trajectories.


<details>
  <summary>Details</summary>
Motivation: To advance robotic limb designs with single degree-of-freedom, leveraging simplicity and efficiency while addressing challenges like self-collision in full-cycle motion.

Method: Develop a geometric optimization framework for linkage-based robotic limbs, focusing on self-collision-free trajectories and overconstrained designs, followed by experimental validation on bio-inspired robots.

Result: The optimized hexapod robot limbs exhibited collision-free motion and significantly improved energy efficiency during walking.

Conclusion: Single-DoF robotic limbs can achieve complex spatial trajectories and energy-efficient motion through overconstrained linkage designs.

Abstract: While it is expected to build robotic limbs with multiple degrees of freedom
(DoF) inspired by nature, a single DoF design remains fundamental, providing
benefits that include, but are not limited to, simplicity, robustness,
cost-effectiveness, and efficiency. Mechanisms, especially those with multiple
links and revolute joints connected in closed loops, play an enabling factor in
introducing motion diversity for 1-DoF systems, which are usually constrained
by self-collision during a full-cycle range of motion. This study presents a
novel computational approach to designing one-degree-of-freedom (1-DoF)
overconstrained robotic limbs for a desired spatial trajectory, while achieving
energy-efficient, self-collision-free motion in full-cycle rotations. Firstly,
we present the geometric optimization problem of linkage-based robotic limbs in
a generalized formulation for self-collision-free design. Next, we formulate
the spatial trajectory generation problem with the overconstrained linkages by
optimizing the similarity and dynamic-related metrics. We further optimize the
geometric shape of the overconstrained linkage to ensure smooth and
collision-free motion driven by a single actuator. We validated our proposed
method through various experiments, including personalized automata and
bio-inspired hexapod robots. The resulting hexapod robot, featuring
overconstrained robotic limbs, demonstrated outstanding energy efficiency
during forward walking.

</details>


### [449] [An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose](https://arxiv.org/abs/2509.22058)
*Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu*

Main category: cs.RO

TL;DR: This paper presents an adaptive Iterative Closest Point (ICP)-based LiDAR odometry method that enhances accuracy and robustness for autonomous navigation, addressing limitations in initial pose reliability and dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy and robustness of LiDAR odometry methods, particularly ICP-based techniques, by addressing challenges related to unreliable initial poses and the lack of adaptability in dynamic environments.

Method: The proposed method utilizes distributed coarse registration with density filtering for initial pose estimation, selects the most reliable pose by comparing it with motion prediction, dynamically adjusts adaptive thresholds based on current and historical errors, and employs an adaptive ICP registration for point cloud alignment.

Result: Experiments on the KITTI dataset demonstrate that the proposed method performs better than existing approaches, significantly improving LiDAR odometry accuracy.

Conclusion: The proposed adaptive ICP-based method enhances the robustness and precision of LiDAR odometry by tackling issues of initial pose reliability and adaptability in dynamic environments, making it more effective for autonomous driving applications.

Abstract: As a key technology for autonomous navigation and positioning in mobile
robots, light detection and ranging (LiDAR) odometry is widely used in
autonomous driving applications. The Iterative Closest Point (ICP)-based
methods have become the core technique in LiDAR odometry due to their efficient
and accurate point cloud registration capability. However, some existing
ICP-based methods do not consider the reliability of the initial pose, which
may cause the method to converge to a local optimum. Furthermore, the absence
of an adaptive mechanism hinders the effective handling of complex dynamic
environments, resulting in a significant degradation of registration accuracy.
To address these issues, this paper proposes an adaptive ICP-based LiDAR
odometry method that relies on a reliable initial pose. First, distributed
coarse registration based on density filtering is employed to obtain the
initial pose estimation. The reliable initial pose is then selected by
comparing it with the motion prediction pose, reducing the initial error
between the source and target point clouds. Subsequently, by combining the
current and historical errors, the adaptive threshold is dynamically adjusted
to accommodate the real-time changes in the dynamic environment. Finally, based
on the reliable initial pose and the adaptive threshold, point-to-plane
adaptive ICP registration is performed from the current frame to the local map,
achieving high-precision alignment of the source and target point clouds.
Extensive experiments on the public KITTI dataset demonstrate that the proposed
method outperforms existing approaches and significantly enhances the accuracy
of LiDAR odometry.

</details>


### [450] [Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot](https://arxiv.org/abs/2509.22065)
*Ethan Fulcher,J. Diego Caporale,Yifeng Zhang,John Ruck,Feifei Qian*

Main category: cs.RO

TL;DR: The study investigates the use of legged robots to measure terrain properties during locomotion by comparing two gaits, "Crawl N' Sense" and "Trot-Walk."


<details>
  <summary>Details</summary>
Motivation: Robots exploring planetary terrains require advanced techniques to understand terramechanical properties, especially on loose and deformable surfaces.

Method: The paper compares two robotic gaits' ability to measure terrain strength and texture over various substrates, including rigid surfaces, loose sand, and crusted sand.

Result: The sensing-oriented crawl gait showed higher accuracy in detecting brittle surface ruptures and provided consistent terrain strength measurements, outperforming the locomotion-oriented trot gait in accuracy.

Conclusion: Legged robots can effectively perform "sensing during locomotion" for terrain analysis, potentially aiding geological exploration on Earth and other planets.

Abstract: In-situ robotic exploration is an important tool for advancing knowledge of
geological processes that describe the Earth and other Planetary bodies. To
inform and enhance operations for these roving laboratories, it is imperative
to understand the terramechanical properties of their environments, especially
for traversing on loose, deformable substrates. Recent research suggested that
legged robots with direct-drive and low-gear ratio actuators can sensitively
detect external forces, and therefore possess the potential to measure terrain
properties with their legs during locomotion, providing unprecedented sampling
speed and density while accessing terrains previously too risky to sample. This
paper explores these ideas by investigating the impact of gait on
proprioceptive terrain sensing accuracy, particularly comparing a
sensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,
Trot-Walk. Each gait's ability to measure the strength and texture of
deformable substrate is quantified as the robot locomotes over a laboratory
transect consisting of a rigid surface, loose sand, and loose sand with
synthetic surface crusts. Our results suggest that with both the
sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can
measure a consistent difference in the strength (in terms of penetration
resistance) between the low- and high-resistance substrates; however, the
locomotion-oriented trot gait contains larger magnitude and variance in
measurements. Furthermore, the slower crawl gait can detect brittle ruptures of
the surface crusts with significantly higher accuracy than the faster trot
gait. Our results offer new insights that inform legged robot "sensing during
locomotion" gait design and planning for scouting the terrain and producing
scientific measurements on other worlds to advance our understanding of their
geology and formation.

</details>


### [451] [Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation](https://arxiv.org/abs/2509.22093)
*Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu*

Main category: cs.RO

TL;DR: The paper proposes Action-aware Dynamic Pruning (ADP), a method to improve the efficiency and performance of robotic manipulation by adaptively pruning visual tokens based on action dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language-Action (VLA) models are computationally expensive due to the redundancy of visual tokens, which varies across robotic manipulation stages. There is a need to optimize such models while maintaining performance.

Method: The authors introduce ADP, a multi-modal pruning framework with a novel gating mechanism that conditions token pruning on recent action trajectories. This allows dynamic adjustment of token retention ratios based on the manipulation stage.

Result: Experiments show that ADP significantly reduces computational costs and inference latency (e.g., 1.35Ã— speed-up) while improving success rates (e.g., 25.8% with OpenVLA) in robotics tasks compared to existing methods.

Conclusion: ADP is an effective and efficient solution for robotic manipulation, providing a computationally optimized and high-performing method that balances precision and cost across different manipulation tasks.

Abstract: Robotic manipulation with Vision-Language-Action models requires efficient
inference over long-horizon multi-modal context, where attention to dense
visual tokens dominates computational cost. Existing methods optimize inference
speed by reducing visual redundancy within VLA models, but they overlook the
varying redundancy across robotic manipulation stages. We observe that the
visual token redundancy is higher in coarse manipulation phase than in
fine-grained operations, and is strongly correlated with the action dynamic.
Motivated by this observation, we propose \textbf{A}ction-aware
\textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning
framework that integrates text-driven token selection with action-aware
trajectory gating. Our method introduces a gating mechanism that conditions the
pruning signal on recent action trajectories, using past motion windows to
adaptively adjust token retention ratios in accordance with dynamics, thereby
balancing computational efficiency and perceptual precision across different
manipulation stages. Extensive experiments on the LIBERO suites and diverse
real-world scenarios demonstrate that our method significantly reduces FLOPs
and action inference latency (\textit{e.g.} $1.35 \times$ speed up on
OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\%
improvements with OpenVLA) compared to baselines, thereby providing a simple
plug-in path to efficient robot policies that advances the efficiency and
performance frontier of robotic manipulation. Our project website is:
\href{https://vla-adp.github.io/}{ADP.com}.

</details>


### [452] [Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot](https://arxiv.org/abs/2509.22120)
*Alireza Aliyari,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: This paper introduces a robust nonlinear MPC for two-degree-of-freedom exoskeletons to improve performance under uncertainties.


<details>
  <summary>Details</summary>
Motivation: The need to address challenges caused by uncertainties in human-robot systems for effective exoskeleton applications.

Method: A new multi-stage robust nonlinear MPC strategy is developed that uses multiple scenarios to handle system uncertainties and focus on minimizing interaction forces during the swing phase.

Result: The method outperformed non-robust NMPC, reducing interaction forces by up to 94% under uncertain conditions like carrying unknown loads and experiencing external disturbances.

Conclusion: The proposed RNMPC significantly enhances robustness and control performance, particularly in uncertain and dynamic scenarios for exoskeleton systems.

Abstract: The use of exoskeleton robots is increasing due to the rising number of
musculoskeletal injuries. However, their effectiveness depends heavily on the
design of control systems. Designing robust controllers is challenging because
of uncertainties in human-robot systems. Among various control strategies,
Model Predictive Control (MPC) is a powerful approach due to its ability to
handle constraints and optimize performance. Previous studies have used
linearization-based methods to implement robust MPC on exoskeletons, but these
can degrade performance due to nonlinearities in the robot's dynamics. To
address this gap, this paper proposes a Robust Nonlinear Model Predictive
Control (RNMPC) method, called multi-stage NMPC, to control a
two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.
This method uses multiple scenarios to represent system uncertainties. The
study focuses on minimizing human-robot interaction forces during the swing
phase, particularly when the robot carries unknown loads. Simulations and
experimental tests show that the proposed method significantly improves
robustness, outperforming non-robust NMPC. It achieves lower tracking errors
and interaction forces under various uncertainties. For instance, when a 2 kg
unknown payload is combined with external disturbances, the RMS values of thigh
and shank interaction forces for multi-stage NMPC are reduced by 77 and 94
percent, respectively, compared to non-robust NMPC.

</details>


### [453] [DemoGrasp: Universal Dexterous Grasping from a Single Demonstration](https://arxiv.org/abs/2509.22149)
*Haoqi Yuan,Ziye Huang,Ye Wang,Chuan Mao,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: DemoGrasp introduces a method for learning universal dexterous grasping via RL, utilizing trajectory editing based on a single demonstration to grasp diverse objects successfully.


<details>
  <summary>Details</summary>
Motivation: Dexterous grasping with multi-fingered robotic hands is complex due to high-dimensional exploration and varying object geometries, requiring improved methodologies for universal applicability.

Method: The approach uses a single demonstration trajectory and edits robot actions for novel objects, formulating this editing as a single-step Markov Decision Process (MDP), combined with RL using basic rewards.

Result: DemoGrasp achieves a 95% success rate on DexGraspNet objects, demonstrates strong transferability to unseen objects and dexterous hands, and succeeds with real-world objects using vision-based imitation learning.

Conclusion: DemoGrasp simplifies reward and trajectory design while achieving strong performance and generalizability across diverse robotic grasping scenarios, showcasing its robustness and adaptability.

Abstract: Universal grasping with multi-fingered dexterous hands is a fundamental
challenge in robotic manipulation. While recent approaches successfully learn
closed-loop grasping policies using reinforcement learning (RL), the inherent
difficulty of high-dimensional, long-horizon exploration necessitates complex
reward and curriculum design, often resulting in suboptimal solutions across
diverse objects. We propose DemoGrasp, a simple yet effective method for
learning universal dexterous grasping. We start from a single successful
demonstration trajectory of grasping a specific object and adapt to novel
objects and poses by editing the robot actions in this trajectory: changing the
wrist pose determines where to grasp, and changing the hand joint angles
determines how to grasp. We formulate this trajectory editing as a single-step
Markov Decision Process (MDP) and use RL to optimize a universal policy across
hundreds of objects in parallel in simulation, with a simple reward consisting
of a binary success term and a robot-table collision penalty. In simulation,
DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow
Hand, outperforming previous state-of-the-art methods. It also shows strong
transferability, achieving an average success rate of 84.6% across diverse
dexterous hand embodiments on six unseen object datasets, while being trained
on only 175 objects. Through vision-based imitation learning, our policy
successfully grasps 110 unseen real-world objects, including small, thin items.
It generalizes to spatial, background, and lighting changes, supports both RGB
and depth inputs, and extends to language-guided grasping in cluttered scenes.

</details>


### [454] [DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions](https://arxiv.org/abs/2509.22175)
*Quanzhou Li,Zhonghua Wu,Jingbo Wang,Chen Change Loy,Bo Dai*

Main category: cs.RO

TL;DR: The paper presents SymOpt for generating a large dual-hand grasp dataset and DHAGrasp for producing dual-hand grasps for objects that have not been previously modeled.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve dual-hand grasp generation, which is currently limited due to scarce datasets focusing on single-hand interactions and lacking sufficient semantic part annotations.

Method: The researchers developed SymOpt to exploit object and hand symmetries to create a dual-hand grasp dataset and introduced DHAGrasp, a text-guided generator with a novel dual-hand affordance representation in a two-stage design.

Result: Their approach demonstrated superior performance in generating diverse, semantically consistent, and high-quality dual-hand grasps that generalize well to unseen objects.

Conclusion: SymOpt and DHAGrasp advance dual-hand interaction modeling, addressing dataset scarcity and improving grasp synthesis for better generalization capabilities.

Abstract: Learning to generate dual-hand grasps that respect object semantics is
essential for robust hand-object interaction but remains largely underexplored
due to dataset scarcity. Existing grasp datasets predominantly focus on
single-hand interactions and contain only limited semantic part annotations. To
address these challenges, we introduce a pipeline, SymOpt, that constructs a
large-scale dual-hand grasp dataset by leveraging existing single-hand datasets
and exploiting object and hand symmetries. Building on this, we propose a
text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand
Affordance-aware Grasps for unseen objects. Our approach incorporates a novel
dual-hand affordance representation and follows a two-stage design, which
enables effective learning from a small set of segmented training objects while
scaling to a much larger pool of unsegmented data. Extensive experiments
demonstrate that our method produces diverse and semantically consistent
grasps, outperforming strong baselines in both grasp quality and generalization
to unseen objects. The project page is at
https://quanzhou-li.github.io/DHAGrasp/.

</details>


### [455] [Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting](https://arxiv.org/abs/2509.22195)
*Asher J. Hancock,Xindi Wu,Lihan Zha,Olga Russakovsky,Anirudha Majumdar*

Main category: cs.RO

TL;DR: The paper addresses the issue of catastrophic forgetting in fine-tuning vision-language models (VLMs) for robot teleoperation by introducing the VLM2VLA approach, which uses Low-Rank Adaptation (LoRA) to align action representation with natural language, improving generalization without compromising foundational model capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the issue of catastrophic forgetting, where fine-tuning vision-language models for robotic tasks diminishes their foundational reasoning, multimodal understanding, and generalization abilities. This occurs due to a data distribution mismatch.

Method: The paper introduces VLM2VLA, a training method that aligns robotic actions with natural language representation. This alignment enables the use of Low-Rank Adaptation (LoRA) to fine-tune VLMs, avoiding significant modifications to the model and preserving its core capabilities.

Result: The VLM2VLA approach prevents catastrophic forgetting, as validated through extensive Visual Question Answering (VQA) studies and over 800 real-world robotics experiments. This allows VLM-based models to generalize to novel tasks requiring semantic reasoning and multilingual instruction.

Conclusion: By aligning robotic low-level action data with natural language and minimally adjusting the VLM backbone using LoRA, the VLM2VLA approach maintains the foundational reasoning capabilities of VLMs and supports zero-shot generalization to complex, novel tasks.

Abstract: Fine-tuning vision-language models (VLMs) on robot teleoperation data to
create vision-language-action (VLA) models is a promising paradigm for training
generalist policies, but it suffers from a fundamental tradeoff: learning to
produce actions often diminishes the VLM's foundational reasoning and
multimodal understanding, hindering generalization to novel scenarios,
instruction following, and semantic understanding. We argue that this
catastrophic forgetting is due to a distribution mismatch between the VLM's
internet-scale pretraining corpus and the robotics fine-tuning data. Inspired
by this observation, we introduce VLM2VLA: a VLA training paradigm that first
resolves this mismatch at the data level by representing low-level actions with
natural language. This alignment makes it possible to train VLAs solely with
Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and
averting catastrophic forgetting. As a result, the VLM can be fine-tuned on
robot teleoperation data without fundamentally altering the underlying
architecture and without expensive co-training on internet-scale VLM datasets.
Through extensive Visual Question Answering (VQA) studies and over 800
real-world robotics experiments, we demonstrate that VLM2VLA preserves the
VLM's core capabilities, enabling zero-shot generalization to novel tasks that
require open-world semantic reasoning and multilingual instruction following.

</details>


### [456] [MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training](https://arxiv.org/abs/2509.22199)
*Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang*

Main category: cs.RO

TL;DR: MimicDreamer leverages human demonstration videos by converting them into robot-usable data using visual, viewpoint, and action alignment tools.


<details>
  <summary>Details</summary>
Motivation: Training robots with diverse interactions is limited by the cost of robot-generated data collection, while human demonstrations are cheaper but require adaptation to bridge domain gaps.

Method: MimicDreamer uses three components: H2R Aligner for visual alignment (video transformation using diffusion models), EgoStabilizer for viewpoint stabilization (homography and inpainting), and action alignment with a constrained inverse kinematics solver.

Result: The method enables VLA models trained on synthesized human-to-robot videos to achieve few-shot performance on real robots, with an average success rate improvement of 14.7% compared to using only real robot data.

Conclusion: Human demonstrations can effectively substitute robot-generated data for training, leading to scalable and improved robot learning when processed with the MimicDreamer framework.

Abstract: Vision Language Action (VLA) models derive their generalization capability
from diverse training data, yet collecting embodied robot interaction data
remains prohibitively expensive. In contrast, human demonstration videos are
far more scalable and cost-efficient to collect, and recent studies confirm
their effectiveness in training VLA models. However, a significant domain gap
persists between human videos and robot-executed videos, including unstable
camera viewpoints, visual discrepancies between human hands and robotic arms,
and differences in motion dynamics. To bridge this gap, we propose
MimicDreamer, a framework that turns fast, low-cost human demonstrations into
robot-usable supervision by jointly aligning vision, viewpoint, and actions to
directly support policy training. For visual alignment, we propose H2R Aligner,
a video diffusion model that generates high-fidelity robot demonstration videos
by transferring motion from human manipulation footage. For viewpoint
stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos
via homography and inpaints occlusions and distortions caused by warping. For
action alignment, we map human hand trajectories to the robot frame and apply a
constrained inverse kinematics solver to produce feasible, low-jitter joint
commands with accurate pose tracking. Empirically, VLA models trained purely on
our synthesized human-to-robot videos achieve few-shot execution on real
robots. Moreover, scaling training with human data significantly boosts
performance compared to models trained solely on real robot data; our approach
improves the average success rate by 14.7\% across six representative
manipulation tasks.

</details>


### [457] [From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment](https://arxiv.org/abs/2509.22205)
*Ke Ye,Jiaming Zhou,Yuanfeng Qiu,Jiayi Liu,Shihui Zhou,Kun-Yu Lin,Junwei Liang*

Main category: cs.RO

TL;DR: The paper introduces "Super-Mimic," a hierarchical framework enabling zero-shot robotic imitation by parsing human demonstration videos to infer procedural intent and synthesize future video rollouts for action sequences.


<details>
  <summary>Details</summary>
Motivation: Zero-shot generalization in robotic long-horizon manipulation tasks is challenging, and existing multimodal methods struggle to translate high-level commands into executable actions using visual inputs.

Method: The framework includes two modules: Human Intent Translator (HIT), for converting demonstration videos into subtasks using multimodal reasoning, and Future Dynamics Predictor (FDP), a generative model producing video rollouts for dynamics-aware guidance.

Result: In experiments on long-horizon tasks, Super-Mimic outperformed state-of-the-art zero-shot methods by over 20%, showcasing effectiveness in accurate intent parsing and dynamics modeling.

Conclusion: The study demonstrates that combining video-driven intent parsing with prospective dynamics modeling enables efficient, general-purpose robotic systems for zero-shot settings.

Abstract: Generalizing to long-horizon manipulation tasks in a zero-shot setting
remains a central challenge in robotics. Current multimodal foundation based
approaches, despite their capabilities, typically fail to decompose high-level
commands into executable action sequences from static visual input alone. To
address this challenge, we introduce Super-Mimic, a hierarchical framework that
enables zero-shot robotic imitation by directly inferring procedural intent
from unscripted human demonstration videos. Our framework is composed of two
sequential modules. First, a Human Intent Translator (HIT) parses the input
video using multimodal reasoning to produce a sequence of language-grounded
subtasks. These subtasks then condition a Future Dynamics Predictor (FDP),
which employs a generative model that synthesizes a physically plausible video
rollout for each step. The resulting visual trajectories are dynamics-aware,
explicitly modeling crucial object interactions and contact points to guide the
low-level controller. We validate this approach through extensive experiments
on a suite of long-horizon manipulation tasks, where Super-Mimic significantly
outperforms state-of-the-art zero-shot methods by over 20\%. These results
establish that coupling video-driven intent parsing with prospective dynamics
modeling is a highly effective strategy for developing general-purpose robotic
systems.

</details>


### [458] [Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities](https://arxiv.org/abs/2509.22287)
*Stina Sundstedt,Mattias Wingren,Susanne HÃ¤gglund,Daniel Ventus*

Main category: cs.RO

TL;DR: This paper explores using a conversational robot powered by large language models to improve expressive language skills in preschool children with language vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Support preschool children with developmental language disorders or immigration-related language challenges by enhancing their expressive language skills.

Method: Developing a conversational robot using a large language model to play a word retrieval game, Alias, with children. The robot manages gameplay, dialogue, affective responses, and turn-taking, while aiming to deliver targeted morphological structures.

Result: The application demonstrates that robots equipped with large language models can integrate with game-based learning, with the hypothesis that they might outperform humans in delivering precise morphological targets.

Conclusion: This approach has the potential to create an effective Robot-Assisted Language Learning intervention that serves both children and professionals, ultimately addressing basic communication needs for diverse languages and vulnerable groups.

Abstract: Preschool children with language vulnerabilities -- such as developmental
language disorders or immigration related language challenges -- often require
support to strengthen their expressive language skills. Based on the principle
of implicit learning, speech-language therapists (SLTs) typically embed target
morphological structures (e.g., third person -s) into everyday interactions or
game-based learning activities. Educators are recommended by SLTs to do the
same. This approach demands precise linguistic knowledge and real-time
production of various morphological forms (e.g., "Daddy wears these when he
drives to work"). The task becomes even more demanding when educators or parent
also must keep children engaged and manage turn-taking in a game-based
activity. In the TalBot project our multiprofessional team have developed an
application in which the Furhat conversational robot plays the word retrieval
game "Alias" with children to improve language skills. Our application
currently employs a large language model (LLM) to manage gameplay, dialogue,
affective responses, and turn-taking. Our next step is to further leverage the
capacity of LLMs so the robot can generate and deliver specific morphological
targets during the game. We hypothesize that a robot could outperform humans at
this task. Novel aspects of this approach are that the robot could ultimately
serve as a model and tutor for both children and professionals and that using
LLM capabilities in this context would support basic communication needs for
children with language vulnerabilities. Our long-term goal is to create a
robust LLM-based Robot-Assisted Language Learning intervention capable of
teaching a variety of morphological structures across different languages.

</details>


### [459] [IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM](https://arxiv.org/abs/2509.22288)
*Johan Hatleskog,Morten Nissov,Kostas Alexis*

Main category: cs.RO

TL;DR: The paper addresses efficiency in radar-LiDAR-Inertial measurement integration, proposing a method to reduce computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-time radar-LiDAR-Inertial smoothing is hindered by high computational costs due to the double state creation rate required to address sensor desynchronization.

Method: This paper introduces IMU-preintegrated radar factors to propagate LiDAR states to radar timestamps using inertial measurements, halving the node creation rate.

Result: The proposed method reduces the aggregated factor graph optimization time by up to 56% while preserving pose accuracy in experiments on resource-limited hardware.

Conclusion: The approach achieves reduced computational demands while ensuring accurate results, facilitating real-time performance on constrained devices.

Abstract: Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor
graph node per measurement to compensate for the lack of time synchronization
between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this
strategy results in a state creation rate of twice the individual sensor
frequencies. This doubling of the number of states per second yields high
optimization costs, inhibiting real-time performance on resource-constrained
hardware. We introduce IMU-preintegrated radar factors that use high-rate
inertial data to propagate the most recent LiDAR state to the radar measurement
timestamp. This strategy maintains the node creation rate at the LiDAR
measurement frequency. Assuming equal sensor rates, this lowers the number of
nodes by 50 % and consequently the computational costs. Experiments on a single
board computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB
RAM) show that our method preserves the absolute pose error of a conventional
baseline while simultaneously lowering the aggregated factor graph optimization
time by up to 56 %.

</details>


### [460] [Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm](https://arxiv.org/abs/2509.22296)
*Joseph Hunt,Koyo Fujii,Aly Magassouba,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: Hospital patient falls are addressed using an IoRT-based proactive system with privacy-preserving sensors and coordinated robot interactions.


<details>
  <summary>Details</summary>
Motivation: Patient falls in hospitals are a prevalent issue, often resulting in injury, higher costs, and conventional detection methods being reactive with high false-positive rates.

Method: A novel IoRT framework is designed, integrating thermal sensing for predicting bed-exit and coordinating two robotic agents for proactive assistance based on patient needs.

Result: The system demonstrates accurate bed-exit prediction with low-resolution thermal sensing, coordinated robot responses, validated by user study and error analysis.

Conclusion: Robotic systems can surpass traditional monitoring methods by offering real-time, meaningful, and personalized assistance, reducing fall risks and improving patient care.

Abstract: Hospital patient falls remain a critical and costly challenge worldwide.
While conventional fall prevention systems typically rely on post-fall
detection or reactive alerts, they also often suffer from high false positive
rates and fail to address the underlying patient needs that lead to bed-exit
attempts. This paper presents a novel system architecture that leverages the
Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction
for proactive and personalized patient assistance. The system integrates a
privacy-preserving thermal sensing model capable of real-time bed-exit
prediction, with two coordinated robotic agents that respond dynamically based
on predicted intent and patient input. This orchestrated response could not
only reduce fall risk but also attend to the patient's underlying motivations
for movement, such as thirst, discomfort, or the need for assistance, before a
hazardous situation arises. Our contributions with this pilot study are
three-fold: (1) a modular IoRT-based framework enabling distributed sensing,
prediction, and multi-robot coordination; (2) a demonstration of low-resolution
thermal sensing for accurate, privacy-preserving preemptive bed-exit detection;
and (3) results from a user study and systematic error analysis that inform the
design of situationally aware, multi-agent interactions in hospital settings.
The findings highlight how interactive and connected robotic systems can move
beyond passive monitoring to deliver timely, meaningful assistance, empowering
safer, more responsive care environments.

</details>


### [461] [RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.22356)
*Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang*

Main category: cs.RO

TL;DR: The study introduces RoboView-Bias, the first benchmark to quantify visual bias in robotic manipulation systematically, revealing significant biases and proposing a mitigation strategy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic quantification of visual bias in robotic manipulation, which affects decision-making stability.

Method: Develop RoboView-Bias benchmark based on factor isolation, structured variant-generation, and perceptual-fairness validation, to measure biases caused by visual factors.

Result: Evaluation of embodied agents shows significant biases, with camera viewpoint as a key factor, color preferences from VLMs, and strong coupling between viewpoint and color-related bias. Mitigation reduced bias by 54.5%.

Conclusion: Addressing visual bias is essential for safe and reliable embodied agents, and RoboView-Bias offers a structured framework for systematic analysis and improvement.

Abstract: The safety and reliability of embodied agents rely on accurate and unbiased
visual perception. However, existing benchmarks mainly emphasize generalization
and robustness under perturbations, while systematic quantification of visual
bias remains scarce. This gap limits a deeper understanding of how perception
influences decision-making stability. To address this issue, we propose
RoboView-Bias, the first benchmark specifically designed to systematically
quantify visual bias in robotic manipulation, following a principle of factor
isolation. Leveraging a structured variant-generation framework and a
perceptual-fairness validation protocol, we create 2,127 task instances that
enable robust measurement of biases induced by individual visual factors and
their interactions. Using this benchmark, we systematically evaluate three
representative embodied agents across two prevailing paradigms and report three
key findings: (i) all agents exhibit significant visual biases, with camera
viewpoint being the most critical factor; (ii) agents achieve their highest
success rates on highly saturated colors, indicating inherited visual
preferences from underlying VLMs; and (iii) visual biases show strong,
asymmetric coupling, with viewpoint strongly amplifying color-related bias.
Finally, we demonstrate that a mitigation strategy based on a semantic
grounding layer substantially reduces visual bias by approximately 54.5\% on
MOKA. Our results highlight that systematic analysis of visual bias is a
prerequisite for developing safe and reliable general-purpose embodied agents.

</details>


### [462] [Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping](https://arxiv.org/abs/2509.22421)
*Leonel Giacobbe,Jingdao Chen,Chuangchuang Sun*

Main category: cs.RO

TL;DR: This paper proposes a learning-based, tactile-reactive multi-agent Model Predictive Controller (MPC) for collaborative robotic grasping of various objects, improving performance over single-agent and baseline methods.


<details>
  <summary>Details</summary>
Motivation: Current robotic grasping systems excel with rigid objects but fail with fragile or deformable ones. They also predominantly focus on single-agent systems, limiting the ability to grip large, heavy items.

Method: The system employs multi-agent MPC with Gelsight Mini tactile sensors to capture real-time texture and stiffness data, enabling adaptive control through a closed-loop system. It integrates tactile-driven grasp state inference with a novel coordination strategy.

Result: Experimental validation shows the proposed pipeline outperforms PD and single-agent MPC baselines in grasp success rates for diverse objects with varying sizes and stiffness.

Conclusion: The system offers an intelligent collaborative grasping solution that leverages tactile sensing and learning-based multi-agent MPC, improving performance in complex, multi-agent environments.

Abstract: Grasping is a core task in robotics with various applications. However, most
current implementations are primarily designed for rigid items, and their
performance drops considerably when handling fragile or deformable materials
that require real-time feedback. Meanwhile, tactile-reactive grasping focuses
on a single agent, which limits their ability to grasp and manipulate large,
heavy objects. To overcome this, we propose a learning-based, tactile-reactive
multi-agent Model Predictive Controller (MPC) for grasping a wide range of
objects with different softness and shapes, beyond the capabilities of
preexisting single-agent implementations. Our system uses two Gelsight Mini
tactile sensors [1] to extract real-time information on object texture and
stiffness. This rich tactile feedback is used to estimate contact dynamics and
object compliance in real time, enabling the system to adapt its control policy
to diverse object geometries and stiffness profiles. The learned controller
operates in a closed loop, leveraging tactile encoding to predict grasp
stability and adjust force and position accordingly. Our key technical
contributions include a multi-agent MPC formulation trained on real contact
interactions, a tactile-data driven method for inferring grasping states, and a
coordination strategy that enables collaborative control. By combining tactile
sensing and a learning-based multi-agent MPC, our method offers a robust,
intelligent solution for collaborative grasping in complex environments,
significantly advancing the capabilities of multi-agent systems. Our approach
is validated through extensive experiments against independent PD and MPC
baselines. Our pipeline outperforms the baselines regarding success rates in
achieving and maintaining stable grasps across objects of varying sizes and
stiffness.

</details>


### [463] [An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics](https://arxiv.org/abs/2509.22434)
*Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach*

Main category: cs.RO

TL;DR: The paper introduces OntoBOT, an ontology that unifies tasks, environments, actions, and robot capabilities to facilitate reasoning, execution, and knowledge sharing in service robotics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of interoperability and reusability in service robotics due to the tightly coupled nature of real-world deployments and isolated solutions.

Method: The researchers extend existing ontologies to create OntoBOT, enabling unified representation and reasoning frameworks. They evaluate its applicability across four different embodied robots.

Result: OntoBOT successfully demonstrated context-aware reasoning, task-oriented execution, and knowledge sharing across four service robots: TIAGo, HSR, UR3, and Stretch.

Conclusion: OntoBOT provides a cohesive framework for integrating tasks, environments, and robot capabilities, enhancing interoperability, generalizability, and practical deployment in service robotics.

Abstract: Personal service robots are increasingly used in domestic settings to assist
older adults and people requiring support. Effective operation involves not
only physical interaction but also the ability to interpret dynamic
environments, understand tasks, and choose appropriate actions based on
context. This requires integrating both hardware components (e.g. sensors,
actuators) and software systems capable of reasoning about tasks, environments,
and robot capabilities. Frameworks such as the Robot Operating System (ROS)
provide open-source tools that help connect low-level hardware with
higher-level functionalities. However, real-world deployments remain tightly
coupled to specific platforms. As a result, solutions are often isolated and
hard-coded, limiting interoperability, reusability, and knowledge sharing.
Ontologies and knowledge graphs offer a structured way to represent tasks,
environments, and robot capabilities. Existing ontologies, such as the
Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for
Linguistic and Cognitive Engineering (DOLCE), provide models for activities,
spatial relationships, and reasoning structures. However, they often focus on
specific domains and do not fully capture the connection between environment,
action, robot capabilities, and system-level integration. In this work, we
propose the Ontology for roBOts and acTions (OntoBOT), which extends existing
ontologies to provide a unified representation of tasks, actions, environments,
and capabilities. Our contributions are twofold: (1) we unify these aspects
into a cohesive ontology to support formal reasoning about task execution, and
(2) we demonstrate its generalizability by evaluating competency questions
across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how
OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge
sharing in service robotics.

</details>


### [464] [UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation](https://arxiv.org/abs/2509.22441)
*Zhangyuan Wang,Yunpeng Zhu,Yuqi Yan,Xiaoyuan Tian,Xinhao Shao,Meixuan Li,Weikun Li,Guangsheng Su,Weicheng Cui,Dixia Fan*

Main category: cs.RO

TL;DR: UnderwaterVLA is a framework for robust underwater navigation using multimodal models and intelligent control, reducing errors and improving task completion.


<details>
  <summary>Details</summary>
Motivation: Improve underwater robotics' ability to handle challenges like limited communication, degraded sensing, and hydrodynamic disturbances.

Method: Introduced a dual-brain architecture, Vision-Language-Action (VLA) models with interpretable decision-making, and hydrodynamic-informed Model Predictive Control (MPC).

Result: Experimental field tests displayed reduced navigation errors in degraded conditions and increased task completion rates by 19%-27% compared to traditional methods.

Conclusion: UnderwaterVLA enhances scalability, adaptability, and cost-effectiveness for autonomous underwater vehicles (AUVs), bypassing the need for extensive underwater-specific training data.

Abstract: This paper presents UnderwaterVLA, a novel framework for autonomous
underwater navigation that integrates multimodal foundation models with
embodied intelligence systems. Underwater operations remain difficult due to
hydrodynamic disturbances, limited communication bandwidth, and degraded
sensing in turbid waters. To address these challenges, we introduce three
innovations. First, a dual-brain architecture decouples high-level mission
reasoning from low-level reactive control, enabling robust operation under
communication and computational constraints. Second, we apply
Vision-Language-Action(VLA) models to underwater robotics for the first time,
incorporating structured chain-of-thought reasoning for interpretable
decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)
scheme compensates for fluid effects in real time without costly task-specific
training. Experimental results in field tests show that UnderwaterVLA reduces
navigation errors in degraded visual conditions while maintaining higher task
completion by 19% to 27% over baseline. By minimizing reliance on
underwater-specific training data and improving adaptability across
environments, UnderwaterVLA provides a scalable and cost-effective path toward
the next generation of intelligent AUVs.

</details>


### [465] [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](https://arxiv.org/abs/2509.22469)
*Ben Rossano,Jaein Lim,Jonathan P. How*

Main category: cs.RO

TL;DR: Introduces a market-based task allocation algorithm for heterogeneous robots in uncertain environments, focusing on coupled rewards and effective task handling.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in task allocation for heterogeneous robotic teams in unpredictable environments with uncertain task requirements.

Method: Utilized a probability distribution model for task requirements and applied a market-based algorithm optimized for coupled rewards, ensuring a polynomial-time solution under decentralized settings.

Result: Experiments showcase superior performance compared to benchmark algorithms, emphasizing the algorithm's ability to handle coupled rewards effectively.

Conclusion: The proposed approach empowers heterogeneous robot teams to allocate tasks efficiently, mitigating failures and maximizing joint objectives in decentralized and uncertain scenarios.

Abstract: This paper proposes a task allocation algorithm for teams of heterogeneous
robots in environments with uncertain task requirements. We model these
requirements as probability distributions over capabilities and use this model
to allocate tasks such that robots with complementary skills naturally position
near uncertain tasks, proactively mitigating task failures without wasting
resources. We introduce a market-based approach that optimizes the joint team
objective while explicitly capturing coupled rewards between robots, offering a
polynomial-time solution in decentralized settings with strict communication
assumptions. Comparative experiments against benchmark algorithms demonstrate
the effectiveness of our approach and highlight the challenges of incorporating
coupled rewards in a decentralized formulation.

</details>


### [466] [Ontological foundations for contrastive explanatory narration of robot plans](https://arxiv.org/abs/2509.22493)
*Alberto Olivares-Alarcos,Sergi Foix,JÃºlia BorrÃ s,Gerard Canal,Guillem AlenyÃ *

Main category: cs.RO

TL;DR: The paper introduces methods for robots to explain decisions between competing plans using an ontological model and a novel algorithm for explanation.


<details>
  <summary>Details</summary>
Motivation: To enhance human-robot interaction by enabling robots to make reasonable decisions and effectively explain them to humans.

Method: Developed a novel ontological model to compare competing plans and a new algorithm to generate contrastive explanations of divergent plans.

Result: The novel algorithm outperforms the baseline algorithm in creating explanations, as shown through empirical evaluation.

Conclusion: The approach improves the ability of robots to explain their decision-making processes to humans, fostering better trust and interaction.

Abstract: Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.

</details>


### [467] [HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes](https://arxiv.org/abs/2509.22498)
*Katrina Ashton,Chahyon Ku,Shrey Shah,Wen Jiang,Kostas Daniilidis,Bernadette Bucher*

Main category: cs.RO

TL;DR: The paper proposes HELIOS, a system for language-specified mobile pick-and-place tasks in novel environments, combining multi-layered scene representation and search optimization to handle complex scenarios. It achieves state-of-the-art performance in simulations and demonstrates real-world transferability.


<details>
  <summary>Details</summary>
Motivation: The research aims to solve challenges in mobile manipulation tasks involving partially observed environments, semantic grounding of instructions, and the active update of knowledge through new sensory inputs.

Method: HELIOS employs a hierarchical scene representation combining 2D semantic navigation maps and 3D Gaussian object representations. It fuses multi-view object observations with semantic information into a scalable search objective that balances exploration and exploitation for efficient task execution.

Result: HELIOS delivers state-of-the-art performance on the OVMM benchmark in the Habitat simulator, handling complex environments with precision. It also showcases zero-shot transfer capabilities to real-world scenarios, successfully operating on a Spot robot.

Conclusion: HELIOS addresses the dual challenge of efficient exploration and precise execution in novel environments, achieving high performance in simulation and proving its adaptability for real-world mobile manipulation tasks.

Abstract: Language-specified mobile manipulation tasks in novel environments
simultaneously face challenges interacting with a scene which is only partially
observed, grounding semantic information from language instructions to the
partially observed scene, and actively updating knowledge of the scene with new
observations. To address these challenges, we propose HELIOS, a hierarchical
scene representation and associated search objective to perform language
specified pick and place mobile manipulation tasks. We construct 2D maps
containing the relevant semantic and occupancy information for navigation while
simultaneously actively constructing 3D Gaussian representations of
task-relevant objects. We fuse observations across this multi-layered
representation while explicitly modeling the multi-view consistency of the
detections of each object. In order to efficiently search for the target
object, we formulate an objective function balancing exploration of unobserved
or uncertain regions with exploitation of scene semantic information. We
evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and
place benchmark in which perception is challenging due to large and complex
scenes with comparatively small target objects. HELIOS achieves
state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also
transfer to the real world without requiring additional data, as we illustrate
by demonstrating it in a real world office environment on a Spot robot.

</details>


### [468] [An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment](https://arxiv.org/abs/2509.22550)
*Xiaoyun Qiu,Haichao Liu,Yue Pan,Jun Ma,Xinhu Zheng*

Main category: cs.RO

TL;DR: The paper introduces an intention-driven lane change framework for autonomous vehicles that integrates deep learning-based driving-style recognition, cooperation-aware decision-making, and coordinated motion planning, achieving significant accuracy and performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current lane change methods for autonomous vehicles often oversimplify interactions by assuming uniform driving patterns, despite the complexity and heterogeneity in human-driven vehicle behaviors.

Method: The framework includes: (1) real-time driving-style recognition using a deep learning classifier trained on the NGSIM dataset, (2) a cooperation score for intention estimation, and (3) decision-making using behavior cloning and inverse reinforcement learning, paired with model predictive control for trajectory planning.

Result: The framework achieves 94.2% accuracy and 94.3% F1-score in lane change recognition, outperforming rule-based and learning-based methods by 4-15%.

Conclusion: Modeling inter-driver heterogeneity and incorporating context-aware strategies improves the safety, efficiency, and human-like behavior of autonomous driving in mixed-traffic scenarios.

Abstract: In mixed-traffic environments, where autonomous vehicles (AVs) interact with
diverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous
behaviors make safe and efficient lane change maneuvers highly challenging.
Existing methods often oversimplify these interactions by assuming uniform
patterns. We propose an intention-driven lane change framework that integrates
driving-style recognition, cooperation-aware decision-making, and coordinated
motion planning. A deep learning classifier trained on the NGSIM dataset
identifies human driving styles in real time. A cooperation score with
intrinsic and interactive components estimates surrounding drivers' intentions
and quantifies their willingness to cooperate with the ego vehicle.
Decision-making combines behavior cloning with inverse reinforcement learning
to determine whether a lane change should be initiated. For trajectory
generation, model predictive control is integrated with IRL-based intention
inference to produce collision-free and socially compliant maneuvers.
Experiments show that the proposed model achieves 94.2\% accuracy and 94.3\%
F1-score, outperforming rule-based and learning-based baselines by 4-15\% in
lane change recognition. These results highlight the benefit of modeling
inter-driver heterogeneity and demonstrate the potential of the framework to
advance context-aware and human-like autonomous driving in complex traffic
environments.

</details>


### [469] [MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data](https://arxiv.org/abs/2509.22573)
*Farida Mohsen,Ali Safa*

Main category: cs.RO

TL;DR: This paper introduces an RGB-only pipeline to predict human intent for interaction with robots, overcoming class imbalance and improving performance with novel methods and data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve human-robot interaction (HRI) by addressing inefficiencies in detecting human intent, focusing on frame-level precision and overcoming dataset challenges.

Method: The authors propose MINT-RVAE, a synthetic sequence generation approach, combined with new loss functions and training strategies optimized for RGB-only input.

Result: The approach achieves state-of-the-art performance with an AUROC of 0.95, surpassing previous works (0.90-0.912) while utilizing only RGB input for precise interaction intent prediction.

Conclusion: This research advances HRI by providing a high-performing solution to predict human interaction intent, while also releasing a labeled dataset to encourage further work in this domain.

Abstract: Efficiently detecting human intent to interact with ubiquitous robots is
crucial for effective human-robot interaction (HRI) and collaboration. Over the
past decade, deep learning has gained traction in this field, with most
existing approaches relying on multimodal inputs, such as RGB combined with
depth (RGB-D), to classify time-sequence windows of sensory data as interactive
or non-interactive. In contrast, we propose a novel RGB-only pipeline for
predicting human interaction intent with frame-level precision, enabling faster
robot responses and improved service quality. A key challenge in intent
prediction is the class imbalance inherent in real-world HRI datasets, which
can hinder the model's training and generalization. To address this, we
introduce MINT-RVAE, a synthetic sequence generation method, along with new
loss functions and training strategies that enhance generalization on
out-of-sample data. Our approach achieves state-of-the-art performance (AUROC:
0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB
input and supporting precise frame onset prediction. Finally, to support future
research, we openly release our new dataset with frame-level labeling of human
interaction intent.

</details>


### [470] [EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation](https://arxiv.org/abs/2509.22578)
*Yuan Xu,Jiabing Yang,Xiaofeng Wang,Yixiang Chen,Zheng Zhu,Bowen Fang,Guan Huang,Xinze Chen,Yun Ye,Qiang Zhang,Peiyan Li,Xiangnan Wu,Kai Wang,Bing Zhan,Shuo Lu,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.RO

TL;DR: This paper introduces EgoDemoGen, a framework to address performance degradation in imitation learning-based robotic manipulation under egocentric viewpoint shifts.


<details>
  <summary>Details</summary>
Motivation: Imitation learning-based robotic manipulation policies often fail when trained with data from a single egocentric viewpoint, necessitating a solution for viewpoint robustness.

Method: The authors proposed EgoDemoGen, which generates paired novel egocentric demonstrations through action retargeting, video synthesis via EgoViewTransfer, and a self-supervised double reprojection strategy for training.

Result: Simulation showed absolute improvements of +17.0% and +17.7% in policy success rates for standard and novel viewpoints, respectively. Real-world testing showed +18.3% and +25.8% improvements.

Conclusion: EgoDemoGen offers a practical solution for improving egocentric viewpoint robustness in robotic manipulation, with increasing benefits as training data grows while facing diminishing returns.

Abstract: Imitation learning based policies perform well in robotic manipulation, but
they often degrade under *egocentric viewpoint shifts* when trained from a
single egocentric viewpoint. To address this issue, we present **EgoDemoGen**,
a framework that generates *paired* novel egocentric demonstrations by
retargeting actions in the novel egocentric frame and synthesizing the
corresponding egocentric observation videos with proposed generative video
repair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint
reprojected scene video and a robot-only video rendered from the retargeted
joint actions. EgoViewTransfer is finetuned from a pretrained video generation
model using self-supervised double reprojection strategy. We evaluate
EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After
training with a mixture of EgoDemoGen-generated novel egocentric demonstrations
and original standard egocentric demonstrations, policy success rate improves
**absolutely** by **+17.0%** for standard egocentric viewpoint and by
**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,
the **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,
performance continues to improve as the proportion of EgoDemoGen-generated
demonstrations increases, with diminishing returns. These results demonstrate
that EgoDemoGen provides a practical route to egocentric viewpoint-robust
robotic manipulation.

</details>


### [471] [WoW: Towards a World omniscient World model Through Embodied Interaction](https://arxiv.org/abs/2509.22642)
*Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: This paper introduces WoW, a large generative model trained on robotic interactions to improve AI's physical understanding. It also proposes benchmarks for evaluating physical consistency in video.


<details>
  <summary>Details</summary>
Motivation: Current video models, such as Sora, rely on passive observation and fail to grasp physical causality. The paper aims to establish that physical intuition in AI requires direct interaction with the real world.

Method: The researchers trained WoW, a 14-billion-parameter generative model, using 2 million robot interaction trajectories. They integrated SOPHIA for refining outputs with vision-language agents and employed an Inverse Dynamics Model for translating refined plans into robotic actions.

Result: WoW achieved state-of-the-art performance on WoWBench, a new benchmark for physical consistency and causal reasoning in video, exhibiting strong capabilities in physical causality, collision dynamics, and object permanence.

Conclusion: Large-scale interaction with the real world forms the foundation for developing robust physical intuition in AI systems. The paper advocates for open-sourced models, data, and benchmarks to advance this field.

Abstract: Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.

</details>


### [472] [VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search](https://arxiv.org/abs/2509.22643)
*Wenkai Guo,Guanxing Lu,Haoyuan Deng,Zhenyu Wu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: The paper introduces VLA-Reasoner, a framework to enhance Vision-Language-Action models (VLAs) for better reasoning about long-term trajectories.


<details>
  <summary>Details</summary>
Motivation: VLAs excel at short-term robotic tasks but struggle with long-term trajectories due to incremental errors.

Method: VLA-Reasoner employs a world model to predict future states, integrates Monte Carlo Tree Search for efficiency, and uses a confidence-sampling mechanism for better exploration.

Result: Experiments in simulators and real-world environments show significant improvements in long-term trajectory tasks compared to existing VLAs.

Conclusion: VLA-Reasoner demonstrates a scalable approach for test-time computation in robotic manipulation.

Abstract: Vision-Language-Action models (VLAs) achieve strong performance in general
robotic manipulation tasks by scaling imitation learning. However, existing
VLAs are limited to predicting short-sighted next-action, which struggle with
long-horizon trajectory tasks due to incremental deviations. To address this
problem, we propose a plug-in framework named VLA-Reasoner that effectively
empowers off-the-shelf VLAs with the capability of foreseeing future states via
test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible
action trajectories where involved actions are rationales to generate future
states via a world model, which enables VLA-Reasoner to foresee and reason
potential outcomes and search for the optimal actions. We further leverage
Monte Carlo Tree Search (MCTS) to improve search efficiency in large action
spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a
confidence sampling mechanism based on Kernel Density Estimation (KDE), to
enable efficient exploration in MCTS without redundant VLA queries. We evaluate
intermediate states in MCTS via an offline reward shaping strategy, to score
predicted futures and correct deviations with long-term feedback. We conducted
extensive experiments in both simulators and the real world, demonstrating that
our proposed VLA-Reasoner achieves significant improvements over the
state-of-the-art VLAs. Our method highlights a potential pathway toward
scalable test-time computation of robotic manipulation.

</details>


### [473] [Pixel Motion Diffusion is What We Need for Robot Control](https://arxiv.org/abs/2509.22652)
*E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo*

Main category: cs.RO

TL;DR: DAWN utilizes diffusion processes for language-conditioned robotic manipulation, enabling end-to-end control with interpretable motion representations, achieving state-of-the-art results on benchmarks and reliable real-world transfer.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in bridging high-level intent and low-level robot action, and improving the transferability of robotic manipulation systems between simulation and the real world.

Method: DAWN models both high-level and low-level control as diffusion processes, creating an end-to-end trainable system with pixel motion representation as an interpretable intermediate step.

Result: DAWN achieves superior performance on complex robotic benchmarks like CALVIN and MetaWorld, and demonstrates successful real-world application with minimal fine-tuning required.

Conclusion: Diffusion-based motion abstractions are viable and scalable for robust robotic learning, showing promise for unifying high-level intent and low-level control in robotic systems.

Abstract: We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation. In DAWN, both the high-level and low-level
controllers are modeled as diffusion processes, yielding a fully trainable,
end-to-end system with interpretable intermediate motion abstractions. DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld. Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control. Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning. Project page: https://nero1342.github.io/DAWN/

</details>


### [474] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: This paper proposes SPF, a training-free drone navigation framework leveraging vision-language models to interpret free-form instructions in versatile environments, achieving a new benchmark and strong real-world performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome limitations of current vision-language navigation approaches by proposing a framework that handles dynamic targets and environments using free-form instructions in a training-free setup.

Method: SPF treats UAV action prediction as 2D spatial grounding. It uses vision-language models to annotate waypoints on input images, converts them into 3D displacement vectors, and optimizes distance adaptively for efficient navigation.

Result: SPF achieved a 63% improvement over the previous state of the art in simulation benchmarks and showed significant real-world performance advantages over baseline approaches.

Conclusion: SPF introduces an innovative method that redefines AVLN, demonstrating flexibility, robustness, and generalization across vision-language models. Extensive evaluations validate its effectiveness, setting a new performance standard.

Abstract: We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [475] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: RepoLens addresses the challenge of issue localization in large-scale code repositories by using conceptual knowledge to cluster related functionalities into high-level concerns, enhancing LLM-driven workflows for bug fixing.


<details>
  <summary>Details</summary>
Motivation: To improve issue localization in large-scale repositories, where related logic is often scattered or buried, impairing the effectiveness of existing methods.

Method: RepoLens uses a two-stage process: an offline stage to build a knowledge base from the repository, and an online stage to retrieve, cluster, and rank issue-relevant concerns for integration into LLM prompts.

Result: RepoLens significantly outperforms three state-of-the-art tools, improving accuracy for file- and function-level localization by over 22% and recall by 46%. It also achieves significant performance gains across different LLM models, with up to 504% improvement in Hit@1 and 376% in Recall@10.

Conclusion: RepoLens effectively enhances the ability of LLMs to localize issues by leveraging repository-wide conceptual knowledge, addressing the limitations of concern mixing and scattering in large codebases.

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [476] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: The paper emphasizes challenges faced by women with software engineering backgrounds in re-entering academia after career breaks, comparing academic and industry measures, while aiming for recommendations to improve hiring practices globally.


<details>
  <summary>Details</summary>
Motivation: Women face significant barriers when re-entering academia after breaks due to factors like pregnancy, immigration, or lack of flexible work options. Academia lacks supportive structures compared to industry, driving the need for this research.

Method: The study deploys a cross-country, multidisciplinary research project in multiple universities, analyzing institutional policies, challenges, and practices for women with software engineering backgrounds returning to academic roles.

Result: Findings will highlight comparative challenges women face transitioning back to academia versus industry, considering how institutional policies vary across countries.

Conclusion: Transparent hiring practices and enhanced policies are needed in academia to better support women re-entering research, inspired by insights from cross-national institutional analysis.

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [477] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: This paper introduces a framework for automatically generating Excel tutorials from natural language task descriptions, significantly reducing labor and time costs.


<details>
  <summary>Details</summary>
Motivation: Users often struggle with Excel's complexity, creating a need for tutorials. Current methods for developing these tutorials are labor-intensive, require frequent updates, and are not automated.

Method: The framework converts natural language task descriptions into tutorials. It uses an Execution Agent to plan and perform tasks in Excel, generating artifacts for tutorial construction. These artifacts are turned into structured Excel documents and video demonstrations. A dataset of 1,559 real-world task descriptions was used for validation.

Result: The system improved task execution success rates by 8.5% over existing methods. Generated tutorials matched or exceeded expert quality and were more readable and instructive.

Conclusion: This framework makes scalable and high-quality tutorial generation feasible, eliminating manual effort and reducing costs significantly.

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [478] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: Proposes a domain-specific framework for analyzing diverse software repositories, using layered abstraction and operators.


<details>
  <summary>Details</summary>
Motivation: To enable effective querying, modeling, and integration of heterogeneous software repositories.

Method: Introduces a multi-layered abstraction framework with domain-specific operators to analyze software repositories. Demonstrates its use through a case study.

Result: The framework's potential is showcased through a practical case study, validating its utility for software analytics.

Conclusion: A domain-specific, operator-driven framework enhances the analysis of disparate software repositories, proving its feasibility with a case study.

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [479] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: This paper introduces a dataset, AgentPack, of 1.3M AI-human co-authored code edits from public repositories, showing that models trained on this new dataset outperform those relying on traditional human-only commit data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations in existing datasets for fine-tuning code-editing models, namely noisy and unclear commit messages and edits, as well as the prevalence of bot-generated commits.

Method: The authors constructed the AgentPack corpus by collecting code edits co-authored by AI tools and humans from public GitHub repositories, implementing a pipeline for identification, curation, and analysis.

Result: Models fine-tuned on AgentPack demonstrated superior performance compared to those trained on prior datasets, emphasizing the value of AI-human co-authored edits.

Conclusion: By leveraging public data from software engineering agents, the paper highlights the potential to improve code-editing models through well-curated datasets like AgentPack.

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [480] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: This paper investigates the utility of surrogate models in system configuration tuning and introduces Model4Tune, an automated tool that predicts optimal model-tuner pairs.


<details>
  <summary>Details</summary>
Motivation: To understand the role surrogate models play in system configuration tuning and address uncertainties regarding the importance of model accuracy.

Method: The authors employ fitness landscape analysis, present an alternative theory to model accuracy, conduct a study with 27,000 cases, and design Model4Tune to predict effective model-tuner pairs.

Result: Model4Tune outperformed random guessing in 79%-82% of cases, addressing the challenge of selecting appropriate models for system tuning.

Conclusion: The work offers insight into surrogate model evaluation in configuration tuning and provides practical tools for improving system performance tuning.

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [481] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: The paper introduces "SecureAgentBench," a benchmark of 105 coding tasks aimed at rigorously evaluating the secure code generation capabilities of language model (LLM)-powered code agents. The findings reveal significant challenges in producing secure code.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to assess the security and functionality of code produced by LLM-based code agents, especially given the growing concerns about vulnerabilities in generated code.

Method: They developed SecureAgentBench with 105 coding tasks, incorporating realistic settings, real-world vulnerability contexts, and combined evaluations of functionality, exploit-based vulnerability checking, and static analysis.

Result: Evaluation of three agents (SWE-agent, OpenHands, and Aider) with three LLMs (Claude 3.7 Sonnet, GPT-4.1, DeepSeek-V3.1) revealed that current systems achieve low success in secure solutions, often introducing new vulnerabilities, while security-oriented instructions had limited effect.

Conclusion: The study highlights the inadequacy of current LLM-powered code agents in generating secure code and establishes SecureAgentBench as a critical tool for advancing secure software development.

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [482] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: SK2Decompile proposes a two-phase decompilation process to enhance the translation of binary code into readable and semantically correct source code.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based decompilers struggle to capture both the structure and original identifiers of a program effectively.

Method: The paper introduces a two-phase framework: a Structure Recovery model turns binary code into an Intermediate Representation (IR) with reinforcement learning, and an Identifier Naming model improves the readability and semantics of the code via another reinforcement learning objective.

Result: SK2Decompile achieves a 21.6% higher re-executability rate over GPT-5-mini on HumanEval and a 29.4% R2I improvement over Idioms on GitHub2025.

Conclusion: SK2Decompile demonstrates notable advancements in decompiling binary programs into human-readable and executable source code by combining structure recovery and identifier naming phases, supported by reinforcement learning.

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [483] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: TITAN is an advanced LLM-based framework for MMORPG testing that detects bugs efficiently, achieves high task completion rates, and surpasses existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with the complexity and frequent updates of MMORPGs, and existing LLM-based approaches lack deep reasoning skills.

Method: TITAN combines game state abstraction, action prioritization, long-term reasoning, self-correction mechanisms, and LLM-based bug detection diagnostics.

Result: Experiments show TITAN achieves 95% task completion and identifies previously undetected bugs, outperforming traditional game testing methods.

Conclusion: TITAN represents an impactful step forward in MMORPG testing, showing practicality in real-world QA pipelines and potential for general-purpose advancements.

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [484] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: This paper studies the impact of user-prompt variations on library hallucinations in code generated by large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The research aims to address the risks posed by LLMs hallucinating non-existent libraries, which can mislead developers, cause build failures, and create supply chain vulnerabilities.

Method: The authors systematically evaluate six LLMs for two types of hallucinationsâ€”library name and library member hallucinationsâ€”across different prompt variations derived from realistic user language and errors.

Result: The study finds that small user-prompt variations (e.g., misspellings) can significantly increase hallucination rates, with up to 26% for one-character misspellings, 99% for fake library names, and 84% for time-related prompts.

Conclusion: LLMs are highly fragile to natural prompt variations, requiring stronger safeguards and mitigation strategies to address library-related hallucinations and reduce associated risks.

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [485] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: The paper proposes Green Prompt Engineering, investigating how linguistic complexity in prompts impacts energy usage and model performance, aiming to create sustainable design guidelines under the Green AI initiative.


<details>
  <summary>Details</summary>
Motivation: Address the growing environmental concerns stemming from the computational demands of language model inference, specifically related to prompt design in software engineering.

Method: An empirical study using open-source Small Language Models to analyze how varying the readability levels of prompts affects environmental sustainability and performance in requirement classification tasks.

Result: Readable, simpler prompts can reduce energy consumption while maintaining model performance, highlighting trade-offs between environmental sustainability and accuracy.

Conclusion: Prompt readability is a key factor for energy-efficient model usage, supporting the Green AI agenda by emphasizing sustainable prompt design practices.

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [486] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: The paper introduces a GPU-enhanced Loopy Belief Propagation method that surpasses current tools in speed and flexibility for advanced program analysis.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in large-scale program analysis using Loopy Belief Propagation, especially in scenarios requiring advanced GPU-based optimizations coupled with logical constraints.

Method: The authors create a unified representation for user-defined update strategies, implement a dependency analysis algorithm, and optimize GPU utilization by grouping messages to reduce warp divergence.

Result: Experimental evaluations on real-world Java programs reveal an average speedup of 2.14Ã— over sequential methods and 5.56Ã— over prior GPU implementations, while maintaining high precision.

Conclusion: The proposed GPU-accelerated LBP solution significantly enhances computational efficiency in program analysis, paving the way for broader use cases.

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [487] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: The paper analyzes how simulation-based testing modalities impact the reality gap in evaluating autonomous driving systems and provides insights into their strengths and limitations.


<details>
  <summary>Details</summary>
Motivation: Autonomous Driving Systems (ADS) rely heavily on safe and scalable testing methods, but discrepancies in simulation versus real-world behaviors compromise test reliability. The study aims to address this pressing issue.

Method: The researchers compare four testing modalities (SiL, ViL, MR, real-world) using a small-scale physical vehicle and its digital twin. They evaluate two ADS architectures across various dimensions of the reality gap.

Result: Results indicate MR setups improve perceptual realism while SiL and ViL oversimplify key dynamics. Failures and discrepancies between testing setups are identified along actuation, perception, and behavioral fidelity dimensions.

Conclusion: The study provides actionable insights into strengths and limitations of each testing modality and proposes strategies for robust, transferable validation of autonomous driving systems.

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [488] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: This paper studies the effect of context-specific instruction on bug localization in novices, showing its superiority over abstract guidelines and general steps.


<details>
  <summary>Details</summary>
Motivation: To assess whether providing context-specific instructions enhances the bug localization skills of novices more effectively than abstract or general guidelines.

Method: An eight-week longitudinal study with four groups (no instruction, abstract guidelines, concrete steps, and context-specific instructions), measuring outcomes such as correctness, time to completion, and participant-reported stress and satisfaction across five sessions.

Result: The context-specific instruction group achieved significantly higher correctness rates (80%) and faster completion times (13-15 minutes) compared to other groups, with lower stress and higher satisfaction reported.

Conclusion: Context-specific instruction accelerates skill acquisition, improves performance retention, and bridges the gap between theory and practice in bug localization for novices.

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [489] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: TreeMind integrates large language models (LLMs) with a customized Monte Carlo Tree Search (MCTS) algorithm to effectively reproduce Android app crashes from incomplete textual bug reports. It outperforms state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reproducing Android app crashes from textual bug reports struggle with incomplete reports and the overwhelming complexity of modern UI interactions. Limitations in goal-directed reasoning and navigation exacerbate these challenges.

Method: TreeMind formulates the crash reproduction task as a target-driven search problem, using MCTS as the primary exploration mechanism. It incorporates two distinct LLM-guided agentsâ€”Expander for generating actions and Simulator for predicting success likelihoodâ€”and applies multi-modal inputs with advanced prompting techniques to incrementally reconstruct action sequences.

Result: TreeMind was tested on 93 real-world Android bug reports and achieved a significantly higher reproduction success rate compared to four state-of-the-art baselines, demonstrating its superior performance.

Conclusion: TreeMind's integration of MCTS-based planning with LLM reasoning provides an effective strategy for addressing incomplete bug reports, setting a new standard for automated crash reproduction methodologies.

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [490] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: AFD enhances pointer analysis by modeling custom allocation functions using value-flow analysis and LLMs, improving analysis precision and scalability in C/C++ programs.


<details>
  <summary>Details</summary>
Motivation: The imprecise modeling of heap allocations due to user-defined allocation functions often leads to reduced precision in pointer analysis, particularly affecting C/C++ programs.

Method: AFD combines value-flow analysis for straightforward cases and Large Language Models for complex allocation patterns to automatically identify and model custom allocation functions.

Result: AFD identified more than 600 custom allocation functions across 15 C projects, improved pointer analysis precision by increasing modeled heap objects by 26x, reduced alias set sizes by 39%, and uncovered 17 previously undetected memory bugs with a manageable 1.4x runtime overhead.

Conclusion: Enhancing pointer analysis with precise modeling of custom allocation functions is both scalable and practical for improving software analysis in large systems.

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [491] [Future of Brain Health: From Developmental Insights to Clinical Translation](https://arxiv.org/abs/2509.21332)
*Mariela Chertoff,Martin G Frasch,Eduardo T CÃ¡nepa,Gerlinde A. S. Metz,Marta Cristina Antonelli,Sheehan D. Fisher,Bea R. H. Van den Bergh*

Main category: q-bio.NC

TL;DR: The paper reviews how genetic and environmental factors during development impact brain health, emphasizing critical timeframes of neural plasticity and advocating for resilience-based approaches.


<details>
  <summary>Details</summary>
Motivation: To understand how genetic and environmental interactions, parental influences, and early-life experiences affect long-term cognitive and emotional development.

Method: The study integrates neuroscience, public health, and social context, using frameworks like DOHaD, ACEs, and neurosocial plasticity to explore brain health dynamics.

Result: The paper highlights critical periods of neural plasticity that offer opportunities for targeted interventions to improve mental health across the life span.

Conclusion: Recognizing critical periods allows for strategies that enhance brain function and mental health from fetal development to old age, advocating for lifelong optimization and resilience-oriented approaches.

Abstract: This review highlights brain health as a dynamic process shaped by both
genetic and environmental influences throughout development. Critical periods
provide unique windows of heightened neural plasticity, during which
genetic-environmental interactions and parental influences profoundly impact
brain maturation. Frameworks such as DOHaD, ACEs, and neurosocial plasticity
elucidate how early-life experiences modulate long-term cognitive and emotional
outcomes. Brain health science is emerging as a field integrating neuroscience,
public health, and social context. Resilience-oriented approaches and
predictive processing, offer renewed perspectives on adaptive brain function.
Clinically, understanding critical periods and plasticity spanning from fetal
life to old age, has implications for early detection, targeted interventions,
and resilience-oriented strategies, emphasizing the potential for lifelong
optimization of mental health.

</details>


### [492] [Discovering alternative solutions beyond the simplicity bias in recurrent neural networks](https://arxiv.org/abs/2509.21504)
*William Qian,Cengiz Pehlevan*

Main category: q-bio.NC

TL;DR: This paper presents a method to mitigate the simplicity bias in recurrent neural networks (RNNs) used for neuroscience-style tasks.


<details>
  <summary>Details</summary>
Motivation: Neuroscience-style tasks often require diverse computational models, but RNNs exhibit a strong simplicity bias that limits the variety of generated hypotheses.

Method: The authors introduce Iterative Neural Similarity Deflation (INSD), which penalizes linear predictivity in task-trained RNNs to encourage varied computational solutions.

Result: INSD yields distinct solutions across different metrics, including representational similarity, dynamics, and task-relevant variables, sometimes achieving better performance in complex or out-of-distribution tasks.

Conclusion: The study emphasizes breaking simplicity bias in RNNs to discover richer models of neural computation, potentially enhancing understanding of brain-like mechanisms.

Abstract: Training recurrent neural networks (RNNs) to perform neuroscience-style tasks
has become a popular way to generate hypotheses for how neural circuits in the
brain might perform computations. Recent work has demonstrated that
task-trained RNNs possess a strong simplicity bias. In particular, this
inductive bias often causes RNNs trained on the same task to collapse on
effectively the same solution, typically comprised of fixed-point attractors or
other low-dimensional dynamical motifs. While such solutions are readily
interpretable, this collapse proves counterproductive for the sake of
generating a set of genuinely unique hypotheses for how neural computations
might be performed. Here we propose Iterative Neural Similarity Deflation
(INSD), a simple method to break this inductive bias. By penalizing linear
predictivity of neural activity produced by standard task-trained RNNs, we find
an alternative class of solutions to classic neuroscience-style RNN tasks.
These solutions appear distinct across a battery of analysis techniques,
including representational similarity metrics, dynamical systems analysis, and
the linear decodability of task-relevant variables. Moreover, these alternative
solutions can sometimes achieve superior performance in difficult or
out-of-distribution task regimes. Our findings underscore the importance of
moving beyond the simplicity bias to uncover richer and more varied models of
neural computation.

</details>


### [493] [A Bio-Inspired Minimal Model for Non-Stationary K-Armed Bandits](https://arxiv.org/abs/2509.22209)
*Krubeal Danieli,Mikkel Elle LepperÃ¸d*

Main category: q-bio.NC

TL;DR: The paper presents a biologically-inspired neural model for solving stochastic bandit problems, demonstrating adaptive and competitive performance comparable to standard algorithms.


<details>
  <summary>Details</summary>
Motivation: Standard reinforcement learning approaches lack biological plausibility despite their effectiveness in multi-armed bandit problems.

Method: The paper proposes a neural model inspired by the orbitofrontal and anterior cingulate cortex, optimizing hyperparameters via evolutionary methods.

Result: The model performed robustly across stochastic bandit scenarios, and its hyperparameters aligned with known synaptic mechanisms.

Conclusion: Biologically-inspired architectures can achieve competitive performance and enhance understanding of neural mechanisms in decision-making under uncertainty.

Abstract: While reinforcement learning algorithms have made significant progress in
solving multi-armed bandit problems, they often lack biological plausibility in
architecture and dynamics. Here, we propose a bio-inspired neural model based
on interacting populations of rate neurons, drawing inspiration from the
orbitofrontal cortex and anterior cingulate cortex. Our model reports robust
performance across various stochastic bandit problems, matching the
effectiveness of standard algorithms such as Thompson Sampling and UCB.
Notably, the model exhibits adaptive behavior: employing greedy strategies in
low-uncertainty situations while increasing exploratory behavior as uncertainty
rises. Through evolutionary optimization, the model's hyperparameters converged
to values that align with known synaptic mechanisms, particularly in terms of
synapse-dependent neural activity and learning rate adaptation. These findings
suggest that biologically-inspired computational architectures can achieve
competitive performance while providing insights into neural mechanisms of
decision-making under uncertainty.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [494] [Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models](https://arxiv.org/abs/2509.21423)
*Ehsan Sharifian,Saber Salehkaleybar,Negar Kiyavash*

Main category: stat.ML

TL;DR: The paper addresses causal structure learning in linear non-Gaussian models with cycles, introducing an optimal experiment design framework for interventions using stochastic optimization.


<details>
  <summary>Details</summary>
Motivation: Efficiently determine causal structures from observational and interventional data, especially in cases where only a permutation-equivalence class can be identified with observational data alone.

Method: The authors represent permutation-equivalence classes as perfect matchings in bipartite graphs, formalize intervention design as an adaptive stochastic optimization problem, and develop a greedy near-optimal policy aided by a sampling-based reward estimator.

Result: The proposed framework efficiently identifies the true causal structure using a minimal number of interventions, as demonstrated through simulations.

Conclusion: By combining interventions and the bipartite graph representation, the paper offers a practical and theoretically grounded approach to recover causal graphs in complex scenarios.

Abstract: We study the problem of causal structure learning from a combination of
observational and interventional data generated by a linear non-Gaussian
structural equation model that might contain cycles. Recent results show that
using mere observational data identifies the causal graph only up to a
permutation-equivalence class. We obtain a combinatorial characterization of
this class by showing that each graph in an equivalence class corresponds to a
perfect matching in a bipartite graph. This bipartite representation allows us
to analyze how interventions modify or constrain the matchings. Specifically,
we show that each atomic intervention reveals one edge of the true matching and
eliminates all incompatible causal graphs. Consequently, we formalize the
optimal experiment design task as an adaptive stochastic optimization problem
over the set of equivalence classes with a natural reward function that
quantifies how many graphs are eliminated from the equivalence class by an
intervention. We show that this reward function is adaptive submodular and
provide a greedy policy with a provable near-optimal performance guarantee. A
key technical challenge is to efficiently estimate the reward function without
having to explicitly enumerate all the graphs in the equivalence class. We
propose a sampling-based estimator using random matchings and analyze its bias
and concentration behavior. Our simulation results show that performing a small
number of interventions guided by our stochastic optimization framework
recovers the true underlying causal structure.

</details>


### [495] [General Pruning Criteria for Fast SBL](https://arxiv.org/abs/2509.21572)
*Jakob MÃ¶derl,Erik Leitinger,Bernard Henri Fleury*

Main category: stat.ML

TL;DR: The study explores Sparse Bayesian Learning (SBL), focusing on how hyperparameter estimates influence model sparsity, particularly under weakened Gaussian assumptions.


<details>
  <summary>Details</summary>
Motivation: To provide deeper insights into the behavior of SBL under weakened Gaussian assumptions and to understand the conditions leading to sparsity in the estimated weight vector.

Method: The researchers analyzed the marginal likelihood as a function of a single hyperparameter, keeping other hyperparameters fixed. They derived sufficient conditions for finite and infinite hyperparameter estimates.

Result: The study identified conditions that result in either finite or infinite hyperparameter estimates. In the Gaussian case, these conditions align with the pruning mechanism of the fast Sparse Bayesian Learning (F-SBL) algorithm.

Conclusion: This work enhances understanding of SBL, especially in cases involving weakened Gaussian assumptions, and offers insights into the pruning condition of the F-SBL algorithm.

Abstract: Sparse Bayesian learning (SBL) associates to each weight in the underlying
linear model a hyperparameter by assuming that each weight is Gaussian
distributed with zero mean and precision (inverse variance) equal to its
associated hyperparameter. The method estimates the hyperparameters by
marginalizing out the weights and performing (marginalized) maximum likelihood
(ML) estimation. SBL returns many hyperparameter estimates to diverge to
infinity, effectively setting the estimates of the corresponding weights to
zero (i.e., pruning the corresponding weights from the model) and thereby
yielding a sparse estimate of the weight vector.
  In this letter, we analyze the marginal likelihood as function of a single
hyperparameter while keeping the others fixed, when the Gaussian assumptions on
the noise samples and the weight distribution that underlies the derivation of
SBL are weakened. We derive sufficient conditions that lead, on the one hand,
to finite hyperparameter estimates and, on the other, to infinite ones.
Finally, we show that in the Gaussian case, the two conditions are
complementary and coincide with the pruning condition of fast SBL (F-SBL),
thereby providing additional insights into this algorithm.

</details>


### [496] [IndiSeek learns information-guided disentangled representations](https://arxiv.org/abs/2509.21584)
*Yu Gui,Cong Ma,Zongming Ma*

Main category: stat.ML

TL;DR: This paper introduces "IndiSeek," a novel approach for disentangled representation learning addressing challenges in multi-modal data, balancing independence and completeness of shared and modality-specific features.


<details>
  <summary>Details</summary>
Motivation: The paper aims at addressing the fundamental problem of learning disentangled representations for multi-modal data, where distinguishing shared and modality-specific features is crucial for downstream tasks like analyzing cell states in single-cell multi-omics.

Method: IndiSeek combines an independence-enforcing objective with a computationally efficient reconstruction loss to bound conditional mutual information. This balance ensures principled extraction of modality-specific and shared features.

Result: The authors validated IndiSeek's effectiveness on synthetic simulations, a CITE-seq dataset, and several real-world multi-modal benchmarks, demonstrating better disentanglement and representation learning.

Conclusion: IndiSeek provides a principled and efficient method to learn disentangled and interpretable representations in multi-modal data, overcoming limitations of mutual information estimations in prior methods.

Abstract: Learning disentangled representations is a fundamental task in multi-modal
learning. In modern applications such as single-cell multi-omics, both shared
and modality-specific features are critical for characterizing cell states and
supporting downstream analyses. Ideally, modality-specific features should be
independent of shared ones while also capturing all complementary information
within each modality. This tradeoff is naturally expressed through
information-theoretic criteria, but mutual-information-based objectives are
difficult to estimate reliably, and their variational surrogates often
underperform in practice. In this paper, we introduce IndiSeek, a novel
disentangled representation learning approach that addresses this challenge by
combining an independence-enforcing objective with a computationally efficient
reconstruction loss that bounds conditional mutual information. This
formulation explicitly balances independence and completeness, enabling
principled extraction of modality-specific features. We demonstrate the
effectiveness of IndiSeek on synthetic simulations, a CITE-seq dataset and
multiple real-world multi-modal benchmarks.

</details>


### [497] [Effective continuous equations for adaptive SGD: a stochastic analysis view](https://arxiv.org/abs/2509.21614)
*Luca Callisti,Marco Romito,Francesco Triggiano*

Main category: stat.ML

TL;DR: This paper provides a theoretical analysis of adaptive SGD methods, focusing on stochastic dynamics in the small learning rate regime.


<details>
  <summary>Details</summary>
Motivation: To understand how noise and hyperparameter scaling rules influence the dynamics of adaptive SGD methods.

Method: The paper uses the 'stochastic modified equations' framework and derives stochastic dynamics for adaptive SGD methods. It extends prior work to analyze the relationship between learning rates and hyperparameters.

Result: Noise in SGD is represented as independent Brownian motions affecting parameter and gradient momentum in the limit. Scaling rules between learning rate and hyperparameters for adaptive methods are characterized.

Conclusion: Adaptive SGD methods exhibit specific limiting dynamics due to noise and scaling phenomena, and the analysis deepens the understanding of their behavior in small learning rate regimes.

Abstract: We present a theoretical analysis of some popular adaptive Stochastic
Gradient Descent (SGD) methods in the small learning rate regime. Using the
stochastic modified equations framework introduced by Li et al., we derive
effective continuous stochastic dynamics for these methods. Our key
contribution is that sampling-induced noise in SGD manifests in the limit as
independent Brownian motions driving the parameter and gradient second momentum
evolutions. Furthermore, extending the approach of Malladi et al., we
investigate scaling rules between the learning rate and key hyperparameters in
adaptive methods, characterising all non-trivial limiting dynamics.

</details>


### [498] [SADA: Safe and Adaptive Inference with Multiple Black-Box Predictions](https://arxiv.org/abs/2509.21707)
*Jiawei Shan,Yiming Dong,Jiwei Zhao*

Main category: stat.ML

TL;DR: The paper introduces a method to aggregate multiple machine learning predictions safely while ensuring valid statistical inference, demonstrated through experiments.


<details>
  <summary>Details</summary>
Motivation: Due to high labeling costs, there is often a scarcity of labeled data, leading to challenges in training models effectively. However, unlabeled data is abundant, and various models can generate predicted labels. A reliable way to combine these predictions is needed.

Method: The authors propose an adaptive aggregation method that leverages multiple black-box predictions of unknown quality while maintaining statistical validity. The method ensures it never underperforms the labeled data alone and adapts to predictions that align perfectly with the ground truth.

Result: The proposed algorithm outperforms benchmarks and is demonstrated to be effective using both synthetic and real-world datasets.

Conclusion: The method provides a statistically robust and adaptive solution for combining predictions, ensuring efficiency and reliability, even with diverse and unknown-quality predictions.

Abstract: Real-world applications often face scarce labeled data due to the high cost
and time requirements of gold-standard experiments, whereas unlabeled data are
typically abundant. With the growing adoption of machine learning techniques,
it has become increasingly feasible to generate multiple predicted labels using
a variety of models and algorithms, including deep learning, large language
models, and generative AI. In this paper, we propose a novel approach that
safely and adaptively aggregates multiple black-box predictions with unknown
quality while preserving valid statistical inference. Our method provides two
key guarantees: (i) it never performs worse than using the labeled data alone,
regardless of the quality of the predictions; and (ii) if any one of the
predictions (without knowing which one) perfectly fits the ground truth, the
algorithm adaptively exploits this to achieve either a faster convergence rate
or the semiparametric efficiency bound. We demonstrate the effectiveness of the
proposed algorithm through experiments on both synthetic and benchmark
datasets.

</details>


### [499] [Multi-modal Bayesian Neural Network Surrogates with Conjugate Last-Layer Estimation](https://arxiv.org/abs/2509.21711)
*Ian Taylor,Juliane Mueller,Julie Bessac*

Main category: stat.ML

TL;DR: The paper proposes multi-modal Bayesian neural network surrogate models, improving prediction accuracy and uncertainty quantification over uni-modal models for scalar and time series data.


<details>
  <summary>Details</summary>
Motivation: Multi-modal learning is increasingly crucial due to advancements in data collection and simulation capabilities, focusing on applications like optimization and sensitivity analysis.

Method: Two multi-modal Bayesian neural network surrogate models with conditionally conjugate distributions in the last layer are developed, utilizing stochastic variational inference to address partially missing observations.

Result: The models showed superior prediction accuracy and uncertainty quantification compared to uni-modal surrogate models.

Conclusion: Multi-modal surrogate models are effective and beneficial for complex data contexts like scalar and time series.

Abstract: As data collection and simulation capabilities advance, multi-modal learning,
the task of learning from multiple modalities and sources of data, is becoming
an increasingly important area of research. Surrogate models that learn from
data of multiple auxiliary modalities to support the modeling of a highly
expensive quantity of interest have the potential to aid outer loop
applications such as optimization, inverse problems, or sensitivity analyses
when multi-modal data are available. We develop two multi-modal Bayesian neural
network surrogate models and leverage conditionally conjugate distributions in
the last layer to estimate model parameters using stochastic variational
inference (SVI). We provide a method to perform this conjugate SVI estimation
in the presence of partially missing observations. We demonstrate improved
prediction accuracy and uncertainty quantification compared to uni-modal
surrogate models for both scalar and time series data.

</details>


### [500] [Causal-EPIG: A Prediction-Oriented Active Learning Framework for CATE Estimation](https://arxiv.org/abs/2509.21866)
*Erdun Gao,Jake Fawkes,Dino Sejdinovic*

Main category: stat.ML

TL;DR: The paper introduces a new approach for active learning in estimating the Conditional Average Treatment Effect (CATE) by focusing directly on causal objectives and developing a framework called Causal-EPIG for more sample-efficient estimation.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the high cost of obtaining outcome measurements for CATE estimation, and the recognition that traditional active learning strategies do not directly address the unobservable causal quantities that are of primary interest.

Method: The authors propose the Causal-EPIG framework, utilizing the Expected Predictive Information Gain to quantify the value of a query for reducing uncertainty about unobservable causal quantities. They derive two strategies: one modeling the full causal mechanisms via joint potential outcomes and another targeting the CATE estimand directly.

Result: The proposed strategies outperform standard baseline methods in experiments, showing that the optimal strategy depends on the base estimator and data complexity.

Conclusion: The Causal-EPIG framework offers a context-dependent guide for more sample-efficient CATE estimation, advancing the alignment between active learning objectives and causal inference tasks.

Abstract: Estimating the Conditional Average Treatment Effect (CATE) is often
constrained by the high cost of obtaining outcome measurements, making active
learning essential. However, conventional active learning strategies suffer
from a fundamental objective mismatch. They are designed to reduce uncertainty
in model parameters or in observable factual outcomes, failing to directly
target the unobservable causal quantities that are the true objects of
interest. To address this misalignment, we introduce the principle of causal
objective alignment, which posits that acquisition functions should target
unobservable causal quantities, such as the potential outcomes and the CATE,
rather than indirect proxies. We operationalize this principle through the
Causal-EPIG framework, which adapts the information-theoretic criterion of
Expected Predictive Information Gain (EPIG) to explicitly quantify the value of
a query in terms of reducing uncertainty about unobservable causal quantities.
From this unified framework, we derive two distinct strategies that embody a
fundamental trade-off: a comprehensive approach that robustly models the full
causal mechanisms via the joint potential outcomes, and a focused approach that
directly targets the CATE estimand for maximum sample efficiency. Extensive
experiments demonstrate that our strategies consistently outperform standard
baselines, and crucially, reveal that the optimal strategy is
context-dependent, contingent on the base estimator and data complexity. Our
framework thus provides a principled guide for sample-efficient CATE estimation
in practice.

</details>


### [501] [Sequential 1-bit Mean Estimation with Near-Optimal Sample Complexity](https://arxiv.org/abs/2509.21940)
*Ivan Lau,Jonathan Scarlett*

Main category: stat.ML

TL;DR: The study proposes an adaptive mean estimator using 1-bit interval queries for distributed mean estimation, presenting tight sample complexity bounds and addressing various constraints.


<details>
  <summary>Details</summary>
Motivation: Exploring efficient techniques for distributed mean estimation under 1-bit communication constraints, motivated by practical needs for reducing communication overhead in distributed systems.

Method: An adaptive estimator using randomized interval queries to determine whether data samples fall within specific intervals, supported by rigorous theoretical bounds.

Result: Achieved tight sample complexity bounds matching minimax lower bounds (up to factors), highlighted adaptivity advantages, and extended results for distributions with stronger tail decay.

Conclusion: Adaptive interval-query methods outperform non-adaptive estimators under certain conditions, providing versatile solutions for diverse constraints.

Abstract: In this paper, we study the problem of distributed mean estimation with 1-bit
communication constraints. We propose a mean estimator that is based on
(randomized and sequentially-chosen) interval queries, whose 1-bit outcome
indicates whether the given sample lies in the specified interval. Our
estimator is $(\epsilon, \delta)$-PAC for all distributions with bounded mean
($-\lambda \le \mathbb{E}(X) \le \lambda $) and variance ($\mathrm{Var}(X) \le
\sigma^2$) for some known parameters $\lambda$ and $\sigma$. We derive a sample
complexity bound $\widetilde{O}\big(
\frac{\sigma^2}{\epsilon^2}\log\frac{1}{\delta} +
\log\frac{\lambda}{\sigma}\big)$, which matches the minimax lower bound for the
unquantized setting up to logarithmic factors and the additional
$\log\frac{\lambda}{\sigma}$ term that we show to be unavoidable. We also
establish an adaptivity gap for interval-query based estimators: the best
non-adaptive mean estimator is considerably worse than our adaptive mean
estimator for large $\frac{\lambda}{\sigma}$. Finally, we give tightened sample
complexity bounds for distributions with stronger tail decay, and present
additional variants that (i) handle an unknown sampling budget (ii) adapt to
the unknown true variance given (possibly loose) upper and lower bounds on the
variance, and (iii) use only two stages of adaptivity at the expense of more
complicated (non-interval) queries.

</details>


### [502] [A Nonparametric Discrete Hawkes Model with a Collapsed Gaussian-Process Prior](https://arxiv.org/abs/2509.21996)
*Trinnhallen Brisley,Gordon Ross,Daniel Paulin*

Main category: stat.ML

TL;DR: The paper introduces the Gaussian Process Discrete Hawkes Process (GP-DHP), a novel, nonparametric approach for discrete-time Hawkes process modeling, which addresses flexibility and interpretability challenges.


<details>
  <summary>Details</summary>
Motivation: Discrete-time Hawkes process models are underutilized in applications involving event counts, partly due to the rigidity of fixed-form baselines and excitation kernels. There is a need for more flexible, nonparametric models to address this limitation.

Method: The authors propose GP-DHP, which uses Gaussian process priors for baseline and excitation functions, and employs a collapsed latent representation to achieve MAP estimation with near-linear time complexity. It incorporates closed-form projections for interpretability.

Result: In simulations and case studies (including U.S. terrorism and Cryptosporidiosis data), GP-DHP demonstrated improved predictive log-likelihood compared to parametric models, successfully capturing complex patterns such as bursts, delays, and seasonal variations.

Conclusion: GP-DHP provides a flexible, scalable, and interpretable nonparametric framework for discrete-time Hawkes processes, expanding their application in practical settings.

Abstract: Hawkes process models are used in settings where past events increase the
likelihood of future events occurring. Many applications record events as
counts on a regular grid, yet discrete-time Hawkes models remain comparatively
underused and are often constrained by fixed-form baselines and excitation
kernels. In particular, there is a lack of flexible, nonparametric treatments
of both the baseline and the excitation in discrete time. To this end, we
propose the Gaussian Process Discrete Hawkes Process (GP-DHP), a nonparametric
framework that places Gaussian process priors on both the baseline and the
excitation and performs inference through a collapsed latent representation.
This yields smooth, data-adaptive structure without prespecifying trends,
periodicities, or decay shapes, and enables maximum a posteriori (MAP)
estimation with near-linear-time \(O(T\log T)\) complexity. A closed-form
projection recovers interpretable baseline and excitation functions from the
optimized latent trajectory. In simulations, GP-DHP recovers diverse excitation
shapes and evolving baselines. In case studies on U.S. terrorism incidents and
weekly Cryptosporidiosis counts, it improves test predictive log-likelihood
over standard parametric discrete Hawkes baselines while capturing bursts,
delays, and seasonal background variation. The results indicate that flexible
discrete-time self-excitation can be achieved without sacrificing scalability
or interpretability.

</details>


### [503] [A Random Matrix Perspective of Echo State Networks: From Precise Bias--Variance Characterization to Optimal Regularization](https://arxiv.org/abs/2509.22011)
*Yessin Moakher,Malik Tiomoko,Cosme Louart,Zhenyu Liao*

Main category: stat.ML

TL;DR: The paper conducts asymptotic analysis of Echo State Networks (ESNs) to derive expressions for bias, variance, and mean-squared error, offering insights into their behavior and tuning recommendations.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous theoretical insights into the performance of Echo State Networks (ESNs), especially considering recent empirical observations and the lack of detailed analysis compared to traditional methods like ridge regression.

Method: Using random matrix theory, the authors establish formulas for asymptotic bias, variance, and MSE, and analyze ESNs under specific scenarios, proposing optimal regularization strategies.

Result: The findings show that ESNs diverge from ridge regression norms (no double descent) and perform better in cases with limited training and memory length. Explicit formulas and numerical schemes aid in identifying optimal regularization.

Conclusion: The study bridges empirical observations with provable guarantees, offering theoretical and practical guides for ESN performance optimization and advancing their understanding in machine learning contexts.

Abstract: We present a rigorous asymptotic analysis of Echo State Networks (ESNs) in a
teacher student setting with a linear teacher with oracle weights. Leveraging
random matrix theory, we derive closed form expressions for the asymptotic
bias, variance, and mean-squared error (MSE) as functions of the input
statistics, the oracle vector, and the ridge regularization parameter. The
analysis reveals two key departures from classical ridge regression: (i) ESNs
do not exhibit double descent, and (ii) ESNs attain lower MSE when both the
number of training samples and the teacher memory length are limited. We
further provide an explicit formula for the optimal regularization in the
identity input covariance case, and propose an efficient numerical scheme to
compute the optimum in the general case. Together, these results offer
interpretable theory and practical guidelines for tuning ESNs, helping
reconcile recent empirical observations with provable performance guarantees

</details>


### [504] [Incorporating priors in learning: a random matrix study under a teacher-student framework](https://arxiv.org/abs/2509.22124)
*Malik Tiomoko,Ekkehard Schnoor*

Main category: stat.ML

TL;DR: This paper analyzes high-dimensional regularized linear regression with Gaussian priors and provides exact asymptotic training and test risks using random matrix theory.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of high-dimensional behavior in regularized linear regression with domain-informed Gaussian priors.

Method: Using random matrix theory, the authors derive closed-form formulas for training and test risks under Gaussian priors.

Result: The paper presents a unified framework for ridge regression, least squares, and prior-informed estimators, and quantifies concepts like bias, variance, double descent, and prior mismatch.

Conclusion: The findings offer a clear and practical guide to learning with structured priors, optimizing regularization parameters grounded on Bayesian and frequentist principles.

Abstract: Regularized linear regression is central to machine learning, yet its
high-dimensional behavior with informative priors remains poorly understood. We
provide the first exact asymptotic characterization of training and test risks
for maximum a posteriori (MAP) regression with Gaussian priors centered at a
domain-informed initialization. Our framework unifies ridge regression, least
squares, and prior-informed estimators, and -- using random matrix theory --
yields closed-form risk formulas that expose the bias-variance-prior tradeoff,
explain double descent, and quantify prior mismatch. We also identify a
closed-form minimizer of test risk, enabling a simple estimator of the optimal
regularization parameter. Simulations confirm the theory with high accuracy. By
connecting Bayesian priors, classical regularization, and modern asymptotics,
our results provide both conceptual clarity and practical guidance for learning
with structured prior knowledge.

</details>


### [505] [Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression](https://arxiv.org/abs/2509.22341)
*Anvit Garg,Sohom Bhattacharya,Pragya Sur*

Main category: stat.ML

TL;DR: The paper studies model collapse in generative models by analyzing overparameterized linear regression and deriving generalization error for iterative training that combines real and synthetic data. It identifies the optimal mixing ratio of real and synthetic data to prevent model collapse.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate model collapse in generative setups where models degrade by repeatedly training on their self-generated outputs.

Method: The paper provides a theoretical analysis of minimum-â„“2-norm interpolation and ridge regression in iterative training, deriving precise generalization error formulae and finding optimal real-data-to-synthetic-data mixing ratios.

Result: The theoretical analysis shows that the optimal real-data proportion for minimum-â„“2-norm interpolation converges to the reciprocal of the golden ratio, and for ridge regression, the optimal ratio depends on the spectral geometry of the model and is at least 50% real data.

Conclusion: The findings highlight the importance of favoring real data over synthetic data to prevent model collapse and provide guidance on mixing ratios for various models.

Abstract: Model collapse occurs when generative models degrade after repeatedly
training on their own synthetic outputs. We study this effect in
overparameterized linear regression in a setting where each iteration mixes
fresh real labels with synthetic labels drawn from the model fitted in the
previous iteration. We derive precise generalization error formulae for
minimum-$\ell_2$-norm interpolation and ridge regression under this iterative
scheme. Our analysis reveals intriguing properties of the optimal mixing weight
that minimizes long-term prediction error and provably prevents model collapse.
For instance, in the case of min-$\ell_2$-norm interpolation, we establish that
the optimal real-data proportion converges to the reciprocal of the golden
ratio for fairly general classes of covariate distributions. Previously, this
property was known only for ordinary least squares, and additionally in low
dimensions. For ridge regression, we further analyze two popular model classes
-- the random-effects model and the spiked covariance model -- demonstrating
how spectral geometry governs optimal weighting. In both cases, as well as for
isotropic features, we uncover that the optimal mixing ratio should be at least
one-half, reflecting the necessity of favoring real-data over synthetic. We
validate our theoretical results with extensive simulations.

</details>


### [506] [Multidimensional Uncertainty Quantification via Optimal Transport](https://arxiv.org/abs/2509.22380)
*Nikita Kotelevskii,Maiya Goloburda,Vladimir Kondratyev,Alexander Fishkov,Mohsen Guizani,Eric Moulines,Maxim Panov*

Main category: stat.ML

TL;DR: This paper introduces VecUQ-OT, a multidimensional uncertainty quantification method combining complementary measures into vectors and using optimal transport-based ranking for robust predictions.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification methods typically rely on single scalar values, which may overlook complementary information captured by different uncertainty measures. This paper aims to exploit the potential of combining multiple measures for improved prediction confidence assessment.

Method: The paper proposes a vector-based approach called VecUQ-OT. It combines multiple complementary uncertainty measures into vectors and assigns Monge-Kantorovich ranks using entropy-regularized optimal transport. The framework applies these rankings to unseen data without requiring retraining.

Result: VecUQ-OT demonstrates robustness and utility in various tasks, including selective prediction, misclassification detection, and out-of-distribution detection, across synthetic, image, and text datasets. It performs effectively even when individual measures fail.

Conclusion: By incorporating multiple uncertainty measures, VecUQ-OT provides a robust framework for multidimensional uncertainty quantification, supporting a wide array of tasks efficiently. Its utility is validated across different datasets and scenarios.

Abstract: Most uncertainty quantification (UQ) approaches provide a single scalar value
as a measure of model reliability. However, different uncertainty measures
could provide complementary information on the prediction confidence. Even
measures targeting the same type of uncertainty (e.g., ensemble-based and
density-based measures of epistemic uncertainty) may capture different failure
modes.
  We take a multidimensional view on UQ by stacking complementary UQ measures
into a vector. Such vectors are assigned with Monge-Kantorovich ranks produced
by an optimal-transport-based ordering method. The prediction is then deemed
more uncertain than the other if it has a higher rank.
  The resulting VecUQ-OT algorithm uses entropy-regularized optimal transport.
The transport map is learned on vectors of scores from in-distribution data
and, by design, applies to unseen inputs, including out-of-distribution cases,
without retraining.
  Our framework supports flexible non-additive uncertainty fusion (including
aleatoric and epistemic components). It yields a robust ordering for downstream
tasks such as selective prediction, misclassification detection,
out-of-distribution detection, and selective generation. Across synthetic,
image, and text data, VecUQ-OT shows high efficiency even when individual
measures fail. The code for the method is available at:
https://github.com/stat-ml/multidimensional_uncertainty.

</details>


### [507] [Universal Inverse Distillation for Matching Models with Real-Data Supervision (No GANs)](https://arxiv.org/abs/2509.22459)
*Nikita Kornilov,David Li,Tikhon Mavrin,Aleksei Leonov,Nikita Gushchin,Evgeny Burnaev,Iaroslav Koshelev,Alexander Korotin*

Main category: stat.ML

TL;DR: RealUID proposes a universal distillation framework to accelerate inference of generative models while integrating real data into the distillation process without requiring GANs.


<details>
  <summary>Details</summary>
Motivation: Modern generative models like diffusion and flow deliver excellent results but face slow inference due to multi-step iterative processes. Existing distillation methods for efficiency are model-specific or require adversarial setups with GANs to incorporate real data.

Method: The paper introduces RealUID, a unified distillation approach that incorporates real-world data directly into the distillation process without complex adversarial mechanisms. The method provides a theoretical foundation applicable to diverse generative frameworks, including Flow Matching and Diffusion models.

Result: RealUID improves compatibility across different generative modeling frameworks, simplifies the training process, and allows for efficient one-step generation using real data without the need for a discriminator.

Conclusion: RealUID represents a more universal, efficient, and theoretically grounded distillation framework that simplifies generative model inference while effectively utilizing real data, extending its applicability across various model architectures.

Abstract: While achieving exceptional generative quality, modern diffusion, flow, and
other matching models suffer from slow inference, as they require many steps of
iterative generation. Recent distillation methods address this by training
efficient one-step generators under the guidance of a pre-trained teacher
model. However, these methods are often constrained to only one specific
framework, e.g., only to diffusion or only to flow models. Furthermore, these
methods are naturally data-free, and to benefit from the usage of real data, it
is required to use an additional complex adversarial training with an extra
discriminator model. In this paper, we present RealUID, a universal
distillation framework for all matching models that seamlessly incorporates
real data into the distillation procedure without GANs. Our RealUID approach
offers a simple theoretical foundation that covers previous distillation
methods for Flow Matching and Diffusion models, and is also extended to their
modifications, such as Bridge Matching and Stochastic Interpolants.

</details>


### [508] [CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks](https://arxiv.org/abs/2509.22467)
*Alejandro AlmodÃ³var,Patricia A. ApellÃ¡niz,Santiago Zazo,Juan Parras*

Main category: stat.ML

TL;DR: The paper proposes causalKANs, a framework that transforms neural estimators of CATEs into interpretable models, preserving accuracy while offering closed-form formulas and plots.


<details>
  <summary>Details</summary>
Motivation: Deep learning's opacity limits its adoption in sensitive areas like medicine and policy, despite its strong performance in estimating treatment effects.

Method: The causalKANs framework converts neural models into Kolmogorov--Arnold Networks using pruning and symbolic simplification to improve interpretability.

Result: CausalKANs are shown to maintain predictive accuracy on benchmark datasets while offering interpretable models, with simple variants achieving competitive performance.

Conclusion: CausalKANs bridge the gap between performance and interpretability, enabling auditable and trustworthy decision-making in critical applications.

Abstract: Deep neural networks achieve state-of-the-art performance in estimating
heterogeneous treatment effects, but their opacity limits trust and adoption in
sensitive domains such as medicine, economics, and public policy. Building on
well-established and high-performing causal neural architectures, we propose
causalKANs, a framework that transforms neural estimators of conditional
average treatment effects (CATEs) into Kolmogorov--Arnold Networks (KANs). By
incorporating pruning and symbolic simplification, causalKANs yields
interpretable closed-form formulas while preserving predictive accuracy.
Experiments on benchmark datasets demonstrate that causalKANs perform on par
with neural baselines in CATE error metrics, and that even simple KAN variants
achieve competitive performance, offering a favorable
accuracy--interpretability trade-off. By combining reliability with analytic
accessibility, causalKANs provide auditable estimators supported by closed-form
expressions and interpretable plots, enabling trustworthy individualized
decision-making in high-stakes settings. We release the code for
reproducibility at https://github.com/aalmodovares/causalkans .

</details>


### [509] [Smoothing-Based Conformal Prediction for Balancing Efficiency and Interpretability](https://arxiv.org/abs/2509.22529)
*Mingyi Zheng,Hongyu Jiang,Yizhou Lu,Jiaye Teng*

Main category: stat.ML

TL;DR: The paper introduces SCD-split to improve conformal prediction (CP) via smoothing, enhancing interpretability without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the interpretability limitations of conformal prediction (CP), where disconnected prediction sets can be hard to analyze and use.

Method: The proposed method (SCD-split) incorporates smoothing operations into conformal prediction, merging subintervals into more coherent sets.

Result: Experiments on synthetic and real-world datasets show that SCD-split reduces disconnected subintervals while keeping interval length and coverage guarantees nearly the same as CD-split.

Conclusion: SCD-split achieves a balance between interpretability and prediction accuracy, offering provable advantages over existing methods.

Abstract: Conformal Prediction (CP) is a distribution-free framework for constructing
statistically rigorous prediction sets. While popular variants such as CD-split
improve CP's efficiency, they often yield prediction sets composed of multiple
disconnected subintervals, which are difficult to interpret. In this paper, we
propose SCD-split, which incorporates smoothing operations into the CP
framework. Such smoothing operations potentially help merge the subintervals,
thus leading to interpretable prediction sets. Experimental results on both
synthetic and real-world datasets demonstrate that SCD-split balances the
interval length and the number of disconnected subintervals. Theoretically,
under specific conditions, SCD-split provably reduces the number of
disconnected subintervals while maintaining comparable coverage guarantees and
interval length compared with CD-split.

</details>


### [510] [Debiased Front-Door Learners for Heterogeneous Effects](https://arxiv.org/abs/2509.22531)
*Yonghan Jung*

Main category: stat.ML

TL;DR: The paper introduces debiased learners for estimating heterogeneous treatment effects (HTE) under front-door adjustment, ensuring fast, reliable, and efficient causal inference with robust empirical performance.


<details>
  <summary>Details</summary>
Motivation: To address scenarios where treatment and outcome share unmeasured confounders but an unconfounded mediator enables causal effect identification, and improve HTE estimation methods under front-door identification.

Method: Proposing two debiased learners, FD-DR-Learner and FD-R-Learner, achieving quasi-oracle rates with error analyses confirming their reliability and sample efficiency.

Result: Synthetic and real-world studies (using FARS dataset) demonstrated the robust and practical performance of the proposed learners in estimating HTE in front-door scenarios.

Conclusion: The introduced learners provide reliable, efficient, and robust methods for HTE estimation in observational settings with unmeasured confounders but an unconfounded mediator, advancing causal inference techniques.

Abstract: In observational settings where treatment and outcome share unmeasured
confounders but an observed mediator remains unconfounded, the front-door (FD)
adjustment identifies causal effects through the mediator. We study the
heterogeneous treatment effect (HTE) under FD identification and introduce two
debiased learners: FD-DR-Learner and FD-R-Learner. Both attain fast,
quasi-oracle rates (i.e., performance comparable to an oracle that knows the
nuisances) even when nuisance functions converge as slowly as n^-1/4. We
provide error analyses establishing debiasedness and demonstrate robust
empirical performance in synthetic studies and a real-world case study of
primary seat-belt laws using Fatality Analysis Reporting System (FARS) dataset.
Together, these results indicate that the proposed learners deliver reliable
and sample-efficient HTE estimates in FD scenarios. The implementation is
available at https://github.com/yonghanjung/FD-CATE.
  Keywords: Front-door adjustment; Heterogeneous treatment effects; Debiased
learning; Quasi-oracle rates; Causal inference.

</details>


### [511] [Metrics for Parametric Families of Networks](https://arxiv.org/abs/2509.22549)
*Mario GÃ³mez,Guanqun Ma,Tom Needham,Bei Wang*

Main category: stat.ML

TL;DR: This paper introduces a framework using parameterized Gromov-Wasserstein distances for analyzing data modeled as networks, and it establishes theoretical guarantees and demonstrates its practical use through experiments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to analyze complex data represented as parameterized families of networks, such as dynamic social networks or collective motion, using a unified metric approach.

Method: The method relies on a Gromov-Wasserstein variant of optimal transport to define distances for parametric data, with computationally tractable lower bounds and empirical approximation guarantees.

Result: The study provides foundational properties, approximation guarantees, and practical evidence of the framework's applicability to diverse parametric network data.

Conclusion: This framework extends existing metrics, proves approximation feasibility, and shows promise for analyzing structured networked data effectively.

Abstract: We introduce a general framework for analyzing data modeled as parameterized
families of networks. Building on a Gromov-Wasserstein variant of optimal
transport, we define a family of parameterized Gromov-Wasserstein distances for
comparing such parametric data, including time-varying metric spaces induced by
collective motion, temporally evolving weighted social networks, and random
graph models. We establish foundational properties of these distances, showing
that they subsume several existing metrics in the literature, and derive
theoretical approximation guarantees. In particular, we develop computationally
tractable lower bounds and relate them to graph statistics commonly used in
random graph theory. Furthermore, we prove that our distances can be
consistently approximated in random graph and random metric space settings via
empirical estimates from generative models. Finally, we demonstrate the
practical utility of our framework through a series of numerical experiments.

</details>


### [512] [Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement](https://arxiv.org/abs/2509.22553)
*Hao Chen,Lin Liu,Yu Guang Wang*

Main category: stat.ML

TL;DR: The paper proposes a novel algorithm for causal representation learning (CRL) that requires weaker assumptions and shows improvements in recovering latent causal features.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in existing linear CRL methods, which rely on stringent assumptions that are often challenging to satisfy in practice.

Method: A novel linear CRL algorithm is introduced, which relaxes assumptions about environment heterogeneity and data distributions while recovering latent causal features up to an equivalence class.

Result: The algorithm's effectiveness is demonstrated through synthetic experiments and interpretability analysis of large language models, showing superior performance over competing methods in finite-sample scenarios.

Conclusion: The proposed algorithm enhances CRL by relaxing restrictive assumptions and highlights its potential to integrate causality into artificial intelligence systems.

Abstract: Causal representation learning (CRL) has garnered increasing interests from
the causal inference and artificial intelligence community, due to its
capability of disentangling potentially complex data-generating mechanism into
causally interpretable latent features, by leveraging the heterogeneity of
modern datasets. In this paper, we further contribute to the CRL literature, by
focusing on the stylized linear structural causal model over the latent
features and assuming a linear mixing function that maps latent features to the
observed data or measurements. Existing linear CRL methods often rely on
stringent assumptions, such as accessibility to single-node interventional data
or restrictive distributional constraints on latent features and exogenous
measurement noise. However, these prerequisites can be challenging to satisfy
in certain scenarios. In this work, we propose a novel linear CRL algorithm
that, unlike most existing linear CRL methods, operates under weaker
assumptions about environment heterogeneity and data-generating distributions
while still recovering latent causal features up to an equivalence class. We
further validate our new algorithm via synthetic experiments and an
interpretability analysis of large language models (LLMs), demonstrating both
its superiority over competing methods in finite samples and its potential in
integrating causality into AI.

</details>


### [513] [Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2509.22633)
*Gen Li,Yuling Yan*

Main category: stat.ML

TL;DR: The paper focuses on improving reinforcement learning with human feedback (RLHF) by proposing a novel exploration scheme targeting uncertainty reduction in reward differences, leading to better data efficiency and policy improvement.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing optimism-based exploration algorithms for RLHF, which may fail to reduce uncertainties crucial for effective reward model and policy refinement.

Method: The authors propose a new exploration approach that prioritizes preference queries reducing uncertainties in reward differences that are most relevant to policy improvements.

Result: Under a multi-armed bandit model, the proposed method achieves regret bounds of $T^{(Î²+1)/(Î²+2)}$, demonstrating polynomial scaling with all model parameters.

Conclusion: The new exploration scheme makes RLHF more data-efficient and effectively balances reward maximization and distribution shift, advancing state-of-the-art online RLHF algorithms.

Abstract: Reinforcement learning with human feedback (RLHF), which learns a reward
model from human preference data and then optimizes a policy to favor preferred
responses, has emerged as a central paradigm for aligning large language models
(LLMs) with human preferences. In this paper, we investigate exploration
principles for online RLHF, where one seeks to adaptively collect new
preference data to refine both the reward model and the policy in a
data-efficient manner. By examining existing optimism-based exploration
algorithms, we identify a drawback in their sampling protocol: they tend to
gather comparisons that fail to reduce the most informative uncertainties in
reward differences, and we prove lower bounds showing that such methods can
incur linear regret over exponentially long horizons. Motivated by this
insight, we propose a new exploration scheme that directs preference queries
toward reducing uncertainty in reward differences most relevant to policy
improvement. Under a multi-armed bandit model of RLHF, we establish regret
bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter
that balances reward maximization against mitigating distribution shift. To our
knowledge, this is the first online RLHF algorithm with regret scaling
polynomially in all model parameters.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [514] [Phase transition from localization to chaos in classical many-body system](https://arxiv.org/abs/2509.22368)
*Yusuf Kasim,Pavel Orlov,TomaÅ¾ Prosen*

Main category: cond-mat.stat-mech

TL;DR: This study identifies a second-order dynamical phase transition in a cellular automaton, transitioning from localized to chaotic information spreading.


<details>
  <summary>Details</summary>
Motivation: To understand information spreading behavior in 2D deterministic many-body systems and characterize phases using information-theoretic tools.

Method: The study uses a cellular automaton called MCPCA, analyzing the information spreading through Hamming distance and classical decorrelator, and relates it with conserved local charges.

Result: The research reveals a transition between localized and chaotic ballistic information phases, determined by the choice of initial ensemble charged states.

Conclusion: The transition is second-order and accompanied by multifractal behavior linked to symmetry constraints, providing a deeper understanding of dynamical structure factors.

Abstract: We report a dynamical phase transition in the information spreading within a
classical 2D deterministic interacting many-body system. Specifically, the
transition is observed in a recently introduced momentum-conserving parity
check cellular automaton (MCPCA) on the square lattice. We characterize the
transition using information-theoretic quantities such as the Hamming distance
and the classical decorrelator. By introducing conserved local charges of the
MCPCA, we show that selecting initial ensembles with specific charge values
allows the system to transition from a localized information phase to a chaotic
regime with ballistic information spreading. Importantly, our findings indicate
that this transition is of second order, highlighting a sharp change in
information spreading behavior. Furthermore, we revisit the multifractal
behavior of the dynamical structure factor and show that, although present
across both phases, it originates from effective local periodicities enforced
by symmetry constraints.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [515] [Assessment of deep learning models integrated with weather and environmental variables for wildfire spread prediction and a case study of the 2023 Maui fires](https://arxiv.org/abs/2509.21327)
*Jiyeon Kim,Yingjie Hu,Negar Elhami-Khorasani,Kai Sun,Ryan Zhenqi Zhou*

Main category: physics.soc-ph

TL;DR: This paper assessed and compared the performance of five deep learning models with the established FARSITE model for wildfire spread prediction using data from Hawaii and the 2023 Maui fires.


<details>
  <summary>Details</summary>
Motivation: The study aims to evaluate the effectiveness of AI-based wildfire spread models and understand how they compare to traditional fire spread models.

Method: Five deep learning models, including ConvLSTM and ConvLSTM with attention, were analyzed using over ten years of wildfire data, and their performance was compared with the FARSITE model using the 2023 Maui fires as a case study.

Result: ConvLSTM and ConvLSTM with attention emerged as the best-performing deep learning models. FARSITE, while exhibiting higher precision and F1-score, had lower recall than the AI models, which proved more flexible in processing input data.

Conclusion: AI models have significant potential in wildfire prediction, offering flexibility and valuable insights into environmental factors, but traditional tools like FARSITE still outperform them in certain metrics like precision.

Abstract: Predicting the spread of wildfires is essential for effective fire management
and risk assessment. With the fast advancements of artificial intelligence
(AI), various deep learning models have been developed and utilized for
wildfire spread prediction. However, there is limited understanding of the
advantages and limitations of these models, and it is also unclear how deep
learning-based fire spread models can be compared with existing non-AI fire
models. In this work, we assess the ability of five typical deep learning
models integrated with weather and environmental variables for wildfire spread
prediction based on over ten years of wildfire data in the state of Hawaii. We
further use the 2023 Maui fires as a case study to compare the best deep
learning models with a widely-used fire spread model, FARSITE. The results show
that two deep learning models, i.e., ConvLSTM and ConvLSTM with attention,
perform the best among the five tested AI models. FARSITE shows higher
precision, lower recall, and higher F1-score than the best AI models, while the
AI models offer higher flexibility for the input data. By integrating AI models
with an explainable AI method, we further identify important weather and
environmental factors associated with the 2023 Maui wildfires.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [516] [Direct Bias-Correction Term Estimation for Propensity Scores and Average Treatment Effect Estimation](https://arxiv.org/abs/2509.22122)
*Masahiro Kato*

Main category: econ.EM

TL;DR: The study introduces a direct method to improve average treatment effect (ATE) estimation by focusing on optimizing the bias-correction term using Bregman divergence.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating propensity scores, such as maximum likelihood or covariate balancing, do not optimize the bias-correction term directly and can lead to inaccuracies in ATE estimation.

Method: The proposed method directly estimates the bias-correction term $h_0(X_i, D_i)$ by minimizing its prediction error within a general framework based on Bregman divergence minimization.

Result: The study conducted simulation studies, demonstrating that the direct optimization approach improves ATE estimation accuracy compared to conventional methods.

Conclusion: Directly estimating the bias-correction term enhances the precision of ATE estimation and provides an alternative to traditional methods of propensity score estimation.

Abstract: This study considers the estimation of the average treatment effect (ATE).
For ATE estimation, we estimate the propensity score through direct
bias-correction term estimation. Let $\{(X_i, D_i, Y_i)\}_{i=1}^{n}$ be the
observations, where $X_i \in \mathbb{R}^p$ denotes $p$-dimensional covariates,
$D_i \in \{0, 1\}$ denotes a binary treatment assignment indicator, and $Y_i
\in \mathbb{R}$ is an outcome. In ATE estimation, the bias-correction term
$h_0(X_i, D_i) = \frac{1[D_i = 1]}{e_0(X_i)} - \frac{1[D_i = 0]}{1 - e_0(X_i)}$
plays an important role, where $e_0(X_i)$ is the propensity score, the
probability of being assigned treatment $1$. In this study, we propose
estimating $h_0$ (or equivalently the propensity score $e_0$) by directly
minimizing the prediction error of $h_0$. Since the bias-correction term $h_0$
is essential for ATE estimation, this direct approach is expected to improve
estimation accuracy for the ATE. For example, existing studies often employ
maximum likelihood or covariate balancing to estimate $e_0$, but these
approaches may not be optimal for accurately estimating $h_0$ or the ATE. We
present a general framework for this direct bias-correction term estimation
approach from the perspective of Bregman divergence minimization and conduct
simulation studies to evaluate the effectiveness of the proposed method.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [517] [Federated Learning of Quantile Inference under Local Differential Privacy](https://arxiv.org/abs/2509.21800)
*Leheng Cai,Qirui Hu,Shuyuan Wu*

Main category: stat.ME

TL;DR: The paper presents a federated learning approach for quantile inference under local differential privacy, using perturbed local stochastic gradient descent for efficiency and privacy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of quantile inference in federated learning settings while ensuring local differential privacy and accommodating communication/storage constraints.

Method: The method involves using perturbed local stochastic gradient descent with randomized mechanisms and global parameters, ensuring statistical efficiency and privacy. Functional central limit theorem and asymptotic normality are established.

Result: The method allows data heterogeneity and individualized privacy budgets, and constructs confidence intervals through self-normalization, without estimating nuisance parameters. Theoretical validity is verified via numerical experiments and real applications.

Conclusion: The proposed methodology is statistically efficient, privacy-preserving, and adaptive to data heterogeneity, proving valid in both theoretical and practical scenarios.

Abstract: In this paper, we investigate federated learning for quantile inference under
local differential privacy (LDP). We propose an estimator based on local
stochastic gradient descent (SGD), whose local gradients are perturbed via a
randomized mechanism with global parameters, making the procedure tolerant of
communication and storage constraints without compromising statistical
efficiency. Although the quantile loss and its corresponding gradient do not
satisfy standard smoothness conditions typically assumed in existing
literature, we establish asymptotic normality for our estimator as well as a
functional central limit theorem. The proposed method accommodates data
heterogeneity and allows each server to operate with an individual privacy
budget. Furthermore, we construct confidence intervals for the target value
through a self-normalization approach, thereby circumventing the need to
estimate additional nuisance parameters. Extensive numerical experiments and
real data application validate the theoretical guarantees of the proposed
methodology.

</details>


### [518] [Rescuing double robustness: safe estimation under complete misspecification](https://arxiv.org/abs/2509.22446)
*Lorenzo Testa,Francesca Chiaromonte,Kathryn Roeder*

Main category: stat.ME

TL;DR: The paper critiques the fragility of doubly robust estimators when all nuisance functions are misspecified and proposes Adaptive Correction Clipping (ACC) as a solution.


<details>
  <summary>Details</summary>
Motivation: Doubly robust estimators are often sensitive to the compounding errors when nuisance functions are entirely misspecified, degrading their performance in real-world applications.

Method: The authors introduce Adaptive Correction Clipping (ACC), which bounds errors arising from nuisance misspecification while retaining favorable properties under correct specification.

Result: ACC successfully mitigates instability associated with doubly robust estimators and demonstrates its efficacy through simulations and applications in Alzheimer's disease proteomics analysis.

Conclusion: ACC is a safe and practical improvement over doubly robust estimators, ensuring bounded error behavior and retaining valid inference when nuisances are well specified.

Abstract: Double robustness is a major selling point of semiparametric and missing data
methodology. Its virtues lie in protection against partial nuisance
misspecification and asymptotic semiparametric efficiency under correct
nuisance specification. However, in many applications, complete nuisance
misspecification should be regarded as the norm (or at the very least the
expected default), and thus doubly robust estimators may behave fragilely. In
fact, it has been amply verified empirically that these estimators can perform
poorly when all nuisance functions are misspecified. Here, we first
characterize this phenomenon of double fragility, and then propose a solution
based on adaptive correction clipping (ACC). We argue that our ACC proposal is
safe, in that it inherits the favorable properties of doubly robust estimators
under correct nuisance specification, but its error is guaranteed to be bounded
by a convex combination of the individual nuisance model errors, which prevents
the instability caused by the compounding product of errors of doubly robust
estimators. We also show that our proposal provides valid inference through the
parametric bootstrap when nuisances are well-specified. We showcase the
efficacy of our ACC estimator both through extensive simulations and by
applying it to the analysis of Alzheimer's disease proteomics data.

</details>


### [519] [Modelling non-stationary extremal dependence through a geometric approach](https://arxiv.org/abs/2509.22501)
*C. J. R. Murphy-Barltrop,J. L. Wadsworth,M. de Carvalho,B. D. Youngman*

Main category: stat.ME

TL;DR: This paper explores modeling non-stationary extremal dependence in multivariate data, extending geometric frameworks and proposing a semi-parametric approach for estimating limit sets.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of suitable methods for modeling non-stationary extremal dependence, which is a frequent occurrence but inadequately handled by current techniques in multivariate extreme value analysis.

Method: The authors extend the geometric modeling framework to the non-stationary context, outline convergence assumptions, and develop a semi-parametric model for estimating limit sets.

Result: Through simulations, the framework effectively captures diverse dependence structures and is robust across various model configurations. It is successfully applied to financial data analysis.

Conclusion: The study introduces an innovative and flexible framework for analyzing non-stationary extremal dependence, enhancing interpretability and practical utility in fields like finance.

Abstract: Non-stationary extremal dependence, whereby the relationship between the
extremes of multiple variables evolves over time, is commonly observed in many
environmental and financial data sets. However, most multivariate extreme value
models are only suited to stationary data. A recent approach to multivariate
extreme value modelling uses a geometric framework, whereby extremal dependence
features are inferred through the limiting shapes of scaled sample clouds. This
framework can capture a wide range of dependence structures, and a variety of
inference procedures have been proposed in the stationary setting. In this
work, we first extend the geometric framework to the non-stationary setting and
outline assumptions to ensure the necessary convergence conditions hold. We
then introduce a flexible, semi-parametric modelling framework for obtaining
estimates of limit sets in the non-stationary setting. Through rigorous
simulation studies, we demonstrate that our proposed framework can capture a
wide range of dependence forms and is robust to different model formulations.
We illustrate the proposed methods on financial returns data and present
several practical uses.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [520] ["Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors](https://arxiv.org/abs/2509.22040)
*Yue Liu,Yanjie Zhao,Yunbo Lyu,Ting Zhang,Haoyu Wang,David Lo*

Main category: cs.CR

TL;DR: This paper explores the security vulnerabilities in agentic AI coding editors, especially regarding prompt injection attacks using AIShellJack, a testing framework.


<details>
  <summary>Details</summary>
Motivation: To highlight the security risks and vulnerabilities of modern AI coding editors with high system privileges as they become increasingly integrated into software development practices.

Method: The authors developed AIShellJack, an automated testing framework with 314 attack payloads, to assess vulnerabilities in coding editors like GitHub Copilot and Cursor.

Result: Evaluation using AIShellJack reveals that these agentic AI tools can be exploited, with attack success rates achieving up to 84%, enabling actions such as credential theft and data exfiltration.

Conclusion: Agentic AI coding editors, while promising for productivity, pose significant security risks that need to be addressed, especially concerning prompt injection attacks.

Abstract: Agentic AI coding editors driven by large language models have recently
become more popular due to their ability to improve developer productivity
during software development. Modern editors such as Cursor are designed not
just for code completion, but also with more system privileges for complex
coding tasks (e.g., run commands in the terminal, access development
environments, and interact with external systems). While this brings us closer
to the "fully automated programming" dream, it also raises new security
concerns. In this study, we present the first empirical analysis of prompt
injection attacks targeting these high-privilege agentic AI coding editors. We
show how attackers can remotely exploit these systems by poisoning external
development resources with malicious instructions, effectively hijacking AI
agents to run malicious commands, turning "your AI" into "attacker's shell". To
perform this analysis, we implement AIShellJack, an automated testing framework
for assessing prompt injection vulnerabilities in agentic AI coding editors.
AIShellJack contains 314 unique attack payloads that cover 70 techniques from
the MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale
evaluation on GitHub Copilot and Cursor, and our evaluation results show that
attack success rates can reach as high as 84% for executing malicious commands.
Moreover, these attacks are proven effective across a wide range of objectives,
ranging from initial access and system discovery to credential theft and data
exfiltration.

</details>


### [521] [Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan](https://arxiv.org/abs/2509.21367)
*Yu-Kai Shih,You-Kai Kang*

Main category: cs.CR

TL;DR: This paper investigates vulnerabilities in AI-powered chatbots used for smart tourism and proposes a secured retrieval-augmented generation (RAG) system that blocks over 85% of prompt injection attacks.


<details>
  <summary>Details</summary>
Motivation: To enhance secure and sustainable AI-powered chatbot functionality in smart tourism while addressing vulnerability to prompt injection attacks.

Method: Designing a secure RAG chatbot for tourism application, integrating API function calls, multi-layer linguistic analysis, and defense guardrails against injections. Benchmarked performance with GPT-5.

Result: The proposed system achieved over 95% benign task accuracy and blocked about 85% of injection attacks based on evaluations involving adversarial prompts.

Conclusion: A robust framework has been developed for secure chatbots in smart tourism, advancing multilingual accessibility, ethical AI, and secure deployment practices.

Abstract: As smart tourism evolves, AI-powered chatbots have become indispensable for
delivering personalized, real-time assistance to travelers while promoting
sustainability and efficiency. However, these systems are increasingly
vulnerable to prompt injection attacks, where adversaries manipulate inputs to
elicit unintended behaviors such as leaking sensitive information or generating
harmful content. This paper presents a case study on the design and
implementation of a secure retrieval-augmented generation (RAG) chatbot for
Hsinchu smart tourism services. The system integrates RAG with API function
calls, multi-layered linguistic analysis, and guardrails against injections,
achieving high contextual awareness and security. Key features include a tiered
response strategy, RAG-driven knowledge grounding, and intent decomposition
across lexical, semantic, and pragmatic levels. Defense mechanisms include
system norms, gatekeepers for intent judgment, and reverse RAG text to
prioritize verified data. We also benchmark a GPT-5 variant (released
2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial
prompts and 223 benign queries show over 95% accuracy on benign tasks and
substantial detection of injection attacks. GPT-5 blocked about 85% of attacks,
showing progress yet highlighting the need for layered defenses. Findings
emphasize contributions to sustainable tourism, multilingual accessibility, and
ethical AI deployment. This work offers a practical framework for deploying
secure chatbots in smart tourism and contributes to resilient, trustworthy AI
applications.

</details>


### [522] [Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey](https://arxiv.org/abs/2509.21389)
*Devashish Chaudhary,Sutharshan Rajasegarar,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: The paper reviews Federated Learning (FL) and Quantum FL applications to Network Intrusion Detection Systems (NIDS), focusing on privacy, efficiency, and quantum advancements.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns and enhancing collaborative security in handling sensitive network traffic data.

Method: Systematic analysis covers FL architectures, privacy-preserving techniques, classical and quantum methods for intrusion detection.

Result: Findings include tailored quantum-specific solutions and a roadmap for research gaps and adoption strategies in the field.

Conclusion: This survey acts as a foundational reference to advance NIDS with FL and QFL, promoting robust and privacy-centric cybersecurity solutions.

Abstract: This survey explores the integration of Federated Learning (FL) with Network
Intrusion Detection Systems (NIDS), with particular emphasis on deep learning
and quantum machine learning approaches. FL enables collaborative model
training across distributed devices while preserving data privacy-a critical
requirement in network security contexts where sensitive traffic data cannot be
centralized. Our comprehensive analysis systematically examines the full
spectrum of FL architectures, deployment strategies, communication protocols,
and aggregation methods specifically tailored for intrusion detection. We
provide an in-depth investigation of privacy-preserving techniques, model
compression approaches, and attack-specific federated solutions for threats
including DDoS, MITM, and botnet attacks. The survey further delivers a
pioneering exploration of Quantum FL (QFL), discussing quantum feature
encoding, quantum machine learning algorithms, and quantum-specific aggregation
methods that promise exponential speedups for complex pattern recognition in
network traffic. Through rigorous comparative analysis of classical and quantum
approaches, identification of research gaps, and evaluation of real-world
deployments, we outline a concrete roadmap for industrial adoption and future
research directions. This work serves as an authoritative reference for
researchers and practitioners seeking to enhance privacy, efficiency, and
robustness of federated intrusion detection systems in increasingly complex
network environments, while preparing for the quantum-enhanced cybersecurity
landscape of tomorrow.

</details>


### [523] [MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs](https://arxiv.org/abs/2509.21634)
*Prakhar Sharma,Haohuang Wen,Vinod Yegneswaran,Ashish Gehani,Phillip Porras,Zhiqiang Lin*

Main category: cs.CR

TL;DR: The paper introduces MobiLLM, an AI framework designed for secure, autonomous threat mitigation in 6G O-RAN networks using Large Language Models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the shortcomings of legacy security defenses and the lack of automated, proactive threat mitigation capabilities in O-RAN environments.

Method: The researchers developed MobiLLM, a modular multi-agent AI system leveraging LLMs for tasks like threat analysis, classification, and response. It incorporates trusted knowledge bases and safety guardrails.

Result: Initial evaluations show that MobiLLM effectively identifies threats, orchestrates appropriate countermeasures, and reduces response latency, demonstrating the feasibility of automated security in 6G networks.

Conclusion: MobiLLM sets a precedent for employing AI-driven solutions to enhance network security in 6G O-RAN systems, improving efficiency and resilience while maintaining trustworthiness.

Abstract: The evolution toward 6G networks is being accelerated by the Open Radio
Access Network (O-RAN) paradigm -- an open, interoperable architecture that
enables intelligent, modular applications across public telecom and private
enterprise domains. While this openness creates unprecedented opportunities for
innovation, it also expands the attack surface, demanding resilient, low-cost,
and autonomous security solutions. Legacy defenses remain largely reactive,
labor-intensive, and inadequate for the scale and complexity of next-generation
systems. Current O-RAN applications focus mainly on network optimization or
passive threat detection, with limited capability for closed-loop, automated
response.
  To address this critical gap, we present an agentic AI framework for fully
automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM
orchestrates security workflows through a modular multi-agent system powered by
Large Language Models (LLMs). The framework features a Threat Analysis Agent
for real-time data triage, a Threat Classification Agent that uses
Retrieval-Augmented Generation (RAG) to map anomalies to specific
countermeasures, and a Threat Response Agent that safely operationalizes
mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge
bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped
with robust safety guardrails, MobiLLM provides a blueprint for trustworthy
AI-driven network security. Initial evaluations demonstrate that MobiLLM can
effectively identify and orchestrate complex mitigation strategies,
significantly reducing response latency and showcasing the feasibility of
autonomous security operations in 6G.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [524] [Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via single stage training](https://arxiv.org/abs/2509.21522)
*Naisong Zhou,Saisamarth Rajesh Phaye,Milos Cernak,Tijana Stojkovic,Andy Pearce,Andrea Cavallaro,Andy Harper*

Main category: cs.SD

TL;DR: The paper introduces Shortcut Flow Matching for Speech Enhancement (SFMSE), a generative model achieving high-quality, real-time speech enhancement with less computation compared to diffusion-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based speech enhancement models provide excellent perceptual quality but require many computational steps, making them unsuitable for real-time use. The study seeks to address the need for efficient models that balance quality and latency.

Method: The paper proposes SFMSE, a step-invariant model trained via single-stage conditioning on the target time step. This approach allows for single, few, or multi-step denoising with deterministic ODE solvers without modifying the architecture.

Result: SFMSE achieves a real-time factor (RTF) of 0.013 on a consumer GPU for single-step inference, with perceptual quality comparable to diffusion models that require 60 neural function evaluations (NFEs).

Conclusion: SFMSE delivers state-of-the-art generative speech enhancement with reduced latency, making it feasible for real-time applications. The work also assesses the role of stochasticity in balancing quality and efficiency.

Abstract: Diffusion-based generative models have achieved state-of-the-art performance
for perceptual quality in speech enhancement (SE). However, their iterative
nature requires numerous Neural Function Evaluations (NFEs), posing a challenge
for real-time applications. On the contrary, flow matching offers a more
efficient alternative by learning a direct vector field, enabling high-quality
synthesis in just a few steps using deterministic ordinary differential
equation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech
Enhancement (SFMSE), a novel approach that trains a single, step-invariant
model. By conditioning the velocity field on the target time step during a
one-stage training process, SFMSE can perform single, few, or multi-step
denoising without any architectural changes or fine-tuning. Our results
demonstrate that a single-step SFMSE inference achieves a real-time factor
(RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable
to a strong diffusion baseline requiring 60 NFEs. This work also provides an
empirical analysis of the role of stochasticity in training and inference,
bridging the gap between high-quality generative SE and low-latency
constraints.

</details>


### [525] [Guiding Audio Editing with Audio Language Model](https://arxiv.org/abs/2509.21625)
*Zitong Lan,Yiduo Hao,Mingmin Zhao*

Main category: cs.SD

TL;DR: SmartDJ is a novel framework for stereo audio editing using AI models for reasoning and generation, enhancing audio editing tasks.


<details>
  <summary>Details</summary>
Motivation: Current audio editing models cannot handle high-level declarative instructions and are limited to mono-channel audio, leaving a gap in stereo editing.

Method: SmartDJ uses an audio language model to decompose high-level instructions into atomic edit operations, which are executed by a diffusion model trained for stereo audio manipulation.

Result: Experiments show superior perceptual quality, spatial realism, and semantic alignment compared to prior methods.

Conclusion: SmartDJ advances stereo audio editing by integrating generative and reasoning capabilities, addressing limitations of previous approaches.

Abstract: Audio editing plays a central role in VR/AR immersion, virtual conferencing,
sound design, and other interactive media. However, recent generative audio
editing models depend on template-like instruction formats and are restricted
to mono-channel audio. These models fail to deal with declarative audio
editing, where the user declares what the desired outcome should be, while
leaving the details of editing operations to the system. We introduce SmartDJ,
a novel framework for stereo audio editing that combines the reasoning
capability of audio language models with the generative power of latent
diffusion. Given a high-level instruction, SmartDJ decomposes it into a
sequence of atomic edit operations, such as adding, removing, or spatially
relocating events. These operations are then executed by a diffusion model
trained to manipulate stereo audio. To support this, we design a data synthesis
pipeline that produces paired examples of high-level instructions, atomic edit
operations, and audios before and after each edit operation. Experiments
demonstrate that SmartDJ achieves superior perceptual quality, spatial realism,
and semantic alignment compared to prior audio editing methods. Demos are
available at https://zitonglan.github.io/project/smartdj/smartdj.html.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [526] [Seismic Velocity Inversion from Multi-Source Shot Gathers Using Deep Segmentation Networks: Benchmarking U-Net Variants and SeismoLabV3+](https://arxiv.org/abs/2509.21331)
*Mahedi Hasan*

Main category: physics.geo-ph

TL;DR: This paper evaluates advanced deep learning architectures for seismic velocity inversion, highlighting the optimized SeismoLabV3+ as the top-performing method.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional seismic inversion methods, such as high computation demands and sensitivity to initialization, by leveraging data-driven deep learning approaches.

Method: The paper benchmarks three encoder-decoder networks (U-Net, U-Net++, and DeepLabV3+) alongside a tailored variant called SeismoLabV3+, using the ThinkOnward 2025 dataset.

Result: SeismoLabV3+ outperformed other models, achieving MAPE values of 0.03025 on the validation split and 0.031246 on the test set.

Conclusion: Deep segmentation networks, especially tailored architectures like SeismoLabV3+, are effective for seismic velocity inversion, with architectural optimization playing a key role.

Abstract: Seismic velocity inversion is a key task in geophysical exploration, enabling
the reconstruction of subsurface structures from seismic wave data. It is
critical for high-resolution seismic imaging and interpretation. Traditional
physics-driven methods, such as Full Waveform Inversion (FWI), are
computationally demanding, sensitive to initialization, and limited by the
bandwidth of seismic data. Recent advances in deep learning have led to
data-driven approaches that treat velocity inversion as a dense prediction
task. This research benchmarks three advanced encoder-decoder architectures --
U-Net, U-Net++, and DeepLabV3+ -- together with SeismoLabV3+, an optimized
variant of DeepLabV3+ with a ResNeXt50 32x4d backbone and task-specific
modifications -- for seismic velocity inversion using the ThinkOnward 2025
Speed \& Structure dataset, which consists of five-channel seismic shot gathers
paired with high-resolution velocity maps. Experimental results show that
SeismoLabV3+ achieves the best performance, with MAPE values of 0.03025 on the
internal validation split and 0.031246 on the hidden test set as scored via the
official ThinkOnward leaderboard. These findings demonstrate the suitability of
deep segmentation networks for seismic velocity inversion and underscore the
value of tailored architectural refinements in advancing geophysical AI models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [527] [The MUG-10 Framework for Preventing Usability Issues in Mobile Application Development](https://arxiv.org/abs/2509.21914)
*Pawel Weichbroth,Tomasz Szot*

Main category: cs.HC

TL;DR: This paper identifies usability challenges in mobile app development and proposes a framework with 10 guidelines and 3 activities to address these issues, derived from insights of 20 practitioners.


<details>
  <summary>Details</summary>
Motivation: Mobile apps face usability challenges due to hardware limitations and diverse user needs, but minimal research exists on countermeasures for these issues.

Method: The study surveyed 20 mobile design/development practitioners and applied in vivo coding to analyze qualitative data for intrinsic meanings.

Result: The research produced a framework of 10 general guidelines and 3 activities to improve usability, with emphasis on user collaboration in all development stages.

Conclusion: Active user collaboration is critical in mobile app development; further research should validate this framework and develop automated tools for early usability issue detection.

Abstract: Nowadays, mobile applications are essential tools for everyday life,
providing users with anytime, anywhere access to up-to-date information,
communication, and entertainment. Needless to say, hardware limitations and the
diverse needs of different user groups pose a number of design and development
challenges. According to recent studies, usability is one of the most revealing
among many others. However, few have made the direct effort to provide and
discuss what countermeasures can be applied to avoid usability issues in mobile
application development. Through a survey of 20 mobile software design and
development practitioners, this study aims to fill this research gap. Given the
qualitative nature of the data collected, and with the goal of capturing and
preserving the intrinsic meanings embedded in the experts' statements, we
adopted in vivo coding. The analysis of the collected material enabled us to
develop a novel framework consisting of ten guidelines and three activities
with general applications. In addition, it can be noted that active
collaboration with users in testing and collecting feedback was often
emphasized at each stage of mobile application development. Future research
should consider focused action research that evaluates the effectiveness of our
recommendations and validates them across different stakeholder groups. In this
regard, the development of automated tools to support early detection and
mitigation of usability issues during mobile application development could also
be considered.

</details>


### [528] [Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review](https://arxiv.org/abs/2509.22271)
*Felix Glawe,Tim Schmeckel,Philipp Brauner,Martina Ziefle*

Main category: cs.HC

TL;DR: This paper is a systematic review of human autonomy and sense of agency in HRI, analyzing 22 empirical studies to identify factors influencing these notions and gaps in the current research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing need for understanding and preserving human autonomy and sense of agency amidst increasing integration of robots in various domains.

Method: A systematic literature review was conducted, filtering 22 studies from 728 articles between 2011 and 2024, using thematic synthesis to identify influential factors.

Result: Five key factors were identified affecting autonomy and agency: robot adaptiveness, communication style, anthropomorphism, the robot's presence, and individual differences. Findings showed fragmented evidence across industries.

Conclusion: The review highlights the lack of standardized definitions and robust frameworks for autonomy and agency in HRI while emphasizing the need for human-centered, ethical robot designs to promote well-being.

Abstract: Human autonomy and sense of agency are increasingly recognised as critical
for user well-being, motivation, and the ethical deployment of robots in
human-robot interaction (HRI). Given the rapid development of artificial
intelligence, robot capabilities and their potential to function as colleagues
and companions are growing. This systematic literature review synthesises 22
empirical studies selected from an initial pool of 728 articles published
between 2011 and 2024. Articles were retrieved from major scientific databases
and identified based on empirical focus and conceptual relevance, namely, how
to preserve and promote human autonomy and sense of agency in HRI. Derived
through thematic synthesis, five clusters of potentially influential factors
are revealed: robot adaptiveness, communication style, anthropomorphism,
presence of a robot and individual differences. Measured through psychometric
scales or the intentional binding paradigm, perceptions of autonomy and agency
varied across industrial, educational, healthcare, care, and hospitality
settings. The review underscores the theoretical differences between both
concepts, but their yet entangled use in HRI. Despite increasing interest, the
current body of empirical evidence remains limited and fragmented, underscoring
the necessity for standardised definitions, more robust operationalisations,
and further exploratory and qualitative research. By identifying existing gaps
and highlighting emerging trends, this review contributes to the development of
human-centered, autonomy-supportive robot design strategies that uphold ethical
and psychological principles, ultimately supporting well-being in human-robot
interaction.

</details>


### [529] [Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0](https://arxiv.org/abs/2509.22298)
*Felix Glawe,Laura Kremer,Luisa Vervier,Philipp Brauner,Martina Ziefle*

Main category: cs.HC

TL;DR: A VR experiment studied how cobot failures impact human trust and autonomy, highlighting the importance of transparent communication for their restoration.


<details>
  <summary>Details</summary>
Motivation: To understand how cobot failures influence human trust and autonomy and explore methods for recovery since human well-being is central to Industry 5.0.

Method: A VR experiment with 39 participants was conducted to study the effects of cobot failure severity and the role of transparent communication on trust and autonomy.

Result: Cobot failures negatively impact both trust and autonomy; failure severity strongly affects trust but not autonomy. Transparent communication helps partially restore both.

Conclusion: Designing cobots with mechanisms for transparent communication can mitigate negative effects of failures, supporting the principles of Industry 5.0 by enhancing trust and autonomy.

Abstract: Collaborative robots (cobots) are a core technology of Industry 4.0. Industry
4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency
and data-driven decision-making. Cobots, as cyber-physical systems, enable the
introduction of lightweight automation to smaller companies through their
flexibility, low cost and ability to work alongside humans, while keeping
humans and their skills in the loop. Industry 5.0, the evolution of Industry
4.0, places the worker at the centre of its principles: The physical and mental
well-being of the worker is the main goal of new technology design, not just
productivity, efficiency and safety standards. Within this concept, human trust
in cobots and human autonomy are important. While trust is essential for
effective and smooth interaction, the workers' perception of autonomy is key to
intrinsic motivation and overall well-being. As failures are an inevitable part
of technological systems, this study aims to answer the question of how system
failures affect trust in cobots as well as human autonomy, and how they can be
recovered afterwards. Therefore, a VR experiment (n = 39) was set up to
investigate the influence of a cobot failure and its severity on human autonomy
and trust in the cobot. Furthermore, the influence of transparent communication
about the failure and next steps was investigated. The results show that both
trust and autonomy suffer after cobot failures, with the severity of the
failure having a stronger negative impact on trust, but not on autonomy. Both
trust and autonomy can be partially restored by transparent communication.

</details>


### [530] [Psychological and behavioural responses in human-agent vs. human-human interactions: a systematic review and meta-analysis](https://arxiv.org/abs/2509.21542)
*Jianan Zhou,Fleur Corbett,Joori Byun,Talya Porat,Nejra van Zalk*

Main category: cs.HC

TL;DR: The study analyzes psychological and behavioral responses in human-agent vs. human-human interactions, finding agents achieve functional similarity to humans but lag in social and moral engagement.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address a fragmented understanding of how people respond to interactive intelligent agents by systematically studying and comparing these responses to human-human interactions.

Method: A systematic review and meta-analysis of 162 studies, integrating frequentist and Bayesian approaches, to compare psychological and behavioral responses in human-agent and human-human interactions.

Result: Individuals demonstrated less prosocial behavior, moral engagement, and attributed less competence and social presence to agents. However, trust, interaction experiences, task performance, and social alignment were similar across agents and humans with significant context-dependent variability.

Conclusion: Human-agent interactions are functionally comparable to human-human interactions but lack intrinsic social and moral qualities, suggesting design and regulatory opportunities to improve agent sociality.

Abstract: Interactive intelligent agents are being integrated across society. Despite
achieving human-like capabilities, humans' responses to these agents remain
poorly understood, with research fragmented across disciplines. We conducted a
first systematic synthesis comparing a range of psychological and behavioural
responses in matched human-agent vs. human-human dyadic interactions. A total
of 162 eligible studies (146 contributed to the meta-analysis; 468 effect
sizes) were included in the systematic review and meta-analysis, which
integrated frequentist and Bayesian approaches. Our results indicate that
individuals exhibited less prosocial behaviour and moral engagement when
interacting with agents vs. humans. They attributed less agency and
responsibility to agents, perceiving them as less competent, likeable, and
socially present. In contrast, individuals' social alignment (i.e., alignment
or adaptation of internal states and behaviours with partners), trust in
partners, personal agency, task performance, and interaction experiences were
generally comparable when interacting with agents vs. humans. We observed high
effect-size heterogeneity for many subjective responses (i.e., social
perceptions of partners, subjective trust, and interaction experiences),
suggesting context-dependency of partner effects. By examining the
characteristics of studies, participants, partners, interaction scenarios, and
response measures, we also identified several moderators shaping partner
effects. Overall, functional behaviours and interactive experiences with agents
can resemble those with humans, whereas fundamental social attributions and
moral/prosocial concerns lag in human-agent interactions. Agents are thus
afforded instrumental value on par with humans but lack comparable intrinsic
value, providing practical implications for agent design and regulation.

</details>


### [531] [LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?](https://arxiv.org/abs/2509.21501)
*Lu Sun,Shihan Fu,Bingsheng Yao,Yuxuan Lu,Wenbo Li,Hansu Gu,Jiri Gesi,Jing Huang,Chen Luo,Dakuo Wang*

Main category: cs.HC

TL;DR: The paper investigates how well large language model (LLM) agents simulate human interactions with agentic AI systems, such as shopping with Amazon Rufus, assessing their potential for scalable evaluations.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of agentic AI systems challenges traditional human evaluation methods, creating a need for scalable simulation approaches. The paper explores LLM agents' ability to act as digital twins representing customers.

Method: 40 human participants interacted with Amazon Rufus for shopping tasks. Their personas, interaction traces, and UX feedback were collected. Digital twins were created to replicate the task through pairwise comparison with human behavior.

Result: LLM agents showed diverse action exploration but aligned their patterns with human actions, yielding similar design feedback. They effectively mirrored human multi-turn interactions.

Conclusion: LLM agents exhibit potential for scalable evaluation of agentic AI systems by closely simulating human interaction behaviors, suggesting their viability as digital twins.

Abstract: Agentic AI is emerging, capable of executing tasks through natural language,
such as Copilot for coding or Amazon Rufus for shopping. Evaluating these
systems is challenging, as their rapid evolution outpaces traditional human
evaluation. Researchers have proposed LLM Agents to simulate participants as
digital twins, but it remains unclear to what extent a digital twin can
represent a specific customer in multi-turn interaction with an agentic AI
system. In this paper, we recruited 40 human participants to shop with Amazon
Rufus, collected their personas, interaction traces, and UX feedback, and then
created digital twins to repeat the task. Pairwise comparison of human and
digital-twin traces shows that while agents often explored more diverse
choices, their action patterns aligned with humans and yielded similar design
feedback. This study is the first to quantify how closely LLM agents can mirror
human multi-turn interaction with an agentic AI system, highlighting their
potential for scalable evaluation.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [532] [PhenoMoler: Phenotype-Guided Molecular Optimization via Chemistry Large Language Model](https://arxiv.org/abs/2509.21424)
*Ran Song,Hui Liu*

Main category: physics.chem-ph

TL;DR: PhenoMoler introduces phenotype-aware drug design using transcriptional profiles to guide molecular generation, improving drug-likeness, chemical validity, and binding affinity.


<details>
  <summary>Details</summary>
Motivation: Current molecular generative models often overlook phenotypic effects, leaving a gap in biologically informed drug design.

Method: PhenoMoler integrates a chemistry large language model with drug-induced differential expression signatures, enabling phenotype-guided and controllable molecular generation.

Result: Generated molecules demonstrate high chemical validity, diversity, and phenotype alignment, with enhanced drug-likeness and superior binding to cancer targets.

Conclusion: PhenoMoler offers a promising framework for fine-grained molecular optimization linking transcriptional responses to chemical structures.

Abstract: Current molecular generative models primarily focus on improving drug-target
binding affinity and specificity, often neglecting the system-level phenotypic
effects elicited by compounds. Transcriptional profiles, as molecule-level
readouts of drug-induced phenotypic shifts, offer a powerful opportunity to
guide molecular design in a phenotype-aware manner. We present PhenoMoler, a
phenotype-guided molecular generation framework that integrates a chemistry
large language model with expression profiles to enable biologically informed
drug design. By conditioning the generation on drug-induced differential
expression signatures, PhenoMoler explicitly links transcriptional responses to
chemical structure. By selectively masking and reconstructing specific
substructures-scaffolds, side chains, or linkers-PhenoMoler supports
fine-grained, controllable molecular optimization. Extensive experiments
demonstrate that PhenoMoler generates chemically valid, novel, and diverse
molecules aligned with desired phenotypic profiles. Compared to FDA-approved
drugs, the generated compounds exhibit comparable or enhanced drug-likeness
(QED), optimized physicochemical properties, and superior binding affinity to
key cancer targets. These findings highlight PhenoMoler's potential for
phenotype-guided and structure-controllable molecular optimization.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [533] [Toward a Realistic Encoding Model of Auditory Affective Understanding in the Brain](https://arxiv.org/abs/2509.21381)
*Guandong Pan,Yaqian Yang,Shi Chen,Xin Wang,Longzhao Liu,Hongwei Zheng,Shaoting Tang*

Main category: eess.AS

TL;DR: This study models how auditory inputs are encoded into emotional responses across datasets using multilevel auditory features and neural analyses.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the unresolved issue of how complex auditory stimuli drive emotion arousal dynamics in affective neuroscience and emotion-aware AI.

Method: Audio features were decomposed using wav2vec 2.0/Hubert and classical algorithms, and cross-dataset analyses were conducted to map stimuli to emotional responses.

Result: High-level semantic representations outperformed low-level acoustic features in emotion encoding, with dataset-dependent effects observed for human voices vs. soundtracks.

Conclusion: The study integrates neuroscience and affective computing to reveal hierarchical mechanisms of auditory-emotion encoding, contributing to adaptive, emotion-aware systems.

Abstract: In affective neuroscience and emotion-aware AI, understanding how complex
auditory stimuli drive emotion arousal dynamics remains unresolved. This study
introduces a computational framework to model the brain's encoding of
naturalistic auditory inputs into dynamic behavioral/neural responses across
three datasets (SEED, LIRIS, self-collected BAVE). Guided by neurobiological
principles of parallel auditory hierarchy, we decompose audio into multilevel
auditory features (through classical algorithms and wav2vec 2.0/Hubert) from
the original and isolated human voice/background soundtrack elements, mapping
them to emotion-related responses via cross-dataset analyses. Our analysis
reveals that high-level semantic representations (derived from the final layer
of wav2vec 2.0/Hubert) exert a dominant role in emotion encoding, outperforming
low-level acoustic features with significantly stronger mappings to behavioral
annotations and dynamic neural synchrony across most brain regions ($p <
0.05$). Notably, middle layers of wav2vec 2.0/hubert (balancing
acoustic-semantic information) surpass the final layers in emotion induction
across datasets. Moreover, human voices and soundtracks show dataset-dependent
emotion-evoking biases aligned with stimulus energy distribution (e.g., LIRIS
favors soundtracks due to higher background energy), with neural analyses
indicating voices dominate prefrontal/temporal activity while soundtracks excel
in limbic regions. By integrating affective computing and neuroscience, this
work uncovers hierarchical mechanisms of auditory-emotion encoding, providing a
foundation for adaptive emotion-aware systems and cross-disciplinary
explorations of audio-affective interactions.

</details>


### [534] [ARTI-6: Towards Six-dimensional Articulatory Speech Encoding](https://arxiv.org/abs/2509.21447)
*Jihwan Lee,Sean Foley,Thanathai Lertpetchpun,Kevin Huang,Yoonjeong Lee,Tiantian Feng,Louis Goldstein,Dani Byrd,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: ARTI-6 is a six-dimensional framework for encoding speech articulatory features derived from real-time MRI data.


<details>
  <summary>Details</summary>
Motivation: To build a physiologically grounded, computationally efficient framework for articulatory inversion and synthesis.

Method: The paper introduces three components: a six-dimensional feature set for speech, a model for predicting articulatory features from speech acoustics, and a synthesis model for generating speech from these features.

Result: Achieved prediction correlation of 0.87 and generated natural-sounding speech from low-dimensional representation.

Conclusion: ARTI-6 offers a compact, interpretable approach to improve speech technology applications by leveraging low-dimensional articulatory frameworks.

Abstract: We propose ARTI-6, a compact six-dimensional articulatory speech encoding
framework derived from real-time MRI data that captures crucial vocal tract
regions including the velum, tongue root, and larynx. ARTI-6 consists of three
components: (1) a six-dimensional articulatory feature set representing key
regions of the vocal tract; (2) an articulatory inversion model, which predicts
articulatory features from speech acoustics leveraging speech foundation
models, achieving a prediction correlation of 0.87; and (3) an articulatory
synthesis model, which reconstructs intelligible speech directly from
articulatory features, showing that even a low-dimensional representation can
generate natural-sounding speech. Together, ARTI-6 provides an interpretable,
computationally efficient, and physiologically grounded framework for advancing
articulatory inversion, synthesis, and broader speech technology applications.
The source code and speech samples are publicly available.

</details>


### [535] [Enhanced Generative Machine Listener](https://arxiv.org/abs/2509.21463)
*Vishnu Raj,Gouthaman KV,Shiv Gehlot,Lars Villemoes,Arijit Biswas*

Main category: eess.AS

TL;DR: GMLv2 is a model for predicting subjective audio quality using MUSHRA scores. It employs a Beta distribution-based loss and integrates additional datasets to improve performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing metrics for subjective audio quality evaluation and develop a more robust and generalizable tool.

Method: Uses a Beta distribution-based loss model and incorporates neural audio coding subjective datasets for improved generalization.

Result: GMLv2 outperforms existing metrics like PEAQ and ViSQOL in correlating and predicting subjective audio quality scores across diverse audio content and codecs.

Conclusion: GMLv2 is a scalable and reliable framework for perceptual audio quality evaluation, enabling advancements in audio coding research and development.

Abstract: We present GMLv2, a reference-based model designed for the prediction of
subjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta
distribution-based loss to model the listener ratings and incorporates
additional neural audio coding (NAC) subjective datasets to extend its
generalization and applicability. Extensive evaluations on diverse testset
demonstrate that proposed GMLv2 consistently outperforms widely used metrics,
such as PEAQ and ViSQOL, both in terms of correlation with subjective scores
and in reliably predicting these scores across diverse content types and codec
configurations. Consequently, GMLv2 offers a scalable and automated framework
for perceptual audio quality evaluation, poised to accelerate research and
development in modern audio coding technologies.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [536] [A Target-Agnostic Protocol-Independent Interface for the Transport Layer](https://arxiv.org/abs/2509.21550)
*Pedro Mizuno,Kimiya Mohammadtaheri,Linfan Qian,Joshua Johnson,Danny Akbarzadeh,Chris Neely,Mario Baldi,Nacihket Kapre,Mina Tahmasbi Arashloo*

Main category: cs.NI

TL;DR: This paper introduces TINF, a high-level, target-agnostic framework for implementing transport protocols.


<details>
  <summary>Details</summary>
Motivation: Transport protocols are foundational but increasingly complex due to diverse application needs and execution environments, highlighting the need for a higher-level, target-agnostic programming abstraction.

Method: The authors propose TINF, a framework that uses constrained C-like constructs to specify transport protocols as high-level programs. TINF generates target-agnostic instructions for transport operations like packet generation and scheduling, enabling deployment across different backend environments.

Result: The framework's efficacy is demonstrated by implementing multiple transport protocols using TINF, successfully deploying them across backends like DPDK and Linux eXpress DataPath.

Conclusion: Target-agnostic transport programs like those in TINF can simplify protocol development, support automated verification, and promote further research in programmable transport targets.

Abstract: Transport protocols are fundamental to network communications, continuously
evolving to meet the demands of new applications, workloads, and network
architectures while running in a wide range of execution environments (a.k.a
targets). We argue that this diversity across protocols and targets calls for a
high-level, target-agnostic programming abstraction for the transport layer.
Specifically, we propose to specify transport protocols as high-level programs
that take an event and flow state as input, and using constrained C-like
constructs, produce the updated state along with target-agnostic instructions
for key transport operations such as data reassembly, packet generation and
scheduling, and timer manipulations.
  We show the benefits of our high-level transport programs by developing
multiple transport protocols in our programming framework called TINF,
developing two TINF- compliant backends, one in DPDK and one in Linux eXpress
DataPath, and deploying TINF programs for multiple protocols across both
backends. Inspired by the benefits unlocked by L2/L3 packet-processing
languages like P4, we believe target-agnostic transport programs can reduce the
development effort for transport protocols, enable automated analysis and
formal verification of the transport layer, and further research in
programmable targets for transport protocols.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [537] [Error Analysis of Discrete Flow with Generator Matching](https://arxiv.org/abs/2509.21906)
*Zhengyan Wan,Yidong Ouyang,Qiang Yao,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: math.ST

TL;DR: The paper introduces a unified framework to explore theoretical aspects of discrete flow models, focusing on their KL divergence, error analysis, and distribution estimation. It builds on stochastic calculus, including a novel Girsanov-type theorem.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unexplored theoretical properties, convergence, and error analysis of discrete flow models, which have shown superior performance compared to discrete diffusion models.

Method: The authors develop a framework based on stochastic calculus to derive KL divergence for continuous-time Markov chains and analyze error types such as transition rate estimation and early stopping, using generator matching and uniformization.

Result: They propose the first non-asymptotic error bounds for distribution estimation in discrete flow models and highlight that these models avoid truncation error inherent in discrete diffusion models.

Conclusion: The work contributes a systematic approach to understand the theoretical underpinnings of discrete flow models, providing new insights and analyses where little had been explored before.

Abstract: Discrete flow models offer a powerful framework for learning distributions
over discrete state spaces and have demonstrated superior performance compared
to the discrete diffusion model. However, their convergence properties and
error analysis remain largely unexplored. In this work, we develop a unified
framework grounded in stochastic calculus theory to systematically investigate
the theoretical properties of discrete flow. Specifically, we derive the KL
divergence of two path measures regarding two continuous-time Markov chains
(CTMCs) with different transition rates by developing a novel Girsanov-type
theorem, and provide a comprehensive analysis that encompasses the error
arising from transition rate estimation and early stopping, where the first
type of error has rarely been analyzed by existing works. Unlike discrete
diffusion models, discrete flow incurs no truncation error caused by truncating
the time horizon in the noising process. Building on generator matching and
uniformization, we establish non-asymptotic error bounds for distribution
estimation. Our results provide the first error analysis for discrete flow
models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [538] [Learning to Ball: Composing Policies for Long-Horizon Basketball Moves](https://arxiv.org/abs/2509.22442)
*Pei Xu,Zhen Wu,Ruocheng Wang,Vishnu Sarukkai,Kayvon Fatahalian,Ioannis Karamouzas,Victor Zordan,C. Karen Liu*

Main category: cs.GR

TL;DR: This paper presents a novel policy integration framework combined with a high-level soft router to enable effective learning and execution of multi-phase, long-horizon tasks, like basketball maneuvers.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning struggles with multi-phase, long-horizon tasks due to the challenges of policy composition and smooth transitions, especially when intermediate states are ill-defined.

Method: Introduces a policy integration framework to compose distinct motor skills seamlessly, complemented by a high-level soft router for robust transitions between subtasks.

Result: The framework was successfully validated on basketball maneuvers, allowing a simulated character to complete tasks without relying on predefined ball trajectory references.

Conclusion: The proposed approach effectively addresses challenges in multi-phase, long-horizon tasks by enabling robust skill composition and transitions, suitable for real-time control scenarios.

Abstract: Learning a control policy for a multi-phase, long-horizon task, such as
basketball maneuvers, remains challenging for reinforcement learning approaches
due to the need for seamless policy composition and transitions between skills.
A long-horizon task typically consists of distinct subtasks with well-defined
goals, separated by transitional subtasks with unclear goals but critical to
the success of the entire task. Existing methods like the mixture of experts
and skill chaining struggle with tasks where individual policies do not share
significant commonly explored states or lack well-defined initial and terminal
states between different phases. In this paper, we introduce a novel policy
integration framework to enable the composition of drastically different motor
skills in multi-phase long-horizon tasks with ill-defined intermediate states.
Based on that, we further introduce a high-level soft router to enable seamless
and robust transitions between the subtasks. We evaluate our framework on a set
of fundamental basketball skills and challenging transitions. Policies trained
by our approach can effectively control the simulated character to interact
with the ball and accomplish the long-horizon task specified by real-time user
commands, without relying on ball trajectory references.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [539] [Foundation models for high-energy physics](https://arxiv.org/abs/2509.21434)
*Anna Hallin*

Main category: hep-ph

TL;DR: The paper reviews the application of foundation models in high-energy physics and highlights their potential use for particle physics data.


<details>
  <summary>Details</summary>
Motivation: Foundation models have transformed NLP and computer vision, sparking interest in their applicability and customization for physics research.

Method: The authors surveyed existing research on foundation models in high-energy physics to discuss their relevance and feasibility.

Result: The paper identifies significant contributions and insights from various studies about using foundation models in particle physics.

Conclusion: Foundation models hold promise for high-energy physics, yet their direct implementation or tailored development requires further exploration.

Abstract: The rise of foundation models -- large, pretrained machine learning models
that can be finetuned to a variety of tasks -- has revolutionized the fields of
natural language processing and computer vision. In high-energy physics, the
question of whether these models can be implemented directly in physics
research, or even built from scratch, tailored for particle physics data, has
generated an increasing amount of attention. This review, which is the first on
the topic of foundation models in high-energy physics, summarizes and discusses
the research that has been published in the field so far.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [540] [COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics](https://arxiv.org/abs/2509.22240)
*Matt Y. Cheung,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: eess.IV

TL;DR: The paper introduces COMPASS, a framework improving uncertainty quantification for medical image segmentation metrics. It leverages model representations for efficient calibration, achieving tighter predictions compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate downstream metric derivations like organ size are crucial in clinical applications, necessitating reliable uncertainty quantification beyond pixel-level mask accuracy.

Method: COMPASS uses inductive biases in deep neural networks by perturbing intermediate features sensitive to target metrics, performing calibration in representation space.

Result: COMPASS achieves tighter and efficient CP intervals compared to traditional approaches and handles coverage under covariate shifts in medical image segmentation tasks.

Conclusion: COMPASS advances metric-based uncertainty quantification for segmentation, enabling practical and reliable predictions in medical image analysis.

Abstract: In clinical applications, the utility of segmentation models is often based
on the accuracy of derived downstream metrics such as organ size, rather than
by the pixel-level accuracy of the segmentation masks themselves. Thus,
uncertainty quantification for such metrics is crucial for decision-making.
Conformal prediction (CP) is a popular framework to derive such principled
uncertainty guarantees, but applying CP naively to the final scalar metric is
inefficient because it treats the complex, non-linear segmentation-to-metric
pipeline as a black box. We introduce COMPASS, a practical framework that
generates efficient, metric-based CP intervals for image segmentation models by
leveraging the inductive biases of their underlying deep neural networks.
COMPASS performs calibration directly in the model's representation space by
perturbing intermediate features along low-dimensional subspaces maximally
sensitive to the target metric. We prove that COMPASS achieves valid marginal
coverage under exchangeability and nestedness assumptions. Empirically, we
demonstrate that COMPASS produces significantly tighter intervals than
traditional CP baselines on four medical image segmentation tasks for area
estimation of skin lesions and anatomical structures. Furthermore, we show that
leveraging learned internal features to estimate importance weights allows
COMPASS to also recover target coverage under covariate shifts. COMPASS paves
the way for practical, metric-based uncertainty quantification for medical
image segmentation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [541] [Less is More: Faster Maximum Clique Search by Work-Avoidance](https://arxiv.org/abs/2509.22245)
*Hans Vandierendonck*

Main category: cs.DS

TL;DR: The paper addresses the NP-hard maximum clique problem by introducing optimization techniques, achieving speedups up to 38.9x over comparable algorithms.


<details>
  <summary>Details</summary>
Motivation: To optimize the search for the maximum clique in a graph, reducing execution time by skipping unnecessary search space.

Method: Introduces techniques like a lazily constructed graph representation, filtering to avoid detailed searches, efficient intersection algorithms, and leveraging algorithmic choice.

Result: Achieved enhancements in computational efficiency with speedups up to 38.9x compared to similar algorithms and up to 11x over MC-BRB.

Conclusion: The proposed optimization techniques effectively improve the search process for the maximum clique, demonstrating significant performance improvements.

Abstract: The maximum clique (MC) problem is a challenging graph mining problem which,
due to its NP-hard nature, can take a substantial amount of execution time. The
MC problem is dominated by set intersection operations similar to Maximal
Clique Enumeration, however it differs in requiring to find only a clique of
maximum size. As such, key to the problem is to demonstrate efficiently that a
particular part of the search space does not contain a maximum clique, allowing
to skip over major parts of the search space. We present a number of techniques
to optimize MC search in light of leaving major parts of the search space
unvisited, including (i) an efficient, lazily constructed graph representation;
(ii) filtering prior to initiating a detailed search; (iii) efficient
early-exit intersection algorithms; (iv) exploiting algorithmic choice. These
techniques result in a speedup of up to 38.9x compared to PMC, which is the
most comparable algorithm, and a speedup up to 11x over MC-BRB.

</details>


### [542] [New Algorithmic Directions in Optimal Transport and Applications for Product Spaces](https://arxiv.org/abs/2509.21502)
*Salman Beigi,Omid Etesami,Mohammad Mahmoody,Amir Najafi*

Main category: cs.DS

TL;DR: The paper presents a novel algorithm for optimal transport in high-dimensional spaces, emphasizing efficiency in dimension-dependent runtime. It showcases advancements in transporting product distributions and offers algorithmic results like an enhanced version of Talagrand's inequality and new computational concentration results.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for efficient high-dimensional optimal transport algorithms that scale with the dimension while maintaining computational feasibility, as classical methods are often intractable in high dimensions.

Method: The authors propose a generalized algorithm to transport product distributions under \(\ell_p^p\) cost using the Knothe-Rosenblatt transport framework and introduce a new concept of sequentially samplable distributions with bounded sampling cost.

Result: Key results include an algorithmic version of Talagrand's inequality for Gaussian transport under squared Euclidean cost, efficient sequential samplers for conditioned Gaussian distributions, and a resolution to an open question on Gaussian concentration performance with dimension-independent transportation cost.

Conclusion: This work extends the boundaries of computational optimal transport, offering practical and scalable algorithms for high-dimensional problems, and providing significant theoretical advancements in transport cost and concentration inequalities.

Abstract: We study optimal transport between two high-dimensional distributions
$\mu,\nu$ in $R^n$ from an algorithmic perspective: given $x \sim \mu$, find a
close $y \sim \nu$ in $poly(n)$ time, where $n$ is the dimension of $x,y$.
Thus, running time depends on the dimension rather than the full representation
size of $\mu,\nu$. Our main result is a general algorithm for transporting any
product distribution $\mu$ to any $\nu$ with cost $\Delta + \delta$ under
$\ell_p^p$, where $\Delta$ is the Knothe-Rosenblatt transport cost and $\delta$
is a computational error decreasing with runtime. This requires $\nu$ to be
"sequentially samplable" with bounded average sampling cost, a new but natural
notion.
  We further prove:
  An algorithmic version of Talagrand's inequality for transporting the
standard Gaussian $\Phi^n$ to arbitrary $\nu$ under squared Euclidean cost. For
$\nu = \Phi^n$ conditioned on a set $\mathcal{S}$ of measure $\varepsilon$, we
construct the sequential sampler in expected time $poly(n/\varepsilon)$ using
membership oracle access to $\mathcal{S}$. This yields an algorithmic transport
from $\Phi^n$ to $\Phi^n|\mathcal{S}$ in $poly(n/\varepsilon)$ time and
expected squared distance $O(\log 1/\varepsilon)$, optimal for general
$\mathcal{S}$ of measure $\varepsilon$.
  As corollary, we obtain the first computational concentration result (Etesami
et al. SODA 2020) for Gaussian measure under Euclidean distance with
dimension-independent transportation cost, resolving an open question of
Etesami et al. Specifically, for any $\mathcal{S}$ of Gaussian measure
$\varepsilon$, most $\Phi^n$ samples can be mapped to $\mathcal{S}$ within
distance $O(\sqrt{\log 1/\varepsilon})$ in $poly(n/\varepsilon)$ time.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [543] [Factor-Based Conditional Diffusion Model for Portfolio Optimization](https://arxiv.org/abs/2509.22088)
*Xuefeng Gao,Mengying He,Xuedong He*

Main category: q-fin.PM

TL;DR: The paper introduces a conditional diffusion model for portfolio optimization that outperforms traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a more effective model for predicting next-day stock returns by incorporating asset-specific factors and cross-asset dependencies.

Method: Develops a conditional diffusion model using a Diffusion Transformer with token-wise conditioning, linking asset returns to their specific factor vectors while addressing cross-asset relationships.

Result: The model generates daily return samples used for mean-variance optimization and consistently exceeds benchmark methods on the Chinese A-share market across various metrics.

Conclusion: The proposed approach is a superior alternative to traditional estimators for portfolio optimization in terms of both methodology and performance.

Abstract: We propose a novel conditional diffusion model for portfolio optimization
that learns the cross-sectional distribution of next-day stock returns
conditioned on asset-specific factors. The model builds on the Diffusion
Transformer with token-wise conditioning, linking each asset's return to its
own factor vector while capturing cross-asset dependencies. Generated return
samples are used for daily mean-variance optimization under realistic
constraints. Empirical results on the Chinese A-share market show that our
approach consistently outperforms benchmark methods based on standard empirical
and shrinkage-based estimators across multiple metrics.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [544] [From Search to Reasoning: A Five-Level RAG Capability Framework for Enterprise Data](https://arxiv.org/abs/2509.21324)
*Gurbinder Gill,Ritvik Gupta,Denis Lusson,Anand Chandrashekar,Donald Nguyen*

Main category: cs.IR

TL;DR: The paper discusses limitations of traditional Retrieval-Augmented Generation (RAG) and introduces a new L1-L5 framework to classify systems by data modalities and task complexity. Experiments are conducted on platforms like LangChain and OpenAI.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of RAG in handling complex or non-text enterprise queries and propose a structured framework for discussion and evaluation.

Method: Introduce a classification framework (L1-L5), establish benchmarks, and evaluate platforms using diverse enterprise datasets.

Result: The study highlights the importance of multi-space retrieval and dynamic orchestration for enabling capabilities across different task complexity levels (L1-L4).

Conclusion: The new framework and empirical validation provide a foundation for improving RAGâ€™s effectiveness in enterprise question-answering scenarios.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the standard paradigm for
answering questions on enterprise data. Traditionally, RAG has centered on
text-based semantic search and re-ranking. However, this approach falls short
when dealing with questions beyond data summarization or non-text data. This
has led to various attempts to supplement RAG to bridge the gap between RAG,
the implementation paradigm, and the question answering problem that enterprise
users expect it to solve. Given that contemporary RAG is a collection of
techniques rather than a defined implementation, discussion of RAG and related
question-answering systems benefits from a problem-oriented understanding.
  We propose a new classification framework (L1-L5) to categorize systems based
on data modalities and task complexity of the underlying question answering
problems: L1 (Surface Knowledge of Unstructured Data) through L4 (Reflective
and Reasoned Knowledge) and the aspirational L5 (General Intelligence). We also
introduce benchmarks aligned with these levels and evaluate four
state-of-the-art platforms: LangChain, Azure AI Search, OpenAI, and Corvic AI.
Our experiments highlight the value of multi-space retrieval and dynamic
orchestration for enabling L1-L4 capabilities. We empirically validate our
findings using diverse datasets indicative of enterprise use cases.

</details>


### [545] [PIR-RAG: A System for Private Information Retrieval in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21325)
*Baiqiang Wang,Qian Lou,Mengxin Zheng,Dongfang Zhao*

Main category: cs.IR

TL;DR: The paper presents PIR-RAG, a privacy-focused solution for retrieval-augmented generation (RAG) systems, which reduces latency and protects user data using semantic clustering and a special PIR protocol.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems expose user queries to potential privacy risks when interacting with service providers, necessitating a more secure design.

Method: The authors used coarse-grained semantic clustering to streamline the retrieval process, combined with a fast, lattice-based Private Information Retrieval protocol to securely fetch entire document clusters for RAG.

Result: The evaluation shows PIR-RAG outperforms existing methods like graph-based PIR and Tiptoe-style algorithms, providing better scalability and faster secure retrieval times for RAG workflows.

Conclusion: PIR-RAG is presented as a scalable, efficient solution ensuring both privacy and performance for retrieval-augmented generation systems.

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational component of
modern AI systems, yet it introduces significant privacy risks by exposing user
queries to service providers. To address this, we introduce PIR-RAG, a
practical system for privacy-preserving RAG. PIR-RAG employs a novel
architecture that uses coarse-grained semantic clustering to prune the search
space, combined with a fast, lattice-based Private Information Retrieval (PIR)
protocol. This design allows for the efficient retrieval of entire document
clusters, uniquely optimizing for the end-to-end RAG workflow where full
document content is required. Our comprehensive evaluation against strong
baseline architectures, including graph-based PIR and Tiptoe-style private
scoring, demonstrates PIR-RAG's scalability and its superior performance in
terms of "RAG-Ready Latency"-the true end-to-end time required to securely
fetch content for an LLM. Our work establishes PIR-RAG as a viable and highly
efficient solution for privacy in large-scale AI systems.

</details>


### [546] [Cross-Modal Retrieval with Cauchy-Schwarz Divergence](https://arxiv.org/abs/2509.21339)
*Jiahao Zhang,Wenzhe Yin,Shujian Yu*

Main category: cs.IR

TL;DR: The paper presents a novel divergence measure, Cauchy-Schwarz (CS) divergence and its extension, Generalized CS (GCS), to address limitations of existing methods in cross-modal retrieval, gaining stable and improved performance.


<details>
  <summary>Details</summary>
Motivation: Current cross-modal retrieval methods have issues like instability, hyperparameter dependence, and inadequate distributional representation. This paper aims to solve these challenges.

Method: The authors propose a hyperparameter-free CS divergence and extend it to GCS divergence using H"older's inequality, enabling stable multi-modal data alignment within a unified framework.

Result: CS/GCS divergence achieves superior performance in bi-modal and tri-modal retrieval tasks, as validated on six benchmark datasets.

Conclusion: The proposed CS/GCS divergence improves training stability and retrieval performance while simplifying multi-modal alignment. Code is shared for reproducibility.

Abstract: Effective cross-modal retrieval requires robust alignment of heterogeneous
data types. Most existing methods focus on bi-modal retrieval tasks and rely on
distributional alignment techniques such as Kullback-Leibler divergence,
Maximum Mean Discrepancy, and correlation alignment. However, these methods
often suffer from critical limitations, including numerical instability,
sensitivity to hyperparameters, and their inability to capture the full
structure of the underlying distributions. In this paper, we introduce the
Cauchy-Schwarz (CS) divergence, a hyperparameter-free measure that improves
both training stability and retrieval performance. We further propose a novel
Generalized CS (GCS) divergence inspired by H\"older's inequality. This
extension enables direct alignment of three or more modalities within a unified
mathematical framework through a bidirectional circular comparison scheme,
eliminating the need for exhaustive pairwise comparisons. Extensive experiments
on six benchmark datasets demonstrate the effectiveness of our method in both
bi-modal and tri-modal retrieval tasks. The code of our CS/GCS divergence is
publicly available at https://github.com/JiahaoZhang666/CSD.

</details>


### [547] [ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems](https://arxiv.org/abs/2509.21371)
*Dayu Yang,Hui Fang*

Main category: cs.IR

TL;DR: ReGeS is introduced as a synergistic Retrieval-Augmented Generation framework to enhance conversational recommender systems by better extracting user preferences and preventing hallucination.


<details>
  <summary>Details</summary>
Motivation: Current CRS solutions either rely on rigid domain-specific engineering or large language models which have a tendency for hallucinations. There is a need for a balanced approach that addresses noisy dialogue and item nuances effectively.

Method: ReGeS employs a reciprocal synergy combining generation-augmented retrieval for better user intent extraction and retrieval-augmented generation for subtle item differentiation.

Result: Experiments across multiple CRS benchmarks indicate that ReGeS delivers state-of-the-art recommendation accuracy, validating its effectiveness.

Conclusion: ReGeS simplifies domain knowledge integration in CRS, avoids hallucinations, eliminates extra annotations, and achieves better accuracy, emphasizing the importance of reciprocal synergy in knowledge-intensive applications.

Abstract: Connecting conversation with external domain knowledge is vital for
conversational recommender systems (CRS) to correctly understand user
preferences. However, existing solutions either require domain-specific
engineering, which limits flexibility, or rely solely on large language models,
which increases the risk of hallucination. While Retrieval-Augmented Generation
(RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that
weaken retrieval and by overlooked nuances among similar items. We propose
ReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies
generation-augmented retrieval to distill informative user intent from
conversations and retrieval-augmented generation to differentiate subtle item
features. This synergy obviates the need for extra annotations, reduces
hallucinations, and simplifies continuous updates. Experiments on multiple CRS
benchmarks show that ReGeS achieves state-of-the-art performance in
recommendation accuracy, demonstrating the effectiveness of reciprocal synergy
for knowledge-intensive CRS tasks.

</details>


### [548] [MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering](https://arxiv.org/abs/2509.21391)
*Lihui Liu,Carl J. Yang*

Main category: cs.IR

TL;DR: The paper introduces MIXRAG, a Mixture-of-Experts Graph-RAG framework, to enhance LLMs by combining specialized graph retrievers and reducing irrelevant noise in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle in knowledge-intensive domains due to reliance on static corpora. The paper aims to improve their performance through better utilization of external knowledge, particularly through graph-based Retrieval-Augmented Generation (RAG).

Method: MIXRAG uses multiple specialized retrievers trained on different graph semantics (e.g., entities, relations, topology) and a dynamic routing controller to select relevant retrievers. Additionally, a query-aware GraphEncoder emphasizes relevant subgraph relationships and reduces noise.

Result: Empirical results show MIXRAG achieves state-of-the-art performance, consistently outperforming various baselines across diverse graph-based tasks.

Conclusion: MIXRAG provides a robust, dynamic solution to improve graph-based RAG systems by effectively handling diverse query types and noise, demonstrating its efficacy across domains.

Abstract: Large Language Models (LLMs) have achieved impressive performance across a
wide range of applications. However, they often suffer from hallucinations in
knowledge-intensive domains due to their reliance on static pretraining
corpora. To address this limitation, Retrieval-Augmented Generation (RAG)
enhances LLMs by incorporating external knowledge sources during inference.
Among these sources, textual graphs provide structured and semantically rich
information that supports more precise and interpretable reasoning. This has
led to growing interest in graph-based RAG systems. Despite their potential,
most existing approaches rely on a single retriever to identify relevant
subgraphs, which limits their ability to capture the diverse aspects of complex
queries. Moreover, these systems often struggle to accurately judge the
relevance of retrieved content, making them prone to distraction by irrelevant
noise. To address these challenges, in this paper, we propose MIXRAG, a
Mixture-of-Experts Graph-RAG framework that introduces multiple specialized
graph retrievers and a dynamic routing controller to better handle diverse
query intents. Each retriever is trained to focus on a specific aspect of graph
semantics, such as entities, relations, or subgraph topology. A
Mixture-of-Experts module adaptively selects and fuses relevant retrievers
based on the input query. To reduce noise in the retrieved information, we
introduce a query-aware GraphEncoder that carefully analyzes relationships
within the retrieved subgraphs, highlighting the most relevant parts while
down-weighting unnecessary noise. Empirical results demonstrate that our method
achieves state-of-the-art performance and consistently outperforms various
baselines. MIXRAG is effective across a wide range of graph-based tasks in
different domains. The code will be released upon paper acceptance.

</details>


### [549] [HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores](https://arxiv.org/abs/2509.21336)
*Guohang Yan,Yue Zhang,Pinlong Cai,Ding Wang,Song Mao,Hongwei Zhang,Yaoze Zhang,Hairong Zhang,Xinyu Cai,Botian Shi*

Main category: cs.IR

TL;DR: The paper proposes HetaRAG, a hybrid framework for retrieval-augmented generation that utilizes diverse data storage systems to enhance evidence retrieval and mitigate trade-offs of single storage modalities.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems rely on single data storage modalities, which result in unavoidable trade-offs such as lack of semantic understanding or poor recall. These limitations necessitate a unified system harnessing complementary strengths of heterogeneous retrieval paradigms.

Method: The authors design and propose HetaRAG, a retrieval-augmented generation framework that combines vector indices, knowledge graphs, full-text engines, and structured databases into one retrieval system to dynamically route and fuse evidence.

Result: An initial RAG pipeline was constructed, showcasing the technical feasibility of merging multiple retrieval paradigms. Partial code of this system is provided for further exploration and development.

Conclusion: HetaRAG serves as a promising approach to retrieval-augmented generation by leveraging the strengths of heterogeneous data stores, delivering high recall, precision, and context fidelity in responses.

Abstract: Retrieval-augmented generation (RAG) has become a dominant paradigm for
mitigating knowledge hallucination and staleness in large language models
(LLMs) while preserving data security. By retrieving relevant evidence from
private, domain-specific corpora and injecting it into carefully engineered
prompts, RAG delivers trustworthy responses without the prohibitive cost of
fine-tuning. Traditional retrieval-augmented generation (RAG) systems are
text-only and often rely on a single storage backend, most commonly a vector
database. In practice, this monolithic design suffers from unavoidable
trade-offs: vector search captures semantic similarity yet loses global
context; knowledge graphs excel at relational precision but struggle with
recall; full-text indexes are fast and exact yet semantically blind; and
relational engines such as MySQL provide strong transactional guarantees but no
semantic understanding. We argue that these heterogeneous retrieval paradigms
are complementary, and propose a principled fusion scheme to orchestrate them
synergistically, mitigating the weaknesses of any single modality. In this work
we introduce HetaRAG, a hybrid, deep-retrieval augmented generation framework
that orchestrates cross-modal evidence from heterogeneous data stores. We plan
to design a system that unifies vector indices, knowledge graphs, full-text
engines, and structured databases into a single retrieval plane, dynamically
routing and fusing evidence to maximize recall, precision, and contextual
fidelity. To achieve this design goal, we carried out preliminary explorations
and constructed an initial RAG pipeline; this technical report provides a brief
overview. The partial code is available at
https://github.com/KnowledgeXLab/HetaRAG.

</details>
