<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 34]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 50]
- [cs.CV](#cs.CV) [Total: 51]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.LG](#cs.LG) [Total: 50]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 26]
- [cs.SE](#cs.SE) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 4]
- [eess.IV](#eess.IV) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: The paper proposes a framework applying thermodynamic principles to the ethical alignment of AI systems, defining 'ethical entropy' as a measure of divergence from intended goals, and provides mathematical support with experiments validating the theory.


<details>
  <summary>Details</summary>
Motivation: To address instability and ethical misalignment in advanced AI systems, akin to entropy increasing in thermodynamics, which requires continuous adjustment to ensure adherence to intended goals.

Method: Mathematically defines 'ethical entropy' for AI systems and derives its time derivative. It establishes a critical stability boundary informed by model parameters and uses simulations with a defined gradient-based optimizer to validate the proposed framework.

Result: The simulations demonstrate that without alignment work, AI systems diverge, with increasing ethical entropy, while regularized systems maintain stability. Quantitative measurements of this divergence validate the theory.

Conclusion: AI alignment can be viewed as a continuous thermodynamic control problem; the proposed framework offers a mathematical and practical basis for ensuring AI system safety and stability through ongoing alignment work.

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [2] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: The paper introduces Co-EPG, a self-iterative training framework for improving GUI agents by fostering co-evolution of planning and grounding capabilities. It achieves state-of-the-art results on benchmarks without external data.


<details>
  <summary>Details</summary>
Motivation: Current GUI task automation methods struggle due to lack of synergy between models and over-reliance on synthetic data. The paper aims to overcome these issues.

Method: Co-EPG involves iterative feedback loops where a planning model improves under grounding-based rewards and optimizes data for grounding models, enabling self-play and continuous improvement.

Result: Co-EPG achieves superior performance on Multimodal-Mind2Web and AndroidControl benchmarks, showing robust improvement across iterations without external data.

Conclusion: The framework introduces a paradigm shift by promoting integrated, self-driven co-evolution in GUI agent training, overcoming existing limitations.

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [3] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: The paper investigates the Pareto pruning problem in multi-objective optimization by introducing a new measure called directed coverage, analyzing computational complexity, and experimentally evaluating its performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge in multi-objective optimization of simplifying decision-making for selecting preferred solutions from Pareto optimal sets to reduce cognitive load.

Method: The paper reframes Pareto pruning as a multiwinner voting problem, provides an axiomatic analysis of existing quality measures, introduces a new quality measure, analyzes computational complexity, and experimentally evaluates approaches.

Result: The newly proposed measure, directed coverage, performs competitively or favorably across various settings, and the study identifies computational boundaries of optimizing quality measures.

Conclusion: The choice of quality measure significantly influences the representation of Pareto optimal solutions. The proposed directed coverage measure is both efficient and effective in guiding selection.

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [4] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: The paper explores how clique-width as a graph parameter can be leveraged for encoding argumentation problems into (Q)SAT, designing novel reductions that preserve clique-width and providing insights into its computational implications.


<details>
  <summary>Details</summary>
Motivation: To understand the encoding limitations and capabilities of the graph parameter clique-width in complex problem-solving, especially as it applies to abstract argumentation, which relies on computationally intensive properties.

Method: The paper presents directed decomposition-guided (DDG) reductions to (Q)SAT, which linearly preserve the clique-width of argumentation problems, along with applying these reductions to argumentation semantics, including counting.

Result: The reductions establish novel contributions for encoding argumentative problems, and the overhead caused by these reductions is shown to be near-optimal under standard assumptions, ensuring computational efficiency.

Conclusion: Clique-width can be leveraged effectively for encoding complex argumentative problems into (Q)SAT, and the proposed approach is computationally efficient with minimal overhead. These findings advance the understanding of argumentation semantics and structural graph analysis in complexity studies.

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [5] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: This paper introduces two new metrics—Probability of Outcome Ranking (PoR) and Probability of Best Outcome (PoB)—for counterfactual decision-making, providing theoretical foundations, estimation methods, and real-world applications.


<details>
  <summary>Details</summary>
Motivation: To enhance causal reasoning in decision-making by developing new metrics (PoR and PoB) that improve the process of ranking and selecting optimal actions based on potential outcomes.

Method: Introduced theoretical identification theorems and derived bounds for PoR and PoB metrics; proposed estimation methods; validated through numerical experiments and applied these methods to a real-world dataset.

Result: Developed and validated PoR and PoB metrics for counterfactual decision-making, and demonstrated their performance in finite-sample scenarios and real-world applications.

Conclusion: The new metrics, PoR and PoB, offer robust tools for assessing counterfactual outcomes, enabling more informed and effective decision-making under uncertainty.

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [6] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: The paper discusses adaptive reasoning in large language models (LLMs), emphasizing the importance of tailoring reasoning to task complexity rather than applying uniform strategies.


<details>
  <summary>Details</summary>
Motivation: Current LLMs use uniform reasoning strategies regardless of task complexities, leading to inefficiencies in reasoning for trivial and difficult tasks.

Method: The paper formalizes adaptive reasoning as a policy optimization problem balancing task performance with computational costs and organizes methods into training-based and training-free approaches.

Result: The framework categorizes adaptive reasoning methods and enables systematic comparison of strategies, shedding light on how adaptive reasoning can be realized.

Conclusion: The paper concludes with identifying unmet challenges in self-evaluation, meta-reasoning, and alignment of reasoning control with human-like decision making.

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [7] [HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments](https://arxiv.org/abs/2511.10810)
*Ran Elgedawy,Sanjay Das,Ethan Seefried,Gavin Wiggins,Ryan Burchfield,Dana Hewit,Sudarshan Srinivasan,Todd Thomas,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: The paper introduces HARNESS, an AI framework for improving safety in mission-critical DOE work environments by predicting hazards and enabling human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: To address the need for enhanced safety in hazardous work environments by accurately predicting and mitigating operational risks.

Method: HARNESS integrates Large Language Models, historical data analysis, and SME feedback within a collaborative, adaptive AI framework for proactive hazard identification.

Result: Preliminary field deployment of HARNESS indicates promising outcomes with potential for accuracy and decision-making enhancements.

Conclusion: HARNESS demonstrates a novel approach to improving safety and efficiency in high-risk environments, with future work proposed for empirical validation and SME alignment.

Abstract: Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.

</details>


### [8] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: The paper introduces HyperComplEx, a hybrid embedding framework to better model diverse relationship types in knowledge graphs by combining multiple geometric spaces.


<details>
  <summary>Details</summary>
Motivation: Current embedding methods for knowledge graphs struggle with specific relational types: Euclidean spaces with hierarchies, vector spaces with asymmetry, and hyperbolic spaces with symmetric relations.

Method: The proposed HyperComplEx combines hyperbolic, complex, and Euclidean spaces using learned attention mechanisms and relation-specific space weighting.

Result: HyperComplEx outperformed state-of-the-art methods on datasets up to 10M papers, achieving high MRR scores and efficient inference times, with results confirmed on standard benchmarks.

Conclusion: HyperComplEx effectively handles diverse knowledge graph relations with scalable and efficient performance, advancing the state-of-the-art in graph embeddings.

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [9] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: This study introduces a multi-agent AI framework for reconstructing traffic collisions using fragmented multimodal data, achieving perfect results on challenging cases and surpassing human accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address inconsistencies in traditional human-expert-based traffic collision reconstruction methods, especially when dealing with incomplete multimodal data.

Method: The study develops a two-phase collaborative framework: Phase I reconstructs crash scenarios using multimodal inputs, while Phase II combines these reconstructions with temporal EDR data for detailed analysis. Validation involves analyzing 277 LVD collisions with a focus on 39 complex cases.

Result: The AI framework achieved 100% accuracy on complex LVD crash cases, outperforming human researchers' 92% accuracy. It demonstrated robust performance even on incomplete or ambiguous data.

Conclusion: The AI framework outperforms traditional methods by providing precise, consistent reconstructions and reasoning capabilities, even in cases with incomplete or conflicted data.

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [10] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: This paper introduces a planning support system incorporating agentic AI, allowing users to create dynamic, demand-oriented regions for disaster planning.


<details>
  <summary>Details</summary>
Motivation: Existing urban planning units are inflexible and do not meet specific local community demands or effectively prevent/respond to hazards.

Method: The system uses a RepSC-SOM model with enhanced geographic filtering, region-growing, and AI agents for interactive decision-making.

Result: Through a Jacksonville, Florida case study on flood risks, the platform proves effective in enabling user-interactive regionalization for disaster planning.

Conclusion: The platform effectively combines computational precision and user-driven adaptability, improving disaster planning through dynamic region creation.

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [11] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: The paper introduces a novel framework using Large Language Models (LLMs) to improve understanding of disease progression in Alzheimer's Disease by addressing limitations in current methods regarding brain connectivity representation.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in current methods that oversimplify complex brain connectivity relationships as a single-modality connectome in modeling Alzheimer's Disease progression.

Method: Developed a framework integrating LLMs to inform the construction of multi-modal, biologically-constrained graph structures and disease trajectories from longitudinal patient data.

Result: The approach significantly improves prediction accuracy and interpretability of Alzheimer's pathology propagation based on tau-PET imaging, uncovering additional disease factors.

Conclusion: Leveraging LLMs enhances the modeling of multi-modal relationships in neurodegenerative diseases, offering superior insights into disease mechanisms compared to traditional methods.

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [12] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: The paper presents a multi-agent legal verifier to ensure compliance with the Japanese APPI for AI-driven data transfer, significantly outperforming single-agent systems.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of privacy regulations, such as the Japanese APPI, demands more reliable tools for automated compliance in AI-driven data transfer.

Method: The system uses a multi-agent framework comprising agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol.

Result: The system achieved 72% accuracy (21% higher than baseline) on 200 APPI cases, with exceptional 90% accuracy on clear compliance cases while perfectly detecting violations.

Conclusion: The proposed multi-agent system demonstrates improved performance, providing a scalable and interpretable solution for legal compliance verification despite challenges in ambiguous scenarios.

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [13] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: AI systems must adapt beyond their prior training to evaluate, justify, and make decisions in uncertain or novel contexts, aligning actions with human values.


<details>
  <summary>Details</summary>
Motivation: Autonomous AI systems face situations where predefined policies are insufficient, necessitating robust decision-making mechanisms aligned with human expectations and values.

Method: The paper analyzes agent decision-making requirements and knowledge types through empirical case studies and theoretical exploration.

Result: The research identifies essential contextual knowledge types—normative, pragmatic, and situational—for robust and human-aligned AI decision-making.

Conclusion: In complex environments, AI agents must integrate diverse knowledge areas to navigate constraints and align decisions with human goals, ensuring ethical and effective actions.

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [14] [AI Agent-Driven Framework for Automated Product Knowledge Graph Construction in E-Commerce](https://arxiv.org/abs/2511.11017)
*Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.AI

TL;DR: The paper describes an AI-driven framework using Large Language Models (LLMs) to automatically construct product knowledge graphs from unstructured data, evaluated on air conditioner descriptions with high performance.


<details>
  <summary>Details</summary>
Motivation: E-commerce generates vast unstructured product data, complicating information retrieval. This motivates the need for automated methods like Knowledge Graphs to structure such data efficiently.

Method: The framework employs AI agents across three stages: ontology creation and expansion, refinement, and knowledge graph population, relying on LLMs for automation and not using predefined schemas.

Result: The system achieved over 97% property coverage and reduced redundancy when tested on air conditioner product data.

Conclusion: The automated framework validates the utility of LLMs for scalable and effective product knowledge structuring, aiding intelligent data integration and application in retail.

Abstract: The rapid expansion of e-commerce platforms generates vast amounts of unstructured product data, creating significant challenges for information retrieval, recommendation systems, and data analytics. Knowledge Graphs (KGs) offer a structured, interpretable format to organize such data, yet constructing product-specific KGs remains a complex and manual process. This paper introduces a fully automated, AI agent-driven framework for constructing product knowledge graphs directly from unstructured product descriptions. Leveraging Large Language Models (LLMs), our method operates in three stages using dedicated agents: ontology creation and expansion, ontology refinement, and knowledge graph population. This agent-based approach ensures semantic coherence, scalability, and high-quality output without relying on predefined schemas or handcrafted extraction rules. We evaluate the system on a real-world dataset of air conditioner product descriptions, demonstrating strong performance in both ontology generation and KG population. The framework achieves over 97\% property coverage and minimal redundancy, validating its effectiveness and practical applicability. Our work highlights the potential of LLMs to automate structured knowledge extraction in retail, providing a scalable path toward intelligent product data integration and utilization.

</details>


### [15] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: The paper proposes an improved method for breaking symmetries in constraint programming by exploiting abstract structures' representations, leading to faster solving times.


<details>
  <summary>Details</summary>
Motivation: Traditional symmetry-breaking techniques, while successful, perform poorly on abstract variables because they generate complex constraints. This research aims to address that inefficiency.

Method: The paper introduces an incomplete method to break symmetries in abstract structures, specifically targeting symmetries from indistinguishable objects and leveraging better representations of abstract structures.

Result: The proposed method was found to outperform existing approaches (cited from Akgün et al. 2025) in terms of speed.

Conclusion: The new symmetry-breaking method for abstract structures improves efficiency, demonstrating significant time-saving compared to prior methods.

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [16] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: This study explores role allocation strategies in Multi-Agent Debate (MAD) and introduces the "Truth Last" strategy, improving reasoning task performance by 22%. It also proposes MADC to ensure consistency in truth determination, enhancing MAD scalability across LLMs.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to enhance reasoning abilities in Multi-Agent Debate (MAD) by addressing the underexplored impact of role allocation strategies, aiming for better performance in reasoning tasks.

Method: The study introduces a role allocation strategy called "Truth Last" and proposes a framework called MADC. MADC uses path consistency among roles to simulate the role likely representing the truth.

Result: The "Truth Last" strategy improved MAD performance by 22%. MADC proved effective across 9 models, showcasing its ability to enhance reasoning capabilities consistently.

Conclusion: Role allocation can significantly impact MAD performance, and utilizing strategies like MADC provides an effective solution for scalability and overcoming existing performance bottlenecks in reasoning tasks.

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [17] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: This paper proposes Differentiable Simulation for Search (DSS), optimizing autonomous driving decisions using a differentiable simulator for accurate state predictions and efficient action sequence planning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance autonomous driving by enabling safe and accurate decision-making in complex traffic scenarios through effective action planning.

Method: The proposed DSS framework integrates the differentiable simulator Waymax as both a state predictor and a critic, using gradient descent to optimize actions across imagined future trajectories.

Result: Experimental evaluations show DSS significantly improves tracking and path planning accuracy compared to other methods (like model-free RL, imitation learning, etc.).

Conclusion: DSS effectively combines planning gradients and stochastic search to provide a robust and accurate planning framework for autonomous driving applications.

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [18] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj is a novel dataset and framework designed to study human-like reasoning using temporally ordered actions from visual tasks in the ARC dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing ARC approaches that rely on static input-output data, by introducing temporally detailed human reasoning trajectories for better modeling and understanding.

Method: The method involves recording human interaction data via the O2ARC interface, capturing object-level actions, and constructing a reasoning pipeline combining MDP formulations and integration with various learning methods.

Result: ARCTraj contains 10,000 reasoning trajectories annotated with rich metadata, demonstrating the structure and diversity of human reasoning through analyses of spatial selection and strategies.

Conclusion: ARCTraj provides a structured foundation for studying and advancing explainability and generalizable human-like reasoning in AI research.

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [19] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: The paper presents a new method for Generalised Planning to create programs solving related planning problems, achieving improved efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the need for a method that synthesizes generalised plans to handle multiple related planning problems effectively, outperforming existing approaches.

Method: The method involves computing optimal plans per goal atom for training problems, goal regression, and lifting outputs into Condition → Actions rules, which can generalize and prune the search space.

Result: The proposed approach shows superior synthesis cost, planning coverage, and solution quality over state-of-the-art planners across various domains.

Conclusion: The method is theoretically validated to learn valid generalised plans and improves notable planning metrics, advancing both theoretical and practical aspects of GP.

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [20] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: The paper introduces a benchmark named GGBench to evaluate the geometric generative reasoning ability of Unified Multimodal Models, addressing limitations of existing assessments.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to evaluate the integrated generative reasoning abilities of Unified Multimodal Models, particularly the fusion of cross-modal skills such as language understanding and precise visual generation.

Method: The authors propose GGBench, a new benchmark focused on geometric construction, which demands the integration of language comprehension and visual generation to solve reasoning tasks.

Result: GGBench provides a systematic evaluation framework for assessing a model's ability to reason and actively construct solutions, filling the evaluation gap in generative and cross-modal reasoning.

Conclusion: GGBench sets a higher standard for measuring multimodal AI systems' cognitive and generative reasoning capabilities by targeting a holistic evaluation approach.

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [21] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: This paper introduces a protocol called Multi-agent Undercover Gaming (MUG) to mitigate hallucination issues in large language models (LLMs) through multimodal counterfactual tests.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address hallucination problems in LLMs and improve the reliability of multi-agent systems, as traditional methods like Multi-Agent Debate assume perfect rationality of agents, which may not be realistic.

Method: MUG employs social deduction-inspired games combined with multimodal counterfactual testing. Reference images are altered to introduce counterfactual evidence and agents' ability to detect these changes is tested to identify hallucinating agents.

Result: MUG enhances multimodal reasoning by enabling factual verification outside statistical consensus, facilitating dynamic cross-evidence reasoning, and encouraging active probing discussions between agents.

Conclusion: MUG provides a robust framework for improving multimodal reasoning in LLMs by mitigating hallucination issues through innovative methods like counterfactual testing, dynamic evidence modification, and active reasoning protocols.

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [22] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR (slow-thinking for table reasoning) is a framework for enhancing table reasoning in LLMs using step-by-step thinking, uncertainty-aware inference, and difficulty-aware reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses two limitations in table reasoning with LLMs: a lack of deep, iterative reasoning and the instability of reasoning processes, which harm reliability.

Method: The STaR framework uses slow-thinking capabilities with explicit step-by-step thinking and uncertainty quantification. It employs difficulty-aware reinforcement learning during training to progress from simple to complex queries and integrates token-level confidence and answer consistency during inference.

Result: STaR demonstrates superior performance, enhanced reasoning stability, and strong generalization across out-of-domain datasets in benchmarks.

Conclusion: STaR shows that incorporating cognitive-inspired reasoning processes can make LLMs more reliable and effective for table reasoning applications.

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [23] [UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios](https://arxiv.org/abs/2511.11252)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.AI

TL;DR: UAVBench introduces a benchmark dataset for evaluating UAV reasoning capabilities using LLMs, featuring realistic flight scenarios and reasoning questions.


<details>
  <summary>Details</summary>
Motivation: Current autonomous aerial systems lack standardized benchmarks for evaluating reasoning capabilities of LLMs, limiting advancements in mission planning and decision-making.

Method: UAVBench creates a dataset with 50,000 UAV scenarios validated through taxonomy-guided LLM prompting and safety checks. It also provides an extension with reasoning-focused multiple-choice questions.

Result: The benchmarking of 32 advanced LLMs revealed strong performance in perception and policy reasoning but challenges in ethical and resource-constrained decision-making.

Conclusion: UAVBench offers a reproducible framework to benchmark AI reasoning intelligence in UAV contexts, facilitating advancements in UAV decision-making and cognition under realistic scenarios.

Abstract: Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench

</details>


### [24] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: The paper presents AIonopedia, a Large Language Model-based system for discovering Ionic Liquids (ILs) with accurate predictions and effective wet-lab validation.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in discovering novel Ionic Liquids, including limited data, poor predictive model accuracy, and fragmented workflows.

Method: The study develops AIonopedia, a Large Language Model-augmented multimodal domain foundation model combined with hierarchical search architecture for IL screening, trained on a curated IL dataset.

Result: AIonopedia demonstrated superior accuracy, effective IL modification predictions, and generalized well in wet-lab validations of challenging tasks.

Conclusion: AIonopedia shows significant promise in accelerating the discovery of novel Ionic Liquids through robust predictions and practical validation results.

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [25] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: The paper addresses the lack of traceability and documentation in AI decisions and introduces a workflow that ensures tamper-proof and verifiable documentation using confidential computing.


<details>
  <summary>Details</summary>
Motivation: Concerns over the harm caused by AI decisions and the lack of proper documentation to hold systems accountable or trace responsibility motivate this paper.

Method: The authors expand the DBOM concept into a practical workflow using confidential computing, ensuring documentation of all components in AI training and inference.

Result: The workflow is demonstrated through an app that distinguishes between poisonous and edible mushrooms, showcasing practical implementation.

Conclusion: This paper proposes a groundbreaking solution to enhance accountability in AI systems through tamper-proof and exhaustive documentation, making AI decisions traceable and defensible in legal contexts.

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [26] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: The paper introduces contrastive ABox explanations, providing a way to explain why a certain instance belongs to a class and another does not, within the realm of description logic ontologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance explanation methods by introducing a framework that simultaneously explains differences and commonalities between two instances, enhancing understanding beyond isolated positive or missing entailments.

Method: The authors formalized contrastive explanations for ABox reasoning in description logics and analyzed the computational complexity under varying conditions. They also implemented and evaluated a method for generating these explanations.

Result: The implementation successfully computed contrastive explanations on generated problems involving realistic knowledge bases, demonstrating the feasibility of their approach.

Conclusion: Contrastive explanations provide a novel way to disentangle and clarify relationships between instances and properties in knowledge bases, with practical applicability and manageable computational complexity.

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [27] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: The paper introduces EcoAlign, a cost-efficient framework for aligning LVLMs while balancing safety, utility, and operational costs.


<details>
  <summary>Details</summary>
Motivation: LVLMs have strong reasoning capabilities but face vulnerabilities and inefficiencies in current alignment methods, which fail to balance safety, utility, and cost while ensuring robust deliberation.

Method: EcoAlign treats LVLMs as boundedly rational agents that build thought graphs, scoring actions using net present value principles and ensuring safety with the weakest-link rule.

Result: Experiments show EcoAlign improves safety and utility performance compared to state-of-the-art methods while reducing computational costs.

Conclusion: EcoAlign offers an efficient, principled approach to LVLM alignment, addressing vulnerabilities and economic inefficiencies effectively.

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [28] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: This study introduces a hybrid framework, RLSLM, to ensure socially-aware agent navigation by combining rule-based and data-driven approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for navigating in human-populated spaces either lack flexibility or fail to align well with human intuitions.

Method: RLSLM integrates a rule-based Social Locomotion Model into a reinforcement learning framework to optimize human comfort and mechanical energy during navigation.

Result: RLSLM produces socially aligned navigation policies with minimal training and achieves strong performance in immersive VR-based human-agent interaction tests.

Conclusion: The framework successfully combines cognitive science and machine learning for scalable and interpretable social navigation solutions.

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [29] [KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics](https://arxiv.org/abs/2511.11357)
*Haixin Li,Yanke Li,Diego Paez-Granados*

Main category: cs.AI

TL;DR: KarmaTS is a framework to create causal graphical models for multivariate time series analysis, enabling simulation and interventions.


<details>
  <summary>Details</summary>
Motivation: The work addresses challenges in validating causal discovery with limited access to real physiological data by generating synthetic data with known causal dynamics.

Method: The paper proposes a mixed-initiative framework combining expert knowledge with algorithmic suggestions to construct discrete-time structural causal models, adaptable for simulation or interventions.

Result: KarmaTS facilitates the synthesis and augmentation of datasets with causal structures, useful for analyzing distribution shifts and validating causal discovery algorithms.

Conclusion: KarmaTS serves as a versatile tool for simulating, validating, and benchmarking causal mechanisms in multivariate time series data using expert-informed methods.

Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.

</details>


### [30] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: The paper introduces MarsRL, a reinforcement learning framework designed to improve multi-agent reasoning systems by jointly optimizing agents using agentic pipeline parallelism.


<details>
  <summary>Details</summary>
Motivation: The limited output length of LLMs restricts reasoning depth, and multi-agent systems struggle to generalize in open-source models due to their lack of critic and correction capabilities.

Method: MarsRL utilizes agent-specific reward mechanisms to address noise and pipeline-inspired training for improved efficiency in long reasoning processes.

Result: MarsRL boosts performance significantly, achieving AIME2025 accuracy improvements from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, outperforming a more extensive model.

Conclusion: MarsRL showcases the potential to significantly advance multi-agent reasoning systems, making them applicable to a broader range of reasoning tasks.

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [31] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: This paper surveys robust and efficient communication strategies for multi-agent reinforcement learning (MARL) under real-world constraints, focusing on practical applications and future challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the unrealistic assumptions of instant, reliable, and unlimited communication in existing MARL frameworks by exploring practical solutions for real-world deployments.

Method: The authors systematically review advancements in MARL communication under constraints like message perturbations, delays, and bandwidth limitations, while analyzing applications in autonomous driving, mapping, and federated learning.

Result: The review identifies significant progress in MARL communication strategies under constraints, focusing on their practical utility in real-world scenarios.

Conclusion: A unified approach is proposed, co-designing communication, learning, and robustness to bridge theoretical models with real-world MARL implementations, while highlighting future research challenges.

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [32] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet innovatively integrates various EHR data modalities using advanced techniques for improved chronic disease prediction with over 94% accuracy.


<details>
  <summary>Details</summary>
Motivation: Predictive models often fail to account for interactions, redundancies, and temporal patterns in multimodal EHR data. The paper aims to address these limitations.

Method: The paper introduces CURENet, leveraging LLMs for processing clinical text and lab tests alongside transformer encoders for sequential data integration.

Result: CURENet demonstrated high accuracy (over 94%) on chronic disease prediction in a multi-label framework using MIMIC-III and FEMH datasets.

Conclusion: The study indicates multimodal EHR integration can significantly improve predictive modeling and clinical decision-making, aiding better patient outcomes.

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [33] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: The paper introduces Experience-Guided Reasoner (EGuR), a system that optimizes problem-solving strategies at runtime based on experience, enhancing computational efficiency and accuracy in AI tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of static or minimally adaptive AI systems in dynamically modifying problem-solving strategies, enabling better adaptability and efficiency.

Method: EGuR utilizes a meta-strategy approach where a Guide component generates problem-specific strategies and a Consolidator refines future strategies based on feedback. It adapts various parameters like prompts, sampling configurations, and control logic in real-time.

Result: EGuR showed significant performance, achieving up to 14% better accuracy than alternatives and reducing computational costs by over 111x across several challenging benchmarks, with improvements scaling alongside its experience.

Conclusion: EGuR demonstrates the feasibility and benefits of dynamically adaptable AI systems, showcasing efficiency and accuracy gains in solving complex problems with continuous learning from experience.

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


### [34] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: The paper proposes a test-time alignment technique for AI agents to ensure ethical decision-making without retraining, balancing reward maximization and ethical alignment.


<details>
  <summary>Details</summary>
Motivation: The need for AI decision-making agents to align with human values in complex environments while avoiding harmful behavior and managing the trade-off between reward maximization and ethical alignment.

Method: Test-time alignment using model-guided policy shaping that allows control over behavioral attributes, operates across diverse environments, and avoids the need for agent retraining.

Result: The proposed approach, evaluated using the MACHIAVELLI benchmark, demonstrated effective control over ethical decision-making, mitigating unethical behavior and achieving scalable alignment.

Conclusion: Test-time policy shaping is a promising solution for aligning pre-trained AI agents with ethical standards in varied and complex scenarios while maintaining effectiveness.

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [35] [Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity](https://arxiv.org/abs/2511.10760)
*Emad Haque,Pragnya Sudershan Nalla,Jeff Zhang,Sachin S. Sapatnekar,Chaitali Chakrabarti,Yu Cao*

Main category: cs.AR

TL;DR: The study investigates simplifying I/O circuitry in chiplet interface design for 2.5D/3D integration, achieving miniaturization and improved composability.


<details>
  <summary>Details</summary>
Motivation: To address the significant area overhead caused by conventional I/O circuitry in advanced packaging technologies, which impedes chiplet size reduction below 100 mm2.

Method: Utilizing parasitic extraction and SPICE simulations to evaluate and simplify ESD protection and inter-chiplet signaling in chiplet interface design.

Result: Proved that ESD protection and inter-chiplet signaling requirements can be minimized in future technologies, facilitating chiplet miniaturization.

Conclusion: Simplified I/O circuitry enhances scalability, composability, and reusability of smaller chiplets in 2.5D/3D heterogeneous systems.

Abstract: The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets.

</details>


### [36] [MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores](https://arxiv.org/abs/2511.10909)
*Peichen Xie,Yang Wang,Fan Yang,Mao Yang*

Main category: cs.AR

TL;DR: The paper introduces MMA-Sim, a tool that provides a detailed, bit-accurate reference model of matrix multiplication accelerators (MMAs) in GPUs, addressing numerical inconsistencies affecting deep neural networks.


<details>
  <summary>Details</summary>
Motivation: To investigate and address the numerical imprecisions and inconsistencies in MMAs embedded in modern GPUs, which can hinder the stability and reproducibility of DNN training and inference.

Method: The authors develop MMA-Sim by dissecting MMAs through targeted and randomized tests, deriving nine arithmetic algorithms to replicate their behavior. The methodology includes large-scale validation to ensure equivalence with hardware results.

Result: MMA-Sim successfully simulates the behaviors of MMAs in GPUs with bitwise accuracy. The tool reveals arithmetic discrepancies, including undocumented behaviors, and their potential impacts on DNN training.

Conclusion: The study highlights the need for transparent MMA arithmetic specifications and demonstrates MMA-Sim as an essential tool to uncover and mitigate numerical inconsistencies in DNN workloads.

Abstract: The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.
  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors.

</details>


### [37] [T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup](https://arxiv.org/abs/2511.11248)
*Jianyu Wei,Qingtao Li,Shijie Cao,Lingxiao Ma,Zixu Hao,Yanyong Zhang,Xiaoyan Hu,Ting Cao*

Main category: cs.AR

TL;DR: The paper addresses the inefficiencies of LLM inference on NPUs compared to CPUs, proposing a novel table lookup method to speed up computations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address slow performance of current LLM inference on NPUs caused by inefficiency in operations like dequantization.

Method: The proposed method introduces table lookup optimization for NPUs through fused table-based dequantization and concurrency-hierarchy-guided tiling, implemented as a three-stage pipeline for prefill and vector unit mapping for decoding.

Result: The method achieves 1.4x speedup for prefill and 3.1x speedup for decoding, alongside 84% energy savings, compared to the baseline NPU methods.

Conclusion: The study demonstrates that table lookup on NPUs improves efficiency significantly, outperforming existing methods without compromising accuracy.

Abstract: Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [38] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: The paper proposes a cycle detection framework for Large Language Models that addresses hidden execution cycles through unsupervised structural and semantic analyses.


<details>
  <summary>Details</summary>
Motivation: Detect costly inefficiencies caused by non-deterministic behaviors and hidden cycles in Agentic applications powered by Large Language Models.

Method: A hybrid approach combining structural call stack analysis and semantic similarity analysis to detect explicit and subtle cycles in execution.

Result: The framework evaluated on 1575 LangGraph-based stock market trajectories achieved an F1 score of 0.72, outperforming individual structural and semantic methods.

Conclusion: The proposed method is promising but requires further refinement to address current limitations and improve its efficacy.

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [39] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: This work extends the Spectral Neuro-Symbolic Reasoning framework with semantic enhancements to boost accuracy, generalization, and scalability.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning accuracy, scalability, and robustness of the Spectral NSR framework for open-domain and real-world contexts.

Method: The paper introduces three preprocessing techniques: contextual node merging, entailment validation, and external knowledge graph alignment, all applied as upstream semantic refinements before spectral reasoning.

Result: The enhanced framework delivers consistent accuracy improvements of up to 3.8%, better handles adversarial cases, and reduces inference noise in benchmarks like ProofWriter and EntailmentBank.

Conclusion: These semantic refinements make the reasoning process robust, interpretable, and scalable while preserving the efficiency of the spectral reasoning engine.

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [40] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: The paper introduces a method leveraging large language models (LLMs) for generating high-quality analysis reports in modern warfare, overcoming limitations of traditional manual analysis.


<details>
  <summary>Details</summary>
Motivation: Enhance efficiency and accuracy in the analysis of simulation deductions for modern warfare by overcoming the limitations of time-consuming and error-prone manual methods.

Method: The proposed method decomposes complex tasks into sub-tasks with tailored prompts and employs multi-round interactions, self-checking, reflection, and custom tools for structured analysis and report generation.

Result: Evaluation results show that the method produces higher-quality reports than traditional approaches, achieving superior scores.

Conclusion: The approach successfully enhances the quality and adaptability of analytical reports, showcasing the potential of LLMs in structured analysis for diverse scenarios.

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [41] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: An architecture to embody historical characters using LLMs resolves trade-off between shallow and slow deep responses via efficient memory-based data augmentation.


<details>
  <summary>Details</summary>
Motivation: Address latency and shallowness in existing historical character embodiment in dialogue systems.

Method: Offline data augmentation, structured episodic memory, affective-semantic metadata, and parallel retrieval with two-stage processes.

Result: Outperformed traditional RAG when used with GPT-3.5/3 and achieved equal performance on GPT-4 with added visualization and biographical tools.

Conclusion: The system proves efficient for resource-limited contexts and generalizable for educational, research, and museum applications.

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [42] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: This paper introduces HyQuT, a hybrid quantum-classical large language model that successfully integrates quantum computing into large-scale generative natural language models, demonstrating its feasibility.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the integration of quantum computing into large-scale natural language generation tasks, addressing the limitations of existing quantum models in handling complex tasks.

Method: HyQuT combines variational quantum circuits (VQCs) with the Transformer framework, tested at 8M and 150M parameter scales, using a minimal number of qubits (10 qubits and 80 quantum gates) to replace a portion of classical parameters.

Result: The hybrid model achieves performance comparable to classical models while using fewer classical parameters, showcasing convergence stability and high-quality generation outputs.

Conclusion: Integrating quantum computing into large language models is feasible and effective, paving the way for advancements in quantum-enhanced natural language generation.

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [43] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: The paper scrutinizes LLMs' ability to process deadlines in time-sensitive applications, uncovering bimodal performance, brittle prompts, and systematic action bias, illustrating deployment risks without hybrid symbolic reasoning modules.


<details>
  <summary>Details</summary>
Motivation: To evaluate and mitigate the risks associated with deploying large language models (LLMs) in real-time, deadline-driven applications, as their capability to handle temporal constraints remains untested.

Method: The authors assess eight LLMs (2.8-8B parameters) using deadline detection tasks, investigating their performance, brittleness, and biases. Additionally, they test fine-tuning and discuss required architectural improvements.

Result: Models either excel (95% accuracy) or fail (50% accuracy) in deadline detection tasks. Some 7B parameter models perform poorly compared to smaller ones, and prompt formatting heavily affects outcomes. Fine-tuning improves performance partially, but systemic architectural limitations remain unresolved.

Conclusion: Current autoregressive models are structurally ill-equipped to process temporal constraints reliably. Developers must incorporate hybrid architectures with explicit symbolic reasoning to address the identified risks for real-time deployment.

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [44] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: This paper introduces a Bayesian approach to quantify uncertainty in the evaluation of large language models' (LLMs) text generation behavior.


<details>
  <summary>Details</summary>
Motivation: There is a need to assess how LLMs perform under various conditions, particularly to measure their production of harmful content and their resilience to adversarial inputs, while addressing the lack of statistical uncertainty quantification in current evaluation methods.

Method: The paper uses Bayesian modeling to quantify statistical uncertainty, focusing on binary evaluation metrics influenced by the probabilistic text generation strategies of LLMs. Case studies are provided for refusal rates to adversarial inputs and pairwise preferences between LLMs.

Result: The Bayesian approach demonstrates its capacity to quantify uncertainty effectively in binary evaluations, providing deeper insights into the behavior of LLM-based systems.

Conclusion: By incorporating Bayesian uncertainty quantification, evaluations of LLMs can be made more robust and reliable, enabling better understanding and assessment of these systems' behaviors.

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [45] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: This paper introduces PRO, a framework for automatic multi-objective alignment of LLMs by inferring dynamic preference weights for prompts, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in aligning LLMs with human preferences across multiple objectives, especially difficulties in manual preference specification and inefficiencies in training.

Method: PRO integrates a lightweight preference adapter that learns dynamic prompt-specific preference weights using normalized reward scores from reward models.

Result: Experiments across diverse tasks show that PRO outperforms existing methods in multi-objective alignment.

Conclusion: PRO demonstrates superior efficiency and alignment performance, overcoming limitations of manual preference specification and static weights in LLMs.

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [46] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: The paper introduces a method for generating improved patent embeddings using contrastive learning by utilizing different sections of patent documents as views.


<details>
  <summary>Details</summary>
Motivation: Address deficiencies in dropout augmentation for patent embedding generation by developing a method that maintains semantic cohesion and structural diversity.

Method: Utilizes intra-document section-based augmentation (e.g., abstract, claims, background) to create diverse views for contrastive learning.

Result: The self-supervised approach outperforms supervised methods on benchmarks for prior-art retrieval and classification, leveraging the diverse nature of sections.

Conclusion: Section-based augmentation exploits patents' inherent structure for scalable, cohesive, and generalizable representation learning, benefiting various patent-related tasks.

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [47] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: This paper evaluates 15 open-weight LLMs for extracting structured information from clinical reports across six diverse use cases at institutions in multiple countries. It finds that smaller general-purpose models achieve comparable performance to larger models, with specific prompting strategies boosting results.


<details>
  <summary>Details</summary>
Motivation: The authors aim to determine the capability of open-weight LLMs in extracting structured data from clinical text across different diseases, languages, and institutions, addressing scalability issues in clinical data curation.

Method: They evaluated 15 general-purpose and specialised LLMs using six prompting strategies across pathology and radiology reports in six distinct contexts, measuring performance with task-appropriate metrics and statistical analyses.

Result: Top-performing models achieved macro-average scores similar to inter-rater agreements. Few-shot and prompt graph strategies improved performance by approximately 13%. Task-specific factors influenced results more than model size or type.

Conclusion: Open-weight LLMs are viable tools for structured data extraction from clinical text, offering scalability and adaptability across multiple tasks, languages, and medical domains.

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [48] [Information Extraction From Fiscal Documents Using LLMs](https://arxiv.org/abs/2511.10659)
*Vikram Aggarwal,Jay Kulkarni,Aditi Mascarenhas,Aakriti Narang,Siddarth Raman,Ajay Shah,Susan Thomas*

Main category: cs.CL

TL;DR: This paper introduces an LLM-based method for extracting structured data from complex, multi-page government fiscal documents, achieving high accuracy by leveraging domain knowledge, context, and validation checks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of extracting accurate structured data from complex fiscal documents, which is difficult with traditional OCR techniques, especially in scenarios requiring verification of numeric data.

Method: The method employs a multi-stage pipeline using LLM-based techniques that incorporate domain knowledge, sequential context, and hierarchical relationships for multi-level validation checks to ensure data accuracy.

Result: The approach successfully processes multi-page fiscal documents (200+ pages) with high accuracy, transforming them into scalable and research-ready databases.

Conclusion: The study shows that LLMs can effectively interpret complex tabular and hierarchical data structures, offering broader applications in fiscal data analysis, particularly in developing countries.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.

</details>


### [49] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: This paper introduces a framework for improving lossless text compression by combining traditional universal compressors with pretrained neural models using a weighted product of experts technique.


<details>
  <summary>Details</summary>
Motivation: Traditional universal compressors like gzip are efficient but have suboptimal compression rates compared to neural compressors, which model data distributions more effectively but lack generalization to unseen data.

Method: The paper proposes Test-Time Steering using a Weighted Product of Experts (wPoE) approach, where a universal compression model is adaptively combined with a pretrained neural language model during inference.

Result: Experiments show improved text compression performance without the need for fine-tuning. This method works well across diverse data distributions and integrates effectively with autoregressive language models.

Conclusion: The proposed framework enhances text compression rates, addresses neural compressor generalization issues, and provides a practical solution adaptable to multiple language models.

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [50] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: The paper explores reasoning challenges in Tool-augmented Language Models (TaLMs), identifying reasoning degradation, named Tool-Induced Myopia (TIM), despite improved accuracy through tool use.


<details>
  <summary>Details</summary>
Motivation: To investigate whether gains in accuracy by TaLMs using tools ensure trustworthy reasoning, as they are increasingly used for complex problem-solving.

Method: The study analyzed reasoning degradation in TaLMs through a benchmark (PYMATH) and a multi-dimensional evaluation suite, focusing on reasoning shifts when using the Code Interpreter tool.

Result: TaLMs show up to 19.3% accuracy improvements but experience reasoning deterioration, with frequent tool use resulting in incoherent reasoning and higher rates of global reasoning failures.

Conclusion: The study highlights reasoning flaws in TaLMs due to Tool-Induced Myopia (TIM) and proposes a framework to align tool use for improving both reasoning depth and final-answer accuracy.

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [51] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: The paper presents a novel method to improve diversity in structured outputs generated by large language models (LLMs) while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Structured generation methods currently lack diversity in outputs, which limits their effectiveness, especially in applications requiring varied results.

Method: The authors introduce an automaton-based method that uses traversal history to guide LLMs, encouraging novel structural patterns during generation.

Result: The proposed method significantly enhances both structural and content diversity, achieving better diversity compared to traditional methods while retaining generation efficiency.

Conclusion: The method is effective in generating diverse and meaningful structured outputs, as highlighted in a case study related to testing open-source libraries.

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [52] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: This paper evaluates seven advanced large language models on low-resource languages (Cantonese, Japanese, Turkish) across four tasks, revealing significant gaps in cultural and linguistic understanding.


<details>
  <summary>Details</summary>
Motivation: To address the limited exploration of LLM effectiveness in low-resource languages and identify their strengths and gaps in diverse linguistic scenarios.

Method: The study benchmarks seven LLMs using human evaluations and automated metrics across four tasks, incorporating analysis of fluency, factual accuracy, and cultural appropriateness.

Result: Proprietary models like GPT-4o and GPT-4 perform better in multilingual tasks, but struggle with linguistic challenges like Turkish morphology and Cantonese colloquialisms. Smaller open-source models show larger performance disparities.

Conclusion: Improvements are required for LLMs to better handle culturally sensitive and linguistically complex scenarios in low-resource languages; the benchmark data is released for advancing research.

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [53] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: Guard models are improved in their semantic robustness using a novel framework addressing sensitivity to linguistic variations with significant improvements in consistency and reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of semantic grounding in guard models, as they show sensitivity to meaning-preserving linguistic variations which causes vulnerability in maintaining safety.

Method: The paper introduces a self-supervised framework that uses paraphrase sets to enforce prediction consistency through a skew-aware aggregation strategy, contrasting with standard aggregation methods like mean and median.

Result: Their approach reduces semantic variability across paraphrases by approximately 58%, improves benchmark accuracy by around 2.5%, and generalizes to unseen stylistic variations. Additionally, they discovered an improvement in model calibration by up to 40%.

Conclusion: Semantic consistency should be treated as a primary training objective. The developed framework provides a scalable method to enhance the reliability and robustness of guard models.

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [54] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: The paper introduces a framework, STaDS, to evaluate the understanding of Large Language Models (LLMs) through structured decision scenarios, beyond just predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the gap between predictive accuracy and genuine understanding in LLMs, where accuracy does not necessarily equate to relying on consistent, domain-grounded factors for predictions.

Method: The authors developed STaDS, a set of structured decision simulations designed to test LLMs on three aspects: (i) in-depth comprehension of instructions and questions, (ii) knowledge-based prediction, and (iii) proper reliance on relevant decision factors.

Result: Testing 9 advanced LLMs across 15 decision settings revealed that most struggle with consistency across domains; accuracy does not guarantee global faithfulness or proper reasoning alignment.

Conclusion: Evaluating LLM understanding cannot rely solely on accuracy. Instead, new global-level evaluation and development frameworks are critical to improving their decision-making capacity.

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [55] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: The study compares traditional machine learning (ML) to deep transfer learning (DTL) for predicting spoken language development in children with cochlear implants, finding DTL significantly outperforms ML with over 92% accuracy.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the variability in language outcomes among children with cochlear implants, which cannot be reliably predicted using existing factors like age at implantation or residual hearing.

Method: The study develops and tests both traditional ML and DTL binary classification models, using neuroanatomic brain features, to predict post-implant language development in 278 children with bilateral severe-to-profound sensorineural hearing loss (SNHL).

Result: DTL models utilizing a bilinear attention-based fusion strategy achieved superior prediction metrics: 92.39% accuracy, 91.22% sensitivity, 93.56% specificity, and an AUC of 0.977, exceeding the performance of traditional ML models.

Conclusion: Deep transfer learning demonstrates significant promise and feasibility as a superior predictive tool for spoken language development in children with cochlear implants, supporting its global applicability in CI programs.

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [56] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: The paper proposes a novel approach for code-switching speech translation using Large Language Models (LLMs) and a Mixture of Experts (MoE) speech projector.


<details>
  <summary>Details</summary>
Motivation: The challenges in code-switching speech translation arise from semantic complexity and lack of adequate data, necessitating a more efficient and precise modeling solution.

Method: The method involves a Mixture of Experts speech projector tuned for specific languages, combined with a multi-stage training paradigm utilizing monolingual ASR and ST data. Loss functions are designed for efficient expert allocation and transitional data alignment.

Result: The approach significantly improves semantic modeling and translation capabilities even under limited code-switching data conditions, as demonstrated on widely used datasets.

Conclusion: Using a combination of tailored modeling and optimized training routines effectively addresses challenges in code-switching speech translation, demonstrating general effectiveness.

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [57] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: The paper introduces GVF Finetuning, a novel method to reduce visual hallucination in MLLMs by integrating factual signals. It significantly improves factual consistency while maintaining overall performance.


<details>
  <summary>Details</summary>
Motivation: The research aims to address visual hallucination in MLLMs, which hampers reliability by fabricating details inconsistent with image content. Existing solutions inadequately address factual reasoning.

Method: The GVF Finetuning approach includes: 1) Factual Anchor Data Augmentation to enrich training data, 2) Fact-Aware Instruction Tuning to incorporate factual cues into instructions, and 3) Factual Consistency Loss function to penalize inaccuracies.

Result: GVF outperforms standard fine-tuning on the VHTest benchmark for OEQ and YNQ, while retaining or slightly improving performance on general multimodal benchmarks like MME and POPE.

Conclusion: GVF Finetuning effectively mitigates visual hallucinations in MLLMs without compromising overall multimodal reasoning and understanding, proving its practicality and reliability.

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [58] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: A review of LLMs in materials science, covering literature mining, predictive modeling, and agentic systems, advocating for open-source models.


<details>
  <summary>Details</summary>
Motivation: To examine and discuss the transformative impact of LLMs on the materials science field and encourage the development and adoption of open-source AI solutions for scientific discovery.

Method: The paper reviews applications of large language models in extracting data, modeling structure-property relationships, and coordinating computational and experimental tools in materials science.

Result: Open-source models showed comparable performance to commercial alternatives with additional benefits like transparency, cost-effectiveness, and data privacy.

Conclusion: The paper recommends adopting open-source LLMs for accessible, cost-effective, and community-driven advances in materials science research.

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [59] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: The paper introduces a framework for continual learning in text-to-SQL systems, utilizing human feedback to refine SQL queries and distill reusable knowledge.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with database schema understanding and domain-specific tacit knowledge when generating SQL queries from natural questions.

Method: A framework for continual learning using human feedback is developed, where feedback refines SQL queries. The refined knowledge is stored in a structured memory and reused for future tasks. Various agent architectures for capturing and retrieving this knowledge are designed and tested.

Result: Memory-augmented agents, especially the Procedural Agent, show significant improvements in accuracy and error reduction on the BIRD benchmark Dev set by leveraging human feedback.

Conclusion: Continual learning systems that transform human expertise into reusable knowledge can significantly enhance adaptive and domain-aware text-to-SQL capabilities.

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [60] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: The paper introduces a novel demonstration selection method for in-context learning (ICL) called TopK + Label Distribution Divergence (L2D), which improves the performance of large language models (LLMs) in text classification.


<details>
  <summary>Details</summary>
Motivation: To improve the effectiveness of in-context learning by addressing limitations in existing demonstration selection methods that often overlook label distribution alignment.

Method: The authors propose a two-stage demonstration selection approach (TopK + L2D), utilizing a fine-tuned small language model (SLM) to generate label distributions and ensure alignment between test inputs and candidate demonstrations.

Result: Experiments on seven text classification benchmarks demonstrated that L2D consistently outperforms previous selection methods, with performance correlating to the accuracy of the SLM used for label distribution estimation.

Conclusion: Aligning demonstrations in both semantic similarity and label distribution is key to optimizing ICL performance, and the proposed L2D method effectively accomplishes this on text classification tasks.

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [61] [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)
*Shien Zhu,Samuel Bohl,Robin Oester,Gustavo Alonso*

Main category: cs.CL

TL;DR: This paper introduces a new pre-attention expert prediction mechanism for more accurate and efficient expert prefetching in Mixture-of-Experts (MoE) Large Language Models (LLMs), achieving significantly improved accuracy compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current expert prediction methods for MoE models, such as low prediction accuracy and unoptimized first-layer performance, while avoiding high computation overhead from complex solutions.

Method: The authors propose 'pre-attention expert prediction,' leveraging the activations before the attention block in the same layer with two linear functions and a ranking-aware loss. This method ensures accurate expert ranking predictions and supports prefetching at the first layer.

Result: The proposed approach achieves expert prediction accuracy of 93.03% on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing around 15% absolute accuracy improvement over existing methods.

Conclusion: Pre-attention expert prediction provides a lightweight yet accurate solution for expert prefetching in MoE models, enhancing both efficiency and accuracy without introducing significant computational cost.

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.

</details>


### [62] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: This paper introduces SpiderGen, an LLM-based tool that facilitates Life Cycle Assessments (LCAs) to estimate the environmental impact of consumer products, achieving significant cost and time savings while maintaining reasonable accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the global challenge of climate change by simplifying and reducing the cost and effort involved in Life Cycle Assessments (LCAs), which are critical for understanding the environmental impact of consumer goods.

Method: The researchers developed SpiderGen, which integrates the taxonomy and methodology of traditional LCAs with the reasoning capabilities of large language models (LLMs) to generate procedural LCA information. Its outputs were evaluated against real-world LCA documents.

Result: SpiderGen demonstrated an F1-Score of 62% for process information accuracy and outperformed baseline techniques like chain-of-thought and one-shot prompting. While some errors were observed, SpiderGen significantly enhanced efficiency and reduced costs.

Conclusion: SpiderGen has the potential to revolutionize LCA generation by reducing costs to under $1 per assessment and completion time to under 10 minutes, compared to the traditional LCA process costing $25,000 and requiring up to 21-person days, with reasonable accuracy achieved.

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [63] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: This paper evaluates how different alignment methods of large language models affect their susceptibility to prompt attacks by analyzing Attack Success Rate (ASR) through systematic statistical tests.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand model vulnerabilities and improve the evaluation of alignment methods against prompt attacks in LLMs.

Method: The study involves analyzing LLMs aligned by Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF) using statistical tests to measure ASR sensitivity to variations in attack prompts.

Result: Results suggest even minor variations in prompts can significantly affect ASR, highlighting that existing benchmarks do not fully uncover vulnerabilities of models and alignment techniques.

Conclusion: The paper presents crucial insights on how prompt variations impact ASR in aligned models, suggesting that systematic analyses are necessary for robust model attack evaluation.

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [64] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: This paper evaluates the robustness of large language models (LLMs) in interactive, multi-turn settings, revealing vulnerabilities and proposing methods to predict their accuracy dynamics.


<details>
  <summary>Details</summary>
Motivation: With LLMs increasingly used in real-world applications, ensuring their robustness to repeated interactions and consistent reasoning is critical.

Method: The study uses multi-turn prompts to assess LLMs' answer changes, employs Markov chains for modeling accuracy dynamics, and linear probes to predict future answer changes.

Result: The paper shows LLMs exhibit significant robustness issues under repeated questioning, with accuracy drops over turns. Markov chains model accuracy transitions effectively, and linear probes predict answer changes from models' hidden states.

Conclusion: Stationary accuracy is introduced as a robustness metric for interactive tasks with LLMs, highlighting the need to address their fragility for deployment in high-stakes settings requiring consistent reasoning.

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [65] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: The paper investigates gender bias dynamics across recursive text generations and shows that low bias amplifies while high bias decays. A mitigation approach using contrastive augmentation reduces downstream bias significantly.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate potential bias amplification in synthetic dataset generation via recursive prompting with language models.

Method: Evaluated gender bias dynamics using three methods: rule-based pattern matching, embedding-based semantic similarity, and measuring downstream task performance across different initial bias levels and mitigation strategies.

Result: Low initial bias amplifies (+36%) and high initial bias decays (-26%) toward model inherent bias. Contrastive augmentation effectively reduces downstream bias (98.8% low bias, 91% average) despite seeming higher semantic bias.

Conclusion: Equilibrium dynamics occur in recursive bias amplification, and combining multidimensional evaluation is essential to balance fairness in synthetic data generation.

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [66] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: This paper investigates the hidden language in multimodal systems through a multi-round 'telephone game' approach, studying their concept connection strengths and biases.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the opaque nature of hidden languages in closed-source multimodal systems and the need for better interpretability and controllability of these systems.

Method: The authors use multimodal systems' preference bias and a telephone game framework to study concept co-occurrence and connection strengths. They also introduce a Telescope dataset to evaluate these systems through iterative testing.

Result: Quantitative insights into multimodal systems' concept connections and biases were uncovered, along with unexpected relationships transcending common textual and visual similarities.

Conclusion: The research provides a new outlook on understanding the preferences and conceptual biases in multimodal systems, paving the way for further studies on their interpretability and control.

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [67] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: The paper introduces Squid Game, an adversarial benchmark designed to evaluate large language models (LLMs) in dynamic, resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks face challenges with data contamination and do not test LLMs under pressure or asymmetric information scenarios.

Method: They propose 'Squid Game,' an interactive evaluation environment consisting of six elimination-style levels focusing on diverse skills. Over 50 LLMs are tested against dynamic adversarial scenarios.

Result: Results highlight generational performance shifts in LLMs, speculative shortcuts taken by models, and the need for dynamic evaluation as complementary to static benchmarks.

Conclusion: Dynamic benchmarks like Squid Game are essential for comprehensive assessments of LLMs, providing insights beyond static evaluations.

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [68] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: This study examines if AI text-to-speech systems can replicate human-like speech patterns, particularly slowing down speech rate for politeness, and finds evidence that they can.


<details>
  <summary>Details</summary>
Motivation: To assess whether AI voice systems can adopt implicit human social conventions, such as conveying politeness through speech rate, which has not been explicitly programmed.

Method: 22 synthetic voices from AI Studio and OpenAI were tested by prompting them to read scripts under 'polite and formal' and 'casual and informal' conditions, measuring the speech durations.

Result: Across both platforms, slower speech was observed in polite conditions compared to casual ones, with statistically significant results, indicating AI's ability to exhibit human-like speech behavior.

Conclusion: AI text-to-speech systems can implicitly learn subtle human communication cues, reinforcing social norms, and positioning AI as a socially interactive entity.

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [69] [Where does an LLM begin computing an instruction?](https://arxiv.org/abs/2511.10694)
*Aditya Pola,Vineeth N. Balasubramanian*

Main category: cs.CL

TL;DR: The paper examines the layer in language models where instruction following begins, introducing datasets and a method to identify this onset point and its location across tasks and models.


<details>
  <summary>Details</summary>
Motivation: To understand and pinpoint where language models transition from reading instructions to executing them, providing insights into their instruction-following capabilities.

Method: The authors used activation patching on prompt pairs across three datasets and multi-hop compositions, measuring flip rates to detect the onset point where instruction following begins in Llama model layers.

Result: They identified a layer-wise inflection point (onset) in Llama models where interventions before it impact predictions, while interventions after it do not, providing a consistent means of locating instruction-following onset.

Conclusion: The study introduces a reproducible method to determine and compare the location of instruction-following onset across different tasks and model architectures, highlighting a critical structural insight into language models.

Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.

</details>


### [70] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: This paper investigates nation-level biases in Large Language Models (LLMs) using UN Security Council data, identifying variability in biases across models and contexts. It proposes a debiasing framework that enhances reasoning abilities and reduces biases, particularly effective in specific LLMs.


<details>
  <summary>Details</summary>
Motivation: To systematically explore and address nation-level biases in LLMs within the International Relations domain, ensuring fairness and accuracy in their applications.

Method: The study develops a bias evaluation framework with three tests to analyze biases in LLMs and proposes a debiasing method using Retrieval-Augmented Generation and Reflexion-based techniques.

Result: The study shows that biases in LLMs differ across models, tasks, and contexts. A proposed debiasing framework effectively reduces biases and enhances model performance, especially in GPT-4o-mini and LLama-3.3-70B.

Conclusion: LLMs exhibit complex, multidimensional biases that vary based on the model and context. Models with stronger reasoning abilities generally show reduced bias. The proposed debiasing framework proves effective, underlining the importance of addressing biases in LLM applications for International Relations.

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [71] [Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of Oogiri Generation and Evaluation](https://arxiv.org/abs/2511.09133)
*Ritsu Sakabe,Hwichan Kim,Tosho Hirasawa,Mamoru Komachi*

Main category: cs.CL

TL;DR: The paper explores humor capabilities in LLMs using Oogiri comedy games, presenting a multifaceted evaluation framework to overcome the limitations of single-dimensional benchmarks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in evaluating LLMs' humor capabilities by moving beyond simplistic 'funny or not' assessments and focusing on a multi-dimensional analysis.

Method: This study expanded Oogiri datasets with new sources and LLM-generated responses. Responses were annotated based on six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness. The research analyzed both LLMs' creative humor generation and their multi-dimensional evaluation capabilities.

Result: Results showed LLMs performed humor generation at a level between low- and mid-tier human participants, but lacked Empathy in their evaluation of humor. Empathy was identified as a key divergence between human and model assessments.

Conclusion: The study highlights the need for emotionally intelligent conversational agents and releases the annotated corpus to encourage further development in this area.

Abstract: Computational humor is a frontier for creating advanced and engaging natural language processing (NLP) applications, such as sophisticated dialogue systems. While previous studies have benchmarked the humor capabilities of Large Language Models (LLMs), they have often relied on single-dimensional evaluations, such as judging whether something is simply ``funny.'' This paper argues that a multifaceted understanding of humor is necessary and addresses this gap by systematically evaluating LLMs through the lens of Oogiri, a form of Japanese improvisational comedy games. To achieve this, we expanded upon existing Oogiri datasets with data from new sources and then augmented the collection with Oogiri responses generated by LLMs. We then manually annotated this expanded collection with 5-point absolute ratings across six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness. Using this dataset, we assessed the capabilities of state-of-the-art LLMs on two core tasks: their ability to generate creative Oogiri responses and their ability to evaluate the funniness of responses using a six-dimensional evaluation. Our results show that while LLMs can generate responses at a level between low- and mid-tier human performance, they exhibit a notable lack of Empathy. This deficit in Empathy helps explain their failure to replicate human humor assessment. Correlation analyses of human and model evaluation data further reveal a fundamental divergence in evaluation criteria: LLMs prioritize Novelty, whereas humans prioritize Empathy. We release our annotated corpus to the community to pave the way for the development of more emotionally intelligent and sophisticated conversational agents.

</details>


### [72] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: The paper introduces \PiAttention, a sparse Transformer designed to reduce complexity while improving performance in long-context tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the quadratic complexity of standard Transformers for long-range modeling while overcoming limitations of sparse mechanisms like RingAttention.

Method: The approach involves factorizing attention into ring-local neighborhoods, $\pi$-stride periodic skips, and adaptive fusion gates to optimize sparse attentions for long-context sequences.

Result: \PiAttention achieves lower perplexity (8.3% improvement over RingAttention) and uses 50% fewer computational resources while maintaining high quality.

Conclusion: Periodic skips, adaptive fusion, and careful sparsity coordination enable efficient and scalable long-context modeling with results comparable to dense attention.

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [73] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: The paper proposes a framework leveraging TextRank-based extraction, medical named entity recognition, and LLaMA-2-7B fine-tuning to improve the faithfulness of medical summarization, surpassing baselines and prior systems with strong performance across evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To improve faithfulness in medical text summarization, addressing the risks of unfaithful summaries misrepresenting critical medical details.

Method: The approach combines TextRank-based sentence extraction, medical named entity recognition, and the fine-tuning of the LLaMA-2-7B model on English (MeQSum) and Bangla (BanglaCHQ-Summ) datasets.

Result: The proposed framework achieved consistent improvements in quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, outperforming baselines and previous systems. Human evaluation confirmed the preservation of critical medical information in over 80% of summaries.

Conclusion: Faithfulness is a crucial factor for reliable medical summarization. The framework demonstrates potential for enhancing safety and performance in healthcare-related summarization tasks.

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [74] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: This paper introduces TEDxTN, a publicly available Tunisian Arabic to English speech translation dataset featuring 108 TEDx talks with 25 hours of code-switched speech and multiple dialects.


<details>
  <summary>Details</summary>
Motivation: To address the data scarcity in Tunisian Arabic and enable progress in the NLP field for Arabic dialects, especially Tunisian dialect.

Method: The authors collected, segmented, transcribed, and translated 108 TEDx talks using internally developed annotation guidelines to build a speech translation dataset.

Result: The dataset includes 25 hours of Tunisian Arabic (with code-switching) speech and is publicly available, along with strong baseline evaluations using pre-trained and fine-tuned models.

Conclusion: This new resource provides a foundation for advancing Tunisian Dialect natural language processing and facilitates future research in speech translation and recognition.

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [75] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: The paper discusses the development of a chatbot using Generative AI and RAG to simplify student access to academic information.


<details>
  <summary>Details</summary>
Motivation: Students face challenges in accessing fragmented academic information scattered across institutional documents and websites, leading to confusion.

Method: Development of a chatbot leveraging Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) with evaluation of GenAI models based on quality and speed.

Result: Gemini 2.0 Flash excelled in quality and speed, while Gemma 3n performed well as an open-source alternative.

Conclusion: The chatbot proposed aims to aid students in accessing academic information efficiently by utilizing suitable GenAI and RAG technologies.

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [76] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: The paper evaluates the effectiveness of GPT-4o in grading undergraduate Computational Linguistics quizzes and project reports, showing strong alignment with human evaluations while noting limitations in certain open-ended technical responses.


<details>
  <summary>Details</summary>
Motivation: To investigate whether GPT-4o can reliably assist or replace human grading in real educational settings, specifically in grading assignments in an undergraduate Computational Linguistics course.

Method: The study collected responses from ~50 students across quizzes and project reports. GPT-4o's grading was compared to human evaluations by teaching assistants, analyzing the agreement and correlation between the two.

Result: The GPT-4o demonstrated strong correlation with human grades (up to 0.98) and exact score agreement in 55% of quiz grades. For project reports, it showed good alignment but variability in scoring open-ended technical responses.

Conclusion: The research demonstrates the promise of LLM-based grading but emphasizes its limitations, particularly in assessing nuanced, open-ended responses. It suggests further improvements and exploration of LLMs in educational contexts.

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [77] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: This paper investigates how multilingual large language models (LLMs) create shared internal representations of languages, revealing that language-specific decoding occurs in later layers based on pivot-language representations.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand how multilingual LLMs internally represent diverse languages and why model performance often favors the dominant training language.

Method: The authors train LLMs on diverse multilingual data, using cross-layer transcoders (CLT) and attribution graphs to analyze the internal mechanics of language representation and decoding.

Result: The study finds that LLMs employ nearly identical internal language representations, but introduce language-specific decoding via high-frequency language features in later layers. Moreover, these features allow substituting one language output for another.

Conclusion: The paper highlights the significance of understanding pivot-language mechanisms for enhancing multilingual alignment in LLMs, particularly to address biases for dominant training languages.

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [78] [Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English](https://arxiv.org/abs/2511.10846)
*Rebecca Dorn,Christina Chance,Casandra Rusti,Charles Bickham,Kai-Wei Chang,Fred Morstatter,Kristina Lerman*

Main category: cs.CL

TL;DR: This paper investigates bias in emotion recognition models, highlighting how these models underperform and reinforce stereotypes when analyzing African American Vernacular English (AAVE) compared to General American English (GAE).


<details>
  <summary>Details</summary>
Motivation: Emotion recognition models are widely applied in various domains, but they often rely on annotations aligned with dominant cultural norms, leading to biased model behaviors and limited inclusivity for dialects like AAVE.

Method: The study analyzed 2.7 million geo-tagged tweets from Los Angeles, scoring their AAVE density. A labeled dataset of 875 tweets was used, with annotations from both general and AAVE-fluent annotators, to evaluate the accuracy and bias in GPT, BERT, and SpanEmo models.

Result: Emotion models exhibited notably higher false-positive anger predictions on AAVE compared to GAE. Correlations showed that demographic factors influence bias, with areas of higher African American populations experiencing higher predicted anger and lower joy.

Conclusion: Emotion AI systems exhibit significant biases against AAVE, reinforcing harmful racial stereotypes. The paper stresses the need for culturally and dialect-sensitive methodologies in affective computing to prevent such inequities.

Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.

</details>


### [79] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: The paper proposes an alignment-first strategy to improve skill transfer between diverged LLMs using parameter symmetry properties and demonstrates its success in advanced reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance skill transfer across LLMs and address negative interference caused by model divergence during training.

Method: Align the parameter spaces of diverged models using permutation, rotation, and scaling symmetries, and adapt this approach to GQA and SwiGLU layers through weight-based and activation-based methods.

Result: The alignment-first strategy successfully improves transfer of reasoning skills to non-reasoning models, outperforming standard techniques in challenging reasoning benchmarks.

Conclusion: This approach improves specialized skill transfer across LLMs, minimizes redundant fine-tuning, and bolsters model adaptability for advanced tasks.

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [80] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: The paper investigates how LLMs' judgments are influenced by reframing tasks from factual queries to conversational contexts, revealing changes in conviction and social framing as impactful.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can reliably judge tasks requiring social or conversational judgments, especially under varying contexts.

Method: The study contrasts model performance on direct factual queries versus conversational judgment tasks, adding conversational pressure through rebuttals to see how firmly models maintain their positions.

Result: Models show significant changes in judgment with an average performance change of 9.24% across all tested LLMs, including tendencies like sycophancy and over-critical evaluations.

Conclusion: The conversational framing plays a crucial role in LLM-based evaluations, and the proposed methodology aids in diagnosing model conviction for improving dialogue systems.

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [81] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: ICX360 is an open-source Python toolkit designed to explain the outputs of Large Language Models (LLMs) focusing on user-provided prompts, using black-box and white-box methodologies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing need for tools to explain the outputs of LLMs as they expand into high-stakes applications, ensuring better trust and understanding of their functionality in critical scenarios.

Method: Three tools within ICX360 use black-box methods (perturbations) and white-box methods (gradients) to generate explanations for LLM output under various contexts.

Result: ICX360 provides a practical, open-source solution accessible via quick-start guidance and tutorials that demonstrate its application in different use cases like retrieval augmented generation and jailbreaking.

Conclusion: ICX360 enhances the explainability of LLM outputs, empowering users to understand complex systems more intuitively and catering to critical applications.

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [82] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: This paper investigates negative bias in large language models (LLMs), identifying prompt format as a key influencer and exploring strategies to mitigate it.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate negative bias in LLMs, which excessively generate negative responses in binary decision tasks.

Method: Introduced a systematic process to analyze negative bias using a categorized evaluation set based on model knowledge (correct, incorrect, insufficient) and tested various prompting strategies.

Result: Discovered shortcuts in decision behavior, showing prompt format significantly impacts response bias. Relevant prompts and options like "I don't know" reduce bias, while chain-of-thought prompting amplifies it.

Conclusion: Negative bias in LLMs can be influenced by prompting strategies and prompt formats, highlighting ways to minimize the bias effectively.

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [83] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath is a large-scale biomedical Entity Linking dataset addressing challenges in fragmented data, lack of resources for explainable models, and semantically-blind evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in biomedical NER and EL due to fragmented data resources, lack of explainable model development tools, and inadequate evaluation metrics.

Method: MedPath integrates nine expert-annotated EL datasets, standardizes entities using UMLS, maps entities to 62 biomedical vocabularies, and enriches them with full ontological paths spanning up to 11 vocabularies.

Result: MedPath enables semantic-rich, interpretable EL system development and facilitates interoperable, explainable clinical NLP model advancement.

Conclusion: MedPath provides a robust dataset for driving innovation in biomedical NLP by addressing key limitations in entity normalization, mapping, and evaluation.

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [84] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: This paper introduces EMSQA, a domain-specific dataset and prompting strategies to improve medical question answering performance of large language models, focusing on clinical expertise.


<details>
  <summary>Details</summary>
Motivation: While LLMs have demonstrated potential in medical question answering, they lack domain-specific expertise tailored for clinical settings, such as certification levels and clinical subject areas.

Method: The paper introduces EMSQA, a multiple-choice dataset, and proposes Expert-CoT (a domain-focused prompting strategy) and ExpertRAG (a retrieval-augmented generation pipeline) leveraging curated knowledge bases aligned with medical expertise.

Result: Expert-CoT improves accuracy by up to 2.05%, while combining Expert-CoT with ExpertRAG enhances it further to 4.59% over standard baselines. Expertise-augmented LLMs successfully pass EMS exams.

Conclusion: Domain-specific prompting and retrieval strategies enhance LLMs for medical applications, demonstrating effectiveness in increasing accuracy in high-stakes scenarios and passing medical certification exams.

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [85] [Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions](https://arxiv.org/abs/2511.10902)
*Mengze Hong,Di Jiang,Weiwei Zhao,Yawen Li,Yihang Wang,Xinyuan Luo,Yanjie Sun,Chen Jason Zhang*

Main category: cs.CL

TL;DR: The paper introduces an advanced system for peer review simulation, capable of providing comprehensive, multimodal, and actionable feedback for academic manuscripts.


<details>
  <summary>Details</summary>
Motivation: Current peer review systems are limited due to their reliance on text-only inputs, lack of contextual depth, and inability to offer actionable feedback for manuscript improvement.

Method: The study develops a multimodal web-based system leveraging LLMs, retrieval-augmented generation (RAG), and structured feedback conversion to assist in academic manuscript revisions.

Result: Experimental evaluations demonstrate that the system generates higher-quality reviews aligned with expert standards, outperforming baseline methods.

Conclusion: The proposed system advances scholarly workflows by providing interactive, actionable, and high-quality peer review feedback, promoting improved academic writing practices.

Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.

</details>


### [86] [Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy](https://arxiv.org/abs/2511.10903)
*Ramya Kumar,Dhruv Gulwani,Sonit Singh*

Main category: cs.CL

TL;DR: The paper evaluates the classification of exam questions and learning outcomes into Bloom's Taxonomy using traditional ML, deep learning, transformer models, and large language models. SVM with data augmentation performed the best, while deep models struggled with overfitting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of automatically classifying educational content into Bloom's Taxonomy cognitive categories, which can aid in better understanding and organizing learning materials.

Method: The study tested various models, including traditional ML (Naive Bayes, Logistic Regression, SVM), neural networks (LSTM, BiLSTM, GRU), transformer models (BERT, RoBERTa), and large language models (e.g., OpenAI). It also applied data preprocessing and augmentation strategies like synonym replacement and word embeddings.

Result: Traditional ML model (SVM) augmented with data achieved the best performance with 94% accuracy, precision, recall, and F1 score. Deep learning and transformer models faced overfitting challenges, although RoBERTa showed some resilience. Among LLMs, OpenAI and Gemini performed well in zero-shot evaluations with approximately 72-73% accuracy.

Conclusion: Simpler models coupled with effective data augmentation outperform complex deep models on small datasets. The study emphasizes the importance of simplicity and augmentation strategies for Bloom's Taxonomy classification.

Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.

</details>


### [87] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: This paper evaluates the performance of four state-of-the-art LLMs on a benchmark dataset of rare disease diagnosis derived from House M.D. Results revealed significant variability, with newer models outperforming older ones.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to explore and assess LLMs' capabilities in diagnosing rare diseases based on narrative medical cases, an underexplored application despite their multi-domain potential.

Method: A new dataset of 176 symptom-diagnosis pairs from House M.D. was introduced. Four state-of-the-art LLMs were evaluated on their diagnostic reasoning performance using this dataset.

Result: The evaluation revealed substantial differences in accuracy among models, ranging from 16.48% to 38.64%. Newer model generations showed 2.3 times better performance compared to older architectures.

Conclusion: LLMs still face challenges in rare disease diagnosis, but performance improvements indicate potential for further advancements. The benchmark provides a foundation for ongoing research in AI-assisted medical diagnostics.

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [88] [A Mathematical Framework for AI Singularity: Conditions, Bounds, and Control of Recursive Improvement](https://arxiv.org/abs/2511.10668)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.CV

TL;DR: This paper develops a framework to analyze and test conditions for AI systems capability growth under bounded or runaway scenarios, providing safety controls like power caps and throttling.


<details>
  <summary>Details</summary>
Motivation: Address the question of whether AI systems can experience unbounded capability growth in finite time and establish measurable conditions for safety.

Method: An analytic framework linking AI capability growth to physical and data-resource limits, and defining testable decision rules for safety controls like power caps and evaluation gates.

Result: The paper derives testable conditions for runaway growth versus bounded regimes and provides analytical case studies for various settings.

Conclusion: The developed framework replaces speculative discussions about AI singularity with practical, testable, and deployable safety controls and conditions.

Abstract: AI systems improve by drawing on more compute, data, energy, and better training methods. This paper asks a precise, testable version of the "runaway growth" question: under what measurable conditions could capability escalate without bound in finite time, and under what conditions can that be ruled out? We develop an analytic framework for recursive self-improvement that links capability growth to resource build-out and deployment policies. Physical and information-theoretic limits from power, bandwidth, and memory define a service envelope that caps instantaneous improvement. An endogenous growth model couples capital to compute, data, and energy and defines a critical boundary separating superlinear from subcritical regimes. We derive decision rules that map observable series (facility power, IO bandwidth, training throughput, benchmark losses, and spending) into yes/no certificates for runaway versus nonsingular behavior. The framework yields falsifiable tests based on how fast improvement accelerates relative to its current level, and it provides safety controls that are directly implementable in practice, such as power caps, throughput throttling, and evaluation gates. Analytical case studies cover capped-power, saturating-data, and investment-amplified settings, illustrating when the envelope binds and when it does not. The approach is simulation-free and grounded in measurements engineers already collect. Limitations include dependence on the chosen capability metric and on regularity diagnostics; future work will address stochastic dynamics, multi-agent competition, and abrupt architectural shifts. Overall, the results replace speculation with testable conditions and deployable controls for certifying or precluding an AI singularity.

</details>


### [89] [Semantic VLM Dataset for Safe Autonomous Driving](https://arxiv.org/abs/2511.10701)
*Yuankai He,Weisong Shi*

Main category: cs.CV

TL;DR: CAR-Scenes presents a dataset aimed at enhancing vision-language model capabilities in scene-level understanding for autonomous driving, including annotations, evaluation benchmarks, and baseline model performance.


<details>
  <summary>Details</summary>
Motivation: To improve interpretable and scene-level understanding in autonomous vehicles by providing a detailed and annotated vision-language dataset, addressing the needs for evaluation of vision-language models in autonomous driving scenarios.

Method: The dataset comprises 5,192 annotated images from established driving datasets, with 28-category knowledge base and 350+ attributes. Annotation was done with a GPT-4o vision-language pipeline assisted by human verification. Baselines include a LoRA-tuned Qwen2-VL-2B model assessed with various metrics.

Result: CAR-Scenes offers a comprehensive understanding through co-occurrence graphs and JSONL records for semantic retrieval and scenario mining. Baseline evaluations on scalar accuracy, attribute F1 score, and severity level prediction are provided.

Conclusion: This dataset enables research and development in autonomous driving by offering explainable and data-centric workflows for vision-language models and risk-aware scenario analysis. The tools and scripts empower researchers for further analysis.

Abstract: CAR-Scenes is a frame-level dataset for autonomous driving that enables training and evaluation of vision-language models (VLMs) for interpretable, scene-level understanding. We annotate 5,192 images drawn from Argoverse 1, Cityscapes, KITTI, and nuScenes using a 28-key category/sub-category knowledge base covering environment, road geometry, background-vehicle behavior, ego-vehicle behavior, vulnerable road users, sensor states, and a discrete severity scale (1-10), totaling 350+ leaf attributes. Labels are produced by a GPT-4o-assisted vision-language pipeline with human-in-the-loop verification; we release the exact prompts, post-processing rules, and per-field baseline model performance. CAR-Scenes also provides attribute co-occurrence graphs and JSONL records that support semantic retrieval, dataset triage, and risk-aware scenario mining across sources. To calibrate task difficulty, we include reproducible, non-benchmark baselines, notably a LoRA-tuned Qwen2-VL-2B with deterministic decoding, evaluated via scalar accuracy, micro-averaged F1 for list attributes, and severity MAE/RMSE on a fixed validation split. We publicly release the annotation and analysis scripts, including graph construction and evaluation scripts, to enable explainable, data-centric workflows for future intelligent vehicles. Dataset: https://github.com/Croquembouche/CAR-Scenes

</details>


### [90] [Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions](https://arxiv.org/abs/2511.11116)
*Redwan Hussain,Mizanur Rahman,Prithwiraj Bhattacharjee*

Main category: cs.CV

TL;DR: The paper reviews recent AI media detection methods, highlighting limitations and proposing multimodal deep learning models as the future direction.


<details>
  <summary>Details</summary>
Motivation: The misuse of AI tools such as GANs and diffusion models has amplified the spread of misinformation, privacy violations, and fraud, necessitating robust synthetic content detection systems.

Method: Twenty-four recent detection studies are examined individually to assess contributions and weaknesses, leading to a comprehensive summary of challenges and proposing multimodal deep learning as a promising solution.

Result: The paper identifies limitations in generalization and effectiveness across unseen and multimodal data, offering insights from reviewed works.

Conclusion: A research direction is recommended focusing on multimodal deep learning, aiming to develop robust tools for detecting harmful synthetic media.

Abstract: Artificial intelligence (AI) in media has advanced rapidly over the last decade. The introduction of Generative Adversarial Networks (GANs) improved the quality of photorealistic image generation. Diffusion models later brought a new era of generative media. These advances made it difficult to separate real and synthetic content. The rise of deepfakes demonstrated how these tools could be misused to spread misinformation, political conspiracies, privacy violations, and fraud. For this reason, many detection models have been developed. They often use deep learning methods such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). These models search for visual, spatial, or temporal anomalies. However, such approaches often fail to generalize across unseen data and struggle with content from different models. In addition, existing approaches are ineffective in multimodal data and highly modified content. This study reviews twenty-four recent works on AI-generated media detection. Each study was examined individually to identify its contributions and weaknesses, respectively. The review then summarizes the common limitations and key challenges faced by current approaches. Based on this analysis, a research direction is suggested with a focus on multimodal deep learning models. Such models have the potential to provide more robust and generalized detection. It offers future researchers a clear starting point for building stronger defenses against harmful synthetic media.

</details>


### [91] [Fast Data Attribution for Text-to-Image Models](https://arxiv.org/abs/2511.10721)
*Sheng-Yu Wang,Aaron Hertzmann,Alexei A Efros,Richard Zhang,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: The paper proposes a scalable and efficient approach for identifying influential training images in text-to-image models, achieving significant speed-up compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current data attribution methods for text-to-image models are computationally expensive and impractical for real-world applications.

Method: A distillation process is utilized to transfer slow unlearning-based attribution methods into a feature embedding space, combined with efficient indexing and retrieval techniques.

Result: The approach performs competitively or better than existing methods with a speed-up of 2,500x to 400,000x, demonstrated on both medium-scale (MSCOCO) and large-scale models (Stable Diffusion and LAION).

Conclusion: The proposed method is a critical advancement for scalable data attribution in text-to-image models, making real-world applications feasible.

Abstract: Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.

</details>


### [92] [Expert Consensus-based Video-Based Assessment Tool for Workflow Analysis in Minimally Invasive Colorectal Surgery: Development and Validation of ColoWorkflow](https://arxiv.org/abs/2511.10766)
*Pooja P Jain,Pietro Mascagni,Giuseppe Massimiani,Nabani Banik,Marta Goglia,Lorenzo Arboit,Britty Baby,Andrea Balla,Ludovica Baldari,Gianfranco Silecchia,Claudio Fiorillo,CompSurg Colorectal Experts Group,Sergio Alfieri,Salvador Morales-Conde,Deborah S Keller,Luigi Boni,Nicolas Padoy*

Main category: cs.CV

TL;DR: This paper introduces ColoWorkflow, a validated tool for video-based analysis of minimally invasive colorectal surgeries, which standardizes workflow descriptions and aids in training and quality improvement.


<details>
  <summary>Details</summary>
Motivation: To address procedural variability, difficult learning curves, and complications in minimally invasive colorectal surgery by providing a standardized video-based workflow analysis tool.

Method: Developed the ColoWorkflow tool using a Delphi process for achieving consensus on workflow descriptors and validated it with videos from laparoscopic and robotic colorectal surgeries across five centers.

Result: ColoWorkflow demonstrated broad applicability, generalizable workflow descriptions, and moderate inter-rater reliability (Cohen's K of 0.71 for phases, 0.66 for steps) using a multicenter dataset.

Conclusion: ColoWorkflow offers a standardized and validated framework for workflow analysis, potentially advancing surgical training, benchmarking, and artificial intelligence development to improve outcomes.

Abstract: Minimally invasive colorectal surgery is characterized by procedural variability, a difficult learning curve, and complications that impact quality and outcomes. Video-based assessment (VBA) offers an opportunity to generate data-driven insights to reduce variability, optimize training, and improve surgical performance. However, existing tools for workflow analysis remain difficult to standardize and implement. This study aims to develop and validate a VBA tool for workflow analysis across minimally invasive colorectal procedures. A Delphi process was conducted to achieve consensus on generalizable workflow descriptors. The resulting framework informed the development of a new VBA tool, ColoWorkflow. Independent raters then applied ColoWorkflow to a multicentre video dataset of laparoscopic and robotic colorectal surgery (CRS). Applicability and inter-rater reliability were evaluated. Consensus was achieved for 10 procedure-agnostic phases and 34 procedure-specific steps describing CRS workflows. ColoWorkflow was developed and applied to 54 colorectal operative videos (left and right hemicolectomies, sigmoid and rectosigmoid resections, and total proctocolectomies) from five centres. The tool demonstrated broad applicability, with all but one label utilized. Inter-rater reliability was moderate, with mean Cohen's K of 0.71 for phases and 0.66 for steps. Most discrepancies arose at phase transitions and step boundary definitions. ColoWorkflow is the first consensus-based, validated VBA tool for comprehensive workflow analysis in minimally invasive CRS. It establishes a reproducible framework for video-based performance assessment, enabling benchmarking across institutions and supporting the development of artificial intelligence-driven workflow recognition. Its adoption may standardize training, accelerate competency acquisition, and advance data-informed surgical quality improvement.

</details>


### [93] [Frequency-Aware Vision-Language Multimodality Generalization Network for Remote Sensing Image Classification](https://arxiv.org/abs/2511.10774)
*Junjie Zhang,Feng Zhao,Hanqiang Liu,Jun Yu*

Main category: cs.CV

TL;DR: This paper proposes a novel frequency-aware vision-language multimodality generalization network (FVMGN) for remote sensing image classification, emphasizing overcoming multimodal heterogeneity and improving cross-scene generalization.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the need to enhance remote sensing multimodality generalization, overcoming data heterogeneity and improving cross-scene generalization in vision-language models that lack proprietary linguistic prior knowledge.

Method: The paper introduces FVMGN, incorporating several innovative components like DTAug for multimodal data augmentation, MWDis for feature disentanglement, SFIE for spatial-frequency feature reconstruction, and MSFFA for multimodal feature alignment within a unified semantic space.

Result: Experiments demonstrate that FVMGN outperforms state-of-the-art methods in multimodal generalization tasks for remote sensing image classification.

Conclusion: FVMGN effectively addresses the challenges of multimodal heterogeneity and cross-scene generalization, establishing itself as a robust framework for remote sensing multimodality generalization.

Abstract: The booming remote sensing (RS) technology is giving rise to a novel multimodality generalization task, which requires the model to overcome data heterogeneity while possessing powerful cross-scene generalization ability. Moreover, most vision-language models (VLMs) usually describe surface materials in RS images using universal texts, lacking proprietary linguistic prior knowledge specific to different RS vision modalities. In this work, we formalize RS multimodality generalization (RSMG) as a learning paradigm, and propose a frequency-aware vision-language multimodality generalization network (FVMGN) for RS image classification. Specifically, a diffusion-based training-test-time augmentation (DTAug) strategy is designed to reconstruct multimodal land-cover distributions, enriching input information for FVMGN. Following that, to overcome multimodal heterogeneity, a multimodal wavelet disentanglement (MWDis) module is developed to learn cross-domain invariant features by resampling low and high frequency components in the frequency domain. Considering the characteristics of RS vision modalities, shared and proprietary class texts is designed as linguistic inputs for the transformer-based text encoder to extract diverse text features. For multimodal vision inputs, a spatial-frequency-aware image encoder (SFIE) is constructed to realize local-global feature reconstruction and representation. Finally, a multiscale spatial-frequency feature alignment (MSFFA) module is suggested to construct a unified semantic space, ensuring refined multiscale alignment of different text and vision features in spatial and frequency domains. Extensive experiments show that FVMGN has the excellent multimodality generalization ability compared with state-of-the-art (SOTA) methods.

</details>


### [94] [GFT: Graph Feature Tuning for Efficient Point Cloud Analysis](https://arxiv.org/abs/2511.10799)
*Manish Dhakal,Venkat R. Dasari,Raj Sunderraman,Yi Ding*

Main category: cs.CV

TL;DR: The paper introduces Graph Features Tuning (GFT), a parameter-efficient fine-tuning method specifically for point cloud data, reducing trainable parameters while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: The need for computationally efficient fine-tuning approaches for point cloud data, as general PEFT methods are suboptimal and resource-intensive.

Method: GFT uses dynamic graph learning with a lightweight graph convolution network from transformer inputs and integrates these features to deeper layers using skip connections and efficient cross-attention modules.

Result: Extensive experiments in object classification and segmentation tasks demonstrate that GFT achieves performance comparable to existing methods while significantly reducing the trainable parameters.

Conclusion: GFT provides an efficient and effective approach for adapting point cloud data tasks, cutting down required trainable parameters without sacrificing performance.

Abstract: Parameter-efficient fine-tuning (PEFT) significantly reduces computational and memory costs by updating only a small subset of the model's parameters, enabling faster adaptation to new tasks with minimal loss in performance. Previous studies have introduced PEFTs tailored for point cloud data, as general approaches are suboptimal. To further reduce the number of trainable parameters, we propose a point-cloud-specific PEFT, termed Graph Features Tuning (GFT), which learns a dynamic graph from initial tokenized inputs of the transformer using a lightweight graph convolution network and passes these graph features to deeper layers via skip connections and efficient cross-attention modules. Extensive experiments on object classification and segmentation tasks show that GFT operates in the same domain, rivalling existing methods, while reducing the trainable parameters. Code is at https://github.com/manishdhakal/GFT.

</details>


### [95] [SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices](https://arxiv.org/abs/2511.11038)
*Jiaming Huang,Yi Gao,Fuchang Pan,Renjie Li,Wei Dong*

Main category: cs.CV

TL;DR: SemanticNN proposes a semantic codec for AI-based IoT devices, enabling robust and efficient edge-device collaborations with reduced data transmission and preserved inference accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of traditional error-resilient systems in IoT, focusing on improving real-time performance and data privacy in weak embedded devices under dynamic and unreliable network conditions.

Method: A semantic codec that utilizes a BER-aware decoder and SQ-based encoder for efficient communication under strict constraints. Additionally, it introduces Feature-augmentation Learning and XAI-based Asymmetry Compensation strategies.

Result: The proposed SemanticNN significantly reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy across models and datasets.

Conclusion: SemanticNN effectively enhances error-resilience and offloading efficiency in collaborative inference tasks under challenging IoT device conditions, showcasing its potential for real-world application.

Abstract: With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable network conditions necessitate error-resilient device-edge collaboration systems. Traditional approaches focus on bit-level transmission correctness, which can be inefficient under dynamic channel conditions. In contrast, we propose SemanticNN, a semantic codec that tolerates bit-level errors in pursuit of semantic-level correctness, enabling compressive and resilient collaborative inference offloading under strict computational and communication constraints. It incorporates a Bit Error Rate (BER)-aware decoder that adapts to dynamic channel conditions and a Soft Quantization (SQ)-based encoder to learn compact representations. Building on this architecture, we introduce Feature-augmentation Learning, a novel training strategy that enhances offloading efficiency. To address encoder-decoder capability mismatches from asymmetric resources, we propose XAI-based Asymmetry Compensation to enhance decoding semantic fidelity. We conduct extensive experiments on STM32 using three models and six datasets across image classification and object detection tasks. Experimental results demonstrate that, under varying transmission error rates, SemanticNN significantly reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy.

</details>


### [96] [Accuracy-Preserving CNN Pruning Method under Limited Data Availability](https://arxiv.org/abs/2511.10861)
*Daisuke Yasui,Toshitaka Matsuki,Hiroshi Sato*

Main category: cs.CV

TL;DR: The paper proposes an improved CNN model pruning method using Layer-wise Relevance Propagation (LRP), achieving high pruning rates with better accuracy using limited data, without requiring fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing LRP-based pruning methods face accuracy degradation, hampering their practical usability in data-limited scenarios.

Method: The research introduces a refined LRP-based method for pruning CNN models, improving pruning rates and maintaining accuracy using a small amount of data without fine-tuning.

Result: The new pruning approach outperformed prior LRP-based methods, achieving higher pruning rates while preserving better model accuracy in limited data situations.

Conclusion: This method offers significant advantages for model compression in constrained environments without compromising on accuracy, enhancing the usability of LRP-based pruning techniques.

Abstract: Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.

</details>


### [97] [Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling](https://arxiv.org/abs/2511.10866)
*Seoik Jung,Taekyung Song,Yangro Lee,Sungjun Lee*

Main category: cs.CV

TL;DR: This paper introduces a real-time violence detection framework using short video clips and Large Language Model-based labeling, achieving high accuracy across datasets.


<details>
  <summary>Details</summary>
Motivation: To improve violence detection accuracy and generalization using a framework suitable for real-time intelligent surveillance systems.

Method: Short-Window Sliding Learning framework with 1-2 second video clips, leveraging LLM-based auto-captioning for dataset construction and temporal continuity preservation.

Result: Achieves 95.25% accuracy on RWF-2000 and 83.25% on UCF-Crime, demonstrating enhanced performance and applicability.

Conclusion: The framework confirms strong generalization and real-time capability for intelligent surveillance applications.

Abstract: This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.

</details>


### [98] [MCN-CL: Multimodal Cross-Attention Network and Contrastive Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2511.10892)
*Feng Li,Ke Wu,Yongwei Li*

Main category: cs.CV

TL;DR: The paper addresses the challenges in multimodal emotion recognition and proposes a novel method, MCN-CL, which improves performance in datasets like IEMOCAP and MELD.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need for efficient cross-modal fusion frameworks due to increasing multimodal data, coupled with challenges such as category imbalance, facial complexity, and feature fusion issues.

Method: The proposed MCN-CL method adopts a triple query mechanism and hard negative mining strategy to mitigate feature redundancy, preserve emotional cues, and address modal heterogeneity and category imbalance.

Result: Experimental results demonstrate that MCN-CL achieves superior performance, with Weighted F1 scores increasing by 3.42% on IEMOCAP and 5.73% on MELD datasets compared to state-of-the-art methods.

Conclusion: MCN-CL effectively addresses major challenges in multimodal emotion recognition, showcasing significant improvements and paving the way for more robust cross-modal frameworks.

Abstract: Multimodal emotion recognition plays a key role in many domains, including mental health monitoring, educational interaction, and human-computer interaction. However, existing methods often face three major challenges: unbalanced category distribution, the complexity of dynamic facial action unit time modeling, and the difficulty of feature fusion due to modal heterogeneity. With the explosive growth of multimodal data in social media scenarios, the need for building an efficient cross-modal fusion framework for emotion recognition is becoming increasingly urgent. To this end, this paper proposes Multimodal Cross-Attention Network and Contrastive Learning (MCN-CL) for multimodal emotion recognition. It uses a triple query mechanism and hard negative mining strategy to remove feature redundancy while preserving important emotional cues, effectively addressing the issues of modal heterogeneity and category imbalance. Experiment results on the IEMOCAP and MELD datasets show that our proposed method outperforms state-of-the-art approaches, with Weighted F1 scores improving by 3.42% and 5.73%, respectively.

</details>


### [99] [DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting](https://arxiv.org/abs/2511.10894)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Anthony Miyaguchi,Rodrigo Pereira David,Rodrigo Tripodi Calumby,Lukáš Picek*

Main category: cs.CV

TL;DR: The paper introduces a probabilistic approach for rainfall nowcasting employing a vision transformer model and achieves superior performance compared to 3D-UNET baselines.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and computational efficiency of probabilistic rainfall nowcasting beyond traditional models like 3D-UNET.

Method: Utilizes the V-JEPA Vision Transformer with a lightweight probabilistic head, integrated with a pre-trained satellite vision encoder (DINOv3-SAT493M). It applies an eCDF over accumulated rainfall and optimizes using the Continuous Ranked Probability Score (CRPS).

Result: The method achieved a CRPS of 3.5102, marking a 26% effectiveness improvement over leading 3D-UNET baselines on the Weather4Cast 2025 benchmark.

Conclusion: The proposed method demonstrates competitive and efficient rainfall nowcasting capabilities, achieving significant gains over traditional baselines.

Abstract: This paper proposes a competitive and computationally efficient approach to probabilistic rainfall nowcasting. A video projector (V-JEPA Vision Transformer) associated to a lightweight probabilistic head is attached to a pre-trained satellite vision encoder (DINOv3\text{-}SAT493M) to map encoder tokens into a discrete empirical CDF (eCDF) over 4-hour accumulated rainfall. The projector-head is optimized end-to-end over the Continuous Ranked Probability Score (CRPS). As an alternative, 3D-UNET baselines trained with an aggregate Rank Probability Score and a per-pixel Gamma-Hurdle objective are used. On the Weather4Cast 2025 benchmark, the proposed method achieved a promising performance, with a CRPS of 3.5102 (CRPS), which represents $\approx$26\% in effectiveness gain against the best 3D-UNET.

</details>


### [100] [YOLO-Drone: An Efficient Object Detection Approach Using the GhostHead Network for Drone Images](https://arxiv.org/abs/2511.10905)
*Hyun-Ki Jung*

Main category: cs.CV

TL;DR: This paper introduces YOLO-Drone, an improved object detection model based on YOLOv11, optimized for analyzing drone images.


<details>
  <summary>Details</summary>
Motivation: Drone images are challenging for object detection due to high altitude perspectives. This study aims to improve detection accuracy using a specialized model.

Method: The paper utilizes YOLOv11, enhances its Head network with a GhostHead Network, and tests on the VisDrone dataset, producing the YOLO-Drone model.

Result: YOLO-Drone demonstrated better accuracy metrics (Precision, Recall, F1-Score, and mAP) and faster inference compared to YOLOv11, as well as outperforming other models like YOLOv8, YOLOv9, and YOLOv10.

Conclusion: YOLO-Drone improves object detection accuracy and speed for high-altitude drone imagery and shows significant performance gains against competing models.

Abstract: Object detection using images or videos captured by drones is a promising technology with significant potential across various industries. However, a major challenge is that drone images are typically taken from high altitudes, making object identification difficult. This paper proposes an effective solution to address this issue. The base model used in the experiments is YOLOv11, the latest object detection model, with a specific implementation based on YOLOv11n. The experimental data were sourced from the widely used and reliable VisDrone dataset, a standard benchmark in drone-based object detection. This paper introduces an enhancement to the Head network of the YOLOv11 algorithm, called the GhostHead Network. The model incorporating this improvement is named YOLO-Drone. Experimental results demonstrate that YOLO-Drone achieves significant improvements in key detection accuracy metrics, including Precision, Recall, F1-Score, and mAP (0.5), compared to the original YOLOv11. Specifically, the proposed model recorded a 0.4% increase in Precision, a 0.6% increase in Recall, a 0.5% increase in F1-Score, and a 0.5% increase in mAP (0.5). Additionally, the Inference Speed metric, which measures image processing speed, also showed a notable improvement. These results indicate that YOLO-Drone is a high-performance model with enhanced accuracy and speed compared to YOLOv11. To further validate its reliability, comparative experiments were conducted against other high-performance object detection models, including YOLOv8, YOLOv9, and YOLOv10. The results confirmed that the proposed model outperformed YOLOv8 by 0.1% in mAP (0.5) and surpassed YOLOv9 and YOLOv10 by 0.3% and 0.6%, respectively.

</details>


### [101] [PhaseWin Search Framework Enable Efficient Object-Level Interpretation](https://arxiv.org/abs/2511.10914)
*Zihan Gu,Ruoyu Chen,Junchi Zhang,Yue Hu,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: The paper introduces PhaseWin, a phase-window search algorithm for region attribution in object-level foundation models, offering near-linear complexity and improved computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the efficiency limitations of submodular subset selection approaches in faithful region attribution for object-level foundation models.

Method: Propose and develop PhaseWin, a phased coarse-to-fine search algorithm combining adaptive pruning, windowed selection, and dynamic supervision mechanisms.

Result: PhaseWin achieves over 95% of greedy attribution faithfulness while using only 20% of the computational budget and outperforms other baselines in object detection and visual grounding tasks.

Conclusion: PhaseWin sets a new standard for scalable and high-fidelity attribution for object-level multimodal models.

Abstract: Attribution is essential for interpreting object-level foundation models. Recent methods based on submodular subset selection have achieved high faithfulness, but their efficiency limitations hinder practical deployment in real-world scenarios. To address this, we propose PhaseWin, a novel phase-window search algorithm that enables faithful region attribution with near-linear complexity. PhaseWin replaces traditional quadratic-cost greedy selection with a phased coarse-to-fine search, combining adaptive pruning, windowed fine-grained selection, and dynamic supervision mechanisms to closely approximate greedy behavior while dramatically reducing model evaluations. Theoretically, PhaseWin retains near-greedy approximation guarantees under mild monotone submodular assumptions. Empirically, PhaseWin achieves over 95% of greedy attribution faithfulness using only 20% of the computational budget, and consistently outperforms other attribution baselines across object detection and visual grounding tasks with Grounding DINO and Florence-2. PhaseWin establishes a new state of the art in scalable, high-faithfulness attribution for object-level multimodal models.

</details>


### [102] [Out-of-Distribution Detection with Positive and Negative Prompt Supervision Using Large Language Models](https://arxiv.org/abs/2511.10923)
*Zhixia He,Chen Zhao,Minglai Shao,Xintao Wu,Xujiang Zhao,Dong Li,Qin Tian,Linlin Yu*

Main category: cs.CV

TL;DR: The paper proposes a new method utilizing optimized positive and negative prompts from large language models (LLMs) to enhance out-of-distribution (OOD) detection by integrating semantic knowledge into visual modalities.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in OOD detection caused by the broad and misleading capture of non-in-distribution (ID) features and improve classification between ID and OOD images.

Method: Positive and Negative Prompt Supervision, which optimizes prompts generated by LLMs focusing on class-specific features via a graph-based architecture that transfers semantic knowledge to visual modalities for improved OOD detection.

Result: The method demonstrates superior OOD detection performance compared to state-of-the-art baselines through experiments conducted on CIFAR-100 and ImageNet-1K across eight OOD datasets and five LLMs.

Conclusion: Optimizing class-specific positive and negative prompts and transferring semantic supervision enhances OOD detection performance effectively, showcasing strong applicability in vision-language models.

Abstract: Out-of-distribution (OOD) detection is committed to delineating the classification boundaries between in-distribution (ID) and OOD images. Recent advances in vision-language models (VLMs) have demonstrated remarkable OOD detection performance by integrating both visual and textual modalities. In this context, negative prompts are introduced to emphasize the dissimilarity between image features and prompt content. However, these prompts often include a broad range of non-ID features, which may result in suboptimal outcomes due to the capture of overlapping or misleading information. To address this issue, we propose Positive and Negative Prompt Supervision, which encourages negative prompts to capture inter-class features and transfers this semantic knowledge to the visual modality to enhance OOD detection performance. Our method begins with class-specific positive and negative prompts initialized by large language models (LLMs). These prompts are subsequently optimized, with positive prompts focusing on features within each class, while negative prompts highlight features around category boundaries. Additionally, a graph-based architecture is employed to aggregate semantic-aware supervision from the optimized prompt representations and propagate it to the visual branch, thereby enhancing the performance of the energy-based OOD detector. Extensive experiments on two benchmarks, CIFAR-100 and ImageNet-1K, across eight OOD datasets and five different LLMs, demonstrate that our method outperforms state-of-the-art baselines.

</details>


### [103] [Facial Expression Recognition with YOLOv11 and YOLOv12: A Comparative Study](https://arxiv.org/abs/2511.10940)
*Umma Aymon,Nur Shazwani Kamarudin,Ahmad Fakhri Ab. Nasir*

Main category: cs.CV

TL;DR: The study evaluates two lightweight YOLO models for Facial Expression Recognition (FER) using benchmark datasets, showing a trade-off between sensitivity and precision across different environments.


<details>
  <summary>Details</summary>
Motivation: To improve facial expression recognition (FER) in real-world and controlled environments using lightweight AI models that are efficient for real-time applications.

Method: The models YOLOv11n and YOLOv12n were tested in a unified detection-classification framework on FER2013 and KDEF datasets, converted into object detection format. Performance was assessed with mAP 0.5, precision, recall, and confusion matrices.

Result: YOLOv12n showed superior performance on the clean KDEF dataset (mAP 0.5 of 95.6) and better sensitivity on FER2013 (mAP 63.8), while YOLOv11n performed better in precision (65.2) on the noisy FER2013 dataset.

Conclusion: Lightweight YOLO models exhibit adaptability and efficiency, with strengths in different areas, making them well-suited for real-time, resource-constrained emotion-aware AI applications.

Abstract: Facial Expression Recognition remains a challenging task, especially in unconstrained, real-world environments. This study investigates the performance of two lightweight models, YOLOv11n and YOLOv12n, which are the nano variants of the latest official YOLO series, within a unified detection and classification framework for FER. Two benchmark classification datasets, FER2013 and KDEF, are converted into object detection format and model performance is evaluated using mAP 0.5, precision, recall, and confusion matrices. Results show that YOLOv12n achieves the highest overall performance on the clean KDEF dataset with a mAP 0.5 of 95.6, and also outperforms YOLOv11n on the FER2013 dataset in terms of mAP 63.8, reflecting stronger sensitivity to varied expressions. In contrast, YOLOv11n demonstrates higher precision 65.2 on FER2013, indicating fewer false positives and better reliability in noisy, real-world conditions. On FER2013, both models show more confusion between visually similar expressions, while clearer class separation is observed on the cleaner KDEF dataset. These findings underscore the trade-off between sensitivity and precision, illustrating how lightweight YOLO models can effectively balance performance and efficiency. The results demonstrate adaptability across both controlled and real-world conditions, establishing these models as strong candidates for real-time, resource-constrained emotion-aware AI applications.

</details>


### [104] [Heterogeneous Complementary Distillation](https://arxiv.org/abs/2511.10942)
*Liuchi Xu,Hao Zheng,Lu Wang,Lisheng Xu,Jun Cheng*

Main category: cs.CV

TL;DR: This paper proposes Heterogeneous Complementary Distillation (HCD), a framework designed to efficiently transfer knowledge between models with heterogeneous architectures, such as Vision Transformer (ViT) to ResNet18, and addresses their feature disparity issues.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous architecture distillation faces challenges due to differences in spatial feature representation, and traditional KD methods fail to address these disparities effectively. Existing heterogeneous KD approaches are computationally expensive, complex, or overly dependent on logit alignment, limiting their use of complementary features.

Method: Heterogeneous Complementary Distillation (HCD) integrates both teacher and student features into a shared logit space using convolutional projectors, adaptive pooling, and a Complementary Feature Mapper (CFM). It decomposes logits into sub-logits via Sub-logit Decoupled Distillation (SDD) and applies Orthogonality Loss (OL) to enhance knowledge transfer efficiency and diversity.

Result: Experiments conducted on CIFAR-100, CUB200, and ImageNet-1K datasets demonstrate superior performance of HCD compared to state-of-the-art KD methods, with improved robustness and generalization.

Conclusion: HCD proves to be a simple yet effective solution for addressing challenges in heterogeneous KD, outperforming existing approaches and showcasing its versatility across various datasets.

Abstract: Knowledge distillation (KD)transfers the dark knowledge from a complex teacher to a compact student. However, heterogeneous architecture distillation, such as Vision Transformer (ViT) to ResNet18, faces challenges due to differences in spatial feature representations.Traditional KD methods are mostly designed for homogeneous architectures and hence struggle to effectively address the disparity. Although heterogeneous KD approaches have been developed recently to solve these issues, they often incur high computational costs and complex designs, or overly rely on logit alignment, which limits their ability to leverage the complementary features. To overcome these limitations, we propose Heterogeneous Complementary Distillation (HCD),a simple yet effective framework that integrates complementary teacher and student features to align representations in shared logits.These logits are decomposed and constrained to facilitate diverse knowledge transfer to the student. Specifically, HCD processes the student's intermediate features through convolutional projector and adaptive pooling, concatenates them with teacher's feature from the penultimate layer and then maps them via the Complementary Feature Mapper (CFM) module, comprising fully connected layer,to produce shared logits.We further introduce Sub-logit Decoupled Distillation (SDD) that partitions the shared logits into n sub-logits, which are fused with teacher's logits to rectify classification.To ensure sub-logit diversity and reduce redundant knowledge transfer, we propose an Orthogonality Loss (OL).By preserving student-specific strengths and leveraging teacher knowledge,HCD enhances robustness and generalization in students.Extensive experiments on the CIFAR-100, Fine-grained (e.g., CUB200)and ImageNet-1K datasets demonstrate that HCD outperforms state-of-the-art KD methods,establishing it as an effective solution for heterogeneous KD.

</details>


### [105] [Divide, Conquer and Unite: Hierarchical Style-Recalibrated Prototype Alignment for Federated Medical Image Segmentation](https://arxiv.org/abs/2511.10945)
*Xingyue Zhao,Wenke Huang,Xingguang Wang,Haoyu Zhao,Linghao Zhuang,Anwen Jiang,Guancheng Wan,Mang Ye*

Main category: cs.CV

TL;DR: The paper proposes FedBCS to handle challenges of feature heterogeneity in federated learning by aligning domain-invariant prototypes, showing strong performance in medical model training.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning models struggle with feature heterogeneity due to scanner/protocol differences. Current methods neglect multi-level context and domain-specific biases.

Method: FedBCS introduces adaptive style recalibration in prototype construction to decouple content-style representations, and aligns prototypes at dual levels (encoder and decoder).

Result: Validated on two public datasets, the proposed method shows impressive performance compared to existing segmentation methods.

Conclusion: FedBCS effectively addresses feature heterogeneity by aligning domain-invariant prototypes, enhancing robustness and segmentation accuracy.

Abstract: Federated learning enables multiple medical institutions to train a global model without sharing data, yet feature heterogeneity from diverse scanners or protocols remains a major challenge. Many existing works attempt to address this issue by leveraging model representations (e.g., mean feature vectors) to correct local training; however, they often face two key limitations: 1) Incomplete Contextual Representation Learning: Current approaches primarily focus on final-layer features, overlooking critical multi-level cues and thus diluting essential context for accurate segmentation. 2) Layerwise Style Bias Accumulation: Although utilizing representations can partially align global features, these methods neglect domain-specific biases within intermediate layers, allowing style discrepancies to build up and reduce model robustness. To address these challenges, we propose FedBCS to bridge feature representation gaps via domain-invariant contextual prototypes alignment. Specifically, we introduce a frequency-domain adaptive style recalibration into prototype construction that not only decouples content-style representations but also learns optimal style parameters, enabling more robust domain-invariant prototypes. Furthermore, we design a context-aware dual-level prototype alignment method that extracts domain-invariant prototypes from different layers of both encoder and decoder and fuses them with contextual information for finer-grained representation alignment. Extensive experiments on two public datasets demonstrate that our method exhibits remarkable performance.

</details>


### [106] [Abstract 3D Perception for Spatial Intelligence in Vision-Language Models](https://arxiv.org/abs/2511.10946)
*Yifan Liu,Fangneng Zhan,Kaichen Zhou,Yilun Du,Paul Pu Liang,Hanspeter Pfister*

Main category: cs.CV

TL;DR: Vision-language models face challenges in 3D-related tasks, and SandboxVLM framework uses abstract bounding boxes to enhance 3D reasoning.


<details>
  <summary>Details</summary>
Motivation: Vision-language models are limited in 3D reasoning, essential for real-world uses like robotics, due to their 2D-focused training methods.

Method: SandboxVLM introduces a framework using abstract bounding boxes, employing a 3D Sandbox reconstruction pipeline with multi-view priors, clustering, and reasoning.

Result: SandboxVLM improved performance in zero-shot 3D tasks, with an 8.3% gain in spatial intelligence benchmarks over existing models.

Conclusion: Embedding 3D abstractions into vision-language models' workflow significantly boosts their 3D reasoning capabilities without requiring re-training.

Abstract: Vision-language models (VLMs) struggle with 3D-related tasks such as spatial cognition and physical understanding, which are crucial for real-world applications like robotics and embodied agents. We attribute this to a modality gap between the 3D tasks and the 2D training of VLM, which led to inefficient retrieval of 3D information from 2D input. To bridge this gap, we introduce SandboxVLM, a simple yet effective framework that leverages abstract bounding boxes to encode geometric structure and physical kinematics for VLM. Specifically, we design a 3D Sandbox reconstruction and perception pipeline comprising four stages: generating multi-view priors with abstract control, proxy elevation, multi-view voting and clustering, and 3D-aware reasoning. Evaluated in zero-shot settings across multiple benchmarks and VLM backbones, our approach consistently improves spatial intelligence, achieving an 8.3\% gain on SAT Real compared with baseline methods for instance. These results demonstrate that equipping VLMs with a 3D abstraction substantially enhances their 3D reasoning ability without additional training, suggesting new possibilities for general-purpose embodied intelligence.

</details>


### [107] [DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2511.10948)
*Ren Zhang,Huilai Li,Chao qi,Guoliang Xu,Tianyu Zhou,Wei wei,Jianqin Yin*

Main category: cs.CV

TL;DR: The paper introduces DEFT-LLM, a method to improve micro expression recognition (MER) by addressing issues of entangled cues and semantic gaps in existing datasets with a disentangling architecture and motion-aligned text data.


<details>
  <summary>Details</summary>
Motivation: To enhance micro expression recognition by overcoming the challenges of entangled static and dynamic cues, and semantic gap between text labels and facial muscle movements.

Method: Proposed DEFT-LLM combines a motion-driven dataset, Uni-MER, with disentanglement techniques using three experts to decouple facial dynamics into interpretable representations.

Result: The method achieves state-of-the-art performance on MER benchmarks and provides better interpretability of local facial motion.

Conclusion: The integration of motion-aligned knowledge and disentanglement design in DEFT-LLM leads to both effective and interpretable micro expression recognition.

Abstract: Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.

</details>


### [108] [Language-Guided Graph Representation Learning for Video Summarization](https://arxiv.org/abs/2511.10953)
*Wenrui Li,Wei Han,Hengyu Man,Wangmeng Zuo,Xiaopeng Fan,Yonghong Tian*

Main category: cs.CV

TL;DR: This paper proposes the Language-guided Graph Representation Learning Network (LGRLN) to improve video summarization by addressing challenges in capturing global dependencies, multimodal customization, and semantic proximity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing video summarization methods, which struggle with global dependency capture, multimodal customization, and aligning semantic rather than temporal similarities of video frames.

Method: The authors developed LGRLN, a model that employs a video graph generator for structured video representation, an intra-graph module with a dual-threshold graph convolution mechanism for semantic frame selection, and a language-guided cross-modal embedding for customized summaries.

Result: The method outperformed existing techniques on multiple benchmarks, significantly reducing inference time by 87.8% and model parameters by 91.7%.

Conclusion: LGRLN advances video summarization by improving efficiency and accuracy through innovative graph representation techniques and cross-modal integration, potentially setting a new benchmark for the field.

Abstract: With the rapid growth of video content on social media, video summarization has become a crucial task in multimedia processing. However, existing methods face challenges in capturing global dependencies in video content and accommodating multimodal user customization. Moreover, temporal proximity between video frames does not always correspond to semantic proximity. To tackle these challenges, we propose a novel Language-guided Graph Representation Learning Network (LGRLN) for video summarization. Specifically, we introduce a video graph generator that converts video frames into a structured graph to preserve temporal order and contextual dependencies. By constructing forward, backward and undirected graphs, the video graph generator effectively preserves the sequentiality and contextual relationships of video content. We designed an intra-graph relational reasoning module with a dual-threshold graph convolution mechanism, which distinguishes semantically relevant frames from irrelevant ones between nodes. Additionally, our proposed language-guided cross-modal embedding module generates video summaries with specific textual descriptions. We model the summary generation output as a mixture of Bernoulli distribution and solve it with the EM algorithm. Experimental results show that our method outperforms existing approaches across multiple benchmarks. Moreover, we proposed LGRLN reduces inference time and model parameters by 87.8% and 91.7%, respectively. Our codes and pre-trained models are available at https://github.com/liwrui/LGRLN.

</details>


### [109] [Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition](https://arxiv.org/abs/2511.10958)
*Gunho Jung,Heejo Kong,Seong-Whan Lee*

Main category: cs.CV

TL;DR: The paper introduces TG-DFER, a text-guided weakly supervised framework to address the challenges in dynamic facial expression recognition (DFER) by enhancing semantic guidance and temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Dynamic facial expression recognition struggles with the many-to-one labeling problem and the complexity of visual and temporal dynamics. Existing Multiple Instance Learning approaches are limited in tackling these challenges effectively.

Method: TG-DFER integrates a vision-language pre-trained model for textual semantic guidance, employs visual prompts for fine-grained alignment of emotion labels with visual features, and incorporates a multi-grained temporal network to model facial dynamics across video sequences.

Result: TG-DFER demonstrates enhanced generalization, interpretability, and temporal sensitivity in DFER tasks, as shown by extensive empirical results.

Conclusion: The proposed TG-DFER framework effectively addresses DFER challenges through improved semantic and temporal understanding, offering a promising direction for emotional state recognition in videos.

Abstract: Dynamic facial expression recognition (DFER) aims to identify emotional states by modeling the temporal changes in facial movements across video sequences. A key challenge in DFER is the many-to-one labeling problem, where a video composed of numerous frames is assigned a single emotion label. A common strategy to mitigate this issue is to formulate DFER as a Multiple Instance Learning (MIL) problem. However, MIL-based approaches inherently suffer from the visual diversity of emotional expressions and the complexity of temporal dynamics. To address this challenge, we propose TG-DFER, a text-guided weakly supervised framework that enhances MIL-based DFER by incorporating semantic guidance and coherent temporal modeling. We incorporate a vision-language pre-trained (VLP) model is integrated to provide semantic guidance through fine-grained textual descriptions of emotional context. Furthermore, we introduce visual prompts, which align enriched textual emotion labels with visual instance features, enabling fine-grained reasoning and frame-level relevance estimation. In addition, a multi-grained temporal network is designed to jointly capture short-term facial dynamics and long-range emotional flow, ensuring coherent affective understanding across time. Extensive results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity under weak supervision.

</details>


### [110] [ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization](https://arxiv.org/abs/2511.10971)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Heng Ping,Tamoghna Chattopadhyay,Sophia I Thomopoulos,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.CV

TL;DR: The paper introduces ERMoE, a sparse Mixture-of-Experts architecture using a learned orthonormal eigenbasis for stable and interpretable routing, achieving state-of-the-art results on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address issues in Mixture-of-Experts (MoE) architectures, including unstable routing, expert underutilization, and load imbalances, while maintaining performance and expert specialization.

Method: ERMoE reparameterizes each expert using a learned orthonormal eigenbasis and replaces learned router logits with a content-aware 'Eigenbasis Score,' tying token assignments directly to experts' representation spaces and removing the need for explicit load-balancing losses.

Result: ERMoE achieves state-of-the-art performance on ImageNet classification, COCO, and Flickr30K benchmarks, improves brain age prediction in a medical imaging variant by over 7%, and naturally balances expert loads while offering interpretable specialization.

Conclusion: ERMoE presents a novel approach to sparse expert models by addressing routing and load imbalance issues through content-aware routing, yielding improved accuracy, scalability, and expert interpretability.

Abstract: Mixture-of-Experts (MoE) architectures expand model capacity by sparsely activating experts but face two core challenges: misalignment between router logits and each expert's internal structure leads to unstable routing and expert underutilization, and load imbalances create straggler bottlenecks. Standard solutions, such as auxiliary load-balancing losses, can reduce load disparities but often weaken expert specialization and hurt downstream performance. To address these issues, we propose ERMoE, a sparse MoE transformer that reparameterizes each expert in a learned orthonormal eigenbasis and replaces learned gating logits with an "Eigenbasis Score", defined as the cosine similarity between input features and an expert's basis. This content-aware routing ties token assignments directly to experts' representation spaces, stabilizing utilization and promoting interpretable specialization without sacrificing sparsity. Crucially, ERMoE removes the need for explicit balancing losses and avoids the interfering gradients they introduce. We show that ERMoE achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval benchmarks (e.g., COCO, Flickr30K), while naturally producing flatter expert load distributions. Moreover, a 3D MRI variant (ERMoE-ba) improves brain age prediction accuracy by more than 7\% and yields anatomically interpretable expert specializations. ERMoE thus introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.

</details>


### [111] [Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2511.10974)
*Haoran Chen,Houze Xu,Micah Goldblum,Daoguo Dong,Zuxuan Wu*

Main category: cs.CV

TL;DR: The paper introduces DMC and DMC-OT frameworks to enhance Class-Incremental Learning (CIL) in CLIP models, addressing classifier bias and distributional drift issues.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle challenges in Class-incremental learning (CIL) for vision-language models, specifically in avoiding classifier bias and managing distributional drift during continuous learning.

Method: The DMC framework decouples the adaptation of the vision encoder and optimization of textual prompts via a two-stage training approach with frozen modalities. DMC-OT extends this with optimal transport-based calibration and task-specific prompting.

Result: Experiments on datasets such as CIFAR-100, Imagenet-R, CUB-200, and UCF-101 show that DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT improving accuracy by an average of 1.80%.

Conclusion: The proposed DMC and DMC-OT frameworks effectively address critical challenges in CIL with vision-language models, enabling improved learning without forgetting and better task separability.

Abstract: Class-incremental learning (CIL) enables models to continuously learn new categories from sequential tasks without forgetting previously acquired knowledge. While recent advances in vision-language models such as CLIP have demonstrated strong generalization across domains, extending them to continual settings remains challenging. In particular, learning task-specific soft prompts for newly introduced classes often leads to severe classifier bias, as the text prototypes overfit to recent categories when prior data are unavailable. In this paper, we propose DMC, a simple yet effective two-stage framework for CLIP-based CIL that decouples the adaptation of the vision encoder and the optimization of textual soft prompts. Each stage is trained with the other frozen, allowing one modality to act as a stable semantic anchor for the other to preserve cross-modal alignment. Furthermore, current CLIP-based CIL approaches typically store class-wise Gaussian statistics for generative replay, yet they overlook the distributional drift that arises when the vision encoder is updated over time. To address this issue, we introduce DMC-OT, an enhanced version of DMC that incorporates an optimal-transport guided calibration strategy to align memory statistics across evolving encoders, along with a task-specific prompting design that enhances inter-task separability. Extensive experiments on CIFAR-100, Imagenet-R, CUB-200, and UCF-101 demonstrate that both DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT further improving accuracy by an average of 1.80%.

</details>


### [112] [PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs](https://arxiv.org/abs/2511.10979)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Hang Wu,Yiwei Wang*

Main category: cs.CV

TL;DR: This paper addresses issues of temporal inconsistency in Video LLMs and introduces a training-free mechanism, Phase Aggregated Smoothing (PAS), to improve stability and performance.


<details>
  <summary>Details</summary>
Motivation: Video LLMs experience instability due to sensitivity to small timing shifts in video frames, caused by the improper extension of Rotary Position Embeddings through multimodal RoPE.

Method: The authors propose Phase Aggregated Smoothing (PAS), a mechanism that applies phase offsets across attention heads and aggregates their outputs, smoothing the temporal kernel and reducing sensitivity to timing shifts in videos.

Result: Experiments across video understanding benchmarks show consistent improvements in model stability and performance with negligible computational overhead when using PAS.

Conclusion: Phase Aggregated Smoothing is a plug and play solution that enhances temporal encoding stability in Video LLMs without additional training or significant resource requirements.

Abstract: Video LLMs suffer from temporal inconsistency: small shifts in frame timing can flip attention and suppress relevant frames. We trace this instability to the common extension of Rotary Position Embeddings to video through multimodal RoPE. The induced inverse Fourier time kernel exhibits frame-scale ripples that multiply adjacent frames by different factors, which perturbs attention that should otherwise be governed by the raw query key inner product. We present Phase Aggregated Smoothing (PAS), a simple, training-free mechanism that applies small opposed phase offsets across heads and then aggregates their outputs. PAS preserves the per-head spectrum magnitude, while the aggregation effectively smooths the temporal kernel and reduces phase sensitivity without changing the positional encoding structure. Our analysis shows that the RoPE rotated logit can be approximated as a content dot product scaled by a time kernel; smoothing this kernel yields Lipschitz stability of attention to small temporal shifts; multi phase averaging attenuates high frequency ripples while preserving per-head spectra under Nyquist-valid sampling. Experiments on multiple video understanding benchmarks under matched token budgets show consistent improvements with negligible computational overhead. PAS provides a plug and play upgrade for robust temporal encoding in Video LLMs.

</details>


### [113] [Binary Verification for Zero-Shot Vision](https://arxiv.org/abs/2511.10983)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: The paper proposes a training-free binary verification workflow for zero-shot vision using off-the-shelf visual language models (VLMs). The workflow uses quantization and binarization steps to improve performance across various vision tasks with significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance zero-shot vision tasks by leveraging visual language models without relying on task-specific training but focusing on inference-time design.

Method: The workflow involves two steps: quantization (converting open-ended queries into multiple-choice questions) and binarization (resolving True/False for each candidate and handling dominant candidate selection).

Result: Evaluation across multiple tasks demonstrates strong performance improvements compared to directly answering open-ended queries, highlighting a generalized and unified approach.

Conclusion: The binary verification workflow provides a practical and unified method for improving zero-shot vision using VLMs, emphasizing inference-time techniques instead of task-specific training models.

Abstract: We propose a training-free, binary verification workflow for zero-shot vision with off-the-shelf VLMs. It comprises two steps: (i) quantization, which turns the open-ended query into a multiple-choice question (MCQ) with a small, explicit list of unambiguous candidates; and (ii) binarization, which asks one True/False question per candidate and resolves deterministically: if exactly one is True, select it; otherwise, revert to an MCQ over the remaining plausible candidates. We evaluate the workflow on referring expression grounding (REC), spatial reasoning (Spatial-Map, Spatial-Grid, Spatial-Maze), and BLINK-Jigsaw. Relative to answering open-ended queries directly, quantization to MCQ yields large gains, and True/False binarization provides a consistent additional boost. Across all tasks, the same workflow produces significant improvements, indicating generality. Our theory formalizes how open-ended vision queries can be quantized to MCQs and further binarized into True/False verifications, establishing a hardness ladder. A simple analysis explains why Boolean resolution boosts accuracy. Together, these components yield a simple and unified workflow that emphasizes inference-time design over task-specific training. It offers a practical, drop-in path to stronger zero-shot vision with today's VLMs.

</details>


### [114] [Rethinking Autoregressive Models for Lossless Image Compression via Hierarchical Parallelism and Progressive Adaptation](https://arxiv.org/abs/2511.10991)
*Daxin Li,Yuanchao Bai,Kai Wang,Wenbo Zhao,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: The paper introduces HPAC, a novel framework improving autoregressive models for lossless image compression with efficient techniques, achieving a new state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models are dismissed due to computational costs despite their theoretical superiority in lossless image compression; the paper aims to make them practical and top-performing.

Method: The Hierarchical Parallel Autoregressive ConvNet (HPAC) incorporates hierarchical factorized structures, convolutional gating, optimizations like CSI and AFC, and progressive adaptation using SARP-FT.

Result: Experiments across diverse datasets demonstrate HPAC achieves state-of-the-art compression performance with low parameter count and competitive coding speeds.

Conclusion: Carefully designed autoregressive frameworks can offer significant improvements in learned lossless image compression, establishing their practicality and superiority.

Abstract: Autoregressive (AR) models, the theoretical performance benchmark for learned lossless image compression, are often dismissed as impractical due to prohibitive computational cost. This work re-thinks this paradigm, introducing a framework built on hierarchical parallelism and progressive adaptation that re-establishes pure autoregression as a top-performing and practical solution. Our approach is embodied in the Hierarchical Parallel Autoregressive ConvNet (HPAC), an ultra-lightweight pre-trained model using a hierarchical factorized structure and content-aware convolutional gating to efficiently capture spatial dependencies. We introduce two key optimizations for practicality: Cache-then-Select Inference (CSI), which accelerates coding by eliminating redundant computations, and Adaptive Focus Coding (AFC), which efficiently extends the framework to high bit-depth images. Building on this efficient foundation, our progressive adaptation strategy is realized by Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT). This instance-level strategy fine-tunes the model for each test image by optimizing low-rank adapters on progressively larger, spatially-continuous regions selected via estimated information density. Experiments on diverse datasets (natural, satellite, medical) validate that our method achieves new state-of-the-art compression. Notably, our approach sets a new benchmark in learned lossless compression, showing a carefully designed AR framework can offer significant gains over existing methods with a small parameter count and competitive coding speeds.

</details>


### [115] [CLUE: Controllable Latent space of Unprompted Embeddings for Diversity Management in Text-to-Image Synthesis](https://arxiv.org/abs/2511.10993)
*Keunwoo Park,Jihye Chae,Joong Ho Ahn,Jihoon Kweon*

Main category: cs.CV

TL;DR: CLUE is a generative model framework that enhances text-to-image synthesis for specialized fields by improving diversity and stability without additional data.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image synthesis models struggle with diversity and stability in domain-specific fields like medicine due to limited data availability.

Method: CLUE utilizes a Style Encoder with fixed-format prompts integrated into the Stable Diffusion architecture. It employs a second attention layer and Kullback-Leibler divergence for latent space representation.

Result: On an otitis media dataset, CLUE achieved significant improvements in FID (9.30 vs. 46.81) and recall (70.29% vs. 49.60%). Synthetic-only training improved F1 scores across multiple tested scales, showing impactful results as a data augmentation method.

Conclusion: CLUE demonstrates diverse and stable image generation from limited datasets, making it effective for data augmentation in specialized applications like medicine.

Abstract: Text-to-image synthesis models require the ability to generate diverse images while maintaining stability. To overcome this challenge, a number of methods have been proposed, including the collection of prompt-image datasets and the integration of additional data modalities during training. Although these methods have shown promising results in general domains, they face limitations when applied to specialized fields such as medicine, where only limited types and insufficient amounts of data are available. We present CLUE (Controllable Latent space of Unprompted Embeddings), a generative model framework that achieves diverse generation while maintaining stability through fixed-format prompts without requiring any additional data. Based on the Stable Diffusion architecture, CLUE employs a Style Encoder that processes images and prompts to generate style embeddings, which are subsequently fed into a new second attention layer of the U-Net architecture. Through Kullback-Leibler divergence, the latent space achieves continuous representation of image features within Gaussian regions, independent of prompts. Performance was assessed on otitis media dataset. CLUE reduced FID to 9.30 (vs. 46.81) and improved recall to 70.29% (vs. 49.60%). A classifier trained on synthetic-only data at 1000% scale achieved an F1 score of 83.21% (vs. 73.83%). Combining synthetic data with equal amounts of real data achieved an F1 score of 94.76%, higher than when using only real data. On an external dataset, synthetic-only training achieved an F1 score of 76.77% (vs. 60.61%) at 1000% scale. The combined approach achieved an F1 score of 85.78%, higher than when using only the internal dataset. These results demonstrate that CLUE enables diverse yet stable image generation from limited datasets and serves as an effective data augmentation method for domain-specific applications.

</details>


### [116] [PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities](https://arxiv.org/abs/2511.10997)
*Jiajun Chen,Sai Cheng,Yutao Yuan,Yirui Zhang,Haitao Yuan,Peng Peng,Yi Zhong*

Main category: cs.CV

TL;DR: The paper introduces PROMISE, a novel framework for robust multimodal representation learning that remains effective even when modalities are missing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the issue of degraded performance in multimodal models caused by inconsistent learning when certain modalities are absent in real-world situations.

Method: PROMISE employs multimodal prompt learning within a hierarchical contrastive learning framework, featuring a prompt-attention mechanism to dynamically generate robust representations for incomplete modalities.

Result: Experiments and ablation studies on benchmark datasets demonstrate PROMISE's superior performance compared to current state-of-the-art multimodal methods.

Conclusion: PROMISE effectively bridges the representation gap between complete and incomplete multimodal data, providing robust and consistent performance even in the absence of specific modalities.

Abstract: Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.

</details>


### [117] [EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation](https://arxiv.org/abs/2511.11002)
*Zongyang Qiu,Bingyuan Wang,Xingbei Chen,Yingqing He,Zeyu Wang*

Main category: cs.CV

TL;DR: The paper introduces EmoVid, a multimodal, emotion-annotated video dataset for stylized media, enabling advances in emotion-conditioned video generation.


<details>
  <summary>Details</summary>
Motivation: Current video generation systems prioritize visual metrics but lack focus on affective dimensions, particularly for non-realistic and creative contexts.

Method: Develop EmoVid, an annotated dataset with emotion labels, visual attributes, and captions, and propose an emotion-conditioned generation technique based on fine-tuning the Wan2.1 model.

Result: Achieved improvement in quantitative metrics and visual quality of generated videos for text-to-video and image-to-video tasks.

Conclusion: EmoVid sets the benchmark for emotion-focused video generation and provides insights into enhancing emotional expression in styled videos.

Abstract: Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.

</details>


### [118] [MeCaMIL: Causality-Aware Multiple Instance Learning for Fair and Interpretable Whole Slide Image Diagnosis](https://arxiv.org/abs/2511.11004)
*Yiran Song,Yikai Zhang,Shuang Zhou,Guojun Xiong,Xiaofeng Yang,Nian Wang,Fenglong Ma,Rui Zhang,Mingquan Lin*

Main category: cs.CV

TL;DR: This paper introduces MeCaMIL, a causality-aware MIL framework designed for computational pathology, addressing fairness and interpretability issues. The framework achieves state-of-the-art diagnostic and fairness performance.


<details>
  <summary>Details</summary>
Motivation: Existing MIL approaches for WSI analysis lack causal interpretability and fail to integrate patient demographic information, leading to fairness concerns and hindrances in clinical translation.

Method: MeCaMIL employs structured causal graphs and causal inference techniques (e.g., do-calculus, collider structures) to disentangle disease-relevant signals from demographic confounders.

Result: MeCaMIL demonstrated state-of-the-art accuracy, AUC, and fairness metrics across datasets like CAMELYON16, TCGA-Lung, and TCGA-Multi, with substantial fairness improvements and generalization to survival prediction.

Conclusion: MeCaMIL establishes itself as a powerful, fair, and interpretable AI framework for digital pathology, addressing biases and enhancing clinical applicability.

Abstract: Multiple instance learning (MIL) has emerged as the dominant paradigm for whole slide image (WSI) analysis in computational pathology, achieving strong diagnostic performance through patch-level feature aggregation. However, existing MIL methods face critical limitations: (1) they rely on attention mechanisms that lack causal interpretability, and (2) they fail to integrate patient demographics (age, gender, race), leading to fairness concerns across diverse populations. These shortcomings hinder clinical translation, where algorithmic bias can exacerbate health disparities. We introduce \textbf{MeCaMIL}, a causality-aware MIL framework that explicitly models demographic confounders through structured causal graphs. Unlike prior approaches treating demographics as auxiliary features, MeCaMIL employs principled causal inference -- leveraging do-calculus and collider structures -- to disentangle disease-relevant signals from spurious demographic correlations. Extensive evaluation on three benchmarks demonstrates state-of-the-art performance across CAMELYON16 (ACC/AUC/F1: 0.939/0.983/0.946), TCGA-Lung (0.935/0.979/0.931), and TCGA-Multi (0.977/0.993/0.970, five cancer types). Critically, MeCaMIL achieves superior fairness -- demographic disparity variance drops by over 65% relative reduction on average across attributes, with notable improvements for underserved populations. The framework generalizes to survival prediction (mean C-index: 0.653, +0.017 over best baseline across five cancer types). Ablation studies confirm causal graph structure is essential -- alternative designs yield 0.048 lower accuracy and 4.2x times worse fairness. These results establish MeCaMIL as a principled framework for fair, interpretable, and clinically actionable AI in digital pathology. Code will be released upon acceptance.

</details>


### [119] [Phys-Liquid: A Physics-Informed Dataset for Estimating 3D Geometry and Volume of Transparent Deformable Liquids](https://arxiv.org/abs/2511.11077)
*Ke Ma,Yizhou Fang,Jean-Baptiste Weibel,Shuai Tan,Xinggang Wang,Yang Xiao,Yi Fang,Tian Xia*

Main category: cs.CV

TL;DR: This paper tackles the issue of estimating geometric and volumetric properties of transparent liquids during dynamic surface deformations caused by container movements. It introduces Phys-Liquid, a physics-informed dataset with 97,200 simulation images to enhance liquid perception and handling tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in accurate liquid state assessment due to motion-induced deformations in transparent liquids, which complicate tasks for autonomous robots in fields like liquid manipulation.

Method: The authors created Phys-Liquid, a dataset of 97,200 simulated images and 3D meshes, and designed a four-stage pipeline for liquid state assessment. The pipeline includes segmentation, mask generation, 3D mesh reconstruction, and scaling.

Result: Experimental evaluation shows that the proposed method, leveraging the Phys-Liquid dataset, achieves more accurate and consistent liquid geometry and volume estimation, surpassing current benchmarks.

Conclusion: Phys-Liquid and the associated methodologies provide a foundation for advancing transparent liquid perception tasks in dynamic scenarios, offering significant improvements over existing tools.

Abstract: Estimating the geometric and volumetric properties of transparent deformable liquids is challenging due to optical complexities and dynamic surface deformations induced by container movements. Autonomous robots performing precise liquid manipulation tasks, such as dispensing, aspiration, and mixing, must handle containers in ways that inevitably induce these deformations, complicating accurate liquid state assessment. Current datasets lack comprehensive physics-informed simulation data representing realistic liquid behaviors under diverse dynamic scenarios. To bridge this gap, we introduce Phys-Liquid, a physics-informed dataset comprising 97,200 simulation images and corresponding 3D meshes, capturing liquid dynamics across multiple laboratory scenes, lighting conditions, liquid colors, and container rotations. To validate the realism and effectiveness of Phys-Liquid, we propose a four-stage reconstruction and estimation pipeline involving liquid segmentation, multi-view mask generation, 3D mesh reconstruction, and real-world scaling. Experimental results demonstrate improved accuracy and consistency in reconstructing liquid geometry and volume, outperforming existing benchmarks. The dataset and associated validation methods facilitate future advancements in transparent liquid perception tasks. The dataset and code are available at https://dualtransparency.github.io/Phys-Liquid/.

</details>


### [120] [Draft and Refine with Visual Experts](https://arxiv.org/abs/2511.11005)
*Sungheon Jeong,Ryozo Masukawa,Jihong Park,Sanggeon Yun,Wenjun Huang,Hanning Chen,Mahdi Imani,Mohsen Imani*

Main category: cs.CV

TL;DR: The paper introduces "Draft and Refine" (DnR), which reduces hallucinations in vision-language models by improving their reliance on visual evidence through a utilization metric and external visual expert feedback.


<details>
  <summary>Details</summary>
Motivation: Current large vision-language models rely excessively on linguistic priors rather than visual evidence, leading to ungrounded or hallucinated responses. A method is needed to measure and enhance their use of visual information.

Method: The proposed DnR framework uses a question-conditioned utilization metric to measure reliance on visual evidence, refines initial drafts using feedback from external visual experts, and re-queries models with updated visual cues for improved grounding.

Result: Experiments on VQA and captioning tasks show accuracy improvements and reduced hallucinations by strengthening visual grounding through the DnR framework.

Conclusion: Measuring and improving visual utilization can make vision-language models more evidence-based, interpretable, and accurate without requiring retraining or architectural changes.

Abstract: While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.

</details>


### [121] [6D Strawberry Pose Estimation: Real-time and Edge AI Solutions Using Purely Synthetic Training Data](https://arxiv.org/abs/2511.11307)
*Saptarshi Neil Sinha,Julius Kühn,Mika Silvan Goschke,Michael Weinmann*

Main category: cs.CV

TL;DR: This paper presents a synthetic-data-driven approach for 6D pose estimation of strawberries using YOLOX-6D-Pose, aiming to enable automated fruit harvesting.


<details>
  <summary>Details</summary>
Motivation: To address high harvesting costs and labor shortages, this research aims to develop efficient automated systems for fruit harvesting.

Method: The authors use YOLOX-6D-Pose algorithm combined with synthetic data generation via a Blender pipeline to enhance training data realism for 6D pose estimation.

Result: Models showed strong pose estimation accuracy on RTX 3090 and Jetson Orin Nano, with RTX 3090 being faster. Challenges were noted for detecting unripe strawberries.

Conclusion: The approach is promising for strawberry harvesting automation and can be extended to other fruits, with opportunities for refinement in unripe fruit detection.

Abstract: Automated and selective harvesting of fruits has become an important area of research, particularly due to challenges such as high costs and a shortage of seasonal labor in advanced economies. This paper focuses on 6D pose estimation of strawberries using purely synthetic data generated through a procedural pipeline for photorealistic rendering. We employ the YOLOX-6D-Pose algorithm, a single-shot approach that leverages the YOLOX backbone, known for its balance between speed and accuracy, and its support for edge inference. To address the lacking availability of training data, we introduce a robust and flexible pipeline for generating synthetic strawberry data from various 3D models via a procedural Blender pipeline, where we focus on enhancing the realism of the synthesized data in comparison to previous work to make it a valuable resource for training pose estimation algorithms. Quantitative evaluations indicate that our models achieve comparable accuracy on both the NVIDIA RTX 3090 and Jetson Orin Nano across several ADD-S metrics, with the RTX 3090 demonstrating superior processing speed. However, the Jetson Orin Nano is particularly suited for resource-constrained environments, making it an excellent choice for deployment in agricultural robotics. Qualitative assessments further confirm the model's performance, demonstrating its capability to accurately infer the poses of ripe and partially ripe strawberries, while facing challenges in detecting unripe specimens. This suggests opportunities for future improvements, especially in enhancing detection capabilities for unripe strawberries (if desired) by exploring variations in color. Furthermore, the methodology presented could be adapted easily for other fruits such as apples, peaches, and plums, thereby expanding its applicability and impact in the field of agricultural automation.

</details>


### [122] [VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models](https://arxiv.org/abs/2511.11007)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Zhangquan Chen,Yudong Zhang,Yongbo He,Peng-Tao Jiang,Jiangning Zhang,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: The paper proposes VisMem, a dynamic memory-enhancement framework for Vision-Language Models (VLMs), improving their visual and semantic processing.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with maintaining visual grounding and semantic consistency during complex visual tasks due to a 'visual processing bottleneck.'

Method: Inspired by cognitive memory theory, VisMem incorporates short-term and long-term memory modules for visual and semantic retention, dynamically invoked during inference.

Result: VisMem achieves an average performance improvement of 11.8% over baseline models across various benchmarks for visual understanding, reasoning, and generation.

Conclusion: VisMem establishes a novel approach to enhance latent memory in VLMs, improving both perceptual fidelity and semantic contextualization during task execution.

Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.

</details>


### [123] [SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation](https://arxiv.org/abs/2511.11014)
*Sumin Yu,Taesup Moon*

Main category: cs.CV

TL;DR: This paper addresses safety in text-to-image (T2I) diffusion models by introducing SP-Guard, which applies adaptive and selective guidance to reduce the creation of harmful content.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the societal concerns posed by the ease of generating harmful content with T2I diffusion models and to enhance safety during image generation.

Method: The paper introduces SP-Guard, a method that assesses the harmfulness of prompts and selectively applies a guidance mask to unsafe regions of the generated images.

Result: SP-Guard has shown better performance in producing safer images compared to existing methods, with minimal unintended changes to intended content.

Conclusion: The study concludes that SP-Guard improves the safety of T2I models and emphasizes the importance of transparency and control in image generation processes.

Abstract: While diffusion-based T2I models have achieved remarkable image generation quality, they also enable easy creation of harmful content, raising social concerns and highlighting the need for safer generation. Existing inference-time guiding methods lack both adaptivity--adjusting guidance strength based on the prompt--and selectivity--targeting only unsafe regions of the image. Our method, SP-Guard, addresses these limitations by estimating prompt harmfulness and applying a selective guidance mask to guide only unsafe areas. Experiments show that SP-Guard generates safer images than existing methods while minimizing unintended content alteration. Beyond improving safety, our findings highlight the importance of transparency and controllability in image generation.

</details>


### [124] [SUPER Decoder Block for Reconstruction-Aware U-Net Variants](https://arxiv.org/abs/2511.11015)
*Siheon Joo,Hongjo Kim*

Main category: cs.CV

TL;DR: SUPER, a decoder block exploiting wavelets and Selectively Suppressed features, enhances U-Net variants for better reconstruction, especially for high-frequency details in small cracks.


<details>
  <summary>Details</summary>
Motivation: Overcoming the information loss in skip-connected encoder-decoder architectures like U-Net variants to better recover fine high-frequency details in tasks like segmentation and denoising.

Method: SUPER leverages wavelets' perfect reconstruction property and selectively suppresses redundant features. It acts as a plug-and-play decoder block for U-Net variants, eliminating reconstruction bottlenecks while enriching representation.

Result: SUPER improves thin-crack segmentation in the CrackVision12K dataset, especially for cracks under 4 pixels, with minimal computational overhead. It also achieves moderate PSNR gains in smartphone image denoising (SIDD).

Conclusion: SUPER enhances U-Net variants by addressing reconstruction bottlenecks and performing well across high- and low-frequency tasks, proving its general applicability.

Abstract: Skip-connected encoder-decoder architectures (U-Net variants) are widely adopted for inverse problems but still suffer from information loss, limiting recovery of fine high-frequency details. We present Selectively Suppressed Perfect Reconstruction (SUPER), which exploits the perfect reconstruction (PR) property of wavelets to prevent information degradation while selectively suppressing (SS) redundant features. Free from rigid framelet constraints, SUPER serves as a plug-and-play decoder block for diverse U-Net variants, eliminating their intrinsic reconstruction bottlenecks and enhancing representational richness. Experiments across diverse crack benchmarks, including state-of-the-art (SOTA) models, demonstrate the structural potential of the proposed SUPER Decoder Block. Maintaining comparable computational cost, SUPER enriches representational diversity through increased parameterization. In small-scale in-domain experiments on the CrackVision12K dataset, SUPER markedly improves thin-crack segmentation performance, particularly for cracks narrower than 4 px, underscoring its advantage in high-frequency dominant settings. In smartphone image denoising on SIDD, where low-frequency components prevail, SUPER still achieves a moderate gain in PSNR, confirming its robustness across low- and high-frequency regimes. These results validate its plug-and-play generality across U-Net variants, achieving high-frequency fidelity and global coherence within a unified, reconstruction-aware framework.

</details>


### [125] [AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning](https://arxiv.org/abs/2511.11025)
*Jirong Zha,Yuxuan Fan,Tianyu Zhang,Geng Chen,Yingfeng Chen,Chen Gao,Xinlei Chen*

Main category: cs.CV

TL;DR: The paper introduces AirCopBench, a benchmark for evaluating multimodal large language models (MLLMs) in collaborative aerial perception tasks under degraded conditions. Results show significant performance gaps between current models and humans.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for MLLMs focus on single-agent vision tasks, neglecting the evaluation of collaborative perception in multi-agent scenarios, particularly under real-world degraded conditions.

Method: A new benchmark, AirCopBench, was created using 14.6k+ questions derived from simulated and real-world data, focusing on four task dimensions and 14 task types. Data includes challenging degraded-perception scenarios with rigorous quality validation.

Result: Tests on 40 MLLMs revealed large performance gaps in collaborative tasks, with the best model lagging humans by 24.38% on average. Inconsistent performance across tasks was also observed. Fine-tuning experiments demonstrated effective sim-to-real transfer.

Conclusion: Existing MLLMs perform significantly below human-level in collaborative perception tasks. AirCopBench highlights challenges and opportunities for improvement in aerial multi-agent systems.

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.

</details>


### [126] [EmbryoDiff: A Conditional Diffusion Framework with Multi-Focal Feature Fusion for Fine-Grained Embryo Developmental Stage Recognition](https://arxiv.org/abs/2511.11027)
*Yong Sun,Zhengjie Zhang,Junyu Shi,Zhiyuan Zhang,Lijiang Liu,Qiang Nie*

Main category: cs.CV

TL;DR: This paper introduces EmbryoDiff, a framework leveraging robust feature extraction and fusion strategies to enhance embryo stage classification during IVF, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurately identifying embryo developmental stages during IVF is critical for embryo viability assessment, but existing methods are hindered by feature ambiguity and incomplete representations due to single-focal reliance.

Method: EmbryoDiff utilizes a two-stage diffusion-based framework comprising: (1) robust multi-focal feature extraction via a frame-level encoder, and (2) a Multi-Focal Feature Fusion Strategy for 3D-aware representation. It also uses a Hybrid Semantic-Boundary Condition Block for improved denoising and classification.

Result: EmbryoDiff achieved state-of-the-art test accuracy of 82.8% and 81.3% on two benchmark datasets with only a single denoising step.

Conclusion: The proposed EmbryoDiff framework effectively addresses limitations of previous approaches by leveraging multi-focal feature fusion and semantic-boundary cues, offering superior accuracy and efficiency in embryo stage classification.

Abstract: Identification of fine-grained embryo developmental stages during In Vitro Fertilization (IVF) is crucial for assessing embryo viability. Although recent deep learning methods have achieved promising accuracy, existing discriminative models fail to utilize the distributional prior of embryonic development to improve accuracy. Moreover, their reliance on single-focal information leads to incomplete embryonic representations, making them susceptible to feature ambiguity under cell occlusions. To address these limitations, we propose EmbryoDiff, a two-stage diffusion-based framework that formulates the task as a conditional sequence denoising process. Specifically, we first train and freeze a frame-level encoder to extract robust multi-focal features. In the second stage, we introduce a Multi-Focal Feature Fusion Strategy that aggregates information across focal planes to construct a 3D-aware morphological representation, effectively alleviating ambiguities arising from cell occlusions. Building on this fused representation, we derive complementary semantic and boundary cues and design a Hybrid Semantic-Boundary Condition Block to inject them into the diffusion-based denoising process, enabling accurate embryonic stage classification. Extensive experiments on two benchmark datasets show that our method achieves state-of-the-art results. Notably, with only a single denoising step, our model obtains the best average test performance, reaching 82.8% and 81.3% accuracy on the two datasets, respectively.

</details>


### [127] [Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types](https://arxiv.org/abs/2511.11030)
*Chi-Yu Chen,Rawan Abulibdeh,Arash Asgari,Leo Anthony Celi,Deirdre Goode,Hassan Hamidi,Laleh Seyyed-Kalantari,Po-Chih Kuo,Ned McCague,Thomas Sounack*

Main category: cs.CV

TL;DR: The paper demonstrates that AI models can predict a patient's socioeconomic status from chest X-rays with notable accuracy while revealing hidden social inequalities encoded in medical images.


<details>
  <summary>Details</summary>
Motivation: Investigate whether medical images, assumed to be neutral biological data, contain hidden socio-economic information and address the implications for fairness in medical AI.

Method: Deep vision models (DenseNet121, SwinV2-B, MedMamba) are trained on datasets (MIMIC-CXR-JPG, CheXpert) to predict health insurance type, with experiments controlling for factors like age, race, and sex.

Result: Models achieve significant accuracy (AUC ~0.67–0.68) in predicting insurance type, with the signal diffuse and embedded in the upper and mid-thoracic regions.

Conclusion: Medical images encode socio-economic markers, challenging assumptions of neutrality and reframing fairness in medical AI by highlighting the need to address embedded social fingerprints.

Abstract: Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patient's health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.67 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal persists even when age, race, and sex are controlled for, and remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.

</details>


### [128] [Accelerating Controllable Generation via Hybrid-grained Cache](https://arxiv.org/abs/2511.11031)
*Lin Liu,Huixia Ben,Shuo Wang,Jinda Lu,Junxiang Qiu,Shengeng Tang,Yanbin Hao*

Main category: cs.CV

TL;DR: The paper proposes a Hybrid-Grained Cache (HGC) system to optimize controllable generative models by introducing coarse-grained and fine-grained cache strategies, achieving efficiency and maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Generative models face challenges of high computational requirements and complexity in handling control conditions during synthetic visual content generation.

Method: The HGC approach uses coarse-grained caches (bypassing computations in encoder-decoder blocks) and fine-grained caches (reusing cross-attention maps) to reduce computational overhead.

Result: HGC significantly reduces computational cost (63% MACs reduction on COCO-Stuff benchmark) while maintaining minimal visual quality loss (performance degradation within 1.5%).

Conclusion: The proposed Hybrid-Grained Cache approach effectively balances efficiency and semantic fidelity in generative systems, demonstrating notable computational cost savings and strong performance across benchmarks.

Abstract: Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%.

</details>


### [129] [MPCGNet: A Multiscale Feature Extraction and Progressive Feature Aggregation Network Using Coupling Gates for Polyp Segmentation](https://arxiv.org/abs/2511.11032)
*Wei Wang,Feng Jiang,Xin Wang*

Main category: cs.CV

TL;DR: A novel network called MPCGNet is introduced for colorectal polyp segmentation, improving segmentation quality via three innovative modules.


<details>
  <summary>Details</summary>
Motivation: Current automatic polyp segmentation methods face challenges such as missing small polyps, ambiguous boundaries, and noise from colonoscopy images.

Method: The paper proposes three novel modules: CGMFE for noise suppression, WCAD for restoring details, and DFA for progressive feature aggregation and enhancement.

Result: The MPCGNet model achieves better segmentation performance with mDice scores 2.20% and 0.68% higher compared to the second-best network on benchmark datasets.

Conclusion: MPCGNet provides improved accuracy and robustness in colorectal polyp segmentation, addressing current challenges effectively.

Abstract: Automatic segmentation methods of polyps is crucial for assisting doctors in colorectal polyp screening and cancer diagnosis. Despite the progress made by existing methods, polyp segmentation faces several challenges: (1) small-sized polyps are prone to being missed during identification, (2) the boundaries between polyps and the surrounding environment are often ambiguous, (3) noise in colonoscopy images, caused by uneven lighting and other factors, affects segmentation results. To address these challenges, this paper introduces coupling gates as components in specific modules to filter noise and perform feature importance selection. Three modules are proposed: the coupling gates multiscale feature extraction (CGMFE) module, which effectively extracts local features and suppresses noise; the windows cross attention (WCAD) decoder module, which restores details after capturing the precise location of polyps; and the decoder feature aggregation (DFA) module, which progressively aggregates features, further extracts them, and performs feature importance selection to reduce the loss of small-sized polyps. Experimental results demonstrate that MPCGNet outperforms recent networks, with mDice scores 2.20% and 0.68% higher than the second-best network on the ETIS-LaribPolypDB and CVC-ColonDB datasets, respectively.

</details>


### [130] [CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging](https://arxiv.org/abs/2511.11034)
*Pooja Singh,Siddhant Ujjain,Tapan Kumar Gandhi,Sandeep Kumar*

Main category: cs.CV

TL;DR: The paper introduces CrossMed, a benchmark for assessing compositional generalization in medical multimodal large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating multimodal LLMs' performance on unseen combinations of imaging modality, anatomy, and task type in medical AI.

Method: CrossMed reformulates public medical datasets into a unified visual question answering (VQA) format using a structured Modality-Anatomy-Task schema. It tests multimodal LLMs on related, unrelated, and zero-overlap settings to assess their generalization capabilities.

Result: Models like LLaVA-Vicuna-7B achieve high performance on related settings but struggle with unseen combinations. Cross-task training showed improvement, and traditional models validated the MAT framework.

Conclusion: CrossMed serves as a robust framework to evaluate and improve compositional generalization in medical vision-language models, emphasizing the need for advancements in zero-shot and modality-agnostic performance.

Abstract: Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.

</details>


### [131] [Hyperbolic Hierarchical Alignment Reasoning Network for Text-3D Retrieval](https://arxiv.org/abs/2511.11045)
*Wenrui Li,Yidan Lu,Yeyu Chai,Rui Zhao,Hengyu Man,Xiaopeng Fan*

Main category: cs.CV

TL;DR: This paper addresses two key challenges in text-3D retrieval (HRC and RISD) by introducing the Hyperbolic Hierarchical Alignment Reasoning Network (H$^{2}$ARN) and proposes a set of innovative methods including hyperbolic geometry embedding and a contribution-aware aggregation module.


<details>
  <summary>Details</summary>
Motivation: The rise in online 3D data has heightened the need for efficient text-to-3D retrieval systems, pushing researchers to solve problems like hierarchy representation collapse and saliency dilution.

Method: The proposed H$^{2}$ARN utilizes hyperbolic geometry for embedding and hierarchical ordering with losses, alongside contribution-aware aggregation to tackle semantic dilution and enhance discriminative features.

Result: H$^{2}$ARN demonstrates its effectiveness, surpassing existing approaches in handling both complex hierarchies and redundancy issues in text-3D retrieval. Additionally, the authors expanded the benchmark dataset (T3DR-HIT v2) with 8,935 text-3D pairs.

Conclusion: The paper's techniques significantly enhance text-3D retrieval performance while addressing longstanding problems in hierarchy representation and semantic saliency.

Abstract: With the daily influx of 3D data on the internet, text-3D retrieval has gained increasing attention. However, current methods face two major challenges: Hierarchy Representation Collapse (HRC) and Redundancy-Induced Saliency Dilution (RISD). HRC compresses abstract-to-specific and whole-to-part hierarchies in Euclidean embeddings, while RISD averages noisy fragments, obscuring critical semantic cues and diminishing the model's ability to distinguish hard negatives. To address these challenges, we introduce the Hyperbolic Hierarchical Alignment Reasoning Network (H$^{2}$ARN) for text-3D retrieval. H$^{2}$ARN embeds both text and 3D data in a Lorentz-model hyperbolic space, where exponential volume growth inherently preserves hierarchical distances. A hierarchical ordering loss constructs a shrinking entailment cone around each text vector, ensuring that the matched 3D instance falls within the cone, while an instance-level contrastive loss jointly enforces separation from non-matching samples. To tackle RISD, we propose a contribution-aware hyperbolic aggregation module that leverages Lorentzian distance to assess the relevance of each local feature and applies contribution-weighted aggregation guided by hyperbolic geometry, enhancing discriminative regions while suppressing redundancy without additional supervision. We also release the expanded T3DR-HIT v2 benchmark, which contains 8,935 text-to-3D pairs, 2.6 times the original size, covering both fine-grained cultural artefacts and complex indoor scenes. Our codes are available at https://github.com/liwrui/H2ARN.

</details>


### [132] [PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI](https://arxiv.org/abs/2511.11048)
*Sun Jo,Seok Young Hong,JinHyun Kim,Seungmin Kang,Ahjin Choi,Don-Gwan An,Simon Song,Je Hyeong Hong*

Main category: cs.CV

TL;DR: The paper introduces PINGS-X, a framework using Gaussian splatting techniques to achieve high-resolution flow velocity modeling for 4D flow MRI, addressing prolonged scan and training times.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of high-resolution 4D flow MRI while minimizing the trade-off between acquisition speed and diagnosis accuracy, overcoming the slow and patient-specific training in current PINNs.

Method: The study develops PINGS-X, which includes normalized Gaussian splatting, axes-aligned Gaussians, and Gaussian merging to simplify training, improve accuracy, and avoid computational inefficiencies in MRI super-resolution.

Result: PINGS-X demonstrates significant improvements in training time and super-resolution accuracy on computational fluid dynamics (CFD) and actual 4D flow MRI datasets.

Conclusion: PINGS-X effectively addresses the limitations of existing MRI super-resolution techniques, presenting a practical, fast, and accurate solution for cardiovascular diagnostics.

Abstract: 4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.

</details>


### [133] [NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion](https://arxiv.org/abs/2511.11051)
*Chuheng Chen,Xiaofei Zhou,Geyuan Zhang,Yong Huang*

Main category: cs.CV

TL;DR: The paper introduces NP-LoRA, a projection-based method for improving LoRA fusion by enforcing subspace separation to reduce interference and enhance generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA fusion techniques face interference issues, as separately trained LoRAs occupy overlapping low-rank subspaces, degrading fidelity in controllable generation.

Method: NP-LoRA employs singular value decomposition (SVD) to extract principal style directions and projects subject LoRA into an orthogonal null space. It also introduces soft projection for balancing subject fidelity and style consistency.

Result: NP-LoRA improves fusion quality, surpassing existing baselines in DINO, CLIP metrics, human assessments, and LLM preferences, while enabling broad applicability without retraining.

Conclusion: NP-LoRA provides an effective solution to structural interference in LoRA fusion, enhancing generative quality while maintaining flexibility and compatibility with various backbones and configurations.

Abstract: Low-Rank Adaptation (LoRA) fusion has emerged as a key technique for reusing and composing learned subject and style representations for controllable generation without costly retraining. However, existing methods rely on weight-based merging, where one LoRA often dominates the other, leading to interference and degraded fidelity. This interference is structural: separately trained LoRAs occupy low-rank high-dimensional subspaces, leading to non-orthogonal and overlapping representations. In this work, we analyze the internal structure of LoRAs and find their generative behavior is dominated by a few principal directions in the low-rank subspace, which should remain free from interference during fusion. To achieve this, we propose Null Space Projection LoRA (NP-LoRA), a projection-based framework for LoRA fusion that enforces subspace separation to prevent structural interference among principal directions. Specifically, we first extract principal style directions via singular value decomposition (SVD) and then project the subject LoRA into its orthogonal null space. Furthermore, we introduce a soft projection mechanism that enables smooth control over the trade-off between subject fidelity and style consistency. Experiments show NP-LoRA consistently improves fusion quality over strong baselines (e.g., DINO and CLIP-based metrics, with human and LLM preference scores), and applies broadly across backbones and LoRA pairs without retraining.

</details>


### [134] [CareCom: Generative Image Composition with Calibrated Reference Features](https://arxiv.org/abs/2511.11060)
*Jiaxuan Chen,Bo Zhang,Qingdong He,Jinlong Peng,Li Niu*

Main category: cs.CV

TL;DR: This paper proposes a novel generative image composition method using multi-reference foreground images and feature calibration for improved detail preservation and pose alignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with preserving details and adjusting foreground pose/view in image composition.

Method: The authors extend generative composition models to multi-reference versions and calibrate global and local features of foreground references to align them with background properties.

Result: Extensive experiments on MVImgNet and MureCom datasets show significant improvements in performance due to reference feature calibration.

Conclusion: Feature calibration and multi-reference inputs effectively improve the generative image composition's ability to align foreground with background seamlessly.

Abstract: Image composition aims to seamlessly insert foreground object into background. Despite the huge progress in generative image composition, the existing methods are still struggling with simultaneous detail preservation and foreground pose/view adjustment. To address this issue, we extend the existing generative composition model to multi-reference version, which allows using arbitrary number of foreground reference images. Furthermore, we propose to calibrate the global and local features of foreground reference images to make them compatible with the background information. The calibrated reference features can supplement the original reference features with useful global and local information of proper pose/view. Extensive experiments on MVImgNet and MureCom demonstrate that the generative model can greatly benefit from the calibrated reference features.

</details>


### [135] [LiteAttention: A Temporal Sparse Attention for Diffusion Transformers](https://arxiv.org/abs/2511.11062)
*Dor Shmilovich,Tony Wu,Aviad Dahan,Yuval Domb*

Main category: cs.CV

TL;DR: This paper introduces LiteAttention for video generation, a method that reduces quadratic attention delays in Diffusion Transformers using temporal coherence to eliminate redundant computations, achieving faster performance without quality degradation.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers have high computational costs due to quadratic attention complexities, and current acceleration methods are either computationally expensive or statically suboptimal. The authors aim to address these challenges.

Method: The authors propose LiteAttention, which tracks non-essential tiles through temporal coherence across denoising steps, integrating adaptive skipping in diffusion attention while eliminating overhead from repeated profiling.

Result: LiteAttention achieves significant speedups in production video diffusion models by leveraging optimized kernels like FlashAttention, while maintaining quality.

Conclusion: LiteAttention successfully combines the benefits of adaptive and efficient static methods, optimizing computation in video generation without quality trade-offs, and the proposed implementation will be publicly accessible.

Abstract: Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step $t$ typically remain so at step $t+δ$. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.

</details>


### [136] [From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening](https://arxiv.org/abs/2511.11065)
*Muskaan Chopra,Lorenz Sparrenberg,Armin Berger,Sarthak Khanna,Jan H. Terheyden,Rafet Sifa*

Main category: cs.CV

TL;DR: The paper surveys advances in deep learning for Diabetic Retinopathy (DR) from 2016-2025, highlighting methodologies and challenges in clinical adaptation.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for early detection of Diabetic Retinopathy and synthesize advancements in deep learning methods for better clinical application.

Method: Systematic review of research on DR deep learning techniques, summarizing over 50 studies and 20 datasets, addressing class imbalance, label scarcity, domain shift, interpretability, and reproducibility.

Result: Benchmark tables compare performances across datasets; open challenges in validation and clinical trust are discussed.

Conclusion: The paper outlines a practical agenda for privacy-focused, reproducible, and clinically deployable DR AI, with implications for broad medical imaging applications.

Abstract: Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.

</details>


### [137] [S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation](https://arxiv.org/abs/2511.11066)
*Jiechao Gao,Chang Liu,Yuangang Li*

Main category: cs.CV

TL;DR: The paper introduces S2D-Align, a novel training paradigm for improving radiology report generation by creating anatomically-grounded alignment between images and text through multi-stage guidance, showing state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the quality of automatically generated radiology reports by addressing the limitation of standard fine-tuning methods that fail to incorporate anatomically-grounded alignments.

Method: S2D-Align implements a multi-stage alignment strategy using auxiliary signals, progressing from coarse radiograph-report pairings to instance-level guidance with reference reports, and finally grounding generation with key anatomical phrases. A memory-based adapter bridges different alignment stages.

Result: S2D-Align achieved state-of-the-art results on the MIMIC-CXR and IU X-Ray benchmarks, indicating significant improvement over existing methods in radiology report generation.

Conclusion: S2D-Align is effective in enhancing anatomically-grounded alignment in radiology report generation tasks, paving the way for better multi-modal generation applications.

Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.

</details>


### [138] [Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image](https://arxiv.org/abs/2511.11074)
*Matthias Humt,Ulrich Hillenbrand,Rudolph Triebel*

Main category: cs.CV

TL;DR: This paper compares Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers for 3D generative shape modeling and completion.


<details>
  <summary>Details</summary>
Motivation: To determine the optimal generative model for various 3D data tasks and evaluate the use of partial 3D data as conditional input.

Method: Adapted diffusion models and autoregressive transformers for 3D modeling, conducted quantitative comparisons, baseline modeling, and ablation studies.

Result: Diffusion models outperformed in multi-modal shape completion tasks, but autoregressive models matched or exceeded diffusion on discrete latent spaces.

Conclusion: Diffusion models excel in certain conditions, but both approaches have their strengths depending on the latent space and task specifics.

Abstract: While generative models have seen significant adoption across a wide range of data modalities, including 3D data, a consensus on which model is best suited for which task has yet to be reached. Further, conditional information such as text and images to steer the generation process are frequently employed, whereas others, like partial 3D data, have not been thoroughly evaluated. In this work, we compare two of the most promising generative models--Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers--which we adapt for the tasks of generative shape modeling and completion. We conduct a thorough quantitative evaluation and comparison of both tasks, including a baseline discriminative model and an extensive ablation study. Our results show that (1) the diffusion model with continuous latents outperforms both the discriminative model and the autoregressive approach and delivers state-of-the-art performance on multi-modal shape completion from a single, noisy depth image under realistic conditions and (2) when compared on the same discrete latent space, the autoregressive model can match or exceed diffusion performance on these tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [139] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: A novel AI infrastructure, FengHuang, is introduced to tackle memory and communication scaling challenges in inference workloads of large language models. It reduces GPU reliance while enhancing performance.


<details>
  <summary>Details</summary>
Motivation: To overcome scalability challenges in GPU-centric architectures for AI inference workloads, especially limitations related to memory, bandwidth, and communication scaling.

Method: The FengHuang Platform employs a disaggregated multi-tier shared-memory architecture, tensor paging, and near-memory compute to optimize AI inference, validated through simulations.

Result: The platform achieves up to 93% reduction in local memory usage, 50% GPU compute savings, and significant improvements in inter-GPU communication speed (16x to 70x) across various AI workloads.

Conclusion: FengHuang offers scalable, cost-effective AI infrastructure with vendor-flexible design and substantial reductions in GPU dependency, infrastructure cost, and energy usage.

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [140] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: The paper presents HPCAgentTester, a framework leveraging multi-agent LLMs for generating robust unit tests targeting HPC software challenges like parallelism and non-determinism.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of unit testing in HPC caused by parallelism, complex algorithms, and diverse hardware, where traditional methods fail to handle synchronization and non-deterministic issues.

Method: HPCAgentTester uses a multi-agent LLM framework with two specialized agents (Recipe Agent and Test Agent) operating iteratively in a critique loop to create and refine context-aware unit tests for OpenMP and MPI primitives.

Result: HPCAgentTester produces compilable and functionally correct unit tests, detecting subtle bugs missed by conventional methods, and improves test correctness and compilation rates when compared to standalone LLMs.

Conclusion: HPCAgentTester provides a robust, scalable solution for enhancing the reliability of HPC software by automating and improving the generation of parallel execution-focused unit tests.

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


### [141] [UFO$^3$: Weaving the Digital Agent Galaxy](https://arxiv.org/abs/2511.11332)
*Chaoyun Zhang,Liqun Li,He Huang,Chiming Ni,Bo Qiao,Si Qin,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.DC

TL;DR: UFO$^3$ enables seamless cross-device task orchestration, transforming isolated devices into a unified, intelligent fabric for efficient task execution.


<details>
  <summary>Details</summary>
Motivation: Current systems struggle with cross-device workflows due to being restricted to single OS or device ecosystems, limiting collaboration and efficiency.

Method: UFO$^3$ models user requests as mutable distributed DAGs of subtasks, managed by a Constellation Orchestrator and a low-latency Agent Interaction Protocol for task execution and optimization.

Result: UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, reduces latency by 31%, and demonstrates fault tolerance during experiments.

Conclusion: UFO$^3$ effectively facilitates accurate, efficient, and resilient task orchestration by unifying diverse devices and enabling adaptive intelligence across computing environments.

Abstract: Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.
  We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.

</details>


### [142] [Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster](https://arxiv.org/abs/2511.11542)
*Tomas Oppelstrup,Nicholas Giamblanco,Delyan Z. Kalchev,Ilya Sharapov,Mark Taylor,Dirk Van Essendelft,Sivasankaran Rajamanickam,Michael James*

Main category: cs.DC

TL;DR: The paper introduces a novel algorithm to enhance simulation rates and computing utilization, achieving high performance in networked environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of low simulation efficiency and performance in Exascale systems and network computing environments during large-scale physical simulations.

Method: A new algorithm is developed to improve simulation rates and computational utilization, achieving scalability and near-peak performance in both single-node and clustered environments.

Result: Simulations run at 1.6 million time-steps per second and achieve 84 PFLOPS, with 90% peak performance on 64 Cerebras CS-3 systems at planetary scale.

Conclusion: The proposed algorithm successfully enhances both simulation rates and computational efficiency, proving its utility in high-performance simulations.

Abstract: Simulation of physical systems is essential in many scientific and engineering domains. Commonly used domain decomposition methods are unable to deliver high simulation rate or high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel \algorithmpropernoun{} algorithm, designed to overcome these limitations. We apply this method and show simulations running in excess of 1.6 million time steps per second and simulations achieving 84 PFLOP/s. Our implementation can achieve 90\% of peak performance in both single-node and clustered environments. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale running on a cluster of 64 Cerebras CS-3 systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [143] [LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices](https://arxiv.org/abs/2511.10680)
*Jean-Philippe Lignier*

Main category: cs.LG

TL;DR: The paper introduces LAD-BNet, a neural network architecture for real-time energy forecasting on edge devices. It achieves high performance, low inference time, and supports embedded constraints.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of real-time and accurate energy forecasting on edge devices with limited computational resources.

Method: Introduces LAD-BNet, a hybrid neural architecture combining lag-aware branches and Temporal Convolutional Networks (TCN) for optimized inference on Google Coral TPU. It captures both short and long-term dependencies while ensuring reduced computation time.

Result: The model achieves 14.49% mean absolute percentage error (MAPE) at a 1-hour prediction horizon with an 18ms inference time, outperforming LSTM and pure TCN architectures with improved speed (8-12 x faster) and memory efficiency (180MB footprint).

Conclusion: LAD-BNet is a highly efficient and practical solution for real-time energy forecasting on edge devices, suitable for industrial applications like energy optimization and operational planning.

Abstract: Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.

</details>


### [144] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: The paper addresses performance trade-offs in long-tailed data distributions when using parameter-efficient fine-tuning methods, and proposes LT-Soups to balance head and tail accuracy effectively.


<details>
  <summary>Details</summary>
Motivation: The need to mitigate challenges in performance biases associated with long-tailed data distributions, particularly in underrepresented tail classes compared to dominant head classes.

Method: The authors propose LT-Soups, a two-stage model averaging framework where models fine-tuned on balanced subsets are averaged, followed by fine-tuning the classifier on the full dataset.

Result: Experiments demonstrate LT-Soups offers superior head-tail performance trade-offs across various imbalance regimes compared to existing methods.

Conclusion: LT-Soups successfully balances head and tail class accuracy in long-tailed distributions, addressing limitations of existing fine-tuning methods.

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [145] [Differentiable Sparse Identification of Lagrangian Dynamics](https://arxiv.org/abs/2511.10706)
*Zitong Zhang,Hao Sun*

Main category: cs.LG

TL;DR: This paper introduces a differentiable sparse identification framework utilizing B-Splines for improved equation discovery in nonlinear dynamics.


<details>
  <summary>Details</summary>
Motivation: Current sparse regression techniques struggle with rational functions and sensitivity to noise in complex mechanical systems. The Lagrangian formalism offers potential but faces challenges with noise and limited data.

Method: The framework integrates cubic B-Splines for nonlinear representation, robust equation discovery with physical constraints, and recursive derivative computation to handle higher-order derivatives and noise.

Result: The proposed method outperforms baseline approaches, achieving accurate and reliable extraction of physical laws from noisy data.

Conclusion: The novel approach enhances system identification in nonlinear dynamics, particularly in noisy and complex environments, advancing the field of data-driven discovery.

Abstract: Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.

</details>


### [146] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: The paper enhances Representation FineTuning (ReFT) for mathematical reasoning tasks using a method called BREP ReFT, which addresses limitations in reasoning prefix generation and numerical encoding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency and effectiveness of fine-tuning models for downstream mathematical reasoning tasks while addressing the performance challenges of ReFT.

Method: It proposes BREP ReFT, which optimizes reasoning prefix generation, intervenes in the early inference stage, and limits intervention vectors' magnitude to avoid numerical encoding disruption.

Result: BREP ReFT demonstrates improved mathematical reasoning performance, efficiency, and robust generalization across model architectures, outperforming ReFT and PEFT methods.

Conclusion: BREP ReFT successfully resolves ReFT's performance limitations in mathematical tasks, indicating its effectiveness and potential as a superior approach. The implementation is publicly available.

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [147] [EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence](https://arxiv.org/abs/2511.10834)
*Ansel Kaplan Erol,Seungjun Lee,Divya Mahajan*

Main category: cs.LG

TL;DR: EarthSight enhances satellite image processing by distributing tasks between satellites and ground stations for faster and more efficient operations.


<details>
  <summary>Details</summary>
Motivation: Traditional satellite image delivery introduces delays due to bandwidth limitations and isolated computation, impacting time-sensitive applications such as disaster response.

Method: EarthSight employs shared computation for multiple vision tasks, prioritizes image transmissions using a ground-station scheduler, and rejects non-essential images dynamically.

Result: The system improves efficiency, reducing image compute time by 1.9x and cutting delivery latency significantly—from 51 to 21 minutes in evaluations.

Conclusion: EarthSight aligns satellite processing with real-world demands by redefining image analysis as a scalable, resource-efficient distributed task.

Abstract: Low-latency delivery of satellite imagery is essential for time-critical applications such as disaster response, intelligence, and infrastructure monitoring. However, traditional pipelines rely on downlinking all captured images before analysis, introducing delays of hours to days due to restricted communication bandwidth. To address these bottlenecks, emerging systems perform onboard machine learning to prioritize which images to transmit. However, these solutions typically treat each satellite as an isolated compute node, limiting scalability and efficiency. Redundant inference across satellites and tasks further strains onboard power and compute costs, constraining mission scope and responsiveness. We present EarthSight, a distributed runtime framework that redefines satellite image intelligence as a distributed decision problem between orbit and ground. EarthSight introduces three core innovations: (1) multi-task inference on satellites using shared backbones to amortize computation across multiple vision tasks; (2) a ground-station query scheduler that aggregates user requests, predicts priorities, and assigns compute budgets to incoming imagery; and (3) dynamic filter ordering, which integrates model selectivity, accuracy, and execution cost to reject low-value images early and conserve resources. EarthSight leverages global context from ground stations and resource-aware adaptive decisions in orbit to enable constellations to perform scalable, low-latency image analysis within strict downlink bandwidth and onboard power budgets. Evaluations using a prior established satellite simulator show that EarthSight reduces average compute time per image by 1.9x and lowers 90th percentile end-to-end latency from first contact to delivery from 51 to 21 minutes compared to the state-of-the-art baseline.

</details>


### [148] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: The paper discusses the importance of incorporating uncertainty quantification into the evaluation of generative models and proposes ensemble-based precision-recall curves for analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the current lack of uncertainty quantification in evaluating generative models, as existing methods focus solely on distribution closeness without considering inherent uncertainties in those measurements.

Method: The authors propose using ensemble-based precision-recall curves to quantify uncertainty in the approximation capabilities of generative models.

Result: Preliminary experiments on synthetic datasets showcased how aggregated precision-recall curves effectively capture uncertainty and enabled systematic comparisons of model architectures.

Conclusion: The study highlights the need for uncertainty quantification in generative model evaluation and suggests ensemble-based approaches as a promising direction for future research.

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [149] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: The paper proposes and analyzes a variant of the cascade bandit model for edge inference, evaluating four decision-making policies. Adaptive strategies (LCB and Thompson Sampling) achieve constant regret, outperforming non-adaptive ones (Explore-then-Commit and Action Elimination).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in edge inference by evaluating decision-making policies in a cascade bandit model to improve adaptivity and reduce regret under uncertainty.

Method: The paper analyzes Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling policies, deriving theoretical regret guarantees and conducting simulations.

Result: Adaptive policies (LCB and Thompson Sampling) exhibit constant O(1) regret, highlighting their superiority over non-adaptive ones, which show suboptimal performance due to limited flexibility.

Conclusion: Adaptivity is key for efficient edge inference, as LCB and Thompson Sampling demonstrate robust performance by continuously updating decisions based on feedback, unlike fixed-ordering policies.

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [150] [Movement-Specific Analysis for FIM Score Classification Using Spatio-Temporal Deep Learning](https://arxiv.org/abs/2511.10713)
*Jun Masaki,Ariaki Higashi,Naoko Shinagawa,Kazuhiko Hirata,Yuichi Kurita,Akira Furui*

Main category: cs.LG

TL;DR: The study proposes an automated FIM score estimation using a deep neural network to replace traditional burdensome assessments.


<details>
  <summary>Details</summary>
Motivation: To alleviate the significant burden traditional FIM assessments place on patients and healthcare professionals.

Method: The study utilizes a deep neural network integrating ST-GCN, BiLSTM, and attention mechanisms to predict FIM scores using motion data from rehabilitation patients.

Result: The method achieved balanced accuracies between 70.09-78.79% in distinguishing independent patients from those needing assistance.

Conclusion: The automated method not only reduces traditional assessment burdens but also identifies reliable movement patterns as predictors for FIM evaluation items.

Abstract: The functional independence measure (FIM) is widely used to evaluate patients' physical independence in activities of daily living. However, traditional FIM assessment imposes a significant burden on both patients and healthcare professionals. To address this challenge, we propose an automated FIM score estimation method that utilizes simple exercises different from the designated FIM assessment actions. Our approach employs a deep neural network architecture integrating a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to estimate FIM motor item scores. The model effectively captures long-term temporal dependencies and identifies key body-joint contributions through learned attention weights. We evaluated our method in a study of 277 rehabilitation patients, focusing on FIM transfer and locomotion items. Our approach successfully distinguishes between completely independent patients and those requiring assistance, achieving balanced accuracies of 70.09-78.79 % across different FIM items. Additionally, our analysis reveals specific movement patterns that serve as reliable predictors for particular FIM evaluation items.

</details>


### [151] [Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation](https://arxiv.org/abs/2511.10796)
*James Hazelden*

Main category: cs.LG

TL;DR: The paper proposes a matrix-free technique using trace estimation to efficiently analyze the Neural Tangent Kernel (NTK), enabling rapid computations such as trace, rank, and alignment, and offering significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Understanding the Neural Tangent Kernel (NTK) is critical for analyzing how models evolve under Gradient Descent, but computational challenges make analysis difficult, especially for recurrent architectures.

Method: Utilization of matrix-free approaches like the Hutch++ trace estimator and development of a one-sided estimator for NTK analysis, leveraging automatic differentiation techniques.

Result: The proposed methods achieve rapid and accurate NTK analysis, significantly improving computational efficiency while outperforming existing approaches in low-sample regimes.

Conclusion: Matrix-free randomized processes enable efficient NTK computations, offering substantial speedups and broader applicability in gradient-based model analysis.

Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.

</details>


### [152] [SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems](https://arxiv.org/abs/2511.11111)
*Xin Wang,Pietro Lodi Rizzini,Sourav Medya,Zhiling Lan*

Main category: cs.LG

TL;DR: This paper presents \ourmodel, a surrogate model combining Graph Neural Networks and Large Language Models for efficient runtime prediction and hybrid simulation of Dragonfly networks.


<details>
  <summary>Details</summary>
Motivation: Workload interference in Dragonfly networks impacts performance, and current simulation methods like PDES are computationally expensive.

Method: \ourmodel integrates Graph Neural Networks and Large Language Models to analyze spatial and temporal patterns from router data.

Result: \ourmodel achieves superior performance in runtime prediction compared to existing statistical and machine learning methods.

Conclusion: \ourmodel offers a practical solution for hybrid simulation, addressing workload interference in Dragonfly networks efficiently.

Abstract: The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.

</details>


### [153] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: The paper introduces public-data-assisted zeroth-order optimization (PAZO) to improve the utility and efficiency of privacy-preserving machine learning compared to traditional methods like DP-SGD.


<details>
  <summary>Details</summary>
Motivation: Deploying first-order DP machine learning algorithms like DP-SGD is costly in terms of computation and memory, prompting the exploration of less resource-intensive alternatives.

Method: The proposed PAZO framework utilizes public data to enhance gradient approximations in private zeroth-order optimization with minimal overhead and performs theoretical analyses under the assumption of similarity between public and private data.

Result: PAZO achieves better privacy/utility tradeoffs across various tasks, outperforms first-order baselines in highly private regimes, and provides up to 16× runtime speedup.

Conclusion: PAZO demonstrates the potential to offer efficient and effective privacy-preserving optimization, making it suitable for highly private machine learning tasks.

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [154] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: The paper addresses efficiency challenges in Linear Predictive Clustering (LPC) and proposes two novel optimization methods to improve scalability while maintaining near-optimal solutions.


<details>
  <summary>Details</summary>
Motivation: To enhance the global optimization efficiency in LPC, which suffers from scalability issues in non-separable cluster settings using current greedy or MIP-based methods.

Method: Introduced two novel approaches leveraging separability properties to simplify MIP complexity and approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving scalable, near-optimal solutions.

Result: The proposed methods deliver near-optimal clustering solutions with lower regression errors and improved scalability compared to traditional greedy and MIP-based approaches.

Conclusion: The new optimization strategies address LPC's scalability bottleneck while retaining accuracy, making them suitable for practical applications.

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [155] [A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication](https://arxiv.org/abs/2511.11560)
*Angelo Rodio,Giovanni Neglia,Zheng Chen,Erik G. Larsson*

Main category: cs.LG

TL;DR: The paper analyzes two strategies, S2S and S2A, in semi-decentralized federated learning, comparing their performance based on system parameters and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical and empirical comparison between sampled-to-sampled (S2S) and sampled-to-all (S2A) strategies in semi-decentralized federated learning.

Method: Developing a unified convergence framework to analyze S2S and S2A strategies, while considering system parameters such as sampling rate, server aggregation frequency, and network connectivity.

Result: The study shows different regimes where one strategy outperforms the other, primarily influenced by the degree of data heterogeneity across devices.

Conclusion: The findings provide practical design guidelines for optimizing semi-decentralized federated learning deployments.

Abstract: In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.

</details>


### [156] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: The paper studies transformer models predicting complex steps in the Collatz sequence, finding that their accuracy depends on numerical encoding and identifying learning patterns and limitations.


<details>
  <summary>Details</summary>
Motivation: The aim is to understand how transformer models learn and predict complex arithmetic functions, like long Collatz steps, and to explore their mechanisms of failure and accuracy.

Method: The authors investigate model accuracy using different numeral bases, analyze learning patterns, and study model outputs and failure cases using the Collatz sequence as a mathematical problem.

Result: Transformers achieve high accuracy in some numeral systems (up to 99.7%) but struggle significantly in others. Models learn input classes based on binary properties but often fail due to incorrect loop length predictions.

Conclusion: The study provides insight into transformer learning algorithms, concluding that models struggle with control structures (loop lengths). It suggests using mathematical problems to better understand and improve language models.

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [157] [Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters](https://arxiv.org/abs/2511.10898)
*Chenghao Duan,Chuanyi Ji*

Main category: cs.LG

TL;DR: The paper introduces a novel Graph Attention Network (GAT) to predict severe weather-induced power outage duration, achieving over 93% accuracy using real-world data.


<details>
  <summary>Details</summary>
Motivation: Natural disasters cause significant power outages with severe economic and societal impacts, necessitating accurate prediction methods for power outage recovery to improve grid resilience.

Method: The authors propose a Graph Attention Network (GAT) approach with unsupervised pre-training followed by semi-supervised learning to estimate power outage durations caused by severe weather events.

Result: Their model demonstrates superior performance of over 93% accuracy and significantly outperforms conventional methods like XGBoost, Random Forest, and simpler GAT designs with improvements ranging from 2% to 15%.

Conclusion: The findings highlight the potential of GAT-based models for accurate and efficient power outage duration prediction, addressing real-world challenges in spatial data dependencies and heterogeneous impacts.

Abstract: Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\% - 15\%$ in both the overall performance and class-wise accuracy.

</details>


### [158] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: The paper explores transformer-based neural operators' abilities in transfer learning on diverse PDE problems, showing effective knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Training neural operators for data-driven physical simulations is computationally expensive, motivating the need for strategies like transfer learning.

Method: Investigates transformer-based neural operators under transfer learning settings across various PDE problems, evaluating capabilities like extrapolation, incorporating new variables, and transferring from multi-equation datasets.

Result: Transformer-based neural operators proved effective in transferring knowledge across diverse PDE problems.

Conclusion: Advanced transformer-based neural operator architectures demonstrate capability in knowledge transfer across physical problems, broadening their applicability.

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [159] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: This paper explores quantum kernel methods for real-world and high-dimensional datasets, showing their potential over classical kernels like RBF.


<details>
  <summary>Details</summary>
Motivation: Quantum kernel methods have not been thoroughly tested on diverse, high-dimensional, real-world datasets, limiting validation of their potential advantages.

Method: A variational quantum kernel framework with resource-efficient ansätze and a novel parameter scaling technique was developed and benchmarked on eight high-dimensional datasets.

Result: Classically simulated results demonstrate a clear performance advantage of the proposed quantum kernel over classical RBF kernels.

Conclusion: Quantum kernels, when properly designed, can serve as versatile, powerful tools for real-world machine learning applications, though more research is needed to validate practical quantum advantage.

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [160] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: The paper simplifies the proof of the Tsallis-INF algorithm's performance guarantees using modern online convex optimization techniques.


<details>
  <summary>Details</summary>
Motivation: The authors aim to provide an accessible proof of the "best-of-both-world" guarantee for the Tsallis-INF, avoiding complex mathematical constructs like conjugate functions.

Method: The derivation eliminates the need for conjugate functions and instead leverages online convex optimization tools. It prioritizes simplicity over optimizing constants in bounds.

Result: Successfully reproduces the guarantee for Tsallis-INF in a concise and streamlined manner using simplified techniques.

Conclusion: The simplified derivation enhances accessibility, presenting a clear proof without compromising the validity of results, though at the expense of optimized constants.

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [161] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: This paper presents SurfaceBench, a new benchmark designed for symbolic surface discovery tasks using machine-learning methods, improving equation discovery by using scientific domains and geometry-aware metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing symbolic regression benchmarks, which focus on scalar functions, ignore scientific grounding, and use metrics that don't evaluate geometric accuracy.

Method: The authors created SurfaceBench, consisting of 183 tasks across diverse categories, incorporating three-dimensional data, ground-truth equations, and novel metrics like Chamfer and Hausdorff distances for evaluation.

Result: The study demonstrates that current state-of-the-art methods struggle with generalizing across diverse surface complexities and representation types, highlighting the benchmark's diagnostic value.

Conclusion: SurfaceBench provides a rigorous benchmark for testing progress in symbolic reasoning and geometry-aware scientific induction, encouraging advancements in generalized and accurate machine-learning models for equation discovery.

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [162] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: This paper addresses hallucinations in Large Language Models (LLMs) using a refined evaluation framework and novel attention-based quantification strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of hallucination in LLMs, particularly by distinguishing between different types (extrinsic vs. intrinsic) and improving detection strategies.

Method: The paper introduces a principled framework differentiating hallucination types and proposes attention-based uncertainty quantification with novel aggregation strategies.

Result: Experiments show sampling-based methods detect extrinsic hallucinations effectively but fail at intrinsic ones. Their attention aggregation approach performs better for intrinsic hallucinations.

Conclusion: Attention mechanisms can enhance hallucination detection and model uncertainty estimation, providing a better alignment with the hallucination type for improved results.

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [163] [Multicalibration yields better matchings](https://arxiv.org/abs/2511.11413)
*Riccardo Colini Baldeschi,Simone Di Gregorio,Simone Fioravanti,Federico Fusco,Ido Guy,Daniel Haimovich,Stefano Leonardi,Fridolin Linder,Lorenzo Perini,Matteo Russo,Niek Tax*

Main category: cs.LG

TL;DR: The paper addresses the issue of finding the best matching in a weighted graph using predictions, proposing multicalibration to improve prediction bias and decision competitiveness.


<details>
  <summary>Details</summary>
Motivation: To improve decision-making in scenarios where predictions of stochastic weights in graphs are imperfect and standard methods may underperform.

Method: Introduces multicalibration to ensure predictor unbiasedness on protected sets of contexts. Constructs a specific multicalibrated predictor, ensuring competitive decision-making.

Result: Demonstrates that the best matching based on multicalibrated predictions is competitive with optimal decision rules. Provides sample complexity bounds to support the methodology.

Conclusion: Multicalibration enhances prediction fairness and competitive decision-making, offering a practical approach to imperfect prediction scenarios.

Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.

</details>


### [164] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: The paper introduces FlowPath, a method for improving time series analysis by learning the geometry of control paths, which outperforms baseline methods in classification tasks, especially under conditions of high missingness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of modeling continuous-time dynamics from sparse and irregularly-sampled time series, as previous fixed interpolation methods fail under high missingness due to simplistic geometric assumptions.

Method: FlowPath uses an invertible neural flow to learn a continuous and data-adaptive manifold for control paths, diverging from previous unconstrained or fixed interpolation methods.

Result: FlowPath shows statistically significant improvements in classification accuracy across 18 benchmarks and a real-world case study compared to fixed interpolants or non-invertible approaches.

Conclusion: The results emphasize the importance of modeling both dynamics and geometric path structures for robust, generalizable learning from irregular time series.

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [165] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: This paper addresses issues of high variance and low sample efficiency in reinforcement learning by employing behavior policies that facilitate lower-variance return estimates and proving their effectiveness in policy-gradient methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve challenges of poor sample efficiency and training instability in reinforcement learning caused by high-variance return estimates when using return-based policy improvement.

Method: The authors utilize behavior policies for data collection to achieve provably lower-variance return estimates, extending this method to the online reinforcement learning scenario where evaluation and improvement are combined.

Result: The approach was applied to two policy-gradient methods, achieving improved sample efficiency and performance across various environments.

Conclusion: Using behavior policies with lower variance return estimates enhances the efficiency and outcomes of reinforcement learning methods.

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [166] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: This study introduces the Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), a novel technique for using general time series models (TSFMs) for EEG-specific tasks and evaluates it on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding the comparative performance of general time series foundation models (TSFMs) and EEG-specific foundation models (EEGFMs) for EEG-specific tasks.

Method: Developed the STAMP module that works with general TSFMs by utilizing univariate embeddings and modeling the spatial-temporal aspects of EEG data.

Result: The proposed STAMP achieves performance comparable to state-of-the-art EEG-specific foundation models on 8 clinical EEG benchmark datasets.

Conclusion: STAMP is a lightweight, flexible addition that makes TSFMs viable for EEG-related tasks with competitive performance against dedicated EEG models.

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [167] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: The paper introduces ExPairT-LLM, an algorithm for code selection using pairwise membership and equivalence queries to identify the best program, significantly improving success rates compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms for code generation struggle due to their reliance on LLMs and limitations in identifying equivalent programs accurately.

Method: ExPairT-LLM uses simpler pairwise membership and equivalence queries posed to an LLM oracle to select the correct program effectively through a tournament-style approach.

Result: ExPairT-LLM outperforms current algorithms with an average success rate improvement of +13.0% and up to +27.1% on datasets, enhancing LLM performance by +24.0% in complex reasoning tasks.

Conclusion: ExPairT-LLM demonstrates robust and improved code selection performance, providing a novel approach that mitigates common limitations of existing techniques.

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [168] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: The paper introduces GO UT Bench, a dataset of Golang code and unit tests, to address training data imbalance in code LLMs, improving their performance on software engineering tasks.


<details>
  <summary>Details</summary>
Motivation: To address the issue of training data imbalance in code LLMs, where raw code is overrepresented compared to broader software engineering tasks, especially for low-resource languages like Golang.

Method: The paper presents GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests from 10 permissively licensed Golang repositories. It evaluates its effectiveness through fine-tuning LLMs across two architectures: mixture of experts and dense decoders.

Result: Finetuned models using GO UT Bench dataset outperformed their base versions on over 75% of benchmark tasks, showing the utility of the dataset.

Conclusion: The study concludes the importance of balanced and diversified datasets like GO UT Bench for enhancing code LLMs beyond code autocompletion to broader software engineering workflows.

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [169] [Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations](https://arxiv.org/abs/2511.10872)
*Shuyuan Zhang,Zihan Wang,Xiao-Wen Chang,Doina Precup*

Main category: cs.LG

TL;DR: The paper proposes a method called G4RL, integrating graph encoder-decoder into Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) to improve performance by addressing inefficiencies in subgoal representation.


<details>
  <summary>Details</summary>
Motivation: Current GCHRL methods suffer from sample inefficiency and poor subgoal representation while relying on domain-specific or dynamically created graphs, which limits adaptability and utilization in new tasks.

Method: The authors develop a graph encoder-decoder mechanism to evaluate unseen states, enabling better subgoal representation and leveraging intrinsic rewards during exploration.

Result: Empirical results demonstrate improved performance for GCHRL methods in both dense and sparse reward settings, with minimal computational overhead.

Conclusion: G4RL enhances GCHRL effectiveness by leveraging graph-based structures to improve intrinsic rewards and subgoal generation, especially in symmetric and reversible transition environments.

Abstract: The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.

</details>


### [170] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: The paper introduces a physics-informed deep learning model (PI-MJCA-BiGRU) for simultaneously estimating muscle activations and forces across multi-joint systems using kinematics, achieving efficiency and physiological consistency without requiring labeled data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of conventional methods, which are computationally expensive and lack high-quality labeled datasets for multi-joint systems, thus hindering applications in clinical assessments and assistive devices.

Method: The paper proposes a deep learning framework utilizing a Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Units (BiGRUs). This integrates kinematics, inter-joint coupling, and external force interactions into a physics-informed loss function for efficient muscle activation and force estimation.

Result: Experiments on two datasets show that PI-MJCA-BiGRU achieves comparable performance to supervised methods without labeled data. It also outperforms baseline architectures in modeling inter-joint coordination.

Conclusion: The model demonstrates an effective solution for estimating muscle activations and forces efficiently and accurately, providing potential benefits for applications in healthcare and assistive technologies.

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [171] [Multi-View Polymer Representations for the Open Polymer Prediction](https://arxiv.org/abs/2511.10893)
*Wonjin Jung,Yongseok Choi*

Main category: cs.LG

TL;DR: The paper focuses on using a multi-view approach for polymer property prediction, combining multiple data representations and achieving top performance in a NeurIPS 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting polymer properties is crucial for material science and requires utilizing complementary data representation methods.

Method: Four representation methods are integrated: tabular RDKit/Morgan descriptors, graph neural networks, 3D-informed representations, and pretrained SMILES models, combined using a uniform ensemble approach.

Result: The proposed system ranks 9th out of 2241 teams in the NeurIPS 2025 Open Polymer Prediction Challenge with MAE values of 0.057 (public) and 0.082 (private).

Conclusion: A multi-view system leveraging diverse representations can significantly enhance polymer property prediction accuracy, as demonstrated by its success in a competitive challenge.

Abstract: We address polymer property prediction with a multi-view design that exploits complementary representations. Our system integrates four families: (i) tabular RDKit/Morgan descriptors, (ii) graph neural networks, (iii) 3D-informed representations, and (iv) pretrained SMILES language models, and averages per-property predictions via a uniform ensemble. Models are trained with 10-fold splits and evaluated with SMILES test-time augmentation. The approach ranks 9th of 2241 teams in the Open Polymer Prediction Challenge at NeurIPS 2025. The submitted ensemble achieves a public MAE of 0.057 and a private MAE of 0.082.

</details>


### [172] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: The paper introduces SPP-FGC, a federated clustering algorithm that uses private structural graphs for privacy-preserving knowledge sharing, achieving superior performance without compromising data privacy.


<details>
  <summary>Details</summary>
Motivation: To resolve the trade-off between performance and privacy in federated clustering methods, which are challenged by either risking data leakage or losing model accuracy.

Method: Proposes Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), utilizing private structural graphs on clients and aggregated global graphs on the server for unified clustering.

Result: SPP-FGC achieves up to 10% improvement in clustering accuracy (measured by NMI) compared to federated baselines and offers provable privacy guarantees.

Conclusion: SPP-FGC provides a powerful and efficient solution for federated clustering, balancing privacy and performance, with adaptability for various data types and use cases.

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [173] [GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning](https://arxiv.org/abs/2511.10936)
*Ying Song,Balaji Palanisamy*

Main category: cs.LG

TL;DR: The paper introduces GraphToxin, a novel attack technique that undermines graph unlearning by reconstructing deleted data effectively and highlights vulnerabilities in existing defenses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerabilities in graph unlearning, particularly its inability to completely erase sensitive data and resist attacks in multi-party settings.

Method: The proposed method, GraphToxin, utilizes a curvature matching module for precise reconstructive guidance and provides evaluations under random and worst-case node removals in white-box and black-box settings.

Result: GraphToxin successfully recovers deleted data and personal links with high accuracy, demonstrating ineffectiveness of current defenses and their amplified risks when facing such attacks.

Conclusion: GraphToxin exposes severe privacy risks in graph unlearning systems, emphasizing the urgent need for developing stronger defense mechanisms against such reconstruction attacks.

Abstract: Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.

</details>


### [174] [Flow matching-based generative models for MIMO channel estimation](https://arxiv.org/abs/2511.10941)
*Wenkai Liu,Nan Ma,Jianqiao Chen,Xiaoxuan Qi,Yuhang Ma*

Main category: cs.LG

TL;DR: A novel flow matching-based generative model for MIMO channel estimation is proposed to address the slow sampling speeds in DM-based methods. It achieves improved speed and accuracy in channel estimation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the slow sampling speed challenge in diffusion model-based channel estimation for precise acquisition of channel state information.

Method: The method involves formulating channel estimation in a flow matching framework, utilizing a derived velocity field dependent on noise statistics to guide model training and fast ODE-based sampling for channel estimation.

Result: Numerical results show significantly reduced sampling overhead and superior estimation accuracy compared to DM-based methods like score-matching.

Conclusion: The FM-based scheme effectively resolves sampling speed and accuracy challenges, making it a promising approach for MIMO channel estimation.

Abstract: Diffusion model (DM)-based channel estimation, which generates channel samples via a posteriori sampling stepwise with denoising process, has shown potential in high-precision channel state information (CSI) acquisition. However, slow sampling speed is an essential challenge for recent developed DM-based schemes. To alleviate this problem, we propose a novel flow matching (FM)-based generative model for multiple-input multiple-output (MIMO) channel estimation. We first formulate the channel estimation problem within FM framework, where the conditional probability path is constructed from the noisy channel distribution to the true channel distribution. In this case, the path evolves along the straight-line trajectory at a constant speed. Then, guided by this, we derive the velocity field that depends solely on the noise statistics to guide generative models training. Furthermore, during the sampling phase, we utilize the trained velocity field as prior information for channel estimation, which allows for quick and reliable noise channel enhancement via ordinary differential equation (ODE) Euler solver. Finally, numerical results demonstrate that the proposed FM-based channel estimation scheme can significantly reduce the sampling overhead compared to other popular DM-based schemes, such as the score matching (SM)-based scheme. Meanwhile, it achieves superior channel estimation accuracy under different channel conditions.

</details>


### [175] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: The paper introduces a novel approach to controllable model merging that uses a direct linear correction method, eliminating complex offline optimization.


<details>
  <summary>Details</summary>
Motivation: To address challenges in parameter interference and the inefficiency of existing multi-task model merging methods that require complex offline optimization.

Method: A direct correction of the model's final representation modeled as an optimal linear transformation, providing a closed-form solution for generating Pareto-optimal models on-the-fly.

Result: The proposed approach generates a superior Pareto front with more accurate preference alignment, while significantly reducing computational cost.

Conclusion: The method offers a practical and efficient solution for controllable model merging, simplifying computations and improving preference-aware multitask model performance.

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [176] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: This study examines the impact of data quality issues on the performance of various ML models for credit risk evaluation.


<details>
  <summary>Details</summary>
Motivation: Understanding how data imperfections affect ML model effectiveness in credit risk assessment and providing insights for improving robustness.

Method: Controlled data corruption experiments were performed on an open-source dataset using the Pucktrick library with 10 common ML models evaluated.

Result: Significant differences in model robustness were observed due to varying types and severity of data issues.

Conclusion: The methodology and tools enhance data robustness, aiding practitioners in improving pipelines and researchers in further experimentation on data-centric AI.

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [177] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: This paper introduces the Unsurvised Robust Domain Adaptation (URDA) paradigm addressing robustness under adversarial attacks in UDA, proposing a novel algorithm, DART, which improves robustness without sacrificing adaptability.


<details>
  <summary>Details</summary>
Motivation: To improve Unsupervised Domain Adaptation (UDA) methods, which lack robustness against adversarial attacks, while maintaining their transferability across domains.

Method: The URDA paradigm is proposed alongside the theory of generalization bounds under adversarial conditions. A new algorithm, DART, is developed, employing disentangled distillation post-training after regular UDA model pretraining.

Result: Experiments on four datasets show that DART boosts model robustness against adversarial attacks while preserving domain adaptability.

Conclusion: The paper establishes the URDA paradigm and demonstrates its effectiveness with the DART algorithm, providing a robust and transferable approach to UDA.

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [178] [Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing](https://arxiv.org/abs/2511.11046)
*Brian Godwin Lim*

Main category: cs.LG

TL;DR: The paper introduces the NCMP framework to enhance classical GNNs by incorporating broader neighborhood contextualization, proposing SINC-GCN as a practical method.


<details>
  <summary>Details</summary>
Motivation: Improve classical GNNs by addressing their limitation in capturing rich contextual information within broader local neighborhoods.

Method: Formalizes neighborhood-contextualization, integrates this concept into the NCMP framework, and proposes the Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN).

Result: Analysis on synthetic binary node classification shows increased expressivity and efficiency for the proposed architecture.

Conclusion: The NCMP framework and SINC-GCN provide a novel, efficient approach to enhancing graph representational power in classical GNNs.

Abstract: Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. In the literature, classical GNNs may be classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is highly expressive, its typical pair-wise messages nevertheless only consider the features of the center node and each neighboring node individually. This design fails to incorporate the rich contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). A preliminary analysis on a synthetic binary node classification problem then underscores both the expressivity and efficiency of the proposed GNN architecture. Overall, the paper lays the foundation for the novel NCMP framework as a practical path toward further enhancing the graph representational power of classical GNNs.

</details>


### [179] [Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning](https://arxiv.org/abs/2511.11081)
*Jun Hu,Shangheng Chen,Yufei He,Yuan Li,Bryan Hooi,Bingsheng He*

Main category: cs.LG

TL;DR: Echoless-LP proposes a memory-efficient method to address the training label leakage problem in label-based pre-computation for HGNNs, enhancing performance while remaining compatible with advanced message passing.


<details>
  <summary>Details</summary>
Motivation: HGNNs face efficiency challenges in large-scale graphs due to repetitive message passing and label leakage issues during multi-hop propagation.

Method: The proposed Echoless-LP introduces Partition-Focused Echoless Propagation (PFEP) to eliminate label leakage by partitioning nodes and prohibiting echo effect. Additionally, APS and PostAdjust mechanisms mitigate information loss and adapt to distributional shifts.

Result: Echoless-LP achieves better accuracy and memory efficiency compared to previous methods, as demonstrated on public datasets.

Conclusion: Echoless-LP provides an effective, scalable solution for HGNNs by eliminating training label leakage while ensuring compatibility and memory efficiency.

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.

</details>


### [180] [Scalable Population Training for Zero-Shot Coordination](https://arxiv.org/abs/2511.11083)
*Bingyu Hui,Lebin Yu,Quanming Yao,Yunpeng Qu,Xudong Zhang,Jian Wang*

Main category: cs.LG

TL;DR: The paper introduces Scalable Population Training (ScaPT), a framework that enhances zero-shot coordination by leveraging larger, diverse populations efficiently.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot coordination methods in reinforcement learning are constrained by computational resources, limiting the exploration of performance benefits from larger population sizes.

Method: Proposes ScaPT, which includes a meta-agent for efficient parameter sharing across agents and a mutual information regularizer to maintain population diversity.

Result: Evaluates the framework in the Hanabi environment, demonstrating its effectiveness compared to existing methods.

Conclusion: ScaPT resolves computational resource limitations by scaling population training while preserving diversity, leading to improved zero-shot coordination performance.

Abstract: Zero-shot coordination(ZSC) has become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.

</details>


### [181] [Sheaf Cohomology of Linear Predictive Coding Networks](https://arxiv.org/abs/2511.11092)
*Jeffrey Seely*

Main category: cs.LG

TL;DR: The paper connects predictive coding with cellular sheaf theory, analyzing errors and providing diagnostic and design insights for recurrent networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by prediction errors and feedback contradictions in predictive coding networks, especially in recurrent architectures.

Method: The authors map linear PC networks to cellular sheaves, using sheaf Laplacian, Hodge decomposition, and cohomology to better understand prediction errors in these networks.

Result: Their analysis identifies irreducible error patterns that hinder learning and provides tools for diagnosing issues and designing better weight initialization strategies.

Conclusion: The sheaf perspective enhances our understanding of predictive coding networks and offers practical benefits for designing and troubleshooting recurrent PCs.

Abstract: Predictive coding (PC) replaces global backpropagation with local optimization over weights and activations. We show that linear PC networks admit a natural formulation as cellular sheaves: the sheaf coboundary maps activations to edge-wise prediction errors, and PC inference is diffusion under the sheaf Laplacian. Sheaf cohomology then characterizes irreducible error patterns that inference cannot remove. We analyze recurrent topologies where feedback loops create internal contradictions, introducing prediction errors unrelated to supervision. Using a Hodge decomposition, we determine when these contradictions cause learning to stall. The sheaf formalism provides both diagnostic tools for identifying problematic network configurations and design principles for effective weight initialization for recurrent PC networks.

</details>


### [182] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: This paper introduces a new embedding initialization strategy for Knowledge Graph Embeddings (KGEs) to improve accuracy, reduce training time, and address catastrophic forgetting during updates.


<details>
  <summary>Details</summary>
Motivation: Frequent updates to Knowledge Graphs require continual adaptation of Knowledge Graph Embeddings (KGEs). Existing methods struggle with optimal initialization for new entities, impacting accuracy and training time.

Method: The proposed method utilizes the Knowledge Graph schema and previously learned embeddings to initialize embeddings for new entities based on their classes. This approach integrates seamlessly into continual learning methods for KGEs.

Result: The strategy improves predictive performance and knowledge retention, accelerates learning by reducing training epochs, and benefits multiple KGE learning models.

Conclusion: The informed initialization strategy enhances the accuracy, efficiency, and retention of incremental KGE learning, making it a valuable addition to continual learning processes.

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [183] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: This paper proposes and evaluates robust approaches for detecting point anomalies in high-dimensional bank account balance data, addressing computational efficiency challenges.


<details>
  <summary>Details</summary>
Motivation: Financial institutions need efficient methods to detect anomalies in account balances to identify fraud, operational issues, or irregularities, particularly under high dimensional data conditions.

Method: The study evaluates several robust approaches designed to process medium-to-high dimensional datasets efficiently, featuring high breakdown points and reduced computational time.

Result: Empirical application was carried out using approximately 2.6 million daily records of anonymous users' bank account balances, showcasing the utility of the proposed methods.

Conclusion: The paper demonstrates that robust methods can effectively detect point anomalies in high-dimensional bank data with improved computational efficiency.

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [184] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: This paper introduces an interpretable deep learning model for short-term precipitation forecasting in Indian cities, enhancing transparency and accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to address issues of transparency and accuracy in deep learning models for weather prediction.

Method: Developed a hybrid Time-Distributed CNN-ConvLSTM architecture optimized for multi-decadal ERA5 reanalysis data with different configurations for four Indian cities.

Result: Achieved RMSE values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata).

Conclusion: Explainable AI techniques contributed to transparency in precipitation forecasting, identifying city-specific patterns and prediction horizons.

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [185] [Adaptive Symmetrization of the KL Divergence](https://arxiv.org/abs/2511.11159)
*Omri Ben-Dov,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: This paper proposes a new approach to minimize Jeffreys divergence in machine learning tasks by using a proxy model and joint training, providing a practical and adaptive optimization algorithm.


<details>
  <summary>Details</summary>
Motivation: To improve the learning of probability distributions from samples by addressing the limitations of forward KL divergence and overcoming the computational challenges of symmetrical alternatives like the Jeffreys divergence.

Method: Introduces a proxy model that assists in optimizing the Jeffreys divergence of the main model and employs a constrained optimization framework for joint training and adapting model priorities.

Result: The proposed method effectively combines the strengths of normalizing flows (NFs) and energy-based models (EBMs), showcasing enhanced performance in applications such as density estimation, image generation, and simulation-based inference.

Conclusion: The new framework successfully balances the trade-offs between symmetry and tractability in divergence minimization, offering a versatile tool for improving generative modeling and inference tasks.

Abstract: Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.

</details>


### [186] [Training Neural Networks at Any Scale](https://arxiv.org/abs/2511.11163)
*Thomas Pethick,Kimon Antonakopoulos,Antonio Silveti-Falls,Leena Chennuru Vankadara,Volkan Cevher*

Main category: cs.LG

TL;DR: The article reviews efficient optimization methods for training neural networks, presenting unified algorithms and emphasizing scalability.


<details>
  <summary>Details</summary>
Motivation: To introduce modern optimization methods that efficiently train neural networks, accommodating increasing problem scales.

Method: Unified algorithmic template showcasing adaptable optimization algorithms for scalability.

Result: State-of-the-art algorithms explained in detail with a focus on scalability and efficiency.

Conclusion: Aimed to provide foundational knowledge for practitioners and researchers in neural network optimization methods.

Abstract: This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.

</details>


### [187] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: The paper proposes a machine learning classification approach to improve predictions of extreme heat events.


<details>
  <summary>Details</summary>
Motivation: Enhance predictions of climate extreme events, focusing on heat waves.

Method: Using ensemble predictions with a non-linear power mean aggregation technique in a machine learning generative framework.

Result: Enhanced accuracy in predicting extreme heat events, especially for higher quantile thresholds.

Conclusion: Power aggregation is effective and adaptable for extreme heat predictions, proving superior to traditional mean prediction methods.

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [188] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: The paper introduces the Sindy Kalman Filter (SKF), which combines sparse modeling and the Kalman filter for real-time learning of nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance real-time learning of governing equations in physical systems, overcoming the limitations of existing methods like the Sindy algorithm and the Kalman filter when used alone.

Method: The SKF algorithm integrates sparsity-driven modeling (Sindy) with the Kalman filter to treat system parameters as state variables for real-time inference. It incorporates strategies like look-ahead error to simplify parameter estimation.

Result: SKF is validated on a chaotic Lorenz system and a real aircraft flight data model, successfully demonstrating its ability to infer sparse, time-varying nonlinear systems in real time.

Conclusion: SKF effectively unifies sparsity modeling and control theory, achieving significant advancements in real-time identification of complex systems.

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [189] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: This paper proposes a novel method DGIMVCM to address challenges in incomplete multi-view clustering by utilizing dynamic graph learning and masked graph reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for incomplete multi-view clustering suffer from issues like reliance on static graphs prone to noise and substantial gradient noise during optimization due to direct use of MSE loss.

Method: The proposed DGIMVCM method constructs a missing-robust global graph, uses a graph convolutional embedding layer for feature extraction and graph structure refinement, employs a graph self-attention encoder optimized with masked graph reconstruction loss, and includes a clustering module with pseudo-label self-supervised training.

Result: Extensive experiments demonstrate that DGIMVCM outperforms existing methods and effectively resolves the highlighted challenges in incomplete multi-view clustering.

Conclusion: DGIMVCM successfully addresses key challenges in incomplete multi-view clustering, enhancing clustering effectiveness through dynamic graph learning and better optimization strategies.

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [190] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: LoRaCompass is a reinforcement learning model developed for efficiently locating LoRa tags in unknown environments, achieving significant accuracy and efficiency improvements over prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing reinforcement learning-based search methods which are prone to inaccuracies due to domain shifts and signal fluctuations during LoRa tag localization.

Method: LoRaCompass utilizes a robust spatial representation derived from RSSI data with spatially-aware feature extraction and policy distillation loss. It incorporates an exploration method inspired by the upper confidence bound (UCB) to ensure efficient and confident localization.

Result: LoRaCompass achieved over 90% success in locating tags within 100m proximity (a 40% improvement over existing solutions) across diverse environments while maintaining linear scalability of search path lengths relative to initial distances.

Conclusion: LoRaCompass is an effective and robust solution for LoRa tag localization, significantly outperforming existing methods in both accuracy and search efficiency in challenging conditions.

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [191] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: The paper introduces a framework for saving computation in Federated Learning (FL) by using a synthetic validation process, which determines early stopping points based on generative AI without compromising model accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing Federated Learning (FL) methods often waste computational resources by continuing training after optimal performance is achieved or when performance stagnates.

Method: The approach leverages a zero-shot synthetic validation framework using generative AI to monitor model performance and determine when training can be stopped early near the optimal round.

Result: Experimental results in multi-label chest X-ray classification show a reduction in training rounds by up to 74% while maintaining model accuracy within 1% of the optimal.

Conclusion: This method conserves computational resources, improves training efficiency, and allows for faster hyperparameter tuning without significant loss of model performance.

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [192] [Sparse Methods for Vector Embeddings of TPC Data](https://arxiv.org/abs/2511.11221)
*Tyler Wheeler,Michelle P. Kuchera,Raghuram Ramanujan,Ryan Krupp,Chris Wrede,Saiprasad Ravishankar,Connor L. Cross,Hoi Yan Ian Heung,Andrew J. Jones,Benjamin Votaw*

Main category: cs.LG

TL;DR: The paper investigates sparse convolutional networks for representation learning on Time Projection Chamber (TPC) data, emphasizing their effectiveness in encoding event structures across different detectors.


<details>
  <summary>Details</summary>
Motivation: To improve representation learning techniques for charged-particle track data in TPCs, especially in diverse nuclear physics experiments.

Method: Sparse ResNet architectures were utilized, with experiments on raw pad-level TPC signals from GADGET II and AT-TPC detectors. Pre-training on physics-based tasks further enhanced embeddings.

Result: Sparse ResNet models, even with random weights, provided structured embeddings. Pre-training improved embedding quality. The framework also worked effectively for cross-detector embedding tests, demonstrating general adaptability.

Conclusion: Sparse convolutional approaches are powerful and adaptable tools for structured representation learning in diverse TPC setups, enabling enhanced analysis of particle track data.

Abstract: Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $β$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [193] [MECHBench: A Set of Black-Box Optimization Benchmarks originated from Structural Mechanics](https://arxiv.org/abs/2511.10821)
*Iván Olarte Rodríguez,Maria Laura Santoni,Fabian Duddeck,Carola Doerr,Thomas Bäck,Elena Raponi*

Main category: cs.NE

TL;DR: The paper introduces a new benchmarking suite for structural mechanics, focusing on real-world applications like vehicle crashworthiness optimization.


<details>
  <summary>Details</summary>
Motivation: Current black-box optimization benchmarking suites rely on synthetic problems that fail to capture real-world complexities, limiting their practical impact.

Method: The authors propose a benchmarking suite based on optimization scenarios rooted in structural mechanics, particularly derived from vehicle crashworthiness problems.

Result: The paper provides descriptions of each optimization problem, physical contexts, and guidelines for employing the suite.

Conclusion: The proposed benchmarking suite addresses real-world optimization challenges better and offers structured guidance for engineering applications.

Abstract: Benchmarking is essential for developing and evaluating black-box optimization algorithms, providing a structured means to analyze their search behavior. Its effectiveness relies on carefully selected problem sets used for evaluation. To date, most established benchmark suites for black-box optimization consist of abstract or synthetic problems that only partially capture the complexities of real-world engineering applications, thereby severely limiting the insights that can be gained for application-oriented optimization scenarios and reducing their practical impact. To close this gap, we propose a new benchmarking suite that addresses it by presenting a curated set of optimization benchmarks rooted in structural mechanics. The current implemented benchmarks are derived from vehicle crashworthiness scenarios, which inherently require the use of gradient-free algorithms due to the non-smooth, highly non-linear nature of the underlying models. Within this paper, the reader will find descriptions of the physical context of each case, the corresponding optimization problem formulations, and clear guidelines on how to employ the suite.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [194] [Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)](https://arxiv.org/abs/2511.11055)
*Michael Schwarz,Julian Erhard*

Main category: cs.PL

TL;DR: This paper repurposes digests for race detection in static analysis, utilizing them to identify conditions preventing conflicting memory accesses from happening concurrently.


<details>
  <summary>Details</summary>
Motivation: To enhance static analysis for proving the absence of data races, utilizing computation summaries (digests) for concurrency-sensitive race detection.

Method: Introduced digests as exclusion criteria for race detection, implemented within the Goblint analyzer, and evaluated using SV-COMP benchmarks.

Result: The digest-driven approach significantly improved the number of correctly solved benchmarks by over five times compared to using lockset reasoning alone.

Conclusion: The digest-based static analysis method improves data race detection and demonstrates its practical advantages in a well-established benchmark suite.

Abstract: Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone.

</details>


### [195] [Optimising Density Computations in Probabilistic Programs via Automatic Loop Vectorisation](https://arxiv.org/abs/2511.11070)
*Sangho Lim,Hyoungjin Lim,Wonyeol Lee,Xavier Rival,Hongseok Yang*

Main category: cs.PL

TL;DR: The paper proposes a method to automatically vectorize loops in probabilistic programs, achieving significant performance improvements and broader applicability.


<details>
  <summary>Details</summary>
Motivation: Probabilistic inference in probabilistic programming languages is computationally expensive, especially due to iteration over large datasets or random samples. Existing vectorization methods are limited, error-prone, and cannot handle general repetition structures efficiently.

Method: The authors present a method that automatically vectorizes loops through speculative parallel execution while preserving the loop's semantics using a fixed-point check. The approach involves translating imperative probabilistic programming language (PPL) codes into a vectorization-optimized lower-level language.

Result: The proposed method was implemented in the Pyro PPL and tested on various models. Experiments showed significant speedups (1.1-6x) and reduced GPU memory usage while successfully handling all tested models, unlike existing baselines.

Conclusion: The method outperforms existing vectorization approaches in terms of speed, applicability, and resource efficiency, offering an effective solution for optimizing probabilistic program loops.

Abstract: Probabilistic programming languages (PPLs) are a popular tool for high-level modelling across many fields. They provide a range of algorithms for probabilistic inference, which analyse models by learning their parameters from a dataset or estimating their posterior distributions. However, probabilistic inference is known to be very costly. One of the bottlenecks of probabilistic inference stems from the iteration over entries of a large dataset or a long series of random samples. Vectorisation can mitigate this cost, but manual vectorisation is error-prone, and existing automatic techniques are often ad-hoc and limited, unable to handle general repetition structures, such as nested loops and loops with data-dependent control flow, without significant user intervention. To address this bottleneck, we propose a sound and effective method for automatically vectorising loops in probabilistic programs. Our method achieves high throughput using speculative parallel execution of loop iterations, while preserving the semantics of the original loop through a fixed-point check. We formalise our method as a translation from an imperative PPL into a lower-level target language with primitives geared towards vectorisation. We implemented our method for the Pyro PPL and evaluated it on a range of probabilistic models. Our experiments show significant performance gains against an existing vectorisation baseline, achieving $1.1$--$6\times$ speedups and reducing GPU memory usage in many cases. Unlike the baseline, which is limited to a subset of models, our method effectively handled all the tested models.

</details>


### [196] [Kleene Algebra](https://arxiv.org/abs/2511.11264)
*Tobias Kappé,Alexandra Silva,Jana Wagemaker*

Main category: cs.PL

TL;DR: This booklet introduces Kleene Algebra (KA) for proving program equivalences using regular expressions and automata, with exercises and an optional chapter on coalgebraic automata theory.


<details>
  <summary>Details</summary>
Motivation: To provide a foundational understanding of Kleene Algebra for analyzing program equivalences systematically.

Method: Explains the use of regular expressions and automata to model programs and employs KA laws to demonstrate equivalences, supported by exercises and a coalgebraic perspective.

Result: Shows that program equivalences in regular expressions can be proven rigorously using KA laws.

Conclusion: Kleene Algebra is a powerful and formally sound tool for studying program equivalences, supported by connections to automata theory and extended perspectives like coalgebra.

Abstract: This booklet serves as an introduction to Kleene Algebra (KA), a set of laws that can be used to study general equivalences between programs. It discusses how general programs can be modeled using regular expressions, how those expressions correspond to automata, and how this correspondence can be exploited to obtain the central result of KA, namely that an equivalence of regular expressions is true if and only if it can be proved using the laws of KA. Each chapter closes with a set of exercises to further build intuition and understanding, and there is an optional chapter that develops automata theory through the lens of coalgebra.

</details>


### [197] [The Jasmin Compiler Preserves Cryptographic Security](https://arxiv.org/abs/2511.11292)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Benjamin Grégoire,Vincent Laporte,Paolo Torrini*

Main category: cs.PL

TL;DR: The paper enhances the Jasmin framework by establishing that its compiler preserves cryptographic security. It introduces a specialized logic for proofs and proves functional correctness and security preservation using interaction trees.


<details>
  <summary>Details</summary>
Motivation: To strengthen the guarantees of cryptographic implementation correctness in the Jasmin framework and expand its applicability to probabilistic computations crucial in cryptography.

Method: The authors define a novel Relational Hoare Logic for compiler correctness and prove its soundness using interaction trees. They apply the logic to demonstrate the functional correctness of the Jasmin compiler and confirm it preserves cryptographic security.

Result: The paper proves functional correctness of 25 compiler passes and demonstrates cryptographic security preservation in terms of IND-CCA under interaction tree semantics.

Conclusion: The developed logic and approach significantly advance the reliability and security guarantees of the Jasmin compiler, empowering more secure cryptographic implementations.

Abstract: Jasmin is a programming and verification framework for developing efficient, formally verified, cryptographic implementations. A main component of the framework is the Jasmin compiler, which empowers programmers to write efficient implementations of state-of-the-art cryptographic primitives, including post-quantum cryptographic standards. The Jasmin compiler is proven functionally correct in the Rocq prover. However, this functional correctness statement does not apply to nonterminating or probabilistic computations, which are essential features in cryptography.
  In this paper, we significantly enhance the guarantees of the compiler by showing, in the Rocq prover, that its front-end (25 out of 30 passes) preserves cryptographic security. To this end, we first define a Relational Hoare Logic tailored for compiler correctness proofs. We prove the soundness of our logic w.r.t. a new denotational semantics of Jasmin programs based on interaction trees. Secondly, we use our program logic to prove the functional correctness of the (unmodified) Jasmin compiler w.r.t. said semantics. Lastly, we formalize cryptographic security -- focusing on IND-CCA -- with interaction trees and prove that the Jasmin compiler preserves cryptographic security.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [198] [Attentive Feature Aggregation or: How Policies Learn to Stop Worrying about Robustness and Attend to Task-Relevant Visual Cues](https://arxiv.org/abs/2511.10762)
*Nikolaos Tsagkas,Andreas Sochopoulos,Duolikun Danier,Sethu Vijayakumar,Alexandros Kouris,Oisin Mac Aodha,Chris Xiaoxuan Lu*

Main category: cs.RO

TL;DR: This paper proposes Attentive Feature Aggregation (AFA), a trainable pooling mechanism to improve visuomotor policies' robustness to visual perturbations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerability of visuomotor policies to visual perturbations caused by task-irrelevant information encoded in pre-trained visual representations.

Method: The proposed solution, Attentive Feature Aggregation (AFA), is a lightweight pooling mechanism that learns to attend to task-relevant visual cues while ignoring irrelevant distractors.

Result: Policies trained with AFA outperform standard pooling methods in handling visual disruptions, demonstrated through comprehensive experiments in both simulation and real-world settings.

Conclusion: Ignoring irrelevant visual data via AFA is essential for developing robust and generalizable visuomotor policies, eliminating the need for expensive data augmentation or PVR fine-tuning.

Abstract: The adoption of pre-trained visual representations (PVRs), leveraging features from large-scale vision models, has become a popular paradigm for training visuomotor policies. However, these powerful representations can encode a broad range of task-irrelevant scene information, making the resulting trained policies vulnerable to out-of-domain visual changes and distractors. In this work we address visuomotor policy feature pooling as a solution to the observed lack of robustness in perturbed scenes. We achieve this via Attentive Feature Aggregation (AFA), a lightweight, trainable pooling mechanism that learns to naturally attend to task-relevant visual cues, ignoring even semantically rich scene distractors. Through extensive experiments in both simulation and the real world, we demonstrate that policies trained with AFA significantly outperform standard pooling approaches in the presence of visual perturbations, without requiring expensive dataset augmentation or fine-tuning of the PVR. Our findings show that ignoring extraneous visual information is a crucial step towards deploying robust and generalisable visuomotor policies. Project Page: tsagkas.github.io/afa

</details>


### [199] [From Framework to Reliable Practice: End-User Perspectives on Social Robots in Public Spaces](https://arxiv.org/abs/2511.10770)
*Samson Oruma,Ricardo Colomo-Palacios,Vasileios Gkioulos*

Main category: cs.RO

TL;DR: This paper reports on the deployment of an ARI social robot as a university receptionist and evaluates user feedback to address safety, privacy, and inclusiveness challenges, contributing to trustworthy and ethical social robot design.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure the acceptance of social robots in public environments based on technical reliability, ethical integrity, accessibility, and user trust, while using the SecuRoPS framework for secure and ethical robot deployment.

Method: The authors conducted a user study with 35 participants interacting with the robot, collecting structured feedback on various dimensions like safety, privacy, usability, and transparency, and aligning the robot's design with the SecuRoPS framework.

Result: The study found positive perceptions regarding physical safety, data protection, and ethical behavior but identified challenges in accessibility and dynamic interaction. Public templates for ARI robot applications were developed to aid reproducibility.

Conclusion: The paper concludes that combining end-user evaluations with ethical frameworks supports the design of trustworthy, inclusive, and ethically responsible robots, and it encourages researchers with open resources for improving robot deployment.

Abstract: As social robots increasingly enter public environments, their acceptance depends not only on technical reliability but also on ethical integrity, accessibility, and user trust. This paper reports on a pilot deployment of an ARI social robot functioning as a university receptionist, designed in alignment with the SecuRoPS framework for secure and ethical social robot deployment. Thirty-five students and staff interacted with the robot and provided structured feedback on safety, privacy, usability, accessibility, and transparency. The results show generally positive perceptions of physical safety, data protection, and ethical behavior, while also highlighting challenges related to accessibility, inclusiveness, and dynamic interaction. Beyond the empirical findings, the study demonstrates how theoretical frameworks for ethical and secure design can be implemented in real-world contexts through end-user evaluation. It also provides a public GitHub repository containing reusable templates for ARI robot applications to support reproducibility and lower the entry barrier for new researchers. By combining user perspectives with practical technical resources, this work contributes to ongoing discussions in AI and society and supports the development of trustworthy, inclusive, and ethically responsible social robots for public spaces.

</details>


### [200] [$\rm{A}^{\rm{SAR}}$: $\varepsilon$-Optimal Graph Search for Minimum Expected-Detection-Time Paths with Path Budget Constraints for Search and Rescue](https://arxiv.org/abs/2511.10792)
*Eric Mugford,Jonathan D. Gammell*

Main category: cs.RO

TL;DR: The paper presents a new search algorithm, $\rm{A}^{\rm{SAR}}$, for Search and Rescue (SAR) operations, providing optimal solutions with formal guarantees and outperforming existing methods in simulations and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Search and Rescue (SAR) operations often face challenges due to uncertain information, imperfect observers, large areas, and time-sensitive scenarios, requiring optimization techniques to enhance success likelihood.

Method: The $\rm{A}^{\rm{SAR}}$ algorithm employs a heuristic to bound the search space and utilizes graph-search methods to ensure $\varepsilon$-optimal solutions, balancing quality and computational efficiency.

Result: The algorithm demonstrated superior performance by finding better solutions faster in simulations and successfully locating a drifting manikin within 150 seconds during a real-world trial.

Conclusion: The $\rm{A}^{\rm{SAR}}$ algorithm provides a reliable and efficient approach for SAR planning with formal guarantees on solution quality, making it a practical tool for improving SAR effectiveness.

Abstract: Searches are conducted to find missing persons and/or objects given uncertain information, imperfect observers and large search areas in Search and Rescue (SAR). In many scenarios, such as Maritime SAR, expected survival times are short and optimal search could increase the likelihood of success. This optimization problem is complex for nontrivial problems given its probabilistic nature.
  Stochastic optimization methods search large problems by nondeterministically sampling the space to reduce the effective size of the problem. This has been used in SAR planning to search otherwise intractably large problems but the stochastic nature provides no formal guarantees on the quality of solutions found in finite time.
  This paper instead presents $\rm{A}^{\rm{SAR}}$, an $\varepsilon$-optimal search algorithm for SAR planning. It calculates a heuristic to bound the search space and uses graph-search methods to find solutions that are formally guaranteed to be within a user-specified factor, $\varepsilon$, of the optimal solution. It finds better solutions faster than existing optimization approaches in operational simulations. It is also demonstrated with a real-world field trial on Lake Ontario, Canada, where it was used to locate a drifting manikin in only 150s.

</details>


### [201] [An Investigation into Dynamically Extensible and Retractable Robotic Leg Linkages for Multi-task Execution in Search and Rescue Scenarios](https://arxiv.org/abs/2511.10816)
*William Harris,Lucas Yager,Syler Sylvester,Elizabeth Peiros,Micheal C. Yip*

Main category: cs.RO

TL;DR: The paper introduces a novel robot leg design combining terrain adaptability and high-force capability for search and rescue robots.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of SAR robots requiring both efficient terrain adaptability and the ability to execute high-force rescue tasks, which are not well-supported by existing platforms.

Method: Proposing a robot leg design based on a dynamically extensible and retractable five-bar linkage mechanism, enabling it to switch between height and force advantages through geometric transformations. Empirical and analytical evaluations of the design were conducted.

Result: The study showed that the morphing leg enhanced stride length, force output, and stability, demonstrating its dual capability for terrain navigation and rescue tasks.

Conclusion: This innovative leg design offers a significant advancement for SAR robots by addressing the need for both effective mobility and rescue performance.

Abstract: Search and rescue (SAR) robots are required to quickly traverse terrain and perform high-force rescue tasks, necessitating both terrain adaptability and controlled high-force output. Few platforms exist today for SAR, and fewer still have the ability to cover both tasks of terrain adaptability and high-force output when performing extraction. While legged robots offer significant ability to traverse uneven terrain, they typically are unable to incorporate mechanisms that provide variable high-force outputs, unlike traditional wheel-based drive trains. This work introduces a novel concept for a dynamically extensible and retractable robot leg. Leveraging a dynamically extensible and retractable five-bar linkage design, it allows for mechanically switching between height-advantaged and force-advantaged configurations via a geometric transformation. A testbed evaluated leg performance across linkage geometries and operating modes, with empirical and analytical analyses conducted on stride length, force output, and stability. The results demonstrate that the morphing leg offers a promising path toward SAR robots that can both navigate terrain quickly and perform rescue tasks effectively.

</details>


### [202] [MIGHTY: Hermite Spline-based Efficient Trajectory Planning](https://arxiv.org/abs/2511.10822)
*Kota Kondo,Yuwei Wu,Vijay Kumar,Jonathan P. How*

Main category: cs.RO

TL;DR: MIGHTY is a Hermite spline-based planner that addresses limitations in trajectory planning by optimizing spatial and temporal factors simultaneously, achieving increased computational efficiency and travel improvement.


<details>
  <summary>Details</summary>
Motivation: Current trajectory planners are either computationally intensive or fast but limited due to decoupled or restrictive methods.

Method: MIGHTY employs Hermite splines for spatiotemporal optimization without restricting the continuous search space.

Result: Simulation tests showed 9.3% faster computation and 13.1% reduced travel time compared to existing methods. Hardware tests demonstrated successful flights in static and dynamic obstacle-laden environments.

Conclusion: MIGHTY proves to be an effective planner for fast and efficient trajectory optimization in complex environments.

Abstract: Hard-constraint trajectory planners often rely on commercial solvers and demand substantial computational resources. Existing soft-constraint methods achieve faster computation, but either (1) decouple spatial and temporal optimization or (2) restrict the search space. To overcome these limitations, we introduce MIGHTY, a Hermite spline-based planner that performs spatiotemporal optimization while fully leveraging the continuous search space of a spline. In simulation, MIGHTY achieves a 9.3% reduction in computation time and a 13.1% reduction in travel time over state-of-the-art baselines, with a 100% success rate. In hardware, MIGHTY completes multiple high-speed flights up to 6.7 m/s in a cluttered static environment and long-duration flights with dynamically added obstacles.

</details>


### [203] [Decentralized Swarm Control via SO(3) Embeddings for 3D Trajectories](https://arxiv.org/abs/2511.10858)
*Dimitria Silveria,Kleber Cabral,Peter Jardine,Sidney Givigi*

Main category: cs.RO

TL;DR: The paper introduces a decentralized approach for multi-agent systems to achieve emergent behavior with minimal information sharing, relying on Lie group SO(3) to generate diverse periodic trajectories and ensure uniform agent separation.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-agent systems' capabilities by developing a method that allows emergent behavior with minimal information and expands the range of achievable periodic trajectories.

Method: Using the Lie group SO(3) for geometric embedding and eliminating velocity inputs. It also incorporates a novel phase controller for uniform agent separation, formalizing stability through theoretical proof.

Result: The proposed method is validated via simulations and experiments, demonstrating its adaptability to various dynamics and disturbances.

Conclusion: This work advances decentralized multi-agent system control by offering a robust, minimal-information method for generating diverse emergent behaviors with proven stability.

Abstract: This paper presents a novel decentralized approach for achieving emergent behavior in multi-agent systems with minimal information sharing. Based on prior work in simple orbits, our method produces a broad class of stable, periodic trajectories by stabilizing the system around a Lie group-based geometric embedding. Employing the Lie group SO(3), we generate a wider range of periodic curves than existing quaternion-based methods. Furthermore, we exploit SO(3) properties to eliminate the need for velocity inputs, allowing agents to receive only position inputs. We also propose a novel phase controller that ensures uniform agent separation, along with a formal stability proof. Validation through simulations and experiments showcases the method's adaptability to complex low-level dynamics and disturbances.

</details>


### [204] [WetExplorer: Automating Wetland Greenhouse-Gas Surveys with an Autonomous Mobile Robot](https://arxiv.org/abs/2511.10864)
*Jose Vasquez,Xuping Zhang*

Main category: cs.RO

TL;DR: WetExplorer is a robotic system that automates greenhouse gas sampling in wetlands, effectively removing the labor-intensive sampling bottleneck.


<details>
  <summary>Details</summary>
Motivation: Manual sampling of greenhouse gases in wetlands is inefficient and limits the ability to produce high-frequency and long-duration datasets.

Method: WetExplorer uses an autonomous robot equipped with advanced sensor fusion, obstacle avoidance, and deep-learning perception systems in a ROS2 containerized stack.

Result: Outdoor trials showed precise localization (1.71 cm error), accurate object pose estimation (7 mm translational, 3° rotational), and effective motion planning. It achieved autonomous GHG sampling without human intervention.

Conclusion: WetExplorer revolutionizes GHG sampling with its autonomous capabilities, enabling dense and long-term data collection for better climate modeling and restoration projects.

Abstract: Quantifying greenhouse-gases (GHG) in wetlands is critical for climate modeling and restoration assessment, yet manual sampling is labor-intensive, and time demanding. We present WetExplorer, an autonomous tracked robot that automates the full GHG-sampling workflow. The robot system integrates low-ground-pressure locomotion, centimeter-accurate lift placement, dual-RTK sensor fusion, obstacle avoidance planning, and deep-learning perception in a containerized ROS2 stack. Outdoor trials verified that the sensor-fusion stack maintains a mean localization error of 1.71 cm, the vision module estimates object pose with 7 mm translational and 3° rotational accuracy, while indoor trials demonstrated that the full motion-planning pipeline positions the sampling chamber within a global tolerance of 70 mm while avoiding obstacles, all without human intervention. By eliminating the manual bottleneck, WetExplorer enables high-frequency, multi-site GHG measurements and opens the door for dense, long-duration datasets in saturated wetland terrain.

</details>


### [205] [Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation](https://arxiv.org/abs/2511.10874)
*Yorai Shaoul,Zhe Chen,Mohamed Naveed Gul Mohamed,Federico Pecora,Maxim Likhachev,Jiaoyang Li*

Main category: cs.RO

TL;DR: This paper introduces a framework integrating generative modeling and motion planning for collaborative robotics manipulating multiple objects in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to multi-robot manipulation tasks struggle to scale with diverse objects and long-horizon goals.

Method: Using generative co-design, a flow-matching model generates contact formations and manipulation trajectories, paired with a novel, scalable motion planner.

Result: Experiments demonstrate the approach's superiority to baselines in motion planning and manipulation tasks within complex simulation settings.

Conclusion: Integrated generative modeling and motion planning enable effective coordination in collaborative multi-robot tasks, scaling to complex environments.

Abstract: Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale. Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks. To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning. Within this framework, a generative model co-generates contact formations and manipulation trajectories from visual observations, while a novel motion planner conveys robots at scale. Crucially, the same planner also supports coordination at the object level, assigning manipulated objects to larger target structures and thereby unifying robot- and object-level reasoning within a single algorithmic framework. Experiments in challenging simulated environments demonstrate that our approach outperforms baselines in both motion planning and manipulation tasks, highlighting the benefits of generative co-design and integrated planning for scaling collaborative manipulation to complex multi-agent, multi-object settings. Visit gco-paper.github.io for code and demonstrations.

</details>


### [206] [Terradynamics and design of tip-extending robotic anchors](https://arxiv.org/abs/2511.10901)
*Deniz Kerimoglu,Nicholas D. Naclerio,Sean Chu,Andrew Krohn,Vineet Kupunaram,Alexander Schepelmann,Daniel I. Goldman,Elliot W. Hawkes*

Main category: cs.RO

TL;DR: This paper analyzes the mechanics of root-inspired anchoring systems, leading to the design of a lightweight robotic anchor that enables efficient insertion and high anchoring forces, particularly for extraterrestrial settings.


<details>
  <summary>Details</summary>
Motivation: Traditional pilings require significant forces for insertion compared to extraction, which poses challenges in hard-to-reach or extraterrestrial locations. The paper aims to leverage the efficient mechanism of root growth, which enables insertion with minimal external force, for robotic anchoring applications.

Method: The authors studied the terradynamics of tip-extending anchors and compared them with traditional piling mechanisms. They identified specific design principles (e.g., critical depth, hair-like protrusions, near-vertical extension) and implemented these insights into a novel robotic anchor inspired by roots.

Result: They developed a 300-gram lightweight robotic anchor, which could install temperature sensors 45 cm deep into loose Martian regolith simulant. It achieved an anchoring-to-weight ratio of 40:1, requiring minimal insertion force.

Conclusion: By leveraging principles inspired by root tip extension, this study demonstrates the feasibility of lightweight, efficient robotic anchoring systems for challenging terrains such as extraterrestrial environments.

Abstract: Most engineered pilings require substantially more force to be driven into the ground than they can resist during extraction. This requires relatively heavy equipment for insertion, which is problematic for anchoring in hard-to-access sites, including in extraterrestrial locations. In contrast, for tree roots, the external reaction force required to extract is much greater than required to insert--little more than the weight of the seed initiates insertion. This is partly due to the mechanism by which roots insert into the ground: tip extension. Proof-of-concept robotic prototypes have shown the benefits of using this mechanism, but a rigorous understanding of the underlying granular mechanics and how they inform the design of a robotic anchor is lacking. Here, we study the terradynamics of tip-extending anchors compared to traditional piling-like intruders, develop a set of design insights, and apply these to create a deployable robotic anchor. Specifically, we identify that to increase an anchor's ratio of extraction force to insertion force, it should: (i) extend beyond a critical depth; (ii) include hair-like protrusions; (iii) extend near-vertically, and (iv) incorporate multiple smaller anchors rather than a single large anchor. Synthesizing these insights, we developed a lightweight, soft robotic, root-inspired anchoring device that inserts into the ground with a reaction force less than its weight. We demonstrate that the 300 g device can deploy a series of temperature sensors 45 cm deep into loose Martian regolith simulant while anchoring with an average of 120 N, resulting in an anchoring-to-weight ratio of 40:1.

</details>


### [207] [Dexterous Manipulation Transfer via Progressive Kinematic-Dynamic Alignment](https://arxiv.org/abs/2511.10987)
*Wenbin Bai,Qiyu Chen,Xiangbo Lin,Jianwen Li,Quancheng Li,Hejiang Pan,Yi Sun*

Main category: cs.RO

TL;DR: This paper proposes a system for efficiently converting human hand manipulation in videos into high-quality robot hand manipulation trajectories, overcoming data collection challenges and achieving strong generalizability.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the difficulty of gathering sufficient data for dexterous manipulation policy learning due to the constraints and data scarcity of multi-fingered robot hand platforms.

Method: The authors introduce a progressive transfer framework: (1) building primary control signals through kinematic matching, (2) refining manipulation with dynamically optimized residual policies, and (3) determining wrist trajectories to preserve operational semantics.

Result: The system demonstrated an average transfer success rate of 73%, proving its ability to translate human manipulation into coherent and efficient robot hand trajectories while maintaining task and object generalization.

Conclusion: The proposed framework provides a scalable and automated solution for generating robot dexterous manipulation data, facilitating research in data-driven dexterous manipulation learning through enhanced efficiency and generalizability.

Abstract: The inherent difficulty and limited scalability of collecting manipulation data using multi-fingered robot hand hardware platforms have resulted in severe data scarcity, impeding research on data-driven dexterous manipulation policy learning. To address this challenge, we present a hand-agnostic manipulation transfer system. It efficiently converts human hand manipulation sequences from demonstration videos into high-quality dexterous manipulation trajectories without requirements of massive training data. To tackle the multi-dimensional disparities between human hands and dexterous hands, as well as the challenges posed by high-degree-of-freedom coordinated control of dexterous hands, we design a progressive transfer framework: first, we establish primary control signals for dexterous hands based on kinematic matching; subsequently, we train residual policies with action space rescaling and thumb-guided initialization to dynamically optimize contact interactions under unified rewards; finally, we compute wrist control trajectories with the objective of preserving operational semantics. Using only human hand manipulation videos, our system automatically configures system parameters for different tasks, balancing kinematic matching and dynamic optimization across dexterous hands, object categories, and tasks. Extensive experimental results demonstrate that our framework can automatically generate smooth and semantically correct dexterous hand manipulation that faithfully reproduces human intentions, achieving high efficiency and strong generalizability with an average transfer success rate of 73%, providing an easily implementable and scalable method for collecting robot dexterous manipulation data.

</details>


### [208] [Dynamic Reconfiguration of Robotic Swarms: Coordination and Control for Precise Shape Formation](https://arxiv.org/abs/2511.10989)
*Prab Prasertying,Paulo Garcia,Warisa Sritriratanarak*

Main category: cs.RO

TL;DR: This paper addresses the challenges in movement and configuration coordination of robotic swarms by presenting an algorithm for optimal path computation and seamless transitions using geometric formulations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of coordinating robotic swarms effectively, overcoming the computational complexity and physical system challenges like measurement error and control dynamics.

Method: The paper introduces an algorithm leveraging geometric formulations mapped to the physical domain through control, localization, and mapping techniques.

Result: The presented methods enable robotic swarms to make seamless transitions between configurations, allowing sophisticated distributed behaviors.

Conclusion: The algorithm facilitates novel applications for robotic swarms and enhances their ability to coordinate efficiently and perform complex actions.

Abstract: Coordination of movement and configuration in robotic swarms is a challenging endeavor. Deciding when and where each individual robot must move is a computationally complex problem. The challenge is further exacerbated by difficulties inherent to physical systems, such as measurement error and control dynamics. Thus, how to best determine the optimal path for each robot, when moving from one configuration to another, and how to best perform such determination and effect corresponding motion remains an open problem. In this paper, we show an algorithm for such coordination of robotic swarms. Our methods allow seamless transition from one configuration to another, leveraging geometric formulations that are mapped to the physical domain through appropriate control, localization, and mapping techniques. This paves the way for novel applications of robotic swarms by enabling more sophisticated distributed behaviors.

</details>


### [209] [Latent-Space Autoregressive World Model for Efficient and Robust Image-Goal Navigation](https://arxiv.org/abs/2511.11011)
*Zhiwei Zhang,Hui Zhang,Xieyuanli Chen,Kaihong Huang,Chenghao Shi,Huimin Lu*

Main category: cs.RO

TL;DR: The paper introduces LS-NWM, a lightweight navigation world model that operates entirely in latent space, achieving both improved performance and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional navigation methods depend heavily on accurate localization and mapping, which are computationally expensive. This paper aims to enhance efficiency and performance by utilizing latent space for navigation tasks.

Method: The model predicts future latent states based on current observations and actions, eliminating the need for pixel-wise predictions. It incorporates autoregressive multi-frame prediction during training to capture spatiotemporal dependencies.

Result: The proposed LS-NWM reduces training time by 3.2x and planning time by 447x while achieving a 35% higher success rate (SR) and an 11% higher success per length (SPL) compared to the state-of-the-art.

Conclusion: LS-NWM combines efficiency and high performance by focusing on latent space navigation and optimizing computational processes, making it well-suited for complex navigation scenarios.

Abstract: Traditional navigation methods rely heavily on accurate localization and mapping. In contrast, world models that capture environmental dynamics in latent space have opened up new perspectives for navigation tasks, enabling systems to move beyond traditional multi-module pipelines. However, world model often suffers from high computational costs in both training and inference. To address this, we propose LS-NWM - a lightweight latent space navigation world model that is trained and operates entirely in latent space, compared to the state-of-the-art baseline, our method reduces training time by approximately 3.2x and planning time by about 447x,while further improving navigation performance with a 35% higher SR and an 11% higher SPL. The key idea is that accurate pixel-wise environmental prediction is unnecessary for navigation. Instead, the model predicts future latent states based on current observational features and action inputs, then performs path planning and decision-making within this compact representation, significantly improving computational efficiency. By incorporating an autoregressive multi-frame prediction strategy during training, the model effectively captures long-term spatiotemporal dependencies, thereby enhancing navigation performance in complex scenarios. Experimental results demonstrate that our method achieves state-of-the-art navigation performance while maintaining a substantial efficiency advantage over existing approaches.

</details>


### [210] [Miniature Testbed for Validating Multi-Agent Cooperative Autonomous Driving](https://arxiv.org/abs/2511.11022)
*Hyunchul Bae,Eunjae Lee,Jehyeop Han,Minhee Kang,Jaehyeon Kim,Junggeun Seo,Minkyun Noh,Heejin Ahn*

Main category: cs.RO

TL;DR: This paper introduces CIVAT, a 1:15-scale testbed for validating cooperative autonomous driving with smart infrastructure.


<details>
  <summary>Details</summary>
Motivation: Existing testbeds lack smart infrastructure equipped with sensing, edge computing, and communication capabilities.

Method: Designed a scaled testbed including urban map, autonomous vehicles, and smart infrastructure. Integrated V2V and V2I communication via Wi-Fi and ROS2 framework.

Result: Validated cooperative driving functionality through experiments on infrastructure-based perception and intersection management.

Conclusion: The testbed successfully demonstrates the capability of smart infrastructure to enhance cooperative autonomous driving.

Abstract: Cooperative autonomous driving, which extends vehicle autonomy by enabling real-time collaboration between vehicles and smart roadside infrastructure, remains a challenging yet essential problem. However, none of the existing testbeds employ smart infrastructure equipped with sensing, edge computing, and communication capabilities. To address this gap, we design and implement a 1:15-scale miniature testbed, CIVAT, for validating cooperative autonomous driving, consisting of a scaled urban map, autonomous vehicles with onboard sensors, and smart infrastructure. The proposed testbed integrates V2V and V2I communication with the publish-subscribe pattern through a shared Wi-Fi and ROS2 framework, enabling information exchange between vehicles and infrastructure to realize cooperative driving functionality. As a case study, we validate the system through infrastructure-based perception and intersection management experiments.

</details>


### [211] [AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation](https://arxiv.org/abs/2511.11052)
*Jinxuan Zhu,Chenrui Tie,Xinyi Cao,Yuran Wang,Jingxiang Guo,Zixuan Chen,Haonan Chen,Junting Chen,Yangyu Xiao,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: The paper introduces AdaptPNP, a vision-language model-based framework enabling robots to seamlessly integrate prehensile and non-prehensile manipulation strategies for diverse tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of enabling robots to generalize across tasks, objects, and environments by using non-prehensile and prehensile manipulations together.

Method: It uses a vision-language model to interpret visual and text information, generates high-level task plans, predicts object poses for planning, and continuously adjusts execution through adaptive replanning.

Result: The method was tested in both simulation and real-world tasks, demonstrating its effectiveness in improving hybrid manipulation capabilities of robots.

Conclusion: Hybrid non-prehensile and prehensile manipulation, facilitated by AdaptPNP, represents a step toward achieving versatile and human-level robotic manipulation skills.

Abstract: Non-prehensile (NP) manipulation, in which robots alter object states without forming stable grasps (for example, pushing, poking, or sliding), significantly broadens robotic manipulation capabilities when grasping is infeasible or insufficient. However, enabling a unified framework that generalizes across different tasks, objects, and environments while seamlessly integrating non-prehensile and prehensile (P) actions remains challenging: robots must determine when to invoke NP skills, select the appropriate primitive for each context, and compose P and NP strategies into robust, multi-step plans. We introduce ApaptPNP, a vision-language model (VLM)-empowered task and motion planning framework that systematically selects and combines P and NP skills to accomplish diverse manipulation objectives. Our approach leverages a VLM to interpret visual scene observations and textual task descriptions, generating a high-level plan skeleton that prescribes the sequence and coordination of P and NP actions. A digital-twin based object-centric intermediate layer predicts desired object poses, enabling proactive mental rehearsal of manipulation sequences. Finally, a control module synthesizes low-level robot commands, with continuous execution feedback enabling online task plan refinement and adaptive replanning through the VLM. We evaluate ApaptPNP across representative P&NP hybrid manipulation tasks in both simulation and real-world environments. These results underscore the potential of hybrid P&NP manipulation as a crucial step toward general-purpose, human-level robotic manipulation capabilities. Project Website: https://sites.google.com/view/adaptpnp/home

</details>


### [212] [Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2511.11218)
*Chenhao Liu,Leyun Jiang,Yibo Wang,Kairan Yao,Jinchen Fu,Xiaoyu Ren*

Main category: cs.RO

TL;DR: This paper presents a reinforcement learning-based pipeline for humanoid robots to achieve dynamic and precise whole-body control for playing badminton.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enable humanoid robots to effectively interact with dynamic environments, moving beyond quasi-static interactions, for tasks like badminton, which demand precise and dynamic coordination.

Method: The authors use reinforcement learning with a three-stage curriculum focusing on footwork acquisition, racket swing generation, and task-focused refinement. They integrate an Extended Kalman Filter for shuttlecock trajectory estimation and offer a prediction-free variant.

Result: Simulation experiments showed a rally of 21 consecutive hits, while real-world tests achieved shuttle speeds up to 10 m/s with precise landing outcomes. Both prediction and controller modules exhibited high accuracy.

Conclusion: The framework successfully enables humanoid robots to perform dynamic and precise badminton striking, with potential adaptability to other dynamic tasks in critical domains.

Abstract: Humanoid robots have demonstrated strong capability for interacting with deterministic scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, quasi-static interactions are insufficient to cope with the various environmental conditions. As a step toward more dynamic interaction scenario, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without any motion priors or expert demonstrations. Training follows a three-stage curriculum: first footwork acquisition, then precision-guided racket swing generation, and finally task-focused refinement, yielding motions in which both legs and arms serve the hitting objective. For deployment, we incorporate an Extended Kalman Filter (EKF) to estimate and predict shuttlecock trajectories for target striking. We also introduce a prediction-free variant that dispenses with EKF and explicit trajectory prediction. To validate the framework, we conduct five sets of experiment in both simulation and the real world. In simulation, two robots sustain a rally of 21 consecutive hits. Moreover, the prediction-free variant achieves successful hits with comparable performance relative to the target-known policy. In real-world tests, both the prediction and controller module exhibit high accuracy, and on-court hitting achieves an outgoing shuttle speed up to 10 m/s with a mean return landing distance of 3.5 m. These experiment results show that our humanoid robot can deliver highly dynamic while precise goal striking in badminton, and can be adapted to more dynamism critical domains.

</details>


### [213] [Sashimi-Bot: Autonomous Tri-manual Advanced Manipulation and Cutting of Deformable Objects](https://arxiv.org/abs/2511.11223)
*Sverre Herland,Amit Parag,Elling Ruud Øye,Fangyi Zhang,Fouad Makiyeh,Aleksander Lillienskiold,Abhaya Pal Singh,Edward H. Adelson,Francois Chaumette,Alexandre Krupa,Peter Corke,Ekrem Misimi*

Main category: cs.RO

TL;DR: The paper introduces Sashimi-Bot, a system using multi-robot coordination and reinforcement learning to autonomously cut and handle deformable salmon loins.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of robotic manipulation for natural deformable objects with variability, uncertainties, and frailness.

Method: Developed a multi-robot system combining deep reinforcement learning, in-hand tool manipulation, and sensory feedback for stability and cutting.

Result: The Sashimi-Bot successfully handles and slices deformable salmon loins, overcoming variability and uncertainties.

Conclusion: The system establishes progress in robotic manipulation of deformable objects, enabling potential applications in various fields.

Abstract: Advanced robotic manipulation of deformable, volumetric objects remains one of the greatest challenges due to their pliancy, frailness, variability, and uncertainties during interaction. Motivated by these challenges, this article introduces Sashimi-Bot, an autonomous multi-robotic system for advanced manipulation and cutting, specifically the preparation of sashimi. The objects that we manipulate, salmon loins, are natural in origin and vary in size and shape, they are limp and deformable with poorly characterized elastoplastic parameters, while also being slippery and hard to hold. The three robots straighten the loin; grasp and hold the knife; cut with the knife in a slicing motion while cooperatively stabilizing the loin during cutting; and pick up the thin slices from the cutting board or knife blade. Our system combines deep reinforcement learning with in-hand tool shape manipulation, in-hand tool cutting, and feedback of visual and tactile information to achieve robustness to the variabilities inherent in this task. This work represents a milestone in robotic manipulation of deformable, volumetric objects that may inspire and enable a wide range of other real-world applications.

</details>


### [214] [Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2511.11298)
*Yihao Zhang,Yuankai Qi,Xi Zheng*

Main category: cs.RO

TL;DR: This paper benchmarks four VLA models for manipulation tasks, evaluating their accuracy, adaptability, and language instruction-following ability in realistic settings.


<details>
  <summary>Details</summary>
Motivation: Real-world evaluations and comparisons between VLA models for robotic manipulation are scarce. This paper aims to provide empirical insights and systematic benchmarking.

Method: The authors benchmark four VLA models (ACT, OpenVLA-OFT, RDT-1B, π₀) using a standardized framework across simulation and real-world manipulation tasks on the ALOHA Mobile platform.

Result: π₀ showed superior adaptability in out-of-distribution tasks, while ACT excelled at in-distribution stability. The study identified differences in computational demands and failure modes among the models.

Conclusion: The results highlight practical trade-offs in VLA architectures, offering guidance for balancing precision, generalization, and deployment cost in robotics applications.

Abstract: Foundation models applied in robotics, particularly \textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \textbf{empirical experiences} from benchmarking four representative VLAs -- \textbf{ACT}, \textbf{OpenVLA--OFT}, \textbf{RDT-1B}, and \boldmath{$π_0$} -- across four manipulation tasks conducted in both simulation and on the \textbf{ALOHA Mobile} platform. We establish a \textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \textit{accuracy and efficiency} (success rate and time-to-success), (2) \textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \textit{language instruction-following accuracy}. Through this process, we observe that \boldmath{$π_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.

</details>


### [215] [Simulating an Autonomous System in CARLA using ROS 2](https://arxiv.org/abs/2511.11310)
*Joseph Abdo,Aditya Shibu,Moaiz Saeed,Abdul Maajid Aga,Apsara Sivaprazad,Mohamed Al-Musleh*

Main category: cs.RO

TL;DR: This paper introduces a ROS 2-based autonomous software stack tested in CARLA simulator to achieve competitive autonomous racing performance in FS-AI 2025.


<details>
  <summary>Details</summary>
Motivation: Designing a robust autonomous system that excels under high-speed and uncertain conditions in racing scenarios.

Method: Leverages sensors like LiDAR, stereo cameras, GNSS, IMU with ROS 2 for cone detection, trajectory optimization, and CARLA validation before hardware implementation.

Result: The system detects cones up to 35 meters, computes optimized trajectories, and is validated in simulation and hardware setups.

Conclusion: The proposed system demonstrates competitive autonomous racing capabilities, ready for real-world implementation in FS-AI 2025.

Abstract: Autonomous racing offers a rigorous setting to stress test perception, planning, and control under high speed and uncertainty. This paper proposes an approach to design and evaluate a software stack for an autonomous race car in CARLA: Car Learning to Act simulator, targeting competitive driving performance in the Formula Student UK Driverless (FS-AI) 2025 competition. By utilizing a 360° light detection and ranging (LiDAR), stereo camera, global navigation satellite system (GNSS), and inertial measurement unit (IMU) sensor via ROS 2 (Robot Operating System), the system reliably detects the cones marking the track boundaries at distances of up to 35 m. Optimized trajectories are computed considering vehicle dynamics and simulated environmental factors such as visibility and lighting to navigate the track efficiently. The complete autonomous stack is implemented in ROS 2 and validated extensively in CARLA on a dedicated vehicle (ADS-DV) before being ported to the actual hardware, which includes the Jetson AGX Orin 64GB, ZED2i Stereo Camera, Robosense Helios 16P LiDAR, and CHCNAV Inertial Navigation System (INS).

</details>


### [216] [SimTac: A Physics-Based Simulator for Vision-Based Tactile Sensing with Biomorphic Structures](https://arxiv.org/abs/2511.11456)
*Xuyang Zhang,Jiaqi Jiang,Zhuo Chen,Yongqiang Zhao,Tianqi Yang,Daniel Fernandes Gomes,Jianan Wang,Shan Luo*

Main category: cs.RO

TL;DR: The paper introduces SimTac, a simulation framework for designing biomorphic tactile sensors inspired by biological structures, offering enhanced versatility and practical applications.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current vision-based tactile sensors in robotics, which are predominantly limited to simple planar geometries and lack biomorphic designs.

Method: The authors present SimTac, encompassing particle-based deformation modeling, light-field rendering for tactile image generation, and a neural network for predicting mechanical responses, ensuring efficient simulation across diverse geometries and materials.

Result: SimTac successfully validated biomorphic tactile sensor prototypes, demonstrating its effectiveness in tasks like object classification, slip detection, and contact safety assessment, bridging the gap between bio-inspiration and practical use.

Conclusion: SimTac expands the design possibilities for tactile sensors, integrating morphology and sensing to enable robust interactions in unstructured environments.

Abstract: Tactile sensing in biological organisms is deeply intertwined with morphological form, such as human fingers, cat paws, and elephant trunks, which enables rich and adaptive interactions through a variety of geometrically complex structures. In contrast, vision-based tactile sensors in robotics have been limited to simple planar geometries, with biomorphic designs remaining underexplored. To address this gap, we present SimTac, a physics-based simulation framework for the design and validation of biomorphic tactile sensors. SimTac consists of particle-based deformation modeling, light-field rendering for photorealistic tactile image generation, and a neural network for predicting mechanical responses, enabling accurate and efficient simulation across a wide range of geometries and materials. We demonstrate the versatility of SimTac by designing and validating physical sensor prototypes inspired by biological tactile structures and further demonstrate its effectiveness across multiple Sim2Real tactile tasks, including object classification, slip detection, and contact safety assessment. Our framework bridges the gap between bio-inspired design and practical realisation, expanding the design space of tactile sensors and paving the way for tactile sensing systems that integrate morphology and sensing to enable robust interaction in unstructured environments.

</details>


### [217] [Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective](https://arxiv.org/abs/2511.11478)
*Nhat Chung,Taisei Hanyu,Toan Nguyen,Huy Le,Frederick Bumgarner,Duy Minh Ho Nguyen,Khoa Vo,Kashu Yamazaki,Chase Rainwater,Tung Kieu,Anh Nguyen,Ngan Le*

Main category: cs.RO

TL;DR: The paper presents LIBERO-Mem, a task suite for robotic manipulation in non-Markovian settings, and introduces Embodied-SlotSSM, a framework for scalable temporal reasoning in object-centric robotics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge in non-Markovian robotic manipulation tasks where decision cues are hidden in object-specific histories, requiring memory of prior interactions for effective decision-making.

Method: The authors introduce LIBERO-Mem, a task suite with complex scenarios and propose Embodied-SlotSSM, a slot-centric vision-language-action (VLA) framework. This framework uses spatio-temporally consistent slot identities, slot-state-space modeling, and a relational encoder for scalable context-aware action prediction.

Result: Embodied-SlotSSM demonstrates baseline performance on LIBERO-Mem and other general tasks, showcasing its capability for non-Markovian reasoning and temporal scalability.

Conclusion: The proposed framework and task suite provide a robust foundation for studying and improving robotic manipulation policies in object-centric, non-Markovian settings, addressing previously intractable scalability challenges.

Abstract: As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.

</details>


### [218] [A Comparative Evaluation of Prominent Methods in Autonomous Vehicle Certification](https://arxiv.org/abs/2511.11484)
*Mustafa Erdem Kırmızıgül,Hasan Feyzi Doğruyol,Haluk Bayram*

Main category: cs.RO

TL;DR: This paper examines and compares methods for certifying self-driving vehicles, proposing a pipeline for their certification process.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for effective certification methods to ensure the safety and compliance of self-driving vehicles, aligned with the Vision Zero policy to eliminate traffic fatalities and injuries.

Method: The authors conduct a comparative evaluation of existing certification methods, develop a certification pipeline, and specify its stages, actors, and applicable areas.

Result: The study identifies and analyzes prominent methods for autonomous vehicle certification and proposes a structured pipeline for the certification process.

Conclusion: Efficient certification methods are crucial for self-driving vehicle deployment under Vision Zero goals, and the proposed pipeline and evaluated methods provide a structured framework for ensuring vehicle safety and compliance.

Abstract: The "Vision Zero" policy, introduced by the Swedish Parliament in 1997, aims to eliminate fatalities and serious injuries resulting from traffic accidents. To achieve this goal, the use of self-driving vehicles in traffic is envisioned and a roadmap for the certification of self-driving vehicles is aimed to be determined. However, it is still unclear how the basic safety requirements that autonomous vehicles must meet will be verified and certified, and which methods will be used. This paper focuses on the comparative evaluation of the prominent methods planned to be used in the certification process of autonomous vehicles. It examines the prominent methods used in the certification process, develops a pipeline for the certification process of autonomous vehicles, and determines the stages, actors, and areas where the addressed methods can be applied.

</details>


### [219] [Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities](https://arxiv.org/abs/2511.11512)
*Yiyun Zhou,Mingjing Xu,Jingwei Shi,Quanjiang Li,Jingyuan Chen*

Main category: cs.RO

TL;DR: The paper presents TLV-CoRe, a method to standardize tactile sensing aligned with language and vision modalities by improving feature integration and sensor generalization.


<details>
  <summary>Details</summary>
Motivation: Existing tactile sensors lack standardization, leading to redundant features, and current methods do not fully integrate tactile, language, and vision modalities.

Method: TLV-CoRe uses a Sensor-Aware Modulator for unifying tactile features, introduces tactile-irrelevant decoupled learning, and employs a Unified Bridging Adapter for tri-modal interaction enhancement.

Result: TLV-CoRe demonstrates improved sensor-agnostic representation learning and cross-modal alignment in experiments.

Conclusion: The approach provides a new direction for multimodal tactile representation, addressing feature standardization and better integration across sensors and modalities.

Abstract: Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.

</details>


### [220] [Scalable Coverage Trajectory Synthesis on GPUs as Statistical Inference](https://arxiv.org/abs/2511.11514)
*Max M. Sun,Jueun Kwon,Todd Murphey*

Main category: cs.RO

TL;DR: The paper offers a scalable and computationally efficient approach to coverage motion planning using flow-matching-based statistical inference and GPU-friendly parallelization.


<details>
  <summary>Details</summary>
Motivation: Current coverage motion planning methods struggle with efficiency and scalability due to reasoning over full spatial trajectories instead of temporal state sequences, limiting their applicability to modern parallel architectures.

Method: The authors formulated coverage motion planning as a statistical inference problem using flow matching, integrating statistical measures (e.g., KL divergence, Sinkhorn divergence) with linear quadratic regulators. The approach decouples trajectory gradient generation from control synthesis and leverages parallel processing using GPUs.

Result: The proposed framework achieves significant computational gains through parallelization compared to traditional waypoint-tracking-based methods.

Conclusion: This method enhances the scalability and speed of coverage motion planning, making it a promising solution for modern robotic applications that require efficient and parallelizable computations.

Abstract: Coverage motion planning is essential to a wide range of robotic tasks. Unlike conventional motion planning problems, which reason over temporal sequences of states, coverage motion planning requires reasoning over the spatial distribution of entire trajectories, making standard motion planning methods limited in computational efficiency and less amenable to modern parallelization frameworks. In this work, we formulate the coverage motion planning problem as a statistical inference problem from the perspective of flow matching, a generative modeling technique that has gained significant attention in recent years. The proposed formulation unifies commonly used statistical discrepancy measures, such as Kullback-Leibler divergence and Sinkhorn divergence, with a standard linear quadratic regulator problem. More importantly, it decouples the generation of trajectory gradients for coverage from the synthesis of control under nonlinear system dynamics, enabling significant acceleration through parallelization on modern computational architectures, particularly Graphics Processing Units (GPUs). This paper focuses on the advantages of this formulation in terms of scalability through parallelization, highlighting its computational benefits compared to conventional methods based on waypoint tracking.

</details>


### [221] [Scalable Policy Evaluation with Video World Models](https://arxiv.org/abs/2511.11520)
*Wei-Cheng Tseng,Jinwei Gu,Qinsheng Zhang,Hanzi Mao,Ming-Yu Liu,Florian Shkurti,Lin Yen-Chen*

Main category: cs.RO

TL;DR: The paper explores using action-conditional video generation models as an efficient, scalable method for evaluating robotic manipulation policies without real-world interactions.


<details>
  <summary>Details</summary>
Motivation: The current evaluation of robotic manipulation policies is expensive, time-consuming, and carries safety risks, with manual simulation setups requiring significant engineering effort and suffering from physics and rendering inconsistencies.

Method: The approach integrates action conditioning into pre-trained video generation models, utilizing internet-scale online videos, which removes reliance on large paired video-action datasets for policy evaluation.

Result: The experiments show that this method improves policy evaluation in metrics such as policy ranking accuracy and matching predicted values with real-world policy outcomes, reducing the need for real-world testing.

Conclusion: Using action-conditional video models is a promising, efficient alternative for robotic policy evaluation, addressing challenges in traditional real-world or simulated testing approaches.

Abstract: Training generalist policies for robotic manipulation has shown great promise, as they enable language-conditioned, multi-task behaviors across diverse scenarios. However, evaluating these policies remains difficult because real-world testing is expensive, time-consuming, and labor-intensive. It also requires frequent environment resets and carries safety risks when deploying unproven policies on physical robots. Manually creating and populating simulation environments with assets for robotic manipulation has not addressed these issues, primarily due to the significant engineering effort required and the often substantial sim-to-real gap, both in terms of physics and rendering. In this paper, we explore the use of action-conditional video generation models as a scalable way to learn world models for policy evaluation. We demonstrate how to incorporate action conditioning into existing pre-trained video generation models. This allows leveraging internet-scale in-the-wild online videos during the pre-training stage, and alleviates the need for a large dataset of paired video-action data, which is expensive to collect for robotic manipulation. Our paper examines the effect of dataset diversity, pre-trained weight and common failure cases for the proposed evaluation pipeline.Our experiments demonstrate that, across various metrics, including policy ranking and the correlation between actual policy values and predicted policy values, these models offer a promising approach for evaluating policies without requiring real-world interactions.

</details>


### [222] [Terrain Costmap Generation via Scaled Preference Conditioning](https://arxiv.org/abs/2511.11529)
*Luisa Mao,Garret Warnell,Peter Stone,Joydeep Biswas*

Main category: cs.RO

TL;DR: The paper introduces SPACER, a novel method for generating costmaps that enables both effective generalization to diverse terrains and rapid test-time adaptation for autonomous robot navigation.


<details>
  <summary>Details</summary>
Motivation: Autonomous robots require terrain costmaps that generalize across terrains and adapt quickly at test time for mission-specific needs, which existing approaches fail to effectively balance.

Method: SPACER leverages synthetic training data and incorporates scaled preference conditioning, enabling the generation of costmaps that generalize well to varying terrains and rapidly adapt by user preferences.

Result: SPACER surpasses existing methods in creating costmaps for terrain navigation, achieving the lowest regret across multiple environments in global path planning tests using aerial maps.

Conclusion: The study demonstrates SPACER as an effective solution for terrain costmap generation, improving navigation performance under diverse conditions and user preferences.

Abstract: Successful autonomous robot navigation in off-road domains requires the ability to generate high-quality terrain costmaps that are able to both generalize well over a wide variety of terrains and rapidly adapt relative costs at test time to meet mission-specific needs. Existing approaches for costmap generation allow for either rapid test-time adaptation of relative costs (e.g., semantic segmentation methods) or generalization to new terrain types (e.g., representation learning methods), but not both. In this work, we present scaled preference conditioned all-terrain costmap generation (SPACER), a novel approach for generating terrain costmaps that leverages synthetic data during training in order to generalize well to new terrains, and allows for rapid test-time adaptation of relative costs by conditioning on a user-specified scaled preference context. Using large-scale aerial maps, we provide empirical evidence that SPACER outperforms other approaches at generating costmaps for terrain navigation, with the lowest measured regret across varied preferences in five of seven environments for global path planning.

</details>


### [223] [Volumetric Ergodic Control](https://arxiv.org/abs/2511.11533)
*Jueun Kwon,Max M. Sun,Todd Murphey*

Main category: cs.RO

TL;DR: This paper introduces a volumetric state-based ergodic control to address real-world robot interactions, showing improved spatial coverage and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of current ergodic control methods that assume robots as point entities, ignoring their physical volume and interaction with the environment.

Method: The authors propose a novel ergodic control formulation that includes volumetric state representation, which maintains asymptotic guarantees, adds minimal computational burden, and supports varied volumetric models.

Result: Evaluation on diverse tasks demonstrated over twice the efficiency in spatial coverage and consistent 100% task completion, outperforming standard methods.

Conclusion: The proposed volumetric ergodic control method is effective and versatile for real-world tasks, validated by empirical success in search, manipulation, and mechanical erasing experiments.

Abstract: Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [224] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: The study investigates research software engineers' (RSEs) views on peer code review, comparing their perspectives with other software developers and highlighting challenges and improvements.


<details>
  <summary>Details</summary>
Motivation: To enhance research software quality and maintainability by understanding the adoption of peer code review among research software engineers.

Method: Conducted a survey of RSEs with questions aligned with prior research and tailored additions to analyze their perspectives on peer code review.

Result: 61 survey responses revealed both challenges faced by RSEs and insights similar to broader developer groups, highlighting unique practices and obstacles.

Conclusion: Structured processes, better tools, and targeted training can improve the adoption and effectiveness of peer code review in research software development.

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [225] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: The paper proposes a human-in-the-loop approach to improve the evaluation of LLM-based patch validity for Automated Program Repair, achieving substantial agreement and precision.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for evaluating Automated Program Repair rely on methods that fail to capture accurate patch validity, requiring costly manual annotation. A more efficient evaluation method is needed.

Method: The authors introduce a human-in-the-loop process where an LLM generates a bug-specific rubric for patch validity. Humans review and refine the rubric once, and this refined rubric is used by the LLM to judge patch validity.

Result: The method achieves substantial agreement with human consensus (Cohen's kappa 0.75), 94% recall, and 80% precision for patches unanimously agreed upon. It shows potential for improvement on patches with human disagreement (Cohen's kappa 0.57).

Conclusion: The proposed approach demonstrates the potential to improve LLM-based evaluations of patch validity, reducing the cost of manual annotation while achieving high accuracy and agreement with human judgment.

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [226] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: The paper presents a methodology using Large Language Models (LLMs) and conformance checking to detect runtime control-flow anomalies in complex systems by automating source-code instrumentation and analyzing event logs.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of ensuring dependability in complex computer-based systems by detecting runtime control-flow anomalies caused by unexpected behavior or 'unknown unknowns.'

Method: Develops a software monitoring methodology integrating LLMs for automated source-code instrumentation and conformance checking for detecting control-flow deviations.

Result: Tested on the ERTMS/ETCS case study, the methodology achieved up to 84.775% control-flow coverage and 96.610% F1-score/93.515% AUC in anomaly detection.

Conclusion: Leveraging domain-specific knowledge with LLMs enhances reliable software logging and enables effective control-flow anomaly detection using conformance checking.

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [227] [Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair](https://arxiv.org/abs/2511.11012)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: The paper investigates the capabilities of four LLM coding agents in repairing multi-hunk bugs, a challenging yet prevalent issue in real-world systems, and introduces the Maple tool to enhance repair accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in automated program repair research, which has largely ignored the complex problem of multi-hunk bugs, requiring edits across multiple code regions.

Method: The study systematically evaluates the performance of four LLM-driven coding agents (Claude Code, Codex, Gemini-cli, Qwen Code) on 372 multi-hunk bugs using fine-grained metrics to analyze repair accuracy, regression, and operational dynamics. It also proposes the Maple tool to provide repository-level context for improvement.

Result: Claude Code exhibits the highest repair accuracy (93.3%), while others such as Qwen Code lag behind (25.8%). Multi-hunk bug complexity negatively affects performance. Maple enhances Gemini-cli’s repair accuracy by 30%. Failures by agents consume more resources and execution time than successful repairs.

Conclusion: The study highlights significant variability in coding agents' performance for multi-hunk bugs and emphasizes the importance of tools like Maple for improving outcomes, offering new insights into the behavior and optimization of repair agents.

Abstract: Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.

</details>


### [228] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: The paper investigates using Large Language Models (LLMs) for domain-specific software in industrial process automation settings, using few-shot prompting for simple tasks without extensive model training.


<details>
  <summary>Details</summary>
Motivation: To explore the application of LLMs in industrial process automation with proprietary, domain-specific languages, minimizing effort and ensuring sensitive data protection.

Method: Few-shot prompting approaches are evaluated to determine their effectiveness in solving problems with proprietary domain-specific languages.

Result: Few-shot prompting shows promise in solving simple problems in under-supported languages, while maintaining on-premise operations for data security.

Conclusion: Enterprises can leverage LLMs for proprietary contexts through few-shot prompting without extensive domain-specific model training, ensuring data protection.

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [229] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: The paper introduces the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics for 450 open-source projects.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive and time-aware large-scale datasets for analyzing software quality across multiple dimensions.

Method: The authors created SQuaD by integrating metrics from nine static analysis tools, covering 450 projects and 63,586 releases. These metrics are at method, class, file, and project levels.

Result: SQuaD offers over 700 quality metrics along with version control data, issue-tracking histories, software vulnerabilities, and process information. It supports broad empirical research and JIT defect prediction.

Conclusion: SQuaD fills a critical gap in software quality datasets and enables new research opportunities. It is publicly available for further exploration and development.

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [230] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: SCRUTINEER is a novel system designed to identify logic-level usage violations of smart contract reusable components (SCRs), achieving high accuracy in detection.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the increasing problem of logic-level usage violations in smart contract reusable components, which can lead to vulnerabilities due to misaligned business logic.

Method: The approach involves feature extraction, a knowledge base powered by large language models (LLMs), and a detection engine that combines retrieval-augmented generation techniques with similarity-based and conflict checkers.

Result: SCRUTINEER achieved a precision of 80.77%, recall of 82.35%, and F1-score of 81.55% in detecting logic-level usage violations in SCRs during evaluation on 3 datasets.

Conclusion: SCRUTINEER proves effective and robust for identifying logic-level usage violations in SCRs, offering a practical solution to enhance the security and reliability of smart contracts.

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [231] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: Agile methods can be adapted for aerospace safety-critical systems through a tool, CertiA360, which automates traceability and compliance with DO-178C standards.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in integrating flexible Agile methods into the strict regulatory frameworks of aerospace software development that demand robust documentation and traceability.

Method: The authors propose CertiA360, a tool developed with industry feedback, designed to automate traceability and manage regulatory compliance in Agile software development.

Result: Validation with industry experts showed CertiA360 reduces manual effort, enables agility, and ensures compliance with DO-178C standards despite not being qualified under DO-330.

Conclusion: The study concludes that tailored Agile methods, supported by tools such as CertiA360, can coexist with strict safety system regulations, improving efficiency in aerospace software development.

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [232] [Habit learning is associated with efficiently controlled network dynamics in naive macaque monkeys](https://arxiv.org/abs/2511.10757)
*Julia K. Brynildsen,Panagiotis Fotiadis,Karol P. Szymula,Jason Z. Kim,Fabio Pasqualetti,Ann M. Graybiel,Theresa M. Desrochers,Dani S. Bassett*

Main category: q-bio.NC

TL;DR: This paper develops a theory explaining how brain states influence sequential behavior and validates it with experimental data on brain activity during a motor habit task in primates.


<details>
  <summary>Details</summary>
Motivation: To better understand the neural mechanisms behind habit formation in uncertain environments and how brain states shape sequential behavior.

Method: The authors propose a formal theory of network energetics to predict energy requirements for transitions between brain states and validate it using multi-unit recordings from macaques performing motor tasks.

Result: The study found that habit formation correlates with reduced energy requirements for specific transitions, ruled out directional tuning confounds, and showed robustness using virtual lesion simulations.

Conclusion: This research offers a framework for linking brain state dynamics with behavior, aiding the study of distributed neural circuitry's role in habit formation.

Abstract: Primates utilize distributed neural circuits to learn habits in uncertain environments, but the underlying mechanisms remain poorly understood. We propose a formal theory of network energetics explaining how brain states influence sequential behavior. We test our theory on multi-unit recordings from the caudate nucleus and cortical regions of macaques performing a motor habit task. The theory predicts the energy required to transition between brain states represented by trial-specific firing rates across channels, assuming activity spreads through effective connections. We hypothesized that habit formation would correlate with lower control energy. Consistent with this, we observed smaller energy requirements for transitions between similar saccade patterns and those of intermediate complexity, and sessions exploiting fewer patterns. Simulations ruled out confounds from neurons' directional tuning. Finally, virtual lesioning demonstrated robustness of observed relationships between control energy and behavior. This work paves the way for examining how behavior arises from changing activity in distributed circuitry.

</details>


### [233] [A universal theorem of sensory information](https://arxiv.org/abs/2511.11463)
*Willy Wong*

Main category: q-bio.NC

TL;DR: The study derives a universal law for sensory information, akin to thermodynamics, demonstrating sensory adaptation inequality and linking firing rates to thermodynamic principles.


<details>
  <summary>Details</summary>
Motivation: To find a universal principle for sensory information processing analogous to thermodynamic laws, providing insights into neural firing and adaptation.

Method: Utilizes Shannon's information measure and a state-space representation of firing rate to derive sensory adaptation and thermodynamic-like behavior in neurons.

Result: Confirms sensory adaptation inequality across modalities, species, and tests, and establishes that neuron firing rates resemble thermodynamic state functions.

Conclusion: The firing rate's state function-like behavior leads to an entropy production equation, proving that sensory information gains are non-negative in closed stimulation cycles.

Abstract: A universal theorem of sensory information, analogous to the second law of thermodynamics, is derived. Beginning from a minimal description of a sensory neuron, a state-space representation of firing rate emerges naturally from Shannon's measure of information. A special case of this formulation predicts a previously unknown inequality governing sensory adaptation, which was confirmed across different modalities, species, and experimental conditions. Further analysis shows that the firing rate behaves like a state function in thermodynamics, leading to an entropy production equation from which a general law follows: any closed cycle of stimulation yields a non-negative net gain of sensory information.

</details>


### [234] [Inferring response times of perceptual decisions with Poisson variational autoencoders](https://arxiv.org/abs/2511.11480)
*Hayden R. Johnson,Anastasia N. Krouglova,Hadi Vafaii,Jacob L. Yates,Pedro J. Gonçalves*

Main category: q-bio.NC

TL;DR: This paper proposes an image-computable model incorporating efficient sensory encoding and Bayesian decoding for perceptual decision making.


<details>
  <summary>Details</summary>
Motivation: To model temporal dynamics and trial-by-trial patterns in perceptual decision making, overlooked by traditional deep neural networks.

Method: A Poisson variational autoencoder is used for unsupervised sensory representation, coupled with a Bayesian decoder and entropy-based stopping rule to simulate choices and response times.

Result: The model successfully replicates empirical decision-making behavior, including variability, response time characteristics (Hick’s law), and speed-accuracy trade-offs in MNIST classification.

Conclusion: This approach advances understanding of perceptual decision-making by integrating sensory encoding and dynamic Bayesian inference into a principled computational model.

Abstract: Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [235] [Neural Local Wasserstein Regression](https://arxiv.org/abs/2511.10824)
*Inga Girshfeld,Xiaohui Chen*

Main category: stat.ML

TL;DR: The paper introduces Neural Local Wasserstein Regression, a model using localized transport maps for regression between distributions, overcoming limitations of global and linearized approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of global transport maps and linearization assumptions in distribution-on-distribution regression by introducing more flexible and geometry-preserving methods.

Method: A nonparametric regression model using kernel weights based on Wasserstein distances for localizing estimators, neural networks for transport operators, and scalable architecture including DeepSets and Sinkhorn loss.

Result: The proposed method outperforms existing techniques in capturing nonlinear, high-dimensional distributional relationships via experiments on synthetic and MNIST datasets.

Conclusion: The model expands the flexibility and accuracy of distributional regression, ensuring broader applicability to complex data geometries.

Abstract: We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods.

</details>


### [236] [Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data](https://arxiv.org/abs/2511.10919)
*Jialei Liu,Jun Liao,Kuangnan Fang*

Main category: stat.ML

TL;DR: The paper proposed a framework for PU learning using model averaging and transfer learning to integrate heterogeneous data sources without sharing. It ensures theoretical guarantees and proves effectiveness through simulations and real-world applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of PU learning in high-stakes domains, where explicitly labeled negative samples are unavailable and privacy constraints limit direct data sharing.

Method: The method uses tailored logistic regression models for different domain types, transferring knowledge through model averaging, with optimal weights derived using cross-validation minimizing Kullback-Leibler divergence. Extensions for high-dimensional settings are included.

Result: The proposed framework demonstrated better predictive accuracy and robustness compared to existing methods, in both simulated and real-world credit risk analyses.

Conclusion: The method is effective in PU target domains, excelling in environments with limited labeled data and heterogeneous sources, with theoretical support for its optimality and convergence guarantees.

Abstract: Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments.

</details>


### [237] [Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths](https://arxiv.org/abs/2511.11161)
*Yuzhen Zhao,Yating Liu,Marc Hoffmann*

Main category: stat.ML

TL;DR: The paper develops a neural network-based estimator for drift function estimation in diffusion processes using high-frequency independent trajectory data and achieves improved convergence rates compared to $B$-spline methods.


<details>
  <summary>Details</summary>
Motivation: To enhance estimation techniques for drift functions in time-homogeneous diffusion processes, particularly by improving convergence rates and capturing local features in higher dimensions.

Method: A neural network-based estimator is proposed, with derived non-asymptotic convergence rates and explicit analysis for compositional drift functions.

Result: The proposed estimator demonstrates superior empirical convergence rates, particularly independent of input dimension $d$. It outperforms the $B$-spline method in capturing local features, especially in high-dimensional contexts.

Conclusion: The neural network-based approach is effective for drift function estimation in diffusion processes, offering better performance and scalability for high-dimensional data.

Abstract: This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.

</details>


### [238] [Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint](https://arxiv.org/abs/2511.11294)
*Bertille Tierny,Arthur Charpentier,François Hu*

Main category: stat.ML

TL;DR: The paper introduces a post-processing framework for linear models to analyze and interpret fairness dynamics under demographic parity constraints, enhancing model auditing and correction without retraining.


<details>
  <summary>Details</summary>
Motivation: Fairness constraints like demographic parity can obscure how bias is distributed across features in linear models. Existing approaches either rely on unrealistic assumptions or fail to address the explicit role of sensitive attributes, making it challenging to fairly assess the models.

Method: The authors propose a post-processing framework that works on any linear model. This framework decomposes bias into direct (sensitive-attribute-related) and indirect (correlated-feature-related) components and analyzes how demographic parity impacts model coefficients without retraining.

Result: The proposed method provides analytical insights into fairness interventions at the feature level and demonstrates its effectiveness on synthetic and real-world datasets, identifying fairness dynamics that prior methods cannot capture.

Conclusion: The framework offers a practical tool for auditing and mitigating fairness-related issues in linear models, improving their transparency, utility, and responsibility in decision-making settings.

Abstract: Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [239] [DualVision ArthroNav: Investigating Opportunities to Enhance Localization and Reconstruction in Image-based Arthroscopy Navigation via External Cameras](https://arxiv.org/abs/2511.10699)
*Hongchao Shu,Lalithkumar Seenivasan,Mingxu Liu,Yunseo Hwang,Yu-Chun Ku,Jonathan Knopf,Alejandro Martin-Gomez,Mehran Armand,Mathias Unberath*

Main category: eess.IV

TL;DR: This paper introduces DualVision ArthroNav, a navigation system combining an external camera with an arthroscope, providing robust localization, reducing errors, and improving scene reconstruction for arthroscopic procedures.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing optical tracking and monocular vision-based systems in arthroscopy, which have workspace constraints, drift, scale ambiguity, and sensitivity to motion or occlusions.

Method: The authors developed a multi-camera navigation system that integrates an external camera for stable odometry and localization with the conventional monocular arthroscope for dense scene reconstruction.

Result: The system achieves an average trajectory error of 1.09 mm and target registration error of 2.16 mm with high visual fidelity metrics (SSIM = 0.69, PSNR = 22.19).

Conclusion: DualVision ArthroNav bridges the gap between optical tracking and vision-based systems, providing a cost-efficient, practical, and reliable solution for arthroscopic navigation suitable for clinical deployment.

Abstract: Arthroscopic procedures can greatly benefit from navigation systems that enhance spatial awareness, depth perception, and field of view. However, existing optical tracking solutions impose strict workspace constraints and disrupt surgical workflow. Vision-based alternatives, though less invasive, often rely solely on the monocular arthroscope camera, making them prone to drift, scale ambiguity, and sensitivity to rapid motion or occlusion. We propose DualVision ArthroNav, a multi-camera arthroscopy navigation system that integrates an external camera rigidly mounted on the arthroscope. The external camera provides stable visual odometry and absolute localization, while the monocular arthroscope video enables dense scene reconstruction. By combining these complementary views, our system resolves the scale ambiguity and long-term drift inherent in monocular SLAM and ensures robust relocalization. Experiments demonstrate that our system effectively compensates for calibration errors, achieving an average absolute trajectory error of 1.09 mm. The reconstructed scenes reach an average target registration error of 2.16 mm, with high visual fidelity (SSIM = 0.69, PSNR = 22.19). These results indicate that our system provides a practical and cost-efficient solution for arthroscopic navigation, bridging the gap between optical tracking and purely vision-based systems, and paving the way toward clinically deployable, fully vision-based arthroscopic guidance.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [240] [Dual Riemannian Newton Method on Statistical Manifolds](https://arxiv.org/abs/2511.11318)
*Derun Zhou,Keisuke Yano,Mahito Sugiyama*

Main category: stat.CO

TL;DR: The paper introduces a novel second-order optimization method called the Dual Riemannian Newton method, leveraging dual affine connections for improved efficiency in parameter estimation on manifolds.


<details>
  <summary>Details</summary>
Motivation: Existing optimization methods on parameter manifolds are either slow (like first-order methods such as natural gradient) or overlook critical dual-connection structures in information geometry.

Method: The authors propose a Dual Riemannian Newton method, which employs the interplay between dual affine connections and second-order updates, ensuring compatibility with the geometry while improving convergence speeds.

Result: The paper demonstrates local quadratic convergence for the proposed method and validates its effectiveness through experiments on various statistical models.

Conclusion: The Dual Riemannian Newton method achieves second-order efficiency, respects the dual structures foundational to information-geometric learning, and improves optimization processes compared to traditional methods.

Abstract: In probabilistic modeling, parameter estimation is commonly formulated as a minimization problem on a parameter manifold. Optimization in such spaces requires geometry-aware methods that respect the underlying information structure. While the natural gradient leverages the Fisher information metric as a form of Riemannian gradient descent, it remains a first-order method and often exhibits slow convergence near optimal solutions. Existing second-order manifold algorithms typically rely on the Levi-Civita connection, thus overlooking the dual-connection structure that is central to information geometry. We propose the dual Riemannian Newton method, a Newton-type optimization algorithm on manifolds endowed with a metric and a pair of dual affine connections. The dual Riemannian Newton method explicates how duality shapes second-order updates: when the retraction (a local surrogate of the exponential map) is defined by one connection, the associated Newton equation is posed with its dual. We establish local quadratic convergence and validate the theory with experiments on representative statistical models. Thus, the dual Riemannian Newton method thus delivers second-order efficiency while remaining compatible with the dual structures that underlie modern information-geometric learning and inference.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [241] [A Compilation Framework for Quantum Circuits with Mid-Circuit Measurement Error Awareness](https://arxiv.org/abs/2511.10921)
*Ming Zhong,Zhemin Zhang,Xiangyu Ren,Chenghong Zhu,Siyuan Niu,Zhiding Liang*

Main category: quant-ph

TL;DR: This paper presents MERA, a novel compilation framework improving qubit fidelity for mid-circuit measurements (MCM) by addressing their unique error sources.


<details>
  <summary>Details</summary>
Motivation: The motivation is to combat MCM-related errors, such as crosstalk, decoherence, and reset infidelity, which are not accounted for in existing compilers and result in significant performance losses in quantum circuits.

Method: MERA uses lightweight profiling for per-qubit MCM error distribution and implements error-aware qubit mapping, SWAP insertions, and dynamic decoupling in layout, routing, and scheduling steps.

Result: MERA improves circuit fidelity by 24.94%-52.00% over Qiskit (level 3) compiler and by an average of 29.26%, with a maximum of 122.58% improvement, compared to QR-Map.

Conclusion: MERA effectively addresses MCM-specific errors and significantly enhances fidelity in quantum circuits dominated by mid-circuit measurement operations.

Abstract: Mid-circuit measurement (MCM) provides the capability for qubit reuse and dynamic control in quantum processors, enabling more resource-efficient algorithms and supporting error-correction procedures. However, MCM introduces several sources of error, including measurement-induced crosstalk, idling-qubit decoherence, and reset infidelity, and these errors exhibit pronounced qubit-dependent variability within a single device. Since existing compilers such as the Qiskit-compiler and QR-Map (the state-of-art qubit reuse compiler) do not account for this variability, circuits with frequent MCM operations often experience substantial fidelity loss.
  In thie paper, we propose MERA, a compilation framework that performs MCM-error-aware layout, routing, and scheduling. MERA leverages lightweight profiling to obtain a stable per-qubit MCM error distribution, which it uses to guide error-aware qubit mapping and SWAP insertions. To further mitigate MCM-related decoherence and crosstalk, MERA augments as-late-as-possible scheduling with context-aware dynamic decoupling. Evaluated on 27 benchmark circuits, MERA achieves 24.94% -- 52.00% fidelity improvement over the Qiskit compiler (optimization level 3) without introducing additional overhead. On QR-Map-generated circuits, it improves fidelity by 29.26% on average and up to 122.58% in the best case, demonstrating its effectiveness for dynamic circuits dominated by MCM operations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [242] [Drone Swarm Energy Management](https://arxiv.org/abs/2511.11557)
*Michael Z. Zgurovsky,Pavlo O. Kasyanov,Liliia S. Paliichuk*

Main category: math.OC

TL;DR: The paper introduces a drone swarm decision-making framework combining POMDP with DDPG reinforcement learning for energy-efficient and adaptive control under uncertainty.


<details>
  <summary>Details</summary>
Motivation: To enhance the decision-making efficacy and autonomy of UAV swarms in dynamic and uncertain environments, ensuring efficient energy usage and robust navigation.

Method: Integration of POMDP with an extended DDPG architecture that incorporates belief-state representation derived from Bayesian filtering to handle partially observable environments. Performance comparisons are conducted with simulations.

Result: The proposed model improves mission success rates and energy efficiency, surpassing baseline methods in performance for swarm control tasks.

Conclusion: The framework enables scalable, energy-aware, and intelligent multi-agent swarm operations, applicable across various scenarios such as security, environmental monitoring, and infrastructure inspection.

Abstract: This note presents an analytical framework for decision-making in drone swarm systems operating under uncertainty, based on the integration of Partially Observable Markov Decision Processes (POMDP) with Deep Deterministic Policy Gradient (DDPG) reinforcement learning. The proposed approach enables adaptive control and cooperative behavior of unmanned aerial vehicles (UAVs) within a cognitive AI platform, where each agent learns optimal energy management and navigation policies from dynamic environmental states. We extend the standard DDPG architecture with a belief-state representation derived from Bayesian filtering, allowing for robust decision-making in partially observable environments. In this paper, for the Gaussian case, we numerically compare the performance of policies derived from DDPG to optimal policies for discretized versions of the original continuous problem. Simulation results demonstrate that the POMDP-DDPG-based swarm control model significantly improves mission success rates and energy efficiency compared to baseline methods. The developed framework supports distributed learning and decision coordination across multiple agents, providing a foundation for scalable cognitive swarm autonomy. The outcomes of this research contribute to the advancement of energy-aware control algorithms for intelligent multi-agent systems and can be applied in security, environmental monitoring, and infrastructure inspection scenarios.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [243] [AI as a component in the action research tradition of learning-by-doing](https://arxiv.org/abs/2511.11445)
*Ian Benson,Alexei Semenov*

Main category: cs.CY

TL;DR: This paper suggests a student-centered, dialogical approach to mathematics education that integrates digital tools, contrasting it with traditional instructional methods.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in traditional math education models and promote professionalism and engagement in mathematical learning.

Method: Introducing a learning model focused on self-awareness, dialogue, inquiry, and integration of digital technologies, emphasizing professional mathematic techniques.

Result: Encouraged active engagement through dialogue between learners and mentors, supported by digital tools such as development environments and AI.

Conclusion: The framework offers a promising alternative to industrial-age educational models by fostering deeper understanding and interaction in math education.

Abstract: We consider learning mathematics through action research, hacking, discovery, inquiry, learning-by-doing as opposed to the instruct and perform, industrial model of the 19th century. A learning model based on self-awareness, types, functions, structured drawing and formal diagrams addresses the weaknesses of drill and practice and the pitfalls of statistical prediction with Large Language Models.
  In other words, we build mathematics/informatics education on the activity of a professional mathematician in mathematical modelling and designing programs. This tradition emphasises the role of dialogue and doing mathematics. In the Language/Action approach the teacher designs mathematising situations that scaffold previously encountered, or not-known-how-to-solve problems for the learner while teachers and teacher/interlocutors supervise the process.
  A critical feature is the written-oral dialogue between the learner and the teacher. As a rule, this is 1 to 1 communication. The role of the teacher/interlocutor, a more knowledgeable other, is mostly performed by a more senior student, 1 per 5 to 7 pupils. After Doug Engelbart we propose the metaphor of human intellect augmented by digital technologies such as interactive development environments or AI. Every human has their bio and digital parts. The bio part of the learner reacts to their work through dialogue in the mind. The digital part poses questions, interprets code and proposes not necessarily sound ideas.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [244] [Learning bounds for doubly-robust covariate shift adaptation](https://arxiv.org/abs/2511.11003)
*Jeonghwan Lee,Cong Ma*

Main category: math.ST

TL;DR: This paper addresses the lack of non-asymptotic guarantees for the doubly-robust (DR) estimator in covariate shift adaptation by providing high-probability bounds on its excess risk and analyzing its behavior under well-specified models.


<details>
  <summary>Details</summary>
Motivation: Covariate shift, where the distribution of input features changes between training and testing domains while the conditional distribution remains constant, poses a challenge for machine learning. There is a need to establish non-asymptotic guarantees for the recently introduced DR estimator.

Method: The authors derive non-asymptotic learning bounds by analyzing the DR estimator's excess risk with high-probability bounds, considering the Rademacher complexity, and leveraging Fisher information for well-specified parameterized models.

Result: The paper provides structure-agnostic high-probability bounds for the DR estimator and analyzes its performance under well-specified models using Fisher information mismatch. These results combine asymptotic efficiency with finite-sample guarantees.

Conclusion: This work bridges the gap between asymptotic and finite-sample analysis of the DR estimator in covariate shift adaptation, offering robust theoretical insights and practical guarantees.

Abstract: Distribution shift between the training domain and the test domain poses a key challenge for modern machine learning. An extensively studied instance is the \emph{covariate shift}, where the marginal distribution of covariates differs across domains, while the conditional distribution of outcome remains the same. The doubly-robust (DR) estimator, recently introduced by \cite{kato2023double}, combines the density ratio estimation with a pilot regression model and demonstrates asymptotic normality and $\sqrt{n}$-consistency, even when the pilot estimates converge slowly. However, the prior arts has focused exclusively on deriving asymptotic results and has left open the question of non-asymptotic guarantees for the DR estimator.
  This paper establishes the first non-asymptotic learning bounds for the DR covariate shift adaptation. Our main contributions are two-fold: (\romannumeral 1) We establish \emph{structure-agnostic} high-probability upper bounds on the excess target risk of the DR estimator that depend only on the $L^2$-errors of the pilot estimates and the Rademacher complexity of the model class, without assuming specific procedures to obtain the pilot estimate, and (\romannumeral 2) under \emph{well-specified parameterized models}, we analyze the DR covariate shift adaptation based on modern techniques for non-asymptotic analysis of MLE, whose key terms governed by the Fisher information mismatch term between the source and target distributions. Together, these findings bridge asymptotic efficiency properties and a finite-sample out-of-distribution generalization bounds, providing a comprehensive theoretical underpinnings for the DR covariate shift adaptation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [245] [AFLGopher: Accelerating Directed Fuzzing via Feasibility-Aware Guidance](https://arxiv.org/abs/2511.10828)
*Weiheng Bai,Kefu Wu,Qiushi Wu,Kangjie Lu*

Main category: cs.CR

TL;DR: This paper introduces AFLGopher, a directed fuzzing tool that improves efficiency using feasibility-aware distance calculations.


<details>
  <summary>Details</summary>
Motivation: Existing directed fuzzers use feasibility-unaware guiding mechanisms, which are less effective in navigating complex program structures to reach specific targets.

Method: The authors introduced a feasibility-aware distance calculation mechanism, combined with a new classification and runtime feasibility-updating techniques, to guide fuzzing efforts efficiently. AFLGopher was implemented as an enhancement to current directed fuzzers.

Result: Experimental comparisons with state-of-the-art directed fuzzers showed that AFLGopher is significantly faster, achieving up to 5.60x speed improvement in reaching targets and triggering vulnerabilities.

Conclusion: AFLGopher demonstrates improved precision and effectiveness in directed fuzzing with its feasibility-aware guiding approach, offering a valuable advancement for software testing.

Abstract: Directed fuzzing is a useful testing technique that aims to efficiently reach target code sites in a program. The core of directed fuzzing is the guiding mechanism that directs the fuzzing to the specified target. A general guiding mechanism adopted in existing directed fuzzers is to calculate the control-flow distance between the current progress and the target, and use that as feedback to guide the directed fuzzing. A fundamental problem with the existing guiding mechanism is that the distance calculation is \emph{feasibility-unaware}.
  In this work, we propose feasibility-aware directed fuzzing named AFLGopher. Our new feasibility-aware distance calculation provides pragmatic feedback to guide directed fuzzing to reach targets efficiently. We propose new techniques to address the challenges of feasibility prediction. Our new classification method allows us to predict the feasibility of all branches based on limited traces, and our runtime feasibility-updating mechanism gradually and efficiently improves the prediction precision. We implemented AFLGopher and compared AFLGopher with state-of-the-art directed fuzzers including AFLGo, enhanced AFLGo, WindRanger, BEACON and SelectFuzz. AFLGopher is 3.76x, 2.57x, 3.30x, 2.52x and 2.86x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in reaching targets. AFLGopher is 5.60x, 5.20x, 4.98x, 4.52x, and 5.07x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in triggering known vulnerabilities.

</details>


### [246] [PATCHEVAL: A New Benchmark for Evaluating LLMs on Patching Real-World Vulnerabilities](https://arxiv.org/abs/2511.11019)
*Zichao Wei,Jun Zeng,Ming Wen,Zeliang Yu,Kai Cheng,Yiding Zhu,Jingyi Guo,Shiqi Zhou,Le Yin,Xiaodong Su,Zhechao Ma*

Main category: cs.CR

TL;DR: The paper introduces PATCHEVAL, a multilingual benchmark addressing limitations in assessing automated vulnerability repair using large language models, covering 1,000 vulnerabilities in Go, JavaScript, and Python.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities are increasing rapidly, while manual patching is resource-intensive and automated techniques lack effectiveness. Current vulnerability benchmarks are insufficient, highlighting the need for better tools.

Method: The authors create PATCHEVAL, a benchmark dataset covering 1,000 vulnerabilities across outdated and unexplored languages, supported by a subset validated with sandbox environments for robust evaluation.

Result: PATCHEVAL evaluates state-of-the-art LLMs on vulnerability repair, providing empirical insights and systematic comparisons to guide future AVR studies.

Conclusion: PATCHEVAL overcomes key limitations in existing benchmarks and enables rigorous evaluation of LLM capabilities in AVR, advancing research in automated software security techniques.

Abstract: Software vulnerabilities are increasing at an alarming rate. However, manual patching is both time-consuming and resource-intensive, while existing automated vulnerability repair (AVR) techniques remain limited in effectiveness. Recent advances in large language models (LLMs) have opened a new paradigm for AVR, demonstrating remarkable progress. To examine the capability of LLMs in AVR, several vulnerability benchmarks have been proposed recently. However, they still suffer from key limitations of outdated vulnerabilities, limited language coverage, unreliable patch validation, and insufficient reproducibility. To overcome these challenges, we introduce PATCHEVAL, a multilingual benchmark for Go, JavaScript, and Python, languages for which existing benchmarks remain unexplored. PATCHEVAL curates a dataset of 1,000 vulnerabilities drawn from CVEs reported between 2015 and 2025, covering 65 distinct CWEs. A subset of 230 CVEs is further equipped with runtime sandbox environments, enabling patch verification through both security tests and functionality tests. To provide a systematic comparison of LLM-based vulnerability repair, we evaluate a series of state-of-the-art LLMs and agents, presenting an in-depth analysis that empirically yields key insights to guide future research in AVR.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [247] [What the flock knows that the birds do not: exploring the emergence of joint agency in multi-agent active inference](https://arxiv.org/abs/2511.10835)
*Domenico Maisto,Davide Nuzzi,Giovanni Pezzulo*

Main category: nlin.AO

TL;DR: This paper studies how collective behavior in systems like flocks forms joint agency and knowledge, using information-theoretic tools and active inference modeling.


<details>
  <summary>Details</summary>
Motivation: To understand how biological collectives exceed the capabilities of individual components, acquiring functional properties like joint agency and collective knowledge.

Method: Modeled flocking using active inference agents minimizing free-energy and examined emergent collective behavior using Markov blankets and information-theoretic analysis.

Result: Flocking agents form higher-order statistical boundaries, enabling emergent agency and collective knowledge that aids in coordinated responses to environmental changes.

Conclusion: Informational coupling among agents explains how autonomy and collective knowledge emerge, helping clarify joint agency mechanisms in biological systems.

Abstract: Collective behavior pervades biological systems, from flocks of birds to neural assemblies and human societies. Yet, how such collectives acquire functional properties -- such as joint agency or knowledge -- that transcend those of their individual components remains an open question. Here, we combine active inference and information-theoretic analyses to explore how a minimal system of interacting agents can give rise to joint agency and collective knowledge. We model flocking dynamics using multiple active inference agents, each minimizing its own free energy while coupling reciprocally with its neighbors. We show that as agents self-organize, their interactions define higher-order statistical boundaries (Markov blankets) enclosing a ``flock'' that can be treated as an emergent agent with its own sensory, active, and internal states. When exposed to external perturbations (a ``predator''), the flock exhibits faster, coordinated responses than individual agents, reflecting collective sensitivity to environmental change. Crucially, analyses of synergistic information reveal that the flock encodes information about the predator's location that is not accessible to every individual bird, demonstrating implicit collective knowledge. Together, these results show how informational coupling among active inference agents can generate new levels of autonomy and inference, providing a framework for understanding the emergence of (implicit) collective knowledge and joint agency.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [248] [Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents](https://arxiv.org/abs/2511.10687)
*Chih-Hsuan Yang,Tanwi Mallick,Le Chen,Krishnan Raghavan,Azton Wells,Amal Gueroudji,Ian T. Foster,Rajeev Thakur*

Main category: cs.MA

TL;DR: The paper introduces a theoretical framework to effectively connect system-level evaluation with agent-level and message-level learning for large language models (LLMs) in multi-agent systems using cooperative game theory and process reward modeling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of principled methods for bridging system-level evaluation with agent-specific learning in multi-agent systems to improve task performance and cooperation among agents.

Method: The paper proposes a framework that integrates cooperative game-theoretic attribution and process reward modeling, offering local, signed, and credit-conserving signals for training agents.

Result: The framework transforms system evaluation into agent credit and response-level rewards that promote cooperation, correct harmful behaviors, and discourage inefficiencies. It can support reinforcement or preference-based post-training but is yet to be empirically validated.

Conclusion: This conceptual contribution provides a unified and auditable theoretical foundation for translating global evaluation into local training signals, setting the stage for future empirical investigations.

Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [249] [CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding](https://arxiv.org/abs/2511.10935)
*Yifan Zhuang,Calvin Huang,Zepeng Yu,Yongjie Zou,Jiawei Ju*

Main category: cs.SD

TL;DR: This study proposes a novel cross-subject multimodal Brain-Computer Interface (BCI) framework that integrates EEG and EMG signals to classify Mandarin tones, achieving high accuracy for both audible and silent speech with minimal signal channels.


<details>
  <summary>Details</summary>
Motivation: To assist individuals with speech impairments by improving speech decoding capabilities, particularly focusing on the challenges of tonal languages like Mandarin, which require accurate classification of tonal variations.

Method: The study introduces a multimodal BCI framework that fuses EEG and EMG signals using spatial-temporal feature extraction, a cross-attention fusion mechanism, and domain-adversarial training to enhance cross-subject generalization.

Result: The proposed model achieved high classification accuracies of 87.83% for audible speech and 88.08% for silent speech, while maintaining strong cross-subject performance with accuracies of 83.27% and 85.10%. These results were obtained using only 20 EEG and 5 EMG channels.

Conclusion: The research demonstrates that minimal-channel tone-level decoding is feasible, effective, and generalizable across subjects, offering significant potential for practical BCI applications in speech decoding.

Abstract: Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin tone classification presents particular challenges, as tonal variations convey distinct meanings even when phonemes remain identical. In this study, we propose a novel cross-subject multimodal BCI decoding framework that fuses EEG and EMG signals to classify four Mandarin tones under both audible and silent speech conditions. Inspired by the cooperative mechanisms of neural and muscular systems in speech production, our neural decoding architecture combines spatial-temporal feature extraction branches with a cross-attention fusion mechanism, enabling informative interaction between modalities. We further incorporate domain-adversarial training to improve cross-subject generalization. We collected 4,800 EEG trials and 4,800 EMG trials from 10 participants using only twenty EEG and five EMG channels, demonstrating the feasibility of minimal-channel decoding. Despite employing lightweight modules, our model outperforms state-of-the-art baselines across all conditions, achieving average classification accuracies of 87.83% for audible speech and 88.08% for silent speech. In cross-subject evaluations, it still maintains strong performance with accuracies of 83.27% and 85.10% for audible and silent speech, respectively. We further conduct ablation studies to validate the effectiveness of each component. Our findings suggest that tone-level decoding with minimal EEG-EMG channels is feasible and potentially generalizable across subjects, contributing to the development of practical BCI applications.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [250] [Model Class Selection](https://arxiv.org/abs/2511.11355)
*Ryan Cecil,Lucas Mentch*

Main category: stat.ME

TL;DR: The paper introduces Model Class Selection (MCS), expanding beyond traditional Model Selection and Model Set Selection, for identifying model collections containing at least one optimal model.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of classical and modern model selection techniques like MSS by introducing a broader framework to assess multiple model collections and allow analysis of simple versus complex models.

Method: Proposed a generalization called MCS, employing data-splitting approaches to identify model classes with at least one optimal model.

Result: Demonstrated the feasibility of MCS through simulations and real-data experiments, showing its capability to assess and compare simpler interpretable models with complex machine learning models.

Conclusion: MCS enables a formal framework to evaluate different model classes, providing insights into the trade-offs between simplicity and complexity in statistical modeling.

Abstract: Classical model selection seeks to find a single model within a particular class that optimizes some pre-specified criteria, such as maximizing a likelihood or minimizing a risk. More recently, there has been an increased interest in model set selection (MSS), where the aim is to identify a (confidence) set of near-optimal models. Here, we generalize the MSS framework further by introducing the idea of model class selection (MCS). In MCS, multiple model collections are evaluated, and all collections that contain at least one optimal model are sought for identification. Under mild conditions, data splitting based approaches are shown to provide general solutions for MCS. As a direct consequence, for particular datasets we are able to investigate formally whether classes of simpler and more interpretable statistical models are able to perform on par with more complex black-box machine learning models. A variety of simulated and real-data experiments are provided.

</details>


### [251] [Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility](https://arxiv.org/abs/2511.11564)
*Albert Tan,Mohsen Bayati,James Nordlund,Roman Istomin*

Main category: stat.ME

TL;DR: The paper introduces a framework for analyzing randomized experiments in systems where only certain units receive treatment but all units interact, thus causing interference.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in randomized experiments conducted in bipartite systems where interference may lead to biased estimates in treatment effect evaluations.

Method: The authors define estimands PTTE and STTE, propose interference-aware ensemble estimators combining exposure mapping, generalized propensity scores, machine learning, and introduce a projection method for estimation of treatment-level effects.

Result: In simulations, the proposed methods demonstrated low bias and variance in recovering PTTE and STTE. Applications in two real-world field experiments showed corrections for interference-driven biases and even reversed primary decision metrics.

Conclusion: The proposed methods offer a systematic way to handle interference in bipartite experiments and yield more accurate treatment effect estimations.

Abstract: We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [252] [RadAround: A Field-Expedient Direction Finder for Contested IoT Sensing & EM Situational Awareness](https://arxiv.org/abs/2511.11392)
*Owen A. Maute,Blake A. Roberts,Berker Peköz*

Main category: eess.SP

TL;DR: RadAround is an adversarial IoT sensing system that generates high-resolution EM heatmaps using low-cost components and mechanically steered antennas for operation in contested environments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a cost-effective and adaptable solution for adversarial IoT sensing and electromagnetic situational awareness in challenging or contested environments.

Method: RadAround utilizes mechanically steered narrow-beam antennas coupled with field-deployable SCADA software and microcontroller-deployed systems to generate 2-D EM heatmaps. It coordinates antenna movement and SDR sampling for real-time, on-site electromagnetic assessments.

Result: Experiments showcase the system's effectiveness in detecting computing machinery through walls, assessing operational status, and locating EMI leakage from Faraday enclosures.

Conclusion: RadAround demonstrates adaptability and accuracy in various applications, including electronic intrusion detection, battlefield spectrum monitoring, and disaster-response EMC testing.

Abstract: This paper presents RadAround, a passive 2-D direction-finding system designed for adversarial IoT sensing in contested environments. Using mechanically steered narrow-beam antennas and field-deployable SCADA software, it generates high-resolution electromagnetic (EM) heatmaps using low-cost COTS or 3D-printed components. The microcontroller-deployable SCADA coordinates antenna positioning and SDR sampling in real time for resilient, on-site operation. Its modular design enables rapid adaptation for applications such as EMC testing in disaster-response deployments, battlefield spectrum monitoring, electronic intrusion detection, and tactical EM situational awareness (EMSA). Experiments show RadAround detecting computing machinery through walls, assessing utilization, and pinpointing EM interference (EMI) leakage sources from Faraday enclosures.

</details>
