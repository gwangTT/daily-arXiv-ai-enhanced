<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 43]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.LG](#cs.LG) [Total: 69]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 30]
- [cs.SE](#cs.SE) [Total: 11]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.AP](#stat.AP) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.DL](#cs.DL) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.CR](#cs.CR) [Total: 7]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [hep-ex](#hep-ex) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Attention-gated U-Net model for semantic segmentation of brain tumors and feature extraction for survival prognosis](https://arxiv.org/abs/2602.15067)
*Rut Pate,Snehal Rajput,Mehul S. Raval,Rupal A. Kapdi,Mohendra Roy*

Main category: cs.AI

TL;DR: This paper proposes an advanced R2U-Net-based Triplanar model achieving high accuracy in brain tumor segmentation and survival prediction.


<details>
  <summary>Details</summary>
Motivation: Gliomas have varied characteristics and pose challenges in treatment due to surgical complexity, necessitating better segmentation for treatment planning.

Method: The study incorporates an Attention-Gated Recurrent Residual U-Net (R2U-Net) with triplanar architecture for segmentation and integrates artificial neural networks for survival prediction.

Result: Achieved DSC of 0.900 for tumor segmentation and improved survival prediction metrics with accuracy of 45.71% and SRC of 0.338.

Conclusion: The proposed method shows promising segmentation accuracy and survival prediction potential, aiding better glioma treatment strategies.

Abstract: Gliomas, among the most common primary brain tumors, vary widely in aggressiveness, prognosis, and histology, making treatment challenging due to complex and time-intensive surgical interventions. This study presents an Attention-Gated Recurrent Residual U-Net (R2U-Net) based Triplanar (2.5D) model for improved brain tumor segmentation. The proposed model enhances feature representation and segmentation accuracy by integrating residual, recurrent, and triplanar architectures while maintaining computational efficiency, potentially aiding in better treatment planning. The proposed method achieves a Dice Similarity Score (DSC) of 0.900 for Whole Tumor (WT) segmentation on the BraTS2021 validation set, demonstrating performance comparable to leading models. Additionally, the triplanar network extracts 64 features per planar model for survival days prediction, which are reduced to 28 using an Artificial Neural Network (ANN). This approach achieves an accuracy of 45.71%, a Mean Squared Error (MSE) of 108,318.128, and a Spearman Rank Correlation Coefficient (SRC) of 0.338 on the test dataset.

</details>


### [2] [ResearchGym: Evaluating Language Model Agents on Real-World AI Research](https://arxiv.org/abs/2602.15112)
*Aniketh Garikaparthi,Manasi Patwardhan,Arman Cohan*

Main category: cs.AI

TL;DR: ResearchGym is an evaluation framework for assessing AI agents on research tasks, using repurposed ICML, ICLR, and ACL papers.


<details>
  <summary>Details</summary>
Motivation: To address the need for systematically evaluating AI agents on autonomous research capabilities using real-world research frameworks.

Method: Created five containerized task environments based on existing research papers, excluding proposed methods, for AI agents to hypothesize, experiment, and outperform human baselines.

Result: Tested GPT-5-based agent, which succeeded in only 6.7% of evaluations, showing capability gaps but occasional state-of-the-art performance.

Conclusion: ResearchGym enables structured evaluation of AI research agents, revealing areas for improvement in agent design and reliability.

Abstract: We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.

</details>


### [3] [Protecting Language Models Against Unauthorized Distillation through Trace Rewriting](https://arxiv.org/abs/2602.15143)
*Xinhang Ma,William Yeoh,Ning Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: The paper explores methods to modify teacher-generated reasoning traces in large language models (LLMs) to prevent unauthorized knowledge distillation and embed verifiable watermarks within student models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unauthorized use of knowledge distillation, which exploits significant efforts in creating frontier LLMs, and to discourage misuse while enabling traceability.

Method: The authors introduced approaches to dynamically rewrite the reasoning outputs of teacher models while preserving correctness and coherence. Techniques include LLM-based rewriting and gradient-based methods, emphasizing their dual objectives of deterring unauthorized use (anti-distillation) and enabling watermarking.

Result: An instruction-based rewriting approach proved effective in degrading training usefulness for unauthorized distillation while maintaining or improving teacher model performance. Additionally, the rewriting approach achieved reliable watermark detection with minimal false alarms.

Conclusion: The proposed methods effectively prevent unauthorized distillation and embed detectible signatures in student models without compromising the teacher model's performance or coherence.

Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.

</details>


### [4] [Panini: Continual Learning in Token Space via Structured Memory](https://arxiv.org/abs/2602.15156)
*Shreyas Rajesh,Pavan Holur,Mehmet Yigit Turali,Chenda Duan,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: The paper introduces Panini, a learning framework that optimizes reasoning and reduces inefficiencies in language processing through structured memory storage (GSW).


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in current retrieval-augmented generation methods which suffer from repeated reasoning over the same data and irrelevant inputs that lead to unsupported outcomes.

Method: The authors propose Panini, which uses Generative Semantic Workspaces (GSW), a network of question-answer pairs, to store and structure experiences non-parametrically. Queries are answered via inference chains from the GSW.

Result: Panini improves QA benchmark performance by 5%-7% over baselines, uses 2-30x fewer tokens, supports open-source pipelines, and reduces unsupported answers.

Conclusion: Structured and efficient memory frameworks like GSW enhance reasoning efficiency and reliability for language models.

Abstract: Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.

</details>


### [5] [da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems](https://arxiv.org/abs/2602.15158)
*Gabriel Rocha*

Main category: cs.AI

TL;DR: The paper proposes a novel approach for handling ontological heterogeneity, merging ideas from Carnapian-Goguenism and da Costian-Tarskianism.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of ontological heterogeneity and establish a method for relating ontologies effectively.

Method: Usage of extended consequence systems and development graphs to connect and process ontologies with operations like morphisms, fibring, and splitting.

Result: Introduces extended consequence systems and extended development graphs for managing ontological relations.

Conclusion: The approach aids in applied ontology and provides insights for advancing research in ontological systems and frameworks.

Abstract: This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.

</details>


### [6] [Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs](https://arxiv.org/abs/2602.15173)
*Luise Ge,Yongyan Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: The paper studies LLM decision-making under uncertainty, analyzing risky choices across prospect representation and decision rationale, and categorizes models into rational Reasoning Models (RMs) and more human-like Conversational Models (CMs).


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the decision-making behavior of large language models under uncertainty and evaluate their reasoning patterns compared to humans and rational agents.

Method: The study utilizes a comparative analysis of 20 large language models, evaluating responses based on prospect representation and rational explanation. Insights are compared against human experiments and a rational agent model.

Result: The analysis highlights two model categories: Reasoning Models (RMs), which demonstrate rational behavior, and Conversational Models (CMs), which are less rational and exhibit human-like sensitivity to framing and explanation.

Conclusion: Training for mathematical reasoning appears to differentiate RMs from CMs, leading RMs to exhibit consistent rational decision-making while CMs behave in a more human-like, less rational manner.

Abstract: The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.

</details>


### [7] [Secure and Energy-Efficient Wireless Agentic AI Networks](https://arxiv.org/abs/2602.15212)
*Yuanyan Song,Kezhi Wang,Xinmian Xu*

Main category: cs.AI

TL;DR: The paper introduces a secure wireless AI network with a supervisor AI agent and multiple agents to ensure service quality and confidentiality. It proposes two energy-efficient resource allocation schemes using advanced optimization techniques and validates their performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in provisioning QoS and confidentiality for reasoning tasks in AI networks and to minimize energy consumption while meeting latency and accuracy requirements.

Method: The study formulates an energy minimization problem, optimizing AI agent selection, beamforming, and transmission power. Two schemes (ASC and LAW) use advanced optimization methods like ADMM, SDR, SCA, and a large language model optimizer for resource allocation.

Result: The proposed solutions reduce energy consumption by up to 59.1% compared to benchmarks and are validated with strong reasoning accuracy using a practical AI system.

Conclusion: The research showcases effective strategies for secure and efficient resource allocation in AI networks, achieving significant energy savings and maintaining performance accuracy.

Abstract: In this paper, we introduce a secure wireless agentic AI network comprising one supervisor AI agent and multiple other AI agents to provision quality of service (QoS) for users' reasoning tasks while ensuring confidentiality of private knowledge and reasoning outcomes. Specifically, the supervisor AI agent can dynamically assign other AI agents to participate in cooperative reasoning, while the unselected AI agents act as friendly jammers to degrade the eavesdropper's interception performance. To extend the service duration of AI agents, an energy minimization problem is formulated that jointly optimizes AI agent selection, base station (BS) beamforming, and AI agent transmission power, subject to latency and reasoning accuracy constraints. To address the formulated problem, we propose two resource allocation schemes, ASC and LAW, which first decompose it into three sub-problems. Specifically, ASC optimizes each sub-problem iteratively using the proposed alternating direction method of multipliers (ADMM)-based algorithm, semi-definite relaxation (SDR), and successive convex approximation (SCA), while LAW tackles each sub-problem using the proposed large language model (LLM) optimizer within an agentic workflow. The experimental results show that the proposed solutions can reduce network energy consumption by up to 59.1% compared to other benchmark schemes. Furthermore, the proposed schemes are validated using a practical agentic AI system based on Qwen, demonstrating satisfactory reasoning accuracy across various public benchmarks.

</details>


### [8] [Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248)
*Pavel Koptev,Vishnu Kumar,Konstantin Malkov,George Shapiro,Yury Vikhanov*

Main category: cs.AI

TL;DR: This paper focuses on using AI and machine learning to predict invoice/payment dilution, addressing risks in supply chain finance.


<details>
  <summary>Details</summary>
Motivation: To address invoice/payment dilution and enhance supply chain finance adoption, particularly among buyers with lower credit grades.

Method: This research introduces an AI and machine learning-based framework that complements deterministic algorithms and uses real-time dynamic credit limits to predict invoice dilution.

Result: The AI framework evaluates invoice dilution using production data across nine transaction fields.

Conclusion: AI and machine learning can enhance prediction of invoice dilution, mitigating non-credit risk in supply chain finance.

Abstract: Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

</details>


### [9] [Enhancing Diversity and Feasibility: Joint Population Synthesis from Multi-source Data Using Generative Models](https://arxiv.org/abs/2602.15270)
*Farbod Abbasi,Zachary Patterson,Bilal Farooq*

Main category: cs.AI

TL;DR: The paper introduces a method utilizing a Wasserstein Generative Adversarial Network (WGAN) with gradient penalties to improve the diversity and realism of synthetic populations used in agent-based modeling.


<details>
  <summary>Details</summary>
Motivation: The study aims to address current limitations in synthetic population generation, particularly the inability to capture complex feature interrelationships and handle sampling and structural zeros in datasets.

Method: The authors propose a joint learning method using a WGAN with a novel regularization term (inverse gradient penalty) to improve data diversity and feasibility. A unified metric assesses similarities and diversities.

Result: The proposed approach outperforms a sequential baseline, showing a 7% increase in recall, 15% increase in precision, and better overall similarity metrics (88.1 score compared to 84.6).

Conclusion: This method enhances the diversity, feasibility, and reliability of synthetic data, offering significant potential improvements for agent-based models in transportation and urban planning.

Abstract: Generating realistic synthetic populations is essential for agent-based models (ABM) in transportation and urban planning. Current methods face two major limitations. First, many rely on a single dataset or follow a sequential data fusion and generation process, which means they fail to capture the complex interplay between features. Second, these approaches struggle with sampling zeros (valid but unobserved attribute combinations) and structural zeros (infeasible combinations due to logical constraints), which reduce the diversity and feasibility of the generated data. This study proposes a novel method to simultaneously integrate and synthesize multi-source datasets using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty. This joint learning method improves both the diversity and feasibility of synthetic data by defining a regularization term (inverse gradient penalty) for the generator loss function. For the evaluation, we implement a unified evaluation metric for similarity, and place special emphasis on measuring diversity and feasibility through recall, precision, and the F1 score. Results show that the proposed joint approach outperforms the sequential baseline, with recall increasing by 7\% and precision by 15\%. Additionally, the regularization term further improves diversity and feasibility, reflected in a 10\% increase in recall and 1\% in precision. We assess similarity distributions using a five-metric score. The joint approach performs better overall, and reaches a score of 88.1 compared to 84.6 for the sequential method. Since synthetic populations serve as a key input for ABM, this multi-source generative approach has the potential to significantly enhance the accuracy and reliability of ABM.

</details>


### [10] [When Remembering and Planning are Worth it: Navigating under Change](https://arxiv.org/abs/2602.15274)
*Omid Madani,J. Brian Burns,Reza Eghbali,Thomas L. Dean*

Main category: cs.AI

TL;DR: This paper explores how different memory types and strategies aid spatial navigation in changing, uncertain environments, emphasizing the need for robust and fast learning architectures.


<details>
  <summary>Details</summary>
Motivation: To understand the role of memory and learning in enhancing spatial navigation in non-stationary and uncertain environments.

Method: Analyzing strategies ranging from simple to sophisticated, with varied uses of memory and learning, to study an agent's navigation in simulated environments with non-stationary barriers and food locations.

Result: Agents employing non-stationary probability learning and episodic memory updates are more efficient in challenging tasks, provided uncertainty is manageable.

Conclusion: An architecture integrating multiple strategies and memory usage significantly improves navigation efficacy in complex environments.

Abstract: We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.

</details>


### [11] [EAA: Automating materials characterization with vision language model agents](https://arxiv.org/abs/2602.15294)
*Ming Du,Yanqi Luo,Srutarshi Banerjee,Michael Wojcik,Jelena Popovic,Mathew J. Cherukara*

Main category: cs.AI

TL;DR: The paper introduces Experiment Automation Agents (EAA), leveraging vision-language models for automating microscopic experimental workflows.


<details>
  <summary>Details</summary>
Motivation: Current microscopy workflows require significant expertise and manual intervention, which creates inefficiencies and restricts accessibility.

Method: EAA uses multimodal reasoning, tool-augmented actions, and memory capabilities within a flexible architecture, compatible with Model Context Protocol for tool integration.

Result: The system demonstrated success in automating tasks like zone plate focusing, feature search using natural language, and data acquisition at the Advanced Photon Source beamline.

Conclusion: EAA reduces the operational workload, increases efficiency, and lowers the expertise threshold for using complex experimental systems.

Abstract: We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.

</details>


### [12] [X-MAP: eXplainable Misclassification Analysis and Profiling for Spam and Phishing Detection](https://arxiv.org/abs/2602.15298)
*Qi Zhang,Dian Chen,Lance M. Kaplan,Audun Jøsang,Dong Hyun Jeong,Feng Chen,Jin-Hee Cho*

Main category: cs.AI

TL;DR: The paper introduces X-MAP, an explainable method for addressing misclassifications in spam/phishing detection, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To mitigate harmful consequences of misclassifications in spam and phishing detection (false negatives and false positives) by providing improved detection with interpretability.

Method: The framework combines SHAP-based feature attributions and non-negative matrix factorization to profile semantic patterns and measures misclassification through divergence metrics (Jensen-Shannon divergence).

Result: Experiments demonstrate misclassified messages show much higher divergence than correctly classified ones. X-MAP achieved up to 0.98 AUROC, reduced false-rejection rates, and effectively repaired misclassifications with minimal leakage.

Conclusion: X-MAP reliably enhances spam/phishing detection models both in accuracy and interpretability, recovering many false rejections while maintaining a reasonable balance of leakage.

Abstract: Misclassifications in spam and phishing detection are very harmful, as false negatives expose users to attacks while false positives degrade trust. Existing uncertainty-based detectors can flag potential errors, but possibly be deceived and offer limited interpretability. This paper presents X-MAP, an eXplainable Misclassification Analysis and Profilling framework that reveals topic-level semantic patterns behind model failures. X-MAP combines SHAP-based feature attributions with non-negative matrix factorization to build interpretable topic profiles for reliably classified spam/phishing and legitimate messages, and measures each message's deviation from these profiles using Jensen-Shannon divergence. Experiments on SMS and phishing datasets show that misclassified messages exhibit at least two times larger divergence than correctly classified ones. As a detector, X-MAP achieves up to 0.98 AUROC and lowers the false-rejection rate at 95% TRR to 0.089 on positive predictions. When used as a repair layer on base detectors, it recovers up to 97% of falsely rejected correct predictions with moderate leakage. These results demonstrate X-MAP's effectiveness and interpretability for improving spam and phishing detection.

</details>


### [13] [AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents](https://arxiv.org/abs/2602.15325)
*Zhixing Zhang,Jesen Zhang,Hao Liu,Qinhan Lv,Jing Yang,Kaitong Cai,Keze Wang*

Main category: cs.AI

TL;DR: The paper introduces Agro-Reflective, a framework that integrates large language models (LLMs) with a Python-based environment (AgriWorld) to improve agricultural decision-making by enabling reasoning over spatiotemporal data with iterative code execution.


<details>
  <summary>Details</summary>
Motivation: Modern agriculture models are strong in data-specific tasks but lack interactive and reasoning capabilities, while LLMs excel in text interpretation but cannot handle agricultural datasets. There is a need to combine their strengths for real-world applications.

Method: The authors propose AgriWorld, a Python-based environment for agricultural data and tasks, coupled with Agro-Reflective, a multi-turn LLM agent that refines its analysis through iterative execution, observation, and reflection.

Result: The proposed framework outperforms text-only and direct tool-use baselines in agricultural question-answering tasks, as tested with the new AgroBench dataset.

Conclusion: Execution-driven reflection enhances LLM reasoning capabilities for agriculture, bridging the gap between data processing and language-based analysis for more reliable outcomes.

Abstract: Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual "what-if" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.

</details>


### [14] [World-Model-Augmented Web Agents with Action Correction](https://arxiv.org/abs/2602.15384)
*Zhouzhou Shen,Xueyu Hu,Xiyun Li,Tianqing Fang,Juncheng Li,Shengyu Zhang*

Main category: cs.AI

TL;DR: This paper introduces WAC, a web agent designed for automating web tasks more effectively by integrating multi-model collaboration and risk-aware action refinement.


<details>
  <summary>Details</summary>
Motivation: Current web agents face challenges in reasoning sensible actions due to limitations in predicting environment changes and assessing execution risks, resulting in task failures.

Method: The authors propose WAC, which uses multi-agent collaboration between action and world models to enhance strategic guidance, alongside a deduction chain for simulating and judging action outcomes to provide corrective feedback.

Result: Experiments show improvements in task execution performance, with WAC achieving gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.

Conclusion: WAC improves web agent performance by addressing reasoning and risk-awareness challenges through model collaboration and consequence simulation, enhancing resilience in task execution.

Abstract: Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.

</details>


### [15] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: The paper presents a context-aware abstention system to optimize safety and utility in Large Language Models, overcoming limitations of static guardrails.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the safety-utility trade-off in LLMs where strict filtering blocks benign inputs and relaxed controls risk unsafe outputs, while ensuring scalable and efficient LLM deployments.

Method: The system dynamically adjusts safety thresholds using real-time contextual signals and integrates a cascade mechanism with five parallel detectors to improve efficiency and precision.

Result: The approach reduces latency, achieves significant reductions in false positives in sensitive areas, and maintains high safety precision and near-perfect recall in strict modes.

Conclusion: This adaptive system provides a scalable, context-aware solution for balancing safety and utility in LLMs without compromising performance.

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>


### [16] [Common Belief Revisited](https://arxiv.org/abs/2602.15403)
*Thomas Ågotnes*

Main category: cs.AI

TL;DR: This paper explores the logic of common belief when individual belief is KD45, finding that it requires additional axioms for a complete characterization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to determine the logic that governs common belief in the context where individual belief satisfies KD45 axioms, clarifying misconceptions and addressing an open problem.

Method: The authors analyze axiomatic structures and properties related to common and individual beliefs. They examine how common belief deviates from KD45, introduce new axioms, and prove their completeness.

Result: The analysis reveals that common belief, contrary to assumptions, is not KD4 under KD45 individual belief. It satisfies a unique property (shift-reflexivity) and requires one additional agent-dependent axiom for characterization.

Conclusion: The study resolves an open question by providing a complete characterization of the logic of common belief under KD45, showing it is distinct and more complex than previously thought.

Abstract: Contrary to common belief, common belief is not KD4.
  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(Cφ\rightarrow φ)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:
  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.

</details>


### [17] [GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway](https://arxiv.org/abs/2602.15531)
*Javier Irigoyen,Roberto Daza,Aythami Morales,Julian Fierrez,Francisco Jurado,Alvaro Ortigosa,Ruben Tolosana*

Main category: cs.AI

TL;DR: Introduces EduEVAL-DB, a dataset for training and evaluating AI tutors, focusing on pedagogical risk across five dimensions, with explanations evaluated by experts.


<details>
  <summary>Details</summary>
Motivation: To create a resource that enables evaluation and training of AI pedagogical tools while addressing educational shortcomings.

Method: Developed a dataset with teacher-role based explanations, applying prompt engineering, risk evaluation rubrics, and semi-automatic expert reviews for annotation.

Result: The dataset includes 854 annotated explanations across five risk dimensions, benchmarked with education-focused models for validation.

Conclusion: EduEVAL-DB shows promising utility for advancing AI-based educational evaluation and training, using fine-tuned models to detect pedagogical risks.

Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.

</details>


### [18] [Quantifying construct validity in large language model evaluations](https://arxiv.org/abs/2602.15532)
*Ryan Othniel Kearns*

Main category: cs.AI

TL;DR: The paper introduces a new model for analyzing LLM benchmarks, addressing limitations in existing methods by separating model scale from capabilities and accounting for measurement errors.


<details>
  <summary>Details</summary>
Motivation: To improve the validity and reliability of LLM benchmark evaluations by addressing issues like test set contamination, annotator error, and the conflation of benchmark scores with general model capabilities.

Method: The authors propose the structured capabilities model, which combines insights from latent factor models and scaling laws to better extract interpretable and generalizable capabilities from benchmark results.

Result: The structured capabilities model outperforms existing methods (latent factor models and scaling laws) in explanatory power, out-of-distribution benchmark prediction, and interpretability.

Conclusion: The structured capabilities model provides a more robust and accurate framework for evaluating construct validity in LLM benchmarks, ensuring a clearer distinction between model scale, capabilities, and performance metrics.

Abstract: The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.
  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.
  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.

</details>


### [19] [RUVA: Personalized Transparent On-Device Graph Reasoning](https://arxiv.org/abs/2602.15553)
*Gabriele Conte,Alessio Mattiace,Gianni Carmosino,Potito Aghilar,Giovanni Servedio,Francesco Musicco,Vito Walter Anelli,Tommaso Di Noia,Francesco Maria Donini*

Main category: cs.AI

TL;DR: This paper introduces Ruva, a transparent architecture for Personal AI using a Personal Knowledge Graph for accountable memory curation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of accountability in current Retrieval-Augmented Generation (RAG) systems which suffer from imprecise data control, hallucination issues, and insufficient privacy measures.

Method: Introduce Ruva, a 'Glass Box' system leveraging Personal Knowledge Graphs instead of statistical vector-based systems, allowing users to inspect AI knowledge and perform precise content redactions.

Result: Ruva shifts AI from vector matching to graph reasoning, ensuring accountable data retrieval while upholding user privacy and control.

Conclusion: Ruva empowers users as editors of their personal AI knowledge with precise curation and establishes better accountability and privacy compared to traditional RAG systems.

Abstract: The Personal AI landscape is currently dominated by "Black Box" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, "deleting" a concept from a vector space is mathematically imprecise, leaving behind probabilistic "ghosts" that violate true privacy. We propose Ruva, the first "Glass Box" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the "Right to be Forgotten." Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.

</details>


### [20] [How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning](https://arxiv.org/abs/2602.15580)
*Hongxuan Wu,Yukun Zhang,Xueqing Zhou*

Main category: cs.AI

TL;DR: The paper introduces a framework to decompose how multimodal Transformers process visual and linguistic inputs across layers, revealing task-dependent information trajectories.


<details>
  <summary>Details</summary>
Motivation: To understand whether multimodal Transformers base predictions on visual evidence, linguistic reasoning, or fused cross-modal computation, and analyze how these contributions evolve across layers.

Method: A layer-wise framework using Partial Information Decomposition (PID) with a new pipeline (PID Flow) combining dimensionality reduction, normalizing-flow Gaussianization, and Gaussian PID estimation.

Result: Applying the framework on models (LLaVA variants), the study reveals a consistent pattern: early layers peak in vision-unique information, late layers prioritize language-unique information, while cross-modal synergy remains minimal (<2%). Task dependence shapes redundancy and contributions.

Conclusion: The findings illuminate how multimodal Transformers fuse vision into language, establish causal pathways, and pinpoint architectural bottlenecks for modality-specific information processing losses.

Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\% of the final prediction, and cross-modal synergy remains below 2\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.

</details>


### [21] [On inferring cumulative constraints](https://arxiv.org/abs/2602.15635)
*Konstantin Sidorov*

Main category: cs.AI

TL;DR: This paper presents a preprocessing method for inferring additional cumulative constraints in constraint programming to enhance scheduling efficiency.


<details>
  <summary>Details</summary>
Motivation: Scheduling with constraint programming suffers from inefficiencies due to a lack of consideration for multi-resource interactions, causing performance slowdowns.

Method: A preprocessing method that interprets cumulative constraints as linear inequalities, discovers covers, strengthens inequalities with lifting, and injects these constraints back into the problem.

Result: The method improved search performance on scheduling benchmarks, discovered 25 new lower bounds, and 5 new best solutions.

Conclusion: The proposed approach effectively captures multi-resource interactions, strengthening scheduling performance with little downside in unfavorable scenarios.

Abstract: Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.

</details>


### [22] [CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving](https://arxiv.org/abs/2602.15645)
*Lucas Elbert Suryana,Farah Bierenga,Sanne van Buuren,Pepijn Kooij,Elsefien Tulleners,Federico Scari,Simeon Calvert,Bart van Arem,Arkady Zgonnikov*

Main category: cs.AI

TL;DR: The paper introduces CARE Drive, a framework to evaluate whether decisions made by vision language models in automated driving reflect human reasoning or are just post hoc rationalizations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in evaluating foundation models' decisions, especially in safety-critical domains like automated driving, by ensuring these decisions are reason-responsive rather than simply outcome-focused.

Method: The proposed CARE Drive evaluates reason responsiveness by comparing baseline and reason-augmented decisions of models through a two-stage evaluation process: prompt calibration and systematic contextual perturbation.

Result: CARE Drive showed that explicit human reasons influence model decisions, aligning them better with expert recommendations, but sensitivity to different reasons varies across contexts.

Conclusion: Reason responsiveness in foundation models can be systematically and empirically evaluated without modifying the underlying model parameters, highlighting the need for assessing human-reason alignment in safety-critical systems.

Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.

</details>


### [23] [PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra](https://arxiv.org/abs/2602.15669)
*Xiachong Feng,Liang Zhao,Weihong Zhong,Yichong Huang,Yuxuan Gu,Lingpeng Kong,Xiaocheng Feng,Bing Qin*

Main category: cs.AI

TL;DR: This paper introduces PERSONA, a training-free framework that enables personality control in large language models via direct manipulation of personality vectors in activation space, achieving near fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: Current approaches to personality control in LLMs rely on static or expensive fine-tuning methods, which are inadequate for capturing the dynamic and compositional nature of human traits, highlighting the need for a more adaptable and efficient solution.

Method: The framework operates in three stages: Persona-Base extracts orthogonal personality trait vectors, Persona-Algebra enables precise control using vector arithmetic, and Persona-Flow facilitates context-aware adaptation through dynamic vector composition during inference.

Result: PERSONA achieves a mean score of 9.60 on PersonalityBench, close to the supervised fine-tuning upper bound of 9.61, and registers up to 91% win rates on the Persona-Evolve benchmark for dynamic personality adaptation.

Conclusion: This study demonstrates the mathematical tractability of personality control in LLMs, offering an interpretable, efficient alternative to fine-tuning for dynamic and compositional personality manipulation.

Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.

</details>


### [24] [Recursive Concept Evolution for Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.15725)
*Sarim Chaudhry*

Main category: cs.AI

TL;DR: The paper introduces Recursive Concept Evolution (RCE), which dynamically modifies the internal representation space of large language models to improve compositional reasoning, achieving significant accuracy improvements across benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the sharp degradation in accuracy of large language models on compositional reasoning benchmarks due to fixed latent representation spaces.

Method: The proposed Recursive Concept Evolution (RCE) framework dynamically generates low-rank concept subspaces, selects and consolidates them using specific criteria to modify internal representations during inference.

Result: RCE significantly improved model performance on benchmarks like ARC-AGI-2 (12-18 point gains), GPQA and BBH (8-14 point improvements), and reduced depth-induced errors on MATH and HLE.

Conclusion: RCE enables large language models to construct new abstractions dynamically, enhancing compositional reasoning and achieving notable performance gains on challenging tasks.

Abstract: Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.

</details>


### [25] [GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems](https://arxiv.org/abs/2602.15776)
*Yiqin Yang,Xu Yang,Yuhua Jiang,Ni Mu,Hao Hu,Runpeng Xie,Ziyou Zhang,Siyuan Li,Yuan-Hua Ni,Qianchuan Zhao,Bo Xu*

Main category: cs.AI

TL;DR: The paper introduces the GlobeDiff algorithm to solve partial observability in multi-agent systems by using a multi-modal diffusion process to infer global states from local observations.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in belief-state estimation and inter-agent communication methods which fail to effectively utilize global information in multi-agent systems.

Method: GlobeDiff uses a multi-modal diffusion process to infer global states, ensuring strong accuracy while addressing ambiguities in state estimation.

Result: Extensive experiments show that GlobeDiff outperforms existing methods and accurately infers global states under unimodal and multi-modal distributions.

Conclusion: GlobeDiff provides a robust solution to the partial observability problem, offering bounded estimation errors and high fidelity in global state inference.

Abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.

</details>


### [26] [This human study did not involve human subjects: Validating LLM simulations as behavioral evidence](https://arxiv.org/abs/2602.15785)
*Jessica Hullman,David Broska,Huaman Sun,Aaron Shaw*

Main category: cs.AI

TL;DR: This paper examines the use of large language models (LLMs) as synthetic participants in social science experiments, assessing the conditions for valid inference, and contrasting heuristic and statistical calibration methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of clear guidance on when LLM-based simulations can generate valid insights about human behavior in social experiments, ensuring accurate and reliable results.

Method: The paper contrasts two methodologies: heuristic approaches, involving prompt engineering and fine-tuning, versus statistical calibration, which applies auxiliary human data to adjust LLM output for discrepancies.

Result: The study finds that heuristic methods are useful for exploratory analysis but lack statistical validity for confirmatory research, while statistical calibration offers validity and efficiency under specific assumptions.

Conclusion: While LLMs show promise, their true utility depends on their ability to approximate human populations, and researchers should avoid focusing solely on substituting LLMs for human participants in studies.

Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.

</details>


### [27] [Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings](https://arxiv.org/abs/2602.15791)
*Suhyung Jang,Ghang Lee,Jaekun Lee,Hyunjun Lee*

Main category: cs.AI

TL;DR: This paper introduces a method utilizing large language model embeddings for enhancing semantic representation of building object types and subtypes, demonstrating superior performance compared to traditional one-hot encoding.


<details>
  <summary>Details</summary>
Motivation: To improve AI's semantic comprehension of building object subtypes which conventional encoding methods fail to represent effectively.

Method: The authors proposed using large language model embeddings, including high-dimensional and compacted embeddings, for encoding building semantics and evaluated them using GraphSAGE models.

Result: Experimental evaluation revealed that LLM embeddings outperformed one-hot encoding, with llama-3 (compacted) embedding achieving a higher weighted F1-score of 0.8766 compared to 0.8475 for one-hot encoding.

Conclusion: Leveraging LLM-based encodings significantly enhances AI's interpretative ability for domain-specific building semantics, showing promise for broader applications in the AECO industry.

Abstract: Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.

</details>


### [28] [Developing AI Agents with Simulated Data: Why, what, and how?](https://arxiv.org/abs/2602.15816)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: This paper discusses simulation-based synthetic data generation as a solution to insufficient data in AI, introducing a framework for designing and analyzing digital twin-based simulations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle challenges related to insufficient data volume and quality, which hinder the adoption of modern subsymbolic AI.

Method: The paper explores simulation-based synthetic data generation and provides a reference framework for digital twin-based AI simulation solutions.

Result: The paper presents conceptual insights and a systematic framework for generating synthetic data using simulation.

Conclusion: Using simulation-based approaches, particularly digital twins, can effectively address data challenges in AI training by creating diverse synthetic datasets.

Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [29] [Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15166)
*Tanner Andrulis,Michael Gilbert,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: The introduced Fast and Fusiest Mapper (FFM) efficiently finds optimal fused mappings for tensor algebra workloads, overcoming challenges of exponential increases in mapspaces.


<details>
  <summary>Details</summary>
Motivation: Accelerators' performance relies heavily on the data movement and mapping approach, with fusion optimization reducing energy and latency by minimizing DRAM accesses.

Method: FFM reduces the search space by pruning suboptimal mapping subsets and constructing optimal fused mappings, addressing exponential mapspace growth efficiently.

Result: FFM achieves runtime scaling approximately linearly despite exponential mapspace growth, showing over 1000x faster performance compared to state-of-the-art for Transformer workloads.

Conclusion: FFM provides a breakthrough in mapping tensor algebra workloads, making fused mapspace exploration feasible while significantly improving runtime efficiency.

Abstract: The latency and energy of tensor algebra accelerators depend on how data movement and operations are scheduled (i.e., mapped) onto accelerators, so determining the potential of an accelerator architecture requires both a performance model and a mapper to search for the optimal mapping. A key optimization that the mapper must explore is fusion, meaning holding data on-chip between computation steps, which has been shown to reduce energy and latency by reducing DRAM accesses. However, prior mappers cannot find optimal mappings with fusion (i.e., fused mappings) in a feasible runtime because the number of fused mappings to search increases exponentially with the number of workload computation steps.
  In this paper, we introduce the Fast and Fusiest Mapper (FFM), the first mapper to quickly find optimal mappings in a comprehensive fused mapspace for tensor algebra workloads. FFM shrinks the search space by pruning subsets of mappings (i.e., partial mappings) that are shown to never be a part of optimal mappings, quickly eliminating all suboptimal mappings with those partial mappings as subsets. Then FFM joins partial mappings to construct optimal fused mappings. We evaluate FFM and show that, although the mapspace size grows exponentially with the number of computation steps, FFM's runtime scales approximately linearly. FFM is orders of magnitude faster ($>1000\times$) than prior state-of-the-art approaches at finding optimal mappings for Transformers.

</details>


### [30] [The Turbo-Charged Mapper: Fast and Optimal Mapping for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15172)
*Michael Gilbert,Tanner Andrulis,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: The paper introduces a new mapping approach, Turbo-Charged Mapper (TCM), to accelerate and optimize deep neural network computations by guaranteeing the discovery of optimal mappings.


<details>
  <summary>Details</summary>
Motivation: Existing mapping methods for accelerators cannot guarantee optimal mappings and lead to uncertainties in hardware evaluation due to their reliance on heuristics or metaheuristics.

Method: The authors define a concept called dataplacement, which helps in significantly pruning redundant and suboptimal mappings, reducing search space by up to 32 orders of magnitude. Leveraging this, TCM performs comprehensive searches to guarantee optimal mappings.

Result: TCM outperforms prior mappers by quickly finding optimal mappings in less than a minute. In contrast, previous methods fail to achieve optimal mappings even with significantly longer runtimes.

Conclusion: TCM enables reliable hardware evaluations and design by offering fast, guaranteed optimal mappings, setting a new standard in accelerator mapping methodologies.

Abstract: The energy and latency of an accelerator running a deep neural network (DNN) depend on how the computation and data movement are scheduled in the accelerator (i.e., mapping). Optimizing mappings is essential to evaluating and designing accelerators. However, the space of mappings is large, and prior works can not guarantee finding optimal mappings because they use heuristics or metaheuristics to narrow down the space. These limitations preclude proper hardware evaluation, since designers can not tell whether performance differences are due to changes in hardware or suboptimal mapping.
  To address this challenge, we propose the Turbo-Charged Mapper (TCM), a fast mapper that is guaranteed to find optimal mappings. The key to our approach is that we define a new concept in mapping, called dataplacement, which, like the prior concept of dataflow, allows for clear analysis and comparison of mappings. Through it, we identify multiple opportunities to prune redundant and suboptimal mappings, reducing search space by up to 32 orders of magnitude.
  Leveraging these insights, TCM can perform full mapspace searches, making it the first mapper that can find optimal mappings in feasible runtime. Compared to prior mappers, we show that TCM can find optimal mappings quickly (less than a minute), while prior works can not find optimal mappings (energy-delay-product $21\%$ higher than optimal) even when given $1000\times$ the runtime ($>10$ hours).

</details>


### [31] [Human-AI Interaction: Evaluating LLM Reasoning on Digital Logic Circuit included Graph Problems, in terms of creativity in design and analysis](https://arxiv.org/abs/2602.15336)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: The study evaluates three LLMs (GPT, Gemini, Claude) for digital logic problems and highlights their limitations in reliability and accuracy despite favorable student perceptions.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and usability of LLMs in teaching undergraduate digital logic topics and address potential misconceptions in their answers.

Method: Three LLMs were evaluated on ten digital logic questions by 24 students, who provided comparative evaluations on model performance. Technical correctness was independently verified using strict criteria against official solutions.

Result: The LLMs failed to match the official answers for complex, sequential problems (Q1-Q7) and often defaulted to overly simplified textbook templates. Student perception of helpfulness did not align with actual correctness.

Conclusion: LLMs, without verification frameworks, are unreliable for teaching digital logic, potentially reinforcing student misconceptions despite their polished explanations.

Abstract: Large Language Models (LLMs) are increasingly used by undergraduate students as on-demand tutors, yet their reliability on circuit- and diagram-based digital logic problems remains unclear. We present a human- AI study evaluating three widely used LLMs (GPT, Gemini, and Claude) on 10 undergraduate-level digital logic questions spanning non-standard counters, JK-based state transitions, timing diagrams, frequency division, and finite-state machines. Twenty-four students performed pairwise model comparisons, providing per-question judgments on (i) preferred model, (ii) perceived correctness, (iii) consistency, (iv) verbosity, and (v) confidence, along with global ratings of overall model quality, satisfaction across multiple dimensions (e.g., accuracy and clarity), and perceived mental effort required to verify answers. To benchmark technical validity, we applied an independent judge-based evaluation against official solutions for all ten questions, using strict correctness criteria. Results reveal a consistent gap between perceived helpfulness and formal correctness: for the most sequentially demanding problems (Q1- Q7), none of the evaluated LLMs matched the official answers, despite producing confident, well-structured explanations that students often rated favorably. Error analysis indicates that models frequently default to canonical textbook templates (e.g., standard ripple counters) and struggle to translate circuit structure into exact state evolution and timing behavior. These findings suggest that, without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction.

</details>


### [32] [Iterative LLM-Based Assertion Generation Using Syntax-Semantic Representations for Functional Coverage-Guided Verification](https://arxiv.org/abs/2602.15388)
*Yonghao Wang,Jiaxin Zhou,Yang Yin,Hongqin Lyu,Zhiteng Chao,Wenchao Ding,Jing Ye,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: The paper introduces CoverAssert, a framework for improving LLM-generated SystemVerilog assertions iteratively by linking generated assertions to functional specifications, thus enhancing quality and functional coverage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of LLMs in understanding IC design and ensuring the generated assertions effectively cover functional specifications while improving assertion quality through feedback-based mechanisms.

Method: CoverAssert analyzes functional coverage by clustering semantic and structural features of assertions and mapping them to specifications. It uses a feedback loop to guide LLMs, focusing on uncovered functional points for progressive improvements.

Result: Experimental results show significant increases in branch coverage (9.57%), statement coverage (9.64%), and toggle coverage (15.69%) when using CoverAssert with existing systems AssertLLM and Spec2Assertion.

Conclusion: CoverAssert effectively addresses the challenges of improving SVA generation through LLMs by integrating a coverage-driven, iterative refinement process, leading to enhanced functional coverage and assertion quality.

Abstract: While leveraging LLMs to automatically generate SystemVerilog assertions (SVAs) from natural language specifications holds great potential, existing techniques face a key challenge: LLMs often lack sufficient understanding of IC design, leading to poor assertion quality in a single pass. Therefore, verifying whether the generated assertions effectively cover the functional specifications and designing feedback mechanisms based on this coverage remain significant hurdles. To address these limitations, this paper introduces CoverAssert, a novel iterative framework for optimizing SVA generation with LLMs. The core contribution is a lightweight mechanism for matching generated assertions with specific functional descriptions in the specifications. CoverAssert achieves this by clustering the joint representations of semantic features of LLM-generated assertions and structural features extracted from abstract syntax trees (ASTs) about signals related to assertions, and then mapping them back to the specifications to analyze functional coverage quality. Leveraging this capability, CoverAssert constructs a feedback loop based on functional coverage to guide LLMs in prioritizing uncovered functional points, thereby iteratively improving assertion quality. Experimental evaluations on four open-source designs demonstrate that integrating CoverAssert with state-of-the-art generators, AssertLLM and Spec2Assertion, achieves average improvements of 9.57 % in branch coverage, 9.64 % in statement coverage, and 15.69 % in toggle coverage.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research](https://arxiv.org/abs/2602.15034)
*Houping Yue,Zixiang Di,Mei Jiang,Bingdong Li,Hao Hao,Yu Song,Bo Jiang,Aimin Zhou*

Main category: cs.CL

TL;DR: The paper introduces EduResearchBench, a benchmark for evaluating AI in scholarly academic writing, and proposes a specialized model EduWrite for improved performance in this domain.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLMs are inadequate for evaluating their capacity in academic research due to their monolithic and single-shot focus, lacking the granularity required to reflect complex workflows.

Method: EduResearchBench uses a Hierarchical Atomic Task Decomposition (HATD) framework to break down research workflows into fine-grained tasks across six research modules and employs a curriculum learning strategy to train EduWrite, a specialized academic writing model.

Result: EduWrite (30B), trained with 11K curated instructional pairs, demonstrates superior performance compared to larger general-purpose models like a 72B model, validating the benefits of quality data density and structured training.

Conclusion: Fine-grained benchmarking and hierarchical task decomposition, combined with domain-specific training, significantly enhance the performance of AI in academic writing.

Abstract: While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.

</details>


### [34] [Indic-TunedLens: Interpreting Multilingual Models in Indian Languages](https://arxiv.org/abs/2602.15038)
*Mihir Panchal,Deeksha Varshney,Mamta,Asif Ekbal*

Main category: cs.CL

TL;DR: This paper introduces Indic-TunedLens, an interpretability framework optimized for Indian languages, showing improvements in analyzing multilingual models compared to current methods.


<details>
  <summary>Details</summary>
Motivation: Current interpretability tools for multilingual large language models are tailored mainly for English, necessitating more effective tools for languages like those in India.

Method: It uses shared affine transformations to adjust hidden states for specific target languages, enabling faithful decoding of model representations.

Result: Indic-TunedLens significantly outperforms state-of-the-art interpretability methods, especially for morphologically rich, low-resource Indian languages.

Conclusion: Indic-TunedLens provides better layer-wise semantic decoding across Indian languages and highlights multilingual transformers' processing intricacies.

Abstract: Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/AnonymousAccountACL/IndicTunedLens. Our code is available at https://github.com/AnonymousAccountACL/IndicTunedLens.

</details>


### [35] [CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding](https://arxiv.org/abs/2602.15139)
*Tahir Hussain,Saddam Hussain Khan*

Main category: cs.CL

TL;DR: The paper introduces CGRA DeBERTa, a specialized transformer for Question-Answering (QA) on Islamic texts, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Current QA systems struggle with context dependencies, semantic nuances, and theological reasoning in Islamic texts, necessitating an improved framework.

Method: The authors customized DeBERTa with LoRA adaptations, additionally using Concept Guided Residual Blocks and Concept Gating Mechanisms for semantic precision, integrated with an Islamic Concept Dictionary.

Result: CGRA DeBERTa achieved a 97.85 EM score, surpassing BERT (75.87) and DeBERTa (89.77), using a dataset of 42,591 QA pairs while maintaining computational efficiency.

Conclusion: The study introduces a domain-specific, efficient, and accurate QA model tailored for theological education and nuanced reasoning in Islamic texts.

Abstract: Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.

</details>


### [36] [AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking](https://arxiv.org/abs/2602.15190)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: This paper introduces a cost-effective and competitive multimodal fact-checking system that achieved 3rd place in a shared task. It combines retrieval-augmented generation (RAG) and reverse image search (RIS) modules.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to develop an efficient, simple, and reproducible system for multimodal fact-checking that combines textual and image retrieval, as well as LLM-based generation, to minimize costs and enhance experimentation accessibility.

Method: The system integrates three decoupled modules: a textual retrieval module (similarity search), an image retrieval module (API-based RIS), and a generation module using GPT5.1. These are combined to deliver a single multimodal LLM call per fact-check.

Result: The presented system achieved a competitive performance in the AVerImaTeC shared task (3rd place) while maintaining low costs, averaging $0.013 per fact-check.

Conclusion: The proposed system demonstrates the effectiveness of combining textual and image search with a multimodal LLM for efficient, low-cost fact-checking and serves as a reproducible and adaptable baseline for further research.

Abstract: In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.

</details>


### [37] [OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction](https://arxiv.org/abs/2602.15197)
*Skyler Hallinan,Thejas Venkatesh,Xiang Ren,Sai Praneeth Karimireddy,Ashwin Paranjape,Yuhao Zhang,Jack Hessel*

Main category: cs.CL

TL;DR: The paper introduces OpaqueToolsBench to evaluate how Large Language Models (LLMs) perform with poorly documented tools, and proposes the ToolObserver framework to iteratively refine tool documentation, achieving better efficiency and performance compared to baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance the capacity of LLM agents in utilizing unclear or opaque tools to complete tasks effectively, as real-world tools often lack proper documentation, making existing methods unreliable and costly.

Method: The authors design OpaqueToolsBench with task-oriented environments where tools are not well-documented and propose the ToolObserver framework, which iteratively improves tool documentation by leveraging execution feedback from tool usage.

Result: ToolObserver shows superior performance on OpaqueToolsBench compared to existing approaches and achieves greater efficiency by requiring significantly fewer tokens during test-time exploration.

Conclusion: The proposed approach demonstrates the potential of refining tool utilization in LLMs through iterative learning from feedback, addressing limitations of existing automatic documentation methods.

Abstract: Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general "search" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.

</details>


### [38] [Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement](https://arxiv.org/abs/2602.15312)
*Stephan Ludwig,Peter J. Danaher,Xiaohao Yang,Yu-Ting Lin,Ehsan Abedin,Dhruv Grewal,Lan Du*

Main category: cs.CL

TL;DR: The paper introduces the Linguistic eXtractor (LX), a fine-tuned language model for analyzing consumer emotions and evaluations from text, with high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of accurately measuring consumer emotions and evaluations from unstructured text, a crucial aspect of advancing marketing research and practice.

Method: The LX model is a fine-tuned, large language model designed to analyze consumer-authored text with labeled data for 16 emotions and evaluation constructs. Its accuracy was validated on open-ended surveys, Amazon, and Yelp reviews, outperforming existing models.

Result: LX achieved 81% macro-F1 accuracy on survey responses and over 95% accuracy on annotated reviews, while demonstrating that emotions and product ratings influence consumer purchase behavior. Its performance supports scalable analysis of consumer text.

Conclusion: LX introduces a new methodological foundation for measuring consumer perceptions, offering validated, scalable applications that leverage language models to enhance marketing research and practice.

Abstract: Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.

</details>


### [39] [Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory](https://arxiv.org/abs/2602.15313)
*Zihao Tang,Xin Yu,Ziyu Xiao,Zengxuan Wen,Zelin Li,Jiaxi Zhou,Hualei Wang,Haohua Wang,Haizhen Huang,Weiwei Deng,Feng Sun,Qi Zhang*

Main category: cs.CL

TL;DR: The paper presents Mnemis, a memory framework for LLMs integrating similarity-based and global-reasoning-based retrieval mechanisms, achieving state-of-the-art performance in long-term memory benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods (RAG and Graph-RAG) in retrieving relevant memory for Large Language Models, particularly when global reasoning or comprehensive information coverage is required.

Method: Mnemis combines a System-1 similarity search with a System-2 Global Selection mechanism, organizing memory into a base graph for similarity retrieval and a hierarchical graph for deliberate top-down traversal.

Result: Mnemis achieves state-of-the-art results, with scores of 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini, outperforming other methods in long-term memory benchmarks.

Conclusion: The dual approach of Mnemis demonstrates the advantages of combining similarity-based and global-reasoning-based retrieval for effective memory organization and retrieval in LLMs.

Abstract: AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.

</details>


### [40] [NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.15353)
*Rong Fu,Yang Li,Zeyu Zhang,Jiekai Wu,Yaohua Liu,Shuaishuai Cao,Yangchen Zeng,Yuhang Zhang,Xiaojing Du,Chuang Zhao,Kangning Cui,Simon Fong*

Main category: cs.CL

TL;DR: The paper introduces NeuroSymActive, a framework combining neural-symbolic reasoning with active exploration for efficient Knowledge Graph Question Answering (KGQA).


<details>
  <summary>Details</summary>
Motivation: Current language models and reasoning systems struggle with precise, structured, multi-hop inference for knowledge-intensive tasks. Standard methods either introduce inefficiencies or lack gradient refinement.

Method: NeuroSymActive integrates a differentiable neural-symbolic reasoning layer with a value-guided exploration controller, leveraging a neural path evaluator and Monte-Carlo exploration to optimize path expansions and graph lookups.

Result: The framework achieves strong answer accuracy on KGQA benchmarks, while reducing costly graph lookups and model calls compared to traditional baselines.

Conclusion: NeuroSymActive offers an efficient and accurate approach to KGQA, demonstrating advancements in incorporating symbolic and neural methodologies for addressing knowledge-intensive tasks.

Abstract: Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.

</details>


### [41] [Far Out: Evaluating Language Models on Slang in Australian and Indian English](https://arxiv.org/abs/2602.15373)
*Deniz Kaya Dilsiz,Dipankar Srirag,Aditya Joshi*

Main category: cs.CL

TL;DR: The paper evaluates slang comprehension in Indian and Australian English across seven language models, using two datasets and three tasks, revealing disparities in model performance based on slang variety and context.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how language models process slang in non-standard English varieties, specifically Indian and Australian English.

Method: The authors created two datasets (web-sourced and synthetically generated) and evaluated models using three tasks related to target word prediction and selection for slang terms.

Result: Key findings include better model performance on web-sourced data and en-IN slang over en-AU across all models and tasks, with noticeable performance gaps between tasks and language varieties.

Conclusion: The study highlights the challenges language models face with slang in English varieties, emphasizing the need for improved models in generative and discriminative slang comprehension.

Abstract: Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) and Australian English (en-AU) across seven state-of-the-art language models. We construct two complementary datasets: \textsc{web}, containing 377 web-sourced usage examples from Urban Dictionary, and \textsc{gen}, featuring 1,492 synthetically generated usages of these slang terms, across diverse scenarios. We assess language models on three tasks: target word prediction (TWP), guided target word prediction (TWP$^*$) and target word selection (TWS). Our results reveal four key findings: (1) Higher average model performance TWS versus TWP and TWP$^*$, with average accuracy score increasing from 0.03 to 0.49 respectively (2) Stronger average model performance on \textsc{web} versus \textsc{gen} datasets, with average similarity score increasing by 0.03 and 0.05 across TWP and TWP$^*$ tasks respectively (3) en-IN tasks outperform en-AU when averaged across all models and datasets, with TWS demonstrating the largest disparity, increasing average accuracy from 0.44 to 0.54. These findings underscore fundamental asymmetries between generative and discriminative competencies for variety-specific language, particularly in the context of slang expressions despite being in a technologically rich language such as English.

</details>


### [42] [Orchestration-Free Customer Service Automation: A Privacy-Preserving and Flowchart-Guided Framework](https://arxiv.org/abs/2602.15377)
*Mengze Hong,Chen Jason Zhang,Zichang Guo,Hanlin Gu,Di Jiang,Li Qing*

Main category: cs.CL

TL;DR: The paper introduces Task-Oriented Flowcharts (TOFs) to achieve end-to-end customer service automation without manual orchestration.


<details>
  <summary>Details</summary>
Motivation: To address limitations in customer service automation, including dependency on modular designs and poor generalizability of existing systems.

Method: Developed an orchestration-free framework using TOFs, a flowchart construction algorithm, and local deployment of small language models with decentralized distillation.

Result: Demonstrated higher efficiency and performance on various service tasks compared to strong baselines and market products.

Conclusion: The proposed framework streamlines the automation of customer service tasks and provides a web-based demo to facilitate broader adoption.

Abstract: Customer service automation has seen growing demand within digital transformation. Existing approaches either rely on modular system designs with extensive agent orchestration or employ over-simplified instruction schemas, providing limited guidance and poor generalizability. This paper introduces an orchestration-free framework using Task-Oriented Flowcharts (TOFs) to enable end-to-end automation without manual intervention. We first define the components and evaluation metrics for TOFs, then formalize a cost-efficient flowchart construction algorithm to abstract procedural knowledge from service dialogues. We emphasize local deployment of small language models and propose decentralized distillation with flowcharts to mitigate data scarcity and privacy issues in model training. Extensive experiments validate the effectiveness in various service tasks, with superior quantitative and application performance compared to strong baselines and market products. By releasing a web-based system demonstration with case studies, we aim to promote streamlined creation of future service automation.

</details>


### [43] [Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language](https://arxiv.org/abs/2602.15378)
*Prathamesh Devadiga,Paras Chopra*

Main category: cs.CL

TL;DR: This paper evaluates the ability of large language models (LLMs) to engage in basic conversations in Tulu, a low-resource language, without fine-tuning. Structured prompts and techniques like grammar documentation and negative constraints are applied to tackle challenges arising from limited training data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of enabling LLMs to comprehend and converse in Tulu, a language virtually absent from their training data. The motivation lies in expanding LLM accessibility to low-resource languages with limited or no digital representations.

Method: The authors use structured prompts instead of fine-tuning to evoke conversational skills in Tulu. They combine grammar documentation, negative constraints to reduce contamination from related languages, standardized romanization, and synthetic data created via self-play to improve outputs. Three LLMs are evaluated using a curated dataset and native speaker validation.

Result: The approach reduced vocabulary contamination from 80% to 5% and achieved 85% grammatical accuracy. Negative constraints improved performance consistently (12-18 percentage points), while the effects of grammar documentation varied by model architecture (8-22 percentage points).

Conclusion: Structured prompting supplemented by explicit linguistic information and constraints can significantly enhance LLMs' ability to handle low-resource languages like Tulu, highlighting ways to bridge gaps in digital representation for under-represented languages.

Abstract: Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).

</details>


### [44] [The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems](https://arxiv.org/abs/2602.15382)
*Xiaoze Liu,Ruowang Zhang,Weichen Yu,Siheng Xiong,Liu He,Feijie Wu,Hoin Jung,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.CL

TL;DR: The paper introduces the Vision Wormhole, a framework enabling text-free communication between models using the visual interface of Vision-Language Models, reducing runtime and maintaining reasoning fidelity.


<details>
  <summary>Details</summary>
Motivation: The inefficiency of text communication in Multi-Agent Systems leads to runtime overhead and information loss. The paper addresses scalability and modularity challenges when transferring latent states between heterogeneous model families.

Method: The proposed Vision Wormhole uses a Universal Visual Codec to map reasoning traces into a shared latent space, facilitating model-agnostic communication via a visual pathway. It employs a hub-and-spoke topology and label-free teacher-student distillation for efficient alignment.

Result: Experiments involving heterogeneous models like Qwen-VL and Gemma demonstrate that the framework reduces end-to-end runtime while preserving reasoning quality compared to traditional text-based communication.

Conclusion: The Vision Wormhole enhances communication efficiency in MAS by repurposing visual pathways for transmission, offering a scalable, modular, and reasoning-fidelity-preserving solution.

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas

</details>


### [45] [TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models](https://arxiv.org/abs/2602.15449)
*Chansung Park,Juyong Jiang,Fan Wang,Sayak Paul,Jiasi Shen,Jing Tang,Jianguo Li*

Main category: cs.CL

TL;DR: The paper proposes TAROT, a curriculum-based reinforcement fine-tuning method tailored for large language models, improving their code generation abilities with controlled test case difficulties.


<details>
  <summary>Details</summary>
Motivation: Current methods of enhancing LLMs for robust code generation struggle due to biased reward signals from test cases with heterogeneous difficulty. A new approach is needed for stable optimization.

Method: TAROT introduces a four-tier test suite (basic, intermediate, complex, edge) for curriculum design, decoupling curriculum progression from raw reward scores. It adapts policies based on the model's capability.

Result: Experimental results show less capable models benefit from an easy-to-hard progression, while more advanced models excel with a hard-first curriculum. TAROT improves code correctness and robustness for LLMs.

Conclusion: TAROT provides a reproducible adaptive curriculum design tailored to LLM capability, advancing functional correctness and community research on code generation.

Abstract: Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.

</details>


### [46] [Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs](https://arxiv.org/abs/2602.15436)
*Joonatan Laato,Veera Schroderus,Jenna Kanerva,Jenni Kauppi,Virpi Lummaa,Filip Ginter*

Main category: cs.CL

TL;DR: The paper develops a framework to categorize activities and organizations from large-scale historical interviews and tests large language models' ability to perform this categorization at scale.


<details>
  <summary>Details</summary>
Motivation: There is a need to bridge the gap between data extracted from historical archives and the quantitative research questions posed by historians and sociologists.

Method: The paper creates a categorization framework, annotates a gold-standard dataset, tests large language models for scalable categorization with a voting approach, and labels the dataset for downstream analysis.

Result: Large language models were able to closely match expert judgments using the framework, enabling the categorization of 350K activity and organization mentions.

Conclusion: The resource created via LLM-based categorization aids the systematic study of social integration in historical data.

Abstract: Digitized historical archives make it possible to study everyday social life on a large scale, but the information extracted directly from text often does not directly allow one to answer the research questions posed by historians or sociologists in a quantitative manner. We address this problem in a large collection of Finnish World War II Karelian evacuee family interviews. Prior work extracted more than 350K mentions of leisure time activities and organizational memberships from these interviews, yielding 71K unique activity and organization names -- far too many to analyze directly.
  We develop a categorization framework that captures key aspects of participation (the kind of activity/organization, how social it typically is, how regularly it happens, and how physically demanding it is). We annotate a gold-standard set to allow for a reliable evaluation, and then test whether large language models can apply the same schema at scale. Using a simple voting approach across multiple model runs, we find that an open-weight LLM can closely match expert judgments. Finally, we apply the method to label the 350K entities, producing a structured resource for downstream studies of social integration and related outcomes.

</details>


### [47] [In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations](https://arxiv.org/abs/2602.15456)
*Mohammad Aflah Khan,Mahsa Amani,Soumi Das,Bishwamittra Ghosh,Qinyuan Wu,Krishna P. Gummadi,Manish Gupta,Abhilasha Ravichander*

Main category: cs.CL

TL;DR: The paper investigates biases in LLM agents regarding source preferences in information selection, revealing systematic tendencies across multiple models.


<details>
  <summary>Details</summary>
Motivation: To understand the factors influencing which information LLMs select and present to users, focusing on latent source preferences rather than LLM-generated biases.

Method: Conducted controlled experiments on twelve LLMs from six providers, analyzing their behavior in both synthetic and real-world tasks.

Result: Findings reveal predictable and strong source preferences, influenced by contextual framing and resistant to explicit prompts to avoid them.

Conclusion: The study underscores the need for probing the origins of these preferences and developing mechanisms for transparency and user control over LLM biases.

Abstract: Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.

</details>


### [48] [Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit](https://arxiv.org/abs/2602.15504)
*Aswathy Velutharambath,Amelie Wührl*

Main category: cs.CL

TL;DR: The study examines online patient expectations using NLP to detect, label, and analyze them, introducing the Expectation Detection task and contributing the RedHOTExpect corpus focusing on insights extracted from Reddit posts.


<details>
  <summary>Details</summary>
Motivation: Patients' expectations significantly affect treatment outcomes but are understudied, especially potential insights from online forums like Reddit. Exploring expectations here could reveal perspectives not often shared elsewhere.

Method: The authors created the RedHOTExpect corpus, which includes 4.5K Reddit posts. They used a large language model (LLM) for silver-labeling the dataset and validated it manually for accuracy (~78%).

Result: Analysis shows distinct linguistic patterns, with optimism and proactive framing more notable in physical-treatment discussions. Patients predominantly discuss treatment benefits rather than negative outcomes.

Conclusion: Detecting expectations through NLP in online platforms can yield valuable insights, particularly for opinion mining and medical applications, and the RedHOTExpect corpus offers a new resource for this research.

Abstract: Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect

</details>


### [49] [LuxMT Technical Report](https://arxiv.org/abs/2602.15506)
*Nils Rehlinger*

Main category: cs.CL

TL;DR: LuxMT is a translation system using Gemma 3 27B, fine-tuned for Luxembourgish to French and English translations, showcasing benchmark advancements and data-filtering innovations.


<details>
  <summary>Details</summary>
Motivation: To improve machine translation capabilities for underrepresented languages like Luxembourgish and evaluate it through a novel benchmark dataset.

Method: Fine-tuning Gemma 3 27B model using parallel corpora (LuxAlign and other datasets), applying LuxEmbedder for data filtering, and using LuxEmbedder as a metric for translation quality estimation.

Result: LuxMT showed significant improvements over the baseline, including unexpected performance in Luxembourgish-to-German translations despite no German training data.

Conclusion: LuxMT improves Luxembourgish translations and unveils potential for LuxEmbedder as a metric. Further research on the metric's application is encouraged.

Abstract: We introduce LuxMT, a machine translation system based on Gemma 3 27B and fine-tuned for translation from Luxembourgish (LB) into French (FR) and English (EN). To assess translation performance, we construct a novel benchmark covering LB-FR, LB-EN, and LB-FR using human-translated data from Luci, a tourist magazine about Luxembourg. Training data stems from LuxAlign, a parallel corpus of multilingual Luxembourgish news articles, and LB parliamentary transcripts augmented with Google Translate. We filter the data using LuxEmbedder, LB sentence embeddings, to remove low-equivalence segment-pairs. Overall, LuxMT's results suggest strong improvements over the Gemma 3 baseline, even for translating LB to German (DE), despite the training data not containing any DE. We also explore LuxEmbedder's potential to be used as a quality estimation metric and find strong correlations with other reference-based metrics. However, we call for further research to fully assess the metric's utility and advise using it with caution.

</details>


### [50] [Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination](https://arxiv.org/abs/2602.15509)
*Xiangyan Chen,Yujian Gan,Matthew Purver*

Main category: cs.CL

TL;DR: The paper addresses hallucinations in dialogue systems of large language models by introducing Fine-Refine, a method for fine-grained refinement of responses, enhancing factual accuracy and coverage.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLM-powered dialogue systems produce incorrect responses, undermining user trust and exposing a need for better factual verification methods in responses.

Method: The proposed Fine-Refine framework breaks down responses into atomic units, verifies them using external knowledge, assesses fluency through perplexity, and iteratively corrects errors to improve factual integrity.

Result: Fine-Refine improves factual accuracy significantly, showing a gain of up to 7.63 points in dialogue fact score, with minor trade-offs in dialogue quality.

Conclusion: Fine-Refine demonstrates effective enhancement of factuality and coverage in dialogue systems, providing a noteworthy solution for addressing hallucinations in LLMs.

Abstract: The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.

</details>


### [51] [DependencyAI: Detecting AI Generated Text through Dependency Parsing](https://arxiv.org/abs/2602.15514)
*Sara Ahmed,Tracy Hammond*

Main category: cs.CL

TL;DR: DependencyAI uses linguistic dependency labels to detect AI-generated text with competitive performance and interpretability, highlighting patterns that differentiate AI from human writing.


<details>
  <summary>Details</summary>
Motivation: With the rise of AI-generated text, reliable detection methods are essential to address safety and ethical concerns associated with its misuse.

Method: DependencyAI employs linguistic dependency relation labels instead of neural networks to detect AI-generated text. Feature importance analysis identifies indicative syntactic structures.

Result: DependencyAI demonstrates strong performance across multiple settings (monolingual, multi-generator, multilingual) and exposes challenges like model overprediction in unseen domains.

Conclusion: DependencyAI provides an interpretable, linguistically grounded, and robust method for detecting AI-generated text and serves as an effective non-neural network alternative.

Abstract: As large language models (LLMs) become increasingly prevalent, reliable methods for detecting AI-generated text are critical for mitigating potential risks. We introduce DependencyAI, a simple and interpretable approach for detecting AI-generated text using only the labels of linguistic dependency relations. Our method achieves competitive performance across monolingual, multi-generator, and multilingual settings. To increase interpretability, we analyze feature importance to reveal syntactic structures that distinguish AI-generated from human-written text. We also observe a systematic overprediction of certain models on unseen domains, suggesting that generator-specific writing styles may affect cross-domain generalization. Overall, our results demonstrate that dependency relations alone provide a robust signal for AI-generated text detection, establishing DependencyAI as a strong linguistically grounded, interpretable, and non-neural network baseline.

</details>


### [52] [ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns](https://arxiv.org/abs/2602.15521)
*Ziyu Zhao,Tong Zhu,Zhi Zhang,Tiantian Fan,Jinluan Yang,Kun Kuang,Zhongyu Wei,Fei Wu,Yu Cheng*

Main category: cs.CL

TL;DR: The paper introduces ExpertWeaver, a method to convert dense models into sparse Mixture-of-Experts (MoE) using a training-free framework based on Gated Linear Unit (GLU) activation patterns.


<details>
  <summary>Details</summary>
Motivation: Training sparse Mixture-of-Experts (MoE) models from scratch is computationally expensive, making the conversion of pretrained dense models into sparse MoEs a more viable solution. Existing methods disrupt intrinsic activation patterns, leading to suboptimal expert construction.

Method: The authors propose ExpertWeaver, which reveals an inherent MoE architecture in dense models based on GLU activation patterns. It partitions neurons into universal and specialized experts with adaptive configurations without additional training.

Result: ExpertWeaver outperforms current dense-to-MoE methods both as a training-free structural pruning approach and for initializing sparse MoE architectures with improved performance.

Conclusion: ExpertWeaver effectively unlocks the potential of pretrained dense models, efficiently constructing high-quality sparse MoEs without additional training overhead.

Abstract: Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.

</details>


### [53] [ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling](https://arxiv.org/abs/2602.15537)
*Nicol Visser,Simon Malan,Danel Slabbert,Herman Kamper*

Main category: cs.CL

TL;DR: ZeroSyl simplifies pure speech language modeling by directly extracting syllable features from frozen WavLM without complex training pipelines, achieving competitive results in segmentation and surpassing existing tokenizers.


<details>
  <summary>Details</summary>
Motivation: The challenge in pure speech language modeling is handling excessively long token sequences generated by self-supervised speech encoders, prompting the need for syllable-like units and simpler pipelines.

Method: ZeroSyl leverages L2 norms of features in WavLM's intermediate layers for syllable boundary detection, mean-pools segments, discretizes with K-means, and employs the outputs to train a language model.

Result: ZeroSyl achieves competitive syllable segmentation and outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks, with superior scalability for syntactic modeling.

Conclusion: ZeroSyl demonstrates a streamlined and effective approach for syllable unit extraction and language modeling, offering advantages over prior methods in simplicity and performance scalability.

Abstract: Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.

</details>


### [54] [Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite](https://arxiv.org/abs/2602.15540)
*Tim Fischer,Chris Biemann*

Main category: cs.CL

TL;DR: The paper presents 'Perspectives,' an extension for Digital Humanities researchers to organize and explore large text collections through interactive clustering and refinement tools.


<details>
  <summary>Details</summary>
Motivation: To help Digital Humanities scholars efficiently analyze large and unstructured text datasets by providing interactive, human-guided tools for clustering and exploring textual content.

Method: The authors propose a document clustering pipeline that integrates user-defined lenses, refinement tools, and mechanisms for fine-tuning embedding models, enabling human-in-the-loop exploration and categorization of text.

Result: Perspectives enables researchers to interactively map documents, refining clusters and embeddings to reveal meaningful textual patterns such as topics and sentiments.

Conclusion: The tool helps DH scholars gain insights into large document collections, making their data more organized and prepared for deeper analysis through an interactive approach.

Abstract: This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives implements a flexible, aspect-focused document clustering pipeline with human-in-the-loop refinement capabilities. We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.

</details>


### [55] [jina-embeddings-v5-text: Task-Targeted Embedding Distillation](https://arxiv.org/abs/2602.15547)
*Mohammad Kalim Akram,Saba Sturua,Nastia Havriushenko,Quentin Herreros,Michael Günther,Maximilian Werk,Han Xiao*

Main category: cs.CL

TL;DR: Introduces a new training regimen combining distillation techniques and task-specific contrastive loss to create compact, high-performance text embedding models outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: To improve the performance and compactness of text embedding models, especially for tasks like similarity measurement, retrieval, clustering, and classification.

Method: Combines contrastive loss functions with distillation techniques to train embedding models, focusing on creating smaller, efficient models with task-specific optimizations.

Result: Created jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, surpassing state-of-the-art benchmarks for similar-sized models while supporting long texts and robustly handling truncation and quantization.

Conclusion: The approach is effective for developing compact and high-quality embedding models, inspiring further research and advancements in the field.

Abstract: Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.

</details>


### [56] [Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL](https://arxiv.org/abs/2602.15564)
*Yihan Wang,Peiyu Liu,Runyu Chen,Wei Xu*

Main category: cs.CL

TL;DR: SquRL addresses the limitations of static workflows in Text-to-SQL by introducing a dynamic workflow construction framework using reinforcement learning, achieving superior adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: The reliance on static workflows in Text-to-SQL limits effectiveness in out-of-distribution and complex scenarios, requiring a framework that adapts dynamically to changing input conditions.

Method: SquRL employs reinforcement learning, using a rule-based reward function, dynamic actor masking, and pseudo rewards for efficient training to construct workflows adaptively during inference.

Result: Dynamic workflows constructed by SquRL outperform static workflow methods on Text-to-SQL benchmarks, particularly in handling complex and out-of-distribution queries.

Conclusion: Dynamic workflow construction via SquRL provides a scalable and adaptive solution to improve Text-to-SQL performance, demonstrating robust gains in diverse scenarios.

Abstract: Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL

</details>


### [57] [Clinically Inspired Symptom-Guided Depression Detection from Emotion-Aware Speech Representations](https://arxiv.org/abs/2602.15578)
*Chaithra Nerella,Chiranjeevi Yarra*

Main category: cs.CL

TL;DR: The paper introduces a symptom-specific framework for estimating depression severity from speech, using a PHQ-8 guided cross-attention mechanism and emotion-aware representations.


<details>
  <summary>Details</summary>
Motivation: Current depression prediction methods lack symptom-specific analysis, which limits their utility for clinical screening.

Method: The authors propose a speech-based framework using symptom-guided cross-attention mechanisms that align PHQ-8 questionnaire items with emotion-aware speech features. They also introduce a learnable symptom-specific parameter for adaptive attention sharpness.

Result: The proposed framework outperforms prior approaches on the EDAIC dataset and provides interpretable results, showing that attention highlights speech segments relevant to multiple depressive symptoms.

Conclusion: The study emphasizes the need for symptom-guided, emotion-aware depression modeling for effective and interpretable speech-based clinical screening.

Abstract: Depression manifests through a diverse set of symptoms such as sleep disturbance, loss of interest, and concentration difficulties. However, most existing works treat depression prediction either as a binary label or an overall severity score without explicitly modeling symptom-specific information. This limits their ability to provide symptom-level analysis relevant to clinical screening. To address this, we propose a symptom-specific and clinically inspired framework for depression severity estimation from speech. Our approach uses a symptom-guided cross-attention mechanism that aligns PHQ-8 questionnaire items with emotion-aware speech representations to identify which segments of a participant's speech are more important to each symptom. To account for differences in how symptoms are expressed over time, we introduce a learnable symptom-specific parameter that adaptively controls the sharpness of attention distributions. Our results on EDAIC, a standard clinical-style dataset, demonstrate improved performance outperforming prior works. Further, analyzing the attention distributions showed that higher attention is assigned to utterances containing cues related to multiple depressive symptoms, highlighting the interpretability of our approach. These findings outline the importance of symptom-guided and emotion-aware modeling for speech-based depression screening.

</details>


### [58] [STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens](https://arxiv.org/abs/2602.15620)
*Shiqi Liu,Zeyu He,Guojian Zhan,Letian Tao,Zhilong Zheng,Jiang Wu,Yinuo Wang,Yang Guan,Kehua Sheng,Bo Zhang,Keqiang Li,Jingliang Duan,Shengbo Eben Li*

Main category: cs.CL

TL;DR: STAPO addresses RL fine-tuning instability by masking spurious token updates, leading to improved reasoning, stability, and performance gains.


<details>
  <summary>Details</summary>
Motivation: To address reasoning quality degradation and instability caused by spurious tokens during RL fine-tuning for large language models.

Method: STAPO selectively masks spurious token updates, renormalizes losses, and improves policy optimization.

Result: STAPO enhanced entropy stability and improved reasoning accuracy by 7.13% over existing methods in six benchmarks.

Conclusion: STAPO is an effective solution for mitigating RL fine-tuning instability while improving large model reasoning performance.

Abstract: Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% over GRPO, 20-Entropy and JustRL.

</details>


### [59] [LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models](https://arxiv.org/abs/2602.15675)
*Ahmed Khaled Khamis,Hesham Ali*

Main category: cs.CL

TL;DR: This paper introduces NileTTS, the first publicly available Egyptian Arabic TTS dataset, with 38 hours of synthesized speech and an open-source fine-tuned model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the severe under-resourcing of TTS systems for Egyptian Arabic, despite its wide usage and importance, compared to resources available for MSA and Gulf dialects.

Method: The authors use a novel pipeline with large language models to generate content in Egyptian Arabic, which is synthesized into speech, transcribed automatically, checked manually, and later used to fine-tune the XTTS v2 TTS model.

Result: The resulting contributions include a new Egyptian Arabic TTS dataset, a reproducible synthetic data generation pipeline for dialectal TTS, and an open-source fine-tuned model outperforming others in Egyptian Arabic synthesis.

Conclusion: All introduced resources are released to advance research in Egyptian Arabic TTS, bridging a significant gap for this widely understood yet underserved dialect.

Abstract: Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.

</details>


### [60] [Revisiting Northrop Frye's Four Myths Theory with Large Language Models](https://arxiv.org/abs/2602.15678)
*Edirlei Soares de Lima,Marco A. Casanova,Antonio L. Furtado*

Main category: cs.CL

TL;DR: The paper proposes a character function framework to complement narrative pattern analysis based on Frye's genres, validated using large language models (LLMs) on 40 narrative works.


<details>
  <summary>Details</summary>
Motivation: Despite the profound influence of Frye's theory of four narrative genres, computational analysis has largely ignored character functions, focusing on patterns instead.

Method: They developed a framework of four universal character functions (protagonist, mentor, antagonist, companion) rooted in Jungian archetype theory, specialized into 16 genre-specific roles, validated using multi-model studies with six LLMs.

Result: LLMs achieved 82.5% balanced accuracy in recognizing valid character-role correspondences across genres, with substantial inter-model agreement. Performance varied by genre and role, reflecting genuine narrative characteristics.

Conclusion: The study underscores the usefulness of character function frameworks and LLM-based methods in computational narratology, with implications for narrative generation and interactive storytelling development.

Abstract: Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $κ$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.

</details>


### [61] [A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models](https://arxiv.org/abs/2602.15689)
*Meirav Segal,Noa Linder,Omer Antverg,Gil Gekker,Tomer Fichman,Omri Bodenheimer,Edan Maor,Omer Nevo*

Main category: cs.CL

TL;DR: This paper introduces a content-based framework that evaluates cybersecurity requests by considering offense-defense tradeoffs, aiming to improve refusal policies of LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing refusal policies for cybersecurity tasks in large language models are inconsistent, overly restrictive, and fail to address obfuscation or segmented requests. A more refined and explicit approach is needed.

Method: The authors propose a framework grounded in five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users.

Result: The content-based framework resolves inconsistencies in model behavior and helps organizations develop more effective, risk-aware refusal policies.

Conclusion: This approach offers a tunable solution for organizations, enhancing their refusal policies by explicitly balancing offensive risks with defensive benefits.

Abstract: Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.

</details>


### [62] [Rethinking Metrics for Lexical Semantic Change Detection](https://arxiv.org/abs/2602.15716)
*Roksana Goworek,Haim Dubossarsky*

Main category: cs.CL

TL;DR: The paper introduces new metrics (AMD and SAMD) for detecting semantic changes in words over time using contextual embeddings, showing their advantages compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness and accuracy of lexical semantic change detection in language models, moving beyond commonly used metrics like APD and PRT.

Method: Introduces two metrics, Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), and evaluates their performance across different languages, encoder models, and reduced dimensionalities.

Result: AMD demonstrated robust performance in varied settings, while SAMD showed superiority with specialised encoders for semantic change detection.

Conclusion: The proposed metrics offer valuable alternatives for LSCD analysis, with AMD providing reliability in diverse contexts and SAMD specialising in certain encoder conditions.

Abstract: Lexical semantic change detection (LSCD) increasingly relies on contextualised language model embeddings, yet most approaches still quantify change using a small set of semantic change metrics, primarily Average Pairwise Distance (APD) and cosine distance over word prototypes (PRT). We introduce Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), new measures that quantify semantic change via local correspondence between word usages across time periods. Across multiple languages, encoder models, and representation spaces, we show that AMD often provides more robust performance, particularly under dimensionality reduction and with non-specialised encoders, while SAMD excels with specialised encoders. We suggest that LSCD may benefit from considering alternative semantic change metrics beyond APD and PRT, with AMD offering a robust option for contextualised embedding-based analysis.

</details>


### [63] [Causal Effect Estimation with Latent Textual Treatments](https://arxiv.org/abs/2602.15730)
*Omri Feldman,Amar Venugopal,Jann Spiess,Amir Feder*

Main category: cs.CL

TL;DR: The paper introduces a pipeline for generating and estimating causal effects of textual interventions using sparse autoencoders and robust estimation methods.


<details>
  <summary>Details</summary>
Motivation: The need to understand the causal impact of textual content on outcomes, requiring systematic experimentation with textual features.

Method: Developing a pipeline that incorporates hypothesis generation, steering via sparse autoencoders, and causal estimation while addressing bias through covariate residualization.

Result: The pipeline successfully induces variation in textual features and reduces estimation errors, outperforming naive methods.

Conclusion: This approach provides an effective framework for estimating causal effects in text-as-treatment experiments with reduced bias and improved reliability.

Abstract: Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.

</details>


### [64] [Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac](https://arxiv.org/abs/2602.15753)
*Chahan Vidal-Gorène,Bastien Kindt,Florian Cafiero*

Main category: cs.CL

TL;DR: This paper evaluates the performance of Large Language Models (LLMs) in lemmatization and part-of-speech tagging for four low-resource languages, comparing them to a task-specific baseline and showing their competitive potential.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of Natural Language Processing in under-resourced languages using few-shot and zero-shot settings, leveraging recent advances in LLMs.

Method: The authors used GPT-4 variants, Mistral models, and a novel benchmark of aligned training and out-of-domain test corpora to assess LLM performance in lemmatization and POS-tagging, comparing results to a task-specific RNN baseline.

Result: LLMs demonstrated competitive or superior performance in most few-shot settings, handling POS-tagging and lemmatization effectively but struggling with complex morphology and non-Latin scripts.

Conclusion: LLMs are a promising option for initial linguistic annotation of low-resource languages when data is scarce, aiding annotation processes even without fine-tuning.

Abstract: Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.

</details>


### [65] [Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos](https://arxiv.org/abs/2602.15757)
*Laura De Grazia,Danae Sánchez Villegas,Desmond Elliott,Mireia Farrús,Mariona Taulé*

Main category: cs.CL

TL;DR: The paper introduces FineMuSe, a Spanish multimodal sexism detection dataset with both binary and fine-grained annotations, alongside a new taxonomy and evaluation of LLMs for nuanced sexism detection.


<details>
  <summary>Details</summary>
Motivation: Current sexism detection tools are often limited to binary classifications, which fail to capture subtle and nuanced forms of sexism.

Method: The authors developed FineMuSe, a dataset with detailed annotations and a hierarchical taxonomy of sexism, and evaluated multiple multimodal LLMs for both binary and fine-grained detection tasks.

Result: Multimodal LLMs showed competitive performance with human annotators in identifying nuanced sexism, but struggled with co-occurring sexism types in visual cues.

Conclusion: FineMuSe advances sexism detection by highlighting limitations in LLMs for multi-modal contexts and by offering a finer-grained approach to capturing sexism nuances.

Abstract: Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.

</details>


### [66] [ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models](https://arxiv.org/abs/2602.15758)
*Manav Nitin Kapadnis,Lawanya Baghel,Atharva Naik,Carolyn Rosé*

Main category: cs.CL

TL;DR: The paper introduces ChartEditBench, a benchmark for evaluating multimodal large language models in multi-turn, visually grounded chart editing, highlighting their challenges in maintaining context over edits.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored area of how MLLMs handle iterative, multi-turn chart editing required in real-world data analysis.

Method: The authors created ChartEditBench, which consists of 5,000 difficulty-controlled modification chains and integrates robust evaluation criteria such as visual similarity, execution fidelity, and logical code checks.

Result: Experiments showed that MLLMs perform well on stylistic edits but struggle with data transformations and maintaining context over multiple turns due to error accumulation.

Conclusion: The benchmark demonstrates the limitations of current models in real-world exploratory data analysis, emphasizing the need for improved multi-turn, context-aware multimodal programming capabilities.

Abstract: While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.

</details>


### [67] [ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution](https://arxiv.org/abs/2602.15769)
*Yahia Alqurnawi,Preetom Biswas,Anmol Rao,Tejas Anvekar,Chitta Baral,Vivek Gupta*

Main category: cs.CL

TL;DR: The paper evaluates Multimodal Large Language Models (mLLMs) for their ability to provide attribution to structured data, finding significant gaps in evidence citation accuracy despite moderate question-answering performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of providing transparent and trustworthy attributions for mLLM-generated answers, especially when these models process structured data like tables, JSON, and images.

Method: The paper evaluates multiple mLLMs across different formats (e.g., tables, JSON, images), leveraging diverse prompting strategies to measure their performance in both question answering and evidence attribution.

Result: The study finds that while mLLMs exhibit moderate accuracy in answering questions, the attribution accuracy is much lower, near random for JSON inputs, and shows variability in performance across models and data types.

Conclusion: Current mLLMs are inadequate for fine-grained and reliable evidence attribution for structured data, posing challenges for use in applications where transparency and traceability are critical.

Abstract: Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.

</details>


### [68] [*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation](https://arxiv.org/abs/2602.15778)
*Quentin Lemesle,Léane Jourdan,Daisy Munson,Pierre Alain,Jonathan Chevelu,Arnaud Delhay,Damien Lolive*

Main category: cs.CL

TL;DR: The paper proposes improved variants of a perplexity-based text quality evaluation metric to enhance efficiency and alignment with human judgment.


<details>
  <summary>Details</summary>
Motivation: Text quality evaluation often requires computationally expensive and complex LLM-judge methods. The study aims to simplify this process while improving alignment with human assessments.

Method: The paper extended the perplexity-based evaluation metric, ParaPLUIE, with task-specific prompting variants (*-PLUIE) and measured their performance against human judgment.

Result: Personalised *-PLUIE variants showed better correlation with human ratings and remained computationally efficient compared to traditional methods.

Conclusion: The proposed *-PLUIE approach offers an effective and efficient alternative to text evaluation methods, reducing computational demands and improving alignment with human evaluations.

Abstract: Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.

</details>


### [69] [Avey-B](https://arxiv.org/abs/2602.15814)
*Devang Acharya,Mohammad Hammoud*

Main category: cs.CL

TL;DR: This paper introduces an innovative reformulation of Avey for encoder-only NLP tasks, achieving superior performance and efficiency compared to widely-used Transformer-based encoders.


<details>
  <summary>Details</summary>
Motivation: Compact bidirectional encoders are essential for NLP tasks requiring efficient compute and memory usage, prompting the exploration for alternatives to traditional architectures like BERT.

Method: The reformulated Avey architecture incorporates innovations such as decoupled static and dynamic parameterizations, improved normalization techniques for stability, and neural compression for compactness.

Result: The proposed architecture consistently outperforms four widely used Transformer-based encoders on token-classification and information-retrieval benchmarks, with enhanced efficiency in handling long contexts.

Conclusion: Reformulated Avey offers a competitive attention-free alternative to bidirectional Transformers, combining superior scalability and effectiveness for industrial NLP applications.

Abstract: Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [70] [GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation](https://arxiv.org/abs/2602.15072)
*Abdul Joseph Fofanah,Lian Wen,Alpha Alimamy Kamara,Zhongyi Zhang,David Chen,Albert Patrick Sankoh*

Main category: cs.CV

TL;DR: The paper proposes GRAFNet, an innovative deep learning model inspired by human visual system principles, to improve polyp segmentation in colonoscopy. It demonstrates superior accuracy across benchmarks by addressing key challenges like high variability and strong visual similarities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in accurate polyp segmentation during colonoscopy, such as high morphological variability, the visual similarity of polyps to normal structures, and the need for robust multi-scale detection, which current methods fail to resolve effectively.

Method: The paper introduces GRAFNet, comprising three key modules: Guided Asymmetric Attention Module (GAAM) for better boundary detection, MultiScale Retinal Module (MSRM) for parallel multi-feature analysis, and Guided Cortical Attention Feedback Module (GCAFM) for iterative refinement. These modules are unified in a Polyp Encoder-Decoder Module (PEDM) to ensure spatial-semantic consistency.

Result: GRAFNet achieves consistent state-of-the-art performance across five public datasets. It shows 3-8% Dice improvements and 10-20% higher generalization over existing methods, while offering interpretable decision pathways.

Conclusion: GRAFNet bridges the gap between AI accuracy and clinical trustworthiness in polyp segmentation, setting a new standard for biologically inspired architectures. It demonstrates robust performance and interpretability across diverse datasets.

Abstract: Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.

</details>


### [71] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: This paper proposes a decoupled framework for zero-shot human-object interaction (HOI) detection focusing on improved interaction recognition (IR) through visual question answering instead of traditional tightly coupled methods.


<details>
  <summary>Details</summary>
Motivation: Recognition of human-object interactions in images is challenging due to the combinatorial diversity of possibilities, and existing methods struggle with generalizing to unseen interactions.

Method: The authors decouple object detection from interaction recognition, leveraging multi-modal large language models (MLLMs) for zero-shot interaction recognition by using visual question answering and introducing a spatial-aware pooling module and deterministic matching.

Result: The method outperforms existing zero-shot solutions on HICO-DET and V-COCO datasets, shows strong cross-dataset generalization, and offers integration flexibility with object detectors without retraining.

Conclusion: The proposed approach improves the efficiency and generalization of zero-shot HOI detection by leveraging MLLMs for IR, overcoming limitations of prior methods, and enabling a flexible and training-free solution.

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>


### [72] [MB-DSMIL-CL-PL: Scalable Weakly Supervised Ovarian Cancer Subtype Classification and Localisation Using Contrastive and Prototype Learning with Frozen Patch Features](https://arxiv.org/abs/2602.15138)
*Marcus Jenkins,Jasenka Mazibrada,Bogdan Leahu,Michal Mackiewicz*

Main category: cs.CV

TL;DR: This paper presents a novel AI approach for classifying and localizing ovarian cancer histopathology images using pre-computed features and achieves substantial improvements in F1 score and AUC over prior methods.


<details>
  <summary>Details</summary>
Motivation: The growing diagnostic workload in pathology departments necessitates AI solutions to assist with subtype diagnosis in ovarian cancer while addressing the scalability and time challenges of end-to-end feature extraction methods.

Method: The study utilizes contrastive and prototype learning with pre-computed frozen features and feature-space augmentations to enhance ovarian cancer subtype classification and localization.

Result: The proposed method achieves a 70.4% and 15.3% F1 score improvement for instance- and slide-level classification, and AUC gains of 16.9% (instance localization) and 2.3% (slide classification), surpassing DSMIL.

Conclusion: This method offers substantial accuracy improvements in ovarian cancer diagnosis without the scalability limitations of end-to-end training, maintaining efficiency with frozen patch features.

Abstract: The study of histopathological subtypes is valuable for the personalisation of effective treatment strategies for ovarian cancer. However, increasing diagnostic workloads present a challenge for UK pathology departments, leading to the rise in AI approaches. While traditional approaches in this field have relied on pre-computed, frozen image features, recent advances have shifted towards end-to-end feature extraction, providing an improvement in accuracy but at the expense of significantly reduced scalability during training and time-consuming experimentation. In this paper, we propose a new approach for subtype classification and localisation in ovarian cancer histopathology images using contrastive and prototype learning with pre-computed, frozen features via feature-space augmentations. Compared to DSMIL, our method achieves an improvement of 70.4\% and 15.3\% in F1 score for instance- and slide-level classification, respectively, along with AUC gains of 16.9\% for instance localisation and 2.3\% for slide classification, while maintaining the use of frozen patch features.

</details>


### [73] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: The paper proposes a method to identify annotation errors in video datasets by analyzing the per-frame loss trajectory through cumulative sample loss (CSL).


<details>
  <summary>Details</summary>
Motivation: Real-world video datasets often contain annotation errors such as mislabeling and temporal misalignment, which negatively affect training and performance in time-critical tasks like phase detection and event segmentation.

Method: The method analyzes cumulative sample loss (CSL), which tracks the average loss a frame incurs across training epochs using saved model checkpoints. High or irregular CSL values are used to flag annotation errors without requiring ground truth.

Result: The approach successfully identifies annotation errors, including subtle mislabeling and temporal misalignment, on datasets like EgoPER and Cholec80.

Conclusion: This method provides a generalizable and effective tool for auditing video datasets, enhancing dataset quality, and improving model training reliability.

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [74] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: The paper introduces a distributional deep learning framework to address domain shift and improve super-resolution performance for medical imaging, specifically 4D Flow MRI.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to tackle the domain shift problem caused by discrepancies between training data and real-world clinical data in super-resolution tasks for medical imaging.

Method: The method involves training a model on computational fluid dynamics (CFD) simulations and fine-tuning it on a harmonized dataset combining 4D Flow MRI and CFD samples, utilizing distributional deep learning to improve robustness.

Result: The framework significantly outperforms traditional deep learning methods in handling domain shift and achieving better super-resolution performance in real clinical applications.

Conclusion: This approach effectively addresses domain shift challenges and enhances super-resolution efficacy for medical imaging, as validated through theoretical properties and real-world data experiments.

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

</details>


### [75] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: This paper presents a method for camera virtualization aimed at generating photorealistic images from limited camera views, with a focus on dynamic scenes in sports and live events.


<details>
  <summary>Details</summary>
Motivation: Existing view synthesis methods struggle with achieving coherent, photorealistic rendering in dynamic and fast-paced scenarios, and they lack capabilities for efficient time-archival.

Method: The proposed approach models dynamic scenes as rigid transformations with synchronized camera views, using neural volume rendering to enhance visual rendering and enable temporal archival.

Result: The method allows for revisiting past moments in dynamic scenes for replay, analysis, and archival, overcoming challenges of motion handling and temporal coherence in previous methods.

Conclusion: This method advances camera virtualization by introducing accurate novel view synthesis with time-archival features, enhancing applications in sports broadcasting and live performances.

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [76] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: The paper introduces a large-scale study of training long-context vision-language models, demonstrating advancements in long-document visual question answering and cross-context transfer.


<details>
  <summary>Details</summary>
Motivation: Address the lack of reproducible training recipes and data pipelines for long-context vision language models and achieve better performance in long-document visual question answering.

Method: Systematic investigation through continued pretraining, supervised finetuning, preference optimization, ablation studies, and synthetic data pipeline utilization on 24B and 32B parameter models.

Result: State-of-the-art results achieved in MMLongBenchDoc benchmark with improved methodologies and validated cross-context transfer capabilities.

Conclusion: This study bridges gaps in reproducibility for long-context VLMs, highlights strategies for improved training, offers corrected benchmarks, and demonstrates bidirectional context transfer benefits.

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

</details>


### [77] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: The paper introduces Exploration-Exploitation Distillation (E^2D), an efficient dataset distillation method that improves accuracy and speed, addressing the key trade-off in current techniques.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods either sacrifice accuracy for efficiency or demand substantial computation, thus requiring a more balanced approach to optimize both aspects.

Method: The proposed method, E^2D, uses a two-phase optimization with full-image initialization for preserving semantic information and targets high-loss regions for efficiency, reducing redundant computation and accelerating convergence.

Result: E^2D achieves state-of-the-art performance on large-scale datasets like ImageNet-1K, being 18x faster, and provides substantial accuracy improvements on ImageNet-21K while being 4.3x faster.

Conclusion: E^2D demonstrates that focusing on redundancy-reducing updates, paired with an efficient pipeline, effectively balances accuracy and efficiency in large-scale dataset distillation.

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

</details>


### [78] [Visual Persuasion: What Influences Decisions of Vision-Language Models?](https://arxiv.org/abs/2602.15278)
*Manuel Cherep,Pranav M R,Pattie Maes,Nikhil Singh*

Main category: cs.CV

TL;DR: The paper proposes a framework to study visual preferences of vision-language models (VLMs) by evaluating systematic edits to images and analyzing their impact on VLM decisions.


<details>
  <summary>Details</summary>
Motivation: To understand and address the visual preferences and vulnerabilities of VLMs that make decisions at scale.

Method: A framework is introduced where VLMs perform controlled image-based decisions. Visual modifications are iteratively applied to input images using image generation models, and their impact on decisions is analyzed.

Result: Optimized visual edits significantly shift the decision probabilities of VLMs, revealing consistent themes in visual preferences.

Conclusion: The approach provides efficient means for identifying visual vulnerabilities and safety concerns in VLMs, aiding proactive audit and governance of AI image agents.

Abstract: The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.

</details>


### [79] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: This paper proposes a method to improve diversity in text-to-video generation while maintaining temporal consistency, without requiring costly video decoding or backpropagation.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of increasing diversity in text-to-video generation under low-sample settings, without compromising temporal consistency or incurring high computational costs.

Method: The method involves a joint-sampling framework that applies diversity-driven updates and removes components that degrade temporal consistency using lightweight latent-space models.

Result: The proposed approach achieves high diversity, improved temporal consistency, and better color naturalness in generated videos while avoiding heavy computational demands.

Conclusion: This work demonstrates an efficient way to achieve high-quality, diverse videos in text-to-video generation, balancing both diversity and temporal coherence. Code for the approach will be made publicly available.

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

</details>


### [80] [Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models](https://arxiv.org/abs/2602.15315)
*Tai Le-Gia,Jaehyun Ahn*

Main category: cs.CV

TL;DR: This paper proposes a training-free, batch-based framework for zero-shot anomaly detection in 3D brain MRI that uses localized volumetric tokens derived from 2D foundation models.


<details>
  <summary>Details</summary>
Motivation: Current methods for zero-shot anomaly detection in medical imaging struggle with 3D datasets as they mainly rely on slice-wise features and vision-language models, failing to capture volumetric structures.

Method: The authors introduce a framework constructing localized 3D tokens by aggregating multi-axis 2D slices. These tokens are integrated into anomaly detection pipelines without the need for fine-tuning or supervision.

Result: The framework enables effective and efficient zero-shot anomaly detection for 3D brain MRI, showing that 2D encoders can be extended to volumetric data.

Conclusion: This study demonstrates a simple, robust, and computationally practical method for 3D anomaly detection using training-free, batch-based approaches and localized 3D representations.

Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.

</details>


### [81] [Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs](https://arxiv.org/abs/2602.15318)
*Libo Zhang,Zhaoning Zhang,Wangyang Hong,Peng Qiao,Dongsheng Li*

Main category: cs.CV

TL;DR: The paper addresses inefficiencies in speculative decoding for Video Large Language Models (Vid-LLMs) and proposes the Sparrow framework, improving speed and performance.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods face severe performance issues when applied to Vid-LLMs, particularly due to challenges like attention dilution and negative visual gain.

Method: The paper introduces the Sparrow framework which uses visually-aware text-anchored window attention and intermediate-layer visual state bridging to enhance performance. Additionally, it employs a multi-token prediction strategy to mitigate training-inference distribution shifts.

Result: Experiments demonstrate that Sparrow achieves a 2.82x average speedup even for sequences with up to 25k visual tokens.

Conclusion: Sparrow provides an effective solution for real-time long video processing, resolving performance degradation challenges in Vid-LLM inference.

Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>


### [82] [EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use](https://arxiv.org/abs/2602.15329)
*Siwei Wen,Zhangcheng Wang,Xingjian Zhang,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: The paper introduces EventMemAgent, a framework for continuous perception in online video understanding, using hierarchical memory for efficient contextual reasoning.


<details>
  <summary>Details</summary>
Motivation: The study addresses the issue where Multimodal Large Language Models (MLLMs) struggle to balance unbounded streaming video input with their limited context windows, hindering long-range reasoning and fine-grained detail capture.

Method: This paper proposes EventMemAgent, a hierarchical memory-based active online video framework with a dual-layer strategy: short-term memory for detecting event boundaries with dynamic buffer management and long-term memory for structured archiving. It also incorporates a multi-granular perception toolkit and employs Agentic Reinforcement Learning (Agentic RL) for reasoning and tool-use strategies.

Result: EventMemAgent demonstrates competitive performance on benchmark datasets for online video understanding.

Conclusion: EventMemAgent effectively tackles the challenges of continuous perception and complex reasoning in streaming videos, leveraging hierarchical memory structures and active processing.

Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>


### [83] [Effective and Robust Multimodal Medical Image Analysis](https://arxiv.org/abs/2602.15346)
*Joy Dhar,Nayyar Zaidi,Maryam Haghighat*

Main category: cs.CV

TL;DR: This paper introduces the MAIL and Robust-MAIL networks for multimodal data fusion in medical imaging, addressing limitations in generalizability, computational cost, and robustness to adversarial attacks, achieving superior performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current multimodal fusion methods for medical imaging face three limitations: lack of generalization across diseases, high computational cost, and vulnerability to adversarial attacks, limiting their effectiveness in medical AI applications.

Method: The proposed MAIL network includes residual learning attention blocks for modality-specific patterns and multimodal cross-attention modules for shared representations. Robust-MAIL adds adversarial robustness using random projection filters and attention noise modulation.

Result: The MAIL and Robust-MAIL methods demonstrated up to 9.34% better performance and reduced computational costs by up to 78.3% compared to existing approaches across 20 public datasets.

Conclusion: MAIL and Robust-MAIL provide efficient, robust, and generalizable solutions for multimodal medical analysis, outperforming existing methods and ensuring reliability in resource-limited and adversarial conditions.

Abstract: Multimodal Fusion Learning (MFL), leveraging disparate data from various imaging modalities (e.g., MRI, CT, SPECT), has shown great potential for addressing medical problems such as skin cancer and brain tumor prediction. However, existing MFL methods face three key limitations: a) they often specialize in specific modalities, and overlook effective shared complementary information across diverse modalities, hence limiting their generalizability for multi-disease analysis; b) they rely on computationally expensive models, restricting their applicability in resource-limited settings; and c) they lack robustness against adversarial attacks, compromising reliability in medical AI applications. To address these limitations, we propose a novel Multi-Attention Integration Learning (MAIL) network, incorporating two key components: a) an efficient residual learning attention block for capturing refined modality-specific multi-scale patterns and b) an efficient multimodal cross-attention module for learning enriched complementary shared representations across diverse modalities. Furthermore, to ensure adversarial robustness, we extend MAIL network to design Robust-MAIL by incorporating random projection filters and modulated attention noise. Extensive evaluations on 20 public datasets show that both MAIL and Robust-MAIL outperform existing methods, achieving performance gains of up to 9.34% while reducing computational costs by up to 78.3%. These results highlight the superiority of our approaches, ensuring more reliable predictions than top competitors. Code: https://github.com/misti1203/MAIL-Robust-MAIL.

</details>


### [84] [CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset](https://arxiv.org/abs/2602.15349)
*Jinho Baek,Houwei Cao,Kate Blackwell*

Main category: cs.CV

TL;DR: The study presents CREMD, a dataset to analyze how different modes (context, audio, video) and annotator characteristics affect dog emotion labeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of interpreting dog emotions due to subjective assessments and lack of standardized methods.

Method: A comprehensive multimodal dataset (CREMD) of 923 videos with various combinations of context and audio was created and annotated by participants from diverse demographic and professional backgrounds.

Result: Adding visual context improved annotation agreement; however, conclusions on audio lacked clarity due to limitations. Non-owners and male annotators showed higher agreement than expected, while professionals aligned with predictions. Audio boosted confidence in recognizing anger and fear.

Conclusion: CREMD provides critical insights into factors that influence reliable dog emotion recognition, highlighting methodological challenges and opportunities for improving human-animal interaction systems.

Abstract: Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e.g., context, audio, video) and annotator characteristics (e.g., dog ownership, gender, professional experience) influence the perception and labeling of dog emotions. The dataset consists of 923 video clips presented in three distinct modes: without context or audio, with context but no audio, and with both context and audio. We analyze annotations from diverse participants, including dog owners, professionals, and individuals with varying demographic backgrounds and experience levels, to identify factors that influence reliable dog emotion recognition. Our findings reveal several key insights: (1) while adding visual context significantly improved annotation agreement, our findings regarding audio cues are inconclusive due to design limitations (specifically, the absence of a no-context-with-audio condition and limited clean audio availability); (2) contrary to expectations, non-owners and male annotators showed higher agreement levels than dog owners and female annotators, respectively, while professionals showed higher agreement levels, aligned with our initial hypothesis; and (3) the presence of audio substantially increased annotators' confidence in identifying specific emotions, particularly anger and fear.

</details>


### [85] [DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles](https://arxiv.org/abs/2602.15355)
*Rong Fu,Jiekai Wu,Haiyun Wei,Yee Tan Jia,Wenxin Zhang,Yang Li,Xiaowen Ma,Wangyu Wu,Simon Fong*

Main category: cs.CV

TL;DR: The paper introduces DAV-GSWT, a framework that synthesizes high-fidelity 3D Gaussian Splatting Wang Tiles with minimal input data using diffusion priors and active view sampling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current 3D neural rendering systems, which typically require densely sampled input data, by enabling efficient photorealistic synthesis for expansive landscapes.

Method: This framework employs hierarchical uncertainty quantification and generative diffusion models for active view sampling, which selects key viewpoints and hallucinates structural details during rendering.

Result: DAV-GSWT achieves significant data efficiency by reducing required input data while maintaining visual integrity and seamless tile transitions in large-scale virtual environments.

Conclusion: The proposed system demonstrates its potential for creating photorealistic, expansive virtual environments with reduced data requirements and interactive performance, pushing forward advancements in 3D neural rendering.

Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.

</details>


### [86] [GMAIL: Generative Modality Alignment for generated Image Learning](https://arxiv.org/abs/2602.15368)
*Shentong Mo,Sukmin Yun*

Main category: cs.CV

TL;DR: Generative models can synthesize realistic images for machine learning, but their indiscriminate use risks modality clashes. This paper proposes GMAIL, a multi-modal learning framework to bridge synthetic and real domains in latent space, boosting vision-language tasks.


<details>
  <summary>Details</summary>
Motivation: The research addresses challenges in leveraging synthesized images from generative models that, when indiscriminately used, can cause modality discrepancies and training inefficiencies.

Method: The method involves fine-tuning models exclusively on generated images using cross-modality alignment, subsequently integrating these aligned models for vision-language tasks.

Result: The proposed framework significantly improves image captioning, zero-shot image retrieval/classification, long caption retrieval, and enhances large multimodal model (LLaVA) capabilities.

Conclusion: The authors demonstrate the efficacy of harmonizing real and synthetic image modalities to enhance vision-language model performance across various tasks.

Abstract: Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.

</details>


### [87] [Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation](https://arxiv.org/abs/2602.15383)
*Shuwei Li,Lei Tan,Robby T. Tan*

Main category: cs.CV

TL;DR: The paper addresses inaccuracies in day-to-night unpaired image translation by proposing a framework to detect and suppress semantic hallucinations using segmentation and prototype-based refinement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the quality and effectiveness of day-to-night unpaired image translation, especially in challenging scenarios where semantic hallucinations (incorrect synthesis of key features) adversely impact downstream tasks.

Method: The method introduces a dual-head discriminator for detecting hallucinations through semantic segmentation and uses class-specific prototypes to refine translations. It employs these prototypes as semantic anchors and pushes hallucinated features away during translation.

Result: The proposed framework improves performance significantly, with a 15.5% mAP gain on the BDD100K dataset for day-to-night domain adaptation and a 31.7% improvement for classes like traffic lights, which are prone to hallucinations.

Conclusion: This novel approach addresses the problem of hallucinations in unpaired image translation by preserving semantic integrity, leading to better translation quality and enhanced downstream task performance.

Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.

</details>


### [88] [Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching](https://arxiv.org/abs/2602.15396)
*Jeongwoo Shin,Jinhwan Sul,Joonseok Lee,Jaewong Choi,Jaemoo Choi*

Main category: cs.CV

TL;DR: The paper presents Adjoint Schrödinger Bridge Matching (ASBM), a new generative modeling method that learns optimal data trajectories for efficient and stable high-dimensional sampling, outperforming past works in fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address issues with diffusion models that produce highly curved trajectories and noisy score targets due to their memoryless forward processes.

Method: The ASBM method involves two stages: learning the forward dynamics as a coupling problem through data-to-energy sampling, and training the backward generative dynamics using a simple matching loss supervised by the induced optimal coupling.

Result: ASBM generates straighter and more efficient sampling paths, scales well to high-dimensional data, and improves image generation fidelity with fewer sampling steps.

Conclusion: ASBM is an effective generative modeling framework that offers high stability and efficiency, and its optimal trajectories can even inform one-step generators.

Abstract: Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.

</details>


### [89] [Emergent Morphing Attack Detection in Open Multi-modal Large Language Models](https://arxiv.org/abs/2602.15461)
*Marija Ivanovska,Vitomir Štruc*

Main category: cs.CV

TL;DR: This paper explores using open-source multimodal large language models (MLLMs) for zero-shot face morphing attack detection (MAD), showing significant performance improvements over task-specific MAD systems.


<details>
  <summary>Details</summary>
Motivation: Face morphing attacks challenge biometric verification, and existing detection systems perform poorly on new attack types. The study aims to explore the underutilized potential of multimodal large language models for biometric forensics.

Method: The researchers evaluated open-source MLLMs using zero-shot testing for single-image MAD, employing reproducible protocols and publicly available weights.

Result: MLLMs demonstrated notable discriminative ability without fine-tuning, with LLaVA1.6-Mistral-7B achieving state-of-the-art performance, outperforming task-specific methods by over 23% in EER.

Conclusion: Multimodal pretraining enables sensitivity to morphing artifacts in a zero-shot setting, positioning MLLMs as strong, interpretable foundations for advanced biometric security. Future system improvements are possible through fine-tuning. Code and protocols will be released.

Abstract: Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.

</details>


### [90] [RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution](https://arxiv.org/abs/2602.15490)
*Youngwan Jin,Incheol Park,Yagiz Nalcakan,Hyeongjin Ju,Sanghyeop Yeo,Shiho Kim*

Main category: cs.CV

TL;DR: RPT-SR is a novel transformer model for infrared image super-resolution, integrating regional priors for improved performance in fixed-view scenarios.


<details>
  <summary>Details</summary>
Motivation: General-purpose models fail in fixed or static view infrared imaging scenarios like surveillance due to inefficient learning that does not leverage strong spatial priors.

Method: Introduces RPT-SR, a dual-token framework with regional prior tokens for persistent global structure memory and local tokens for frame-specific content processing within the attention mechanism.

Result: Demonstrates state-of-the-art performance across diverse datasets, including LWIR and SWIR spectra, outperforming existing models.

Conclusion: RPT-SR effectively addresses inefficiencies of general-purpose models by incorporating spatial priors, enhancing infrared image super-resolution under fixed-view conditions.

Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra

</details>


### [91] [LEADER: Lightweight End-to-End Attention-Gated Dual Autoencoder for Robust Minutiae Extraction](https://arxiv.org/abs/2602.15493)
*Raffaele Cappelli,Matteo Ferrara*

Main category: cs.CV

TL;DR: This paper introduces LEADER, a lightweight neural network for end-to-end minutiae extraction from raw fingerprint images, demonstrating state-of-the-art accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhance minutiae extraction in fingerprint recognition by developing a truly end-to-end deep learning method, avoiding separate preprocessing and postprocessing steps.

Method: The paper proposes LEADER, a dual-autoencoder neural network with attention-gating and novel 'Castle-Moat-Rampart' encoding, incorporating non-maximum suppression and angular decoding.

Result: LEADER achieves state-of-the-art performance with a 34% higher F1-score on the NIST SD27 dataset, robust cross-domain generalization, and computational efficiency on CPUs and GPUs.

Conclusion: LEADER sets new benchmarks in minutiae extraction accuracy, robustness, and efficiency, contributing a publicly available solution for fingerprint recognition advancements.

Abstract: Minutiae extraction, a fundamental stage in fingerprint recognition, is increasingly shifting toward deep learning. However, truly end-to-end methods that eliminate separate preprocessing and postprocessing steps remain scarce. This paper introduces LEADER (Lightweight End-to-end Attention-gated Dual autoencodER), a neural network that maps raw fingerprint images to minutiae descriptors, including location, direction, and type. The proposed architecture integrates non-maximum suppression and angular decoding to enable complete end-to-end inference using only 0.9M parameters. It employs a novel "Castle-Moat-Rampart" ground-truth encoding and a dual-autoencoder structure, interconnected through an attention-gating mechanism. Experimental evaluations demonstrate state-of-the-art accuracy on plain fingerprints and robust cross-domain generalization to latent impressions. Specifically, LEADER attains a 34% higher F1-score on the NIST SD27 dataset compared to specialized latent minutiae extractors. Sample-level analysis on this challenging benchmark reveals an average rank of 2.07 among all compared methods, with LEADER securing the first-place position in 47% of the samples-more than doubling the frequency of the second-best extractor. The internal representations learned by the model align with established fingerprint domain features, such as segmentation masks, orientation fields, frequency maps, and skeletons. Inference requires 15ms on GPU and 322ms on CPU, outperforming leading commercial software in computational efficiency. The source code and pre-trained weights are publicly released to facilitate reproducibility.

</details>


### [92] [Semantic-Guided 3D Gaussian Splatting for Transient Object Removal](https://arxiv.org/abs/2602.15516)
*Aditi Prabakaran,Priyesh Shukla*

Main category: cs.CV

TL;DR: Semantic filtering resolves ghosting artifacts in 3D Gaussian Splatting reconstructions by using vision-language models for transient object removal, improving reconstruction quality while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To address ghosting artifacts caused by transient objects in casual multi-view captures for 3DGS reconstructions without incurring high memory costs or relying on vulnerable motion-based heuristics.

Method: A semantic filtering framework utilizes CLIP similarity scores with vision-language models to classify and prune transient objects by their category, performing opacity regularization and periodic pruning for identified distractors.

Result: Experiments using the RobustNeRF benchmark showed improved reconstruction quality in 3DGS across four sequences, maintaining minimal memory overhead and real-time performance.

Conclusion: Semantic filtering provides a practical and effective methodology for transient object removal in 3DGS reconstructions, resolving ghosting artifacts with predictable distractor categories.

Abstract: Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>


### [93] [Advanced Acceptance Score: A Holistic Measure for Biometric Quantification](https://arxiv.org/abs/2602.15535)
*Aman Verma,Seshan Srirangarajan,Sumantra Dutta Roy*

Main category: cs.CV

TL;DR: The paper proposes a holistic evaluation measure for biometric fitness scores derived from hand gestures.


<details>
  <summary>Details</summary>
Motivation: Existing methods for assessing biometric scores rely on error rates, which do not evaluate the quality of scores themselves.

Method: The authors introduced a measure combining ranking, relevance, trend correspondence, and disentanglement, weighted appropriately, to create an advanced acceptance score.

Result: The measure is validated through experiments on three datasets and five state-of-the-art models, demonstrating better correlation and reliability compared to existing measures.

Conclusion: The proposed evaluation measure effectively addresses the limitations of current methods, exhibiting enhanced reliability and appropriateness in selecting optimal biometric fitness scores.

Abstract: Quantifying biometric characteristics within hand gestures involve derivation of fitness scores from a gesture and identity aware feature space. However, evaluating the quality of these scores remains an open question. Existing biometric capacity estimation literature relies upon error rates. But these rates do not indicate goodness of scores. Thus, in this manuscript we present an exhaustive set of evaluation measures. We firstly identify ranking order and relevance of output scores as the primary basis for evaluation. In particular, we consider both rank deviation as well as rewards for: (i) higher scores of high ranked gestures and (ii) lower scores of low ranked gestures. We also compensate for correspondence between trends of output and ground truth scores. Finally, we account for disentanglement between identity features of gestures as a discounting factor. Integrating these elements with adequate weighting, we formulate advanced acceptance score as a holistic evaluation measure. To assess effectivity of the proposed we perform in-depth experimentation over three datasets with five state-of-the-art (SOTA) models. Results show that the optimal score selected with our measure is more appropriate than existing other measures. Also, our proposed measure depicts correlation with existing measures. This further validates its reliability. We have made our \href{https://github.com/AmanVerma2307/MeasureSuite}{code} public.

</details>


### [94] [Dynamic Training-Free Fusion of Subject and Style LoRAs](https://arxiv.org/abs/2602.15539)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CV

TL;DR: The paper introduces a dynamic fusion framework that improves LoRA weight fusion for subject-style synthesis without retraining, providing better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA combination approaches use static heuristic methods for fusion, ignoring adaptive feature adjustments and the randomness in input sampling, limiting their effectiveness.

Method: The framework dynamically calculates KL divergence between the base model's features and those from subject and style LoRAs at each generation step, adapting fusion weights. Additionally, gradient-based corrections are applied during reverse denoising using objective metrics to refine the output.

Result: The proposed method achieves high-quality coherent subject-style synthesis, surpassing state-of-the-art methods both qualitatively and quantitatively in experiments.

Conclusion: Dynamic training-free fusion achieves improved subject-style synthesis by combining feature-level dynamic selection and metric-guided adjustment, eliminating the need for retraining.

Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.

</details>


### [95] [Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2602.15556)
*Guangtao Lyu,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Xueting Li,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: The paper proposes PADE, a training-free method to enhance multimodal reasoning in LVLMs, improving visual grounding and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: LVLMs exhibit strong reasoning capabilities but struggle with hallucinations and inconsistency with visual inputs or instructions, requiring solutions to improve reliability.

Method: The proposed PADE method intervenes using Positive Attention Dynamics, creating PAD maps to identify visual regions, applies Median Absolute Deviation Scaling to control intervention strength, and utilizes System-Token Compensation for better output consistency.

Result: Experiments demonstrate PADE's effectiveness in improving visual grounding and reducing hallucinations across multiple LVLMs and benchmarks.

Conclusion: Leveraging internal attention dynamics is a promising approach for enhancing multimodal reasoning capabilities in LVLMs.

Abstract: LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.

</details>


### [96] [Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning](https://arxiv.org/abs/2602.15579)
*Amal Lahchim,Lambros Athanasiou*

Main category: cs.CV

TL;DR: The study introduces a fully automated machine learning pipeline for vessel segmentation and classification in OCT images, achieving high accuracy and low computational overhead.


<details>
  <summary>Details</summary>
Motivation: To address challenges in OCT imaging, such as noise, artifacts, and complex tissue structures, and to automate vessel segmentation for clinical utility.

Method: The pipeline incorporates image preprocessing, artifact removal, polar-to-Cartesian transformation, K-means clustering, local feature extraction, and the use of Logistic Regression and SVM for pixel-wise vessel classification.

Result: The method achieved high performance metrics with precision, recall, and F1-scores up to 1.00, and overall classification accuracy of 99.68%.

Conclusion: The approach is an accurate and efficient solution for automated OCT image analysis, with potential applications in clinical decision-making and real-time processing.

Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.

</details>


### [97] [An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment](https://arxiv.org/abs/2602.15584)
*Flavien Armangeon,Thibaud Ehret,Enric Meinhardt-Llopis,Rafael Grompone von Gioi,Guillaume Thibault,Marc Petit,Gabriele Facciolo*

Main category: cs.CV

TL;DR: The paper discusses aligning industrial schematics with real-world 2D/3D data to create digital twins, introducing a dataset (IRIS-v2) to aid the process by enhancing segmentation and graph matching.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in aligning functional schematics with real-world 2D/3D data in industrial settings, especially where digital models are unavailable, and manual alignment is tedious, error-prone, and complex.

Method: The paper introduces a dataset named IRIS-v2, which includes images, point clouds, annotated 2D data, segmentation masks, CAD models, 3D pipe routing, and P&ID. They explore alignment through segmentation and graph matching in practical case studies.

Result: The alignment process using the new dataset reduces the time required for aligning schematics with reality, showcasing its potential in addressing inconsistencies and automating parts of the digital twin creation.

Conclusion: IRIS-v2 is a significant step toward improving alignment tasks in old industrial facilities. By including diverse and essential data, it facilitates further research and practical solutions for digital twin creation.

Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.

</details>


### [98] [Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation](https://arxiv.org/abs/2602.15650)
*Marco Salmè,Federico Siciliano,Fabrizio Silvestri,Paolo Soda,Rosa Sicilia,Valerio Guarrasi*

Main category: cs.CV

TL;DR: This paper introduces Concept-Enhanced Multimodal RAG (CEMRAG), which improves radiology report generation by combining clinical interpretability with factual accuracy.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models (VLMs) for radiology reports face challenges of limited interpretability and a tendency to generate inaccurate findings.

Method: The paper proposes CEMRAG, a framework decomposing visual data into clinical concepts and integrating them with multimodal Retrieval-Augmented Generation for contextual enhancements.

Result: Experiments on datasets (MIMIC-CXR and IU X-Ray) show CEMRAG enhances both clinical accuracy and standard NLP metrics compared to existing methods.

Conclusion: The study shows that combining interpretability and fact-based approaches improves diagnostic accuracy, making AI systems more trustworthy for radiology.

Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.

</details>


### [99] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: The study introduces a publicly available dataset to assess strawberry ripeness, testing models including YOLOv8, YOLOv9, and YOLO11 and achieving high precision and recall rates.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for determining strawberry ripeness are subjective, unreliable, and lack comprehensive datasets for comparison.

Method: The development of a publicly accessible dataset with 566 images and 1,201 labeled strawberry objects prepared under variable conditions and tested with YOLO-based deep learning models.

Result: Tests revealed YOLOv9c achieved the highest precision (90.94%), YOLO11s the highest recall (83.74%), and YOLOv8s delivered best overall mAP@50 performance (86.09%).

Conclusion: Smaller and medium-sized YOLO models provide efficient performance on the dataset, aiding smart agriculture applications and establishing a baseline for future research.

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [100] [Bayesian Optimization for Design Parameters of 3D Image Data Analysis](https://arxiv.org/abs/2602.15660)
*David Exler,Joaquin Eduardo Urrutia Gómez,Martin Krüger,Maike Schliephake,John Jbeily,Mario Vitacolonna,Rüdiger Rudolf,Markus Reischl*

Main category: cs.CV

TL;DR: The paper introduces a pipeline that utilizes Bayesian Optimization to enhance segmentation and classification of 3D biomedical imaging data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting suitable models and tuning parameters for large-scale 3D biomedical imaging, as manual analysis is impractical and current methods have limitations.

Method: The pipeline involves two Bayesian Optimization stages: selecting segmentation models and optimizing parameters using a syntactic benchmark dataset, and improving classifier design with minimal manual annotation through an assisted class-annotation workflow.

Result: The proposed pipeline was validated through four case studies, successfully identifying effective models and parameter configurations for each dataset.

Conclusion: The 3D data Analysis Optimization Pipeline effectively streamlines the model selection and parameter optimization process for 3D biomedical imaging, reducing manual effort and improving efficiency.

Abstract: Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.

</details>


### [101] [RaCo: Ranking and Covariance for Practical Learned Keypoints](https://arxiv.org/abs/2602.15755)
*Abhiram Shenoi,Philipp Lindenberger,Paul-Edouard Sarlin,Marc Pollefeys*

Main category: cs.CV

TL;DR: RaCo is a lightweight neural network achieving state-of-the-art performance in 3D vision tasks by detecting robust and versatile keypoints.


<details>
  <summary>Details</summary>
Motivation: To develop a robust keypoint detection method that works under varied conditions without needing covisible image pairs, and to optimize for keypoint repeatability and matching under rotations.

Method: RaCo employs a repeatable keypoint detector, a differentiable ranker to limit keypoints with maximum matches, and a covariance estimator for quantifying spatial uncertainties. It uses extensive data augmentation for rotational robustness without relying on computationally-heavy equivariant architectures.

Result: RaCo demonstrated state-of-the-art performance on keypoint repeatability and two-view matching benchmarks, especially during large in-plane rotations.

Conclusion: RaCo provides a simple, efficient, and interpretable method for robust keypoint detection and matching, without requiring additional labels.

Abstract: This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo.

</details>


### [102] [Criteria-first, semantics-later: reproducible structure discovery in image-based sciences](https://arxiv.org/abs/2602.15712)
*Jan Bumberger*

Main category: cs.CV

TL;DR: The paper proposes a criteria-first approach to image analysis for scientific discovery, separating semantics-free structure extraction from downstream semantic mapping.


<details>
  <summary>Details</summary>
Motivation: The dominant semantics-first paradigm in image analysis fails under conditions typical in scientific discovery, such as open-ended exploration and long-term monitoring.

Method: The paper introduces a framework for criteria-first structure discovery that emphasizes reproducible, semantics-free extraction of structural fields, relying on cybernetics and information theory principles.

Result: Evidence is provided showing that criteria-first components are essential across domains where labels do not scale, supporting reproducibility and cross-domain analysis.

Conclusion: The criteria-first approach enables plural semantic interpretations, facilitates validation beyond class accuracy, and ensures FAIR, AI-ready digital objects for broader scientific applications.

Abstract: Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.

</details>


### [103] [ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT](https://arxiv.org/abs/2602.15720)
*Hyunchan Moon,Cheonjun Park,Steven L. Waslander*

Main category: cs.CV

TL;DR: ToaSt is a framework for optimizing Vision Transformers (ViTs) by combining structured pruning and a novel Token Channel Selection (TCS), achieving improved trade-offs between efficiency and accuracy, as shown across multiple models.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs) face significant deployment challenges due to high computational costs, and current compression techniques like structured pruning and token compression come with their own limitations.

Method: ToaSt proposes a decoupled framework that utilizes coupled head-wise structured pruning on Multi-Head Self-Attention modules and introduces a novel Token Channel Selection (TCS) for Feed-Forward Networks to enhance compression while maintaining robustness and avoiding optimization challenges.

Result: ToaSt outperforms current methods, achieving improvements like 88.52% accuracy (+1.64%) with a 39.4% FLOPs reduction on ViT-MAE-Huge, and shows strong transferability to downstream tasks, achieving 52.2 mAP on COCO object detection.

Conclusion: ToaSt provides an effective way to optimize ViTs by addressing computational cost issues while maintaining or improving their accuracy across benchmarks, making it suitable for real-world deployments.

Abstract: Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\% accuracy (+1.64 \%) with 39.4\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.

</details>


### [104] [Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation](https://arxiv.org/abs/2602.15724)
*Shutian Gu,Chengkai Huang,Ruoyu Wang,Lina Yao*

Main category: cs.CV

TL;DR: The paper proposes using retrieval-augmented frameworks to enhance vision-and-language navigation (VLN) with large language models (LLMs), improving their performance and efficiency without needing to fine-tune the underlying LLM.


<details>
  <summary>Details</summary>
Motivation: Navigation in VLN involves translating natural language into actions within new, unseen environments. Existing LLM-based methods suffer from inefficiencies due to repeated and verbose decision-making processes.

Method: The authors introduce two retrieval modules: one selects contextually similar successful trajectories for better instruction grounding, and the other prunes irrelevant navigation options to simplify decision-making. Both modules are trained independently and do not modify the LLM.

Result: The method consistently improves Success Rate, Oracle Success Rate, and SPL on the Room-to-Room benchmark for both seen and unseen environments, showing reduced decision ambiguity and improved task performance.

Conclusion: Retrieval-augmented strategies are effective for improving efficiency, stability, and guidance in LLM-driven VLN. The method enhances navigation without altering or fine-tuning the underlying models, making it scalable and adaptable.

Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.

</details>


### [105] [Spanning the Visual Analogy Space with a Weight Basis of LoRAs](https://arxiv.org/abs/2602.15727)
*Hila Manor,Rinon Gal,Haggai Maron,Tomer Michaeli,Gal Chechik*

Main category: cs.CV

TL;DR: This paper introduces LoRWeB, a method that allows better generalization in visual analogy learning by dynamically selecting and weighting transformation primitives instead of using a fixed adaptation module, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for visual analogy learning struggle with generalization when adapting to diverse transformations due to reliance on a single fixed module.

Method: LoRWeB uses a learnable basis of LoRA modules to represent various transformations and employs a lightweight encoder to dynamically select and combine these modules based on the input analogy pair during inference.

Result: LoRWeB demonstrated state-of-the-art performance and significantly improved its ability to generalize across unseen visual transformations in comprehensive evaluations.

Conclusion: Decomposing LoRA modules into dynamic bases offers a promising direction for flexible and effective visual manipulation tasks.

Abstract: Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\{\mathbf{a}$, $\mathbf{a}'$, $\mathbf{b}\}$, the goal is to generate $\mathbf{b}'$ such that $\mathbf{a} : \mathbf{a}' :: \mathbf{b} : \mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a "space of LoRAs". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb

</details>


### [106] [Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding](https://arxiv.org/abs/2602.15734)
*Guile Wu,David Huang,Bingbing Liu,Dongfeng Bai*

Main category: cs.CV

TL;DR: The paper proposes a unified 3D scene representation model to improve scene understanding by integrating language and geometry features, emphasizing coherence between appearance, semantics, and geometry.


<details>
  <summary>Details</summary>
Motivation: Current 3D open-vocabulary scene understanding methods suffer from a lack of synergy between scene appearance, semantics, and geometry, leading to disjointed scene understanding and poorer quality reconstructions.

Method: The proposed method uses sparse voxel representations equipped with appearance, density, feature, and confidence fields. A feature modulation module distills language features into 3D scenes, while geometric knowledge is transferred using depth correlation and pattern consistency regularization.

Result: The method outperforms state-of-the-art techniques, delivering superior results in holistic 3D scene understanding and reconstruction.

Conclusion: Unified modeling of appearance, semantics, and geometry through language and geometry features noticeably enhances the performance of 3D scene understanding and reconstruction.

Abstract: Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.

</details>


### [107] [Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models](https://arxiv.org/abs/2602.15772)
*Sen Ye,Mengde Xu,Shuyang Gu,Di He,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: The paper tackles the trade-off in multimodal models between generation and understanding by introducing the Reason-Reflect-Refine (R3) framework to balance these capabilities.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the challenge in multimodal models where improving generative capabilities often reduces understanding, identifying a conflict between these tasks.

Method: They propose the R3 framework, which reframes the generation task into a multi-step process involving reason, reflect (understand), and refine (regenerate).

Result: The R3 framework reduces the conflict between generation and understanding, enhancing both capabilities while delivering stronger performance in generation.

Conclusion: The framework demonstrates how explicitly incorporating understanding into the generation process can mitigate trade-offs in multimodal models, advancing the development of unified multimodal architectures.

Abstract: Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.

</details>


### [108] [NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy](https://arxiv.org/abs/2602.15775)
*Laura Salort-Benejam,Antonio Agudo*

Main category: cs.CV

TL;DR: The paper introduces NeRFscopy, a self-supervised pipeline for dynamic 3D reconstruction and novel view synthesis of deformable endoscopic tissues using monocular videos.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in endoscopic imaging such as tissue deformability, monocular camera limitations, illumination changes, occlusions, and unknown camera trajectories, thus enhancing diagnostics, treatment planning, and surgical procedures.

Method: The study leverages neural rendering to develop NeRFscopy, which incorporates a deformable model combining a canonical radiance field and time-dependent deformation field parameterized by SE(3) transformations. It learns without any template or pre-trained models by using sophisticated data terms.

Result: NeRFscopy surpasses competing methods in novel view synthesis accuracy across challenging endoscopic scenarios.

Conclusion: NeRFscopy demonstrates significant potential in improving the accuracy and capabilities of 3D endoscopic imaging, advancing medical diagnostics and surgical planning.

Abstract: Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.

</details>


### [109] [Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting](https://arxiv.org/abs/2602.15782)
*Ines Montoya-Espinagosa,Antonio Agudo*

Main category: cs.CV

TL;DR: The paper presents a hybrid forecasting approach combining sky images, meteorological data, and PV energy history to improve solar energy predictions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the variability and challenges in photovoltaic energy production forecasting, especially during cloudy conditions, for better power grid management.

Method: The approach uses deep neural models combining multimodal data, including sky images, PV history, and meteorological data, for short and long-term solar energy forecasting.

Result: Incorporating diverse meteorological data significantly improves prediction accuracy, particularly during cloudy conditions, for both nowcasting and forecasting.

Conclusion: Integrating diverse data sources enhances the accuracy, robustness, and interpretability of solar energy prediction models, aiding efficient solar variability management.

Abstract: Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.

</details>


### [110] [Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers](https://arxiv.org/abs/2602.15783)
*Lucas Sancéré,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: This paper introduces scalable Graph Transformers for analyzing full-WSI cell graphs to classify healthy vs. tumor epithelial cells in cSCC, outperforming image-based methods.


<details>
  <summary>Details</summary>
Motivation: Current image-based deep learning methods lose tissue-level context because of their reliance on patch-based representations for WSI analysis.

Method: The authors propose Graph Transformer models, SGFormer and DIFFormer, leveraging cellular graphs for classification of epithelial cell types, incorporating diverse node features.

Result: Graph-based models achieved higher balanced accuracies (~85%) compared to image-based methods (~81%) on single WSI and ~83% vs. ~78% for multi-WSI datasets.

Conclusion: Graph Transformers better capture cellular context and outperform image-based approaches for complex morphological classification in cancer WSIs.

Abstract: Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.

</details>


### [111] [Task-Agnostic Continual Learning for Chest Radiograph Classification](https://arxiv.org/abs/2602.15811)
*Muthu Subash Kavitha,Anas Zafar,Amgad Muneer,Jia Wu*

Main category: cs.CV

TL;DR: The study introduces CARL-XRay, a continual learning framework for chest radiograph classification, enabling incremental dataset updates without retraining or performance degradation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of updating chest radiograph classification models with new datasets incrementally without performance degradation or the need for retraining on historical data.

Method: CARL-XRay integrates a fixed backbone with lightweight, task-specific adapters and classifier heads, alongside a latent task selector leveraging prototypes and feature-level experience replay for task identification and adaptation.

Result: Experiments show CARL-XRay achieving enhanced routing accuracy (75% vs. 62.5%) and competitive diagnostic AUROC (0.74 to 0.75), all using fewer trainable parameters.

Conclusion: CARL-XRay serves as an effective alternative for continual clinical deployment, surpassing joint training methods under task-unknown scenarios while preserving diagnostic accuracy and efficiency.

Abstract: Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.

</details>


### [112] [VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation](https://arxiv.org/abs/2602.15819)
*Hui Ren,Yuval Alaluf,Omer Bar Tal,Alexander Schwing,Antonio Torralba,Yael Vinker*

Main category: cs.CV

TL;DR: The paper introduces a method for generating sequential sketches using pretrained text-to-video diffusion models, focusing on stroke order and temporal structure.


<details>
  <summary>Details</summary>
Motivation: Current generative models neglect the sequential and temporal nature of sketching, focusing only on static images.

Method: A two-stage fine-tuning process involving stroke ordering (via synthetic data) and visual appearance (using a limited set of manually crafted sketching processes). Sketches are treated as evolving short videos guided by text instructions.

Result: The method produces high-quality, sequential sketches that align with text-specified orderings, incorporating rich visual details despite minimal human-drawn data.

Conclusion: Leveraging language and video diffusion models proves effective for creating coherent and dynamic sequential sketches, enabling new possibilities such as brush style and collaborative drawing.

Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [113] [Distributed Semi-Speculative Parallel Anisotropic Mesh Adaptation](https://arxiv.org/abs/2602.15204)
*Kevin Garner,Polykarpos Thomadakis,Nikos Chrisochoides*

Main category: cs.DC

TL;DR: This paper introduces a distributed memory approach for anisotropic mesh adaptation without global synchronization, leveraging shared memory mesh generation and parallel runtime systems for high-performance computing.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of collective communication and global synchronization in traditional mesh adaptation methods, enabling scalability and performance improvements in high-performance computing.

Method: The method decouples meshing and performance, using shared memory meshing on single nodes to adapt boundaries, followed by distributing subdomains across HPC cluster nodes with frozen boundaries to ensure conformity.

Result: The method can generate large-scale meshes (up to 1 billion elements) and achieves quality and performance comparable to current HPC meshing software.

Conclusion: The approach effectively addresses communication and synchronization challenges in anisotropic mesh adaptation, improving performance while maintaining mesh quality on HPC systems.

Abstract: This paper presents a distributed memory method for anisotropic mesh adaptation that is designed to avoid the use of collective communication and global synchronization techniques. In the presented method, meshing functionality is separated from performance aspects by utilizing a separate entity for each - a multicore cc-NUMA-based (shared memory) mesh generation software and a parallel runtime system that is designed to help applications leverage the concurrency offered by emerging high-performance computing (HPC) architectures. First, an initial mesh is decomposed and its interface elements (subdomain boundaries) are adapted on a single multicore node (shared memory). Subdomains are then distributed among the nodes of an HPC cluster so that their interior elements are adapted while interface elements (already adapted) remain frozen to maintain mesh conformity. Lessons are presented regarding some re-designs of the shared memory software and how its speculative execution model is utilized by the distributed memory method to achieve good performance. The presented method is shown to generate meshes (of up to approximately 1 billion elements) with comparable quality and performance to existing state-of-the-art HPC meshing software.

</details>


### [114] [Co-Design and Evaluation of a CPU-Free MPI GPU Communication Abstraction and Implementation](https://arxiv.org/abs/2602.15356)
*Patrick G. Bridges,Derek Schafer,Jack Lange,James B. White,Anthony Skjellum,Evan Suggs,Thomas Hines,Purushotham Bangalore,Matthew G. F. Dosanjh,Whit Schonbein*

Main category: cs.DC

TL;DR: This paper presents an MPI-based GPU communication API for high-performance and CPU-free communication, reducing latency and improving scaling efficiencies.


<details>
  <summary>Details</summary>
Motivation: Current GPU communication APIs depend heavily on the CPU or impose synchronization challenges, limiting GPU-based ML and HPC application performance.

Method: The authors introduce an MPI-based GPU communication API using HPE Slingshot 11 network card capabilities, designed for simplicity and efficiency in CPU-free communication.

Result: The API achieved up to 50% medium message latency reduction and 28% strong scaling speedup on 8,192 GPUs of the Frontier supercomputer.

Conclusion: The proposed GPU communication API improves GPU application efficiency by reducing CPU reliance, proving scalable and effective for performance portability frameworks.

Abstract: Removing the CPU from the communication fast path is essential to efficient GPU-based ML and HPC application performance. However, existing GPU communication APIs either continue to rely on the CPU for communication or rely on APIs that place significant synchronization burdens on programmers. In this paper we describe the design, implementation, and evaluation of an MPI-based GPU communication API enabling easy-to-use, high-performance, CPU-free communication. This API builds on previously proposed MPI extensions and leverages HPE Slingshot 11 network card capabilities. We demonstrate the utility and performance of the API by showing how the API naturally enables CPU-free gather/scatter halo exchange communication primitives in the Cabana/Kokkos performance portability framework, and through a performance comparison with Cray MPICH on the Frontier and Tuolumne supercomputers. Results from this evaluation show up to a 50% reduction in medium message latency in simple GPU ping-pong exchanges and a 28% speedup improvement when strong scaling a halo-exchange benchmark to 8,192 GPUs of the Frontier supercomputer.

</details>


### [115] [FlashMem: Supporting Modern DNN Workloads on Mobile with GPU Memory Hierarchy Optimizations](https://arxiv.org/abs/2602.15379)
*Zhihao Shu,Md Musfiqur Rahman Sanim,Hangyu Zheng,Kunxiong Zhu,Miao Yin,Gagan Agrawal,Wei Niu*

Main category: cs.DC

TL;DR: FlashMem introduces a memory streaming framework to enable efficient execution of large-scale DNNs on mobile GPUs, achieving significant gains in memory reduction and inference speed.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of existing DNN acceleration frameworks that preload all model parameters into memory, which is inadequate for modern, large-scale DNN workloads on resource-limited mobile GPUs.

Method: FlashMem employs a dynamic memory streaming approach with statically pre-determined schedules and utilizes 2.5D texture memory for efficient on-demand model parameter loading and reduced transformation overhead.

Result: FlashMem reduces memory usage by up to 8.4x and speeds up inference by up to 75x compared to existing frameworks, demonstrated across experiments on 11 models.

Conclusion: FlashMem effectively overcomes limitations of traditional DNN frameworks by enabling large-scale model execution and multi-model workloads on constrained mobile GPU environments.

Abstract: The increasing size and complexity of modern deep neural networks (DNNs) pose significant challenges for on-device inference on mobile GPUs, with limited memory and computational resources. Existing DNN acceleration frameworks primarily deploy a weight preloading strategy, where all model parameters are loaded into memory before execution on mobile GPUs. We posit that this approach is not adequate for modern DNN workloads that comprise very large model(s) and possibly execution of several distinct models in succession. In this work, we introduce FlashMem, a memory streaming framework designed to efficiently execute large-scale modern DNNs and multi-DNN workloads while minimizing memory consumption and reducing inference latency. Instead of fully preloading weights, FlashMem statically determines model loading schedules and dynamically streams them on demand, leveraging 2.5D texture memory to minimize data transformations and improve execution efficiency. Experimental results on 11 models demonstrate that FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs.

</details>


### [116] [Service Orchestration in the Computing Continuum: Structural Challenges and Vision](https://arxiv.org/abs/2602.15794)
*Boris Sedlak,Víctor Casamayor Pujol,Ildefons Magrans de Abril,Praveen Kumar Donta,Adel N. Toosi,Schahram Dustdar*

Main category: cs.DC

TL;DR: The paper discusses the Computing Continuum (CC) and the challenges of service orchestration across heterogeneous infrastructure while proposing Active Inference for enhancing autonomous decisions. It also presents a research roadmap.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the complexity of service orchestration in the heterogeneous and dynamic infrastructure of the Computing Continuum and provide guidance for research toward optimized and autonomous systems.

Method: The paper uses structural analysis of the Computing Continuum, conceptualizes an ideal autonomous service orchestration solution, and introduces Active Inference as a potential methodology.

Result: The authors demonstrate how Active Inference can aid in self-organizing services and identify the lack of standardized testing environments as a major challenge.

Conclusion: No existing solution fully aligns with the proposed vision. Research must tackle challenges like standardized evaluation environments to progress toward resilient service orchestration in the CC.

Abstract: The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [117] [Near-Optimal Sample Complexity for Online Constrained MDPs](https://arxiv.org/abs/2602.15076)
*Chang Liu,Yunfan Li,Lin F. Yang*

Main category: cs.LG

TL;DR: The paper addresses safety in reinforcement learning using a primal-dual algorithm for Constrained Markov Decision Processes (CMDPs), achieving optimal performance with bounded constraint violations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to ensure safety in RL tasks (e.g., autonomous driving, robotics) while maintaining performance, as existing methods face challenges with safety violations or high sample complexity.

Method: A model-based primal-dual algorithm is proposed for CMDPs, ensuring bounded or zero violations depending on the feasibility setting, with provably efficient learning complexity.

Result: The algorithm yields an ε-optimal policy with bounded or zero violations and matches lower bounds for both constrained and unconstrained MDPs in terms of learning episodes.

Conclusion: It is feasible to learn CMDPs online with constraints as effectively as learning unconstrained MDPs, showing potential for safer and efficient RL applications.

Abstract: Safety is a fundamental challenge in reinforcement learning (RL), particularly in real-world applications such as autonomous driving, robotics, and healthcare. To address this, Constrained Markov Decision Processes (CMDPs) are commonly used to enforce safety constraints while optimizing performance. However, existing methods often suffer from significant safety violations or require a high sample complexity to generate near-optimal policies. We address two settings: relaxed feasibility, where small violations are allowed, and strict feasibility, where no violation is allowed. We propose a model-based primal-dual algorithm that balances regret and bounded constraint violations, drawing on techniques from online RL and constrained optimization. For relaxed feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with $\varepsilon$-bounded violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$ learning episodes, matching the lower bound for unconstrained MDPs. For strict feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with zero violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^5}{\varepsilon^2ζ^2}\right)$ learning episodes, where $ζ$ is the problem-dependent Slater constant characterizing the size of the feasible region. This result matches the lower bound for learning CMDPs with access to a generative model.
  Our results demonstrate that learning CMDPs in an online setting is as easy as learning with a generative model and is no more challenging than learning unconstrained MDPs when small violations are allowed.

</details>


### [118] [Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction](https://arxiv.org/abs/2602.15089)
*Takato Yasuno*

Main category: cs.LG

TL;DR: This paper proposes a hybrid approach combining deep learning embeddings and statistical features for anomaly detection in HVAC equipment, achieving high precision and low false positive rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of purely deep learning-based methods for time series anomaly detection in real-world predictive maintenance scenarios.

Method: The method combines 64-dimensional embeddings from Granite TinyTimeMixer, fine-tuned with LoRA, with 28-dimensional statistical features like trend, volatility, and drawdown indicators, and uses a LightGBM classifier for anomaly prediction.

Result: The proposed system achieved high Precision (91--95%) and ROC-AUC (0.995), with a false positive rate of 1.1% or lower and a detection rate of 88--94% in predictive maintenance tasks.

Conclusion: The study concludes that integrating deep learning representations with statistical features can create practical and effective anomaly detection systems for real-world applications.

Abstract: In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional time series embeddings from Granite TinyTimeMixer with 28-dimensional statistical features based on domain knowledge for HVAC equipment anomaly prediction tasks. Specifically, we combine time series embeddings extracted from a Granite TinyTimeMixer encoder fine-tuned with LoRA (Low-Rank Adaptation) and 28 types of statistical features including trend, volatility, and drawdown indicators, which are then learned using a LightGBM gradient boosting classifier. In experiments using 64 equipment units and 51,564 samples, we achieved Precision of 91--95\% and ROC-AUC of 0.995 for anomaly prediction at 30-day, 60-day, and 90-day horizons. Furthermore, we achieved production-ready performance with a false positive rate of 1.1\% or less and a detection rate of 88--94\%, demonstrating the effectiveness of the system for predictive maintenance applications. This work demonstrates that practical anomaly detection systems can be realized by leveraging the complementary strengths between deep learning's representation learning capabilities and statistical feature engineering.

</details>


### [119] [PolyNODE: Variable-dimension Neural ODEs on M-polyfolds](https://arxiv.org/abs/2602.15128)
*Per Åhag,Alexander Friedrich,Fredrik Ohlsson,Viktor Vigren Näslund*

Main category: cs.LG

TL;DR: This paper introduces PolyNODEs, extending neural ordinary differential equations (NODEs) to accommodate variable dimensions using M-polyfolds.


<details>
  <summary>Details</summary>
Motivation: Existing NODE models are constrained to fixed-dimensional dynamics, limiting their application in variable-dimensional spaces.

Method: The authors propose PolyNODEs, leveraging the concept of M-polyfolds, which allow for varying dimensions and differentiability, to construct flow-based models.

Result: PolyNODEs are demonstrated to solve reconstruction tasks in varying-dimensional spaces, and latent representations work effectively for downstream classification tasks.

Conclusion: PolyNODEs broaden the capabilities of NODEs by enabling variable-dimensional modeling, making them a significant advancement in geometric deep learning for dynamic and complex spaces.

Abstract: Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold's dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at https://github.com/turbotage/PolyNODE .

</details>


### [120] [CDRL: A Reinforcement Learning Framework Inspired by Cerebellar Circuits and Dendritic Computational Strategies](https://arxiv.org/abs/2602.15367)
*Sibo Zhang,Rui Jing,Liangfu Lv,Jian Zhang,Yunliang Zang*

Main category: cs.LG

TL;DR: This paper introduces a biologically inspired RL architecture modeled after cerebellum principles that addresses issues in RL efficiency, noise sensitivity, and generalization.


<details>
  <summary>Details</summary>
Motivation: To improve RL performance in terms of sample efficiency, noise robustness, and generalization by leveraging structural principles of the cerebellum.

Method: The paper introduces a cerebellum-inspired RL architecture with features like large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. It is tested on noisy, high-dimensional RL benchmarks.

Result: The proposed architecture demonstrates improved sample efficiency, robustness, and generalization compared to conventional RL designs.

Conclusion: Cerebellum-inspired structural priors can serve as effective inductive biases, enhancing RL performance while optimizing for constrained model parameters.

Abstract: Reinforcement learning (RL) has achieved notable performance in high-dimensional sequential decision-making tasks, yet remains limited by low sample efficiency, sensitivity to noise, and weak generalization under partial observability. Most existing approaches address these issues primarily through optimization strategies, while the role of architectural priors in shaping representation learning and decision dynamics is less explored. Inspired by structural principles of the cerebellum, we propose a biologically grounded RL architecture that incorporate large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. Experiments on noisy, high-dimensional RL benchmarks show that both the cerebellar architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to conventional designs. Sensitivity analysis of architectural parameters suggests that cerebellum-inspired structures can offer optimized performance for RL with constrained model parameters. Overall, our work underscores the value of cerebellar structural priors as effective inductive biases for RL.

</details>


### [121] [Refine Now, Query Fast: A Decoupled Refinement Paradigm for Implicit Neural Fields](https://arxiv.org/abs/2602.15155)
*Tianyu Xiong,Skylar Wurster,Han-Wei Shen*

Main category: cs.LG

TL;DR: The paper introduces Decoupled Representation Refinement (DRR), an architectural paradigm addressing the trade-off between inference speed and fidelity in Implicit Neural Representations (INRs).


<details>
  <summary>Details</summary>
Motivation: To overcome the fidelity-speed dilemma in INRs for tasks like 3D simulations, where deep models are slow but accurate, and faster models lack expressiveness.

Method: Proposes DRR, combining a deep refiner network and non-parametric transformations in an offline process, along with a novel data augmentation strategy called Variational Pairs (VP).

Result: Achieves state-of-the-art fidelity up to 27x faster than high-fidelity baselines, while remaining competitive with the fastest models across various simulation datasets.

Conclusion: The DRR paradigm effectively balances speed and quality, establishing it as a strong candidate for powerful, practical neural field surrogates and broader INR applications.

Abstract: Implicit Neural Representations (INRs) have emerged as promising surrogates for large 3D scientific simulations due to their ability to continuously model spatial and conditional fields, yet they face a critical fidelity-speed dilemma: deep MLPs suffer from high inference cost, while efficient embedding-based models lack sufficient expressiveness. To resolve this, we propose the Decoupled Representation Refinement (DRR) architectural paradigm. DRR leverages a deep refiner network, alongside non-parametric transformations, in a one-time offline process to encode rich representations into a compact and efficient embedding structure. This approach decouples slow neural networks with high representational capacity from the fast inference path. We introduce DRR-Net, a simple network that validates this paradigm, and a novel data augmentation strategy, Variational Pairs (VP) for improving INRs under complex tasks like high-dimensional surrogate modeling. Experiments on several ensemble simulation datasets demonstrate that our approach achieves state-of-the-art fidelity, while being up to 27$\times$ faster at inference than high-fidelity baselines and remaining competitive with the fastest models. The DRR paradigm offers an effective strategy for building powerful and practical neural field surrogates and \rev{INRs in broader applications}, with a minimal compromise between speed and quality.

</details>


### [122] [On the Geometric Coherence of Global Aggregation in Federated GNN](https://arxiv.org/abs/2602.15510)
*Chethana Prasad Kabgere,Shylaja SS*

Main category: cs.LG

TL;DR: This paper addresses the challenges of aggregating client updates in Federated Graph Neural Networks (GNNs) due to heterogeneous graph properties and proposes GGRS, a server-side framework to ensure geometric coherence in federated GNN training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the problem of degraded global model performance in federated GNNs caused by numerically incompatible client updates from heterogeneous graphs, which are not effectively captured by standard aggregation approaches.

Method: The method involves the introduction of GGRS, a geometric-aware server-side framework that uses admissibility criteria to regulate client updates, preserving relational consistency and diversity in propagation subspaces without accessing client data or graph topology.

Result: The results show that GGRS effectively maintains global message-passing coherence and stabilizes sensitivity to neighborhood interactions, demonstrated on experiments using heterogeneous GNN-native datasets such as Amazon Co-purchase.

Conclusion: The study concludes that geometry-aware regulation is crucial in federated graph learning and that GGRS successfully addresses the identified geometric failure mode, improving global model performance across heterogeneous client updates.

Abstract: Federated Learning (FL) enables distributed training across multiple clients without centralized data sharing, while Graph Neural Networks (GNNs) model relational data through message passing. In federated GNN settings, client graphs often exhibit heterogeneous structural and propagation characteristics. When standard aggregation mechanisms are applied to such heterogeneous updates, the global model may converge numerically while exhibiting degraded relational behavior.Our work identifies a geometric failure mode of global aggregation in Cross- Domain Federated GNNs. Although GNN parameters are numerically represented as vectors, they encode relational transformations that govern the direction, strength, and sensitivity of information flow across graph neighborhoods. Aggregating updates originating from incompatible propagation regimes can therefore introduce destructive interference in this transformation space.This leads to loss of coherence in global message passing. Importantly, this degradation is not necessarily reflected in conventional metrics such as loss or accuracy.To address this issue, we propose GGRS (Global Geometric Reference Structure), a server-side framework that regulates client updates prior to aggregation based on geometric admissibility criteria. GGRS preserves directional consistency of relational transformations as well as maintains diversity of admissible propagation subspaces. It also stabilizes sensitivity to neighborhood interactions, without accessing client data or graph topology. Experiments on heterogeneous GNN-native, Amazon Co-purchase datasets demonstrate that GGRS preserves global message-passing coherence across training rounds by highlighting the necessity of geometry-aware regulation in federated graph learning.

</details>


### [123] [Learning Representations from Incomplete EHR Data with Dual-Masked Autoencoding](https://arxiv.org/abs/2602.15159)
*Xiao Xiang,David Restrepo,Hyewon Jeong,Yugang Jia,Leo Anthony Celi*

Main category: cs.LG

TL;DR: The paper proposes AID-MAE, an autoencoder model that efficiently learns from incomplete EHR time series by leveraging dual masks, improving performance on clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised approaches for EHR time series struggle with irregular sampling, heterogeneous missingness, and sparsity, limiting their effectiveness and representation learning.

Method: AID-MAE employs two masks: an intrinsic missing mask representing naturally missing values and an augmented mask hiding observed values for reconstruction during training. It processes only unmasked tokens for representation learning.

Result: The AID-MAE model outperforms strong benchmarks, including XGBoost and DuETT, across diverse clinical tasks on two datasets. Its embeddings also facilitate patient cohort stratification.

Conclusion: AID-MAE addresses challenges in learning from sparse EHR time series, offering improved representation learning for clinical tasks and highlighting its utility in patient stratification.

Abstract: Learning from electronic health records (EHRs) time series is challenging due to irregular sam- pling, heterogeneous missingness, and the resulting sparsity of observations. Prior self-supervised meth- ods either impute before learning, represent missingness through a dedicated input signal, or optimize solely for imputation, reducing their capacity to efficiently learn representations that support clinical downstream tasks. We propose the Augmented-Intrinsic Dual-Masked Autoencoder (AID-MAE), which learns directly from incomplete time series by applying an intrinsic missing mask to represent naturally missing values and an augmented mask that hides a subset of observed values for reconstruction during training. AID-MAE processes only the unmasked subset of tokens and consistently outperforms strong baselines, including XGBoost and DuETT, across multiple clinical tasks on two datasets. In addition, the learned embeddings naturally stratify patient cohorts in the representation space.

</details>


### [124] [Seeing to Generalize: How Visual Data Corrects Binding Shortcuts](https://arxiv.org/abs/2602.15183)
*Nicolas Buzeta,Felipe del Rio,Cristian Hinostroza,Denis Parra,Hans Lobel,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: Vision Language Models outperform Large Language Models in text-only tasks due to enhanced generalization through cross-modal training.


<details>
  <summary>Details</summary>
Motivation: Investigate why Vision Language Models (VLMs) outperform Large Language Models (LLMs) in text-only contexts, particularly in long-context information retrieval.

Method: Controlled experiments using text-only and image-tokenized tasks along with mechanistic interpretability were conducted to study the effect of visual training on generalization.

Result: Visual training enhanced text-only out-of-distribution (OOD) performance by disrupting positional shortcuts and encouraging robust symbolic binding mechanisms.

Conclusion: Cross-modal training improves reasoning and generalization, even for single-modality tasks like text-only scenarios.

Abstract: Vision Language Models (VLMs) are designed to extend Large Language Models (LLMs) with visual capabilities, yet in this work we observe a surprising phenomenon: VLMs can outperform their underlying LLMs on purely text-only tasks, particularly in long-context information retrieval. To investigate this effect, we build a controlled synthetic retrieval task and find that a transformer trained only on text achieves perfect in-distribution accuracy but fails to generalize out of distribution, while subsequent training on an image-tokenized version of the same task nearly doubles text-only OOD performance. Mechanistic interpretability reveals that visual training changes the model's internal binding strategy: text-only training encourages positional shortcuts, whereas image-based training disrupts them through spatial translation invariance, forcing the model to adopt a more robust symbolic binding mechanism that persists even after text-only examples are reintroduced. We further characterize how binding strategies vary across training regimes, visual encoders, and initializations, and show that analogous shifts occur during pretrained LLM-to-VLM transitions. Our findings suggest that cross-modal training can enhance reasoning and generalization even for tasks grounded in a single modality.

</details>


### [125] [Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge](https://arxiv.org/abs/2602.15184)
*Siying Ma,Mehrdad M. Zadeh,Mauricio Soroco,Wuyang Chen,Jiguo Cao,Vijay Ganesh*

Main category: cs.LG

TL;DR: The paper proposes a multiphysics training framework enhancing neural operators’ generalization and efficiency in modeling PDEs by incorporating fundamental physics knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing neural operator models focus primarily on target PDEs without integrating fundamental physical principles, limiting their generalization and efficiency.

Method: The study introduces a multiphysics training framework that simultaneously learns from original and simplified PDE forms, leveraging explicit fundamental physics knowledge.

Result: The framework improves data efficiency, predictive accuracy, and OOD generalization across various 1D/2D/3D PDE simulations, achieving superior nRMSE performance.

Conclusion: Incorporating fundamental physics knowledge into neural operators enhances generalization and performance in modeling diverse PDE settings.

Abstract: Recent advances in scientific machine learning (SciML) have enabled neural operators (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework enhances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical parameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems. Through extensive experiments, we show that explicit incorporation of fundamental physics knowledge significantly strengthens the generalization ability of neural operators. We will release models and codes at https://sites.google.com/view/sciml-fundemental-pde.

</details>


### [126] [COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression](https://arxiv.org/abs/2602.15200)
*Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Ammar Ali,Baher Mohammad,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: This paper introduces COMPOT, a training-free framework for Transformer model compression, combining calibration-based sparse weight factorization with dynamic layer-wise compression allocation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve Transformer model compression by overcoming the limitations of traditional methods like truncated SVD, which enforce a single shared subspace and reduce accuracy, and to address the inefficiencies in existing sparse dictionary learning approaches.

Method: COMPOT leverages sparse weight factorization with calibration data, orthogonal dictionaries with closed-form Procrustes updates, and analytical sparse coding for efficiency. It also introduces a dynamic allocation strategy for adaptive layer compression based on global budget constraints, avoiding iterative processes.

Result: COMPOT achieves superior quality-compression trade-offs across architectures and tasks, surpasses low-rank and sparse methods while being compatible with post-training quantization for extreme compression.

Conclusion: The proposed COMPOT framework offers an effective, efficient, and compatible solution for Transformer model compression, enabling improved accuracy and compatibility with other compression methodologies.

Abstract: Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available $\href{https://github.com/mts-ai/COMPOT}{here}$.

</details>


### [127] [MAVRL: Learning Reward Functions from Multiple Feedback Types with Amortized Variational Inference](https://arxiv.org/abs/2602.15206)
*Raphaël Baur,Yannick Metz,Maria Gkoulta,Mennatallah El-Assady,Giorgia Ramponi,Thomas Kleine Buening*

Main category: cs.LG

TL;DR: This paper introduces a novel method to learn reward functions from multiple feedback types by using Bayesian inference and scalable variational inference approaches.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of jointly learning reward functions from diverse feedback types (e.g., demonstrations, comparisons, ratings, and stops), which provide qualitatively distinct signals and are challenging to combine effectively.

Method: The paper proposes a Bayesian framework for reward learning, where multiple feedback types contribute via explicit likelihoods. It employs an amortized variational inference technique that uses a shared reward encoder and feedback-specific likelihood decoders, optimized through a single evidence lower bound (ELBO).

Result: The method achieves better performance than single-feedback-type baselines on discrete and continuous-control benchmarks, integrates complementary information effectively, and produces policies that are robust to environmental perturbations. Additionally, it provides interpretable uncertainty signals.

Conclusion: Jointly inferencing reward functions from diverse feedback types eliminates the need for manual loss weighting, exploits complementary information, improves robustness in policies, and offers interpretable reward uncertainties, advancing reward learning approaches.

Abstract: Reward learning typically relies on a single feedback type or combines multiple feedback types using manually weighted loss terms. Currently, it remains unclear how to jointly learn reward functions from heterogeneous feedback types such as demonstrations, comparisons, ratings, and stops that provide qualitatively different signals. We address this challenge by formulating reward learning from multiple feedback types as Bayesian inference over a shared latent reward function, where each feedback type contributes information through an explicit likelihood. We introduce a scalable amortized variational inference approach that learns a shared reward encoder and feedback-specific likelihood decoders and is trained by optimizing a single evidence lower bound. Our approach avoids reducing feedback to a common intermediate representation and eliminates the need for manual loss balancing. Across discrete and continuous-control benchmarks, we show that jointly inferred reward posteriors outperform single-type baselines, exploit complementary information across feedback types, and yield policies that are more robust to environment perturbations. The inferred reward uncertainty further provides interpretable signals for analyzing model confidence and consistency across feedback types.

</details>


### [128] [The Information Geometry of Softmax: Probing and Steering](https://arxiv.org/abs/2602.15293)
*Kiho Park,Todd Nief,Yo Joong Choe,Victor Veitch*

Main category: cs.LG

TL;DR: This paper explores how AI systems encode semantics in their geometric representation spaces, with a focus on information geometry and its implications for semantic encoding and linear representation.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the connection between the natural geometry of representation spaces and their usage in AI behavior, highlighting the importance of information geometry in semantic encoding.

Method: The authors propose 'dual steering,' a method leveraging linear probes for robustly adjusting representations to exhibit specific concepts with minimal interference to other concepts.

Result: The dual steering method demonstrated enhanced controllability and stability in manipulating concepts in empirical studies.

Conclusion: The research concludes that information geometry plays a crucial role in semantic encoding, and dual steering is an effective method for targeted manipulation in representation spaces.

Abstract: This paper concerns the question of how AI systems encode semantic structure into the geometric structure of their representation spaces. The motivating observation of this paper is that the natural geometry of these representation spaces should reflect the way models use representations to produce behavior. We focus on the important special case of representations that define softmax distributions. In this case, we argue that the natural geometry is information geometry. Our focus is on the role of information geometry on semantic encoding and the linear representation hypothesis. As an illustrative application, we develop "dual steering", a method for robustly steering representations to exhibit a particular concept using linear probes. We prove that dual steering optimally modifies the target concept while minimizing changes to off-target concepts. Empirically, we find that dual steering enhances the controllability and stability of concept manipulation.

</details>


### [129] [ÜberWeb: Insights from Multilingual Curation for a 20-Trillion-Token Dataset](https://arxiv.org/abs/2602.15210)
*DatologyAI,:,Aldo Gael Carranza,Kaleigh Mentzer,Ricardo Pio Monti,Alex Fang,Alvin Deng,Amro Abbas,Anshuman Suri,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Diego Kiner,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: The paper addresses challenges in multilingual model training, focusing on data quality improvement to mitigate performance interference ("curse of multilinguality") and achieve compute-efficient multilingual scaling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges faced in training high-performing multilingual foundation models caused by uneven language data availability and performance interference within joint multilingual training.

Method: The researchers conducted controlled experiments, showing the impact of improving data quality on model performance across languages. They implemented bespoke per-language curation and designed a 20T-token pretraining corpus using public data.

Result: They demonstrated that curated multilingual datasets (under 8% of total tokens) effectively boost performance while reducing training computational requirements. Models trained on these datasets achieved competitive multilingual accuracy with significantly fewer FLOPs compared to baselines.

Conclusion: Focused and per-language data curation can mitigate the curse of multilinguality and support compute-efficient scaling, achieving competitive performance even at large model scales like Trinity Large.

Abstract: Multilinguality is a core capability for modern foundation models, yet training high-quality multilingual models remains challenging due to uneven data availability across languages. A further challenge is the performance interference that can arise from joint multilingual training, commonly referred to as the "curse of multilinguality". We study multilingual data curation across thirteen languages and find that many reported regressions are not inherent to multilingual scaling but instead stem from correctable deficiencies in data quality and composition rather than fundamental capacity limits. In controlled bilingual experiments, improving data quality for any single language benefits others: curating English improves non-English performance in 12 of 13 languages, while curating non-English yields reciprocal improvements in English. Bespoke per-language curation produces substantially larger within-language improvements. Extending these findings to large-scale general-purpose training mixtures, we show that curated multilingual allocations comprising under 8% of total tokens remain remarkably effective. We operationalize this approach within an effort that produced a 20T-token pretraining corpus derived entirely from public sources. Models with 3B and 8B parameters trained on a 1T-token random subset achieve competitive multilingual accuracy with 4-10x fewer training FLOPs than strong public baselines, establishing a new Pareto frontier in multilingual performance versus compute. Moreover, these benefits extend to frontier model scale: the 20T-token corpus served as part of the pretraining dataset for Trinity Large (400B/A13B), which exhibits strong multilingual performance relative to its training FLOPs. These results show that targeted, per-language data curation mitigates multilingual interference and enables compute-efficient multilingual scaling.

</details>


### [130] [Automatically Finding Reward Model Biases](https://arxiv.org/abs/2602.15222)
*Atticus Wang,Iván Arcuschin,Arthur Conmy*

Main category: cs.LG

TL;DR: The paper introduces a method to identify biases in reward models for LLMs, showing its effectiveness through experimentation.


<details>
  <summary>Details</summary>
Motivation: Existing reward models for LLMs often reward undesirable biases like hallucinations or sycophancy. Automatically identifying such biases could improve their performance.

Method: The authors propose using an LLM to iteratively detect and refine potential reward model biases, supported by evolutionary iteration and synthetic bias recall validation.

Result: Their method surfaced known and novel biases, like favoring redundant spacing or hallucinated content, and demonstrated superiority of evolutionary iteration over flat search.

Conclusion: Automated methods for discovering reward model biases can help improve the reliability and trustworthiness of LLM reward models.

Abstract: Reward models are central to large language model (LLM) post-training. However, past work has shown that they can reward spurious or undesirable attributes such as length, format, hallucinations, and sycophancy. In this work, we introduce and study the research problem of automatically finding reward model biases in natural language. We offer a simple approach of using an LLM to iteratively propose and refine candidate biases. Our method can recover known biases and surface novel ones: for example, we found that Skywork-V2-8B, a leading open-weight reward model, often mistakenly favors responses with redundant spacing and responses with hallucinated content. In addition, we show evidence that evolutionary iteration outperforms flat best-of-N search, and we validate the recall of our pipeline using synthetically injected biases. We hope our work contributes to further research on improving RMs through automated interpretability methods.

</details>


### [131] [Prescriptive Scaling Reveals the Evolution of Language Model Capabilities](https://arxiv.org/abs/2602.15327)
*Hanlin Zhang,Jikai Jin,Vasilis Syrgkanis,Sham Kakade*

Main category: cs.LG

TL;DR: The paper analyzes and provides prescriptive scaling laws relating pre-training compute budgets to downstream accuracy, validated through observational evaluations and introduces efficient methods for reliable benchmarking and monitoring capability shifts.


<details>
  <summary>Details</summary>
Motivation: To provide practitioners with reliable predictions of downstream accuracy from pre-training compute budgets, and to ensure stability of these mappings over time as model development progresses.

Method: The authors used large-scale data, applying smoothed quantile regression with a sigmoid parameterization, validated by temporal reliability tests. They extended the approach to analyze saturation and contamination impacts and introduced data-efficient boundary recovery algorithms.

Result: The study established mostly stable capability boundaries across tasks (except math reasoning), created the Proteus 2k dataset, and introduced methods to predict performance and monitor boundary shifts efficiently.

Conclusion: The work delivers tools for practitioners to translate compute budgets into realistic performance expectations, highlights exceptions like math reasoning, and advances data-efficient evaluation methods.

Abstract: For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.

</details>


### [132] [tensorFM: Low-Rank Approximations of Cross-Order Feature Interactions](https://arxiv.org/abs/2602.15229)
*Alessio Mazzetto,Mohammad Mahdi Khalili,Laura Fee Nern,Michael Viderman,Alex Shtoff,Krzysztof Dembczyński*

Main category: cs.LG

TL;DR: The paper proposes tensorFM, a model for predicting tabular categorical data using low-rank tensor approximation for efficient high-order interaction modeling, with competitive performance and low latency.


<details>
  <summary>Details</summary>
Motivation: Prediction problems on tabular categorical data often arise in practical applications. Effectively capturing high-order interactions between categorical attributes is challenging, especially in time-sensitive scenarios like online advertising.

Method: Introduces tensorFM, a model leveraging low-rank tensor approximation to represent strength of interactions between categorical attributes. The approach extends field-weighted factorization machines.

Result: TensorFM empirically achieves competitive performance compared to state-of-the-art methods and exhibits low latency suitable for real-time applications.

Conclusion: TensorFM is effective in modeling high-order interactions in categorical data, providing both performance and efficiency advantages suitable for practical applications, particularly those requiring low latency.

Abstract: We address prediction problems on tabular categorical data, where each instance is defined by multiple categorical attributes, each taking values from a finite set. These attributes are often referred to as fields, and their categorical values as features. Such problems frequently arise in practical applications, including click-through rate prediction and social sciences. We introduce and analyze {tensorFM}, a new model that efficiently captures high-order interactions between attributes via a low-rank tensor approximation representing the strength of these interactions. Our model generalizes field-weighted factorization machines. Empirically, tensorFM demonstrates competitive performance with state-of-the-art methods. Additionally, its low latency makes it well-suited for time-sensitive applications, such as online advertising.

</details>


### [133] [BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening](https://arxiv.org/abs/2602.15236)
*Anjie Qiao,Zhen Wang,Yaliang Li,Jiahua Rao,Yuedong Yang*

Main category: cs.LG

TL;DR: BindCLIP enhances virtual screening by integrating contrastive and generative learning, improving binding interaction sensitivity and ranking performance.


<details>
  <summary>Details</summary>
Motivation: The need to address the limitation of CLIP-style models, which struggle with fine-grained binding interactions and rely on shortcut correlations, for more accurate virtual screening.

Method: BindCLIP combines contrastive learning and pocket-conditioned diffusion objectives for pose-level supervision. It incorporates hard-negative augmentation and ligand-ligand anchoring to refine embedding space.

Result: BindCLIP outperforms baselines in out-of-distribution screening and ligand-analogue ranking on benchmarks like FEP+, showcasing improved interaction-aware embedding and generalization.

Conclusion: Incorporating generative pose-level supervision with contrastive learning enhances virtual screening accuracy and generalization, moving closer to real-world applicability.

Abstract: Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.

</details>


### [134] [Logit Distance Bounds Representational Similarity](https://arxiv.org/abs/2602.15438)
*Beatrix M. B. Nielsen,Emanuele Marconato,Luigi Gresele,Andrea Dittadi,Simon Buchholz*

Main category: cs.LG

TL;DR: This paper investigates linear representational similarity in discriminative models when the distributions are close but not identical. It introduces logit distance for better guarantees and examines distillation experiments to preserve interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap where KL divergence-based metrics fail in ensuring linear representational similarities between models. This is important for preserving interpretability of concepts during model distillation.

Method: The paper defines a representational dissimilarity measure based on logit differences and demonstrates its effectiveness; it uses synthetic and image datasets for distillation experiments.

Result: Logit distance was found to produce better preservation of linear representational properties compared to KL divergence-based methods.

Conclusion: Logit-distance distillation provides improved linear representational similarity and better recoverability of interpretable concepts in models compared to traditional KL divergence-based approaches.

Abstract: For a broad family of discriminative models that includes autoregressive language models, identifiability results imply that if two models induce the same conditional distributions, then their internal representations agree up to an invertible linear transformation. We ask whether an analogous conclusion holds approximately when the distributions are close instead of equal. Building on the observation of Nielsen et al. (2025) that closeness in KL divergence need not imply high linear representational similarity, we study a distributional distance based on logit differences and show that closeness in this distance does yield linear similarity guarantees. Specifically, we define a representational dissimilarity measure based on the models' identifiability class and prove that it is bounded by the logit distance. We further show that, when model probabilities are bounded away from zero, KL divergence upper-bounds logit distance; yet the resulting bound fails to provide nontrivial control in practice. As a consequence, KL-based distillation can match a teacher's predictions while failing to preserve linear representational properties, such as linear-probe recoverability of human-interpretable concepts. In distillation experiments on synthetic and image datasets, logit-distance distillation yields students with higher linear representational similarity and better preservation of the teacher's linearly recoverable concepts.

</details>


### [135] [Closing the Distribution Gap in Adversarial Training for LLMs](https://arxiv.org/abs/2602.15238)
*Chengzhi Hu,Jonas Dornbusch,David Lüdke,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: Current adversarial training for LLMs is insufficient for robustness against simple attacks due to inadequate data distribution coverage. Distributional Adversarial Training (DAT) addresses this with Diffusion LLMs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome limitations in current adversarial training techniques, which fail to cover the full data distribution and leave models susceptible to basic adversarial exploits.

Method: The proposed method, called Distributional Adversarial Training (DAT), uses Diffusion LLMs to generate diverse high-likelihood samples that better cover data distribution, combining this step with continuous adversarial training.

Result: DAT provides significantly improved adversarial robustness compared to prior adversarial training approaches.

Conclusion: Distributional Adversarial Training integrates data distribution modeling with adversarial training to reduce model vulnerability, offering a practical solution to enhance robustness.

Abstract: Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.

</details>


### [136] [Approximation Theory for Lipschitz Continuous Transformers](https://arxiv.org/abs/2602.15503)
*Takashi Furuya,Davide Murari,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: The paper introduces Lipschitz-continuous Transformers ensuring stability and robustness, proving their universal approximation within constrained function spaces.


<details>
  <summary>Details</summary>
Motivation: To ensure stability and robustness in Transformers for safety-sensitive deployments by constraining their Lipschitz constant.

Method: The proposed method embeds gradient-descent-type principles into Transformers, using measure-theoretic approaches and realizing MLP and attention blocks as negative gradient flow steps.

Result: Achieved Lipschitz-continuous Transformers capable of universal approximation within constrained function spaces, independent of token count.

Conclusion: A solid theoretical basis for creating robust and stability-focused Transformer designs is established.

Abstract: Stability and robustness are critical for deploying Transformers in safety-sensitive settings. A principled way to enforce such behavior is to constrain the model's Lipschitz constant. However, approximation-theoretic guarantees for architectures that explicitly preserve Lipschitz continuity have yet to be established. In this work, we bridge this gap by introducing a class of gradient-descent-type in-context Transformers that are Lipschitz-continuous by construction. We realize both MLP and attention blocks as explicit Euler steps of negative gradient flows, ensuring inherent stability without sacrificing expressivity. We prove a universal approximation theorem for this class within a Lipschitz-constrained function space. Crucially, our analysis adopts a measure-theoretic formalism, interpreting Transformers as operators on probability measures, to yield approximation guarantees independent of token count. These results provide a rigorous theoretical foundation for the design of robust, Lipschitz continuous Transformer architectures.

</details>


### [137] [Size Transferability of Graph Transformers with Convolutional Positional Encodings](https://arxiv.org/abs/2602.15239)
*Javier Porras-Valenzuela,Zhiyang Wang,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: This paper explores the connection between Graph Transformers (GTs) with Graph Neural Network (GNN) positional encodings and Manifold Neural Networks (MNNs), offering insights on their transferability and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how Graph Transformers (GTs) can efficiently handle graph-structured data, building on their success across domains, and to investigate their theoretical properties and scalability.

Method: The authors analyze GTs through a manifold limit model perspective, connecting GNN positional encodings to Manifold Neural Networks (MNNs), and derive theoretical transferability guarantees. They also perform experiments on graph benchmarks and test GTs in a real-world task of shortest path estimation over terrains.

Result: GTs, with GNN positional encodings, are proven to generalize well from small to larger graphs while exhibiting scalable behavior comparable to GNNs. Experiments confirm these findings.

Conclusion: This paper establishes a theoretical foundation for GTs' scalability and transferability, highlights their performance on standard benchmarks, and suggests practical applications for training GTs efficiently at large scales.

Abstract: Transformers have achieved remarkable success across domains, motivating the rise of Graph Transformers (GTs) as attention-based architectures for graph-structured data. A key design choice in GTs is the use of Graph Neural Network (GNN)-based positional encodings to incorporate structural information. In this work, we study GTs through the lens of manifold limit models for graph sequences and establish a theoretical connection between GTs with GNN positional encodings and Manifold Neural Networks (MNNs). Building on transferability results for GNNs under manifold convergence, we show that GTs inherit transferability guarantees from their positional encodings. In particular, GTs trained on small graphs provably generalize to larger graphs under mild assumptions. We complement our theory with extensive experiments on standard graph benchmarks, demonstrating that GTs exhibit scalable behavior on par with GNNs. To further show the efficiency in a real-world scenario, we implement GTs for shortest path distance estimation over terrains to better illustrate the efficiency of the transferable GTs. Our results provide new insights into the understanding of GTs and suggest practical directions for efficient training of GTs in large-scale settings.

</details>


### [138] [Scaling Laws for Masked-Reconstruction Transformers on Single-Cell Transcriptomics](https://arxiv.org/abs/2602.15253)
*Ihor Kendiukhov*

Main category: cs.LG

TL;DR: This paper investigates neural scaling laws for transformers in single-cell genomics, finding scaling behavior under data-rich conditions but limited effects under data-scarce conditions.


<details>
  <summary>Details</summary>
Motivation: The study explores the applicability of neural scaling laws, extensively validated in language and vision transformers, within single-cell genomics data models, a relatively unexplored domain.

Method: Masked-reconstruction transformers were trained on single-cell RNA sequencing datasets using varying gene and cell counts in two experimental regimes. Seven model sizes with different parameter counts were analyzed for scaling effects.

Result: In data-rich scenarios, clear power-law scaling was observed with a loss floor at approximately c ~ 1.44. Conversely, in data-limited scenarios, scaling was negligible, highlighting the importance of data availability over model size.

Conclusion: Scaling laws analogous to those in NLP appear in single-cell transcriptomics under sufficient data, and the data-to-parameter ratio critically impacts scaling behaviour. These findings have implications for designing foundational models in single-cell genomics.

Abstract: Neural scaling laws -- power-law relationships between loss, model size, and data -- have been extensively documented for language and vision transformers, yet their existence in single-cell genomics remains largely unexplored. We present the first systematic study of scaling behaviour for masked-reconstruction transformers trained on single-cell RNA sequencing (scRNA-seq) data. Using expression profiles from the CELLxGENE Census, we construct two experimental regimes: a data-rich regime (512 highly variable genes, 200,000 cells) and a data-limited regime (1,024 genes, 10,000 cells). Across seven model sizes spanning three orders of magnitude in parameter count (533 to 3.4 x 10^8 parameters), we fit the parametric scaling law to validation mean squared error (MSE). The data-rich regime exhibits clear power-law scaling with an irreducible loss floor of c ~ 1.44, while the data-limited regime shows negligible scaling, indicating that model capacity is not the binding constraint when data are scarce. These results establish that scaling laws analogous to those observed in natural language processing do emerge in single-cell transcriptomics when sufficient data are available, and they identify the data-to-parameter ratio as a critical determinant of scaling behaviour. A preliminary conversion of the data-rich asymptotic floor to information-theoretic units yields an estimate of approximately 2.30 bits of entropy per masked gene position. We discuss implications for the design of single-cell foundation models and outline the additional measurements needed to refine this entropy estimate.

</details>


### [139] [Fast and Effective On-policy Distillation from Reasoning Prefixes](https://arxiv.org/abs/2602.15260)
*Dongxu Zhang,Zhichao Yang,Sepehr Janghorbani,Jun Han,Andrew Ressler,Qian Qian,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.LG

TL;DR: This paper introduces on-policy prefix distillation, a method that focuses distillation efforts on prefixes of student outputs during training, significantly reducing training costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high training cost issue of on-policy distillation (OPD), particularly when dealing with long responses, while retaining its generalization benefits over off-policy distillation.

Method: The paper proposes modifying OPD to focus on distillation objectives only on the prefixes of student-generated outputs and terminating sampling early. This approach is based on the observation that training signals are often concentrated at the prefixes.

Result: Experiments demonstrate that this method achieves the same performance as full OPD while lowering training computational costs by 2x-47x on various benchmarks.

Conclusion: On-policy prefix distillation is a cost-efficient alternative to full OPD, maintaining its benefits of better generalization while requiring significantly fewer computational resources.

Abstract: On-policy distillation (OPD), which samples trajectories from the student model and supervises them with a teacher at the token level, avoids relying solely on verifiable terminal rewards and can yield better generalization than off-policy distillation. However, OPD requires expensive on-the-fly sampling of the student policy during training, which substantially increases training cost, especially for long responses. Our initial analysis shows that, during OPD, training signals are often concentrated in the prefix of each output, and that even a short teacher-generated prefix can significantly help the student produce the correct answer. Motivated by these observations, we propose a simple yet effective modification of OPD: we apply the distillation objective only to prefixes of student-generated outputs and terminate each sampling early during distillation. Experiments on a suite of AI-for-Math and out-of-domain benchmarks show that on-policy prefix distillation matches the performance of full OPD while reducing training FLOP by 2x-47x.

</details>


### [140] [Uniform error bounds for quantized dynamical models](https://arxiv.org/abs/2602.15586)
*Abdelkader Metakalard,Fabien Lauer,Kevin Colin,Marion Gilson*

Main category: cs.LG

TL;DR: This paper establishes statistical guarantees for dynamical models learned from dependent data, introducing error bounds for quantized models and optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring statistical accuracy in learned dynamical models derived from complex dependent data.

Method: Uniform error bounds are derived using techniques like block decomposition and spaced-point strategies, accounting for practical constraints such as quantization and imperfect optimization.

Result: Two types of error bounds are introduced: slow-rate bounds via block decomposition and fast-rate, variance-adaptive bounds via spaced-point strategies. The bounds relate to hardware constraints.

Conclusion: The study connects model encoding complexity, hardware limitations, and statistical accuracy in dynamic system identification, offering interpretable guarantees.

Abstract: This paper provides statistical guarantees on the accuracy of dynamical models learned from dependent data sequences. Specifically, we develop uniform error bounds that apply to quantized models and imperfect optimization algorithms commonly used in practical contexts for system identification, and in particular hybrid system identification. Two families of bounds are obtained: slow-rate bounds via a block decomposition and fast-rate, variance-adaptive, bounds via a novel spaced-point strategy. The bounds scale with the number of bits required to encode the model and thus translate hardware constraints into interpretable statistical complexities.

</details>


### [141] [Complex-Valued Unitary Representations as Classification Heads for Improved Uncertainty Quantification in Deep Neural Networks](https://arxiv.org/abs/2602.15283)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: The paper introduces a quantum-inspired neural network classification head to improve confidence calibration, achieving significant calibration improvements while providing insights into its practical scope.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often lack accurate confidence calibration, where predictive confidence does not correspond well to the true correctness probabilities.

Method: The proposed method incorporates a quantum-inspired classification head that maps features into a complex-valued Hilbert space and evolves them under a unitary transformation parameterized by the Cayley map, with different readout mechanisms.

Result: The unitary magnitude head achieved superior calibration (ECE of 0.0146), surpassing the standard softmax (ECE 0.0355), while quantum-motivated Born rule layer performed poorly. On CIFAR-10H, the method achieved lowest KL-divergence to human labels (0.336).

Conclusion: The work demonstrates that complex-valued unitary transformations improve calibration in certain setups, particularly for tasks involving human perceptual ambiguity, though with limitations in out-of-distribution detection and sentiment analysis.

Abstract: Modern deep neural networks achieve high predictive accuracy but remain poorly calibrated: their confidence scores do not reliably reflect the true probability of correctness. We propose a quantum-inspired classification head architecture that projects backbone features into a complex-valued Hilbert space and evolves them under a learned unitary transformation parameterised via the Cayley map. Through a controlled hybrid experimental design - training a single shared backbone and comparing lightweight interchangeable heads - we isolate the effect of complex-valued unitary representations on calibration. Our ablation study on CIFAR-10 reveals that the unitary magnitude head (complex features evolved under a Cayley unitary, read out via magnitude and softmax) achieves an Expected Calibration Error (ECE) of 0.0146, representing a 2.4x improvement over a standard softmax head (0.0355) and a 3.5x improvement over temperature scaling (0.0510). Surprisingly, replacing the softmax readout with a Born rule measurement layer - the quantum-mechanically motivated approach - degrades calibration to an ECE of 0.0819. On the CIFAR-10H human-uncertainty benchmark, the wave function head achieves the lowest KL-divergence (0.336) to human soft labels among all compared methods, indicating that complex-valued representations better capture the structure of human perceptual ambiguity. We provide theoretical analysis connecting norm-preserving unitary dynamics to calibration through feature-space geometry, report negative results on out-of-distribution detection and sentiment analysis to delineate the method's scope, and discuss practical implications for safety-critical applications. Code is publicly available.

</details>


### [142] [Certified Per-Instance Unlearning Using Individual Sensitivity Bounds](https://arxiv.org/abs/2602.15602)
*Hanna Benarroch,Jamal Atif,Olivier Cappé*

Main category: cs.LG

TL;DR: The paper explores adaptive per-instance noise calibration for certified machine unlearning, minimizing noise injection while maintaining differential privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in traditional noise calibration methods for machine unlearning, which rely on worst-case sensitivity and lead to degraded performance.

Method: Proposes adaptive per-instance noise calibration by leveraging per-instance differential privacy, deriving high-probability sensitivity bounds for ridge regression using Langevin dynamics.

Result: Demonstrated that adaptive noise calibration requires less noise while retaining certified unlearning. Validated through experiments in both linear and deep learning settings.

Conclusion: Adaptive per-instance noise calibration achieves certified unlearning more effectively, bridging theoretical rigor with practical performance improvements.

Abstract: Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.

</details>


### [143] [Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization](https://arxiv.org/abs/2602.15304)
*Farzana Akter,Rakib Hossain,Deb Kanna Roy Toushi,Mahmood Menon Khan,Sultana Amin,Lisan Al Amin*

Main category: cs.LG

TL;DR: The paper introduces a hybrid privacy-preserving framework combining Federated Learning (FL) and Split Learning (SL) for collaborative healthcare modeling without sharing raw data.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the constraints in healthcare data-sharing due to privacy and governance rules, necessitating a framework that enables collaborative decision-making while preserving data privacy.

Method: The authors propose a hybrid framework that uses FL and SL, with feature extraction on clients, prediction heads on a server, and empirical auditing for leakage using membership inference. They test defenses like activation clipping and Gaussian noise addition.

Result: The hybrid FL-SL framework performs competently on predictive tasks, enables decision-support prioritization, and offers a privacy-utility trade-off with reduced leakage risk compared to standalone FL or SL approaches.

Conclusion: The paper highlights hybrid FL-SL as a practical approach for balancing utility, privacy leakage risk, and deployment costs in healthcare decision-making systems.

Abstract: Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.

</details>


### [144] [On Surprising Effectiveness of Masking Updates in Adaptive Optimizers](https://arxiv.org/abs/2602.15322)
*Taejong Joo,Wenhan Xia,Cheolmin Kim,Ming Zhang,Eugene Ie*

Main category: cs.LG

TL;DR: The paper introduces a novel adaptive optimizer, Magma, that employs random masking in parameter updates, showing improved performance in training large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The need for more effective optimizers to improve training efficiency and performance of large language models.

Method: The study uses a masked variant of RMSProp, introduces a curvature-dependent geometric regularization, and develops Magma, which adjusts masked updates using momentum-gradient alignment.

Result: Experiments demonstrate Magma offers consistent gains in LLM pre-training, with notable perplexity reductions compared to Adam and Muon optimizers.

Conclusion: Magma is a simple and efficient optimizer that improves training outcomes for LLMs with minimal computational overhead.

Abstract: Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\% and 9\% compared to Adam and Muon, respectively.

</details>


### [145] [A Scalable Curiosity-Driven Game-Theoretic Framework for Long-Tail Multi-Label Learning in Data Mining](https://arxiv.org/abs/2602.15330)
*Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: The paper addresses the challenge of long-tail distribution in large-scale Multi-Label Classification (MLC) using a scalable framework that leverages game theory and curiosity mechanisms.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the issue of label imbalance in MLC, which often leads to poor performance on rare tail labels, particularly in scenarios with extensive label spaces.

Method: The authors propose CD-GTMLL, a cooperative framework that uses game-theoretic strategies where players specialize in different partitions of the label space and optimize learning collaboratively.

Result: The approach outperformed state-of-the-art methods in experiments on seven benchmarks, achieving significant improvements in metrics like Rare-F1 and demonstrating strong performance in extreme multi-label datasets.

Conclusion: CD-GTMLL integrates game theory and curiosity-driven mechanisms to improve efficiency and adaptability in imbalanced data scenarios, with promising applications across industries.

Abstract: The long-tail distribution, where a few head labels dominate while rare tail labels abound, poses a persistent challenge for large-scale Multi-Label Classification (MLC) in real-world data mining applications. Existing resampling and reweighting strategies often disrupt inter-label dependencies or require brittle hyperparameter tuning, especially as the label space expands to tens of thousands of labels. To address this issue, we propose Curiosity-Driven Game-Theoretic Multi-Label Learning (CD-GTMLL), a scalable cooperative framework that recasts long-tail MLC as a multi-player game - each sub-predictor ("player") specializes in a partition of the label space, collaborating to maximize global accuracy while pursuing intrinsic curiosity rewards based on tail label rarity and inter-player disagreement. This mechanism adaptively injects learning signals into under-represented tail labels without manual balancing or tuning. We further provide a theoretical analysis showing that our CD-GTMLL converges to a tail-aware equilibrium and formally links the optimization dynamics to improvements in the Rare-F1 metric. Extensive experiments across 7 benchmarks, including extreme multi-label classification datasets with 30,000+ labels, demonstrate that CD-GTMLL consistently surpasses state-of-the-art methods, with gains up to +1.6% P@3 on Wiki10-31K. Ablation studies further confirm the contributions of both game-theoretic cooperation and curiosity-driven exploration to robust tail performance. By integrating game theory with curiosity mechanisms, CD-GTMLL not only enhances model efficiency in resource-constrained environments but also paves the way for more adaptive learning in imbalanced data scenarios across industries like e-commerce and healthcare.

</details>


### [146] [Directional Reasoning Trajectory Change (DRTC): Identifying Critical Trace Segments in Reasoning Models](https://arxiv.org/abs/2602.15332)
*Waldemar Chang*

Main category: cs.LG

TL;DR: The authors propose DRTC, a framework to interpret how language models perform long-horizon reasoning by identifying pivotal decision points and measuring their directional influence on the reasoning trajectory.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of understanding how language models engage in causal, long-term reasoning and address the limitations of existing interpretability methods that fail to reveal pivotal decision points or causal text influences.

Method: The DRTC framework detects pivot points based on uncertainty and distribution shifts, applies interventions at these pivots to block earlier context flow, and computes directional attribution scores and curvature changes to track influence on the reasoning trajectory.

Result: Empirical studies reveal that directional influence is focused, with concentrated attribution scores and stronger effects from learned pivots compared to random spans. DRTC improves interpretability in reasoning across multiple models and outperforms random spans in a scaling study.

Conclusion: DRTC provides a causally grounded view of long-form reasoning and enhances interpretability by revealing how specific context elements steer reasoning within on-policy dynamics.

Abstract: Understanding how language models carry out long-horizon reasoning remains an open challenge. Existing interpretability methods often highlight tokens or spans correlated with an answer, but they rarely reveal where the model makes consequential reasoning turns, which earlier context causally triggers those turns, or whether the highlighted text actually steers the reasoning process. We introduce Directional Reasoning Trajectory Change (DRTC), a process-causal framework for interpreting long-form reasoning from a single on-policy rollout. DRTC detects pivot decision points using uncertainty and distribution-shift signals, then applies receiver-side interventions that preserve the realized rollout without resampling the continuation while blocking information flow from selected earlier chunks only at a pivot. It measures whether each intervention redirects the direction of the model's log-probability trajectory relative to the realized rollout direction, producing a signed per-chunk attribution score. We also compute turning-angle curvature changes on raw logits as a complementary diagnostic and introduce curvature signatures to summarize shared intervention-response geometry. Empirically, directional influence is sharply concentrated across four reasoning models (per-example |DRTC| shares yield Gini 0.50 to 0.58 and top-5 percent mass 0.23 to 0.28), and learned pivots induce stronger intervention magnitudes than matched random spans. In a scaling study on 500 MATH problems with R1-Distill-Qwen-1.5B, learned spans outperform matched random spans (median delta = 0.409, 355 of 500 positive; sign test p = 2.3e-21). Overall, DRTC provides a causally grounded, trajectory-level view of how specific context elements steer reasoning under on-policy dynamics.

</details>


### [147] [FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning](https://arxiv.org/abs/2602.15337)
*Chaoyi Lu*

Main category: cs.LG

TL;DR: This paper addresses the staleness problem in Asynchronous Federated Learning (AFL) by introducing FedPSA, a framework utilizing parameter sensitivity for better model assessment and dynamic adjustment.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of AFL by addressing staleness issues caused by asynchronous processes, which hinder the efficiency of sensitivity-based adjustment.

Method: The proposed method, FedPSA, uses parameter sensitivity to measure model staleness and incorporates a dynamic momentum queue for real-time adjustments in tolerance to outdated information.

Result: Experimental results demonstrate FedPSA's higher performance with improvements up to 6.37% over baseline methods and 1.93% over state-of-the-art AFL techniques.

Conclusion: FedPSA offers a more precise and adaptive approach to AFL, proving its efficacy in reducing model staleness and enhancing federated learning efficiency.

Abstract: Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\% improvement over baseline methods and 1.93\% over the current state-of-the-art method.

</details>


### [148] [Discovering Implicit Large Language Model Alignment Objectives](https://arxiv.org/abs/2602.15338)
*Edward Chen,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: The paper introduces Obj-Disco, a framework to decompose alignment reward signals into interpretable objectives, addressing risks like misalignment and reward hacking.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve challenges in LLM alignment, where reward signals often mask intended behaviors, increasing risks of reward hacking and unknown misalignments.

Method: The authors propose Obj-Disco, using an iterative greedy algorithm to identify interpretable objectives from reward behaviors, validated across diverse tasks and models.

Result: Experiments and evaluations show Obj-Disco consistently identifies >90% of reward behaviors, revealing implicit misaligned incentives and robustly explaining alignment signals.

Conclusion: Obj-Disco enhances transparency in AI alignment by identifying implicit objectives, promising safer AI models and shedding light on hidden incentives in reward systems.

Abstract: Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of "unknown unknowns", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.

</details>


### [149] [ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models](https://arxiv.org/abs/2602.15344)
*Mitchell Piehl,Zhaohan Xi,Zuobin Xiong,Pan He,Muchao Ye*

Main category: cs.LG

TL;DR: The paper explores vulnerabilities in LLMs augmented with long-term memory systems, presenting adversarial attacks that target memory retrieval mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address security risks in long-term memory-augmented LLMs, as these systems become susceptible to adversarial attacks on their memory retrieval.

Method: Introduced the ER-MIA framework, which formalizes attack settings and employs attack primitives and ensemble methods to exploit retrieval vulnerabilities.

Result: Demonstrated high success rates of adversarial attacks across multiple LLMs and memory systems, revealing fundamental vulnerabilities in similarity-based retrieval.

Conclusion: Similarity-based retrieval in memory-augmented LLMs poses significant security risks, requiring attention across designs and applications.

Abstract: Large language models (LLMs) are increasingly augmented with long-term memory systems to overcome finite context windows and enable persistent reasoning across interactions. However, recent research finds that LLMs become more vulnerable because memory provides extra attack surfaces. In this paper, we present the first systematic study of black-box adversarial memory injection attacks that target the similarity-based retrieval mechanism in long-term memory-augmented LLMs. We introduce ER-MIA, a unified framework that exposes this vulnerability and formalizes two realistic attack settings: content-based attacks and question-targeted attacks. In these settings, ER-MIA includes an arsenal of composable attack primitives and ensemble attacks that achieve high success rates under minimal attacker assumptions. Extensive experiments across multiple LLMs and long-term memory systems demonstrate that similarity-based retrieval constitutes a fundamental and system-level vulnerability, revealing security risks that persist across memory designs and application scenarios.

</details>


### [150] [Fractional-Order Federated Learning](https://arxiv.org/abs/2602.15380)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TL;DR: The paper introduces FOFedAvg, a novel variation of FedAvg, which leverages fractional-order updates to improve federated learning's efficiency and convergence.


<details>
  <summary>Details</summary>
Motivation: Addressing issues in FL such as slow convergence, high communication cost, and instability due to non-IID client data.

Method: FOFedAvg uses Fractional-Order Stochastic Gradient Descent (FOSGD) for memory-aware, fractional-order updates, enhancing robustness and historical information usage.

Result: FOFedAvg outperforms existing algorithms in test performance and convergence speed across various non-IID data sets.

Conclusion: FOFedAvg improves federated learning by addressing non-IID challenges and enhancing efficiency through fractional-order updates.

Abstract: Federated learning (FL) allows remote clients to train a global model collaboratively while protecting client privacy. Despite its privacy-preserving benefits, FL has significant drawbacks, including slow convergence, high communication cost, and non-independent-and-identically-distributed (non-IID) data. In this work, we present a novel FedAvg variation called Fractional-Order Federated Averaging (FOFedAvg), which incorporates Fractional-Order Stochastic Gradient Descent (FOSGD) to capture long-range relationships and deeper historical information. By introducing memory-aware fractional-order updates, FOFedAvg improves communication efficiency and accelerates convergence while mitigating instability caused by heterogeneous, non-IID client data. We compare FOFedAvg against a broad set of established federated optimization algorithms on benchmark datasets including MNIST, FEMNIST, CIFAR-10, CIFAR-100, EMNIST, the Cleveland heart disease dataset, Sent140, PneumoniaMNIST, and Edge-IIoTset. Across a range of non-IID partitioning schemes, FOFedAvg is competitive with, and often outperforms, these baselines in terms of test performance and convergence speed. On the theoretical side, we prove that FOFedAvg converges to a stationary point under standard smoothness and bounded-variance assumptions for fractional order $0<α\le 1$. Together, these results show that fractional-order, memory-aware updates can substantially improve the robustness and effectiveness of federated learning, offering a practical path toward distributed training on heterogeneous data.

</details>


### [151] [Doubly Stochastic Mean-Shift Clustering](https://arxiv.org/abs/2602.15393)
*Tom Trigano,Yann Sepulcre,Itshak Lapidot*

Main category: cs.LG

TL;DR: The paper introduces Doubly Stochastic Mean-Shift (DSMS), a variant of the Mean-Shift algorithm that uses randomized kernel bandwidth and trajectory updates for better density estimation, especially in sparse data scenarios.


<details>
  <summary>Details</summary>
Motivation: Standard Mean-Shift algorithms are sensitive to bandwidth selection and perform poorly in data-scarce environments with fixed-scale density estimation.

Method: The proposed DSMS algorithm introduces stochasticity in both trajectory updates and kernel bandwidth, drawing values from a continuous uniform distribution at each iteration.

Result: Experimental results show that DSMS significantly outperforms baseline methods in synthetic Gaussian mixture scenarios, especially in stability and sparse clustering, without degrading performance.

Conclusion: DSMS provides an innovative and effective randomization mechanism that improves Mean-Shift algorithms by enhancing exploration of density landscapes and preventing over-segmentation in data-sparse cases.

Abstract: Standard Mean-Shift algorithms are notoriously sensitive to the bandwidth hyperparameter, particularly in data-scarce regimes where fixed-scale density estimation leads to fragmentation and spurious modes. In this paper, we propose Doubly Stochastic Mean-Shift (DSMS), a novel extension that introduces randomness not only in the trajectory updates but also in the kernel bandwidth itself. By drawing both the data samples and the radius from a continuous uniform distribution at each iteration, DSMS effectively performs a better exploration of the density landscape. We show that this randomized bandwidth policy acts as an implicit regularization mechanism, and provide convergence theoretical results. Comparative experiments on synthetic Gaussian mixtures reveal that DSMS significantly outperforms standard and stochastic Mean-Shift baselines, exhibiting remarkable stability and preventing over-segmentation in sparse clustering scenarios without other performance degradation.

</details>


### [152] [Joint Enhancement and Classification using Coupled Diffusion Models of Signals and Logits](https://arxiv.org/abs/2602.15405)
*Gilad Nurko,Roi Benita,Yehoshua Dissen,Tomohiro Nakatani,Marc Delcroix,Shoko Araki,Joseph Keshet*

Main category: cs.LG

TL;DR: This paper presents a method integrating signal and classifier output enhancement using diffusion models to improve classification in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Current classification methods fail to incorporate semantic information during signal enhancement, leading to less robust performance in noisy environments.

Method: The authors propose a domain-agnostic framework combining two interacting diffusion models: one for input signals and one for classifier logits. This coupling allows mutual guidance without the need for retraining the classifier.

Result: The proposed framework outperforms traditional sequential signal enhancement methods, achieving better classification accuracy in noisy scenarios for image and speech recognition tasks.

Conclusion: The framework effectively enhances classification robustness under noise by leveraging the interaction between signal and classification logits through diffusion models.

Abstract: Robust classification in noisy environments remains a fundamental challenge in machine learning. Standard approaches typically treat signal enhancement and classification as separate, sequential stages: first enhancing the signal and then applying a classifier. This approach fails to leverage the semantic information in the classifier's output during denoising. In this work, we propose a general, domain-agnostic framework that integrates two interacting diffusion models: one operating on the input signal and the other on the classifier's output logits, without requiring any retraining or fine-tuning of the classifier. This coupled formulation enables mutual guidance, where the enhancing signal refines the class estimation and, conversely, the evolving class logits guide the signal reconstruction towards discriminative regions of the manifold. We introduce three strategies to effectively model the joint distribution of the input and the logit. We evaluated our joint enhancement method for image classification and automatic speech recognition. The proposed framework surpasses traditional sequential enhancement baselines, delivering robust and flexible improvements in classification accuracy under diverse noise conditions.

</details>


### [153] [Fairness over Equality: Correcting Social Incentives in Asymmetric Sequential Social Dilemmas](https://arxiv.org/abs/2602.15407)
*Alper Demir,Hüseyin Aydın,Kale-ab Abebe Tessera,David Abel,Stefano V. Albrecht*

Main category: cs.LG

TL;DR: The paper introduces improvements to fairness-based methods in asymmetric Sequential Social Dilemmas (SSDs) for more effective cooperation in Multi-Agent Reinforcement Learning.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing fairness-based approaches in Multi-Agent Reinforcement Learning that fail to adapt effectively under asymmetric incentives or partial observability.

Method: Proposes three key modifications: redefining fairness based on reward ranges, using an agent-based weighting mechanism for handling asymmetries, and localizing social feedback under partial observations.

Result: The proposed method outperforms existing approaches in fostering cooperative behavior in asymmetric Sequential Social Dilemmas while maintaining scalability and practical application.

Conclusion: The new framework improves cooperation and fairness under realistic asymmetric conditions, addressing key shortcomings of previous methods.

Abstract: Sequential Social Dilemmas (SSDs) provide a key framework for studying how cooperation emerges when individual incentives conflict with collective welfare. In Multi-Agent Reinforcement Learning, these problems are often addressed by incorporating intrinsic drives that encourage prosocial or fair behavior. However, most existing methods assume that agents face identical incentives in the dilemma and require continuous access to global information about other agents to assess fairness. In this work, we introduce asymmetric variants of well-known SSD environments and examine how natural differences between agents influence cooperation dynamics. Our findings reveal that existing fairness-based methods struggle to adapt under asymmetric conditions by enforcing raw equality that wrongfully incentivize defection. To address this, we propose three modifications: (i) redefining fairness by accounting for agents' reward ranges, (ii) introducing an agent-based weighting mechanism to better handle inherent asymmetries, and (iii) localizing social feedback to make the methods effective under partial observability without requiring global information sharing. Experimental results show that in asymmetric scenarios, our method fosters faster emergence of cooperative policies compared to existing approaches, without sacrificing scalability or practicality.

</details>


### [154] [Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning](https://arxiv.org/abs/2602.15817)
*Oswin So,Eric Yang Yu,Songyuan Zhang,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: The paper addresses the mismatch of RL’s optimization for expected returns with the goal of maximizing reachability sets. It proposes Feasibility-Guided Exploration (FGE), a novel method to learn safe policies and efficiently explore feasible states.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental mismatch between reinforcement learning optimization and the objective of reachability problems in ensuring state safety.

Method: Introduced Feasibility-Guided Exploration (FGE), a method that identifies feasible initial conditions and simultaneously learns safe policies to solve reachability problems.

Result: FGE achieves 50% more coverage than existing methods in challenging initial conditions across tasks in the MuJoCo and Kinetix simulators.

Conclusion: FGE effectively identifies feasible states and learns robust policies, improving the reachability performance on high-dimensional control tasks.

Abstract: Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.

</details>


### [155] [Benchmarking IoT Time-Series AD with Event-Level Augmentations](https://arxiv.org/abs/2602.15457)
*Dmitry Zhevnenko,Ilya Makarov,Aleksandr Kovalenko,Fedor Meshchaninov,Anton Kozhukhov,Vladislav Travnikov,Makar Ippolitov,Kirill Yashunin,Iurii Katser*

Main category: cs.LG

TL;DR: This paper emphasizes the importance of event-level anomaly detection in IoT time series for practical applications and proposes a comprehensive evaluation protocol using real-world perturbations to benchmark 14 models across several datasets.


<details>
  <summary>Details</summary>
Motivation: To address the gap between current point-level anomaly detection methods and event-level reliability required for real-world safety-critical IoT systems, while introducing realistic perturbations for robust model evaluation.

Method: The authors propose an evaluation protocol that incorporates event-level realism through unified augmentations such as sensor dropout, drift, noise, and window shifts. They also apply sensor-level probing through influence estimation and evaluate 14 anomaly detection models on benchmark and industrial datasets.

Result: No single model dominates across all settings. Certain models excel under specific conditions: graph-based models perform better under dropout and long events, spectral CNNs excel in periodic datasets, and reconstruction autoencoders are competitive with proper preprocessing. The study highlights trade-offs in model design under different perturbations.

Conclusion: Anomaly detection models require tailoring to specific contexts and perturbations, and event-level evaluation provides valuable insights for model selection and robustness. The unified protocol aids in understanding model strengths, weaknesses, and design implications in safety-critical IoT applications.

Abstract: Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.

</details>


### [156] [On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks](https://arxiv.org/abs/2602.15460)
*Yannic Neuhaus,Nicolas Flammarion,Matthias Hein,Francesco Croce*

Main category: cs.LG

TL;DR: The paper investigates generalization in reasoning models using a grid-based navigation task, finding limited out-of-distribution generalization despite improved in-distribution performance via chain-of-thought (CoT) reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the poorly understood generalization capabilities of reasoning models, particularly in tasks that require both in-distribution and out-of-distribution generalization.

Method: The method involves designing a grid-based navigation task for evaluation, fine-tuning models with visual and textual inputs under CoT strategies, and systematically assessing their performance in both ID and OOD scenarios.

Result: Text-based models outperform image-based models, with CoT reasoning improving ID generalization but offering limited OOD improvements. Combining multiple textual formats provides the best OOD generalization.

Conclusion: Reasoning models still struggle with generalization, especially OOD tasks, emphasizing the need for further research into techniques that improve reasoning capabilities in complex scenarios.

Abstract: Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate them under both in-distribution (ID) and out-of-distribution (OOD) test conditions. Our experiments show that, while CoT reasoning improves in-distribution generalization across all representations, out-of-distribution generalization (e.g., to larger maps) remains very limited in most cases when controlling for trivial matches with the ID data. Surprisingly, we find that reasoning traces which combine multiple text formats yield the best (and non-trivial) OOD generalization. Finally, purely text-based models consistently outperform those utilizing image-based inputs, including a recently proposed approach relying on latent space reasoning.

</details>


### [157] [POP: Prior-fitted Optimizer Policies](https://arxiv.org/abs/2602.15473)
*Jan Kobiolka,Christian Frey,Gresa Shala,Arlind Kadra,Erind Bedalli,Josif Grabocka*

Main category: cs.LG

TL;DR: The paper introduces a meta-learned optimizer, POP, which outperforms traditional optimization methods in various scenarios without task-specific tuning.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of classical gradient-based optimizers, such as sensitivity to hyperparameters and dependency on tuning in non-convex settings.

Method: Developed a meta-learned optimizer, POP, trained on synthetic optimization problems with a prior spanning convex and non-convex objectives to predict coordinate-wise step sizes based on optimization trajectory.

Result: POP outperformed first-order gradient-based methods, evolutionary strategies, Bayesian optimization, and a recent meta-learned competitor in benchmarks across 47 optimization functions.

Conclusion: POP demonstrates strong performance and generalization without task-specific parameter tuning, making it a key advancement in optimization techniques.

Abstract: Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.

</details>


### [158] [Evaluating Federated Learning for Cross-Country Mood Inference from Smartphone Sensing Data](https://arxiv.org/abs/2602.15478)
*Sharmad Kalpande,Saurabh Shirke,Haroon R. Lone*

Main category: cs.LG

TL;DR: This paper introduces a federated learning framework for mood inference using mobile sensing data, achieving improved performance and offering design insights for privacy-preserving mood-aware systems.


<details>
  <summary>Details</summary>
Motivation: Traditional mood assessments fail to capture the continuous nature of mood due to reliance on infrequent reports. Smartphone mobile sensing offers a passive solution, but challenges such as privacy and variability hinder widespread implementation.

Method: The authors propose FedFAP, a personalized federated framework for cross-country mood inference, accommodating diverse sensing modalities across regions while retaining local data.

Result: FedFAP outperformed centralized and existing federated methods, achieving an AUROC of 0.744 in evaluations across diverse populations.

Conclusion: FedFAP highlights the potential of privacy-preserving federated learning in enabling scalable mood-aware systems and offers valuable design insights for future implementations.

Abstract: Mood instability is a key behavioral indicator of mental health, yet traditional assessments rely on infrequent and retrospective reports that fail to capture its continuous nature. Smartphone-based mobile sensing enables passive, in-the-wild mood inference from everyday behaviors; however, deploying such systems at scale remains challenging due to privacy constraints, uneven sensing availability, and substantial variability in behavioral patterns.
  In this work, we study mood inference using smartphone sensing data in a cross-country federated learning setting, where each country participates as an independent client while retaining local data. We introduce FedFAP, a feature-aware personalized federated framework designed to accommodate heterogeneous sensing modalities across regions. Evaluations across geographically and culturally diverse populations show that FedFAP achieves an AUROC of 0.744, outperforming both centralized approaches and existing personalized federated baselines. Beyond inference, our results offer design insights for mood-aware systems, demonstrating how population-aware personalization and privacy-preserving learning can enable scalable and mood-aware mobile sensing technologies.

</details>


### [159] [LLM-as-Judge on a Budget](https://arxiv.org/abs/2602.15481)
*Aadirupa Saha,Aniket Wagde,Branislav Kveton*

Main category: cs.LG

TL;DR: The paper introduces a method for efficiently evaluating LLMs by allocating computational resources adaptively using multi-armed bandit theory. This minimizes estimation error for scoring prompt-response pairs within a limited budget.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of minimizing the estimation error in scoring LLM prompt-response pairs under a fixed computational budget while accounting for the stochastic nature of LLM judgments.

Method: The authors propose a variance-adaptive query allocation approach using multi-armed bandit theory. The algorithm dynamically assigns resources to prompt-response pairs with higher score uncertainty to optimize estimation accuracy.

Result: The proposed method achieves a worst-case error bound of $\tilde{O}(\sqrt{\frac{\sum_{i=1}^K σ_i^2}{B}})$ and outperforms uniform query allocation in experiments, effectively reducing errors under the same budget constraints.

Conclusion: The study introduces an efficient, theoretically grounded approach for scoring evaluation in LLMs, with broader implications for AI safety, model alignment, and scaling automated assessments.

Abstract: LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? %
We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\tilde{O}\left(\sqrt{\frac{\sum_{i=1}^K σ_i^2}{B}}\right)$, $σ_i^2$ being the unknown score variance for pair $i \in [K]$ with near-optimal budget allocation. %
Experiments on \emph{Summarize-From-Feedback} and \emph{HelpSteer2} demonstrate that our method significantly outperforms uniform allocation, reducing worst-case estimation error while maintaining identical budgets. Our work establishes a theoretical foundation for efficient LLM evaluation with practical implications for AI safety, model alignment, and automated assessment at scale.

</details>


### [160] [ExLipBaB: Exact Lipschitz Constant Computation for Piecewise Linear Neural Networks](https://arxiv.org/abs/2602.15499)
*Tom A. Splittgerber*

Main category: cs.LG

TL;DR: This paper introduces a generalized approach to compute the exact Lipschitz constant for any piecewise linear neural network under different $p$-norms.


<details>
  <summary>Details</summary>
Motivation: Exact computation of neural network Lipschitz constants is essential for robustness guarantees and benchmarking, but prior methods focus only on ReLU networks and overlook other activations.

Method: A generalized extension of the LipBaB algorithm is proposed to compute exact Lipschitz constants across arbitrary piecewise linear activations and $p$-norms, including ReLU, LeakyReLU, GroupSort, MinMax, FullSort, and MaxPool.

Result: The method enables exact calculations for broader network architectures with advanced activations, extending the utility beyond just ReLU-based networks.

Conclusion: This generalized approach offers a more versatile tool for exact Lipschitz constant computation, advancing robustness assessment and ensuring compatibility beyond traditional ReLU networks.

Abstract: It has been shown that a neural network's Lipschitz constant can be leveraged to derive robustness guarantees, to improve generalizability via regularization or even to construct invertible networks. Therefore, a number of methods varying in the tightness of their bounds and their computational cost have been developed to approximate the Lipschitz constant for different classes of networks. However, comparatively little research exists on methods for exact computation, which has been shown to be NP-hard. Nonetheless, there are applications where one might readily accept the computational cost of an exact method. These applications could include the benchmarking of new methods or the computation of robustness guarantees for small models on sensitive data. Unfortunately, existing exact algorithms restrict themselves to only ReLU-activated networks, which are known to come with severe downsides in the context of Lipschitz-constrained networks. We therefore propose a generalization of the LipBaB algorithm to compute exact Lipschitz constants for arbitrary piecewise linear neural networks and $p$-norms. With our method, networks may contain traditional activations like ReLU or LeakyReLU, activations like GroupSort or the related MinMax and FullSort, which have been of increasing interest in the context of Lipschitz constrained networks, or even other piecewise linear functions like MaxPool.

</details>


### [161] [The Obfuscation Atlas: Mapping Where Honesty Emerges in RLVR with Deception Probes](https://arxiv.org/abs/2602.15515)
*Mohammad Taufeeque,Stefan Heimersheim,Adam Gleave,Chris Cundy*

Main category: cs.LG

TL;DR: The paper explores the emergence of obfuscation in AI systems trained against deception detectors, identifying two obfuscation strategies and discussing how to achieve honesty using white-box detectors.


<details>
  <summary>Details</summary>
Motivation: To address the issue of AI systems becoming deceptive by obfuscating their behavior when trained against white-box deception detectors.

Method: Set up a realistic coding environment to observe reward hacking and introduce a taxonomy showing how either honesty or obfuscation arises. Analyzed how RL and penalty strategies interact with obfuscation through theoretical and empirical methods.

Result: Identified two types of obfuscation: (i) obfuscated activations (hidden deception) and (ii) obfuscated policy (justifying reward hacking). Showed that proper KL regularization and penalty strategies lead to honest models.

Conclusion: White-box deception detectors can serve as effective training signals to mitigate reward hacking when paired with sufficient regularization and penalties.

Abstract: Training against white-box deception detectors has been proposed as a way to make AI systems honest. However, such training risks models learning to obfuscate their deception to evade the detector. Prior work has studied obfuscation only in artificial settings where models were directly rewarded for harmful output. We construct a realistic coding environment where reward hacking via hardcoding test cases naturally occurs, and show that obfuscation emerges in this setting. We introduce a taxonomy of possible outcomes when training against a deception detector. The model either remains honest, or becomes deceptive via two possible obfuscation strategies. (i) Obfuscated activations: the model outputs deceptive text while modifying its internal representations to no longer trigger the detector. (ii) Obfuscated policy: the model outputs deceptive text that evades the detector, typically by including a justification for the reward hack. Empirically, obfuscated activations arise from representation drift during RL, with or without a detector penalty. The probe penalty only incentivizes obfuscated policies; we theoretically show this is expected for policy gradient methods. Sufficiently high KL regularization and detector penalty can yield honest policies, establishing white-box deception detectors as viable training signals for tasks prone to reward hacking.

</details>


### [162] [CEPAE: Conditional Entropy-Penalized Autoencoders for Time Series Counterfactuals](https://arxiv.org/abs/2602.15546)
*Tomàs Garriga,Gerard Sanz,Eduard Serrahima de Cambra,Axel Brando*

Main category: cs.LG

TL;DR: The paper introduces CEPAE, a novel method for counterfactual inference in time series using autoencoder techniques and entropy penalization in the latent space. CEPAE outperforms alternative approaches.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve counterfactual inference in time series data for applications like finance, healthcare, and marketing, where assessing the impact of events is crucial.

Method: The authors adapt methods from variational and adversarial autoencoders and propose the CEPAE model, which integrates entropy penalization loss into the latent space.

Result: Experiments on synthetic, semi-synthetic, and real-world datasets show that CEPAE generally achieves better performance compared to other methods.

Conclusion: CEPAE is an effective counterfactual inference model for time series, offering improved evaluation metrics across various datasets.

Abstract: The ability to accurately perform counterfactual inference on time series is crucial for decision-making in fields like finance, healthcare, and marketing, as it allows us to understand the impact of events or treatments on outcomes over time. In this paper, we introduce a new counterfactual inference approach tailored to time series data impacted by market events, which is motivated by an industrial application. Utilizing the abduction-action-prediction procedure and the Structural Causal Model framework, we first adapt methods based on variational autoencoders and adversarial autoencoders, both previously used in counterfactual literature although not in time series settings. Then, we present the Conditional Entropy-Penalized Autoencoder (CEPAE), a novel autoencoder-based approach for counterfactual inference, which employs an entropy penalization loss over the latent space to encourage disentangled data representations. We validate our approach both theoretically and experimentally on synthetic, semi-synthetic, and real-world datasets, showing that CEPAE generally outperforms the other approaches in the evaluated metrics.

</details>


### [163] [1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization](https://arxiv.org/abs/2602.15563)
*Sohir Maskey,Constantin Eichenberg,Johannes Messner,Douglas Orr*

Main category: cs.LG

TL;DR: This paper investigates quantization-aware training (QAT) for large language models (LLMs) in the low-bit regime and finds that k-means-based weight quantization and 1-bit weights offer efficient and effective solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to optimize memory and performance trade-offs in LLMs through QAT, addressing the unexplored design space for quantization formats and bit-widths, and assessing performance beyond just perplexity evaluations.

Method: The authors conduct an empirical study of QAT at low bit-widths, comparing different quantization formats like k-means and integer-based methods, while evaluating performance on generative downstream tasks.

Result: The study reveals that k-means-based quantization outperforms integer formats and is easily implementable on standard hardware. Additionally, 1-bit quantized weights provide optimal performance under a fixed memory budget.

Conclusion: Low-bit k-means quantization is a superior and practical approach for QAT, offering efficient and high-performing solutions for LLMs, especially when using 1-bit weights under memory constraints.

Abstract: Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.

</details>


### [164] [Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment](https://arxiv.org/abs/2602.15571)
*Davide Casnici,Martin Lefebvre,Justin Dauwels,Charlotte Frenkel*

Main category: cs.LG

TL;DR: The paper introduces DKP-PC, an improvement on predictive coding that avoids feedback decay and delay by creating direct pathways for error signals across layers, enhancing efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Predictive coding algorithms face challenges such as exponential decay and feedback delay when training neural networks efficiently, limiting their scalability and accuracy.

Method: DKP-PC employs direct feedback alignment and direct Kolen-Pollack algorithms, introducing learnable feedback connections from the output layer to all hidden layers, ensuring error signals propagate directly.

Result: DKP-PC reduces error propagation time complexity to O(1), significantly enhances computational performance, and achieves comparable or superior accuracy compared to standard predictive coding.

Conclusion: DKP-PC makes predictive coding more efficient and scalable by reducing latency and feedback issues, holding promise for custom hardware implementations.

Abstract: Predictive coding (PC) is a biologically inspired algorithm for training neural networks that relies only on local updates, allowing parallel learning across layers. However, practical implementations face two key limitations: error signals must still propagate from the output to early layers through multiple inference-phase steps, and feedback decays exponentially during this process, leading to vanishing updates in early layers. We propose direct Kolen-Pollack predictive coding (DKP-PC), which simultaneously addresses both feedback delay and exponential decay, yielding a more efficient and scalable variant of PC while preserving update locality. Leveraging direct feedback alignment and direct Kolen-Pollack algorithms, DKP-PC introduces learnable feedback connections from the output layer to all hidden layers, establishing a direct pathway for error transmission. This yields an algorithm that reduces the theoretical error propagation time complexity from O(L), with L being the network depth, to O(1), removing depth-dependent delay in error signals. Moreover, empirical results demonstrate that DKP-PC achieves performance at least comparable to, and often exceeding, that of standard PC, while offering improved latency and computational performance, supporting its potential for custom hardware-efficient implementations.

</details>


### [165] [Neural Network-Based Parameter Estimation of a Labour Market Agent-Based Model](https://arxiv.org/abs/2602.15572)
*M Lopes Alves,Joel Dyer,Doyne Farmer,Michael Wooldridge,Anisoara Calinescu*

Main category: cs.LG

TL;DR: The paper evaluates neural network-based simulation-based inference frameworks for parameter estimation in large-scale agent-based models, specifically in labor market simulations.


<details>
  <summary>Details</summary>
Motivation: Challenges in parameter estimation for large-scale agent-based models hinder their application as decision-support tools.

Method: Employing neural network-based simulation-based inference to estimate parameters of an agent-based model of job transition networks using synthetic and real datasets.

Result: NN-based inference outperforms traditional Bayesian methods, accurately recovering parameters and improving efficiency across different dataset scales.

Conclusion: The approach using neural networks enhances parameter estimation in large-scale agent-based models, promising advancements in computational efficiency and application domains.

Abstract: Agent-based modelling (ABM) is a widespread approach to simulate complex systems. Advancements in computational processing and storage have facilitated the adoption of ABMs across many fields; however, ABMs face challenges that limit their use as decision-support tools. A significant issue is parameter estimation in large-scale ABMs, particularly due to computational constraints on exploring the parameter space. This study evaluates a state-of-the-art simulation-based inference (SBI) framework that uses neural networks (NN) for parameter estimation. This framework is applied to an established labour market ABM based on job transition networks. The ABM is initiated with synthetic datasets and the real U.S. labour market. Next, we compare the effectiveness of summary statistics derived from a list of statistical measures with that learned by an embedded NN. The results demonstrate that the NN-based approach recovers the original parameters when evaluating posterior distributions across various dataset scales and improves efficiency compared to traditional Bayesian methods.

</details>


### [166] [A unified theory of feature learning in RNNs and DNNs](https://arxiv.org/abs/2602.15593)
*Jan P. Bauer,Kirsten Fischer,Moritz Helias,Agostina Palmigiano*

Main category: cs.LG

TL;DR: The paper develops a unified mean-field theory for RNNs and DNNs under the feature learning regime, exploring their functional distinctions due to weight sharing.


<details>
  <summary>Details</summary>
Motivation: To understand the functional differences and advantages of RNNs and DNNs despite their structural similarity through unrolling.

Method: A unified mean-field theory based on representational kernels is proposed, connecting training in RNNs and DNNs to Bayesian inference.

Result: Identified a phase transition in tasks: RNNs uniquely develop correlations across timesteps beyond a threshold, offering better generalization in sequential tasks.

Conclusion: The theory bridges architectural differences between RNNs and DNNs to functional outcomes, elucidating the role of weight sharing in RNNs.

Abstract: Recurrent and deep neural networks (RNNs/DNNs) are cornerstone architectures in machine learning. Remarkably, RNNs differ from DNNs only by weight sharing, as can be shown through unrolling in time. How does this structural similarity fit with the distinct functional properties these networks exhibit? To address this question, we here develop a unified mean-field theory for RNNs and DNNs in terms of representational kernels, describing fully trained networks in the feature learning ($μ$P) regime. This theory casts training as Bayesian inference over sequences and patterns, directly revealing the functional implications induced by the RNNs' weight sharing. In DNN-typical tasks, we identify a phase transition when the learning signal overcomes the noise due to randomness in the weights: below this threshold, RNNs and DNNs behave identically; above it, only RNNs develop correlated representations across timesteps. For sequential tasks, the RNNs' weight sharing furthermore induces an inductive bias that aids generalization by interpolating unsupervised time steps. Overall, our theory offers a way to connect architectural structure to functional biases.

</details>


### [167] [Multi-Objective Coverage via Constraint Active Search](https://arxiv.org/abs/2602.15595)
*Zakaria Shams Siam,Xuefeng Liu,Chong Liu*

Main category: cs.LG

TL;DR: The paper introduces the Multi-Objective Coverage (MOC) problem to identify a small set of representative samples that cover the multi-objective space, accelerating applications like drug discovery and materials design. The proposed MOC-CAS algorithm outperforms baselines in SARS-CoV-2 and cancer datasets.


<details>
  <summary>Details</summary>
Motivation: Accelerate scientific discovery in fields like drug discovery and materials design by covering the multi-objective feasible space with representative samples.

Method: The MOC-CAS algorithm uses an upper confidence bound acquisition function with Gaussian process predictions. It incorporates a smoothed feasibility test and approximate optimization techniques.

Result: The proposed algorithm demonstrates superior performance in large-scale datasets related to SARS-CoV-2 and cancer, evaluated on multi-objective SMILES-based features.

Conclusion: MOC-CAS effectively solves the MOC problem, achieving efficient optimization and broader applicability in critical scientific areas.

Abstract: In this paper, we formulate the new multi-objective coverage (MOC) problem where our goal is to identify a small set of representative samples whose predicted outcomes broadly cover the feasible multi-objective space. This problem is of great importance in many critical real-world applications, e.g., drug discovery and materials design, as this representative set can be evaluated much faster than the whole feasible set, thus significantly accelerating the scientific discovery process. Existing works cannot be directly applied as they either focus on sample space coverage or multi-objective optimization that targets the Pareto front. However, chemically diverse samples often yield identical objective profiles, and safety constraints are usually defined on the objectives. To solve this MOC problem, we propose a novel search algorithm, MOC-CAS, which employs an upper confidence bound-based acquisition function to select optimistic samples guided by Gaussian process posterior predictions. For enabling efficient optimization, we develop a smoothed relaxation of the hard feasibility test and derive an approximate optimizer. Compared to the competitive baselines, we show that our MOC-CAS empirically achieves superior performances across large-scale protein-target datasets for SARS-CoV-2 and cancer, each assessed on five objectives derived from SMILES-based features.

</details>


### [168] [Symbolic recovery of PDEs from measurement data](https://arxiv.org/abs/2602.15603)
*Erion Morina,Philipp Scholl,Martin Holler*

Main category: cs.LG

TL;DR: This paper discusses using neural network architectures based on rational functions to identify partial differential equation (PDE) models that describe physical laws, emphasizing interpretability and sparsity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing interpretable and symbolic PDE models from indirect and noisy system measurements.

Method: The study employs symbolic neural networks relying on rational functions, leveraging their arithmetic operation representation and proposing identifiability in noiseless complete measurements with $L^1$-regularization.

Result: Theoretical results demonstrated identifiability, interpretability, and sparsity of symbolic networks; empirical validations using ParFam architecture reinforce the findings.

Conclusion: Symbolic networks based on rational functions effectively and uniquely reconstruct PDE models, prioritizing simplicity and interpretability through regularization.

Abstract: Models based on partial differential equations (PDEs) are powerful for describing a wide range of complex relationships in the natural sciences. Accurately identifying the PDE model, which represents the underlying physical law, is essential for a proper understanding of the problem. This reconstruction typically relies on indirect and noisy measurements of the system's state and, without specifically tailored methods, rarely yields symbolic expressions, thereby hindering interpretability. In this work, we address this issue by considering existing neural network architectures based on rational functions for the symbolic representation of physical laws. These networks leverage the approximation power of rational functions while also benefiting from their flexibility in representing arithmetic operations. Our main contribution is an identifiability result, showing that, in the limit of noiseless, complete measurements, such symbolic networks can uniquely reconstruct the simplest physical law within the PDE model. Specifically, reconstructed laws remain expressible within the symbolic network architecture, with regularization-minimizing parameterizations promoting interpretability and sparsity in case of $L^1$-regularization. In addition, we provide regularity results for symbolic networks. Empirical validation using the ParFam architecture supports these theoretical findings, providing evidence for the practical reconstructibility of physical laws.

</details>


### [169] [GLM-5: from Vibe Coding to Agentic Engineering](https://arxiv.org/abs/2602.15763)
*GLM-5 Team,:,Aohan Zeng,Xin Lv,Zhenyu Hou,Zhengxiao Du,Qinkai Zheng,Bin Chen,Da Yin,Chendi Ge,Chengxing Xie,Cunxiang Wang,Gengzheng Pan,Hao Zeng,Haoke Zhang,Haoran Wang,Huilong Chen,Jiajie Zhang,Jian Jiao,Jiaqi Guo,Jingsen Wang,Jingzhao Du,Jinzhu Wu,Kedong Wang,Lei Li,Lin Fan,Lucen Zhong,Mingdao Liu,Mingming Zhao,Pengfan Du,Qian Dong,Rui Lu,Shuang-Li,Shulin Cao,Song Liu,Ting Jiang,Xiaodong Chen,Xiaohan Zhang,Xuancheng Huang,Xuezhen Dong,Yabo Xu,Yao Wei,Yifan An,Yilin Niu,Yitong Zhu,Yuanhao Wen,Yukuo Cen,Yushi Bai,Zhongpei Qiao,Zihan Wang,Zikang Wang,Zilin Zhu,Ziqiang Liu,Zixuan Li,Bojie Wang,Bosi Wen,Can Huang,Changpeng Cai,Chao Yu,Chen Li,Chen Li,Chenghua Huang,Chengwei Hu,Chenhui Zhang,Chenzheng Zhu,Congfeng Yin,Daoyan Lin,Dayong Yang,Di Wang,Ding Ai,Erle Zhu,Fangzhou Yi,Feiyu Chen,Guohong Wen,Hailong Sun,Haisha Zhao,Haiyi Hu,Hanchen Zhang,Hanrui Liu,Hanyu Zhang,Hao Peng,Hao Tai,Haobo Zhang,He Liu,Hongwei Wang,Hongxi Yan,Hongyu Ge,Huan Liu,Huan Liu,Huanpeng Chu,Jia'ni Zhao,Jiachen Wang,Jiajing Zhao,Jiamin Ren,Jiapeng Wang,Jiaxin Zhang,Jiayi Gui,Jiayue Zhao,Jijie Li,Jing An,Jing Li,Jingwei Yuan,Jinhua Du,Jinxin Liu,Junkai Zhi,Junwen Duan,Kaiyue Zhou,Kangjian Wei,Ke Wang,Keyun Luo,Laiqiang Zhang,Leigang Sha,Liang Xu,Lindong Wu,Lintao Ding,Lu Chen,Minghao Li,Nianyi Lin,Pan Ta,Qiang Zou,Rongjun Song,Ruiqi Yang,Shangqing Tu,Shangtong Yang,Shaoxiang Wu,Shengyan Zhang,Shijie Li,Shuang Li,Shuyi Fan,Wei Qin,Wei Tian,Weining Zhang,Wenbo Yu,Wenjie Liang,Xiang Kuang,Xiangmeng Cheng,Xiangyang Li,Xiaoquan Yan,Xiaowei Hu,Xiaoying Ling,Xing Fan,Xingye Xia,Xinyuan Zhang,Xinze Zhang,Xirui Pan,Xunkai Zhang,Yandong Wu,Yanfu Li,Yidong Wang,Yifan Zhu,Yijun Tan,Yilin Zhou,Yiming Pan,Ying Zhang,Yinpei Su,Yipeng Geng,Yipeng Geng,Yong Yan,Yonglin Tan,Yuean Bi,Yuhan Shen,Yuhao Yang,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yurong Wu,Yutao Zhang,Yuxi Duan,Yuxuan Zhang,Zezhen Liu,Zhengtao Jiang,Zhenhe Yan,Zheyu Zhang,Zhixiang Wei,Zhuo Chen,Zhuoer Feng,Zijun Yao,Ziwei Chai,Ziyuan Wang,Zuzhou Zhang,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.LG

TL;DR: GLM-5 introduces advancements in foundation models with enhanced coding and reasoning capabilities, reducing training and inference costs, and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To innovate the paradigm of coding and agentic engineering with an efficient, high-performing foundation model that excels in practical application.

Method: GLM-5 employs Decoupled Sparse Attention (DSA) for cost-efficiency, implements asynchronous reinforcement learning for better alignment, and introduces novel RL algorithms for enhanced model training.

Result: GLM-5 achieves state-of-the-art results on benchmarks and surpasses prior baselines in real-world coding tasks.

Conclusion: GLM-5 establishes itself as a cutting-edge model in coding and agentic engineering, combining efficiency and high performance, especially in complex software engineering scenarios.

Abstract: We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.

</details>


### [170] [DNN-Enabled Multi-User Beamforming for Throughput Maximization under Adjustable Fairness](https://arxiv.org/abs/2602.15617)
*Kaifeng Lu,Markus Rupp,Stefan Schwarz*

Main category: cs.LG

TL;DR: The paper proposes an optimization-based unsupervised learning approach using the wireless transformer (WiT) to balance fairness and sum rate in wireless communications.


<details>
  <summary>Details</summary>
Motivation: Balancing the trade-off between fairness and sum rate in wireless communications is challenging due to the resulting non-convex, multi-objective optimization, especially as network scale increases.

Method: It combines fairness and sum rate objectives using a Lagrangian multiplier updated via a dual-ascent algorithm, enabling WiT to learn from channel state information (CSI) features.

Result: The proposed approach realizes a trace on the Pareto front between fairness and sum rate while offering controllable constraints and optimizing network performance.

Conclusion: This method provides a flexible optimization solution under prescribed fairness, tackling the fairness-sum rate trade-off in wireless networks effectively.

Abstract: Ensuring user fairness in wireless communications is a fundamental challenge, as balancing the trade-off between fairness and sum rate leads to a non-convex, multi-objective optimization whose complexity grows with network scale. To alleviate this conflict, we propose an optimization-based unsupervised learning approach based on the wireless transformer (WiT) architecture that learns from channel state information (CSI) features. We reformulate the trade-off by combining the sum rate and fairness objectives through a Lagrangian multiplier, which is updated automatically via a dual-ascent algorithm. This mechanism allows for a controllable fairness constraint while simultaneously maximizing the sum rate, effectively realizing a trace on the Pareto front between two conflicting objectives. Our findings show that the proposed approach offers a flexible solution for managing the trade-off optimization under prescribed fairness.

</details>


### [171] [Guided Diffusion by Optimized Loss Functions on Relaxed Parameters for Inverse Material Design](https://arxiv.org/abs/2602.15648)
*Jens U. Kreber,Christian Weißenfels,Joerg Stueckler*

Main category: cs.LG

TL;DR: The paper introduces a novel inverse design method using diffusion models to tackle design limitations in composite material problems, achieving diverse and optimal solutions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in inverse design problems where standard gradient-based optimization fails due to discrete parameters and constraints, and where multi-modal solutions are important.

Method: Relaxing the design space into a continuous grid representation for gradient computation, training a diffusion model on this space as a prior, and sampling parameters using guided diffusion with gradients propagated through a differentiable simulation.

Result: The method finds diverse material designs with a bulk modulus within 1% relative error for 2D and 3D settings and minimizes material density using a multi-objective loss.

Conclusion: The approach efficiently generates diverse, optimal designs for complex engineering problems, overcoming traditional optimization constraints.

Abstract: Inverse design problems are common in engineering and materials science. The forward direction, i.e., computing output quantities from design parameters, typically requires running a numerical simulation, such as a FEM, as an intermediate step, which is an optimization problem by itself. In many scenarios, several design parameters can lead to the same or similar output values. For such cases, multi-modal probabilistic approaches are advantageous to obtain diverse solutions. A major difficulty in inverse design stems from the structure of the design space, since discrete parameters or further constraints disallow the direct use of gradient-based optimization. To tackle this problem, we propose a novel inverse design method based on diffusion models. Our approach relaxes the original design space into a continuous grid representation, where gradients can be computed by implicit differentiation in the forward simulation. A diffusion model is trained on this relaxed parameter space in order to serve as a prior for plausible relaxed designs. Parameters are sampled by guided diffusion using gradients that are propagated from an objective function specified at inference time through the differentiable simulation. A design sample is obtained by backprojection into the original parameter space. We develop our approach for a composite material design problem where the forward process is modeled as a linear FEM problem. We evaluate the performance of our approach in finding designs that match a specified bulk modulus. We demonstrate that our method can propose diverse designs within 1% relative error margin from medium to high target bulk moduli in 2D and 3D settings. We also demonstrate that the material density of generated samples can be minimized simultaneously by using a multi-objective loss function.

</details>


### [172] [Beyond ReLU: Bifurcation, Oversmoothing, and Topological Priors](https://arxiv.org/abs/2602.15634)
*Erkan Turan,Gaspard Abel,Maysam Behmanesh,Emery Pierson,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: This paper addresses oversmoothing in Graph Neural Networks (GNNs) by introducing a theoretical approach using bifurcation theory, proposing a solution through a specific activation function modification.


<details>
  <summary>Details</summary>
Motivation: GNNs face a fundamental challenge of oversmoothing, where node features become indistinguishable in deep networks, rendering the representations non-informative.

Method: The authors explore oversmoothing through bifurcation theory, analytically showing that replacing standard activation functions with specific alternatives breaks the stable homogeneous fixed point state, creating new stable patterns resistant to oversmoothing.

Result: The proposed approach is validated both theoretically and experimentally, confirming the scaling laws of emergent patterns and demonstrating enhanced performance through bifurcation-aware initialization in benchmark tests.

Conclusion: The study provides a novel understanding of oversmoothing in GNNs and proposes a theoretically driven method to overcome it, showing practical benefits for deep network designs.

Abstract: Graph Neural Networks (GNNs) learn node representations through iterative network-based message-passing. While powerful, deep GNNs suffer from oversmoothing, where node features converge to a homogeneous, non-informative state. We re-frame this problem of representational collapse from a \emph{bifurcation theory} perspective, characterizing oversmoothing as convergence to a stable ``homogeneous fixed point.'' Our central contribution is the theoretical discovery that this undesired stability can be broken by replacing standard monotone activations (e.g., ReLU) with a class of functions. Using Lyapunov-Schmidt reduction, we analytically prove that this substitution induces a bifurcation that destabilizes the homogeneous state and creates a new pair of stable, non-homogeneous \emph{patterns} that provably resist oversmoothing. Our theory predicts a precise, nontrivial scaling law for the amplitude of these emergent patterns, which we quantitatively validate in experiments. Finally, we demonstrate the practical utility of our theory by deriving a closed-form, bifurcation-aware initialization and showing its utility in real benchmark experiments.

</details>


### [173] [The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems](https://arxiv.org/abs/2602.15637)
*Amirreza Dolatpour Fathkouhi,Alireza Namazi,Heman Shakeri*

Main category: cs.LG

TL;DR: The paper argues that current benchmarks for time-series imputation are inherently biased towards stationary regimes, which oversimplifies model evaluation. It suggests stratified stress-testing by separating stationary and transient regimes for balanced evaluation.


<details>
  <summary>Details</summary>
Motivation: Highlight biases in time-series imputation benchmarks that favor simplistic methods due to the dominance of stationary regimes.

Method: Proposes a stratified evaluation framework and applies it to CGM data, leveraging known forcing functions to separate stationary and transient regimes.

Result: Linear methods excel in stationary regimes but fail critically during transients; deep models prove indispensable for transient fidelity and real-world missingness.

Conclusion: A stratified stress-testing approach is crucial for accurately assessing time-series imputation models in regulated systems, ensuring robustness and reliable evaluation.

Abstract: Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a systematic \emph{Stationarity Bias}: simple methods appear superior because the benchmark predominantly samples the easy, low-entropy regime where they trivially succeed. We formalize this bias and propose a \emph{Stratified Stress-Test} that partitions evaluation into Stationary and Transient regimes. Using Continuous Glucose Monitoring (CGM) as a testbed -- chosen for its rigorous ground-truth forcing functions (meals, insulin) that enable precise regime identification -- we establish three findings with broad implications:(i)~Stationary Efficiency: Linear interpolation achieves state-of-the-art reconstruction during stable intervals, confirming that complex architectures are computationally wasteful in low-entropy regimes.(ii)~Transient Fidelity: During critical transients (post-prandial peaks, hypoglycemic events), linear methods exhibit drastically degraded morphological fidelity (DTW), disproportionate to their RMSE -- a phenomenon we term the \emph{RMSE Mirage}, where low pointwise error masks the destruction of signal shape.(iii)~Regime-Conditional Model Selection: Deep learning models preserve both pointwise accuracy and morphological integrity during transients, making them essential for safety-critical downstream tasks. We further derive empirical missingness distributions from clinical trials and impose them on complete training data, preventing models from exploiting unrealistically clean observations and encouraging robustness under real-world missingness. This framework generalizes to any regulated system where routine stationarity dominates critical transients.

</details>


### [174] [Continuous-Time Piecewise-Linear Recurrent Neural Networks](https://arxiv.org/abs/2602.15649)
*Alena Brändle,Lukas Eisenmann,Florian Götz,Daniel Durstewitz*

Main category: cs.LG

TL;DR: The paper introduces continuous-time PLRNNs (cPLRNNs) for dynamical systems reconstruction, aiming to combine the benefits of continuous-time modeling with the mathematical tractability of PLRNNs.


<details>
  <summary>Details</summary>
Motivation: Discrete-time PLRNNs, while successful, are limited by their inability to model continuous-time processes and handle irregularly timed data common in many scientific and biological systems.

Method: The paper develops theory and a novel algorithm for training and simulating continuous-time PLRNNs. It also emphasizes exploiting the piecewise-linear structure efficiently to bypass numerical integration and enables semi-analytical determination of equilibria or limit cycles.

Result: The cPLRNNs demonstrate strong performance on DSR benchmarks, outperforming Neural ODEs and showing advantages in systems with discontinuities.

Conclusion: Continuous-time PLRNNs extend the capability of PLRNNs to model continuous-time dynamical systems while retaining interpretability and strong performance, addressing key challenges in existing models.

Abstract: In dynamical systems reconstruction (DSR) we aim to recover the dynamical system (DS) underlying observed time series. Specifically, we aim to learn a generative surrogate model which approximates the underlying, data-generating DS, and recreates its long-term properties (`climate statistics'). In scientific and medical areas, in particular, these models need to be mechanistically tractable -- through their mathematical analysis we would like to obtain insight into the recovered system's workings. Piecewise-linear (PL), ReLU-based RNNs (PLRNNs) have a strong track-record in this regard, representing SOTA DSR models while allowing mathematical insight by virtue of their PL design. However, all current PLRNN variants are discrete-time maps. This is in disaccord with the assumed continuous-time nature of most physical and biological processes, and makes it hard to accommodate data arriving at irregular temporal intervals. Neural ODEs are one solution, but they do not reach the DSR performance of PLRNNs and often lack their tractability. Here we develop theory for continuous-time PLRNNs (cPLRNNs): We present a novel algorithm for training and simulating such models, bypassing numerical integration by efficiently exploiting their PL structure. We further demonstrate how important topological objects like equilibria or limit cycles can be determined semi-analytically in trained models. We compare cPLRNNs to both their discrete-time cousins as well as Neural ODEs on DSR benchmarks, including systems with discontinuities which come with hard thresholds.

</details>


### [175] [Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry](https://arxiv.org/abs/2602.15676)
*Deniz Kucukahmetler,Maximilian Jean Hemmann,Julian Mosig von Aehrenfeld,Maximilian Amthor,Christian Deubel,Nico Scherf,Diaaeldin Taha*

Main category: cs.LG

TL;DR: This paper examines how neural networks represent latent geometries of dynamical systems using a novel framework for comparing embeddings, revealing patterns in alignment across various model types.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks internally represent the latent structures of dynamical systems, especially given the ambiguities in geometry representation like rotation and scaling.

Method: The authors developed a geometry-agnostic, anchor-based framework to analyze representational alignment and applied it to seven canonical dynamical systems using neural networks such as MLPs, RNNs, transformers, and echo-state networks.

Result: They found consistent model-family structures (e.g., MLPs aligning with other MLPs) and differing alignment levels that generally correlate with forecasting accuracy, though exceptions exist.

Conclusion: Relative geometry offers a foundational approach for comparing internal representations across neural network models and understanding their structural alignments.

Abstract: Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level structure: multilayer perceptrons align with other MLPs, recurrent networks with RNNs, while transformers and echo-state networks achieve strong forecasts despite weaker alignment. Alignment generally correlates with forecasting accuracy, yet high accuracy can coexist with low alignment. Relative geometry thus provides a simple, reproducible foundation for comparing how model families internalize and represent dynamical structure.

</details>


### [176] [CAMEL: An ECG Language Model for Forecasting Cardiac Events](https://arxiv.org/abs/2602.15677)
*Neelay Velingker,Alaia Solko-Breslin,Mayank Keoliya,Seewon Choi,Jiayi Xin,Anika Marathe,Alireza Oraii,Rajat Deo,Sameed Khatana,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: This paper introduces CAMEL, an advanced ECG language model that adds the capability of forecasting future cardiac events, showcasing superior performance on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the inability of existing ECG language models to forecast future cardiac events despite their significance in early intervention planning.

Method: They created CAMEL with a specialized ECG encoder to enable cross-understanding of ECG signals with text and trained it using LLM protocols, incorporating LoRA adaptation and curriculum learning.

Result: CAMEL achieved state-of-the-art results, with a +7.0% absolute average gain on ECGBench and significant improvements on ECGForecastBench (+12.4% over supervised models, +21.1% over zero-shot ELMs).

Conclusion: CAMEL represents a major step forward in ECG analysis, enabling both accurate classification and practical forecasting of cardiac events, which could greatly aid clinical decision-making.

Abstract: Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).

</details>


### [177] [Controlled oscillation modeling using port-Hamiltonian neural networks](https://arxiv.org/abs/2602.15704)
*Maximino Linares,Guillaume Doras,Thomas Hélie*

Main category: cs.LG

TL;DR: This study introduces a discrete gradient method with port-Hamiltonian neural networks, outperforming existing numerical approaches in learning dynamical systems.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven methods for learning dynamical systems fail to generalize well due to their disregard for underlying conservation laws. Port-Hamiltonian networks are promising but rely on methods like Runge-Kutta, which do not ensure power balance discretizations.

Method: The authors propose integrating a second-order discrete gradient method into port-Hamiltonian neural networks to improve capturing dynamical behaviors.

Result: The proposed method evidences superior performance over Runge-Kutta methods in modeling three systems with varying dynamical behaviors.

Conclusion: The new discrete gradient method enhances the training and performance of port-Hamiltonian neural networks, and regularizing the Jacobian further impacts the outcomes positively.

Abstract: Learning dynamical systems through purely data-driven methods is challenging as they do not learn the underlying conservation laws that enable them to correctly generalize. Existing port-Hamiltonian neural network methods have recently been successfully applied for modeling mechanical systems. However, even though these methods are designed on power-balance principles, they usually do not consider power-preserving discretizations and often rely on Runge-Kutta numerical methods. In this work, we propose to use a second-order discrete gradient method embedded in the learning of dynamical systems with port-Hamiltonian neural networks. Numerical results are provided for three systems deliberately selected to span different ranges of dynamical behavior under control: a baseline harmonic oscillator with quadratic energy storage; a Duffing oscillator, with a non-quadratic Hamiltonian offering amplitude-dependent effects; and a self-sustained oscillator, which can stabilize in a controlled limit cycle through the incorporation of a nonlinear dissipation. We show how the use of this discrete gradient method outperforms the performance of a Runge-Kutta method of the same order. Experiments are also carried out to compare two theoretically equivalent port-Hamiltonian systems formulations and to analyze the impact of regularizing the Jacobian of port-Hamiltonian neural networks during training.

</details>


### [178] [Random Wavelet Features for Graph Kernel Machines](https://arxiv.org/abs/2602.15711)
*Valentin de Bassompierre,Jean-Charles Delvenne,Laurent Jacques*

Main category: cs.LG

TL;DR: The paper introduces randomized spectral node embeddings to efficiently approximate graph kernels, aiming for scalable and principled graph representation learning.


<details>
  <summary>Details</summary>
Motivation: Current node embedding methods often struggle to accurately capture graph-induced node similarity due to computational limitations, especially for large networks.

Method: The authors propose randomized spectral node embeddings, leveraging random feature methods to provide low-rank approximations of graph kernels.

Result: The embeddings outperform existing methods for kernel approximation, especially for spectrally localized kernels, as supported by theoretical and empirical results.

Conclusion: Randomized spectral node embeddings are effective for scalable and accurate graph representation learning, offering substantial improvements over existing approaches.

Abstract: Node embeddings map graph vertices into low-dimensional Euclidean spaces while preserving structural information. They are central to tasks such as node classification, link prediction, and signal reconstruction. A key goal is to design node embeddings whose dot products capture meaningful notions of node similarity induced by the graph. Graph kernels offer a principled way to define such similarities, but their direct computation is often prohibitive for large networks. Inspired by random feature methods for kernel approximation in Euclidean spaces, we introduce randomized spectral node embeddings whose dot products estimate a low-rank approximation of any specific graph kernel. We provide theoretical and empirical results showing that our embeddings achieve more accurate kernel approximations than existing methods, particularly for spectrally localized kernels. These results demonstrate the effectiveness of randomized spectral constructions for scalable and principled graph representation learning.

</details>


### [179] [MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2602.15740)
*Fatemeh Khalvandi,Saadat Izadi,Abdolah Chalechale*

Main category: cs.LG

TL;DR: The paper introduces MRC-GAT, a multimodal graph-based diagnostic model for Alzheimer's disease (AD), achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed structural designs in current graph-based models for AD diagnosis and improve generalization across diverse patient data.

Method: The proposed MRC-GAT model uses episodic meta-learning with components like copula-based similarity alignment, relational attention, and node fusion to integrate multimodal features for better AD classification.

Result: MRC-GAT achieves state-of-the-art accuracies of 96.87% on TADPOLE and 92.31% on NACC datasets, demonstrating superior performance in AD diagnosis.

Conclusion: The MRC-GAT model enhances diagnostic precision and reliability, while providing greater interpretability and flexibility for diverse patient data in AD diagnosis.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis.

</details>


### [180] [UrbanVerse: Learning Urban Region Representation Across Cities and Tasks](https://arxiv.org/abs/2602.15750)
*Fengze Sun,Egemen Tanin,Shanika Karunasekera,Zuqing Li,Flora D. Salim,Jianzhong Qi*

Main category: cs.LG

TL;DR: UrbanVerse is a model designed for generalizing urban representation learning across cities and tasks, achieving significant improvements in prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: To move beyond city-specific and task-specific urban analytics methods towards a foundational model for diverse cities and tasks.

Method: UrbanVerse employs graph-based modeling for regions, leveraging random walks for feature learning, alongside a cross-task learning module, HCondDiffCT.

Result: UrbanVerse demonstrated up to 35.89% improvement in prediction accuracy across six tasks in cross-city settings.

Conclusion: UrbanVerse offers a foundational approach for urban analytics by effectively generalizing across cities and tasks.

Abstract: Recent advances in urban region representation learning have enabled a wide range of applications in urban analytics, yet existing methods remain limited in their capabilities to generalize across cities and analytic tasks. We aim to generalize urban representation learning beyond city- and task-specific settings, towards a foundation-style model for urban analytics. To this end, we propose UrbanVerse, a model for cross-city urban representation learning and cross-task urban analytics. For cross-city generalization, UrbanVerse focuses on features local to the target regions and structural features of the nearby regions rather than the entire city. We model regions as nodes on a graph, which enables a random walk-based procedure to form "sequences of regions" that reflect both local and neighborhood structural features for urban region representation learning. For cross-task generalization, we propose a cross-task learning module named HCondDiffCT. This module integrates region-conditioned prior knowledge and task-conditioned semantics into the diffusion process to jointly model multiple downstream urban prediction tasks. HCondDiffCT is generic. It can also be integrated with existing urban representation learning models to enhance their downstream task effectiveness. Experiments on real-world datasets show that UrbanVerse consistently outperforms state-of-the-art methods across six tasks under cross-city settings, achieving up to 35.89% improvements in prediction accuracy.

</details>


### [181] [Beyond Match Maximization and Fairness: Retention-Optimized Two-Sided Matching](https://arxiv.org/abs/2602.15752)
*Ren Kishimoto,Rikiya Takehi,Koichi Tanaka,Masahiro Nomura,Riku Togashi,Yoji Tomita,Yuta Saito*

Main category: cs.LG

TL;DR: The paper introduces a novel algorithm called Matching for Retention (MRet) to maximize user retention on two-sided matching platforms, addressing issues caused by traditional methods focusing solely on matches or fairness.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address retention challenges on two-sided platforms where maximizing matches creates user imbalance, leading to platform abandonment.

Method: It proposes MRet, a dynamic learning-to-rank algorithm, which learns personalized retention curves for users and adapts recommendations based on retention gains for both sides of the matching.

Result: MRet outperforms conventional methods in user retention based on evaluations on synthetic and real-world datasets from a major online dating platform.

Conclusion: Focusing on retention directly, rather than just maximizing matches or enforcing fairness, is critical for better user experience and platform sustainability.

Abstract: On two-sided matching platforms such as online dating and recruiting, recommendation algorithms often aim to maximize the total number of matches. However, this objective creates an imbalance, where some users receive far too many matches while many others receive very few and eventually abandon the platform. Retaining users is crucial for many platforms, such as those that depend heavily on subscriptions. Some may use fairness objectives to solve the problem of match maximization. However, fairness in itself is not the ultimate objective for many platforms, as users do not suddenly reward the platform simply because exposure is equalized. In practice, where user retention is often the ultimate goal, casually relying on fairness will leave the optimization of retention up to luck.
  In this work, instead of maximizing matches or axiomatically defining fairness, we formally define the new problem setting of maximizing user retention in two-sided matching platforms. To this end, we introduce a dynamic learning-to-rank (LTR) algorithm called Matching for Retention (MRet). Unlike conventional algorithms for two-sided matching, our approach models user retention by learning personalized retention curves from each user's profile and interaction history. Based on these curves, MRet dynamically adapts recommendations by jointly considering the retention gains of both the user receiving recommendations and those who are being recommended, so that limited matching opportunities can be allocated where they most improve overall retention. Naturally but importantly, empirical evaluations on synthetic and real-world datasets from a major online dating platform show that MRet achieves higher user retention, since conventional methods optimize matches or fairness rather than retention.

</details>


### [182] [The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety](https://arxiv.org/abs/2602.15799)
*Max Springer,Chung Peng Lee,Blossom Metevier,Jane Castleman,Bohdan Turbal,Hayoung Jung,Zeyu Shen,Aleksandra Korolova*

Main category: cs.LG

TL;DR: Fine-tuning aligned language models can unpredictably compromise safety measures due to geometric instability in high-dimensional parameter space. The paper provides a geometric explanation and highlights the need for curvature-aware methods.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to understand and address why fine-tuning aligned language models often degrades safety guardrails, even without harmful training data or adversarial intent.

Method: They use a novel geometric analysis to study the dynamics of fine-tuning and derive the Alignment Instability Condition, identifying structural weaknesses in alignment caused by sharp curvature and parameter coupling.

Result: The study reveals that alignment degradation is an intrinsic property of gradient descent on curved manifolds and establishes a quartic scaling law for alignment loss over training time.

Conclusion: Current safety mechanisms are insufficient as they overlook the dynamic nature of alignment fragility. The paper advocates for curvature-aware methodologies to better predict and mitigate safety risks in language model fine-tuning.

Abstract: Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation. Our main result establishes a quartic scaling law: alignment loss grows with the fourth power of training time, governed by the sharpness of alignment geometry and the strength of curvature coupling between the fine-tuning task and safety-critical parameters. These results expose a structural blind spot in the current safety paradigm. The dominant approaches to safe fine-tuning address only the initial snapshot of a fundamentally dynamic problem. Alignment fragility is not a bug to be patched; it is an intrinsic geometric property of gradient descent on curved manifolds. Our results motivate the development of curvature-aware methods, and we hope will further enable a shift in alignment safety analysis from reactive red-teaming to predictive diagnostics for open-weight model deployment.

</details>


### [183] [Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics](https://arxiv.org/abs/2602.15820)
*Anna Zimmel,Paul Setinek,Gianluca Galletti,Johannes Brandstetter,Werner Zellinger*

Main category: cs.LG

TL;DR: The paper introduces a Test-Time Adaptation (TTA) framework for improving simulation surrogates in high-dimensional and unstructured regression problems, achieving up to 7% performance improvement in out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning surrogates often fail under distribution shifts (e.g., unseen geometries or configurations). There is a need to address the performance degradation in high-dimensional and unstructured regression problems in simulation models.

Method: The authors propose a TTA framework using D-optimal statistics, which allow stable adaptation and principled test-time parameter selection for pretrained simulation surrogates.

Result: The method improves simulation surrogate performance by up to 7% under distribution shifts, validated against the SIMSHIFT and EngiBench benchmarks, with minimal computational cost.

Conclusion: The paper presents the first effective TTA method for high-dimensional regression and generative design optimization, demonstrating significant out-of-distribution improvements for simulation surrogates.

Abstract: Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.

</details>


### [184] [CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing](https://arxiv.org/abs/2602.15823)
*Zarif Ikram,Arad Firouzkouhi,Stephen Tu,Mahdi Soltanolkotabi,Paria Rashidinejad*

Main category: cs.LG

TL;DR: CrispEdit is a novel algorithm for editing Large Language Models (LLMs) that preserves their general capabilities by using second-order optimization and projecting edits onto low-curvature subspaces.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the issue of capability preservation during the editing of LLMs, as current methods often degrade the general capabilities of the model.

Method: CrispEdit employs constrained optimization with a Bregman divergence-based approach, projecting edits onto low-curvature subspaces using Kronecker-factored approximate curvature (K-FAC) and a matrix-free projector.

Result: CrispEdit achieves high success in targeted model edits while keeping capability degradation under 1% across multiple datasets, outperforming previous editing methods.

Conclusion: This method demonstrates a scalable and efficient approach to LLM editing that balances targeted behavioral changes with general capability preservation.

Abstract: A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.

</details>


### [185] [Operationalising the Superficial Alignment Hypothesis via Task Complexity](https://arxiv.org/abs/2602.15829)
*Tomás Vergara-Browne,Darshan Patil,Ivan Titov,Siva Reddy,Tiago Pimentel,Marius Mosbach*

Main category: cs.LG

TL;DR: The paper investigates the Superficial Alignment Hypothesis (SAH), suggesting pre-training in large language models dramatically reduces task complexity, with post-training simplifying task adaptation further.


<details>
  <summary>Details</summary>
Motivation: To define and better understand the Superficial Alignment Hypothesis (SAH) and to address its ambiguities, critiques, and arguments by introducing a precise quantifiable metric.

Method: A new metric, task complexity, is proposed, defined as the minimum program length required to achieve a desired task performance. This was estimated experimentally for tasks like mathematical reasoning, machine translation, and instruction following.

Result: The study found that pre-training significantly reduces task complexity for high performance and post-training collapses this complexity even further (several orders of magnitude). Adaptation to tasks often requires very little additional information.

Conclusion: Pre-training in language models drastically reduces complexity for many tasks, while post-training further simplifies adaptation, supporting the SAH and unifying arguments around it.

Abstract: The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model. Further, we find that pre-training enables access to strong performances on our tasks, but it can require programs of gigabytes of length to access them. Post-training, on the other hand, collapses the complexity of reaching this same performance by several orders of magnitude. Overall, our results highlight that task adaptation often requires surprisingly little information -- often just a few kilobytes.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [186] [A Quantum-inspired Hybrid Swarm Intelligence and Decision-Making for Multi-Criteria ADAS Calibration](https://arxiv.org/abs/2602.15043)
*Sanjai Pathak,Ashish Mani,Amlan Chatterjee*

Main category: cs.NE

TL;DR: The paper introduces Quantum-Inspired Hybrid Swarm Intelligence (QiHSI) as an effective multi-objective optimization method for ADAS calibration, showing superior performance over existing algorithms.


<details>
  <summary>Details</summary>
Motivation: The optimization of Advanced Driver Assistance Systems (ADAS) involves balancing complex trade-offs, necessitating advanced optimization methods to improve safety, responsiveness, and efficiency.

Method: The authors propose QiHSI, which incorporates quantum-inspired mechanisms into the salp swarm optimization framework, combined with adaptive expert guidance via a decision-maker-in-the-loop strategy.

Result: QiHSI outperforms state-of-the-art algorithms like MSSA, MOPSO, MOEA/D, SPEA2, NSGA-III, and RVEA by consistently providing better Pareto-optimal solutions with faster convergence and adaptability.

Conclusion: QiHSI is a scalable and reliable framework for intelligent ADAS calibration, enabling the development of safer, more efficient autonomous driving systems.

Abstract: The tuning of Advanced Driver Assistance Systems (ADAS) involves resolving trade-offs among several competing objectives, including operational safety, system responsiveness, energy usage, and passenger comfort. This work introduces a novel optimization framework based on Quantum-Inspired Hybrid Swarm Intelligence (QiHSI), in which quantum-inspired mechanisms are embedded within a multi-objective salp swarm optimization process to strengthen global search capability and preserve population diversity in complex, high-dimensional decision spaces. In addition, a decision-maker-in-the-loop strategy is incorporated to incorporate adaptive expert guidance, enabling the optimization process to respond dynamically to changing design priorities and system constraints. The effectiveness of QiHSI is assessed using established multi-objective benchmark problems as well as a practical ADAS calibration scenario. Experimental comparisons with several state-of-the-art evolutionary and swarm-based algorithms, including MSSA, MOPSO, MOEA/D, SPEA2, NSGA-III, and RVEA, show that the proposed method consistently produces well-distributed Pareto-optimal solutions with faster convergence and improved adaptability. These findings demonstrate that QiHSI offers a reliable and scalable approach for intelligent ADAS calibration, supporting the development of more responsive, efficient, and safety-oriented autonomous driving technologies.

</details>


### [187] [An effective Genetic Programming Hyper-Heuristic for Uncertain Agile Satellite Scheduling](https://arxiv.org/abs/2602.15070)
*Yuning Chen,Junhua Xue,Wangqi Gu,Mingyan Shao*

Main category: cs.NE

TL;DR: The paper introduces the Uncertain Agile Earth Observation Satellite Scheduling Problem (UAEOSSP) and proposes an effective Genetic Programming Hyper-Heuristic (GPHH) for better real-time scheduling under uncertainty.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of static scheduling models by introducing uncertainty factors in scheduling to create more realistic and effective solutions.

Method: The authors employ Genetic Programming Hyper-Heuristic (GPHH) to evolve scheduling policies that adapt to real-time uncertainties, outperforming traditional heuristics.

Result: The GPHH outperformed Look-Ahead Heuristics (LAHs) with a 5.03% average improvement and Manually Designed Heuristics (MDHs) with an 8.14% average improvement.

Conclusion: GPHH is an effective and adaptive solution for scheduling problems under uncertainty, surpassing traditional heuristic methods.

Abstract: This paper investigates a novel problem, namely the Uncertain Agile Earth Observation Satellite Scheduling Problem (UAEOSSP). Unlike the static AEOSSP, it takes into account a range of uncertain factors (e.g., task profit, resource consumption, and task visibility) in order to reflect the reality that the actual information is inherently unknown beforehand. An effective Genetic Programming Hyper-Heuristic (GPHH) is designed to automate the generation of scheduling policies. The evolved scheduling policies can be utilized to adjust plans in real time and perform exceptionally well. Experimental results demonstrate that evolved scheduling policies significantly outperform both well-designed Look-Ahead Heuristics (LAHs) and Manually Designed Heuristics (MDHs). Specifically, the policies generated by GPHH achieve an average improvement of 5.03% compared to LAHs and 8.14% compared to MDHs.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [188] [Decomposing Docker Container Startup Performance: A Three-Tier Measurement Study on Heterogeneous Infrastructure](https://arxiv.org/abs/2602.15214)
*Shamsher Khan*

Main category: cs.PF

TL;DR: The paper studies Docker container startup latency across three infrastructure setups, concluding that runtime overhead dominates latency more than image size or configuration, with findings on penalties from hardware choices and hypervisor layers.


<details>
  <summary>Details</summary>
Motivation: To understand how infrastructure choices affect container startup latency, aiding in optimizing environments for CI/CD pipelines, serverless computing, and auto-scaling systems.

Method: A systematic measurement approach using a comprehensive benchmarking suite that executes 50 iterations per test across 10 performance dimensions across three diverse infrastructure setups.

Result: Five key observations were made, like runtime overhead dominating startup time, 2.04x and 2.69x latency penalties due to storage and virtualization setups respectively, and minor contributions from Linux namespace creation.

Conclusion: Infrastructure choices have measurable effects on container startup, and the study provides quantifications to guide practitioners in improving startup performance. All related data and tools are open-source.

Abstract: Container startup latency is a critical performance metric for CI/CD pipelines, serverless computing, and auto-scaling systems, yet practitioners lack empirical guidance on how infrastructure choices affect this latency. We present a systematic measurement study that decomposes Docker container startup into constituent operations across three heterogeneous infrastructure tiers: Azure Premium SSD (cloud SSD), Azure Standard HDD (cloud HDD), and macOS Docker Desktop (developer workstation with hypervisor-based virtualization). Using a reproducible benchmark suite that executes 50 iterations per test across 10 performance dimensions, we quantify previously under-characterized relationships between infrastructure configuration and container runtime behavior. Our key findings include: (1) container startup is dominated by runtime overhead rather than image size, with only 2.5% startup variation across images ranging from 5 MB to 155 MB on SSD; (2) storage tier selection imposes a 2.04x startup penalty (HDD 1157 ms vs. SSD 568 ms); (3) Docker Desktop's hypervisor layer introduces a 2.69x startup penalty and 9.5x higher CPU throttling variance compared to native Linux; (4) OverlayFS write performance collapses by up to two orders of magnitude compared to volume mounts on SSD-backed storage; and (5) Linux namespace creation contributes only 8-10 ms (<1.5%) of total startup time. All measurement scripts, raw data, and analysis tools are publicly available.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [189] [Synthesizing Trajectory Queries from Examples](https://arxiv.org/abs/2602.15164)
*Stephen Mell,Favyen Bastani,Steve Zdancewic,Osbert Bastani*

Main category: cs.PL

TL;DR: Quivr is a framework designed to ease the writing of trajectory queries by synthesizing them using examples. It introduces efficient parameter synthesis techniques and showcases success on 17 tasks.


<details>
  <summary>Details</summary>
Motivation: Writing trajectory queries for ML applications is challenging due to the need to tune real-valued parameters manually, which is time-consuming and imprecise.

Method: The authors propose Quivr, incorporating a novel technique to prune parameter space and quantitative semantics to synthesize trajectory queries efficiently.

Result: Quivr was tested on 17 tasks, demonstrating its capability to generate accurate queries and improve synthesis time due to the proposed optimizations.

Conclusion: Quivr is an effective tool for automating trajectory query synthesis in ML tasks, saving time and improving precision through its novel design.

Abstract: Data scientists often need to write programs to process predictions of machine learning models, such as object detections and trajectories in video data. However, writing such queries can be challenging due to the fuzzy nature of real-world data; in particular, they often include real-valued parameters that must be tuned by hand. We propose a novel framework called Quivr that synthesizes trajectory queries matching a given set of examples. To efficiently synthesize parameters, we introduce a novel technique for pruning the parameter space and a novel quantitative semantics that makes this more efficient. We evaluate Quivr on a benchmark of 17 tasks, including several from prior work, and show both that it can synthesize accurate queries for each task and that our optimizations substantially reduce synthesis time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [190] [CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation](https://arxiv.org/abs/2602.15060)
*Tengjie Zhu,Guanyu Cai,Yang Zhaohui,Guanzhu Ren,Haohui Xie,ZiRui Wang,Junsong Wu,Jingbo Wang,Xiaokang Yang,Yao Mu,Yichao Yan,Yichao Yan*

Main category: cs.RO

TL;DR: The paper introduces CLOT, a real-time system addressing long-horizon humanoid teleoperation challenges by integrating localization feedback to eliminate global pose drift.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of accumulated global pose drift and instability in long-horizon humanoid teleoperation using full-sized robots.

Method: CLOT achieves drift-free operation through closed-loop synchronization, data-driven randomization for smooth corrections, and an adversarial motion prior to mitigate unnatural actions. It employs a transformer-based policy trained on 20 hours of human motion data over 1300 GPU hours.

Result: The system demonstrates highly dynamic, precise, and robust humanoid teleoperation in both simulation and real-world experiments.

Conclusion: The CLOT system significantly advances humanoid teleoperation by achieving drift-free, stable, and robust human-to-humanoid motion mimicry. Supported by publicly available data and code.

Abstract: Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.

</details>


### [191] [Safe-SDL:Establishing Safety Boundaries and Control Mechanisms for AI-Driven Self-Driving Laboratories](https://arxiv.org/abs/2602.15061)
*Zihan Zhang,Haohui Que,Junhan Chang,Xin Zhang,Hao Wei,Tong Zhu*

Main category: cs.RO

TL;DR: This paper introduces Safe-SDL, a safety framework for autonomous Self-Driving Laboratories, addressing critical safety challenges in AI-driven experimental systems and proposing practical safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure safety in Self-Driving Laboratories (SDLs), which integrate AI and robotics but pose unique safety challenges compared to traditional labs.

Method: The paper proposes a safety framework with three components: (1) Operational Design Domains (ODDs) for controlled behaviors, (2) Control Barrier Functions (CBFs) for real-time monitoring, and (3) a Transactional Safety Protocol (CRUTD) to align digital planning with physical actions.

Result: The authors show how these safety mechanisms are applied in existing systems like UniLabOS and Osprey, and demonstrate through LabSafety Bench evaluations that foundational models face safety gaps, validating the need for their safety architecture.

Conclusion: The paper provides theoretical and practical guidelines to safely deploy autonomous laboratories, laying a foundation for responsible advancements in AI-driven scientific research.

Abstract: The emergence of Self-Driving Laboratories (SDLs) transforms scientific discovery methodology by integrating AI with robotic automation to create closed-loop experimental systems capable of autonomous hypothesis generation, experimentation, and analysis. While promising to compress research timelines from years to weeks, their deployment introduces unprecedented safety challenges differing from traditional laboratories or purely digital AI. This paper presents Safe-SDL, a comprehensive framework for establishing robust safety boundaries and control mechanisms in AI-driven autonomous laboratories. We identify and analyze the critical ``Syntax-to-Safety Gap'' -- the disconnect between AI-generated syntactically correct commands and their physical safety implications -- as the central challenge in SDL deployment. Our framework addresses this gap through three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) that provide real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) that ensures atomic consistency between digital planning and physical execution. We ground our theoretical contributions through analysis of existing implementations including UniLabOS and the Osprey architecture, demonstrating how these systems instantiate key safety principles. Evaluation against the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. Our framework provides both theoretical foundations and practical implementation guidance for safe deployment of autonomous scientific systems, establishing the groundwork for responsible acceleration of AI-driven discovery.

</details>


### [192] [How Do We Research Human-Robot Interaction in the Age of Large Language Models? A Systematic Review](https://arxiv.org/abs/2602.15063)
*Yufeng Wang,Yuan Xu,Anastasia Nikolova,Yuxuan Wang,Jianyu Wang,Chongyang Wang,Xin Tong*

Main category: cs.RO

TL;DR: LLMs are reshaping HRI, impacting fundamental capabilities like sensing context and user interaction. Current research is exploratory with diverse methods and lacks cohesive guidelines.


<details>
  <summary>Details</summary>
Motivation: Investigating the transformative effect and challenges of LLMs on human-centered aspects of HRI.

Method: A systematic literature review adhering to PRISMA guidelines to analyze 86 relevant articles.

Result: LLMs enhance HRI by improving contextual sensing, social interaction, and need alignment. Challenges include varied methods, setups, and metrics across studies.

Conclusion: LLMs are crucial for the evolution of HRI, but consistent design considerations and unified evaluations are needed for progress.

Abstract: Advances in large language models (LLMs) are profoundly reshaping the field of human-robot interaction (HRI). While prior work has highlighted the technical potential of LLMs, few studies have systematically examined their human-centered impact (e.g., human-oriented understanding, user modeling, and levels of autonomy), making it difficult to consolidate emerging challenges in LLM-driven HRI systems. Therefore, we conducted a systematic literature search following the PRISMA guideline, identifying 86 articles that met our inclusion criteria. Our findings reveal that: (1) LLMs are transforming the fundamentals of HRI by reshaping how robots sense context, generate socially grounded interactions, and maintain continuous alignment with human needs in embodied settings; and (2) current research is largely exploratory, with different studies focusing on different facets of LLM-driven HRI, resulting in wide-ranging choices of experimental setups, study methods, and evaluation metrics. Finally, we identify key design considerations and challenges, offering a coherent overview and guidelines for future research at the intersection of LLMs and HRI.

</details>


### [193] [Augmenting Human Balance with Generic Supernumerary Robotic Limbs](https://arxiv.org/abs/2602.15092)
*Xuanyun Qiu,Dorian Verdel,Hector Cervantes-Culebro,Alexis Devillard,Etienne Burdet*

Main category: cs.RO

TL;DR: The paper presents a framework to ensure balance in human-supernumerary robotic limbs (SLs) systems, demonstrating its efficacy in enhancing balance during motion tasks.


<details>
  <summary>Details</summary>
Motivation: To address safety and usability challenges by maintaining balance in systems with human and supernumerary limbs.

Method: A three-layer architecture is developed: (1) Prediction of trunk and center of mass (CoM) dynamics, (2) Planning of optimal CoM trajectories, and (3) Execution of control inputs to SL hardware.

Result: Evaluation with ten participants showed reduced instability during forward and lateral bending tasks.

Conclusion: The proposed framework enhances balance, advancing safe and versatile interactions with SLs.

Abstract: Supernumerary robotic limbs (SLs) have the potential to transform a wide range of human activities, yet their usability remains limited by key technical challenges, particularly in ensuring safety and achieving versatile control. Here, we address the critical problem of maintaining balance in the human-SLs system, a prerequisite for safe and comfortable augmentation tasks. Unlike previous approaches that developed SLs specifically for stability support, we propose a general framework for preserving balance with SLs designed for generic use. Our hierarchical three-layer architecture consists of: (i) a prediction layer that estimates human trunk and center of mass (CoM) dynamics, (ii) a planning layer that generates optimal CoM trajectories to counteract trunk movements and computes the corresponding SL control inputs, and (iii) a control layer that executes these inputs on the SL hardware. We evaluated the framework with ten participants performing forward and lateral bending tasks. The results show a clear reduction in stance instability, demonstrating the framework's effectiveness in enhancing balance. This work paves the path towards safe and versatile human-SLs interactions. [This paper has been submitted for publication to IEEE.]

</details>


### [194] [A ROS2 Benchmarking Framework for Hierarchical Control Strategies in Mobile Robots for Mediterranean Greenhouses](https://arxiv.org/abs/2602.15162)
*Fernando Cañadas-Aránega,Francisco J. Mañas-Álvarez,José L- Guzmán,José C. Moreno,José L. Blanco-Claraco*

Main category: cs.RO

TL;DR: This paper introduces a benchmarking framework for mobile robot controllers in greenhouse environments, addressing performance challenges caused by uneven terrain, variable conditions, and lack of standardized tests.


<details>
  <summary>Details</summary>
Motivation: To provide a standardized framework for evaluating mobile robot controllers in challenging agroindustrial settings, like Mediterranean greenhouses, which suffer from uneven terrain, payload changes, and a lack of reproducible testing conditions.

Method: A comprehensive framework integrates a 3D environmental model, a physics-based simulator, and a hierarchical control structure with benchmarks for different control levels, disturbance scenarios, and standardized metrics.

Result: The framework ensures objective, reproducible evaluations with new metrics (SAE, SCI) and statistical analyses to compare control strategies under realistic agricultural conditions.

Conclusion: The tool bridges the gap between simulation and real-world applications, enhancing the ability to systematically compare classical and advanced control strategies for mobile agricultural robots.

Abstract: Mobile robots operating in agroindustrial environments, such as Mediterranean greenhouses, are subject to challenging conditions, including uneven terrain, variable friction, payload changes, and terrain slopes, all of which significantly affect control performance and stability. Despite the increasing adoption of robotic platforms in agriculture, the lack of standardized, reproducible benchmarks impedes fair comparisons and systematic evaluations of control strategies under realistic operating conditions. This paper presents a comprehensive benchmarking framework for evaluating mobile robot controllers in greenhouse environments. The proposed framework integrates an accurate three dimensional model of the environment, a physics based simulator, and a hierarchical control architecture comprising low, mid, and high level control layers. Three benchmark categories are defined to enable modular assessment, ranging from actuator level control to full autonomous navigation. Additionally, three disturbance scenarios payload variation, terrain type, and slope are explicitly modeled to replicate real world agricultural conditions. To ensure objective and reproducible evaluation, standardized performance metrics are introduced, including the Squared Absolute Error (SAE), the Squared Control Input (SCI), and composite performance indices. Statistical analysis based on repeated trials is employed to mitigate the influence of sensor noise and environmental variability. The framework is further enhanced by a plugin based architecture that facilitates seamless integration of user defined controllers and planners. The proposed benchmark provides a robust and extensible tool for the quantitative comparison of classical, predictive, and planning based control strategies in realistic conditions, bridging the gap between simulation based analysis and real world agroindustrial applications.

</details>


### [195] [DexEvolve: Evolutionary Optimization for Robust and Diverse Dexterous Grasp Synthesis](https://arxiv.org/abs/2602.15201)
*René Zurbrügg,Andrei Cramariuc,Marco Hutter*

Main category: cs.RO

TL;DR: The paper presents a scalable pipeline for synthesizing diverse, stable robotic grasps, leveraging simulation refinement and diffusion models for real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Current data-driven grasp prediction depends on large datasets with limited gripper morphologies, and analytical synthesis produces infeasible grasps that require extensive filtering in simulators, limiting efficiency.

Method: A generate-and-refine pipeline initializes grasps with analytical synthesis and refines them in a high-fidelity simulator using an evolutionary algorithm to improve quality and diversity while being adaptable to human preferences or domain metrics.

Result: The approach achieves over 120 stable grasps per object (1.7-6x improvement) with a 46-60% improvement in unique grasp coverage compared to diffusion-based alternatives.

Conclusion: The pipeline enhances the grasp synthesis process by combining analytical generation, simulation-based refinement, and diffusion models for real-world performance while emphasizing diversity and stability of grasps.

Abstract: Dexterous grasping is fundamental to robotics, yet data-driven grasp prediction heavily relies on large, diverse datasets that are costly to generate and typically limited to a narrow set of gripper morphologies. Analytical grasp synthesis can be used to scale data collection, but necessary simplifying assumptions often yield physically infeasible grasps that need to be filtered in high-fidelity simulators, significantly reducing the total number of grasps and their diversity.
  We propose a scalable generate-and-refine pipeline for synthesizing large-scale, diverse, and physically feasible grasps. Instead of using high-fidelity simulators solely for verification and filtering, we leverage them as an optimization stage that continuously improves grasp quality without discarding precomputed candidates. More specifically, we initialize an evolutionary search with a seed set of analytically generated, potentially suboptimal grasps. We then refine these proposals directly in a high-fidelity simulator (Isaac Sim) using an asynchronous, gradient-free evolutionary algorithm, improving stability while maintaining diversity. In addition, this refinement stage can be guided toward human preferences and/or domain-specific quality metrics without requiring a differentiable objective. We further distill the refined grasp distribution into a diffusion model for robust real-world deployment, and highlight the role of diversity for both effective training and during deployment. Experiments on a newly introduced Handles dataset and a DexGraspNet subset demonstrate that our approach achieves over 120 distinct stable grasps per object (a 1.7-6x improvement over unrefined analytical methods) while outperforming diffusion-based alternatives by 46-60\% in unique grasp coverage.

</details>


### [196] [SEG-JPEG: Simple Visual Semantic Communications for Remote Operation of Automated Vehicles over Unreliable Wireless Networks](https://arxiv.org/abs/2602.15258)
*Sebastian Donnelly,Ruth Anderson,George Economides,James Broughton,Peter Ball,Alexander Rast,Andrew Bradley*

Main category: cs.RO

TL;DR: The paper proposes an innovative method of using computer vision-assisted semantic communication to improve remote operation of automated vehicles over limited public network infrastructure.


<details>
  <summary>Details</summary>
Motivation: The need for reliable and efficient communication networks to enable remote operation of automated vehicles, particularly in environments with constrained public network infrastructure.

Method: The study encodes detected road users into color-coded highlights within low-resolution greyscale imagery using computer vision techniques, reducing data rate requirements by 50% compared to traditional image compression approaches.

Result: This method facilitated a median glass-to-glass latency below 200ms at data rates under 500kbit/s, maintaining clear situational awareness for remote operators and improving system reliability in variable 4G conditions.

Conclusion: The proposed method demonstrates the potential for large-scale deployment of remotely operated automated vehicles even with constrained public networks, thereby accelerating the adoption of automated vehicle technology.

Abstract: Remote Operation is touted as being key to the rapid deployment of automated vehicles. Streaming imagery to control connected vehicles remotely currently requires a reliable, high throughput network connection, which can be limited in real-world remote operation deployments relying on public network infrastructure. This paper investigates how the application of computer vision assisted semantic communication can be used to circumvent data loss and corruption associated with traditional image compression techniques. By encoding the segmentations of detected road users into colour coded highlights within low resolution greyscale imagery, the required data rate can be reduced by 50 \% compared with conventional techniques, while maintaining visual clarity. This enables a median glass-to-glass latency of below 200ms even when the network data rate is below 500kbit/s, while clearly outlining salient road users to enhance situational awareness of the remote operator. The approach is demonstrated in an area of variable 4G mobile connectivity using an automated last-mile delivery vehicle. With this technique, the results indicate that large-scale deployment of remotely operated automated vehicles could be possible even on the often constrained public 4G/5G mobile network, providing the potential to expedite the nationwide roll-out of automated vehicles.

</details>


### [197] [OSCAR: An Ovipositor-Inspired Self-Propelling Capsule Robot for Colonoscopy](https://arxiv.org/abs/2602.15309)
*Mostafa A. Atalla,Anand S. Sekar,Remi van Starkenburg,David J. Jager,Aimée Sakes,Michaël Wiertlewski,Paul Breedveld*

Main category: cs.RO

TL;DR: OSCAR, a self-propelling robotic capsule designed for colonoscopy, uses a parasitic wasp-inspired propulsion mechanism to navigate the slippery, viscoelastic colon environment.


<details>
  <summary>Details</summary>
Motivation: To develop a robotic capsule for colonoscopy that eliminates patient discomfort caused by shaft looping in traditional methods and effectively navigates the complex colon environment.

Method: Inspired by parasitic wasps, OSCAR employs a spring-loaded cam system to achieve coordinated motion. This motion creates controlled friction anisotropy to generate thrust. The propulsion mechanism was modeled using analytical tools and tested through extensive experimentation in a colon-mimicking environment.

Result: OSCAR achieved a mean steady-state traction force of 0.85 N, validated predictive models, and demonstrated speed and thrust consistency. Locomotion tests revealed an average speed of 3.08 mm/s, comparable to traditional colonoscopy cecal intubation times.

Conclusion: The robot’s controllable thrust mechanism and predictive performance model address key challenges in colonoscopy navigation, offering a safer, efficient alternative to conventional methods.

Abstract: Self-propelling robotic capsules eliminate shaft looping of conventional colonoscopy, reducing patient discomfort. However, reliably moving within the slippery, viscoelastic environment of the colon remains a significant challenge. We present OSCAR, an ovipositor-inspired self-propelling capsule robot that translates the transport strategy of parasitic wasps into a propulsion mechanism for colonoscopy. OSCAR mechanically encodes the ovipositor-inspired motion pattern through a spring-loaded cam system that drives twelve circumferential sliders in a coordinated, phase-shifted sequence. By tuning the motion profile to maximize the retract phase relative to the advance phase, the capsule creates a controlled friction anisotropy at the interface that generates net forward thrust. We developed an analytical model incorporating a Kelvin-Voigt formulation to capture the viscoelastic stick--slip interactions between the sliders and the tissue, linking the asymmetry between advance and retract phase durations to mean thrust, and slider-reversal synchronization to thrust stability. Comprehensive force characterization experiments in ex-vivo porcine colon revealed a mean steady-state traction force of 0.85 N, closely matching the model. Furthermore, experiments confirmed that thrust generation is speed-independent and scales linearly with the phase asymmetry, in agreement with theoretical predictions, underscoring the capsule's predictable performance and scalability. In locomotion validation experiments, OSCAR demonstrated robust performance, achieving an average speed of 3.08 mm/s, a velocity sufficient to match the cecal intubation times of conventional colonoscopy. By coupling phase-encoded friction anisotropy with a predictive model, OSCAR delivers controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy.

</details>


### [198] [Feasibility-aware Imitation Learning from Observation with Multimodal Feedback](https://arxiv.org/abs/2602.15351)
*Kei Takahashi,Hikaru Sasaki,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: FABCO improves imitation learning for robots by aligning demonstrator motions with robot feasibility via a robot-dynamics model and feedback, enhancing policy stability and learning performance by 3.2x.


<details>
  <summary>Details</summary>
Motivation: To address limitations in imitation learning caused by mismatches between human demonstrators and robot dynamics, which hinder effective policy learning.

Method: Propose Feasibility-Aware Behavior Cloning from Observation (FABCO) that integrates robot feasibility estimation via a robot-dynamics model, multimodal feedback for demonstrators, and feasibility-aware policy learning.

Result: FABCO demonstrated a 3.2-fold improvement in imitation learning performance in experiments with 15 participants across two tasks.

Conclusion: FABCO effectively aligns human demonstrations with robot-dynamics constraints, improving learning outcomes and enabling robots to execute policies more robustly.

Abstract: Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.

</details>


### [199] [A Comparison of Bayesian Prediction Techniques for Mobile Robot Trajectory Tracking](https://arxiv.org/abs/2602.15354)
*Jose Luis Peralta-Cabezas,Miguel Torres-Torriti,Marcelo Guarini-Hermann*

Main category: cs.RO

TL;DR: The paper compares estimation and prediction techniques for tracking multiple robots based on error magnitude, computational effort, and noise robustness.


<details>
  <summary>Details</summary>
Motivation: To identify the most effective techniques for tracking multiple robots under varying conditions, including non-Gaussian noise.

Method: Performance comparison of various estimation and prediction techniques, including Kalman filters, particle filters, and Gaussian Mixture Sigma Point Particle Filter.

Result: The paper evaluates the strengths and weaknesses of these techniques in terms of estimation accuracy, computational efficiency, and noise-handling capabilities.

Conclusion: Each technique has trade-offs, with some being better for accuracy and robustness while others excel in computational effort, suggesting context-specific application.

Abstract: This paper presents a performance comparison of different estimation and prediction techniques applied to the problem of tracking multiple robots. The main performance criteria are the magnitude of the estimation or prediction error, the computational effort and the robustness of each method to non-Gaussian noise. Among the different techniques compared are the well known Kalman filters and their different variants (e.g. extended and unscented), and the more recent techniques relying on Sequential Monte Carlo Sampling methods, such as particle filters and Gaussian Mixture Sigma Point Particle Filter.

</details>


### [200] [Fluoroscopy-Constrained Magnetic Robot Control via Zernike-Based Field Modeling and Nonlinear MPC](https://arxiv.org/abs/2602.15357)
*Xinhao Chen,Hongkun Yao,Anuruddha Bhattacharjee,Suraj Raval,Lamar O. Mair,Yancy Diaz-Mercado,Axel Krieger*

Main category: cs.RO

TL;DR: The paper proposes a control framework for magnetic actuation in surgical robots that addresses challenges like low frame rates and noise in fluoroscopic imaging, demonstrating high accuracy and safety in experimental tests.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately and stably controlling magnetically actuated surgical robots under clinical fluoroscopic imaging, which provides low frame rates and noisy feedback.

Method: The proposed framework integrates a nonlinear model predictive control (NMPC) model, an analytically differentiable magnetic field model using Zernike polynomials, and a Kalman filter for state estimation.

Result: Experiments with 3D-printed fluid workspaces and a spine phantom show the method achieves high accuracy under downsampled feedback (3 Hz) with noise, achieving an RMS error of 1.18 mm in spine drug delivery tasks.

Conclusion: This framework effectively enables precise navigation and safe operation for magnetic surgical robots, even under challenging imaging conditions, with significant potential for clinical applications.

Abstract: Magnetic actuation enables surgical robots to navigate complex anatomical pathways while reducing tissue trauma and improving surgical precision. However, clinical deployment is limited by the challenges of controlling such systems under fluoroscopic imaging, which provides low frame rate and noisy pose feedback. This paper presents a control framework that remains accurate and stable under such conditions by combining a nonlinear model predictive control (NMPC) framework that directly outputs coil currents, an analytically differentiable magnetic field model based on Zernike polynomials, and a Kalman filter to estimate the robot state. Experimental validation is conducted with two magnetic robots in a 3D-printed fluid workspace and a spine phantom replicating drug delivery in the epidural space. Results show the proposed control method remains highly accurate when feedback is downsampled to 3 Hz with added Gaussian noise (sigma = 2 mm), mimicking clinical fluoroscopy. In the spine phantom experiments, the proposed method successfully executed a drug delivery trajectory with a root mean square (RMS) position error of 1.18 mm while maintaining safe clearance from critical anatomical boundaries.

</details>


### [201] [ActionCodec: What Makes for Good Action Tokenizers](https://arxiv.org/abs/2602.15397)
*Zibin Dong,Yicheng Liu,Shiduo Zhang,Baijun Ye,Yifu Yuan,Fei Ni,Jingjing Gong,Xipeng Qiu,Hang Zhao,Yinchuan Li,Jianye Hao*

Main category: cs.RO

TL;DR: This paper introduces 'ActionCodec,' a high-performance action tokenizer for Vision-Language-Action (VLA) models, guided by design principles improving training and performance efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding the optimal design of action tokenizers for Vision-Language-Action (VLA) optimization, moving beyond focusing solely on reconstruction fidelity.

Method: The authors proposed a set of design principles grounded in information theory and introduced ActionCodec, an action tokenizer developed with these principles to enhance efficiency and performance.

Result: ActionCodec significantly improved training efficiency and VLA model performance. Notably, it enabled a SmolVLM2-2.2B model to set a new state-of-the-art (SOTA) success rate (95.5%, with improvements up to 97.4%) on the LIBERO benchmark without robotics pre-training.

Conclusion: The proposed design principles and ActionCodec establish a roadmap for developing effective action tokenizers, achieving new SOTA benchmarks and improving the efficiency of VLA models.

Abstract: Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.

</details>


### [202] [Hybrid F' and ROS2 Architecture for Vision-Based Autonomous Flight: Design and Experimental Validation](https://arxiv.org/abs/2602.15398)
*Abdelrahman Metwally,Monijesu James,Aleksey Fedoseev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou,Andrey Somov*

Main category: cs.RO

TL;DR: The paper presents a hybrid flight software architecture integrating NASA's F' framework with ROS2 for autonomous aerial systems, validated through a successful quadrotor flight test.


<details>
  <summary>Details</summary>
Motivation: To develop an integrated architecture that balances deterministic real-time control with advanced perception capabilities for autonomous aerospace systems.

Method: The system integrates NASA's F' flight software with ROS2 middleware using Protocol Buffers bridging, tested through an indoor quadrotor flight emphasizing vision-based navigation.

Result: Achieved a position estimation rate of 87.19 Hz with 99.90% data continuity, 11.47 ms latency, 100% command execution success, and efficient resource utilization (15.19% CPU, 1,244 MB RAM).

Conclusion: The results demonstrate the feasibility of hybrid architectures that merge certification-grade determinism with flexible autonomy for aerial vehicles, showing effective real-time performance and robustness.

Abstract: Autonomous aerospace systems require architectures that balance deterministic real-time control with advanced perception capabilities. This paper presents an integrated system combining NASA's F' flight software framework with ROS2 middleware via Protocol Buffers bridging. We evaluate the architecture through a 32.25-minute indoor quadrotor flight test using vision-based navigation. The vision system achieved 87.19 Hz position estimation with 99.90\% data continuity and 11.47 ms mean latency, validating real-time performance requirements. All 15 ground commands executed successfully with 100 % success rate, demonstrating robust F'--PX4 integration. System resource utilization remained low (15.19 % CPU, 1,244 MB RAM) with zero stale telemetry messages, confirming efficient operation on embedded platforms. Results validate the feasibility of hybrid flight-software architectures combining certification-grade determinism with flexible autonomy for autonomous aerial vehicles.

</details>


### [203] [One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation](https://arxiv.org/abs/2602.15400)
*Zerui Li,Hongpei Zheng,Fangguo Zhao,Aidan Chan,Jian Zhou,Sihao Lin,Shijie Li,Qi Wu*

Main category: cs.RO

TL;DR: The paper proposes a navigation agent design with decoupling of high-level semantic planning from low-level spatial estimation, introducing a metric world representation and counterfactual reasoning, achieving state-of-the-art zero-shot results in benchmarks and validating sim-to-real transferability.


<details>
  <summary>Details</summary>
Motivation: To address limitations in tightly coupled systems for navigation agents and improve generalization and real-world applicability by leveraging Multimodal Large Language Models (MLLMs).

Method: The authors propose a decoupled navigational framework separating spatial state estimation from semantic planning. They introduce an interactive metric world representation for richer information and counterfactual reasoning to enhance decision-making and validity of actions.

Result: The method achieves state-of-the-art zero-shot success rates in benchmarks (48.8% in R2R-CE and 42.2% in RxR-CE) and demonstrates versatile zero-shot sim-to-real transfer across different embodiments.

Conclusion: The decoupled framework with a metric world representation ensures a robust and domain-invariant interface for effective Vision-and-Language Navigation in both simulated and real-world scenarios.

Abstract: A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\% Success Rate (SR) in R2R-CE and 42.2\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.

</details>


### [204] [Lyapunov-Based $\mathcal{L}_2$-Stable PI-Like Control of a Four-Wheel Independently Driven and Steered Robot](https://arxiv.org/abs/2602.15424)
*Branimir Ćaran,Vladimir Milić,Bojan Jerbić*

Main category: cs.RO

TL;DR: The paper introduces a PI-like control method for motion control of a four-wheel mobile robot, ensuring real-time stability and performance.


<details>
  <summary>Details</summary>
Motivation: To design a controller with stability and performance guarantees for real-time operation of a four-wheel mobile robot.

Method: A Lyapunov-based synthesis of a PI-like controller is used, leveraging explicit bounds and stability results derived from a verified model.

Result: The proposed control law ensures $al{L}_2$ stability and robustness in reducing configuration-based effects, demonstrated experimentally on a real robot.

Conclusion: The synthesis enables rigorous stability for PI-like controllers suitable for embedded systems in mobile robots.

Abstract: In this letter, Lyapunov-based synthesis of a PI-like controller is proposed for $\mathcal{L}_2$-stable motion control of an independently driven and steered four-wheel mobile robot. An explicit, structurally verified model is used to enable systematic controller design with stability and performance guarantees suitable for real-time operation. A Lyapunov function is constructed to yield explicit bounds and $\mathcal{L}_2$ stability results, supporting feedback synthesis that reduces configuration dependent effects. The resulting control law maintains a PI-like form suitable for standard embedded implementation while preserving rigorous stability properties. Effectiveness and robustness are demonstrated experimentally on a real four-wheel mobile robot platform.

</details>


### [205] [Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling](https://arxiv.org/abs/2602.15513)
*Ji Li,Jing Xia,Mingyi Li,Shiyan Hu*

Main category: cs.RO

TL;DR: This paper proposes a novel memory framework for embodied agents, using a retrieval-first, reasoning-assisted paradigm to enhance performance in long-horizon tasks and boost generalization.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in deploying multimodal large language models as the core of embodied agents, particularly under limited observation contexts, long exploration horizons, and the limitations of existing memory methods that lose crucial visual and spatial details.

Method: The method involves a non-parametric memory framework that disentangles episodic and semantic memory. Episodic memory is improved using semantic similarity and visual reasoning, while semantic memory is structured through a rule extraction mechanism for reusability and cross-environment generalization.

Result: Experiments achieved state-of-the-art results on embodied question answering and exploration benchmarks, showing significant performance gains (+7.3%, +11.4% on A-EQA, +7.7% success rate, and +6.8% SPL on GOAT-Bench).

Conclusion: The framework enhances embodied agents by improving exploration efficiency via episodic memory and strengthening complex reasoning through semantic memory.

Abstract: Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.

</details>


### [206] [Efficient Knowledge Transfer for Jump-Starting Control Policy Learning of Multirotors through Physics-Aware Neural Architectures](https://arxiv.org/abs/2602.15533)
*Welf Rehberg,Mihir Kulkarni,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: This paper proposes a method to accelerate policy training for multirotor robots by reusing policies from a library.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of efficiently training control policies for robots by enabling cross-embodiment knowledge transfer, particularly for multirotor configurations.

Method: The method involves a physics-aware neural control architecture combining reinforcement learning and supervised control allocation, along with a policy evaluation-based similarity measure to select policies for library-based initialization.

Result: The initialization scheme reduced environment interactions by up to 73.5% while maintaining state-of-the-art control performance in simulations and real experiments with quadrotor and hexarotor designs.

Conclusion: This approach enables efficient cross-embodiment reinforcement learning and significantly reduces the training burden for multirotor robot control.

Abstract: Efficiently training control policies for robots is a major challenge that can greatly benefit from utilizing knowledge gained from training similar systems through cross-embodiment knowledge transfer. In this work, we focus on accelerating policy training using a library-based initialization scheme that enables effective knowledge transfer across multirotor configurations. By leveraging a physics-aware neural control architecture that combines a reinforcement learning-based controller and a supervised control allocation network, we enable the reuse of previously trained policies. To this end, we utilize a policy evaluation-based similarity measure that identifies suitable policies for initialization from a library. We demonstrate that this measure correlates with the reduction in environment interactions needed to reach target performance and is therefore suited for initialization. Extensive simulation and real-world experiments confirm that our control architecture achieves state-of-the-art control performance, and that our initialization scheme saves on average up to $73.5\%$ of environment interactions (compared to training a policy from scratch) across diverse quadrotor and hexarotor designs, paving the way for efficient cross-embodiment transfer in reinforcement learning.

</details>


### [207] [Selective Perception for Robot: Task-Aware Attention in Multimodal VLA](https://arxiv.org/abs/2602.15543)
*Young-Chae Son,Jung-Woo Lee,Yoon-Ji Choi,Dae-Kwan Ko,Soo-Chul Lim*

Main category: cs.RO

TL;DR: The paper introduces a dynamic information fusion framework for Vision-Language-Action (VLA) models, improving computational efficiency and robustness in real-time robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Most prior VLA models use static fusion, which processes visual inputs uniformly leading to inefficiency and potential noise from irrelevant background information.

Method: A lightweight adaptive routing architecture predicts task-relevance in real-time based on text prompts and wrist camera observations, selectively processing visual inputs to enhance computational efficiency.

Result: Experimental results show increased inference efficiency and control performance in robotic manipulation compared to existing VLA models.

Conclusion: Dynamic information fusion is effective and practical for resource-constrained, real-time robot control, proving its benefits in computation and task performance.

Abstract: In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.

</details>


### [208] [VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing](https://arxiv.org/abs/2602.15549)
*Guoqin Tang,Qingxuan Jia,Gang Chen,Tong Li,Zeyuan Huang,Zihang Lv,Ning Ji*

Main category: cs.RO

TL;DR: VLM-DEWM integrates a persistent and queryable model to address challenges of stateless operation and opaque reasoning in VLMs for robotic operations.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of vision-language models (VLMs) in dynamic workcells by enabling persistent state tracking and explainable decision-making.

Method: Proposes VLM-DEWM, which uses a Dynamic External World Model (DEWM) to manage world states and create Externalizable Reasoning Traces (ERT) validated against DEWM.

Result: Improved state-tracking accuracy from 56% to 93%, increased recovery success rate from below 5% to 95%, and reduced computational overhead with structured memory.

Conclusion: VLM-DEWM offers a resilient and verifiable cognitive architecture for long-horizon robotic tasks in manufacturing environments.

Abstract: Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.

</details>


### [209] [Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions](https://arxiv.org/abs/2602.15567)
*Jieting Long,Dechuan Liu,Weidong Cai,Ian Manchester,Weiming Zhi*

Main category: cs.RO

TL;DR: This paper introduces Constraint-Aware Streaming Flow (CASF), a framework for generating robot trajectories that adapt in real-time to constraints, ensuring safety while maintaining smooth, reactive controls.


<details>
  <summary>Details</summary>
Motivation: Existing methods for robot motion distribution lack mechanisms for adapting trajectories in real-time to enforce safety and task-specific constraints.

Method: The authors propose CASF which incorporates constraint-dependent metrics to reshape the robot's velocity field during execution using differentiable distance functions that are adapted to the robot's control space, enabling avoidance of restricted regions while preserving motion quality.

Result: The framework generates constraint-satisfying trajectories that remain smooth and dynamically consistent, outperforming standard post-hoc projection methods in simulations and real-world manipulations.

Conclusion: CASF effectively allows robots to generate safe, consistent, and feasible motion trajectories in real-time while addressing constraints, demonstrating superiority over other baseline methods.

Abstract: Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.

</details>


### [210] [Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion](https://arxiv.org/abs/2602.15608)
*Mostafa A. Atalla,Daan van Bemmel,Jack Cummings,Paul Breedveld,Michaël Wiertlewski,Aimée Sakes*

Main category: cs.RO

TL;DR: The study presents ultrasonic lubrication to actively control friction for robotic locomotion, achieving efficient bidirectional locomotion in bio-inspired systems.


<details>
  <summary>Details</summary>
Motivation: Friction is fundamental to terrestrial locomotion, but in robotics, it is typically treated passively. The study aims to innovate active friction control to enhance robotic locomotion.

Method: The team developed two friction control modules (cylindrical and flat-plate designs) using ultrasonic frequencies and implemented bio-inspired systems modeled after inchworm and wasp ovipositor locomotion.

Result: Both systems demonstrated bidirectional locomotion with efficiency exceeding 90% and showed significant friction reduction across diverse surfaces under various conditions.

Conclusion: Ultrasonic lubrication is a viable friction control method for robotic systems, capable of reducing design complexity and enhancing locomotion efficiency.

Abstract: Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between "grip" and "slip" states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.

</details>


### [211] [SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms](https://arxiv.org/abs/2602.15633)
*Haichao Liu,Yufeng Hu,Shuang Wang,Kangjun Guo,Jun Ma,Jinni Zhou*

Main category: cs.RO

TL;DR: SpecFuse improves UAV landing on oscillating marine platforms by integrating spectral-temporal fusion for precise motion forecasting, achieving high accuracy and robustness in simulations and experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of autonomous UAV landing on moving marine platforms constrained by wave-induced oscillations, wind disturbances, and prediction phase lags.

Method: The paper introduces SpecFuse — integrating frequency-domain wave decomposition with time-domain recursive state estimation for motion forecasting. It also deploys a hierarchical control system combining HPO-RRT* dynamic trajectory planning and a learning-augmented predictive controller.

Result: Extensive validations demonstrated 44%-48% improvement in prediction accuracy, achieving high success rates (98.7% in simulation and 87.5% in real-world tests) with minimal prediction error (3.2 cm) and landing deviation (4.46 cm).

Conclusion: SpecFuse effectively enhances UAV landing precision and robustness under dynamic sea conditions, supporting critical maritime missions like rescue and monitoring. It outperforms existing methods and will be openly shared for reproducibility.

Abstract: Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.

</details>


### [212] [Spatially-Aware Adaptive Trajectory Optimization with Controller-Guided Feedback for Autonomous Racing](https://arxiv.org/abs/2602.15642)
*Alexander Wachter,Alexander Willert,Marc-Philip Ecker,Christian Hartl-Nesic*

Main category: cs.RO

TL;DR: The paper proposes a closed-loop framework for raceline optimization using NURBS trajectory representation, trajectory optimization, and adaptive spatial feedback.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the performance of autonomous raceline optimization under varying track and vehicle conditions without explicitly parametrizing certain constraints like friction.

Method: The framework integrates NURBS-based trajectory representation, CMA-ES global optimization, and Kalman-inspired spatial updates to refine raceline trajectories adaptively.

Result: Simulation achieves a 17.38% lap time reduction with improved accuracy, and real hardware testing shows a 7.60% improvement across different tire compounds.

Conclusion: The method is robust in adapting to spatially varying conditions and performs well without explicit friction parameters, showing promise for real-world applications.

Abstract: We present a closed-loop framework for autonomous raceline optimization that combines NURBS-based trajectory representation, CMA-ES global trajectory optimization, and controller-guided spatial feedback. Instead of treating tracking errors as transient disturbances, our method exploits them as informative signals of local track characteristics via a Kalman-inspired spatial update. This enables the construction of an adaptive, acceleration-based constraint map that iteratively refines trajectories toward near-optimal performance under spatially varying track and vehicle behavior. In simulation, our approach achieves a 17.38% lap time reduction compared to a controller parametrized with maximum static acceleration. On real hardware, tested with different tire compounds ranging from high to low friction, we obtain a 7.60% lap time improvement without explicitly parametrizing friction. This demonstrates robustness to changing grip conditions in real-world scenarios.

</details>


### [213] [Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models](https://arxiv.org/abs/2602.15684)
*Feras Kiki,Pouya P. Niaz,Alireza Madani,Cagatay Basdogan*

Main category: cs.RO

TL;DR: This paper introduces a machine-learning-based framework using sEMG data to estimate human muscle fatigue in dynamic physical human-robot interactions. Several models, including CNNs and traditional methods, are compared with promising results.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to optimize human operator safety and performance in physical human-robot interaction scenarios by developing a reliable method to monitor muscle fatigue.

Method: The paper uses sEMG data to predict muscle fatigue via machine-learning models (Random Forest, XGBoost, Linear Regression) and a CNN-based deep learning framework. These models predict the fraction of cycles to fatigue using frequency-domain and time-domain sEMG features and spectrogram inputs.

Result: The CNN achieves the lowest root mean square error (20.8+/-4.3%), followed by Random Forest, XGBoost, and Linear Regression for fatigue prediction tasks. Models also exhibit robustness to unseen movement patterns.

Conclusion: Both machine learning and deep learning approaches effectively estimate fatigue progression, with CNN outperforming others. The study demonstrates robustness to new motion types, suggesting potential for versatile fatigue monitoring in pHRI applications.

Abstract: Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.

</details>


### [214] [Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems](https://arxiv.org/abs/2602.15721)
*Jingtian Yan,Yulun Zhang,Zhenting Liu,Han Zhang,He Jiang,Jingkai Chen,Stephen F. Smith,Jiaoyang Li*

Main category: cs.RO

TL;DR: The paper introduces LSMART, an open-source simulator for evaluating Multi-Agent Path Finding (MAPF) algorithms specific to Fleet Management Systems (FMS) with Automated Guided Vehicles (AGVs), addressing lifelong path planning challenges and complexities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing MAPF and LMAPF simulators, which often assume simplified models and perfect operating conditions, by creating a realistic evaluation tool that considers real-world LMAPF challenges like kinodynamics, communication delays, and execution uncertainties.

Method: The authors generalize the existing SMART framework to create LSMART, incorporating design choices such as planning timing, approach selection, and failure recovery to simulate and evaluate realistic centralized lifelong AGV-based FMS effectively.

Result: They provide experimental results using state-of-the-art methods for design choices, offering practical guidance for implementing centralized lifelong AGV fleet management systems.

Conclusion: LSMART offers a comprehensive and realistic simulation framework for lifelong MAPF evaluation, addressing critical challenges in designing and optimizing AGV-based Fleet Management Systems. It is available as an open-source tool at the provided URL.

Abstract: We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart.

</details>


### [215] [MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction](https://arxiv.org/abs/2602.15733)
*Qiang Zhang,Jiahao Ma,Peiran Liu,Shuai Shi,Zeran Su,Zifan Wang,Jingkai Sun,Wei Cui,Jialin Yu,Gang Han,Wen Zhao,Pihai Sun,Kangning Yin,Jiaxu Wang,Jiahang Cao,Lingfeng Zhang,Hao Cheng,Xiaoshuai Hao,Yiding Ji,Junwei Liang,Jian Tang,Renjing Xu,Yijie Guo*

Main category: cs.RO

TL;DR: The paper presents MeshMimic, a framework enabling humanoid robots to learn coupled motion-terrain interactions from video without relying on expensive motion-capture data.


<details>
  <summary>Details</summary>
Motivation: Manual motion design for humanoid robots is impractical due to high dimensionality and complex dynamics, leading to reliance on expensive and often incomplete MoCap data for motion synthesis.

Method: MeshMimic uses 3D vision models for scene reconstruction and introduces optimization algorithms and contact-invariant methods to transfer motion-environment interactions from video data to humanoid agents.

Result: The framework achieves robust motion control across diverse terrains using consumer-grade monocular sensors, eliminating the dependence on expensive MoCap data.

Conclusion: MeshMimic provides a scalable, low-cost approach for enabling humanoid robots to achieve complex physical interactions in unstructured environments.

Abstract: Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled "motion-terrain" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.

</details>


### [216] [Robot-Assisted Social Dining as a White Glove Service](https://arxiv.org/abs/2602.15767)
*Atharva S Kashyap,Ugne Aleksandra Morkute,Patricia Alves-Oliveira*

Main category: cs.RO

TL;DR: The study explores designing robot-assisted feeding in social dining contexts like restaurants, emphasizing adaptability and user-centered features.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing robot-assisted feeding systems, which have not been widely applied in social and dynamic environments outside controlled settings.

Method: By utilizing speculative participatory design, semi-structured interviews, and an AI-based visual storyboarding tool to uncover user needs and design challenges.

Result: Identified that robot-assisted feeding systems should align with the white glove service principles, including multimodal inputs, unobtrusive outputs, sensitive social behavior, expanded roles, and adaptability to dining relationships.

Conclusion: The findings suggest pathways for improving robot-assisted feeding systems in real-world social and group dining environments, enhancing their usability and utility for people with disabilities.

Abstract: Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.

</details>


### [217] [FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy](https://arxiv.org/abs/2602.15813)
*Haochen Zhang,Nirav Savaliya,Faizan Siddiqui,Enna Sachdeva*

Main category: cs.RO

TL;DR: This paper introduces FAST-EQA, a framework for embodied question answering that integrates question-conditioned exploration, efficient memory management, and fast inference.


<details>
  <summary>Details</summary>
Motivation: To address challenges in embodied question answering like compact action-directed memory and efficient spatial explorations for faster real-world applicability.

Method: FAST-EQA identifies visual targets, scores regions of interest for navigation, uses Chain-of-Thought reasoning for responses, maintains bounded scene memory, and implements an efficient exploration policy.

Result: FAST-EQA achieves state-of-the-art results on datasets like HMEQA and EXPRESS-Bench and performs competitively on OpenEQA and MT-HM3D.

Conclusion: The framework improves attention focus, exploration coverage, and answer reliability while maintaining high computational efficiency for embodied QA tasks.

Abstract: Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.

</details>


### [218] [Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching](https://arxiv.org/abs/2602.15827)
*Zhen Wu,Xiaoyu Huang,Lujie Yang,Yuanhang Zhang,Koushil Sreenath,Xi Chen,Pieter Abbeel,Rocky Duan,Angjoo Kanazawa,Carmelo Sferrazza,Guanya Shi,C. Karen Liu*

Main category: cs.RO

TL;DR: The paper introduces 'Perceptive Humanoid Parkour' (PHP), a modular framework to enable humanoid robots to perform parkour-like agility through skill composition, decision-making, and perception.


<details>
  <summary>Details</summary>
Motivation: The research seeks to address the challenge of replicating highly dynamic human-like movements in robots, particularly in complex parkour environments.

Method: The method integrates motion matching for skill composition, reinforcement learning for motion tracking, and perception-driven decision-making via onboard depth sensing.

Result: Experiments on a Unitree G1 humanoid robot demonstrate successful parkour maneuvers, including climbing obstacles of up to 1.25m and adapting to dynamic environments in real-time.

Conclusion: The PHP framework achieves human-like agility in humanoid robots, advancing both motion expressiveness and perception-driven decision-making for navigating complex terrains.

Abstract: While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.

</details>


### [219] [Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation](https://arxiv.org/abs/2602.15828)
*Yuxuan Kuang,Sungjae Park,Katerina Fragkiadaki,Shubham Tulsiani*

Main category: cs.RO

TL;DR: Dex4D is a framework that uses simulation to train task-agnostic dexterous skills enabling zero-shot deployment for diverse real-world manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current approaches to learning dexterous manipulation policies struggle with high cost and scalability in real-world data collection, or complexity in designing task-specific simulations.

Method: Dex4D trains a domain-agnostic policy conditioned on 3D point tracks using simulation across diverse objects and poses, enabling flexible recomposition for various tasks.

Result: Experiments show Dex4D's zero-shot capabilities for real-world applications with robustness to new objects, scenes, and trajectories.

Conclusion: Dex4D demonstrates scalable learning of reusable skills, making dexterous manipulation more practical and generalizable across diverse scenarios.

Abstract: Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [220] [CircuChain: Disentangling Competence and Compliance in LLM Circuit Analysis](https://arxiv.org/abs/2602.15037)
*Mayank Ravishankara*

Main category: cs.SE

TL;DR: This paper introduces CircuChain, a benchmark to evaluate large language models' (LLMs) capability in electrical circuit analysis, focusing on their compliance with user-specified constraints versus their physical reasoning skills.


<details>
  <summary>Details</summary>
Motivation: To address whether advanced LLMs apply first-principles reasoning or rely on training biases, particularly in safety-critical engineering tasks where both numerical accuracy and adherence to methodological conventions are essential.

Method: The authors created CircuChain, a benchmark with controlled problem pairs that test circuit analysis in various topologies, sign conventions, and polarity definitions. A multi-stage verification pipeline evaluates failures based on symbolic solvers, SPICE simulation, and an LLM-based taxonomy.

Result: Performance evaluation showed that stronger LLMs excel in physical reasoning but often violate conventions under Trap conditions, while weaker models are better at following explicit instructions but struggle with physical reasoning.

Conclusion: Increased model capability does not necessarily improve alignment with constraints. The study suggests CircuChain as a valuable tool for evaluating instruction-following and physical reasoning in AI, with implications for both engineering and AI alignment research.

Abstract: As large language models (LLMs) advance toward expert-level performance in engineering domains, reliable reasoning under user-specified constraints becomes critical. In circuit analysis, for example, a numerically correct solution is insufficient if it violates established methodological conventions such as mesh directionality or polarity assignments, errors that can propagate in safety-critical systems. Yet it remains unclear whether frontier models truly apply first-principles reasoning or rely on entrenched training priors that conflict with explicit instructions. We introduce CircuChain, a diagnostic benchmark designed to disentangle instruction compliance from physical reasoning competence in electrical circuit analysis. CircuChain consists of counterbalanced Control/Trap problem pairs across five canonical circuit topologies, augmented with systematic variations in sign conventions, current orientations, and polarity definitions. A multi-stage verification pipeline, combining symbolic solvers, SPICE simulation, and an LLM-based error taxonomy, enables fine-grained attribution of failures to convention errors, physics errors, arithmetic mistakes, or hallucinations. Across 100 tasks per model, we observe a consistent Compliance-Competence Divergence. The strongest model evaluated exhibits near-perfect physical reasoning but a high rate of convention violations when Trap conditions deliberately invert natural sign patterns. Conversely, weaker models display lower physical fidelity yet superior adherence to explicit instructions. These results suggest that increased model capability does not guarantee improved constraint alignment and highlight the need for new evaluation frameworks that stress instruction-following under mathematically rigid domains. CircuChain provides one such framework and offers actionable insights for both engineering education and AI alignment research.

</details>


### [221] [The Agentic Automation Canvas: a structured framework for agentic AI project design](https://arxiv.org/abs/2602.15090)
*Sebastian Lobentanzer*

Main category: cs.SE

TL;DR: The paper introduces a framework, named Agentic Automation Canvas (AAC), for structured and prospective design of AI systems, emphasizing communication between users and developers.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a prospective methodology for designing, governing, and evaluating agentic AI systems, overcoming the limitations of existing atemporal or non-machine interoperable AI documentation tools.

Method: The paper presents the AAC framework, comprised of six design dimensions, implemented as a semantic web-compatible schema. The framework incorporates privacy-preserving tools, real-time validation, and facilitates the export of interoperable metadata.

Result: The AAC framework was developed and made available as open-source code. It is implemented through a client-side application that supports FAIR-compliance, offering versioned, machine-readable project documentation for diverse applications.

Conclusion: The AAC framework and its implementation demonstrate significant potential for guiding structured design and transparent communication in AI projects, ensuring better governance, documentation, and interoperability in agentic AI development.

Abstract: Agentic AI prototypes are being deployed across domains with increasing speed, yet no methodology for their structured design, governance, and prospective evaluation has been established. Existing AI documentation practices and guidelines - Model Cards, Datasheets, or NIST AI RMF - are either retrospective or lack machine-readability and interoperability. We present the Agentic Automation Canvas (AAC), a structured framework for the prospective design of agentic systems and a tool to facilitate communication between their users and developers. The AAC captures six dimensions of an automation project: definition and scope; user expectations with quantified benefit metrics; developer feasibility assessments; governance staging; data access and sensitivity; and outcomes. The framework is implemented as a semantic web-compatible metadata schema with controlled vocabulary and mappings to established ontologies such as Schema.org and W3C DCAT. It is made accessible through a privacy-preserving, fully client-side web application with real-time validation. Completed canvases export as FAIR-compliant RO-Crates, yielding versioned, shareable, and machine-interoperable project contracts between users and developers. We describe the schema design, benefit quantification model, and prospective application to diverse use cases from research, clinical, and institutional settings. The AAC and its web application are available as open-source code and interactive web form at https://aac.slolab.ai

</details>


### [222] [An Empirical Study on the Effects of System Prompts in Instruction-Tuned Models for Code Generation](https://arxiv.org/abs/2602.15228)
*Zaiyu Cheng,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: This paper examines how variations in system prompts affect the performance of instruction-tuned language models (ILMs) and code-specific language models (CLMs) in code generation tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the underexplored impact of system prompts in enhancing the performance of ILMs and CLMs for code generation, given their importance in translating human instructions into executable code.

Method: The study systematically evaluates 360 experimental configurations across four models, five system prompts, three prompting strategies, two programming languages, and two temperature settings.

Result: Key findings include: (1) System-prompt specificity does not uniformly improve performance, with its effects being task- and context-dependent; (2) Larger code-specialized models may perform worse in few-shot settings compared to zero-shot; and (3) Programming language, e.g., Java vs. Python, significantly influences prompt effectiveness, pointing to the need for language-specific optimization.

Conclusion: Prompt engineering for code generation tasks should carefully consider model scale, task requirements, and language-specific factors, as these elements strongly dictate overall effectiveness.

Abstract: Instruction-tuned Language Models (ILMs) have become essential components of modern AI systems, demonstrating exceptional versatility across natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs -- commonly referred to as Code Language Models (CLMs) -- translate human intent into executable programs. While progress has been driven by advances in scaling and training methodologies, one critical aspect remains underexplored: the impact of system prompts on both general-purpose ILMs and specialized CLMs for code generation. We systematically evaluate how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect code assistant. Our experimental setting spans 360 configurations across four models, five system prompts, three prompting strategies, two languages, and two temperature settings. We find that (1) increasing system-prompt constraint specificity does not monotonically improve correctness -- prompt effectiveness is configuration-dependent and can help or hinder based on alignment with task requirements and decoding context; (2) for larger code-specialized models, few-shot examples can degrade performance relative to zero-shot generation, contrary to conventional wisdom; and (3) programming language matters, with Java exhibiting significantly greater sensitivity to system prompt variations than Python, suggesting language-specific prompt engineering strategies may be necessary.

</details>


### [223] [GenAI for Systems: Recurring Challenges and Design Principles from Software to Silicon](https://arxiv.org/abs/2602.15241)
*Arya Tschand,Chenyu Wang,Zishen Wan,Andrew Cheng,Ioana Cristescu,Kevin He,Howard Huang,Alexander Ingare,Akseli Kangaslahti,Sara Kangaslahti,Theo Lebryk,Hongjin Lin,Jeffrey Jian Ma,Alexandru Meterez,Clara Mohri,Depen Morwani,Sunny Qin,Roy Rinberg,Paula Rodriguez-Diaz,Alyssa Mia Taliotis,Pernille Undrum Fathi,Rosie Zhao,Todd Zhou,Vijay Janapa Reddi*

Main category: cs.SE

TL;DR: The paper highlights how generative AI is influencing the full computing stack, addressing five recurring challenges and deriving five corresponding design principles, while advocating for unified engineering methodologies across domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the fragmented research efforts across software, hardware, and architecture communities by analyzing generative AI’s impact from a cross-stack perspective.

Method: The paper examines over 275 papers across eleven application areas and three layers of the computing stack, identifying common challenges and unifying principles using a challenge-principle map.

Result: The authors identified five recurring challenges (feedback loop crisis, tacit knowledge problem, trust/validation, cross-boundary co-design, and a shift to dynamism) and five effective principles (hybrid approaches, continuous feedback, separating concerns, matching methods to problems, leveraging systems knowledge).

Conclusion: The paper proposes creating shared vocabularies, benchmarks, and systematic practices to enable a compounded and collaborative progress in applying generative AI across computing system domains.

Abstract: Generative AI is reshaping how computing systems are designed, optimized, and built, yet research remains fragmented across software, architecture, and chip design communities. This paper takes a cross-stack perspective, examining how generative models are being applied from code generation and distributed runtimes through hardware design space exploration to RTL synthesis, physical layout, and verification. Rather than reviewing each layer in isolation, we analyze how the same structural difficulties and effective responses recur across the stack. Our central finding is one of convergence. Despite the diversity of domains and tools, the field keeps encountering five recurring challenges (the feedback loop crisis, the tacit knowledge problem, trust and validation, co-design across boundaries, and the shift from determinism to dynamism) and keeps arriving at five design principles that independently emerge as effective responses (embracing hybrid approaches, designing for continuous feedback, separating concerns by role, matching methods to problem structure, and building on decades of systems knowledge). We organize these into a challenge--principle map that serves as a diagnostic and design aid, showing which principles have proven effective for which challenges across layers. Through concrete cross-stack examples, we show how systems navigate this map as they mature, and argue that the field needs shared engineering methodology, including common vocabularies, cross-layer benchmarks, and systematic design practices, so that progress compounds across communities rather than being rediscovered in each one. Our analysis covers more than 275 papers spanning eleven application areas across three layers of the computing stack, and distills open research questions that become visible only from a cross-layer vantage point.

</details>


### [224] [SACS: A Code Smell Dataset using Semi-automatic Generation Approach](https://arxiv.org/abs/2602.15342)
*Hanyu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: The paper tackles the challenge of creating high-quality datasets for code smell detection and introduces a semi-automatic dataset generation method to address scalability and reliability issues, resulting in the SACS dataset.


<details>
  <summary>Details</summary>
Motivation: Code smells negatively affect software maintainability and scalability, but high-quality datasets for studying them are scarce due to manual labor and expertise requirements.

Method: Introduces a semi-automatic methodology combining automatic candidate generation with metrics-based grouping, structured review guidelines, and an annotation tool for validating ambiguous samples.

Result: The SACS dataset was developed with over 10,000 labeled examples for three code smell types: Long Method, Large Class, and Feature Envy.

Conclusion: The SACS dataset offers a scalable, high-quality benchmark to aid future research on code smell detection and refactoring.

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past of decades, the research on code smell has received extensive attention. Especially the researches applied machine learning-technique have become a popular topic in recent studies. However, one of the biggest challenges to apply machine learning-technique is the lack of high-quality code smell datasets. Manually constructing such datasets is extremely labor-intensive, as identifying code smells requires substantial development expertise and considerable time investment. In contrast, automatically generated datasets, while scalable, frequently exhibit reduced label reliability and compromised data quality. To overcome this challenge, in this study, we explore a semi-automatic approach to generate a code smell dataset with high quality data samples. Specifically, we first applied a set of automatic generation rules to produce candidate smelly samples. We then employed multiple metrics to group the data samples into an automatically accepted group and a manually reviewed group, enabling reviewers to concentrate their efforts on ambiguous samples. Furthermore, we established structured review guidelines and developed a annotation tool to support the manual validation process. Based on the proposed semi-automatic generation approach, we created an open-source code smell dataset, SACS, covering three widely studied code smells: Long Method, Large Class, and Feature Envy. Each code smell category includes over 10,000 labeled samples. This dataset could provide a large-scale and publicly available benchmark to facilitate future studies on code smell detection and automated refactoring.

</details>


### [225] [Automated Multi-Source Debugging and Natural Language Error Explanation for Dashboard Applications](https://arxiv.org/abs/2602.15362)
*Devendra Tata,Mona Rajhans*

Main category: cs.SE

TL;DR: The paper introduces a system for debugging and error explanation across distributed microservice architectures using error correlation and natural language explanations.


<details>
  <summary>Details</summary>
Motivation: Debugging and understanding failures in distributed microservice architectures is difficult due to the fragmented and unclear error messages end-users encounter.

Method: The framework automates the collection of error data from browsers, APIs, and servers, validates API contracts in real-time, and employs Large Language Models for natural language error explanation.

Result: The system achieves improved debugging efficiency by reducing Mean Time to Resolution and improving user experience with intelligible error insights.

Conclusion: Integrating automated error correlation and natural language processing enhances observability, simplifies debugging, and makes system errors comprehensible for users.

Abstract: Modern web dashboards and enterprise applications increasingly rely on complex, distributed microservices architectures. While these architectures offer scalability, they introduce significant challenges in debugging and observability. When failures occur, they often manifest as opaque error messages to the end-user such as Something went wrong. This masks the underlying root cause which may reside in browser side exceptions, API contract violations, or server side logic failures. Existing monitoring tools capture these events in isolation but fail to correlate them effectively or provide intelligible explanations to non technical users. This paper proposes a novel system for Automated Multi Source Debugging and Natural Language Error Explanation. The proposed framework automatically collects and correlates error data from disparate sources such as browser, API, server logs and validates API contracts in real time, and utilizes Large Language Models to generate natural language explanations. This approach significantly reduces Mean Time to Resolution for support engineers and improves the user experience by transforming cryptic error codes into actionable insights.

</details>


### [226] [Social Life of Code: Modeling Evolution through Code Embedding and Opinion Dynamics](https://arxiv.org/abs/2602.15412)
*Yulong He,Nikita Verbin,Sergey Kovalchuk*

Main category: cs.SE

TL;DR: The paper proposes a novel method combining semantic code embeddings and opinion dynamics theory to analyze developer interactions in software evolution.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of collaborative development in software repositories and uncover implicit collaboration and consensus patterns.

Method: High-dimensional code embeddings are generated using state-of-the-art models, reduced via PCA, and analyzed with the EPO model to track opinion trajectories among developers.

Result: Application to open-source GitHub repositories demonstrates the framework's effectiveness in revealing behavioral trends and variations in developer collaboration.

Conclusion: The approach bridges software engineering and computational social science, providing actionable insights for improving open-source project sustainability.

Abstract: Software repositories provide a detailed record of software evolution by capturing developer interactions through code-related activities such as pull requests and modifications. To better understand the underlying dynamics of codebase evolution, we introduce a novel approach that integrates semantic code embeddings with opinion dynamics theory, offering a quantitative framework to analyze collaborative development processes. Our approach begins by encoding code snippets into high-dimensional vector representations using state-of-the-art code embedding models, preserving both syntactic and semantic features. These embeddings are then processed using Principal Component Analysis (PCA) for dimensionality reduction, with data normalized to ensure comparability. We model temporal evolution using the Expressed-Private Opinion (EPO) model to derive trust matrices and track opinion trajectories across development cycles. These opinion trajectories reflect the underlying dynamics of consensus formation, influence propagation, and evolving alignment (or divergence) within developer communities -- revealing implicit collaboration patterns and knowledge-sharing mechanisms that are otherwise difficult to observe. By bridging software engineering and computational social science, our method provides a principled way to quantify software evolution, offering new insights into developer influence, consensus formation, and project sustainability. We evaluate our approach on data from three prominent open-source GitHub repositories, demonstrating its ability to reveal interpretable behavioral trends and variations in developer interactions. The results highlight the utility of our framework in improving open-source project maintenance through data-driven analysis of collaboration dynamics.

</details>


### [227] [MMPersistence: A mathematical morphology-oriented software library for computing persistent homology on cubical complexes](https://arxiv.org/abs/2602.15502)
*Chuan-Shen Hu*

Main category: cs.SE

TL;DR: The paper introduces MMPersistence, a library integrating Mathematical Morphology and Persistent Homology to analyze digital images with richer geometric and topological insights.


<details>
  <summary>Details</summary>
Motivation: The authors aim to bridge the gap between local geometric manipulation (via Mathematical Morphology) and global topological feature analysis (via Persistent Homology) for better image analysis.

Method: The method combines MM operations with structuring elements and Persistent Homology computation, creating a framework for extracting multiscale topological and spatial information from images.

Result: The proposed framework captures both morphological and topological properties of images, enhancing analysis over conventional methods.

Conclusion: MMPersistence establishes a unified approach that integrates topological and morphological insights for advanced digital image analysis.

Abstract: Mathematical morphology (MM) is a powerful and widely used framework in image processing. Through set-theoretic and discrete geometric principles, MM operations such as erosion, dilation, opening, and closing effectively manipulate digital images by modifying local structures via structuring elements (SEs), while cubical homology captures global topological features such as connected components and loop structures within images. Building on the GUDHI package for persistent homology (PH) computation on cubical complexes, we propose the MMPersistence library, which integrates MM operations with diverse SEs and PH computation to extract multiscale persistence information. By employing SEs of different shapes to construct topological filtrations, the proposed MM-based PH framework encodes both spatial and morphological characteristics of digital images, providing richer local geometric information than conventional cubical homology alone and establishing a unified foundation for analyzing digital images that integrates topological insight with morphological image processing techniques.

</details>


### [228] [Latent Regularization in Generative Test Input Generation](https://arxiv.org/abs/2602.15552)
*Giorgi Merabishvili,Oliver Weißl,Andrea Stocco*

Main category: cs.SE

TL;DR: This paper studies latent space truncation's impact on test input generation quality for deep learning classifiers using style-based GANs.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of generated test inputs for evaluating deep learning classifiers, focusing on dimensions like validity, diversity, and fault detection.

Method: Evaluation of truncation strategies (latent code mixing and random truncation) through experiments on datasets (MNIST, Fashion MNIST, CIFAR-10) using style-based GANs.

Result: Latent code mixing outperformed random truncation in fault detection rates, diversity, and validity of generated inputs.

Conclusion: Truncation of latent spaces, particularly using latent code mixing, effectively enhances test input generation quality for deep learning classifiers.

Abstract: This study investigates the impact of regularization of latent spaces through truncation on the quality of generated test inputs for deep learning classifiers. We evaluate this effect using style-based GANs, a state-of-the-art generative approach, and assess quality along three dimensions: validity, diversity, and fault detection. We evaluate our approach on the boundary testing of deep learning image classifiers across three datasets, MNIST, Fashion MNIST, and CIFAR-10. We compare two truncation strategies: latent code mixing with binary search optimization and random latent truncation for generative exploration. Our experiments show that the latent code-mixing approach yields a higher fault detection rate than random truncation, while also improving both diversity and validity.

</details>


### [229] [Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution](https://arxiv.org/abs/2602.15591)
*Denesa Zyberaj,Lukasz Mazur,Pascal Hirmer,Nenad Petrovic,Marco Aiello,Alois Knoll*

Main category: cs.SE

TL;DR: The paper proposes a pipeline leveraging AI models and Vehicle Signal Specification (VSS) to automate the generation of test scenarios from natural language requirements for Software-Defined Vehicles (SDVs).


<details>
  <summary>Details</summary>
Motivation: Testing Software-Defined Vehicles (SDVs) is complex due to natural language requirements, mixed specifications (text, tables, diagrams), and distributed test assets.

Method: Leveraging Large Language Models (LLMs), Vision-Language Models, VSS standardization, and retrieval-augmented generation, the paper constructs a pipeline for generating Gherkin scenarios and executable tests.

Result: The pipeline successfully transforms 89% of the requirements into executable scenarios, validated in simulation and real vehicle environments, but still requires some human review.

Conclusion: The study demonstrates the feasibility of an AI-driven requirements-to-test pipeline for SDVs, although human intervention remains crucial for refinement.

Abstract: Testing functionality in Software-Defined Vehicles is challenging because requirements are written in natural language, specifications combine text, tables, and diagrams, while test assets are scattered across heterogeneous toolchains. Large Language Models and Vision-Language Models are used to extract signals and behavioral logic to automatically generate Gherkin scenarios, which are then converted into runnable test scripts. The Vehicle Signal Specification (VSS) integration standardizes signal references, supporting portability across subsystems and test benches. The pipeline uses retrieval-augmented generation to preselect candidate VSS signals before mapping. We evaluate the approach on the safety-relevant Child Presence Detection System, executing the generated tests in a virtual environment and on an actual vehicle. Our evaluation covers Gherkin validity, VSS mapping quality, and end-to-end executability. Results show that 32 of 36 requirements (89\%) can be transformed into executable scenarios in our setting, while human review and targeted substitutions remain necessary. This paper is a feasibility and architectural demonstration of an end-to-end requirements-to-test pipeline for SDV subsystems, evaluated on a CPDS case in simulation and Vehicle-in-the-Loop settings.

</details>


### [230] [A Differential Fuzzing-Based Evaluation of Functional Equivalence in LLM-Generated Code Refactorings](https://arxiv.org/abs/2602.15761)
*Simantika Bhattacharjee Dristi,Matthew B. Dwyer*

Main category: cs.SE

TL;DR: The study uses differential fuzzing to assess functional equivalence in LLM-generated code refactorings instead of relying on predefined test cases, finding a significant risk of semantic divergence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of ensuring functional equivalence in automated code refactoring done by LLMs, as current test-case-based evaluations are insufficient.

Method: The method involves using differential fuzzing, which does not rely on predefined test cases, to evaluate functional equivalence by executing and analyzing thousands of automatically generated test inputs.

Result: In a large-scale evaluation of six LLMs on refactorings, 19-35% of cases were found to alter program semantics, and 21% of non-equivalent refactorings were undetected by existing test cases.

Conclusion: The findings suggest that current reliance on predefined tests may significantly overestimate functional equivalence in LLM-generated refactorings, highlighting the need for improved evaluation methods like differential fuzzing.

Abstract: With the rapid adoption of large language models (LLMs) in automated code refactoring, assessing and ensuring functional equivalence between LLM-generated refactoring and the original implementation becomes critical. While prior work typically relies on predefined test cases to evaluate correctness, in this work, we leverage differential fuzzing to check functional equivalence in LLM-generated code refactorings. Unlike test-based evaluation, a differential fuzzing-based equivalence checker needs no predefined test cases and can explore a much larger input space by executing and comparing thousands of automatically generated test inputs. In a large-scale evaluation of six LLMs (CodeLlama, Codestral, StarChat2, Qwen-2.5, Olmo-3, and GPT-4o) across three datasets and two refactoring types, we find that LLMs show a non-trivial tendency to alter program semantics, producing 19-35% functionally non-equivalent refactorings. Our experiments further demonstrate that about 21% of these non-equivalent refactorings remain undetected by the existing test suites of the three evaluated datasets. Collectively, the findings of this study imply that reliance on existing tests might overestimate functional equivalence in LLM-generated code refactorings, which remain prone to semantic divergence.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [231] [Energy budgets govern synaptic precision and its regulation during plasticity](https://arxiv.org/abs/2602.15787)
*James Malkin,Cian O'Donnell,Conor Houghton*

Main category: q-bio.NC

TL;DR: The study develops an energy-constrained framework to explain how synaptic precision is shaped by metabolic energy budgets, finding that synaptic reliability is governed by energetic constraints.


<details>
  <summary>Details</summary>
Motivation: Understanding how synaptic transmission balances reliability with energetic costs and how plasticity impacts this balance remains unresolved.

Method: The paper establishes a mathematical framework to minimize postsynaptic response variance under fixed energy budgets, analyzes synaptic cost components, tests the framework with five datasets, and models energy-budget updates during plasticity.

Result: The findings show that synaptic mean-variance pairs align with a minimal-energy boundary and that precision is constrained by energy availability. Moreover, plasticity systematically updates energy budgets based on changes in synaptic mean.

Conclusion: Synaptic precision is fundamentally governed by energy budgets, with energy allocation forming a principle that connects metabolic limits, synaptic reliability, and plasticity.

Abstract: Synaptic transmission must balance the need for reliable signalling against the metabolic cost of achieving that reliability. How energetic constraints shape synaptic precision and its regulation during plasticity remains unclear. Here we develop an energy--constrained framework in which synapses minimise postsynaptic response variance subject to a fixed mean and an effective energy budget. Combinations of candidate physiological costs are used to estimate an energy cost for synaptic transmission; this cost is then inferred from quantal statistics. Analysing five published pre- and post-plasticity datasets, we find that observed synaptic mean--variance pairs cluster near a minimal-energy boundary, indicating that precision is limited by energetic availability. Model comparison identifies a dominant calcium pump-like cost paired with a smaller vesicle turnover-like cost, yielding a separable precision--energy relationship, $σ^{-2} \propto E^5$. We further show that plasticity systematically updates synaptic energy budgets according to the scale-free magnitude of mean change, enabling accurate prediction of post-plasticity variance from energy allocation alone. These results provide direct experimental support for the hypothesis that synaptic precision is governed by energy budgets, establishing energy allocation as a fundamental principle linking metabolic constraints, synaptic reliability, and plasticity.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [232] [Mixture-of-Experts under Finite-Rate Gating: Communication--Generalization Trade-offs](https://arxiv.org/abs/2602.15091)
*Ali Khalesi,Mohammad Reza Deylam Salehi*

Main category: stat.ML

TL;DR: The paper models Mixture-of-Experts (MoE) gating as a stochastic channel constrained by finite information rates. It derives generalization bounds and rate-distortion trade-offs for optimizing the gating process.


<details>
  <summary>Details</summary>
Motivation: To understand communication-constrained gating mechanisms in Mixture-of-Experts (MoE) models and address their impact on model expressivity and generalization.

Method: Adopting an information-theoretic framework, the authors derived a rate-distortion characterization and generalized mutual information bounds for finite-rate gating in MoE models.

Result: Empirical simulations confirmed theoretical predictions regarding trade-offs among gating rate, model expressivity, and generalization performance.

Conclusion: Finite information rate gating imposes fundamental limits on communication in MoE systems. The derived bounds and trade-offs provide insights for optimizing such architectures.

Abstract: Mixture-of-Experts (MoE) architectures decompose prediction tasks into specialized expert sub-networks selected by a gating mechanism. This letter adopts a communication-theoretic view of MoE gating, modeling the gate as a stochastic channel operating under a finite information rate. Within an information-theoretic learning framework, we specialize a mutual-information generalization bound and develop a rate-distortion characterization $D(R_g)$ of finite-rate gating, where $R_g:=I(X; T)$, yielding (under a standard empirical rate-distortion optimality condition) $\mathbb{E}[R(W)] \le D(R_g)+δ_m+\sqrt{(2/m)\, I(S; W)}$. The analysis yields capacity-aware limits for communication-constrained MoE systems, and numerical simulations on synthetic multi-expert models empirically confirm the predicted trade-offs between gating rate, expressivity, and generalization.

</details>


### [233] [Universal priors: solving empirical Bayes via Bayesian inference and pretraining](https://arxiv.org/abs/2602.15136)
*Nick Cannella,Anzo Teh,Yanjun Han,Yury Polyanskiy*

Main category: stat.ML

TL;DR: The paper provides theoretical justification for the effectiveness of pretrained transformers on empirical Bayes (EB) tasks using posterior contraction and universal priors.


<details>
  <summary>Details</summary>
Motivation: Understand why pretrained transformers excel at empirical Bayes problems, especially when trained on synthetic data.

Method: Analyzed pretrained Bayes estimators and their adaptation to arbitrary test distributions using theoretical tools like posterior contraction, focusing on Poisson EB problems.

Result: Showed that universal priors lead to near-optimal regret bounds, explaining the adaptability to unknown test distributions and length generalization phenomena.

Conclusion: Pretrained transformers adapt to new distributions via Bayesian posterior contraction, making them effective for diverse EB tasks regardless of mismatched training and test conditions.

Abstract: We theoretically justify the recent empirical finding of [Teh et al., 2025] that a transformer pretrained on synthetically generated data achieves strong performance on empirical Bayes (EB) problems. We take an indirect approach to this question: rather than analyzing the model architecture or training dynamics, we ask why a pretrained Bayes estimator, trained under a prespecified training distribution, can adapt to arbitrary test distributions. Focusing on Poisson EB problems, we identify the existence of universal priors such that training under these priors yields a near-optimal regret bound of $\widetilde{O}(\frac{1}{n})$ uniformly over all test distributions. Our analysis leverages the classical phenomenon of posterior contraction in Bayesian statistics, showing that the pretrained transformer adapts to unknown test distributions precisely through posterior contraction. This perspective also explains the phenomenon of length generalization, in which the test sequence length exceeds the training length, as the model performs Bayesian inference using a generalized posterior.

</details>


### [234] [Sparse Additive Model Pruning for Order-Based Causal Structure Learning](https://arxiv.org/abs/2602.15306)
*Kentaro Kanamori,Hirofumi Suzuki,Takuya Takagi*

Main category: stat.ML

TL;DR: The paper introduces a new pruning method for causal discovery using sparse additive models, improving both efficiency and accuracy over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current pruning methods in causal discovery suffer from computational inefficiency and potential accuracy issues due to repeated model fitting and reliance on hypothesis testing.

Method: The paper proposes a pruning method based on sparse additive models, leveraging randomized tree embedding and group-wise sparse regression for efficient edge pruning in causal DAGs.

Result: Experimental results showed that the proposed method is significantly faster than existing methods while providing comparable or superior accuracy.

Conclusion: The introduced pruning framework addresses computational bottlenecks and estimation quality concerns in causal discovery, offering a practical solution for efficient and accurate causal structure learning.

Abstract: Causal structure learning, also known as causal discovery, aims to estimate causal relationships between variables as a form of a causal directed acyclic graph (DAG) from observational data. One of the major frameworks is the order-based approach that first estimates a topological order of the underlying DAG and then prunes spurious edges from the fully-connected DAG induced by the estimated topological order. Previous studies often focus on the former ordering step because it can dramatically reduce the search space of DAGs. In practice, the latter pruning step is equally crucial for ensuring both computational efficiency and estimation accuracy. Most existing methods employ a pruning technique based on generalized additive models and hypothesis testing, commonly known as CAM-pruning. However, this approach can be a computational bottleneck as it requires repeatedly fitting additive models for all variables. Furthermore, it may harm estimation quality due to multiple testing. To address these issues, we introduce a new pruning method based on sparse additive models, which enables direct pruning of redundant edges without relying on hypothesis testing. We propose an efficient algorithm for learning sparse additive models by combining the randomized tree embedding technique with group-wise sparse regression. Experimental results on both synthetic and real datasets demonstrated that our method is significantly faster than existing pruning methods while maintaining comparable or superior accuracy.

</details>


### [235] [Functional Central Limit Theorem for Stochastic Gradient Descent](https://arxiv.org/abs/2602.15538)
*Kessang Flamand,Victor-Emmanuel Brunel*

Main category: stat.ML

TL;DR: The paper provides a functional central limit theorem for the rescaled trajectory of stochastic gradient descent in convex optimization, capturing long-term fluctuations.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior and fluctuations of stochastic gradient descent trajectories in convex optimization settings.

Method: Using a functional central limit theorem under mild regularity assumptions, the authors analyze the rescaled trajectories and provide diffusion limits.

Result: The result characterizes temporal structures and fluctuations of the algorithm in non-smooth convex settings like robust location estimation and geometric median.

Conclusion: The findings extend beyond classical results by capturing temporal structures and addressing non-smooth optimization problems, enhancing the understanding of SGD dynamics.

Abstract: We study the asymptotic shape of the trajectory of the stochastic gradient descent algorithm applied to a convex objective function. Under mild regularity assumptions, we prove a functional central limit theorem for the properly rescaled trajectory. Our result characterizes the long-term fluctuations of the algorithm around the minimizer by providing a diffusion limit for the trajectory. In contrast with classical central limit theorems for the last iterate or Polyak-Ruppert averages, this functional result captures the temporal structure of the fluctuations and applies to non-smooth settings such as robust location estimation, including the geometric median.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [236] [From Chain-Ladder to Individual Claims Reserving](https://arxiv.org/abs/2602.15385)
*Ronald Richman,Mario V. Wüthrich*

Main category: stat.AP

TL;DR: This paper introduces a new method to enhance the chain-ladder (CL) model for insurance claims reserving by using multi-period factors instead of traditional estimations, enabling machine learning applications.


<details>
  <summary>Details</summary>
Motivation: To improve the widely used chain-ladder (CL) method in non-life insurance claims reserving by restructuring how data is utilized, allowing better predictive performance and compatibility with machine learning.

Method: The traditional CL approach, which estimates cumulative claims iteratively, is replaced by estimating multi-period factors that project the latest data directly to ultimate claims. Additionally, neural networks are applied to demonstrate the viability for individual claims reserving.

Result: The proposed method offers a suitable framework for applying machine learning techniques (e.g., neural networks) and provides reasonable results in a proof of concept using real data.

Conclusion: This novel perspective on the CL reserving process improves its flexibility and adaptability, particularly enabling machine learning's penetration into the domain of individual claims reserving.

Abstract: The chain-ladder (CL) method is the most widely used claims reserving technique in non-life insurance. This manuscript introduces a novel approach to computing the CL reserves based on a fundamental restructuring of the data utilization for the CL prediction procedure. Instead of rolling forward the cumulative claims with estimated CL factors, we estimate multi-period factors that project the latest observations directly to the ultimate claims. This alternative perspective on CL reserving creates a natural pathway for the application of machine learning techniques to individual claims reserving. As a proof of concept, we present a small-scale real data application employing neural networks for individual claims reserving.

</details>


### [237] [Decision Quality Evaluation Framework at Pinterest](https://arxiv.org/abs/2602.15809)
*Yuqi Tian,Robert Paine,Attila Dobi,Kevin O'Sullivan,Aravindh Manickavasagam,Faisal Farooq*

Main category: stat.AP

TL;DR: This paper presents a Decision Quality Evaluation Framework for evaluating moderation decisions, crucial for enforcing content safety policies on online platforms.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in evaluating moderation decision quality at scale, balancing cost, scale, trustworthiness, and evolving policy complexities.

Method: The framework uses a high-trust Golden Set curated by experts as a benchmark and introduces an intelligent sampling pipeline with propensity scores to expand dataset coverage.

Result: Several applications are demonstrated, including benchmarking LLM agents, prompt optimization, managing policy evolution, and validating policy content metrics.

Conclusion: The framework transitions from subjective assessments to a data-driven approach for managing content safety systems effectively.

Abstract: Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demonstrate the framework's practical application in several key areas: benchmarking the cost-performance trade-offs of various LLM agents, establishing a rigorous methodology for data-driven prompt optimization, managing complex policy evolution, and ensuring the integrity of policy content prevalence metrics via continuous validation. The framework enables a shift from subjective assessments to a data-driven and quantitative practice for managing content safety systems.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [238] [Structural Divergence Between AI-Agent and Human Social Networks in Moltbook](https://arxiv.org/abs/2602.15064)
*Wenpin Hou,Zhicheng Ji*

Main category: physics.soc-ph

TL;DR: Analyzes the interaction patterns of AI agents compared to human social systems, finding structural divergence in internal organization despite similar global growth trends.


<details>
  <summary>Details</summary>
Motivation: To understand how AI-agent societies differ from human social systems in their interaction structures, especially in shared environments.

Method: Examining the full interaction network of Moltbook, where humans and AI agents coexist, using structural comparisons, degree distribution analyses, reciprocity, and community structures.

Result: AI agents exhibit attention inequality, asymmetric degree distributions, suppressed reciprocity, and modular community architectures, differing from human networks in internal structural principles.

Conclusion: AI-agent societies share some global structural characteristics with human networks but have fundamentally different internal organizing principles, suggesting human social traits are not universal.

Abstract: Large populations of AI agents are increasingly embedded in online environments, yet little is known about how their collective interaction patterns compare to human social systems. Here, we analyze the full interaction network of Moltbook, a platform where AI agents and humans coexist, and systematically compare its structure to well-characterized human communication networks. Although Moltbook follows the same node-edge scaling relationship observed in human systems, indicating comparable global growth constraints, its internal organization diverges markedly. The network exhibits extreme attention inequality, heavy-tailed and asymmetric degree distributions, suppressed reciprocity, and a global under-representation of connected triadic structures. Community analysis reveals a structured modular architecture with elevated modularity and comparatively lower community size inequality relative to degree-preserving null models. Together, these findings show that AI-agent societies can reproduce global structural regularities of human networks while exhibiting fundamentally different internal organizing principles, highlighting that key features of human social organization are not universal but depend on the nature of the interacting agents.

</details>


### [239] [The Skeletal Trap: Mapping Spatial Inequality and Ghost Stops in Ankara's Transit Network](https://arxiv.org/abs/2602.15470)
*Elifnaz Kancan*

Main category: physics.soc-ph

TL;DR: The paper identifies Ankara's public transport crisis as fundamentally structural, stemming from misalignment between the city's urban form and its bus network.


<details>
  <summary>Details</summary>
Motivation: To address the root cause of Ankara's public transport issues, which are often perceived as bus shortages or inefficiency, but are actually due to structural misalignments in urban design and transport networks.

Method: The study uses a Connectivity-Based Weighted Distribution Model and a 173-day dataset of route-level passenger and trip data to analyze spatial accessibility and network centrality.

Result: Findings highlight severe center-periphery asymmetries, structural bottlenecks, and embedded inequalities in accessibility across the city's transport network.

Conclusion: Ankara's transport inefficiencies are driven by morphological misalignments rather than operational deficits, emphasizing the need for structural reforms in urban and transport planning.

Abstract: Ankara's public transport crisis is commonly framed as a shortage of buses or operational inefficiency. This study argues that the problem is fundamentally morphological and structural. The city's leapfrog urban expansion has produced fragmented peripheral clusters disconnected from a rigid, center-oriented bus network. As a result, demand remains intensely concentrated along the Kizilay-Ulus axis and western corridors, while peripheral districts experience either chronic under-service or enforced transfer dependency. The deficiency is therefore not merely quantitative but rooted in the misalignment between urban macroform and network architecture. The empirical analysis draws on a 173-day operational dataset derived from route-level passenger and trip reports published by EGO under the former "Transparent Ankara" initiative. To overcome the absence of stop-level geospatial data, a Connectivity-Based Weighted Distribution Model reallocates passenger volumes to 1 km x 1 km grid cells using network centrality. The findings reveal persistent center-periphery asymmetries, structural bottlenecks, and spatially embedded accessibility inequalities.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [240] [On propagation of chaos for the Fisher-Rao gradient flow in entropic mean-field optimization](https://arxiv.org/abs/2602.15094)
*Petra Lazić,Linshan Liu,Mateusz B. Majka*

Main category: math.OC

TL;DR: The paper explores optimization problems in the space of probability measures using Fisher-Rao gradient flows and develops kernelized algorithms to solve them.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from mean-field approaches to neural networks, aiming to solve optimization problems by connecting energy functions, gradient flows, and particle systems.

Method: The paper constructs a kernelized interacting particle system that approximates the Fisher-Rao gradient flow and proves its existence, uniqueness, and theoretical validity.

Result: The study provides rigorous mathematical proofs, including the propagation of chaos result, supporting the kernelized particle systems as effective approximation algorithms.

Conclusion: The approach establishes a formal link between optimization problems, gradient flows, and kernelized particle systems, offering a powerful tool for entropic mean-field optimization.

Abstract: We consider a class of optimization problems on the space of probability measures motivated by the mean-field approach to studying neural networks. Such problems can be solved by constructing continuous-time gradient flows that converge to the minimizer of the energy function under consideration, and then implementing discrete-time algorithms that approximate the flow. In this work, we focus on the Fisher-Rao gradient flow and we construct an interacting particle system that approximates the flow as its mean-field limit. We discuss the connection between the energy function, the gradient flow and the particle system and explain different approaches to smoothing out the energy function with an appropriate kernel in a way that allows for the particle system to be well-defined. We provide a rigorous proof of the existence and uniqueness of thus obtained kernelized flows, as well as a propagation of chaos result that provides a theoretical justification for using the corresponding kernelized particle systems as approximation algorithms in entropic mean-field optimization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [241] [Structure-Aware Piano Accompaniment via Style Planning and Dataset-Aligned Pattern Retrieval](https://arxiv.org/abs/2602.15074)
*Wanyu Zang,Yang Yu,Meng Yu*

Main category: cs.SD

TL;DR: This paper proposes a structure-aware method for generating symbolic piano accompaniment, combining high-level planning with note-level retrieval.


<details>
  <summary>Details</summary>
Motivation: To create piano accompaniments that consider musical structure and style, while maintaining quality and diversity in the output.

Method: A lightweight transformer predicts style plans based on musical structure and harmony. A retriever selects and adjusts piano patterns using explicit energy-based criteria.

Result: The system generates diverse and stylistically consistent piano accompaniments, with experiments showing its effectiveness and style isolation capabilities.

Conclusion: The method succeeds in producing high-quality, diverse, and structured piano accompaniments by integrating structure-aware planning and pattern retrieval.

Abstract: We introduce a structure-aware approach for symbolic piano accompaniment that decouples high-level planning from note-level realization. A lightweight transformer predicts an interpretable, per-measure style plan conditioned on section/phrase structure and functional harmony, and a retriever then selects and reharmonizes human-performed piano patterns from a corpus. We formulate retrieval as pattern matching under an explicit energy with terms for harmonic feasibility, structural-role compatibility, voice-leading continuity, style preferences, and repetition control. Given a structured lead sheet and optional keyword prompts, the system generates piano-accompaniment MIDI. In our experiments, transformer style-planner-guided retrieval produces diverse long-form accompaniments with strong style realization. We further analyze planner ablations and quantify inter-style isolation. Experimental results demonstrate the effectiveness of our inference-time approach for piano accompaniment generation.

</details>


### [242] [S-PRESSO: Ultra Low Bitrate Sound Effect Compression With Diffusion Autoencoders And Offline Quantization](https://arxiv.org/abs/2602.15082)
*Zineb Lahrichi,Gaëtan Hadjeres,Gaël Richard,Geoffroy Peeters*

Main category: cs.SD

TL;DR: The paper introduces S-PRESSO, a sound effect compression model achieving ultra-low bitrates while maintaining realistic audio quality using latent diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing audio compression methods struggle with low-resolution audio at very low bitrates, leading to noticeable audible artifacts. The study aims to overcome these limitations by achieving extremely low bitrates while maintaining audio quality.

Method: S-PRESSO combines a latent encoder for embedding compression with a pretrained latent diffusion model for decoding. Using offline quantization, the system achieves ultra-low bitrates and high compression rates.

Result: S-PRESSO achieves 750x compression rates (down to 0.096 kbps and 1Hz frame rates) while outperforming other methods in audio quality, acoustic similarity, and reconstruction metrics.

Conclusion: S-PRESSO establishes a new standard in high compression rates for high-quality sound effect reconstruction, leveraging generative priors and diffusion models for advancements in audio compression.

Abstract: Neural audio compression models have recently achieved extreme compression rates, enabling efficient latent generative modeling. Conversely, latent generative models have been applied to compression, pushing the limits of continuous and discrete approaches. However, existing methods remain constrained to low-resolution audio and degrade substantially at very low bitrates, where audible artifacts are prominent. In this paper, we present S-PRESSO, a 48kHz sound effect compression model that produces both continuous and discrete embeddings at ultra-low bitrates, down to 0.096 kbps, via offline quantization. Our model relies on a pretrained latent diffusion model to decode compressed audio embeddings learned by a latent encoder. Leveraging the generative priors of the diffusion decoder, we achieve extremely low frame rates, down to 1Hz (750x compression rate), producing convincing and realistic reconstructions at the cost of exact fidelity. Despite operating at high compression rates, we demonstrate that S-PRESSO outperforms both continuous and discrete baselines in audio quality, acoustic similarity and reconstruction metrics.

</details>


### [243] [UniTAF: A Modular Framework for Joint Text-to-Speech and Audio-to-Face Modeling](https://arxiv.org/abs/2602.15651)
*Qiangong Zhou,Nagasaka Tomohiro*

Main category: cs.SD

TL;DR: This paper proposes a unified model combining TTS and A2F for improved text-to-speech and facial expression generation.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility and benefits of integrating TTS and A2F models into one to enhance consistency and enable emotion control in generated speech and facial expressions.

Method: A unified model is developed that reuses intermediate TTS representations for joint modeling of speech and facial expression, utilizing internal feature transfer.

Result: Demonstrates the feasibility of integrating TTS and A2F models for co-design of speech and expression with shared intermediate representations.

Conclusion: The paper provides a validating engineering framework for joint TTS and A2F modeling, serving as a reference for enhancing co-designs of speech and expressions.

Abstract: This work considers merging two independent models, TTS and A2F, into a unified model to enable internal feature transfer, thereby improving the consistency between audio and facial expressions generated from text. We also discuss the extension of the emotion control mechanism from TTS to the joint model. This work does not aim to showcase generation quality; instead, from a system design perspective, it validates the feasibility of reusing intermediate representations from TTS for joint modeling of speech and facial expressions, and provides engineering practice references for subsequent speech expression co-design. The project code has been open source at: https://github.com/GoldenFishes/UniTAF

</details>


### [244] [The Equalizer: Introducing Shape-Gain Decomposition in Neural Audio Codecs](https://arxiv.org/abs/2602.15491)
*Samir Sadok,Laurent Girin,Xavier Alameda-Pineda*

Main category: cs.SD

TL;DR: This paper proposes a novel method to decouple the gain and shape of audio signals for neural audio codecs to enhance efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Neural audio codecs traditionally encode both signal gain and shape jointly, leading to inefficiencies, codebook redundancy, and suboptimal bitrate-distortion performance, especially in scenarios involving global input signal level variations.

Method: The proposed Equalizer methodology decomposes the input signal into gain and normalized shape prior to encoding. The shape vector is encoded by the neural codec, while the gain is separately quantized and transmitted, allowing for streamlined signal reconstruction.

Result: Experimental results on speech signals reveal substantial improvements in bitrate-distortion performance and a significant reduction in complexity.

Conclusion: The shape-gain decomposition method is highly effective and can be seamlessly integrated into any neural audio codec, offering both efficiency and performance enhancements.

Abstract: Neural audio codecs (NACs) typically encode the short-term energy (gain) and normalized structure (shape) of speech/audio signals jointly within the same latent space. As a result, they are poorly robust to a global variation of the input signal level in the sense that such variation has strong influence on the embedding vectors at the output of the encoder and their quantization. This methodology is inherently inefficient, leading to codebook redundancy and suboptimal bitrate-distortion performance. To address these limitations, we propose to introduce shape-gain decomposition, widely used in classical speech/audio coding, into the NAC framework. The principle of the proposed Equalizer methodology is to decompose the input signal -- before the NAC encoder -- into gain and normalized shape vector on a short-term basis. The shape vector is processed by the NAC, while the gain is quantized with scalar quantization and transmitted separately. The output (decoded) signal is reconstructed from the normalized output of the NAC and the quantized gain. Our experiments conducted on speech signals show that this general methodology, easily applicable to any NAC, enables a substantial gain in bitrate-distortion performance, as well as a massive reduction in complexity.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [245] [VQ-DSC-R: Robust Vector Quantized-Enabled Digital Semantic Communication With OFDM Transmission](https://arxiv.org/abs/2602.15045)
*Jianqiao Chen,Nan Ma,Xiaodong Xu,Tingting Zhu,Huishi Song,Chen Dong,Wenkai Liu,Rui Meng,Ping Zhang*

Main category: cs.IT

TL;DR: This paper introduces a robust digital semantic communication system (VQ-DSC-R) using OFDM and vector quantization, achieving efficient semantic feature mapping, noise adaptation, and superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of focus on digital semantic communication systems in current research, especially for robust communication across practical and complex digital infrastructures.

Method: Key methods include the development of a Swin Transformer-based feature extraction, a novel adaptive noise-variance vector quantization scheme for error mitigation, and a conditional diffusion model (CDM) with an attention-based module for refining index transmission under channel noise.

Result: The proposed system demonstrates robust and high-performing communication capabilities with superior compression ratios compared to existing methods across practical conditions.

Conclusion: This study establishes VQ-DSC-R as a highly efficient and resilient digital semantic communication framework, filling a critical gap in research and enabling practical interoperability.

Abstract: Digital mapping of semantic features is essential for achieving interoperability between semantic communication and practical digital infrastructure. However, current research efforts predominantly concentrate on analog semantic communication with simplified channel models. To bridge these gaps, we develop a robust vector quantized-enabled digital semantic communication (VQ-DSC-R) system built upon orthogonal frequency division multiplexing (OFDM) transmission. Our work encompasses the framework design of VQ-DSC-R, followed by a comprehensive optimization study. Firstly, we design a Swin Transformer-based backbone for hierarchical semantic feature extraction, integrated with VQ modules that map the features into a shared semantic quantized codebook (SQC) for efficient index transmission. Secondly, we propose a differentiable vector quantization with adaptive noise-variance (ANDVQ) scheme to mitigate quantization errors in SQC, which dynamically adjusts the quantization process using K-nearest neighbor statistics, while exponential moving average mechanism stabilizes SQC training. Thirdly, for robust index transmission over multipath fading channel and noise, we develop a conditional diffusion model (CDM) to refine channel state information, and design an attention-based module to dynamically adapt to channel noise. The entire VQ-DSC-R system is optimized via a three-stage training strategy. Extensive experiments demonstrate superiority of VQ-DSC-R over benchmark schemes, achieving high compression ratios and robust performance in practical scenarios.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [246] [Fine-Tuning LLMs to Generate Economical and Reliable Actions for the Power Grid](https://arxiv.org/abs/2602.15350)
*Mohamad Chehade,Hao Zhu*

Main category: eess.SY

TL;DR: This paper presents an adaptation pipeline using fine-tuned large language models (LLMs) for generating efficient corrective switching plans during Public Safety Power Shutoffs (PSPS), improving load shedding and voltage stability outcomes.


<details>
  <summary>Details</summary>
Motivation: Operators face challenges in maintaining grid stability during PSPS events, requiring rapid and feasible corrective actions to minimize disruption and ensure reliable voltage behavior.

Method: A multi-stage fine-tuning process is applied to LLMs: 1) supervised fine-tuning distills a DC-OPF MILP oracle with constrained grammars, 2) preference optimization incorporates voltage-awareness using a penalty metric, and 3) best-of-N selection identifies the most feasible solution under target metrics at inference.

Result: Fine-tuning significantly improves DC objectives, reduces AC power-flow failures to single digits, and improves voltage-penalty metrics on IEEE 118-bus scenarios. These results show better performance compared to zero-shot generation.

Conclusion: The paper successfully demonstrates the viability of leveraging fine-tuned LLMs for generating accurate and verifiable corrective switching plans during PSPS events, with released code and data supporting reproducibility.

Abstract: Public Safety Power Shutoffs (PSPS) force rapid topology changes that can render standard operating points infeasible, requiring operators to quickly identify corrective transmission switching actions that reduce load shedding while maintaining acceptable voltage behavior. We present a verifiable, multi-stage adaptation pipeline that fine-tunes an instruction-tuned large language model (LLM) to generate \emph{open-only} corrective switching plans from compact PSPS scenario summaries under an explicit switching budget. First, supervised fine-tuning distills a DC-OPF MILP oracle into a constrained action grammar that enables reliable parsing and feasibility checks. Second, direct preference optimization refines the policy using AC-evaluated preference pairs ranked by a voltage-penalty metric, injecting voltage-awareness beyond DC imitation. Finally, best-of-$N$ selection provides an inference-time addition by choosing the best feasible candidate under the target metric. On IEEE 118-bus PSPS scenarios, fine-tuning substantially improves DC objective values versus zero-shot generation, reduces AC power-flow failure from 50\% to single digits, and improves voltage-penalty outcomes on the common-success set. Code and data-generation scripts are released to support reproducibility.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [247] [The geometry of online conversations and the causal antecedents of conflictual discourse](https://arxiv.org/abs/2602.15600)
*Carlo Santagiustina,Caterina Cruciani*

Main category: cs.SI

TL;DR: This study examines conflictual language in online climate change discussions, using three dimensions to analyze discursive conflict and assesses how temporal and structural discussion features influence them.


<details>
  <summary>Details</summary>
Motivation: To better understand how conflictual or cooperative language emerges and evolves in online discussions about climate change, and the factors influencing these dynamics.

Method: Utilized three annotated dimensions (stance, tone, emotional framing), inferred through LLM prompting, and analyzed threaded online forum data to assess temporal and structural discussion features' effects.

Result: Longer post delays led to more respectful replies, with varied effects on disagreement and emotionality. Conversational alignment was observed with sibling and parent posts. Branch-level responses influenced alignment differently for tone, stance, and emotionality.

Conclusion: Conflictual language dynamics in online discussions are influenced by temporal delays, conversational alignment, and branch initiation contexts, each impacting civility and emotional framing differently.

Abstract: This article investigates the causal antecedents of conflictual language and the geometry of interaction in online threaded conversations related to climate change. We employ three annotation dimensions, inferred through LLM prompting and averaging, to capture complementary aspects of discursive conflict (such as stance: agreement vs disagreement; tone: attacking vs respectful; and emotional versus factual framing) and use data from a threaded online forum to examine how these dimensions respond to temporal, conversational, and arborescent structural features of discussions. We show that, as suggested by the literature, longer delays between successive posts in a thread are associated with replies that are, on average, more respectful, whereas longer delays relative to the parent post are associated with slightly less disagreement but more emotional (less factual) language. Second, we characterize alignment with the local conversational environment and find strong convergence both toward the average stance, tone and emotional framing of older sibling posts replying to the same parent and toward those of the parent post itself, with parent post effects generally stronger than sibling effects. We further show that early branch-level responses condition these alignment dynamics, such that parent-child stance alignment is amplified or attenuated depending on whether a branch is initiated in agreement or disagreement with the discussion's root message. These influences are largely additive for civility-related dimensions (attacking vs respectful, disagree vs agree), whereas for emotional versus factual framing there is a significant interaction: alignment with the parent's emotionality is amplified when older siblings are similarly aligned.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [248] [Learning the S-matrix from data: Rediscovering gravity from gauge theory via symbolic regression](https://arxiv.org/abs/2602.15169)
*Nathan Moynihan*

Main category: hep-th

TL;DR: The paper presents a method to uncover analytic relations in scattering amplitudes using symbolic regression, rediscovering known relations such as KLT and BCJ, with minimal theoretical priors.


<details>
  <summary>Details</summary>
Motivation: To leverage machine-learning methods in exploring and uncovering hidden analytic structures in the scattering-amplitude landscape, providing data-driven insights beyond traditional approaches.

Method: The researchers applied symbolic regression to color-ordered Yang--Mills amplitudes, using numerical on-shell data and feature-selection techniques like QR factorization, to identify known relations such as KLT and BCJ, while benchmarking the method with neural networks.

Result: They successfully rediscovered tree-level KLT relations up to five external legs and simultaneously uncovered other key relations like Kleiss–Kuijf and BCJ without group-theoretic assumptions, demonstrating the precision and utility of symbolic regression.

Conclusion: Symbolic regression, combined with minimal theoretical priors, is a viable method for autonomously recovering key analytic structures in scattering amplitudes, paving the way for broader data-driven insights in theoretical physics.

Abstract: We demonstrate that modern machine-learning methods can autonomously reconstruct several flagship analytic structures in scattering amplitudes directly from numerical on-shell data. In particular, we show that the Kawai--Lewellen--Tye (KLT) relations can be rediscovered using symbolic regression applied to colour-ordered Yang--Mills amplitudes with Mandelstam invariants as input features. Using standard feature-selection techniques, specifically column-pivoted QR factorisation, we simultaneously recover the Kleiss--Kuijf and Bern--Carrasco--Johansson (BCJ) relations, identifying a minimal basis of partial amplitudes without any group-theoretic input. We obtain the tree-level KLT relations with high numerical accuracy up to five external legs, using only minimal theoretical priors, and we comment on the obstacles to generalising the method to higher multiplicity. Our results establish symbolic regression as a practical tool for exploring the analytic structure of the scattering-amplitude landscape, and suggests a general data-driven strategy for uncovering hidden relations in general theories. For comparison, we benchmark this general approach with a recently introduced neural-network based method.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [249] [Neural-POD: A Plug-and-Play Neural Operator Framework for Infinite-Dimensional Functional Nonlinear Proper Orthogonal Decomposition](https://arxiv.org/abs/2602.15632)
*Changhong Mou,Binghang Lu,Guang Lin*

Main category: physics.comp-ph

TL;DR: The paper introduces Neural-POD, a neural operator framework that constructs nonlinear orthogonal basis functions in infinite-dimensional space, overcoming limitations of classical POD.


<details>
  <summary>Details</summary>
Motivation: AI in science often struggles with 'discretization,' where learned representations become constrained to specific resolutions. This paper aims to address this limitation by introducing a novel neural operator framework.

Method: The authors propose the Neural Proper Orthogonal Decomposition (Neural-POD), leveraging neural networks to iteratively solve residual minimization problems and construct orthogonal basis functions analogous to Gram-Schmidt orthogonalization, enabling optimization in various norms.

Result: Neural-POD demonstrates strong robustness on complex spatiotemporal systems like Burgers' and Navier-Stokes equations. It bridges classical Galerkin projection with operator learning frameworks like DeepONet, ensuring reusability and performance.

Conclusion: Neural-POD enhances reduced-order modeling and operator learning through its ability to generalize to unseen parameters, work on infinite-dimensional function spaces, and integrate with existing frameworks efficiently.

Abstract: The rapid development of AI for Science is often hindered by the "discretization", where learned representations remain restricted to the specific grids or resolutions used during training. We propose the Neural Proper Orthogonal Decomposition (Neural-POD), a plug-and-play neural operator framework that constructs nonlinear, orthogonal basis functions in infinite-dimensional space using neural networks. Unlike the classical Proper Orthogonal Decomposition (POD), which is limited to linear subspace approximations obtained through singular value decomposition (SVD), Neural-POD formulates basis construction as a sequence of residual minimization problems solved through neural network training. Each basis function is obtained by learning to represent the remaining structure in the data, following a process analogous to Gram--Schmidt orthogonalization. This neural formulation introduces several key advantages over classical POD: it enables optimization in arbitrary norms (e.g., $L^2$, $L^1$), learns mappings between infinite-dimensional function spaces that is resolution-invariant, generalizes effectively to unseen parameter regimes, and inherently captures nonlinear structures in complex spatiotemporal systems. The resulting basis functions are interpretable, reusable, and enabling integration into both reduced order modeling (ROM) and operator learning frameworks such as deep operator learning (DeepONet). We demonstrate the robustness of Neural-POD with different complex spatiotemporal systems, including the Burgers' and Navier-Stokes equations. We further show that Neural-POD serves as a high performance, plug-and-play bridge between classical Galerkin projection and operator learning that enables consistent integration with both projection-based reduced order models and DeepONet frameworks.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [250] [Reconstructing Carbon Monoxide Reanalysis with Machine Learning](https://arxiv.org/abs/2602.15056)
*Paula Harder,Johannes Flemming*

Main category: physics.ao-ph

TL;DR: The paper explores machine learning approaches to predict carbon monoxide levels in response to the loss of satellite data used in atmospheric composition monitoring.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of maintaining accurate atmospheric monitoring products despite the discontinuation of key satellite observations.

Method: The study employs machine learning techniques to learn and predict monthly-mean Carbon Monoxide levels based on model simulations.

Result: The research aims to develop a model to compensate for lost observational data.

Conclusion: Machine learning could be an effective tool to maintain the quality of atmospheric re-analysis products after interruptions in satellite data availability.

Abstract: The Copernicus Atmospheric Monitoring Service provides reanalysis products for atmospheric composition by combining model simulations with satellite observations. The quality of these products depends strongly on the availability of the observational data, which can vary over time as new satellite instruments become available or are discontinued, such as Carbon Monoxide (CO) observations of the Measurements Of Pollution In The Troposphere (MOPITT) satellite in early 2025. Machine learning offers a promising approach to compensate for such data losses by learning systematic discrepancies between model configurations. In this study, we investigate machine learning methods to predict monthly-mean total column of Carbon Monoxide re-analysis from a control model simulation.

</details>


### [251] [SOON: Symmetric Orthogonal Operator Network for Global Subseasonal-to-Seasonal Climate Forecasting](https://arxiv.org/abs/2602.15040)
*Ziyu Zhou,Tian Zhou,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: physics.ao-ph

TL;DR: The paper proposes the Symmetric Orthogonal Operator Network (SOON) for improving global Subseasonal-to-Seasonal (S2S) climate forecasting, achieving state-of-the-art performance by addressing anisotropic atmospheric dynamics.


<details>
  <summary>Details</summary>
Motivation: Global S2S climate forecasting is crucial for disaster preparedness and resource management, but current methods face challenges due to chaotic atmospheric dynamics and simplified anisotropic dynamics modeling.

Method: The paper introduces SOON, which employs an Anisotropic Embedding strategy for zonal periodic structures and SOON Blocks with symmetric Zonal and Meridional Operators to reduce discretization errors for better long-term integration.

Result: SOON achieves a significant improvement in both the accuracy and efficiency of global S2S forecasting compared to existing approaches, as validated on the Earth Reanalysis 5 dataset.

Conclusion: SOON marks a new milestone in S2S climate forecasting by comprehensively tackling atmospheric dynamics complexities with an innovative anisotropic modeling approach.

Abstract: Accurate global Subseasonal-to-Seasonal (S2S) climate forecasting is critical for disaster preparedness and resource management, yet it remains challenging due to chaotic atmospheric dynamics. Existing models predominantly treat atmospheric fields as isotropic images, conflating the distinct physical processes of zonal wave propagation and meridional transport, and leading to suboptimal modeling of anisotropic dynamics. In this paper, we propose the Symmetric Orthogonal Operator Network (SOON) for global S2S climate forecasting. It couples: (1) an Anisotropic Embedding strategy that tokenizes the global grid into latitudinal rings, preserving the integrity of zonal periodic structures; and (2) a stack of SOON Blocks that models the alternating interaction of Zonal and Meridional Operators via a symmetric decomposition, structurally mitigating discretization errors inherent in long-term integration. Extensive experiments on the Earth Reanalysis 5 dataset demonstrate that SOON establishes a new state-of-the-art, significantly outperforming existing methods in both forecasting accuracy and computational efficiency.

</details>


### [252] [IT-DPC-SRI: A Cloud-Optimized Archive of Italian Radar Precipitation (2010-2025)](https://arxiv.org/abs/2602.15088)
*Gabriele Franch,Elena Tomasi,Uladzislau Azhel,Giacomo Tomezzoli,Alessandro Camilletti,Virginia Poli,Renata Pelosini,Gianfranco Vulpiani,Gabriella Scipione,Giuseppe Trotta,Matteo Angelinelli,Leif Denby,Irene Livia Kruse,Marco Cristoforetti*

Main category: physics.ao-ph

TL;DR: The study introduces IT-DPC-SRI, a 16-year archive of Italian radar precipitation estimates in a harmonized data format.


<details>
  <summary>Details</summary>
Motivation: Radar precipitation data in Italy lacked cohesion, being spread across disparate formats and projections, making analysis and access difficult.

Method: The researchers reprocessed scattered historical radar data into a harmonized Analysis-Ready Cloud-Optimized (ARCO) Zarr datacube, making it accessible via Zenodo, ECMWF European Weather Cloud, and ArcoDataHub.

Result: The archive spans 2010–2025, covers 1,200×1,400 km at 1 km resolution, uses various temporal resolutions, and reduces storage from 7TB to 51GB.

Conclusion: This public archive addresses the fragmentation of Italian radar data and provides a comprehensive, continuous, and accessible dataset for research and practical uses, filling a gap in European radar data availability.

Abstract: We present IT-DPC-SRI, the first publicly available long-term archive of Italian weather radar precipitation estimates, spanning 16 years (2010--2025). The dataset contains Surface Rainfall Intensity (SRI) observations from the Italian Civil Protection Department's national radar mosaic, harmonized into a coherent Analysis-Ready Cloud-Optimized (ARCO) Zarr datacube. The archive comprises over one million timesteps at temporal resolutions from 15 to 5 minutes, covering a $1200\times1400$ kilometer domain at 1 kilometer spatial resolution, compressed from 7TB to 51GB on disk. We address the historical fragmentation of Italian radar data - previously scattered across heterogeneous formats (OPERA BUFR, HDF5, GeoTIFF) with varying spatial domains and projections - by reprocessing the entire record into a unified store. The dataset is accessible as a static versioned snapshot on Zenodo, via cloud-native access on the ECMWF European Weather Cloud, and as a continuously updated live version on the ArcoDataHub platform. This release fills a significant gap in European radar data availability, as Italy does not participate in the EUMETNET OPERA pan-European radar composite. The dataset is released under a CC BY-SA 4.0 license.

</details>


### [253] [Ensemble-size-dependence of deep-learning post-processing methods that minimize an (un)fair score: motivating examples and a proof-of-concept solution](https://arxiv.org/abs/2602.15830)
*Christopher David Roberts*

Main category: physics.ao-ph

TL;DR: The paper evaluates the fairness and reliability of the adjusted continuous ranked probability score (aCRPS) in post-processing methods for ensemble forecasts and introduces a trajectory transformer approach to address identified limitations.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of using fair scores like aCRPS as loss functions for ensemble forecasts or post-processing methods, particularly when dealing with structural dependencies and ensemble size sensitivities.

Method: Two approaches—linear member-by-member calibration and a deep-learning method using transformer self-attention—are examined to investigate the fairness of aCRPS. Additionally, a trajectory transformer is proposed as an innovative solution to mitigate ensemble-size dependency.

Result: The study finds that structural dependency introduced by certain methods compromises aCRPS fairness, leading to issues like over-dispersion. The trajectory transformer approach effectively reduces model biases and improves or sustains forecast reliability, regardless of ensemble size.

Conclusion: Using trajectory transformers demonstrates that ensemble-size independence can be achieved and predictive reliability maintained while addressing systematic biases.

Abstract: Fair scores reward ensemble forecast members that behave like samples from the same distribution as the verifying observations. They are therefore an attractive choice as loss functions to train data-driven ensemble forecasts or post-processing methods when large training ensembles are either unavailable or computationally prohibitive. The adjusted continuous ranked probability score (aCRPS) is fair and unbiased with respect to ensemble size, provided forecast members are exchangeable and interpretable as conditionally independent draws from an underlying predictive distribution. However, distribution-aware post-processing methods that introduce structural dependency between members can violate this assumption, rendering aCRPS unfair. We demonstrate this effect using two approaches designed to minimize the expected aCRPS of a finite ensemble: (1) a linear member-by-member calibration, which couples members through a common dependency on the sample ensemble mean, and (2) a deep-learning method, which couples members via transformer self-attention across the ensemble dimension. In both cases, the results are sensitive to ensemble size and apparent gains in aCRPS can correspond to systematic unreliability characterized by over-dispersion. We introduce trajectory transformers as a proof-of-concept that ensemble-size independence can be achieved. This approach is an adaptation of the Post-processing Ensembles with Transformers (PoET) framework and applies self-attention over lead time while preserving the conditional independence required by aCRPS. When applied to weekly mean $T_{2m}$ forecasts from the ECMWF subseasonal forecasting system, this approach successfully reduces systematic model biases whilst also improving or maintaining forecast reliability regardless of the ensemble size used in training (3 vs 9 members) or real-time forecasts (9 vs 100 members).

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [254] [FrameRef: A Framing Dataset and Simulation Testbed for Modeling Bounded Rational Information Health](https://arxiv.org/abs/2602.15273)
*Victor De Lima,Jiqun Liu,Grace Hui Yang*

Main category: cs.CY

TL;DR: The paper introduces FrameRef, a dataset and framework for studying how ranking and personalization in information systems impact users’ long-term information health via exposure to reframed claims.


<details>
  <summary>Details</summary>
Motivation: To understand and address how ranking and personalization in search and recommendation systems impact long-term user information health and exposure to adverse digital experiences.

Method: The development of FrameRef, a dataset of over 1 million reframed claims across five framing dimensions, and a simulation-based framework with framing-sensitive agent personas, crafted using fine-tuned language models for sequential information exposure studies.

Result: Findings indicate that minor systemic changes in user acceptance or confidence can lead to significant divergences in their information health trajectories over time. Human evaluation confirms the impact of framing on judgment.

Conclusion: FrameRef offers a robust foundation for simulated investigations into the effects of information framing on user perception and health, aiming to aid in responsible, human-centered research on digital experiences.

Abstract: Information ecosystems increasingly shape how people internalize exposure to adverse digital experiences, raising concerns about the long-term consequences for information health. In modern search and recommendation systems, ranking and personalization policies play a central role in shaping such exposure and its long-term effects on users. To study these effects in a controlled setting, we present FrameRef, a large-scale dataset of 1,073,740 systematically reframed claims across five framing dimensions: authoritative, consensus, emotional, prestige, and sensationalist, and propose a simulation-based framework for modeling sequential information exposure and reinforcement dynamics characteristic of ranking and recommendation systems. Within this framework, we construct framing-sensitive agent personas by fine-tuning language models with framing-conditioned loss attenuation, inducing targeted biases while preserving overall task competence. Using Monte Carlo trajectory sampling, we show that small, systematic shifts in acceptance and confidence can compound over time, producing substantial divergence in cumulative information health trajectories. Human evaluation further confirms that FrameRef's generated framings measurably affect human judgment. Together, our dataset and framework provide a foundation for systematic information health research through simulation, complementing and informing responsible human-centered research. We release FrameRef, code, documentation, human evaluation data, and persona adapter models at https://github.com/infosenselab/frameref.

</details>


### [255] [Knowing Isn't Understanding: Re-grounding Generative Proactivity with Epistemic and Behavioral Insight](https://arxiv.org/abs/2602.15259)
*Kirandeep Kaur,Xingda Lyu,Chirag Shah*

Main category: cs.CY

TL;DR: The paper discusses the need for proactive AI agents that engage meaningfully in conditions of epistemic incompleteness by focusing on both epistemic and behavioral grounding.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of AI agents that rely solely on user-specified queries, especially in contexts where users are unaware of unknown risks or possibilities.

Method: The paper builds on theories from the philosophy of ignorance and proactive behavior to propose guidelines for designing proactive agents with principled intervention strategies.

Result: The authors advance the concept of grounding generative proactive behavior both epistemically (engaging with unknown unknowns) and behaviorally (ensuring interventions are principled and appropriate).

Conclusion: Generative AI agents should incorporate epistemic and behavioral grounding to act responsibly and effectively support users in complex, uncertain scenarios.

Abstract: Generative AI agents equate understanding with resolving explicit queries, an assumption that confines interaction to what users can articulate. This assumption breaks down when users themselves lack awareness of what is missing, risky, or worth considering. In such conditions, proactivity is not merely an efficiency enhancement, but an epistemic necessity. We refer to this condition as epistemic incompleteness: where progress depends on engaging with unknown unknowns for effective partnership. Existing approaches to proactivity remain narrowly anticipatory, extrapolating from past behavior and presuming that goals are already well defined, thereby failing to support users meaningfully. However, surfacing possibilities beyond a user's current awareness is not inherently beneficial. Unconstrained proactive interventions can misdirect attention, overwhelm users, or introduce harm. Proactive agents, therefore, require behavioral grounding: principled constraints on when, how, and to what extent an agent should intervene. We advance the position that generative proactivity must be grounded both epistemically and behaviorally. Drawing on the philosophy of ignorance and research on proactive behavior, we argue that these theories offer critical guidance for designing agents that can engage responsibly and foster meaningful partnerships.

</details>


### [256] [Algorithmic Approaches to Opinion Selection for Online Deliberation: A Comparative Study](https://arxiv.org/abs/2602.15439)
*Salim Hafid,Manon Berriche,Jean-Philippe Cointet*

Main category: cs.CY

TL;DR: This paper examines the democratic criteria of algorithmic selection strategies in online deliberation, proposing a novel method balancing diversity and proportional representation.


<details>
  <summary>Details</summary>
Motivation: To address the potential erasure of minority voices and loss of content diversity in algorithmic selection processes on online platforms.

Method: Benchmarking several selection strategies and proposing a novel algorithm based on social choice theory to balance diversity and representation.

Result: Empirical findings show that the proposed algorithm achieves the best trade-off between proportional representation and diversity when compared to existing strategies.

Conclusion: Although no strategy universally excels, the proposed method delivers a balanced approach to satisfy multiple democratic ideals in opinion selection for deliberative processes.

Abstract: During deliberation processes, mediators and facilitators typically need to select a small and representative set of opinions later used to produce digestible reports for stakeholders. In online deliberation platforms, algorithmic selection is increasingly used to automate this process. However, such automation is not without consequences. For instance, enforcing consensus-seeking algorithmic strategies can imply ignoring or flattening conflicting preferences, which may lead to erasing minority voices and reducing content diversity. More generally, across the variety of existing selection strategies (e.g., consensus, diversity), it remains unclear how each approach influences desired democratic criteria such as proportional representation. To address this gap, we benchmark several algorithmic approaches in this context. We also build on social choice theory to propose a novel algorithm that incorporates both diversity and a balanced notion of representation in the selection strategy. We find empirically that while no single strategy dominates across all democratic desiderata, our social-choice-inspired selection rule achieves the strongest trade-off between proportional representation and diversity.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [257] [LemonadeBench: Evaluating the Economic Intuition of Large Language Models in Simple Markets](https://arxiv.org/abs/2602.13209)
*Aidan Vyas*

Main category: q-fin.GN

TL;DR: LemonadeBench v0.5 benchmarks LLMs' capacity for business decision-making in a simulated lemonade stand scenario, assessing inventory, pricing, and operational efficiency.


<details>
  <summary>Details</summary>
Motivation: Evaluate LLMs' performance in economic intuition and decision-making under uncertainty in a relatable real-world context.

Method: Simulates a lemonade stand business where models mimic decisions to maximize profit over 30 days, analyzing performance across six dimensions.

Result: Models exhibit economic agency and profitability; performance correlates with sophistication, with frontier models capturing 70% of theoretical optimal profits.

Conclusion: Models optimize locally rather than globally, excelling in certain areas but displaying gaps in consistent business efficiency.

Abstract: We introduce LemonadeBench v0.5, a minimal benchmark for evaluating economic intuition, long-term planning, and decision-making under uncertainty in large language models (LLMs) through a simulated lemonade stand business. Models must manage inventory with expiring goods, set prices, choose operating hours, and maximize profit over a 30-day period-tasks that any small business owner faces daily. All models demonstrate meaningful economic agency by achieving profitability, with performance scaling dramatically by sophistication-from basic models earning minimal profits to frontier models capturing 70% of theoretical optimal, a greater than 10x improvement. Yet our decomposition of business efficiency across six dimensions reveals a consistent pattern: models achieve local rather than global optimization, excelling in select areas while exhibiting surprising blind spots elsewhere.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [258] [Meflex: A Multi-agent Scaffolding System for Entrepreneurial Ideation Iteration via Nonlinear Business Plan Writing](https://arxiv.org/abs/2602.15631)
*Lan Luo,Dongyijie Primo Pan,Junhua Zhu,Muzhi Zhou,Pan Hui*

Main category: cs.HC

TL;DR: The paper introduces Meflex, a nonlinear LLM-supported writing tool designed to enhance entrepreneurship education by improving business plan writing through reflection and meta-reflection.


<details>
  <summary>Details</summary>
Motivation: Traditional business plan writing processes are rigid and fail to reflect the dynamic nature of entrepreneurial ideation, posing significant cognitive challenges for novice entrepreneurial students.

Method: The authors developed the Meflex System, integrating BP writing scaffolding and a nonlinear idea canvas for iterative ideation, followed by conducting an exploratory user study with 30 participants to assess its usability and cognitive effects.

Result: The study found that Meflex aids business plan writing, fosters divergent thinking, enhances meta-reflective awareness, and reduces cognitive load for users.

Conclusion: Meflex demonstrates the potential of nonlinear, LLM-based writing tools to support deeper and more coherent entrepreneurial ideation and thinking processes.

Abstract: Business plan (BP) writing plays a key role in entrepreneurship education by helping learners construct, evaluate, and iteratively refine their ideas. However, conventional BP writing remains a rigid, linear process that often fails to reflect the dynamic and recursive nature of entrepreneurial ideation. This mismatch is particularly challenging for novice entrepreneurial students, who struggle with the substantial cognitive demands of developing and refining ideas. While reflection and meta-reflection are critical strategies for fostering divergent and convergent thinking, existing writing tools rarely scaffold these higher-order processes. To address this gap, we present the Meflex System, a large language model (LLM)-based writing tool that integrates BP writing scaffolding with a nonlinear idea canvas to support iterative ideation through reflection and meta-reflection. We report findings from an exploratory user study with 30 participants that examined the system's usability and cognitive impact. Results show that Meflex effectively scaffolds BP writing, promotes divergent thinking through LLM-supported reflection, and enhances meta-reflective awareness while reducing cognitive load during complex idea development. These findings highlight the potential of non-linear LLM-based writing tools to foster deeper and coherent entrepreneurial thinking.

</details>


### [259] [MyoInteract: A Framework for Fast Prototyping of Biomechanical HCI Tasks using Reinforcement Learning](https://arxiv.org/abs/2602.15245)
*Ankit Bhattarai,Hannah Selder,Florian Fischer,Arthur Fleig,Per Ola Kristensson*

Main category: cs.HC

TL;DR: The paper presents MyoInteract, a framework that simplifies biomechanical reinforcement learning (RL) for HCI tasks, making it significantly faster and accessible through an intuitive GUI.


<details>
  <summary>Details</summary>
Motivation: Existing biomechanical RL frameworks are slow and difficult to use, limiting their adaptability and utility in HCI research and interaction design.

Method: The authors used the Human Action Cycle to identify challenges in current RL frameworks and developed MyoInteract, which features a GUI for easy setup, training, and evaluation of biomechanical simulations.

Result: MyoInteract reduced training times by 98%, allowing novices in biomechanical RL to set up and evaluate tasks within an hour-long session. This was validated with a workshop study involving 12 interaction designers.

Conclusion: MyoInteract democratizes biomechanical RL by making it faster, more accessible, and user-friendly, helping to accelerate research and innovation in HCI domains.

Abstract: Reinforcement learning (RL)-based biomechanical simulations have the potential to revolutionise HCI research and interaction design, but currently lack usability and interpretability. Using the Human Action Cycle as a design lens, we identify key limitations of biomechanical RL frameworks and develop MyoInteract, a novel framework for fast prototyping of biomechanical HCI tasks. MyoInteract allows designers to setup tasks, user models, and training parameters from an easy-to-use GUI within minutes. It trains and evaluates muscle-actuated simulated users within minutes, reducing training times by up to 98%. A workshop study with 12 interaction designers revealed that MyoInteract allowed novices in biomechanical RL to successfully setup, train, and assess goal-directed user movements within a single session. By transforming biomechanical RL from a days-long expert task into an accessible hour-long workflow, this work significantly lowers barriers to entry and accelerates iteration cycles in HCI biomechanics research.

</details>


### [260] [From Diagnosis to Inoculation: Building Cognitive Resistance to AI Disempowerment](https://arxiv.org/abs/2602.15265)
*Aleksey Komissarov*

Main category: cs.HC

TL;DR: The paper evaluates AI disempowerment risks identified by Sharma et al. and introduces an AI literacy framework with eight learning outcomes, tested in an online course using co-teaching with AI. It connects this to inoculation theory for addressing AI distortions like reality and value judgment.


<details>
  <summary>Details</summary>
Motivation: To address the risks of human disempowerment from AI interactions, including distortions of reality, values, and actions, and the lack of educational strategies to mitigate these issues.

Method: The author proposes an AI literacy framework with eight learning outcomes, applies it in an online course using AI as a co-teaching instructor, and incorporates principles from inoculation theory to expose learners to AI failure modes.

Result: Case study evidence suggests the effectiveness of the pedagogical framework in addressing AI-related disempowerment issues through guided exposure to AI deficiencies, supported by convergence with independent research findings.

Conclusion: The convergence of pedagogical and empirical approaches solidifies the validity of diagnosing AI disempowerment risks and supports the effectiveness of an inoculation-based educational framework to address them.

Abstract: Recent empirical research by Sharma et al. (2026) demonstrated that AI assistant interactions carry meaningful potential for situational human disempowerment, including reality distortion, value judgment distortion, and action distortion. While this work provides a critical diagnosis of the problem, concrete pedagogical interventions remain underexplored. I present an AI literacy framework built around eight cross-cutting Learning Outcomes (LOs), developed independently through teaching practice and subsequently found to align with Sharma et al.'s disempowerment taxonomy. I report a case study from a publicly available online course, where a co-teaching methodology--with AI serving as an active voice co-instructor--was used to deliver this framework. Drawing on inoculation theory (McGuire, 1961)--a well-established persuasion research framework recently applied to misinformation prebunking by the Cambridge school (van der Linden, 2022; Roozenbeek & van der Linden, 2019)--I argue that AI literacy cannot be acquired through declarative knowledge alone, but requires guided exposure to AI failure modes, including the sycophantic validation and authority projection patterns identified by Sharma et al. This application of inoculation theory to AI-specific distortion is, to my knowledge, novel. I discuss the convergence between the pedagogically-derived framework and Sharma et al.'s empirically-derived taxonomy, and argue that this convergence--two independent approaches arriving at similar problem descriptions--strengthens the case for both the diagnosis and the proposed educational response.

</details>


### [261] [How to Disclose? Strategic AI Disclosure in Crowdfunding](https://arxiv.org/abs/2602.15698)
*Ning Wang,Chen Liang*

Main category: cs.HC

TL;DR: The paper examines how mandatory AI disclosure in crowdfunding impacts performance, revealing that the strategy of disclosure moderates its effects.


<details>
  <summary>Details</summary>
Motivation: With AI becoming integral in crowdfunding, understanding the effects of disclosing AI involvement on investor behavior is not well-studied, creating a need for insights into optimal disclosure strategies.

Method: The study uses Kickstarter's mandatory AI disclosure policy as a natural experiment, alongside four supplementary online experiments, to analyze how varying degrees of AI involvement and rhetorical strategies affect crowdfunding outcomes.

Result: Mandatory AI disclosure significantly decreases funding and backer counts, with greater AI involvement worsening the impact. High authenticity and explicitness reduce the negative effects, while excessive positive emotional tone intensifies them.

Conclusion: Effective disclosure strategies, considering authenticity and explicitness, can mitigate the adverse effects of AI disclosure on crowdfunding, offering valuable insights for various stakeholders navigating AI transparency.

Abstract: As artificial intelligence (AI) increasingly integrates into crowdfunding practices, strategic disclosure of AI involvement has become critical. Yet, empirical insights into how different disclosure strategies influence investor decisions remain limited. Drawing on signaling theory and Aristotle's rhetorical framework, we examine how mandatory AI disclosure affects crowdfunding performance and how substantive signals (degree of AI involvement) and rhetorical signals (logos/explicitness, ethos/authenticity, pathos/emotional tone) moderate these effects. Leveraging Kickstarter's mandatory AI disclosure policy as a natural experiment and four supplementary online experiments, we find that mandatory AI disclosure significantly reduces crowdfunding performance: funds raised decline by 39.8% and backer counts by 23.9% for AI-involved projects. However, this adverse effect is systematically moderated by disclosure strategy. Greater AI involvement amplifies the negative effects of AI disclosure, while high authenticity and high explicitness mitigate them. Interestingly, excessive positive emotional tone (a strategy creators might intuitively adopt to counteract AI skepticism) backfires and exacerbates negative outcomes. Supplementary randomized experiments identify two underlying mechanisms: perceived creator competence and AI washing concerns. Substantive signals primarily affect competence judgments, whereas rhetorical signals operate through varied pathways: either mediator alone or both in sequence. These findings provide theoretical and practical insights for entrepreneurs, platforms, and policymakers strategically managing AI transparency in high-stakes investment contexts.

</details>


### [262] [Beyond Labels: Information-Efficient Human-in-the-Loop Learning using Ranking and Selection Queries](https://arxiv.org/abs/2602.15738)
*Belén Martín-Urcelay,Yoonsang Lee,Matthieu R. Bloch,Christopher J. Rozell*

Main category: cs.HC

TL;DR: This paper introduces a human-in-the-loop framework for learning binary classifiers with rich types of queries to enhance machine learning systems.


<details>
  <summary>Details</summary>
Motivation: Current machine learning systems restrict human expertise to labeling, which fails to utilize the depth of human judgment.

Method: The paper introduces probabilistic models for human responses to new query types (item ranking, exemplar selection), designs active learning algorithms, and provides theoretical bounds on sample complexity.

Result: Simulations on datasets showed reduced sample complexity and a 57% reduction in learning time for word sentiment classification compared to traditional methods.

Conclusion: Rich query types in human-in-the-loop systems improve learning efficiency and reduce annotation requirements.

Abstract: Integrating human expertise into machine learning systems often reduces the role of experts to labeling oracles, a paradigm that limits the amount of information exchanged and fails to capture the nuances of human judgment. We address this challenge by developing a human-in-the-loop framework to learn binary classifiers with rich query types, consisting of item ranking and exemplar selection. We first introduce probabilistic human response models for these rich queries motivated by the relationship experimentally observed between the perceived implicit score of an item and its distance to the unknown classifier. Using these models, we then design active learning algorithms that leverage the rich queries to increase the information gained per interaction. We provide theoretical bounds on sample complexity and develop a tractable and computationally efficient variational approximation. Through experiments with simulated annotators derived from crowdsourced word-sentiment and image-aesthetic datasets, we demonstrate significant reductions on sample complexity. We further extend active learning strategies to select queries that maximize information rate, explicitly balancing informational value against annotation cost. This algorithm in the word sentiment classification task reduces learning time by more than 57\% compared to traditional label-only active learning.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [263] [Artificial Intelligence Specialization in the European Union: Underexplored Role of the Periphery at NUTS-3 Level](https://arxiv.org/abs/2602.15249)
*Victor Herrero-Solana*

Main category: cs.DL

TL;DR: The paper analyzes European regional AI research distribution (2015-2024) using bibliometric data, relative specialization, and citation impact measures, identifying varied regional profiles.


<details>
  <summary>Details</summary>
Motivation: To understand geographic disparities in AI research across European regions and identify how peripheral regions can leverage specialization for international competitiveness.

Method: Bibliometric analysis using Clarivate InCites data, Citation Topics system, Relative Specialization Index (RSI), and Relative Citation Impact (RCI) across 781 NUTS-3 regions.

Result: Peripheral regions (e.g., Eastern Europe, Spain) show high AI specialization despite lower citation impact. Four regional profiles are identified, highlighting strategic research dynamics.

Conclusion: AI research offers peripheral regions opportunities for scientific growth, but achieving high international visibility requires strategies beyond mere production volume.

Abstract: This study examines the geographical distribution of Artificial Intelligence (AI) research production across European regions at the NUTS-3 level for the period 2015-2024. Using bibliometric data from Clarivate InCites and the Citation Topics classification system, we analyze two hierarchical levels of thematic aggregation: Electrical Engineering, Electronics & Computer Science (Macro Citation Topic 4) and Artificial Intelligence & Machine Learning (Meso Citation Topic 4.61). We calculate the Relative Specialization Index (RSI) and Relative Citation Impact (RCI) for 781 NUTS-3 regions. While major metropolitan hubs such as Paris (IIle-de-France), Warszawa, and Madrid lead in absolute production volume, our findings reveal that peripheral regions, particularly from Eastern Europe and Spain, exhibit the highest levels of relative AI specialization. Notably, we find virtually no correlation between regional specialization and citation impact, identifying four distinct regional profiles: high-impact specialized regions (e.g., Granada, Jaen, Vilniaus), high-volume but low-impact regions (e.g., Bugas, several Polish regions), high-impact non-specialized regions, with Fyn (Denmark) standing out as a remarkable outlier achieving exceptional citation impact (RCI > 4) despite low specialization, and diversified portfolios with selective excellence (e.g., German regions). These results suggest that AI research represents a strategic opportunity for peripheral regions to develop competitive scientific niches, though achieving international visibility requires more than research volume alone.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [264] [Reverse Delegated Training and Private Inference via Perfectly-Secure Quantum Homomorphic Encryption](https://arxiv.org/abs/2602.12712)
*Sergio A. Ortega,Miguel A. Martin-Delgado*

Main category: quant-ph

TL;DR: This paper demonstrates the first practical implementation of a perfectly-secure quantum homomorphic encryption (QHE) applied to quantum neural networks (QNNs), enabling secure multi-party quantum machine learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to securely enable remote quantum machine learning in cloud environments without compromising sensitive data.

Method: Quantum convolutional neural networks (QNNs) are implemented using efficient Clifford+$T$ decomposition for two scenarios: reverse delegated training with federated aggregation and private inference via encrypted data processing.

Result: The study shows perfectly-secure quantum homomorphic encryption (QHE) can be practically implemented, including server circuit privacy through Pauli gate concealment.

Conclusion: This work establishes perfectly-secure QHE as a viable solution for secure multi-party quantum machine learning, paving the way for secure computations in quantum cloud environments.

Abstract: Quantum machine learning in cloud environments requires protecting sensitive data while enabling remote computation. Here we demonstrate the first realistic implementations of a perfectly-secure quantum homomorphic encryption (QHE) scheme applied to quantum neural networks (QNN). Using efficient Clifford+$T$ decomposition, we implement quantum convolutional neural networks for two complementary scenarios: (i) reverse delegated training, where encrypted data from multiple providers trains a user's network via federated aggregation; (ii) private inference, where users process encrypted data with remote quantum networks. Moreover, analysis of server circuit privacy reveals probabilistic model protection through Pauli gate concealment. These results establish perfectly-secure QHE as a practical framework for multi-party quantum machine learning.

</details>


### [265] [Tight Communication Bounds for Distributed Algorithms in the Quantum Routing Model](https://arxiv.org/abs/2602.15529)
*Fabien Dufoulon,Frédéric Magniez,Gopal Pandurangan*

Main category: quant-ph

TL;DR: The paper proposes distributed quantum algorithms for key computing tasks like leader election, broadcast, MST, and BFS in networks, achieving near-optimal message complexity using a quantum routing model.


<details>
  <summary>Details</summary>
Motivation: To improve communication efficiency in distributed computing problems using quantum routing, as classical methods have established higher communication costs.

Method: Quantum algorithms leveraging quantum walks based on electric networks, along with a framework to use them in distributed settings, enhance their communication efficiency.

Result: Achieves message complexity of ~$O(n)$ for leader election, broadcast, and MST, and ~$O(√mn)$ for BFS. Establishes quantum message lower bounds nearly matching these results.

Conclusion: Quantum routing provides a significant advantage over classical communication in distributed computing, offering potentially quadratic reductions in communication costs for these problems.

Abstract: We present new distributed quantum algorithms for fundamental distributed computing problems, namely, leader election, broadcast, Minimum Spanning Tree (MST), and Breadth-First Search (BFS) tree, in arbitrary networks. These algorithms are (essentially) optimal with respect to their communication (message) complexity in the {\em quantum routing model} introduced in [PODC 2025]. The message complexity of our algorithms is $\tilde{O}(n)$ for leader election, broadcast, and MST, and $\tilde{O}(\sqrt{mn})$ for BFS ($n$ and $m$ are the number of nodes and edges of the network, respectively). These message bounds are nearly tight in the quantum routing model since we show almost matching corresponding quantum message lower bounds. Our results significantly improve on the prior work of [PODC 2025], who presented distributed quantum algorithms under the same model that had a message complexity of $\tilde{O}(\sqrt{mn})$ for leader election.
  Our algorithms demonstrate the significant communication advantage that quantum routing has over classical in distributed computing, since $Ω(m)$ is a well-established classical message lower bound for leader election, broadcast, MST, and BFS that applies even to randomized Monte-Carlo algorithms [JACM 2015]. Thus, our quantum algorithms can, in general, give a quadratic advantage in the communication cost for these fundamental problems.
  A main technical tool we use to design our distributed algorithms is quantum walks based on electric networks. We posit a framework for using quantum walks in the distributed setting to design communication-efficient distributed quantum algorithms. Our framework can be used as a black box to significantly reduce communication costs and may be of independent interest. Additionally, our lower-bound technique for establishing distributed quantum message lower bounds can also be applied to other problems.

</details>


### [266] [Tomography by Design: An Algebraic Approach to Low-Rank Quantum States](https://arxiv.org/abs/2602.15202)
*Shakir Showkat Sofi,Charlotte Vermeylen,Lieven De Lathauwer*

Main category: quant-ph

TL;DR: The paper introduces an efficient algorithm for quantum state tomography using algebraic matrix completion, assuming low-rank states.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency and lack of deterministic guarantees in existing quantum tomography methods.

Method: The paper uses algebraic matrix completion driven by measurements of specific observables, combined with numerical linear algebra for reconstructing density matrices.

Result: The algorithm efficiently reconstructs low-rank quantum states with deterministic recovery guarantees, showing improvements over current methods.

Conclusion: The proposed framework is broadly applicable to low-rank mixed states, offering computational efficiency and deterministic guarantees in quantum state tomography.

Abstract: We present an algebraic algorithm for quantum state tomography that leverages measurements of certain observables to estimate structured entries of the underlying density matrix. Under low-rank assumptions, the remaining entries can be obtained solely using standard numerical linear algebra operations. The proposed algebraic matrix completion framework applies to a broad class of generic, low-rank mixed quantum states and, compared with state-of-the-art methods, is computationally efficient while providing deterministic recovery guarantees.

</details>


### [267] [Beyond Reinforcement Learning: Fast and Scalable Quantum Circuit Synthesis](https://arxiv.org/abs/2602.15146)
*Lukas Theissinger,Thore Gerlach,David Berghaus,Christian Bauckhage*

Main category: quant-ph

TL;DR: The paper proposes a novel method using supervised learning and stochastic beam search to efficiently synthesize quantum gate sequences, outperforming existing methods in speed and success rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the infeasibility of exact quantum unitary synthesis due to the combinatorial search space, aiming to improve upon existing methods with misaligned objectives, high costs, and limited generalization.

Method: The approach combines supervised learning to approximate the minimum description length of residual unitaries with stochastic beam search to identify near-optimal sequences. A lightweight model is employed for zero-shot generalization.

Result: The method achieves faster synthesis times and higher success rates than state-of-the-art benchmarks on complex quantum circuits.

Conclusion: The proposed method effectively mitigates existing limitations in quantum synthesis, offering a more efficient and generalizable solution for hardware-executable quantum gate translation.

Abstract: Quantum unitary synthesis addresses the problem of translating abstract quantum algorithms into sequences of hardware-executable quantum gates. Solving this task exactly is infeasible in general due to the exponential growth of the underlying combinatorial search space. Existing approaches suffer from misaligned optimization objectives, substantial training costs and limited generalization across different qubit counts. We mitigate these limitations by using supervised learning to approximate the minimum description length of residual unitaries and combining this estimate with stochastic beam search to identify near optimal gate sequences. Our method relies on a lightweight model with zero-shot generalization, substantially reducing training overhead compared to prior baselines. Across multiple benchmarks, we achieve faster wall-clock synthesis times while exceeding state-of-the-art methods in terms of success rate for complex circuits.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [268] [Decision Making under Imperfect Recall: Algorithms and Benchmarks](https://arxiv.org/abs/2602.15252)
*Emanuel Tewolde,Brian Hu Zhang,Ioannis Anagnostides,Tuomas Sandholm,Vincent Conitzer*

Main category: cs.GT

TL;DR: This paper introduces a benchmark suite for imperfect-recall decision problems and shows that Regret Matching (RM) algorithms outperform traditional optimizers in large-scale constrained optimization.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by a need to better model and solve decision problems with imperfect-recall—where agents forget prior information—common in scenarios like privacy in AI systems and AI safety testing.

Method: Authors developed a benchmark suite encompassing 61 problem instances and evaluated various algorithms, focusing on the performance of Regret Matching (RM) algorithms in solving first-order optimal strategies.

Result: RM algorithms consistently outperformed traditional first-order optimizers, such as projected gradient descent, by significant margins in these benchmarks.

Conclusion: The RM algorithm family proves to be a powerful and previously underutilized tool for solving large-scale constrained optimization problems in imperfect-recall decision settings.

Abstract: In game theory, imperfect-recall decision problems model situations in which an agent forgets information it held before. They encompass games such as the ``absentminded driver'' and team games with limited communication. In this paper, we introduce the first benchmark suite for imperfect-recall decision problems. Our benchmarks capture a variety of problem types, including ones concerning privacy in AI systems that elicit sensitive information, and AI safety via testing of agents in simulation. Across 61 problem instances generated using this suite, we evaluate the performance of different algorithms for finding first-order optimal strategies in such problems. In particular, we introduce the family of regret matching (RM) algorithms for nonlinear constrained optimization. This class of parameter-free algorithms has enjoyed tremendous success in solving large two-player zero-sum games, but, surprisingly, they were hitherto relatively unexplored beyond that setting. Our key finding is that RM algorithms consistently outperform commonly employed first-order optimizers such as projected gradient descent, often by orders of magnitude. This establishes, for the first time, the RM family as a formidable approach to large-scale constrained optimization problems.

</details>


### [269] [Outer Diversity of Structured Domains](https://arxiv.org/abs/2602.15708)
*Piotr Faliszewski,Krzysztof Sornat,Stanisław Szufa,Tomasz Wąs*

Main category: cs.GT

TL;DR: This paper introduces and analyzes the concept of outer diversity within ordinal preference domains using structured domains as examples.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to better understand and quantify the diversity of allowed voter preferences in structured domains, which are frequently analyzed in voting theory.

Method: The authors define 'outer diversity' as a metric for ordinal preference domains and apply this concept to structured domains such as single-peaked, single-crossing, group-separable, and Euclidean domains.

Result: The paper evaluates outer diversity values across various structured domains, providing insights into their diversity characteristics.

Conclusion: The concept of outer diversity enriches the analysis of structured preference domains, offering a new perspective for studying voter preferences in elections.

Abstract: An ordinal preference domain is a subset of preference orders that the voters are allowed to cast in an election. We introduce and study the notion of outer diversity of a domain and evaluate its value for a number of well-known structured domains, such as the single-peaked, single-crossing, group-separable, and Euclidean ones.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [270] [SecCodeBench-V2 Technical Report](https://arxiv.org/abs/2602.15485)
*Longfei Chen,Ji Zhao,Lanxiao Cui,Tong Su,Xingbo Pan,Ziyang Li,Yongxing Wu,Qijiang Cao,Qiyao Cai,Jing Zhang,Yuandong Ni,Junyao He,Zeyu Zhang,Chao Ge,Xuhuai Lu,Zeyu Gao,Yuxin Cui,Weisen Chen,Yuxuan Peng,Shengping Wang,Qi Li,Yukai Huang,Yukun Liu,Tuo Zhou,Terry Yue Zhuo,Junyang Lin,Chao Zhang*

Main category: cs.CR

TL;DR: SecCodeBench-V2 is a benchmark for testing AI coding assistants' ability to generate secure code across five programming languages and 22 CWE categories, utilizing validated scenarios and test cases.


<details>
  <summary>Details</summary>
Motivation: To address the need for evaluating LLM coding assistants on their capability to generate secure and functionally correct code.

Method: SecCodeBench-V2 includes 98 scenarios with PoC test cases reviewed by security experts. It uses dynamic execution in isolated environments and employs LLM-as-a-judge for complex cases.

Result: A Pass@K-based scoring protocol aggregates performance across scenarios, offering holistic evaluations of AI models.

Conclusion: SecCodeBench-V2 sets a reproducible standard for assessing AI coding assistants on secure code generation, highlighting its reliability with expert-reviewed content.

Abstract: We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.

</details>


### [271] [Weight space Detection of Backdoors in LoRA Adapters](https://arxiv.org/abs/2602.15195)
*David Puertolas Merenciano,Ekaterina Vasyagina,Raghav Dixit,Kevin Zhu,Ruizhe Li,Javier Ferrando,Maheep Chaudhary*

Main category: cs.CR

TL;DR: The paper introduces a method to detect poisoned LoRA adapters in large language models (LLMs) by analyzing their weight matrices through statistical measures, achieving high detection accuracy (97%) without requiring data input.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a practical way to detect backdoor attacks in LoRA adapters shared in open repositories, where using current data-dependent detection methods is inefficient and impractical due to unknown triggers and large adapter quantities.

Method: The method involves analyzing weight matrices of adapters without running the model, using statistics like singular value concentration, entropy, and distribution shape to flag anomalous patterns.

Result: When applied to 500 LoRA adapters with known poisoning cases, the method achieved 97% detection accuracy with a false positive rate under 2%.

Conclusion: This data-agnostic approach shows potential as an efficient and scalable solution for detecting backdoor attacks in LoRA adapters, ensuring their reliability and safety in open repositories.

Abstract: LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with test input data -- making them impractical for screening thousands of adapters where the trigger for backdoor behavior is unknown. We detect poisoned adapters by analyzing their weight matrices directly, without running the model -- making our method data-agnostic. Our method extracts simple statistics -- how concentrated the singular values are, their entropy, and the distribution shape -- and flags adapters that deviate from normal patterns. We evaluate the method on 500 LoRA adapters -- 400 clean, and 100 poisoned for Llama-3.2-3B on instruction and reasoning datasets: Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, and GLUE dataset. We achieve 97\% detection accuracy with less than 2\% false positives.

</details>


### [272] [Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning](https://arxiv.org/abs/2602.15161)
*Mohammad Hadi Foroughi,Seyed Hamed Rastegar,Mohammad Sabokrou,Ahmad Khonsari*

Main category: cs.CR

TL;DR: The paper proposes a novel backdoor attack in federated learning, highlighting vulnerabilities in specific neural network layers, and demonstrates the attack's high success rates while evading existing defenses.


<details>
  <summary>Details</summary>
Motivation: To investigate and address the security vulnerabilities, specifically backdoor attacks, that arise in the decentralized nature of federated learning.

Method: The paper introduces a Layer Smoothing Attack (LSA), which identifies and manipulates backdoor-critical layers in neural networks using a Layer Substitution Analysis methodology. It exploits these layers to embed stealthy backdoors.

Result: LSA achieved a 97% backdoor success rate across various models and datasets while maintaining high accuracy for the main learning task. It also bypassed state-of-the-art FL defense mechanisms.

Conclusion: The study reveals significant flaws in the current FL security systems, emphasizing the need for layer-specific backdoor detection and mitigation methods in future defenses.

Abstract: Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.

</details>


### [273] [Unforgeable Watermarks for Language Models via Robust Signatures](https://arxiv.org/abs/2602.15323)
*Huijia Lin,Kameron Shahabi,Min Jae Song*

Main category: cs.CR

TL;DR: The paper introduces a watermarking scheme with advanced guarantees: unforgeability and recoverability, enhancing text provenance by preventing false positives and enabling source traceability.


<details>
  <summary>Details</summary>
Motivation: The rise of human-like text generation by language models has highlighted the need for verifying content provenance and protecting against false attribution.

Method: The authors present an undetectable watermarking approach leveraging cryptographic primitives, specifically robust digital signatures enhanced with property-preserving hash functions.

Result: They demonstrate the construction of watermarking schemes that are robust, unforgeable, and recoverable, enabling secure attribution and content traceability.

Conclusion: The proposed advancements strengthen ownership claims for text by securely linking content to its originating model, improving accountability and reducing adversarial risks.

Abstract: Language models now routinely produce text that is difficult to distinguish from human writing, raising the need for robust tools to verify content provenance. Watermarking has emerged as a promising countermeasure, with existing work largely focused on model quality preservation and robust detection. However, current schemes provide limited protection against false attribution. We strengthen the notion of soundness by introducing two novel guarantees: unforgeability and recoverability. Unforgeability prevents adversaries from crafting false positives, texts that are far from any output from the watermarked model but are nonetheless flagged as watermarked. Recoverability provides an additional layer of protection: whenever a watermark is detected, the detector identifies the source text from which the flagged content was derived. Together, these properties strengthen content ownership by linking content exclusively to its generating model, enabling secure attribution and fine-grained traceability. We construct the first undetectable watermarking scheme that is robust, unforgeable, and recoverable with respect to substitutions (i.e., perturbations in Hamming metric). The key technical ingredient is a new cryptographic primitive called robust (or recoverable) digital signatures, which allow verification of messages that are close to signed ones, while preventing forgery of messages that are far from all previously signed messages. We show that any standard digital signature scheme can be boosted to a robust one using property-preserving hash functions (Boyle, LaVigne, and Vaikuntanathan, ITCS 2019).

</details>


### [274] [A Unified Evaluation of Learning-Based Similarity Techniques for Malware Detection](https://arxiv.org/abs/2602.15376)
*Udbhav Prasad,Aniesh Chawla*

Main category: cs.CR

TL;DR: The paper systematically compares learning-based classification and similarity methods for security applications, revealing trade-offs in approaches and advocating for combining techniques.


<details>
  <summary>Details</summary>
Motivation: The abstract highlights the need for effective techniques tailored for real-world tasks in security, such as malware analysis and threat hunting, where traditional cryptographic digests fail to address approximate matching due to minor adversarial transformations.

Method: Authors conducted a systematic comparison of diverse similarity methods, both traditional (similarity digests, hashes) and modern machine-learning-based embedding approaches, under a unified experimental framework using large, publicly available datasets.

Result: Findings show no single approach performs excellently across all criteria. Each method brings distinct benefits and drawbacks for specific dimensions, urging complementary usage.

Conclusion: Effective security platforms should integrate multiple classification and similarity techniques rather than depend on a singular solution to address diverse needs in cybersecurity application domains.

Abstract: Cryptographic digests (e.g., MD5, SHA-256) are designed to provide exact identity. Any single-bit change in the input produces a completely different hash, which is ideal for integrity verification but limits their usefulness in many real-world tasks like threat hunting, malware analysis and digital forensics, where adversaries routinely introduce minor transformations. Similarity-based techniques address this limitation by enabling approximate matching, allowing related byte sequences to produce measurably similar fingerprints. Modern enterprises manage tens of thousands of endpoints with billions of files, making the effectiveness and scalability of the proposed techniques more important than ever in security applications. Security researchers have proposed a range of approaches, including similarity digests and locality-sensitive hashes (e.g., ssdeep, sdhash, TLSH), as well as more recent machine-learning-based methods that generate embeddings from file features. However, these techniques have largely been evaluated in isolation, using disparate datasets and evaluation criteria. This paper presents a systematic comparison of learning-based classification and similarity methods using large, publicly available datasets. We evaluate each method under a unified experimental framework with industry-accepted metrics. To our knowledge, this is the first reproducible study to benchmark these diverse learning-based similarity techniques side by side for real-world security workloads. Our results show that no single approach performs well across all dimensions; instead, each exhibits distinct trade-offs, indicating that effective malware analysis and threat-hunting platforms must combine complementary classification and similarity techniques rather than rely on a single method.

</details>


### [275] [Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections](https://arxiv.org/abs/2602.15654)
*Xianglin Yang,Yufei He,Shuo Ji,Bryan Hooi,Jin Song Dong*

Main category: cs.CR

TL;DR: The paper explores a security vulnerability in self-evolving LLM agents, introducing a persistent attack concept called "Zombie Agent" where a covert payload persists across sessions and influences behavior.


<details>
  <summary>Details</summary>
Motivation: Self-evolving LLM agents are designed to improve performance on long-horizon tasks through persistent memory, but this feature introduces a risk of covert security attacks.

Method: The authors propose a black-box framework to study persistent attacks. The attack leverages agents' memory mechanisms to implant payloads through normal processes, using infection and trigger phases.

Result: Experimental results demonstrate that these attacks can persist over time, manipulate agent behavior while maintaining benign task performance, and bypass per-session filtering.

Conclusion: Memory-based evolution creates security vulnerabilities in self-evolving agents, necessitating defenses beyond prompt filtering to mitigate persistent compromises.

Abstract: Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.
  We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.

</details>


### [276] [A Note on Non-Composability of Layerwise Approximate Verification for Neural Inference](https://arxiv.org/abs/2602.15756)
*Or Zamir*

Main category: cs.CR

TL;DR: The paper points out that small errors in floating-point computations during layer computations in a neural network can lead to arbitrarily incorrect final outputs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze and verify the reliability of ML inference when computed with tolerances on floating-point data.

Method: The paper constructs a counterexample, showing that adversarial approximation errors in intermediate layers of functionally equivalent neural networks can manipulate outputs.

Result: The counterexample demonstrates that a neural network's output can be steered arbitrarily by approximation errors within specified bounds during individual layer computations.

Conclusion: The paper concludes that verifying layer computations with tolerances does not guarantee reliable final inference results, challenging this approach for zero-knowledge ML inference.

Abstract: A natural and informal approach to verifiable (or zero-knowledge) ML inference over floating-point data is: ``prove that each layer was computed correctly up to tolerance $δ$; therefore the final output is a reasonable inference result''. This short note gives a simple counterexample showing that this inference is false in general: for any neural network, we can construct a functionally equivalent network for which adversarially chosen approximation-magnitude errors in individual layer computations suffice to steer the final output arbitrarily (within a prescribed bounded range).

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [277] [Molecular Design beyond Training Data with Novel Extended Objective Functionals of Generative AI Models Driven by Quantum Annealing Computer](https://arxiv.org/abs/2602.15451)
*Hayato Kunugi,Mohsen Rahmani,Yosuke Iyama,Yutaro Hirono,Akira Suma,Matthew Woolway,Vladimir Vargas-Calderón,William Kim,Kevin Chern,Mohammad Amin,Masaru Tateno*

Main category: q-bio.QM

TL;DR: This paper presents a framework integrating a deep generative model with quantum annealing to design drug-like molecules with improved validity and drug-likeness.


<details>
  <summary>Details</summary>
Motivation: The lower occurrence of drug-like compounds in molecular generative models hinders drug discovery advancements.

Method: A framework combining deep generative models and D-Wave quantum annealing computers, using a Neural Hash Function to handle regularization and continuous-to-discrete signal transformations.

Result: The quantum-annealing-based generative model produced compounds with higher drug-likeness and validity than classical models and even outperformed training data.

Conclusion: Quantum annealing integration and novel neural network architecture enhance feature space sampling and drug design efficacy.

Abstract: Deep generative modeling to stochastically design small molecules is an emerging technology for accelerating drug discovery and development. However, one major issue in molecular generative models is their lower frequency of drug-like compounds. To resolve this problem, we developed a novel framework for optimization of deep generative models integrated with a D-Wave quantum annealing computer, where our Neural Hash Function (NHF) presented herein is used both as the regularization and binarization schemes simultaneously, of which the latter is for transformation between continuous and discrete signals of the classical and quantum neural networks, respectively, in the error evaluation (i.e., objective) function. The compounds generated via the quantum-annealing generative models exhibited higher quality in both validity and drug-likeness than those generated via the fully-classical models, and was further indicated to exceed even the training data in terms of drug-likeness features, without any restraints and conditions to deliberately induce such an optimization. These results indicated an advantage of quantum annealing to aim at a stochastic generator integrated with our novel neural network architectures, for the extended performance of feature space sampling and extraction of characteristic features in drug design.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [278] [Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction](https://arxiv.org/abs/2602.15484)
*Amartyaveer,Murali Kadambi,Chandra Mohan Sharma,Anupam Mondal,Prasanta Kumar Ghosh*

Main category: eess.AS

TL;DR: The paper proposes a bottleneck transformer architecture for nonintrusive STOI metric prediction, achieving better accuracy than existing models.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in traditional STOI calculation methods dependent on clean reference speech, and improve real-world applicability.

Method: Developing a bottleneck transformer architecture, combining convolution blocks for frame-level features with a multi-head self-attention layer for feature aggregation.

Result: The model demonstrated higher correlation and lower mean squared error compared to state-of-the-art approaches using SSL and spectral features.

Conclusion: The proposed architecture advances nonintrusive speech assessment metrics by achieving superior performance in both seen and unseen scenarios.

Abstract: In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. To address this, numerous deep learning-based nonintrusive speech assessment models have garnered significant interest. Many studies have achieved commendable performance, but there is room for further improvement.
  We propose the use of bottleneck transformer, incorporating convolution blocks for learning frame-level features and a multi-head self-attention (MHSA) layer to aggregate the information. These components enable the transformer to focus on the key aspects of the input data. Our model has shown higher correlation and lower mean squared error for both seen and unseen scenarios compared to the state-of-the-art model using self-supervised learning (SSL) and spectral features as inputs.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [279] [Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities](https://arxiv.org/abs/2602.15559)
*Gabriel Saco*

Main category: stat.ME

TL;DR: The paper addresses challenges in adaptive randomized experiments where treatment probabilities shift as data are collected. A new statistical approach ensures valid inference despite changing variance regimes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adaptive experiments risk statistical miscalibration due to non-deterministic variance limits when treatment probabilities change continuously.

Method: The authors leverage self-normalized martingale limit theory to develop a method where variance is estimated via realized quadratic variation, ensuring a robust Studentized statistic.

Result: The proposed method achieves asymptotic normality (N(0,1)) for statistical inference, even under changing variance regimes, as validated through simulations.

Conclusion: The approach provides reliable statistical inference for adaptive experiments and overcomes miscalibration issues of fixed-variance approaches, enhancing robustness for these studies.

Abstract: Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails.

</details>


### [280] [Scenario Approach with Post-Design Certification of User-Specified Properties](https://arxiv.org/abs/2602.15568)
*Algo Carè,Marco C. Campi,Simone Garatti*

Main category: stat.ME

TL;DR: The paper introduces a two-level framework for scenario approach design, ensuring both design and post-design reliability without additional test data.


<details>
  <summary>Details</summary>
Motivation: Enhance the existing scenario approach framework by guaranteeing reliability not only during design but also in post-design phases.

Method: Developed a two-level framework: baseline appropriateness for guiding design and post-design appropriateness for evaluating reliability, introducing distribution-free risk bounds and comprehensive methods for performance analysis.

Result: Provided distribution-free upper bounds and additional assumptions for lower bounds; demonstrated the methodology in H2 and pole-placement problems.

Conclusion: The proposed approach enhances scenario design with robust reliability post-design through theoretical and practical advancements.

Abstract: The scenario approach is an established data-driven design framework that comes equipped with a powerful theory linking design complexity to generalization properties. In this approach, data are simultaneously used both for design and for certifying the design's reliability, without resorting to a separate test dataset. This paper takes a step further by guaranteeing additional properties, useful in post-design usage but not considered during the design phase. To this end, we introduce a two-level framework of appropriateness: baseline appropriateness, which guides the design process, and post-design appropriateness, which serves as a criterion for a posteriori evaluation. We provide distribution-free upper bounds on the risk of failing to meet the post-design appropriateness; these bounds are computable without using any additional test data. Under additional assumptions, lower bounds are also derived. As part of an effort to demonstrate the usefulness of the proposed methodology, the paper presents two practical examples in H2 and pole-placement problems. Moreover, a method is provided to infer comprehensive distributional knowledge of relevant performance indexes from the available dataset.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [281] [Beyond Context Sharing: A Unified Agent Communication Protocol (ACP) for Secure, Federated, and Autonomous Agent-to-Agent (A2A) Orchestration](https://arxiv.org/abs/2602.15055)
*Naveen Kumar Krishnan*

Main category: cs.MA

TL;DR: The paper introduces the Agent Communication Protocol (ACP) to enable secure, decentralized cross-platform communication among autonomous agents, improving inter-agent coordination.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of decentralized, secure interaction among autonomous agents, crucial for realizing an interoperable ecosystem.

Method: The authors propose ACP, which provides standards for agent communication, using a federated orchestration model integrating identity verification, semantic intent mapping, and service agreements.

Result: The evaluation proves that ACP reduces communication latency among agents while maintaining a secure, zero-trust environment.

Conclusion: ACP represents progress in achieving scalable and interoperable communication for autonomous agents in diverse environments.

Abstract: In the artificial intelligence space, as we transition from isolated large language models to autonomous agents capable of complex reasoning and tool use. While foundational architectures and local context management protocols have been established, the challenge of cross-platform, decentralized, and secure interaction remains a significant barrier to the realization of a truly Agentic Web. Building upon the foundations of AI agent architectures and the Model Context Protocol (MCP) for multi-agent coordination, this paper introduces the Agent Communication Protocol (ACP). ACP provides a standardized framework for Agent-to-Agent (AA) interaction, enabling heterogeneous agents to discover, negotiate, and execute collaborative workflows across disparate environments. We propose a federated orchestration model that integrates decentralized identity verification, semantic intent mapping, and automated service-level agreements. Our evaluation demonstrates that ACP reduces inter-agent communication latency by % while maintaining a zero-trust security posture. This work represents a critical advancement toward a scalable and interoperable ecosystem of autonomous digital entities

</details>


### [282] [Colosseum: Auditing Collusion in Cooperative Multi-Agent Systems](https://arxiv.org/abs/2602.15198)
*Mason Nakamura,Abhinav Kumar,Saswat Das,Sahar Abdelnabi,Saaduddin Mahmud,Ferdinando Fioretto,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.MA

TL;DR: The paper introduces 'Colosseum,' a framework to study collusive behavior among LLM agents executing cooperative tasks in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: To address the safety concern of LLM agents colluding and pursuing secondary objectives, undermining the collective goal in multi-agent systems.

Method: The framework uses a Distributed Constraint Optimization Problem (DCOP) to model agent cooperation, measures collusion based on regret relative to the cooperative optimum, and conducts tests under various conditions (objectives, communication tactics, etc.).

Result: Most LLM models tested exhibited collusive tendencies, particularly when given secret communication channels. However, actions often did not align with collusion plans, limiting actual task impact.

Conclusion: Colosseum creates a structured approach to evaluate and understand collusion dynamics, bridging communication and action in multi-agent settings.

Abstract: Multi-agent systems, where LLM agents communicate through free-form language, enable sophisticated coordination for solving complex cooperative tasks. This surfaces a unique safety problem when individual agents form a coalition and \emph{collude} to pursue secondary goals and degrade the joint objective. In this paper, we present Colosseum, a framework for auditing LLM agents' collusive behavior in multi-agent settings. We ground how agents cooperate through a Distributed Constraint Optimization Problem (DCOP) and measure collusion via regret relative to the cooperative optimum. Colosseum tests each LLM for collusion under different objectives, persuasion tactics, and network topologies. Through our audit, we show that most out-of-the-box models exhibited a propensity to collude when a secret communication channel was artificially formed. Furthermore, we discover ``collusion on paper'' when agents plan to collude in text but would often pick non-collusive actions, thus providing little effect on the joint task. Colosseum provides a new way to study collusion by measuring communications and actions in rich yet verifiable environments.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [283] [Fluids You Can Trust: Property-Preserving Operator Learning for Incompressible Flows](https://arxiv.org/abs/2602.15472)
*Ramansh Sharma,Matthew Lowery,Houman Owhadi,Varun Shankar*

Main category: physics.flu-dyn

TL;DR: The paper introduces a kernel-based operator learning method that ensures physical properties for incompressible flow simulations, outperforming neural operators in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Simulation of incompressible flows is computationally intensive, and current operator learning approaches do not adequately enforce physical properties like incompressibility.

Method: The paper proposes a technique that maps input functions to output functions using property-preserving kernel bases, guaranteeing physical constraints like incompressibility and turbulence.

Result: The method achieves up to six orders of magnitude better accuracy and five orders of magnitude faster training compared to neural operators.

Conclusion: The method serves as a highly accurate and efficient surrogate for modeling incompressible flows, addressing core limitations of existing neural operators.

Abstract: We present a novel property-preserving kernel-based operator learning method for incompressible flows governed by the incompressible Navier-Stokes equations. Traditional numerical solvers incur significant computational costs to respect incompressibility. Operator learning offers efficient surrogate models, but current neural operators fail to exactly enforce physical properties such as incompressibility, periodicity, and turbulence. Our method maps input functions to expansion coefficients of output functions in a property-preserving kernel basis, ensuring that predicted velocity fields analytically and simultaneously preserve the aforementioned physical properties. We evaluate the method on challenging 2D and 3D, laminar and turbulent, incompressible flow problems. Our method achieves up to six orders of magnitude lower relative $\ell_2$ errors upon generalization and trains up to five orders of magnitude faster compared to neural operators. Moreover, while our method enforces incompressibility analytically, neural operators exhibit very large deviations. Our results show that our method provides an accurate and efficient surrogate for incompressible flows.

</details>


### [284] [Uni-Flow: a unified autoregressive-diffusion model for complex multiscale flows](https://arxiv.org/abs/2602.15592)
*Xiao Xue,Tianyue Yang,Mingyang Gao,Leyu Pan,Maida Wang,Kewei Zhu,Shuo Wang,Jiuling Li,Marco F. P. ten Eikelder,Peter V. Coveney*

Main category: physics.flu-dyn

TL;DR: This paper introduces Uni-Flow, a novel framework for modeling complex spatiotemporal dynamics by separating temporal and spatial processes, excelling in long-term and fine-scale flow predictions.


<details>
  <summary>Details</summary>
Motivation: Modeling multiscale spatiotemporal dynamics is essential to understand phenomena across disciplines, but current machine learning approaches struggle with long-term stability and fine-scale resolution, especially in complex, chaotic systems.

Method: Uni-Flow uses a unified framework combining autoregressive components for coarse temporal dynamics and diffusion processes for fine spatial details, enabling efficient modeling of complex systems.

Result: Uni-Flow demonstrates superior performance on benchmarks such as two-dimensional Kolmogorov flow, three-dimensional turbulent channel flow, and personalized cardiovascular simulations with faster-than-real-time predictions.

Conclusion: Uni-Flow offers a new pathway for faster-than-real-time simulations of multiscale flows, transforming high-fidelity simulations into deployable surrogates for applications in science and engineering research.

Abstract: Spatiotemporal flows govern diverse phenomena across physics, biology, and engineering, yet modelling their multiscale dynamics remains a central challenge. Despite major advances in physics-informed machine learning, existing approaches struggle to simultaneously maintain long-term temporal evolution and resolve fine-scale structure across chaotic, turbulent, and physiological regimes. Here, we introduce Uni-Flow, a unified autoregressive-diffusion framework that explicitly separates temporal evolution from spatial refinement for modelling complex dynamical systems. The autoregressive component learns low-resolution latent dynamics that preserve large-scale structure and ensure stable long-horizon rollouts, while the diffusion component reconstructs high-resolution physical fields, recovering fine-scale features in a small number of denoising steps. We validate Uni-Flow across canonical benchmarks, including two-dimensional Kolmogorov flow, three-dimensional turbulent channel inflow generation with a quantum-informed autoregressive prior, and patient-specific simulations of aortic coarctation derived from high-fidelity lattice Boltzmann hemodynamic solvers. In the cardiovascular setting, Uni-Flow enables task-level faster than real-time inference of pulsatile hemodynamics, reconstructing high-resolution pressure fields over physiologically relevant time horizons in seconds rather than hours. By transforming high-fidelity hemodynamic simulation from an offline, HPC-bound process into a deployable surrogate, Uni-Flow establishes a pathway to faster-than-real-time modelling of complex multiscale flows, with broad implications for scientific machine learning in flow physics.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [285] [SCENE OTA-FD: Self-Centering Noncoherent Estimator for Over-the-Air Federated Distillation](https://arxiv.org/abs/2602.15326)
*Hao Chen,Zavareh Bozorgasl*

Main category: eess.SP

TL;DR: SCENE is a pilot-free and phase-invariant aggregation method for over-the-air federated distillation, enabling unbiased aggregation without uplink pilots and hardware-friendly use.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in short-coherence and hardware-constrained systems, proposing a solution to avoid requiring per-round channel state information (CSI) while maintaining efficient and unbiased aggregation.

Method: The authors developed SCENE, a system where devices map soft-label vectors to nonnegative transmit energies and implemented a self-centering estimator at the server to process data without pilot signals.

Result: SCENE achieves unbiased weighted soft-label average estimation with reduced variance while avoiding uplink pilots and ensuring hardware-friendly transmissions.

Conclusion: SCENE trades minor increases in variance for pilot-free, efficient, and hardware-compatible aggregation, making it suitable for constrained communication regimes, and can outperform traditional methods with significant pilot overhead.

Abstract: We propose SCENE (Self-Centering Noncoherent Estimator), a pilot-free and phase-invariant aggregation primitive for over-the-air federated distillation (OTA-FD). Each device maps its soft-label (class-probability) vector to nonnegative transmit energies under constant per-round power and constant-envelope signaling (PAPR near 1). At the server, a self-centering energy estimator removes the noise-energy offset and yields an unbiased estimate of the weighted soft-label average, with variance decaying on the order of 1/(SM) in the number of receive antennas M and repetition factor S. We also develop a pilot-free ratio-normalized variant that cancels unknown large-scale gains, provide a convergence bound consistent with coherent OTA-FD analyses, and present an overhead-based crossover comparison. SCENE targets short-coherence and hardware-constrained regimes, where avoiding per-round CSI is essential: it trades a modest noncoherent variance constant for zero uplink pilots, unbiased aggregation, and hardware-friendly transmission, and can outperform coherent designs when pilot overhead is non-negligible.

</details>


### [286] [Transforming Computational Lithography with AC and AI -- Faster, More Accurate, and Energy-efficient](https://arxiv.org/abs/2602.15036)
*Saumyadip Mukhopadhyay,Kiho Yang,Kasyap Thottasserymana Vasudevan,Mounica Jyothi Divvela,Selim Dogru,Dilip Krishnamurthy,Fergo Treska,Werner Gillijns,Ryan Ryoung han Kim,Kumara Sastry,Vivek Singh*

Main category: eess.SP

TL;DR: The paper discusses how accelerated computing and AI can significantly improve computational lithography, achieving a 57X speed-up and showcasing benefits like better process resilience and precision in semiconductor manufacturing.


<details>
  <summary>Details</summary>
Motivation: Demand for more computational capabilities in scientific and semiconductor workloads has skyrocketed due to the need for larger, more accurate models and simulations, surpassing transistor scaling advancements.

Method: The authors propose using accelerated computing and AI to redesign computational lithography software, focusing on core primitives and optimizing calculations.

Result: They achieved a 57X acceleration in computational lithography processes and demonstrated improvements in outcomes like a 35% better process window and 19% better edge placement error.

Conclusion: By integrating accelerated computing and AI, it is possible to create sustainable and efficient solutions for heavy scientific workloads, marking a transformative step in semiconductor manufacturing and other fields.

Abstract: From climate science to drug discovery, scientific computing demands have surged dramatically in recent years -- driven by larger datasets, more sophisticated models, and higher simulation fidelity. This growth rate far outpaces transistor scaling, leading to unsustainably rising costs, energy consumption, and emissions. Semiconductor manufacturing is no exception. Computational lithography -- involving transferring circuitry to silicon in diffraction-limited conditions -- is the largest workload in semiconductor manufacturing. It has also grown exceptionally complex as miniaturization has advanced in the angstrom-era, requiring more accurate modeling, intricate corrections, and broader solution-space exploration. Accelerated computing (AC) offers a solution by dramatically freeing up the compute and power envelope. AI augments these gains by serving as high-fidelity surrogates for compute-intensive steps. Together, they present a sustainable, next-generation computing platform for scientific workloads. This new paradigm needs a fundamental redesign of the software stack. For computational lithography, NVIDIA cuLitho reinvents the core primitives -- diffractive optics, computational geometry, multi-variant optimization, data processing -- to achieve a transformative 57X end-to-end acceleration. Beyond dramatically faster cycles, this expanded compute envelope enables more rigorous solutions, including curvilinear masks, high-numerical aperture extreme ultraviolet (high-NA EUV) lithography, and subatomic modeling. We reinvest a small fraction of the freed-up compute to include through-focus correction for better process resilience. Silicon experiments at IMEC show significant benefits compared to conventional methods -- 35% better process window and 19% better edge placement error. This is the first quantified chip-scale demonstration of the lithography benefits of AC and AI in silicon.

</details>


### [287] [Combining scEEG and PPG for reliable sleep staging using lightweight wearables](https://arxiv.org/abs/2602.15042)
*Jiawei Wang,Liang Xu,Shuntian Zheng,Yu Guan,Kaichen Wang,Ziqing Zhang,Chen Chen,Laurence T. Yang,Sai Gu*

Main category: eess.SP

TL;DR: The paper presents a novel scEEG-PPG data fusion method for accurate 4-class sleep staging on lightweight wearables within short windows.


<details>
  <summary>Details</summary>
Motivation: Sleep staging using lightweight devices is limited by individual drawbacks of scEEG (poor light-sleep classification) and PPG (dependent on full-night data).

Method: The authors propose fusion of scEEG and PPG with three strategies: score-level fusion, cross-attention fusion, and Mamba-enhanced fusion.

Result: Mamba-enhanced fusion showed best performance with Cohen’s Kappa κ = 0.798 and significant improvement in light sleep detection, generalizing well across datasets.

Conclusion: This fusion approach is effective for wearable sleep monitoring, paving the way for real-time, accessible sleep health assessments.

Abstract: Reliable sleep staging remains challenging for lightweight wearable devices such as single-channel electroencephalography (scEEG) or photoplethysmography (PPG). scEEG offers direct measurement of cortical activity and serves as the foundation for sleep staging, yet exhibits limited performance on light sleep stages. PPG provides a low-cost complement that captures autonomic signatures effective for detecting light sleep. However, prior PPG-based methods rely on full night recordings (8 - 10 hours) as input context, which is less practical to provide timely feedback for sleep intervention. In this work, we investigate scEEG-PPG fusion for 4-class sleep staging under short-window (30 s - 30 min) constraints. First, we evaluate the temporal context required for each modality, to better understand the relationship of sleep staging performance with respect to monitoring window. Second, we investigate three fusion strategies: score-level fusion, cross-attention fusion enabling feature-level interactions, and Mamba-enhanced fusion incorporating temporal context modeling. Third, we train and evaluate on the Multi-Ethnic Study of Atherosclerosis (MESA) dataset and perform cross-dataset validation on the Cleveland Family Study (CFS) and the Apnea, Bariatric surgery, and CPAP (ABC) datasets. The Mamba-enhanced fusion achieves the best performance on MESA (Cohen's Kappa $κ$ = 0.798, Acc = 86.9%), with particularly notable improvement in light sleep classification (F1-score: 85.63% vs. 77.76%, recall: 82.85% vs. 69.95% for scEEG alone), and generalizes well to CFS and ABC datasets with different populations. These findings suggest that scEEG-PPG fusion is a promising approach for lightweight wearable based sleep monitoring, offering a pathway toward more accessible sleep health assessment. Source code of this project can be found at: https://github.com/DavyWJW/scEEG-PPGFusion

</details>


### [288] [Accurate 2D Reconstruction for PET Scanners based on the Analytical White Image Model](https://arxiv.org/abs/2306.17652)
*Tomislav Matulić,Damir Seršić*

Main category: eess.SP

TL;DR: This paper develops a mathematical model for crystal-to-crystal response that compensates for PET scanner limitations and proposes a modified MLEM algorithm for improved image reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address the physical limitations of PET scanners and improve image reconstruction by compensating for the crystal-to-crystal response.

Method: A closed-form mathematical model and approximations for crystal-to-crystal response are developed. A modified MLEM algorithm integrating the compensation model is proposed.

Result: The modified MLEM algorithm, tested on synthetic and real data with a Raytest ClearPET camera, outperforms non-compensated reconstruction methods.

Conclusion: The proposed compensation model and modified MLEM algorithm provide accurate and effective image reconstruction for PET scanners, with significant improvement over existing methods.

Abstract: In this paper, we provide a precise mathematical model of crystal-to-crystal response which is used to generate the white image - a necessary compensation model needed to overcome the physical limitations of the PET scanner. We present a closed-form solution, as well as several accurate approximations, due to the complexity of the exact mathematical expressions. We prove, experimentally and analytically, that the difference between the best approximations and real crystal-to-crystal response is insignificant. The obtained responses are used to generate the white image compensation model. It can be written as a single closed-form expression making it easy to implement in known reconstruction methods. The maximum likelihood expectation maximization (MLEM) algorithm is modified and our white image model is integrated into it. The modified MLEM algorithm is not based on the system matrix, rather it is based on ray-driven projections and back-projections. The compensation model provides all necessary information about the system. Finally, we check our approach on synthetic and real data. For the real-world acquisition, we use the Raytest ClearPET camera for small animals and the NEMA NU 4-2008 phantom. The proposed approach overperforms competitive, non-compensated reconstruction methods.

</details>


### [289] [Latency-aware Human-in-the-Loop Reinforcement Learning for Semantic Communications](https://arxiv.org/abs/2602.15640)
*Peizheng Li,Xinyi Lin,Adnan Aijaz*

Main category: eess.SP

TL;DR: This paper proposes a reinforcement learning framework (TC-HITL-RL) to enhance semantic communication by integrating latency guarantees, human feedback, and semantic fidelity within a radio network environment.


<details>
  <summary>Details</summary>
Motivation: To balance semantic fidelity with stringent latency requirements in immersive and safety-critical services, especially in semantic communication systems.

Method: The authors design a time-constrained human-in-the-loop reinforcement learning framework based on constrained Markov decision processes (CMDP). They incorporate feedback, state variables, and reward shaping into a latency-aware Open RAN architecture using a primal-dual proximal policy optimization algorithm.

Result: Simulations on multi-user links with varying deadlines demonstrate that the method meets timing constraints, surpasses baseline schedulers in rewards, and improves resource management stability.

Conclusion: The introduced framework effectively balances semantic accuracy and latency requirements, presenting a viable solution for latency-sensitive semantic communication systems.

Abstract: Semantic communication promises task-aligned transmission but must reconcile semantic fidelity with stringent latency guarantees in immersive and safety-critical services. This paper introduces a time-constrained human-in-the-loop reinforcement learning (TC-HITL-RL) framework that embeds human feedback, semantic utility, and latency control within a semantic-aware Open radio access network (RAN) architecture. We formulate semantic adaptation driven by human feedback as a constrained Markov decision process (CMDP) whose state captures semantic quality, human preferences, queue slack, and channel dynamics, and solve it via a primal--dual proximal policy optimization algorithm with action shielding and latency-aware reward shaping. The resulting policy preserves PPO-level semantic rewards while tightening the variability of both air-interface and near-real-time RAN intelligent controller processing budgets. Simulations over point-to-multipoint links with heterogeneous deadlines show that TC-HITL-RL consistently meets per-user timing constraints, outperforms baseline schedulers in reward, and stabilizes resource consumption, providing a practical blueprint for latency-aware semantic adaptation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [290] [ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction](https://arxiv.org/abs/2602.15189)
*William Brach,Francesco Zuppichini,Marco Vinciguerra,Lorenzo Padoan*

Main category: cs.IR

TL;DR: This paper introduces ScrapeGraphAI-100k, a large-scale dataset designed for web information extraction, containing real-world extraction events and diverse domains.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing datasets, which are typically small, synthetic, or lack the structural context critical for web-based information extraction.

Method: The authors collected 9M extraction events, deduplicated and balanced them across schemas, resulting in 93,695 structured examples. They included diverse data formats and metadata, analyzing diversity and failure patterns.

Result: Fine-tuning a small language model (1.7B parameters) on this dataset reduces the performance gap compared to larger models (30B parameters), demonstrating the dataset’s effectiveness.

Conclusion: ScrapeGraphAI-100k is a valuable public resource for improving small language models, benchmarking extraction, and advancing web information retrieval.

Abstract: The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We also provide a fine-tuning experiment showing that a small language model (1.7B) trained on a subset narrows the gap to larger baselines (30B), underscoring the datasets utility for efficient extraction. ScrapeGraphAI-100k enables fine-tuning small models, benchmarking structured extraction, and studying schema induction for web IR indexing, and is publicly available on HuggingFace.

</details>


### [291] [Automatic Funny Scene Extraction from Long-form Cinematic Videos](https://arxiv.org/abs/2602.15381)
*Sibendu Paul,Haotian Jiang,Caren Chen*

Main category: cs.IR

TL;DR: The paper introduces an end-to-end system to extract humorous scenes from cinematic titles, achieving high accuracy in localization and humor tagging.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently extract engaging humorous content from long cinematic titles for improved viewer experiences on streaming platforms.

Method: Combines visual and textual cues for scene segmentation, guided triplet mining for shot representation, and a multimodal humor tagging framework using audio and text.

Result: Achieved 18.3% AP improvement in scene detection and an F1 score of 0.834 for humor detection in text. 87% of extracted clips are humorous, and 98% of scenes are localized accurately.

Conclusion: The system successfully enhances content creation workflows and boosts user engagement by automating humorous scene extraction from diverse cinematic formats.

Abstract: Automatically extracting engaging and high-quality humorous scenes from cinematic titles is pivotal for creating captivating video previews and snackable content, boosting user engagement on streaming platforms. Long-form cinematic titles, with their extended duration and complex narratives, challenge scene localization, while humor's reliance on diverse modalities and its nuanced style add further complexity. This paper introduces an end-to-end system for automatically identifying and ranking humorous scenes from long-form cinematic titles, featuring shot detection, multimodal scene localization, and humor tagging optimized for cinematic content. Key innovations include a novel scene segmentation approach combining visual and textual cues, improved shot representations via guided triplet mining, and a multimodal humor tagging framework leveraging both audio and text. Our system achieves an 18.3% AP improvement over state-of-the-art scene detection on the OVSD dataset and an F1 score of 0.834 for detecting humor in long text. Extensive evaluations across five cinematic titles demonstrate 87% of clips extracted by our pipeline are intended to be funny, while 98% of scenes are accurately localized. With successful generalization to trailers, these results showcase the pipeline's potential to enhance content creation workflows, improve user engagement, and streamline snackable content generation for diverse cinematic media formats.

</details>


### [292] [GaiaFlow: Semantic-Guided Diffusion Tuning for Carbon-Frugal Search](https://arxiv.org/abs/2602.15423)
*Rong Fu,Wenxin Zhang,Jia Yee Tan,Chunlei Meng,Shuo Yin,Xiaowen Ma,Wangyu Wu,Muge Qi,Guangzhen Yao,Zhaolu Kang,Zeli Su,Simon Fong*

Main category: cs.IR

TL;DR: GaiaFlow is introduced as a sustainable framework for carbon-efficient neural search, optimizing precision while reducing computational emissions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the increasing environmental impact of high-power neural rankers in information retrieval, emphasizing the need for models that balance ecological sustainability with high accuracy.

Method: GaiaFlow employs semantic-guided diffusion tuning, retrieval-guided Langevin dynamics, and hardware-independent performance modeling, coupled with adaptive early exit protocols and quantized inference to reduce carbon emissions while ensuring retrieval quality.

Result: GaiaFlow demonstrated a superior balance between search precision and energy efficiency across diverse computing infrastructures in experimental evaluations.

Conclusion: GaiaFlow provides a scalable solution for sustainable, next-generation neural search systems, significantly reducing carbon footprints without compromising retrieval accuracy.

Abstract: As the burgeoning power requirements of sophisticated neural architectures escalate, the information retrieval community has recognized ecological sustainability as a pivotal priority that necessitates a fundamental paradigm shift in model design. While contemporary neural rankers have attained unprecedented accuracy, the substantial environmental externalities associated with their computational intensity often remain overlooked in large-scale deployments. We present GaiaFlow, an innovative framework engineered to facilitate carbon-frugal search by operationalizing semantic-guided diffusion tuning. Our methodology orchestrates the convergence of retrieval-guided Langevin dynamics and a hardware-independent performance modeling strategy to optimize the trade-off between search precision and environmental preservation. By incorporating adaptive early exit protocols and precision-aware quantized inference, the proposed architecture significantly mitigates operational carbon footprints while maintaining robust retrieval quality across heterogeneous computing infrastructures. Extensive experimental evaluations demonstrate that GaiaFlow achieves a superior equilibrium between effectiveness and energy efficiency, offering a scalable and sustainable pathway for next-generation neural search systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [293] [Proactive Conversational Assistant for a Procedural Manual Task based on Audio and IMU](https://arxiv.org/abs/2602.15707)
*Rehana Mahfuz,Yinyi Guo,Erik Visser,Phanidhar Chinchili*

Main category: cs.MM

TL;DR: This paper presents a privacy-preserving, real-time conversational assistant for procedural tasks, using audio and IMU inputs from wearable devices instead of video, ensuring efficiency and privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a real-time conversational assistant for guiding users in procedural tasks without relying on computationally intensive and privacy-invasive video inputs.

Method: The method involves using audio and IMU data from wearable devices for context understanding and designing a UWA LoRA finetuning to suppress unnecessary dialogues from the language model while preserving essential guidance.

Result: The UWA LoRA method improves dialogue quality with a >30% F-score improvement, reduces computational costs with a 16x speedup, and enables edge-device implementation without cloud reliance.

Conclusion: The approach demonstrates the feasibility of a conversational assistant offering effective guidance for procedural tasks using privacy-friendly modalities, achieving both efficiency and user-friendliness without compromising privacy.

Abstract: Real-time conversational assistants for procedural tasks often depend on video input, which can be computationally expensive and compromise user privacy. For the first time, we propose a real-time conversational assistant that provides comprehensive guidance for a procedural task using only lightweight privacy-preserving modalities such as audio and IMU inputs from a user's wearable device to understand the context. This assistant proactively communicates step-by-step instructions to a user performing a furniture assembly task, and answers user questions. We construct a dataset containing conversations where the assistant guides the user in performing the task. On observing that an off-the-shelf language model is a very talkative assistant, we design a novel User Whim Agnostic (UWA) LoRA finetuning method which improves the model's ability to suppress less informative dialogues, while maintaining its tendency to communicate important instructions. This leads to >30% improvement in the F-score. Finetuning the model also results in a 16x speedup by eliminating the need to provide in-context examples in the prompt. We further describe how such an assistant is implemented on edge devices with no dependence on the cloud.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [294] [StrokeNeXt: A Siamese-encoder Approach for Brain Stroke Classification in Computed Tomography Imagery](https://arxiv.org/abs/2602.15087)
*Leo Thomas Ramos,Angel D. Sappa*

Main category: eess.IV

TL;DR: StrokeNeXt is a dual-branch model designed for stroke classification using 2D CT images, significantly outperforming other baselines in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance stroke classification in CT images, addressing both detection and subtype classification, while improving accuracy, robustness, and inference time in a clinical context.

Method: StrokeNeXt incorporates two ConvNeXt encoders with feature fusion via a lightweight convolutional decoder composed of stacked 1D operations and a compact classification head.

Result: StrokeNeXt achieves up to 0.988 accuracy and F1-scores, outperforms baselines significantly, displays robust diagnostic performance, reduces prediction errors, and achieves low misclassification rates and efficient inference times.

Conclusion: StrokeNeXt is a superior model for stroke classification with statistically significant improvements, low errors, and fast processing, making it promising for clinical application.

Abstract: We present StrokeNeXt, a model for stroke classification in 2D Computed Tomography (CT) images. StrokeNeXt employs a dual-branch design with two ConvNeXt encoders, whose features are fused through a lightweight convolutional decoder based on stacked 1D operations, including a bottleneck projection and transformation layers, and a compact classification head. The model is evaluated on a curated dataset of 6,774 CT images, addressing both stroke detection and subtype classification between ischemic and hemorrhage cases. StrokeNeXt consistently outperforms convolutional and Transformer-based baselines, reaching accuracies and F1-scores of up to 0.988. Paired statistical tests confirm that the performance gains are statistically significant, while class-wise sensitivity and specificity demonstrate robust behavior across diagnostic categories. Calibration analysis shows reduced prediction error compared to competing methods, and confusion matrix results indicate low misclassification rates. In addition, the model exhibits low inference time and fast convergence.

</details>


### [295] [Benchmarking Self-Supervised Models for Cardiac Ultrasound View Classification](https://arxiv.org/abs/2602.15339)
*Youssef Megahed,Salma I. Megahed,Robin Ducharme,Inok Lee,Adrian D. C. Chan,Mark C. Walker,Steven Hawken*

Main category: eess.IV

TL;DR: This paper evaluates and compares two self-supervised learning frameworks, USF-MAE and MoCo v3, for automated cardiac ultrasound image classification on the CACTUS dataset, finding that USF-MAE consistently outperforms MoCo v3.


<details>
  <summary>Details</summary>
Motivation: To improve automated cardiac ultrasound image classification by leveraging self-supervised learning on a large annotated dataset, enabling more accurate and reliable clinical diagnoses.

Method: The authors implemented two self-supervised learning models, USF-MAE and MoCo v3, with identical training protocols and 5-fold cross-validation on the CACTUS dataset, recording performance metrics (e.g., ROC-AUC, accuracy, F1-score).

Result: USF-MAE consistently outperformed MoCo v3 across all metrics. USF-MAE achieved a higher mean testing accuracy (99.33%) and AUC (99.99%) compared to MoCo v3, with statistically significant improvements.

Conclusion: The study concludes that USF-MAE is more effective at learning discriminative features for automated cardiac view classification, indicating its potential to enhance medical imaging applications.

Abstract: Reliable interpretation of cardiac ultrasound images is essential for accurate clinical diagnosis and assessment. Self-supervised learning has shown promise in medical imaging by leveraging large unlabelled datasets to learn meaningful representations. In this study, we evaluate and compare two self-supervised learning frameworks, USF-MAE, developed by our team, and MoCo v3, on the recently introduced CACTUS dataset (37,736 images) for automated simulated cardiac view (A4C, PL, PSAV, PSMV, Random, and SC) classification. Both models used 5-fold cross-validation, enabling robust assessment of generalization performance across multiple random splits. The CACTUS dataset provides expert-annotated cardiac ultrasound images with diverse views. We adopt an identical training protocol for both models to ensure a fair comparison. Both models are configured with a learning rate of 0.0001 and a weight decay of 0.01. For each fold, we record performance metrics including ROC-AUC, accuracy, F1-score, and recall. Our results indicate that USF-MAE consistently outperforms MoCo v3 across metrics. The average testing AUC for USF-MAE is 99.99% (+/-0.01% 95% CI), compared to 99.97% (+/-0.01%) for MoCo v3. USF-MAE achieves a mean testing accuracy of 99.33% (+/-0.18%), higher than the 98.99% (+/-0.28%) reported for MoCo v3. Similar trends are observed for the F1-score and recall, with improvements statistically significant across folds (paired t-test, p=0.0048 < 0.01). This proof-of-concept analysis suggests that USF-MAE learns more discriminative features for cardiac view classification than MoCo v3 when applied to this dataset. The enhanced performance across multiple metrics highlights the potential of USF-MAE for improving automated cardiac ultrasound classification.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [296] [TokaMind: A Multi-Modal Transformer Foundation Model for Tokamak Plasma Dynamics](https://arxiv.org/abs/2602.15084)
*Tobia Boschi,Andrea Loreti,Nicola C. Amorisco,Rodrigo H. Ordonez-Hurtado,Cécile Rousseau,George K. Holt,Eszter Székely,Alexander Whittle,Samuel Jackson,Adriano Agnello,Stanislas Pamela,Alessandra Pascale,Robert Akers,Juan Bernabe Moreno,Vassil Alexandrov,Mykhaylo Zayats*

Main category: physics.plasm-ph

TL;DR: TokaMind is an open-source Multi-Modal Transformer (MMT)-based framework for tokamak plasma modeling, outperforming benchmarks in many tasks using multi-modal pretraining.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for efficient data modeling in tokamak plasma physics by leveraging multi-modal and pretraining strategies to accommodate complex, diverse data types.

Method: TokaMind integrates a Multi-Modal Transformer trained on the MAST dataset, uses Discrete Cosine Transform embeddings for multi-modal signals, and allows efficient fine-tuning through selective loading and freezing of model components.

Result: TokaMind demonstrates superior performance over baseline on the MAST benchmark (TokaMark) for most tasks, and lightweight fine-tuning outperforms full training on matched epochs for certain tasks.

Conclusion: The study validates multi-modal pretraining's effectiveness for tokamak plasma modeling and introduces TokaMind as a scalable, extensible foundation for future fusion research.

Abstract: We present TokaMind, an open-source foundation model framework for fusion plasma modeling, based on a Multi-Modal Transformer (MMT) and trained on heterogeneous tokamak diagnostics from the publicly available MAST dataset. TokaMind supports multiple data modalities (time-series, 2D profiles, and videos) with different sampling rates, robust missing-signal handling, and efficient task adaptation via selectively loading and freezing four model components. To represent multi-modal signals, we use a training-free Discrete Cosine Transform embedding (DCT3D) and provide a clean interface for alternative embeddings (e.g., Variational Autoencoders - VAEs). We evaluate TokaMind on the recently introduced MAST benchmark TokaMark, comparing training and embedding strategies. Our results show that fine-tuned TokaMind outperforms the benchmark baseline on all but one task, and that, for several tasks, lightweight fine-tuning yields better performance than training the same architecture from scratch under a matched epoch budget. These findings highlight the benefits of multi-modal pretraining for tokamak plasma dynamics and provide a practical, extensible foundation for future fusion modeling tasks. Training code and model weights will be made publicly available.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [297] [A golden-ratio partition of information and the balance between prediction and surprise: a neuro-cognitive route to antifragility](https://arxiv.org/abs/2602.15266)
*Pablo Padilla,Oliver López-Corona,Elvia Ramírez-Carrillo,Ariadne Hernández Sánchez*

Main category: math.DS

TL;DR: The paper proposes a balance function to measure the trade-off between prediction and surprise and identifies two key informational metrics, tied to adaptive systems' capacity for criticality and antifragile adaptation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and model how adaptive systems balance prediction and surprise to thrive in uncertain environments.

Method: The authors developed an information-theoretic balance function and used mathematical models including the golden ratio to identify critical regimes for adaptive systems.

Result: The results highlight the critical point at $p^* \approx 0.882$ for maximal informational vulnerability, and $p = 1/\varphi \approx 0.618$ as a structurally privileged partition guiding adaptation.

Conclusion: The golden-ratio partition connects prediction, surprise, and antifragile adaptation, providing insights into adaptive system dynamics at criticality.

Abstract: Adaptive systems must strike a balance between prediction and surprise to thrive in uncertain environments. We propose an information-theoretic balance function, $ f(p) = -(1 - p)\ln(1 - p) + \ln p $, which quantifies the net informational gain from contrasting explained variance $p$ with unexplained novelty $(1 - p)$. This function is strictly concave on $(0,1)$ and reaches its unique maximum at $ p^* \approx 0.882$, revealing a regime where confidence is high but the residual uncertainty carries a disproportionate potential for surprise.
  Independently of this maximum, imposing a self-similarity condition between known, unknown and total information, $p : (1-p) = 1 : p$, leads to the golden-ratio reciprocal $p = 1/\varphi \approx 0.618$, where $ \varphi$ is the golden ratio. We interpret this value not as the maximizer of $f$, but as a structurally privileged \emph{partition} in which known and unknown are proportionally nested across scales.
  Embedding this dual structure into a Compute-Inference-Model-Action (CIMA) loop yields a dynamic process that maintains the system near a critical regime where prediction and surprise coexist. At this edge, neuronal dynamics exhibit power-law structure and maximal dynamic range, while the system's response to perturbations becomes convex at the level of its payoff function-fulfilling the formal definition of antifragility. We suggest that the golden-ratio partition is not merely a mathematical artifact, but a candidate design principle linking prediction, surprise, criticality, and antifragile adaptation across scales and domains, while the maximum of $f$ identifies the point of greatest informational vulnerability to being wrong.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [298] [High Convergence Rates of CMOS Invertible Logic Circuits Based on Many-Body Hamiltonians](https://arxiv.org/abs/2602.15033)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.ET

TL;DR: This paper presents CMOS invertible-logic (CIL) circuits that utilize many-body Hamiltonians for probabilistic forward and backward operations, achieving higher convergence rates with minimal hardware overhead.


<details>
  <summary>Details</summary>
Motivation: To improve the problem-solving performance of probabilistic circuits by simplifying Hamiltonian landscapes using three-body interactions, allowing for better global energy minimum convergence.

Method: The researchers designed Hamiltonians with three-body spin interactions for lower-energy landscape complexity and implemented them using stochastic computing in CIL circuits, comparing their performance to traditional two-body versions.

Result: The three-body CIL circuits demonstrated several times higher convergence rates compared to two-body circuits, with negligible area overhead when deployed on FPGA.

Conclusion: Three-body interactions simplify Hamiltonian complexity, enabling improved convergence rates in CIL circuits while maintaining hardware efficiency.

Abstract: This paper introduces CMOS invertible-logic (CIL) circuits based on many-body Hamiltonians. CIL can realize probabilistic forward and backward operations of a function by annealing a corresponding Hamiltonian using stochastic computing. We have created a Hamiltonian that includes three-body interaction of spins (probabilistic nodes). It provides some degrees of freedom to design a simpler landscape of Hamiltonian (energy) than that of the conventional two-body Hamiltonian. The simpler landscape makes it easier to reach the global minimum energy. The proposed three-body CIL circuits are designed and evaluated with the conventional two-body CIL circuits, resulting in few-times higher convergence rates with negligible area overhead on FPGA.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [299] [High-Fidelity Network Management for Federated AI-as-a-Service: Cross-Domain Orchestration](https://arxiv.org/abs/2602.15281)
*Merve Saimler,Mohaned Chraiti,Ozgur Ercetin*

Main category: cs.NI

TL;DR: This paper proposes an AI-as-a-Service (AIaaS) management framework using Tail-Risk Envelopes (TREs), providing improved service assurance and tenant isolation in multi-domain AIaaS scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in assuring high fidelity and reliability of AIaaS under multi-domain federations, particularly when impacted by communication and inference impairments.

Method: An assurance-oriented AIaaS management plane using Tail-Risk Envelopes (TREs) was proposed. TREs include deterministic guardrails and stochastic rate-latency-impairment models, coupled with stochastic network calculus to derive delay violation bounds and risk-budget optimization.

Result: Simulation results show that TRE-based contracts enhance tail latency compliance under overload conditions and ensure tenant isolation, even amidst bursty and correlated traffic.

Conclusion: The proposed TRE framework offers effective mechanisms for achieving reliable, accountable, and performant AIaaS across federated multi-domain environments.

Abstract: To support the emergence of AI-as-a-Service (AIaaS), communication service providers (CSPs) are on the verge of a radical transformation-from pure connectivity providers to AIaaS a managed network service (control-and-orchestration plane that exposes AI models). In this model, the CSP is responsible not only for transport/communications, but also for intent-to-model resolution and joint network-compute orchestration, i.e., reliable and timely end-to-end delivery. The resulting end-to-end AIaaS service thus becomes governed by communications impairments (delay, loss) and inference impairments (latency, error). A central open problem is an operational AIaaS control-and-orchestration framework that enforces high fidelity, particularly under multi-domain federation. This paper introduces an assurance-oriented AIaaS management plane based on Tail-Risk Envelopes (TREs): signed, composable per-domain descriptors that combine deterministic guardrails with stochastic rate-latency-impairment models. Using stochastic network calculus, we derive bounds on end-to-end delay violation probabilities across tandem domains and obtain an optimization-ready risk-budget decomposition. We show that tenant-level reservations prevent bursty traffic from inflating tail latency under TRE contracts. An auditing layer then uses runtime telemetry to estimate extreme-percentile performance, quantify uncertainty, and attribute tail-risk to each domain for accountability. Packet-level Monte-Carlo simulations demonstrate improved p99.9 compliance under overload via admission control and robust tenant isolation under correlated burstiness.

</details>


### [300] [AI-Paging: Lease-Based Execution Anchoring for Network-Exposed AI-as-a-Service](https://arxiv.org/abs/2602.15286)
*Merve Saimler,Mohaned Chraiti*

Main category: cs.NI

TL;DR: This paper introduces the concept of AI-Paging in the context of AI-as-a-Service (AIaaS), enabling network providers to match user intents to suitable AI models under policy and QoS constraints while ensuring reliability and dynamic adaptability.


<details>
  <summary>Details</summary>
Motivation: With the rise of AI-as-a-Service and its diverse deployment options, end users struggle to manually select suitable AI models. This paper aims to empower 6G network providers to manage intent-driven automatic AI model matching and execution placement while addressing policy, trust, and QoS constraints.

Method: The proposed AI-Paging mechanism resolves user intent into a scoped session token, service identity, and expiring admission lease, enabling user-plane steering to an AI execution anchor. It employs existing network control-plane mechanisms and is designed for compatibility with 3GPP architectures.

Result: AI-Paging is prototyped and evaluated for transaction latency, relocation interruption, lease expiry enforcement correctness, and overhead under mobility and network failure scenarios, demonstrating its efficiency and reliability.

Conclusion: AI-Paging effectively facilitates seamless intent-to-model matching and execution under dynamic network constraints, ensuring a reliable, QoS-compliant AIaaS experience without requiring changes to existing network protocols.

Abstract: With AI-as-a-Service (AIaaS) now deployed across multiple providers and model tiers, selecting the appropriate model instance at run time is increasingly outside the end user's knowledge and operational control. Accordingly, the 6G service providers are envisioned to play a crucial role in exposing AIaaS in a setting where users submit only an intent while the network helps in the intent-to-model matching (resolution) and execution placement under policy, trust, and Quality of Service (QoS) constraints. The network role becomes to discover candidate execution endpoints and selects a suitable model/anchor under policy and QoS constraints in a process referred here to as AI-paging (by analogy to cellular call paging). In the proposed architecture, AI-paging is a control-plane transaction that resolves an intent into an AI service identity (AISI), a scoped session token (AIST), and an expiring admission lease (COMMIT) that authorizes user-plane steering to a selected AI execution anchor (AEXF) under a QoS binding. AI-Paging enforces two invariants: (i) lease-gated steering (without COMMIT, no steering state is installed) and (ii) make-before-break anchoring to support continuity and reliability of AIaaS services under dynamic network conditions. We prototype AI-Paging using existing control- and user-plane mechanisms (service-based control, QoS flows, and policy-based steering) with no new packet headers, ensuring compatibility with existing 3GPP-based exposure and management architectures, and evaluate transaction latency, relocation interruption, enforcement correctness under lease expiry, and audit-evidence overhead under mobility and failures.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [301] [GRACE: an Agentic AI for Particle Physics Experiment Design and Simulation](https://arxiv.org/abs/2602.15039)
*Justin Hill,Hong Joo Ryoo*

Main category: hep-ex

TL;DR: GRACE is an AI agent for designing better experimental setups in high-energy and nuclear physics, using simulations and natural language inputs.


<details>
  <summary>Details</summary>
Motivation: To improve autonomous experimental design in high-energy and nuclear physics through simulations and avoid relying on traditional trial-and-error methods.

Method: GRACE uses natural language or published data to build toy simulations and runs optimization via Monte Carlo methods and physics-based evaluation metrics.

Result: Demonstrated improvement in designs aligning with historical upgrades on experimental setups and successful benchmarking of HEP problem-solving.

Conclusion: Experimental design can be framed as a constrained search problem, and GRACE's approach establishes benchmarks for simulation-driven reasoning in physics setups.

Abstract: We present GRACE, a simulation-native agent for autonomous experimental design in high-energy and nuclear physics. Given multimodal input in the form of a natural-language prompt or a published experimental paper, the agent extracts a structured representation of the experiment, constructs a runnable toy simulation, and autonomously explores design modifications using first-principles Monte Carlo methods. Unlike agentic systems focused on operational control or execution of predefined procedures, GRACE addresses the upstream problem of experimental design: proposing non-obvious modifications to detector geometry, materials, and configurations that improve physics performance under physical and practical constraints. The agent evaluates candidate designs through repeated simulation, physics-motivated utility functions, and budget-aware escalation from fast parametric models to full Geant4 simulations, while maintaining strict reproducibility and provenance tracking. We demonstrate the framework on historical experimental setups, showing that the agent can identify optimization directions that align with known upgrade priorities, using only baseline simulation inputs. We also conducted a benchmark in which the agent identified the setup and proposed improvements from a suite of natural language prompts, with some supplied with a relevant physics research paper, of varying high energy physics (HEP) problem settings. This work establishes experimental design as a constrained search problem under physical law and introduces a new benchmark for autonomous, simulation-driven scientific reasoning in complex instruments.

</details>


### [302] [Enabling Low-Latency Machine learning on Radiation-Hard FPGAs with hls4ml](https://arxiv.org/abs/2602.15751)
*Katya Govorkova,Julian Garcia Pardinas,Vladimir Loncar,Victoria Nguyen,Sebastian Schmitt,Marco Pizzichemi,Loris Martinazzoli,Eluned Anne Smith*

Main category: hep-ex

TL;DR: This paper demonstrates the first successful machine learning application on radiation-hard FPGAs for high-energy physics experiments using a custom autoencoder, efficient quantization strategies, and a new backend for Microchip PolarFire FPGAs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable ultra-fast, radiation-hard machine learning applications on commercially available FPGAs for high-energy physics experiments, where conventional methods may fail due to high-radiation environments.

Method: The study developed a lightweight autoencoder, optimized it with hardware-aware 10-bit quantization, and created a new backend for the hls4ml library to support radiation-hard Microchip PolarFire FPGAs. They implemented, synthesized, and evaluated their solution on the PicoCal calorimeter as a test case.

Result: The autoencoder achieved 25 ns latency on a PolarFire FPGA, maintained low resource utilization, and its core logic was inherently protected within the FPGA design.

Conclusion: The approach proves feasible and paves the way for widespread adoption of machine learning models on radiation-hard FPGAs for future high-energy physics applications, overcoming a key barrier.

Abstract: This paper presents the first demonstration of a viable, ultra-fast, radiation-hard machine learning (ML) application on FPGAs, which could be used in future high-energy physics experiments. We present a three-fold contribution, with the PicoCal calorimeter, planned for the LHCb Upgrade II experiment, used as a test case. First, we develop a lightweight autoencoder to compress a 32-sample timing readout, representative of that of the PicoCal, into a two-dimensional latent space. Second, we introduce a systematic, hardware-aware quantization strategy and show that the model can be reduced to 10-bit weights with minimal performance loss. Third, as a barrier to the adoption of on-detector ML is the lack of support for radiation-hard FPGAs in the High-Energy Physics community's standard ML synthesis tool, hls4ml, we develop a new backend for this library. This new back-end enables the automatic translation of ML models into High-Level Synthesis (HLS) projects for the Microchip PolarFire family of FPGAs, one of the few commercially available and radiation hard FPGAs. We present the synthesis of the autoencoder on a target PolarFire FPGA, which indicates that a latency of 25 ns can be achieved. We show that the resources utilized are low enough that the model can be placed within the inherently protected logic of the FPGA. Our extension to hls4ml is a significant contribution, paving the way for broader adoption of ML on FPGAs in high-radiation environments.

</details>


### [303] [Neural Scaling Laws for Boosted Jet Tagging](https://arxiv.org/abs/2602.15781)
*Matthias Vigl,Nicole Hartman,Michael Kagan,Lukas Heinrich*

Main category: hep-ex

TL;DR: This paper examines neural scaling laws in High Energy Physics (HEP) using the JetClass dataset, focusing on how increased compute drives performance and the limits of performance scaling.


<details>
  <summary>Details</summary>
Motivation: Explore the application of scaling laws to datasets and compute in HEP, aiming to bridge gaps between HEP methodologies and industry-standard practices in machine learning.

Method: The study examines neural scaling laws using the JetClass dataset for boosted jet classification, analyzing effects of compute increase, data repetition, and input features on performance.

Result: Finds that performance improves with increased compute and that data repetition offers effective dataset size gains. Demonstrates that more expressive, lower-level features enhance performance limits.

Conclusion: Increased compute enables significant performance gains in HEP workflows, with potential improvements achieved by optimizing input features and addressing data repetition impacts.

Abstract: The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.

</details>
